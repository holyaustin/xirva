[{"id": "1911.00082", "submitter": "Frank Marrs", "authors": "Frank W. Marrs and Bailey K. Fosdick", "title": "Regression of binary network data with exchangeable latent errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected, binary network data consist of indicators of symmetric relations\nbetween pairs of actors. Regression models of such data allow for the\nestimation of effects of exogenous covariates on the network and for prediction\nof unobserved data. Ideally, estimators of the regression parameters should\naccount for the inherent dependencies among relations in the network that\ninvolve the same actor. To account for such dependencies, researchers have\ndeveloped a host of latent variable network models, however, estimation of many\nlatent variable network models is computationally onerous and which model is\nbest to base inference upon may not be clear. We propose the Probit\nExchangeable (PX) Model for undirected binary network data that is based on an\nassumption of exchangeability, which is common to many of the latent variable\nnetwork models in the literature. The PX model can represent the second moments\nof any exchangeable network model, yet specifies no particular parametric\nmodel. We present an algorithm for obtaining the maximum likelihood estimator\nof the PX model, as well as a modified version of the algorithm that is\nextremely computationally efficient and provides an approximate estimator.\nUsing simulation studies, we demonstrate the improvement in estimation of\nregression coefficients of the proposed model over existing latent variable\nnetwork models. In an analysis of purchases of politically-aligned books, we\ndemonstrate political polarization in purchase behavior and show that the\nproposed estimator significantly reduces runtime relative to estimators of\nlatent variable network models while maintaining predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 19:55:02 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 17:22:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Marrs", "Frank W.", ""], ["Fosdick", "Bailey K.", ""]]}, {"id": "1911.00098", "submitter": "Max Westphal", "authors": "Max Westphal", "title": "Simultaneous Inference for Multiple Proportions: A Multivariate\n  Beta-Binomial Model", "comments": "25 pages, 7 figures; v2: one reference updated, minor cosmetic\n  changes, contents unaffected; v3: slight abstract overhaul, typos fixed,\n  different template, contents unaffected; v4: added comment regarding\n  limitation of copula approach in discussion section, no other changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference in high-dimensional settings is challenging when\nstandard unregularized methods are employed. In this work, we focus on the case\nof multiple correlated proportions for which we develop a Bayesian inference\nframework. For this purpose, we construct an $m$-dimensional Beta distribution\nfrom a $2^m$-dimensional Dirichlet distribution, building on work by Olkin and\nTrikalinos (2015). This readily leads to a multivariate Beta-binomial model for\nwhich simple update rules from the common Dirichlet-multinomial model can be\nadopted. From the frequentist perspective, this approach amounts to adding\npseudo-observations to the data and allows a joint shrinkage estimation of mean\nvector and covariance matrix. For higher dimensions ($m > 10$), the extensive\nmodel based on $2^m$ parameters starts to become numerically infeasible. To\ncounter this problem, we utilize a reduced parametrisation which has only $1 +\nm(m + 1)/2$ parameters describing first and second order moments. A copula\nmodel can then be used to approximate the (posterior) multivariate Beta\ndistribution. A natural inference goal is the construction of multivariate\ncredible regions. The properties of different credible regions are assessed in\na simulation study in the context of investigating the accuracy of multiple\nbinary classifiers. It is shown that the extensive and copula approach lead to\na (Bayes) coverage probability very close to the target level. In this regard,\nthey outperform credible regions based on a normal approximation of the\nposterior distribution, in particular for small sample sizes. Additionally,\nthey always lead to credible regions which lie entirely in the parameter space\nwhich is not the case when the normal approximation is used.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 20:47:56 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 02:49:27 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 22:32:12 GMT"}, {"version": "v4", "created": "Fri, 20 Mar 2020 20:28:27 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Westphal", "Max", ""]]}, {"id": "1911.00115", "submitter": "Harlan Campbell", "authors": "Harlan Campbell", "title": "The consequences of checking for zero-inflation and overdispersion in\n  the analysis of count data", "comments": "30 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Count data are ubiquitous in ecology and the Poisson generalized linear model\n(GLM) is commonly used to model the association between counts and explanatory\nvariables of interest. When fitting this model to the data, one typically\nproceeds by first confirming that the data is not overdispersed and that there\nis no excess of zeros. If the data appear to be overdispersed or if there is\nany zero-inflation, key assumptions of the Poison GLM may be violated and\nresearchers will then typically consider alternatives to the Poison GLM. An\nimportant question is whether the potential model selection bias introduced by\nthis data-driven multi-stage procedure merits concern. In this paper, we\nconduct a large-scale simulation study to investigate the potential\nconsequences of model selection bias that can arise in the simple scenario of\nanalyzing a sample of potentially overdispersed, potentially zero-heavy, count\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 21:24:34 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 19:49:01 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 22:23:11 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 17:47:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Campbell", "Harlan", ""]]}, {"id": "1911.00143", "submitter": "Martin Welk", "authors": "Martin Welk", "title": "Multivariate Medians for Image and Shape Analysis", "comments": "Minor corrections, one additional reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having been studied since long by statisticians, multivariate median concepts\nfound their way into the image processing literature in the course of the last\ndecades, being used to construct robust and efficient denoising filters for\nmultivariate images such as colour images but also matrix-valued images. Based\non the similarities between image and geometric data as results of the sampling\nof continuous physical quantities, it can be expected that the understanding of\nmultivariate median filters for images provides a starting point for the\ndevelopment of shape processing techniques. This paper presents an overview of\nmultivariate median concepts relevant for image and shape processing. It\nfocusses on their mathematical principles and discusses important properties\nespecially in the context of image processing.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 22:54:31 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 16:37:57 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Welk", "Martin", ""]]}, {"id": "1911.00151", "submitter": "Joe Watson", "authors": "Joe Watson, Ruth Joy, Dominic Tollit, Sheila J Thornton, Marie\n  Auger-M\\'eth\\'e", "title": "Estimating animal utilization distributions from multiple data types: a\n  joint spatio-temporal point process framework", "comments": "23 pages of main text + 13 page supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of the spatial distribution of animals provide useful tools to help\necologists quantify species-environment relationships, and they are\nincreasingly being used to help determine the impacts of climate and habitat\nchanges on species. While high-quality survey-style data with known effort are\nsometimes available, often researchers have multiple datasets of varying\nquality and type. In particular, collections of sightings made by citizen\nscientists are becoming increasingly common, with no information typically\nprovided on their observer effort. Many standard modelling approaches ignore\nobserver effort completely, which can severely bias estimates of an animal's\ndistribution. Combining sightings data from observers who followed different\nprotocols is challenging. Any differences in observer skill, spatial effort,\nand the detectability of the animals across space all need to be accounted for.\nTo achieve this, we build upon the recent advancements made in integrative\nspecies distribution models and present a novel marked spatio-temporal point\nprocess framework for estimating the utilization distribution (UD) of the\nindividuals of a highly mobile species. We show that in certain settings, we\ncan also use the framework to combine the UDs from the sampled individuals to\nestimate the species' distribution. We combine the empirical results from a\nsimulation study with the implications outlined in a causal directed acyclic\ngraph to identify the necessary assumptions required for our framework to\ncontrol for observer effort when it is unknown. We then apply our framework to\ncombine multiple datasets collected on the endangered Southern Resident Killer\nWhales, to estimate their monthly effort-corrected space-use.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 23:43:16 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 18:43:09 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Watson", "Joe", ""], ["Joy", "Ruth", ""], ["Tollit", "Dominic", ""], ["Thornton", "Sheila J", ""], ["Auger-M\u00e9th\u00e9", "Marie", ""]]}, {"id": "1911.00198", "submitter": "Longhai Li", "authors": "Tingxuan Wu, Cindy Feng, Longhai Li", "title": "Residual Analysis for Censored Regression via Randomized Survival\n  Probabilities", "comments": "33 pages. Revised with reviewers' comments", "journal-ref": "Statistics in Medicine, 2021, Volume40, Issue6, Pages 1482-1497", "doi": "10.1002/sim.8852", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual analysis is extremely important in regression modelling. Residuals\nare used to graphically and numerically check the overall goodness-of-fit of a\nmodel, to discover the direction for improving the model, and to identify\noutlier observations. Cox-Snell residuals, which are transformed from survival\nprobabilities (SPs), are typically used for checking survival regression models\nfor failure times. Survival probabilities are uniformly distributed under the\ntrue model when there is no censored failure time. However, the SPs for\ncensored failure times are no longer uniformly distributed. Several non-random\nmethods have been proposed to modify CS residuals or SPs in the literature.\nHowever, their sampling distributions under the true model are not\ncharacterized, resulting in a lack of reference distributions for analysis with\nthese modified residuals. In this paper, we propose to use randomized survival\nprobabilities (RSP) to define residuals for censored data. We will show that\nRSPs always have the uniform distribution under the true model even with\ncensored times. Therefore, they can be transformed into residuals with the\nnormal quantile function. We call such residuals by normally-transformed RSP\n(NRSP) residuals. We conduct extensive simulation studies to demonstrate that\nNRSP residuals are normally distributed when the fitted model is correctly\nspecified. Consequently, the GOF test method by applying Shapiro-Wilk normality\ntest to NRSP residuals (NRSP-SW) is well-calibrated. Our simulation studies\nalso show the great power of the NRSP-SW method in detecting three kinds of\nmodel discrepancies. We also demonstrate the effectiveness of NRSP residuals in\nassessing three AFT models for a breast-cancer recurrent-free failure times\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 04:27:59 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 15:50:35 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 16:24:26 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wu", "Tingxuan", ""], ["Feng", "Cindy", ""], ["Li", "Longhai", ""]]}, {"id": "1911.00204", "submitter": "Koji Miyawaki", "authors": "Steven N. MacEachern and Koji Miyawaki", "title": "On the two-dataset problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the two-dataset problem, where data are collected from\ntwo potentially different populations sharing common aspects. This problem\narises when data are collected by two different types of researchers or from\ntwo different sources. We may reach invalid conclusions without using knowledge\nabout the data collection process. To address this problem, this paper develops\nstatistical models focusing on the difference in measurement and proposes two\nprediction errors that help to evaluate the underlying data collection process.\nAs a consequence, it is possible to discuss the heterogeneity/similarity of\ndata in terms of prediction. Two real datasets are selected to illustrate our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 05:25:33 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 01:00:39 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["MacEachern", "Steven N.", ""], ["Miyawaki", "Koji", ""]]}, {"id": "1911.00217", "submitter": "Jan Naudts", "authors": "Jan Naudts", "title": "Update of a conditional probability by minimal divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR physics.data-an quant-ph stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper investigates the situation that two events which are\nbelieved to be independent become statistically dependent during a subsequent\nobservation or measurement. The situation is well-known in quantum statistics\nbut occurs in many other contexts as well. The optimal update is obtained by\nminimizing either the Hellinger distance or the quadratic Bregman divergence.\nThe results obtained by the two methods differ.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 06:12:15 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Naudts", "Jan", ""]]}, {"id": "1911.00240", "submitter": "Tom\\'a\\v{s} Mrkvi\\v{c}ka", "authors": "Tomas Mrkvicka, Jiri Dvorak, Jonatan A. Gonzalez, Jorge Mateu", "title": "Revisiting the random shift approach for testing in spatial statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of non-parametric testing of independence of two\ncomponents of a stationary bivariate spatial process. In particular, we revisit\nthe random shift approach that has become a standard method for testing the\nindependent superposition hypothesis in spatial statistics, and it is widely\nused in a plethora of practical applications. However, this method has a\nproblem of liberality caused by breaking the marginal spatial correlation\nstructure due to the toroidal correction. This indeed causes that the\nassumption of exchangability, which is essential for the Monte Carlo test to be\nexact, is not fulfilled.\n  We present a number of permutation strategies and show that the random shift\nwith the variance correction brings a suitable improvement compared to the\ntorus correction in the random field case. It reduces the liberality and\nachieves the largest power from all investigated variants. To obtain the\nvariance for the variance correction method, several approaches were studied.\nThe best results were achieved, for the sample covariance as the test\nstatistics, with the correction factor $1/n$. This corresponds to the\nasymptotic order of the variance of the test statistics.\n  In the point process case, the problem of deviations from exchangeability is\nfar more complex and we propose an alternative strategy based on the mean cross\nnearest-neighbor distance and torus correction. It reduces the liberality but\nachieves slightly lower power than the usual cross $K$-function. Therefore we\nrecommend it, when the point patterns are clustered, where the cross\n$K$-function achieves liberality.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 07:52:45 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Mrkvicka", "Tomas", ""], ["Dvorak", "Jiri", ""], ["Gonzalez", "Jonatan A.", ""], ["Mateu", "Jorge", ""]]}, {"id": "1911.00347", "submitter": "Andrew Grant", "authors": "Andrew J. Grant and Stephen Burgess", "title": "An efficient and robust approach to Mendelian randomization with\n  measured pleiotropic effects in a high-dimensional setting", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxaa045", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Valid estimation of a causal effect using instrumental variables requires\nthat all of the instruments are independent of the outcome conditional on the\nrisk factor of interest and any confounders. In Mendelian randomization studies\nwith large numbers of genetic variants used as instruments, it is unlikely that\nthis condition will be met. Any given genetic variant could be associated with\na large number of traits, all of which represent potential pathways to the\noutcome which bypass the risk factor of interest. Such pleiotropy can be\naccounted for using standard multivariable Mendelian randomization with all\npossible pleiotropic traits included as covariates. However, the estimator\nobtained in this way will be inefficient if some of the covariates do not truly\nsit on pleiotropic pathways to the outcome. We present a method which uses\nregularization to identify which out of a set of potential covariates need to\nbe accounted for in a Mendelian randomization analysis in order to produce an\nefficient and robust estimator of a causal effect. The method can be used in\nthe case where individual-level data are not available and the analysis must\nrely on summary-level data only. It can also be used in the case where there\nare more covariates under consideration than instruments, which is not possible\nusing standard multivariable Mendelian randomization. We show the results of\nsimulation studies which demonstrate the performance of the proposed\nregularization method in realistic settings. We also illustrate the method in\nan applied example which looks at the causal effect of urate plasma\nconcentration on coronary heart disease.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 12:44:08 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Grant", "Andrew J.", ""], ["Burgess", "Stephen", ""]]}, {"id": "1911.00443", "submitter": "Florent  Sureau", "authors": "Florent Sureau, Alexis Lechat, Jean-Luc Starck", "title": "Deep Learning for space-variant deconvolution in galaxy surveys", "comments": null, "journal-ref": "A&A 641, A67 (2020)", "doi": "10.1051/0004-6361/201937039", "report-no": null, "categories": "astro-ph.IM eess.IV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolution of large survey images with millions of galaxies requires to\ndevelop a new generation of methods which can take into account a space variant\nPoint Spread Function (PSF) and have to be at the same time accurate and fast.\nWe investigate in this paper how Deep Learning (DL) could be used to perform\nthis task. We employ a U-Net Deep Neural Network (DNN) architecture to learn in\na supervised setting parameters adapted for galaxy image processing and study\ntwo strategies for deconvolution. The first approach is a post-processing of a\nmere Tikhonov deconvolution with closed form solution and the second one is an\niterative deconvolution framework based on the Alternating Direction Method of\nMultipliers (ADMM). Our numerical results based on GREAT3 simulations with\nrealistic galaxy images and PSFs show that our two approaches outperforms\nstandard techniques based on convex optimization, whether assessed in galaxy\nimage reconstruction or shape recovery. The approach based on Tikhonov\ndeconvolution leads to the most accurate results except for ellipticity errors\nat high signal to noise ratio where the ADMM approach performs slightly better,\nis also more computation-time efficient to process a large number of galaxies,\nand is therefore recommended in this scenario.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 16:11:48 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 13:13:31 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Sureau", "Florent", ""], ["Lechat", "Alexis", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1911.00448", "submitter": "Alexander Kreuzer", "authors": "Alexander Kreuzer, Luciana Dalla Valle and Claudia Czado", "title": "Bayesian Multivariate Nonlinear State Space Copula Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a flexible class of multivariate nonlinear\nnon-Gaussian state space models, based on copulas. More precisely, we assume\nthat the observation equation and the state equation are defined by copula\nfamilies that are not necessarily equal. For each time point, the resulting\nmodel can be described by a C-vine copula truncated after the first tree, where\nthe root node is represented by the latent state. Inference is performed within\nthe Bayesian framework, using the Hamiltonian Monte Carlo method, where a\nfurther D-vine truncated after the first tree is used as prior distribution to\ncapture the temporal dependence in the latent states. Simulation studies show\nthat the proposed copula-based approach is extremely flexible, since it is able\nto describe a wide range of dependence structures and, at the same time, allows\nus to deal with missing data. The application to atmospheric pollutant\nmeasurement data shows that our approach is suitable for accurate modeling and\nprediction of data dynamics in the presence of missing values. Comparison to a\nGaussian linear state space model and to Bayesian additive regression trees\nshows the superior performance of the proposed model with respect to predictive\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 16:30:14 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Kreuzer", "Alexander", ""], ["Valle", "Luciana Dalla", ""], ["Czado", "Claudia", ""]]}, {"id": "1911.00512", "submitter": "Fui Swen Kuh", "authors": "F. Swen Kuh and Grace S. Chiu and Anton H. Westveld", "title": "Modeling National Latent Socioeconomic Health and Examination of Policy\n  Effects via Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research develops a socioeconomic health index for nations through a\nmodel-based approach which incorporates spatial dependence and examines the\nimpact of a policy through a causal modeling framework. As the gross domestic\nproduct (GDP) has been regarded as a dated measure and tool for benchmarking a\nnation's economic performance, there has been a growing consensus for an\nalternative measure---such as a composite `wellbeing' index---to holistically\ncapture a country's socioeconomic health performance. Many conventional ways of\nconstructing wellbeing/health indices involve combining different observable\nmetrics, such as life expectancy and education level, to form an index.\nHowever, health is inherently latent with metrics actually being observable\nindicators of health. In contrast to the GDP or other conventional health\nindices, our approach provides a holistic quantification of the overall\n`health' of a nation. We build upon the latent health factor index (LHFI)\napproach that has been used to assess the unobservable ecological/ecosystem\nhealth. This framework integratively models the relationship between metrics,\nthe latent health, and the covariates that drive the notion of health. In this\npaper, the LHFI structure is integrated with spatial modeling and statistical\ncausal modeling, so as to evaluate the impact of a policy variable (mandatory\nmaternity leave days) on a nation's socioeconomic health, while formally\naccounting for spatial dependency among the nations. We apply our model to\ncountries around the world using data on various metrics and potential\ncovariates pertaining to different aspects of societal health. The approach is\nstructured in a Bayesian hierarchical framework and results are obtained by\nMarkov chain Monte Carlo techniques.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 18:13:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kuh", "F. Swen", ""], ["Chiu", "Grace S.", ""], ["Westveld", "Anton H.", ""]]}, {"id": "1911.00554", "submitter": "Graciela Boente Prof.", "authors": "Ana M. Bianco, Graciela Boente and Gonzalo Chebi", "title": "Penalized robust estimators in logistic regression with applications to\n  sparse models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse covariates are frequent in classification and regression problems and\nin these settings the task of variable selection is usually of interest. As it\nis well known, sparse statistical models correspond to situations where there\nare only a small number of non--zero parameters and for that reason, they are\nmuch easier to interpret than dense ones. In this paper, we focus on the\nlogistic regression model and our aim is to address robust and penalized\nestimation for the regression parameter. We introduce a family of penalized\nweighted $M-$type estimators for the logistic regression parameter that are\nstable against atypical data. We explore different penalizations functions and\nwe introduce the so--called Sign penalization. This new penalty has the\nadvantage that it depends only on one penalty parameter, avoiding arbitrary\ntuning constants. We discuss the variable selection capability of the given\nproposals as well as their asymptotic behaviour. Through a numerical study, we\ncompare the finite sample performance of the proposal corresponding to\ndifferent penalized estimators either robust or classical, under different\nscenarios. A robust cross--validation criterion is also presented. The analysis\nof two real data sets enables to investigate the stability of the penalized\nestimators to the presence of outliers.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 18:58:42 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 23:37:32 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Bianco", "Ana M.", ""], ["Boente", "Graciela", ""], ["Chebi", "Gonzalo", ""]]}, {"id": "1911.00619", "submitter": "Siddhant Wahal", "authors": "Siddhant Wahal and George Biros", "title": "BIMC: The Bayesian Inverse Monte Carlo method for goal-oriented\n  uncertainty quantification. Part I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating rare event probabilities, focusing on\nsystems whose evolution is governed by differential equations with uncertain\ninput parameters. If the system dynamics is expensive to compute, standard\nsampling algorithms such as the Monte Carlo method may require infeasible\nrunning times to accurately evaluate these probabilities. We propose an\nimportance sampling scheme (which we call BIMC) that relies on solving an\nauxiliary, fictitious Bayesian inverse problem. The solution of the inverse\nproblem yields a posterior PDF, a local Gaussian approximation to which serves\nas the importance sampling density. We apply BIMC to several problems and\ndemonstrate that it can lead to computational savings of several orders of\nmagnitude over the Monte Carlo method. We delineate conditions under which BIMC\nis optimal, as well as conditions when it can fail to yield an effective IS\ndensity.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 00:27:13 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Wahal", "Siddhant", ""], ["Biros", "George", ""]]}, {"id": "1911.00688", "submitter": "Masahiro Kato", "authors": "Masahiro Kato, Hikaru Kawarazaki", "title": "Model Specification Test with Unlabeled Data: Approach from Covariate\n  Shift", "comments": "The proof was wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework of the model specification test in regression\nusing unlabeled test data. In many cases, we have conducted statistical\ninferences based on the assumption that we can correctly specify a model.\nHowever, it is difficult to confirm whether a model is correctly specified. To\novercome this problem, existing works have devised statistical tests for model\nspecification. Existing works have defined a correctly specified model in\nregression as a model with zero conditional mean of the error term over train\ndata only. Extending the definition in conventional statistical tests, we\ndefine a correctly specified model as a model with zero conditional mean of the\nerror term over any distribution of the explanatory variable. This definition\nis a natural consequence of the orthogonality of the explanatory variable and\nthe error term. If a model does not satisfy this condition, the model might\nlack robustness with regards to the distribution shift. The proposed method\nwould enable us to reject a misspecified model under our definition. By\napplying the proposed method, we can obtain a model that predicts the label for\nthe unlabeled test data well without losing the interpretability of the model.\nIn experiments, we show how the proposed method works for synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 10:06:17 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 07:42:42 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Kato", "Masahiro", ""], ["Kawarazaki", "Hikaru", ""]]}, {"id": "1911.00702", "submitter": "Alexander Kreuzer", "authors": "Alexander Kreuzer and Claudia Czado", "title": "Bayesian inference for dynamic vine copulas in higher dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of dynamic vine copula models. This is an extension of\nstatic vine copulas and a generalization of dynamic C-vine and D-vine copulas\nstudied by Almeida et al (2016) and Goel and Mehra (2019). Within this class,\nwe allow for time-varying dependence by driving the vine copula parameters with\nlatent AR(1) processes. This modeling approach is very flexible but estimation\nis not straightforward due to the high-dimensional parameter space. We propose\na Bayesian estimation approach, which relies on a novel approximation of the\nposterior distribution. This approximation allows to use Markov Chain Monte\nCarlo methods, such as elliptical slice sampling, in a sequential way. In\ncontrast to other Bayesian sequential estimation procedures for vine copula\nmodels as proposed by Gruber and Czado (2015), there is no need to collapse\ncopula parameters to point estimates before proceeding to the next tree. Thus\nmore information and uncertainty is propagated from lower to higher trees. A\nsimulation study shows satisfactory performance of the Bayesian procedure. This\ndynamic modeling and inference approach can be applied in various fields, where\nstatic vine copulas have already proven to be successful, including\nenvironmental sciences, medicine and finance. Here we study the dependence\namong 21 exchange rates. For comparison we also estimate a static vine copula\nmodel and dynamic C-vine and D-vine copula models. This comparison shows\nsuperior performance of the proposed dynamic vine copula model with respect to\none day ahead forecasting accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 12:15:37 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Kreuzer", "Alexander", ""], ["Czado", "Claudia", ""]]}, {"id": "1911.00717", "submitter": "Ming-Chung Chang", "authors": "Ming-Chung Chang", "title": "An aberration criterion for conditional models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional models with one pair of conditional and conditioned factors in\nMukerjee et al. (2017) are extended to two pairs in this paper. The extension\nincludes the parametrization, effect hierarchy, sufficient conditions for\nuniversal optimality, aberration, complementary set theory and the strategy for\nfinding minimum aberration designs. A catalog of 16-run minimum aberration\ndesigns under conditional models is provided. For five to twelve factors, all\n16-run minimum aberration designs under conditional models are also minimum\naberration under traditional models.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 13:51:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chang", "Ming-Chung", ""]]}, {"id": "1911.00741", "submitter": "Li-Hsiang Lin", "authors": "Li-Hsiang Lin, Li-Shan Huang", "title": "Yakovlev Promotion Time Cure Model with Local Polynomial Estimation", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modeling survival data with a cure fraction, flexible modeling of\ncovariate effects on the probability of cure has important medical\nimplications, which aids investigators in identifying better treatments to\ncure. This paper studies a semiparametric form of the Yakovlev promotion time\ncure model that allows for nonlinear effects of a continuous covariate. We\nadopt the local polynomial approach and use the local likelihood criterion to\nderive nonlinear estimates of covariate effects on cure rates, assuming that\nthe baseline distribution function follows a parametric form. This way we adopt\na flexible method to estimate the cure rate locally, the important part in cure\nmodels, and a convenient way to estimate the baseline function globally. An\nalgorithm is proposed to implement estimation at both the local and global\nscales. Asymptotic properties of local polynomial estimates, the nonparametric\npart, are investigated in the presence of both censored and cured data, and the\nparametric part is shown to be root-n consistent. The proposed methods are\nillustrated by simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 15:46:22 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 02:25:06 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Lin", "Li-Hsiang", ""], ["Huang", "Li-Shan", ""]]}, {"id": "1911.00770", "submitter": "Daniel Oberski", "authors": "Daniel L. Oberski", "title": "Rank-deficiencies in a reduced information latent variable model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models are well-known to suffer from rank deficiencies,\ncausing problems with convergence and stability. Such problems are compounded\nin the \"reduced-group split-ballot multitrait-multimethod model\", which omits a\nset of moments from the estimation through a planned missing data design. This\npaper demonstrates the existence of rank deficiencies in this model and give\nthe explicit null space. It also demonstrates that sample size and distance\nfrom the rank-deficient point interact in their effects on convergence, causing\nconvergence to improve or worsen depending on both factors simultaneously.\nFurthermore, it notes that the latent variable correlations in the uncorrelated\nmethods SB-MTMM model remain unaffected by the rank deficiency. I conclude that\nmethodological experiments should be careful to manipulate both distance to\nknown rank-deficiencies and sample size, and report all results, not only the\napparently converged ones. Practitioners may consider that, even in the\npresence of nonconvergence or so-called \"inadmissible\" estimates, a subset of\nparameter estimates may still be consistent and stable.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 19:15:55 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Oberski", "Daniel L.", ""]]}, {"id": "1911.00797", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio G\\'omez-Rubio, Roger S. Bivand, H{\\aa}vard Rue", "title": "Bayesian model averaging with the integrated nested Laplace\n  approximation", "comments": "Submitted to Econometrics (MDPI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrated nested Laplace approximation (INLA) for Bayesian inference is\nan efficient approach to estimate the posterior marginal distributions of the\nparameters and latent effects of Bayesian hierarchical models that can be\nexpressed as latent Gaussian Markov random fields (GMRF). The representation as\na GMRF allows the associated software R-INLA to estimate the posterior\nmarginals in a fraction of the time as typical Markov chain Monte Carlo\nalgorithms. INLA can be extended by means of Bayesian model averaging (BMA) to\nincrease the number of models that it can fit to conditional latent GMRF. In\nthis paper we review the use of BMA with INLA and propose a new example on\nspatial econometrics models.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 23:46:07 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["G\u00f3mez-Rubio", "Virgilio", ""], ["Bivand", "Roger S.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1911.00862", "submitter": "Vera Djordjilovi\\'c", "authors": "Vera Djordjilovi\\'c, Jesse Hemerik, Magne Thoresen", "title": "Optimal two-stage testing of multiple mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis in high-dimensional settings often involves identifying\npotential mediators among a large number of measured variables. For this\npurpose, a two step familywise error rate (FWER) procedure called ScreenMin has\nbeen recently proposed (Djordjilovi\\'c et al. 2019). In ScreenMin, variables\nare first screened and only those that pass the screening are tested. The\nproposed threshold for selection has been shown to guarantee asymptotic FWER.\nIn this work, we investigate the impact of the selection threshold on the\nfinite sample FWER. We derive power maximizing selection threshold and show\nthat it is well approximated by an adaptive threshold of Wang et al. (2016). We\nstudy the performance of the proposed procedures in a simulation study, and\napply them to a case-control study examining the effect of fish intake on the\nrisk of colorectal adenoma.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 10:53:23 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Djordjilovi\u0107", "Vera", ""], ["Hemerik", "Jesse", ""], ["Thoresen", "Magne", ""]]}, {"id": "1911.00878", "submitter": "Jagath Senarathne", "authors": "S. G. Jagath Senarathne, Antony M. Overstall, James M. McGree", "title": "Bayesian adaptive N-of-1 trials for estimating population and individual\n  treatment effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel adaptive design algorithm that can be used to\nfind optimal treatment allocations in N-of-1 clinical trials. This new\nmethodology uses two Laplace approximations to provide a computationally\nefficient estimate of population and individual random effects within a\nrepeated measures, adaptive design framework. Given the efficiency of this\napproach, it is also adopted for treatment selection to target the collection\nof data for the precise estimation of treatment effects. To evaluate this\napproach, we consider both a simulated and motivating N-of-1 clinical trial\nfrom the literature. For each trial, our methods were compared to the\nmulti-armed bandit approach and a randomised N-of-1 trial design in terms of\nidentifying the best treatment for each patient and the information gained\nabout the model parameters. The results show that our new approach selects\ndesigns that are highly efficient in achieving each of these objectives. As\nsuch, we propose our Laplace-based algorithm as an efficient approach for\ndesigning adaptive N-of-1 trials.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 12:53:41 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 02:27:53 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 03:59:33 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Senarathne", "S. G. Jagath", ""], ["Overstall", "Antony M.", ""], ["McGree", "James M.", ""]]}, {"id": "1911.00922", "submitter": "Yuhao Su", "authors": "Yuhao Su and Jie Ding", "title": "Variable Grouping Based Bayesian Additive Regression Tree", "comments": "5 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using ensemble methods for regression has been a large success in obtaining\nhigh-accuracy prediction. Examples are Bagging, Random forest, Boosting, BART\n(Bayesian additive regression tree), and their variants. In this paper, we\npropose a new perspective named variable grouping to enhance the predictive\nperformance. The main idea is to seek for potential grouping of variables in\nsuch way that there is no nonlinear interaction term between variables of\ndifferent groups. Given a sum-of-learner model, each learner will only be\nresponsible for one group of variables, which would be more efficient in\nmodeling nonlinear interactions. We propose a two-stage method named variable\ngrouping based Bayesian additive regression tree (GBART) with a well-developed\npython package gbart available. The first stage is to search for potential\ninteractions and an appropriate grouping of variables. The second stage is to\nbuild a final model based on the discovered groups. Experiments on synthetic\nand real data show that the proposed method can perform significantly better\nthan classical approaches.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 16:08:56 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 02:16:02 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Su", "Yuhao", ""], ["Ding", "Jie", ""]]}, {"id": "1911.00955", "submitter": "Chiwoo Park", "authors": "Chiwoo Park, Rahul Rao, Pavel Nikolaev, and Benji Maruyama", "title": "Gaussian process surrogate modeling with manipulating factors for carbon\n  nanotube growth experiments", "comments": "Keywords: Surrogate Modeling, Input Uncertainty, Control Uncertainty,\n  Gaussian Process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Gaussian process (GP) surrogate modeling for\npredicting the outcome of a physical experiment where some experimental inputs\nare controlled by other manipulating factors. Particularly, we are interested\nin the case where the control precision is not very high, so the input factor\nvalues vary significantly even under the same setting of the corresponding\nmanipulating factors. The case is observed in our main application to carbon\nnanotube growth experiments, where one experimental input among many is\nmanipulated by another manipulating factors, and the relation between the input\nand the manipulating factors significantly varies in the dates and times of\noperations. Due to this variation, the standard GP surrogate that directly\nrelates the manipulating factors to the experimental outcome does not provide a\ngreat predictive power on the outcome. At the same time, the GP model relating\nthe main factors to the outcome directly is not appropriate for the prediction\npurpose because the main factors cannot be accurately set as planned for a\nfuture experiment. Motivated by the carbon nanotube example, we propose a\ntwo-tiered GP model, where the bottom tier relates the manipulating factors to\nthe corresponding main factors with potential biases and variation independent\nof the manipulating factors, and the top tier relates the main factors to the\nexperimental outcome. Our two-tier model explicitly models the propagation of\nthe control uncertainty to the experimental outcome through the two GP modeling\ntiers. We present the inference and hyper-parameter estimation of the proposed\nmodel. The proposed approach is illustrated with the motivating example of a\nclosed-loop autonomous research system for carbon nanotube growth experiments,\nand the test results are reported with the comparison to a benchmark method,\ni.e. a standard GP model.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 19:47:08 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 12:30:25 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Park", "Chiwoo", ""], ["Rao", "Rahul", ""], ["Nikolaev", "Pavel", ""], ["Maruyama", "Benji", ""]]}, {"id": "1911.00995", "submitter": "Nicholas James", "authors": "Nick James, Max Menzies, Lamiae Azizi, Jennifer Chan", "title": "Novel semi-metrics for multivariate change point analysis and anomaly\n  detection", "comments": "Accepted manuscript. Minor edits since v2. Equal contribution from\n  first two authors", "journal-ref": "Physica D: Nonlinear Phenomena 412 (2020) 132636", "doi": "10.1016/j.physd.2020.132636", "report-no": null, "categories": "cs.LG math.DS stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for determining similarity and anomalies\nbetween time series, most practically effective in large collections of (likely\nrelated) time series, by measuring distances between structural breaks within\nsuch a collection. We introduce a class of \\emph{semi-metric} distance\nmeasures, which we term \\emph{MJ distances}. These semi-metrics provide an\nadvantage over existing options such as the Hausdorff and Wasserstein metrics.\nWe prove they have desirable properties, including better sensitivity to\noutliers, while experiments on simulated data demonstrate that they uncover\nsimilarity within collections of time series more effectively. Semi-metrics\ncarry a potential disadvantage: without the triangle inequality, they may not\nsatisfy a \"transitivity property of closeness.\" We analyse this failure with\nproof and introduce an computational method to investigate, in which we\ndemonstrate that our semi-metrics violate transitivity infrequently and mildly.\nFinally, we apply our methods to cryptocurrency and measles data, introducing a\njudicious application of eigenvalue analysis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 00:04:30 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 14:58:46 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 10:44:42 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""], ["Azizi", "Lamiae", ""], ["Chan", "Jennifer", ""]]}, {"id": "1911.01018", "submitter": "Chao Gao", "authors": "Chao Gao and Anderson Y. Zhang", "title": "Iterative Algorithm for Discrete Structure Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general modeling and algorithmic framework for discrete\nstructure recovery that can be applied to a wide range of problems. Under this\nframework, we are able to study the recovery of clustering labels, ranks of\nplayers, signs of regression coefficients, cyclic shifts, and even group\nelements from a unified perspective. A simple iterative algorithm is proposed\nfor discrete structure recovery, which generalizes methods including Lloyd's\nalgorithm and the power method. A linear convergence result for the proposed\nalgorithm is established in this paper under appropriate abstract conditions on\nstochastic errors and initialization. We illustrate our general theory by\napplying it on several representative problems: (1) clustering in Gaussian\nmixture model, (2) approximate ranking, (3) sign recovery in compressed\nsensing, (4) multireference alignment, and (5) group synchronization, and show\nthat minimax rate is achieved in each case.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 03:11:10 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 05:49:35 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gao", "Chao", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "1911.01040", "submitter": "Adel Javanmard", "authors": "Yash Deshpande, Adel Javanmard, Mohammad Mehrabi", "title": "Online Debiasing for Adaptively Collected High-dimensional Data with\n  Applications to Time Series Analysis", "comments": "66 pages, 2 tables, 11 figures; updated with minor fixes and\n  reorganization", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive collection of data is commonplace in applications throughout science\nand engineering. From the point of view of statistical inference however,\nadaptive data collection induces memory and correlation in the samples, and\nposes significant challenge. We consider the high-dimensional linear\nregression, where the samples are collected adaptively, and the sample size $n$\ncan be smaller than $p$, the number of covariates. In this setting, there are\ntwo distinct sources of bias: the first due to regularization imposed for\nconsistent estimation, e.g. using the LASSO, and the second due to adaptivity\nin collecting the samples. We propose \"online debiasing\", a general procedure\nfor estimators such as the LASSO, which addresses both sources of bias. In two\nconcrete contexts $(i)$ time series analysis and $(ii)$ batched data\ncollection, we demonstrate that online debiasing optimally debiases the LASSO\nestimate when the underlying parameter $\\theta_0$ has sparsity of order\n$o(\\sqrt{n}/\\log p)$. In this regime, the debiased estimator can be used to\ncompute $p$-values and confidence intervals of optimal size.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 06:03:58 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 17:41:44 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 19:39:33 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Deshpande", "Yash", ""], ["Javanmard", "Adel", ""], ["Mehrabi", "Mohammad", ""]]}, {"id": "1911.01081", "submitter": "Alvaro Mendez Civieta", "authors": "\\'Alvaro M\\'endez Civieta and M. Carmen Aguilera-Morillo and Rosa E.\n  Lillo", "title": "Quantile regression: a penalization approach", "comments": "9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse group LASSO (SGL) is a penalization technique used in regression\nproblems where the covariates have a natural grouped structure and provides\nsolutions that are both between and within group sparse. In this paper the SGL\nis introduced to the quantile regression (QR) framework, and a more flexible\nversion, the adaptive sparse group LASSO (ASGL), is proposed. This proposal\nadds weights to the penalization improving prediction accuracy. Usually,\nadaptive weights are taken as a function of the original nonpenalized solution\nmodel. This approach is only feasible in the n > p framework. In this work, a\nsolution that allows using adaptive weights in high-dimensional scenarios is\nproposed. The benefits of this proposal are studied both in synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 09:17:34 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Civieta", "\u00c1lvaro M\u00e9ndez", ""], ["Aguilera-Morillo", "M. Carmen", ""], ["Lillo", "Rosa E.", ""]]}, {"id": "1911.01287", "submitter": "Masahiro Tanaka", "authors": "Masahiro Tanaka", "title": "Bayesian Matrix Completion Approach to Causal Inference with Panel Data", "comments": null, "journal-ref": "Journal of Statistical Theory and Practice, Volume 15, Article:\n  49, 2021", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a new Bayesian approach to infer binary treatment\neffects. The approach treats counterfactual untreated outcomes as missing\nobservations and infers them by completing a matrix composed of realized and\npotential untreated outcomes using a data augmentation technique. We also\ndevelop a tailored prior that helps in the identification of parameters and\ninduces the matrix of untreated outcomes to be approximately low rank.\nPosterior draws are simulated using a Markov Chain Monte Carlo sampler. While\nthe proposed approach is similar to synthetic control methods and other related\nmethods, it has several notable advantages. First, unlike synthetic control\nmethods, the proposed approach does not require stringent assumptions. Second,\nin contrast to non-Bayesian approaches, the proposed method can quantify\nuncertainty about inferences in a straightforward and consistent manner. By\nmeans of a series of simulation studies, we show that our proposal has a better\nfinite sample performance than that of the existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 15:39:24 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:25:40 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 15:32:27 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 16:01:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tanaka", "Masahiro", ""]]}, {"id": "1911.01331", "submitter": "Philip Chodrow", "authors": "Philip Chodrow and Andrew Mellor", "title": "Annotated Hypergraphs: Models and Applications", "comments": "22 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraphs offer a natural modeling language for studying polyadic\ninteractions between sets of entities. Many polyadic interactions are\nasymmetric, with nodes playing distinctive roles. In an academic collaboration\nnetwork, for example, the order of authors on a paper often reflects the nature\nof their contributions to the completed work. To model these networks, we\nintroduce \\emph{annotated hypergraphs} as natural polyadic generalizations of\ndirected graphs. Annotated hypergraphs form a highly general framework for\nincorporating metadata into polyadic graph models. To facilitate data analysis\nwith annotated hypergraphs, we construct a role-aware configuration null model\nfor these structures and prove an efficient Markov Chain Monte Carlo scheme for\nsampling from it. We proceed to formulate several metrics and algorithms for\nthe analysis of annotated hypergraphs. Several of these, such as assortativity\nand modularity, naturally generalize dyadic counterparts. Other metrics, such\nas local role densities, are unique to the setting of annotated hypergraphs. We\nillustrate our techniques on six digital social networks, and present a\ndetailed case-study of the Enron email data set.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:49:28 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chodrow", "Philip", ""], ["Mellor", "Andrew", ""]]}, {"id": "1911.01340", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon and Arnaud Doucet", "title": "Non-reversible jump algorithms for Bayesian nested model selection", "comments": "To appear in Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-reversible Markov chain Monte Carlo methods often outperform their\nreversible counterparts in terms of asymptotic variance of ergodic averages and\nmixing properties. Lifting the state-space (Chen et al., 1999; Diaconis et al.,\n2000) is a generic technique for constructing such samplers. The idea is to\nthink of the random variables we want to generate as position variables and to\nassociate to them direction variables so as to design Markov chains which do\nnot have the diffusive behaviour often exhibited by reversible schemes. In this\npaper, we explore the benefits of using such ideas in the context of Bayesian\nmodel choice for nested models, a class of models for which the model indicator\nvariable is an ordinal random variable. By lifting this model indicator\nvariable, we obtain non-reversible jump algorithms, a non-reversible version of\nthe popular reversible jump algorithms introduced by Green (1995). This simple\nalgorithmic modification provides samplers which can empirically outperform\ntheir reversible counterparts at no extra computational cost. The code to\nreproduce all experiments is available online.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:05:18 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 16:44:23 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 01:26:00 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gagnon", "Philippe", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1911.01385", "submitter": "Per Block", "authors": "Per Block, James Hollway, Christoph Stadtfeld, Johan Koskinen, Tom\n  Snijders", "title": "\"Predicting\" after peeking into the future: Correcting a fundamental\n  flaw in the SAOM -- TERGM comparison of Leifeld and Cranmer (2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the empirical comparison of SAOMs and TERGMs by Leifeld and Cranmer\n(2019) in Network Science. We note that their model specification uses nodal\ncovariates calculated from observed degrees instead of using structural\neffects, thus turning endogeneity into circularity. In consequence, their\nout-of-sample predictions using TERGMs are based on out-of-sample information\nand thereby predict the future using observations from the future. We conclude\nthat their analysis rest on erroneous model specifications that render the\narticle's conclusions meaningless. Consequently, researchers should disregard\nrecommendations from the criticized paper when making informed modelling\nchoices.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:13:10 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Block", "Per", ""], ["Hollway", "James", ""], ["Stadtfeld", "Christoph", ""], ["Koskinen", "Johan", ""], ["Snijders", "Tom", ""]]}, {"id": "1911.01429", "submitter": "Johann Brehmer Mr", "authors": "Kyle Cranmer, Johann Brehmer, Gilles Louppe", "title": "The frontier of simulation-based inference", "comments": "10 pages, 3 figures, proceedings for the Sackler Colloquia at the US\n  National Academy of Sciences. v2: fixed typos. v3: clarified text, added\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many domains of science have developed complex simulations to describe\nphenomena of interest. While these simulations provide high-fidelity models,\nthey are poorly suited for inference and lead to challenging inverse problems.\nWe review the rapidly developing field of simulation-based inference and\nidentify the forces giving new momentum to the field. Finally, we describe how\nthe frontier is expanding so that a broad audience can appreciate the profound\nchange these developments may have on science.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 19:00:00 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 20:16:42 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 14:08:04 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Cranmer", "Kyle", ""], ["Brehmer", "Johann", ""], ["Louppe", "Gilles", ""]]}, {"id": "1911.01516", "submitter": "Guanhua Fang", "authors": "Guanhua Fang and Zhiliang Ying", "title": "Latent Theme Dictionary Model for Finding Co-occurrent Patterns in\n  Process Data", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Process data, temporally ordered categorical observations, are of recent\ninterest due to its increasing abundance and the desire to extract useful\ninformation. A process is a collection of time-stamped events of different\ntypes, recording how an individual behaves in a given time period. The process\ndata are too complex in terms of size and irregularity for the classical\npsychometric models to be applicable, at least directly, and, consequently, it\nis desirable to develop new ways for modeling and analysis. We introduce herein\na latent theme dictionary model (LTDM) for processes that identifies\nco-occurrent event patterns and individuals with similar behavioral patterns.\nTheoretical properties are established under certain regularity conditions for\nthe likelihood based estimation and inference. A non-parametric Bayes LTDM\nalgorithm using the Markov Chain Monte Carlo method is proposed for\ncomputation. Simulation studies show that the proposed approach performs well\nin a range of situations. The proposed method is applied to an item in the 2012\nProgramme for International Student Assessment with interpretable findings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 22:22:08 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 03:03:37 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Fang", "Guanhua", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1911.01525", "submitter": "Wei Han", "authors": "Wei Han, Yun Yang", "title": "Statistical Inference in Mean-Field Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct non-asymptotic analysis on the mean-field variational inference\nfor approximating posterior distributions in complex Bayesian models that may\ninvolve latent variables. We show that the mean-field approximation to the\nposterior can be well-approximated relative to the Kullback-Leibler divergence\ndiscrepancy measure by a normal distribution whose center is the maximum\nlikelihood estimator (MLE). In particular, our results imply that the center of\nthe mean-field approximation matches the MLE up to higher-order terms and there\nis essentially no loss of efficiency in using it as a point estimator for the\nparameter in any regular parametric model with latent variables. We also\npropose a new class of variational weighted likelihood bootstrap (VWLB) methods\nfor quantifying the uncertainty in the mean-field variational inference. The\nproposed VWLB can be viewed as a new sampling scheme that produces independent\nsamples for approximating the posterior. Comparing with traditional sampling\nalgorithms such Markov Chain Monte Carlo, VWLB can be implemented in parallel\nand is free of tuning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 23:08:49 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Han", "Wei", ""], ["Yang", "Yun", ""]]}, {"id": "1911.01576", "submitter": "Jonas Moss", "authors": "Jonas Moss", "title": "Correcting for attenuation due to measurement error", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a frequentist method for quantifying uncertainty when correcting\ncorrelations for attenuation due to measurement error. The method is\nconservative but has far better coverage properties than the methods currently\nused when sample sizes are small. I recommend the use of confidence curves in\nfavor of confidence intervals when this method is used. I introduce the R\npackage \"attenuation\" which can be used to calculate and visualize the methods\ndescribed in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 09:21:56 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Moss", "Jonas", ""]]}, {"id": "1911.01583", "submitter": "Guanhua Fang", "authors": "Haochen Xu and Guanhua Fang and Zhiliang Ying", "title": "A Latent Topic Model with Markovian Transition for Process Data", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a latent topic model with a Markovian transition for process data,\nwhich consist of time-stamped events recorded in a log file. Such data are\nbecoming more widely available in computer-based educational assessment with\ncomplex problem solving items. The proposed model can be viewed as an extension\nof the hierarchical Bayesian topic model with a hidden Markov structure to\naccommodate the underlying evolution of an examinee's latent state. Using topic\ntransition probabilities along with response times enables us to capture\nexaminees' learning trajectories, making clustering/classification more\nefficient. A forward-backward variational expectation-maximization (FB-VEM)\nalgorithm is developed to tackle the challenging computational problem. Useful\ntheoretical properties are established under certain asymptotic regimes. The\nproposed method is applied to a complex problem solving item in 2012 Programme\nfor International Student Assessment (PISA 2012).\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 02:59:17 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Xu", "Haochen", ""], ["Fang", "Guanhua", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1911.01716", "submitter": "Chao Zheng", "authors": "Chao Zheng, Idris A. Eckley and Paul Fearnhead", "title": "Consistency of a range of penalised cost approaches for detecting\n  multiple changepoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to detect multiple changepoints is to minimise a measure of\ndata fit plus a penalty that is linear in the number of changepoints. This\npaper shows that the general finite sample behaviour of such a method can be\nrelated to its behaviour when analysing data with either none or one\nchangepoint. This results in simpler conditions for verifying whether the\nmethod will consistently estimate the number and locations of the changepoints.\nWe apply and demonstrate the usefulness of this result for a range of\nchangepoint problems. Our new results include a weaker condition on the choice\nof penalty required to have consistency in a change-in-slope model; and the\nfirst results for the accuracy of recently-proposed methods for detecting\nspikes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 11:22:07 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Zheng", "Chao", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1911.01827", "submitter": "Quan Zhang", "authors": "Quan Zhang, Qiang Gao, Mingfeng Lin, Mingyuan Zhou", "title": "Weibull Racing Survival Analysis for Competing Events and a Study of\n  Loan Payoff and Default", "comments": "40 pages, 7 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Bayesian nonparametric Weibull delegate racing (WDR) to explicitly\nmodel surviving under competing events and to interpret how the covariates\naccelerate or decelerate the event times. WDR explains non-monotonic covariate\neffects by racing a potentially infinite number of sub-events, relaxing the\nubiquitous proportional-hazards assumption which may be too restrictive. WDR\ncan handle different types of censoring and missing event times or types. For\ninference, we develop a Gibbs-sampler-based MCMC algorithm along with a maximum\na posteriori estimation for big data applications. We use synthetic data\nanalysis to demonstrate the flexibility and parsimonious nonlinearity of WDR.\nWe also use a data set of time to loan payoff and default from Prosper.com to\nshowcase the interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 02:32:50 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 15:20:23 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhang", "Quan", ""], ["Gao", "Qiang", ""], ["Lin", "Mingfeng", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1911.01850", "submitter": "Niklas Pfister", "authors": "Niklas Pfister, Evan G. Williams, Jonas Peters, Ruedi Aebersold and\n  Peter B\\\"uhlmann", "title": "Stabilizing Variable Selection and Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regression in which one predicts a response $Y$ with a set of\npredictors $X$ across different experiments or environments. This is a common\nsetup in many data-driven scientific fields and we argue that statistical\ninference can benefit from an analysis that takes into account the\ndistributional changes across environments. In particular, it is useful to\ndistinguish between stable and unstable predictors, i.e., predictors which have\na fixed or a changing functional dependence on the response, respectively. We\nintroduce stabilized regression which explicitly enforces stability and thus\nimproves generalization performance to previously unseen environments. Our work\nis motivated by an application in systems biology. Using multiomic data, we\ndemonstrate how hypothesis generation about gene function can benefit from\nstabilized regression. We believe that a similar line of arguments for\nexploiting heterogeneity in data can be powerful for many other applications as\nwell. We draw a theoretical connection between multi-environment regression and\ncausal models, which allows to graphically characterize stable versus unstable\nfunctional dependence on the response. Formally, we introduce the notion of a\nstable blanket which is a subset of the predictors that lies between the direct\ncausal predictors and the Markov blanket. We prove that this set is optimal in\nthe sense that a regression based on these predictors minimizes the mean\nsquared prediction error given that the resulting regression generalizes to\nunseen new environments.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:04:33 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 05:58:56 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Pfister", "Niklas", ""], ["Williams", "Evan G.", ""], ["Peters", "Jonas", ""], ["Aebersold", "Ruedi", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1911.01855", "submitter": "Heather Mathews", "authors": "Heather Mathews, Vaishakhi Mayya, Alexander Volfovsky, Galen Reeves", "title": "Gaussian Mixture Models for Stochastic Block Models with Non-Vanishing\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection tasks have received a lot of attention across statistics,\nmachine learning, and information theory with a large body of work\nconcentrating on theoretical guarantees for the stochastic block model. One\nline of recent work has focused on modeling the spectral embedding of a network\nusing Gaussian mixture models (GMMs) in scaling regimes where the ability to\ndetect community memberships improves with the size of the network. However,\nthese regimes are not very realistic. This paper provides tractable methodology\nmotivated by new theoretical results for networks with non-vanishing noise. We\npresent a procedure for community detection using GMMs that incorporates\ncertain truncation and shrinkage effects that arise in the non-vanishing noise\nregime. We provide empirical validation of this new representation using both\nsimulated and real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:10:55 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Mathews", "Heather", ""], ["Mayya", "Vaishakhi", ""], ["Volfovsky", "Alexander", ""], ["Reeves", "Galen", ""]]}, {"id": "1911.01859", "submitter": "Timothy Cannings", "authors": "Timothy I. Cannings and Yingying Fan", "title": "The correlation-assisted missing data estimator", "comments": "46 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to estimation problems in settings with missing\ndata. Our proposal -- the Correlation-Assisted Missing data (CAM) estimator --\nworks by exploiting the relationship between the observations with missing\nfeatures and those without missing features in order to obtain improved\nprediction accuracy. In particular, our theoretical results elucidate general\nconditions under which the proposed CAM estimator has lower mean squared error\nthan the widely used complete-case approach in a range of estimation problems.\nWe showcase in detail how the CAM estimator can be applied to $U$-Statistics to\nobtain an unbiased, asymptotically Gaussian estimator that has lower variance\nthan the complete-case $U$-Statistic. Further, in nonparametric density\nestimation and regression problems, we construct our CAM estimator using kernel\nfunctions, and show it has lower asymptotic mean squared error than the\ncorresponding complete-case kernel estimator. We also include practical\ndemonstrations throughout the paper using simulated data and the Terneuzen\nbirth cohort and Brandsma datasets available from CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:18:59 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 11:56:44 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Cannings", "Timothy I.", ""], ["Fan", "Yingying", ""]]}, {"id": "1911.01874", "submitter": "Abel Dasylva Dr.", "authors": "Abel Dasylva, Arthur Goussanou, David Ajavon and Hanan Abousaleh", "title": "Revisiting the probabilistic method of record linkage", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DB cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theory, the probabilistic linkage method provides two distinct advantages\nover non-probabilistic methods, including minimal rates of linkage error and\naccurate measures of these rates for data users. However, implementations can\nfall short of these expectations either because the conditional independence\nassumption is made, or because a model with interactions is used but lacks the\nidentification property. In official statistics, this is currently the main\nchallenge to the automated production and use of linked data. To address this\nchallenge, a new methodology is described for proper linkage problems, where\nmatched records may be identified with a probability that is bounded away from\nzero, regardless of the population size. It models the number of neighbours of\na given record, i.e. the number of resembling records. To be specific, the\nproposed model is a finite mixture where each component is the sum of a\nBernoulli variable with an independent Poisson variable. It has the\nidentification property and yields solutions for many longstanding problems,\nincluding the evaluation of blocking criteria and the estimation of linkage\nerrors for probabilistic or non-probabilistic linkages, all without clerical\nreviews or conditional independence assumptions. Thus it also enables\nunsupervised machine learning solutions for record linkage problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:28:46 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Dasylva", "Abel", ""], ["Goussanou", "Arthur", ""], ["Ajavon", "David", ""], ["Abousaleh", "Hanan", ""]]}, {"id": "1911.01929", "submitter": "Pavel Berkovich", "authors": "Pavel Berkovich, Eric Perim, Wessel Bruinsma", "title": "GP-ALPS: Automatic Latent Process Selection for Multi-Output Gaussian\n  Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple and widely adopted approach to extend Gaussian processes (GPs) to\nmultiple outputs is to model each output as a linear combination of a\ncollection of shared, unobserved latent GPs. An issue with this approach is\nchoosing the number of latent processes and their kernels. These choices are\ntypically done manually, which can be time consuming and prone to human biases.\nWe propose Gaussian Process Automatic Latent Process Selection (GP-ALPS), which\nautomatically chooses the latent processes by turning off those that do not\nmeaningfully contribute to explaining the data. We develop a variational\ninference scheme, assess the quality of the variational posterior by comparing\nit against the gold standard MCMC, and demonstrate the suitability of GP-ALPS\nin a set of preliminary experiments.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 16:46:37 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 12:02:55 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Berkovich", "Pavel", ""], ["Perim", "Eric", ""], ["Bruinsma", "Wessel", ""]]}, {"id": "1911.02029", "submitter": "Yifan Cui", "authors": "Yifan Cui and Eric Tchetgen Tchetgen", "title": "Selective machine learning of doubly robust functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While model selection is a well-studied topic in parametric and nonparametric\nregression or density estimation, selection of possibly high-dimensional\nnuisance parameters in semiparametric problems is far less developed. In this\npaper, we propose a selective machine learning framework for making inferences\nabout a finite-dimensional functional defined on a semiparametric model, when\nthe latter admits a doubly robust estimating function and several candidate\nmachine learning algorithms are available for estimating the nuisance\nparameters. We introduce two new selection criteria for bias reduction in\nestimating the functional of interest, each based on a novel definition of\npseudo-risk for the functional that embodies the double robustness property and\nthus is used to select the pair of learners that is nearest to fulfilling this\nproperty. We establish an oracle property for a multi-fold cross-validation\nversion of the new selection criteria which states that our empirical criteria\nperform nearly as well as an oracle with a priori knowledge of the pseudo-risk\nfor each pair of candidate learners. We also describe a smooth approximation to\nthe selection criteria which allows for valid post-selection inference.\nFinally, we apply the approach to model selection of a semiparametric estimator\nof average treatment effect given an ensemble of candidate machine learners to\naccount for confounding in an observational study.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:00:03 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 03:35:30 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 17:49:49 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 18:43:09 GMT"}, {"version": "v5", "created": "Mon, 12 Apr 2021 17:10:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Cui", "Yifan", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1911.02089", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon", "title": "Informed reversible jump algorithms", "comments": "Major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating information about the target distribution in proposal\nmechanisms generally increases the efficiency of Markov chain Monte Carlo\nalgorithms, when compared with those based on naive random walks. Hamiltonian\nMonte Carlo represents a successful example of fixed-dimensional algorithms\nincorporating gradient information. In trans-dimensional algorithms, Green\n(2003) recommended to generate the parameter proposals during model switches\nfrom normal distributions with informative means and covariance matrices. These\nproposal distributions can be viewed as asymptotic approximations to the\nparameter distributions, where the limit is with regard to the sample size.\nModels are typically proposed using uninformed uniform distributions. In this\npaper, we build on the approach of Zanella (2020) for discrete spaces to\nincorporate information about neighbouring models. We rely on approximations to\nposterior model probabilities that are asymptotically exact. We prove that, in\nsome scenarios, the samplers combining this approach with that of Green (2003)\nbehave like those that use the exact model probabilities and sample from the\nparameter distributions, in the large sample regime. We show that the\nimplementation of the proposed samplers is straightforward in some cases. The\nmethodology is applied to a real-data example. The code is available online.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 21:16:11 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 02:51:19 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Gagnon", "Philippe", ""]]}, {"id": "1911.02131", "submitter": "Andrew Raim", "authors": "Darcy Steeg Morris, Andrew M. Raim, and Kimberly F. Sellers", "title": "A Conway-Maxwell-Multinomial Distribution for Flexible Modeling of\n  Clustered Categorical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical data are often observed as counts resulting from a fixed number\nof trials in which each trial consists of making one selection from a\nprespecified set of categories. The multinomial distribution serves as a\nstandard model for such clustered data but assumes that trials are independent\nand identically distributed. Extensions such as Dirichlet-multinomial and\nrandom-clumped multinomial can express positive association, where trials are\nmore likely to result in a common category due to membership in a common\ncluster. This work considers a Conway-Maxwell-multinomial (CMM) distribution\nfor modeling clustered categorical data exhibiting positively or negatively\nassociated trials. The CMM distribution features a dispersion parameter which\nallows it to adapt to a range of association levels and includes several\nrecognizable distributions as special cases. We explore properties of CMM,\nillustrate its flexible characteristics, identify a method to efficiently\ncompute maximum likelihood (ML) estimates, present simulations of small sample\nproperties under ML estimation, and demonstrate the model via several data\nanalysis examples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 23:16:35 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Morris", "Darcy Steeg", ""], ["Raim", "Andrew M.", ""], ["Sellers", "Kimberly F.", ""]]}, {"id": "1911.02155", "submitter": "James Murphy", "authors": "James M. Murphy", "title": "Spatially regularized active diffusion learning for high-dimensional\n  images", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An active learning algorithm for the classification of high-dimensional\nimages is proposed in which spatially-regularized nonlinear diffusion geometry\nis used to characterize cluster cores. The proposed method samples from\nestimated cluster cores in order to generate a small but potent set of training\nlabels which propagate to the remainder of the dataset via the underlying\ndiffusion process. By spatially regularizing the rich, high-dimensional\nspectral information of the image to efficiently estimate the most significant\nand influential points in the data, our approach avoids redundancy in the\ntraining dataset. This allows it to produce high-accuracy labelings with a very\nsmall number of training labels. The proposed algorithm admits an efficient\nnumerical implementation that scales essentially linearly in the number of data\npoints under a suitable data model and enjoys state-of-the-art performance on\nreal hyperspectral images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 00:58:24 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Murphy", "James M.", ""]]}, {"id": "1911.02160", "submitter": "Akihiko Nishimura", "authors": "Akihiko Nishimura and Marc A. Suchard", "title": "Shrinkage with shrunken shoulders: inference via geometrically /\n  uniformly ergodic Gibbs sampler", "comments": "23 pages, 8 figures, (18 pages, 3 figures of Supplement). Code\n  available from https://github.com/aki-nishimura/bayes-bridge", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of continuous shrinkage priors -- with a \"spike\" near zero and\nheavy-tails towards infinity -- is an increasingly popular approach to induce\nsparsity in parameter estimates. When the parameters are only weakly identified\nby the likelihood, however, the posterior may end up with tails as heavy as the\nprior, jeopardizing robustness of inference. A natural solution is to \"shrink\nthe shoulders\" of a shrinkage prior by lightening up its tails beyond a\nreasonable parameter range, yielding the regularized version of the prior. We\ndevelop a regularization approach which, unlike previous proposals, preserves\ncomputationally attractive structures of original shrinkage priors. We study\ntheoretical properties of the Gibbs sampler on resulting posterior\ndistributions, with emphasis on convergence rates of the P{\\'o}lya-Gamma Gibbs\nsampler for sparse logistic regression. Our analysis shows that the proposed\nregularization leads to geometric ergodicity under a broad range of\nglobal-local shrinkage priors. Essentially, the only requirement is for the\nprior $\\pi_{\\rm local}$ on the local scale $\\lambda$ to satisfy $\\pi_{\\rm\nlocal}(0) < \\infty$. If $\\pi_{\\rm local}(\\cdot)$ further satisfies\n$\\lim_{\\lambda \\to 0} \\pi_{\\rm local}(\\lambda) / \\lambda^a < \\infty$ for $a >\n0$, as in the case of Bayesian bridge priors, we show the sampler to be\nuniformly ergodic.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 01:44:07 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 22:46:11 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 20:52:37 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 16:20:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Nishimura", "Akihiko", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1911.02171", "submitter": "Xin Xing", "authors": "Xin Xing, Zuofeng Shang, Pang Du, Ping Ma, Wenxuan Zhong and Jun S.\n  Liu", "title": "Minimax Nonparametric Two-sample Test under Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of comparing probability densities between two\ngroups. A new probabilistic tensor product smoothing spline framework is\ndeveloped to model the joint density of two variables. Under such a framework,\nthe probability density comparison is equivalent to testing the\npresence/absence of interactions. We propose a penalized likelihood ratio test\nfor such interaction testing and show that the test statistic is asymptotically\nchi-square distributed under the null hypothesis. Furthermore, we derive a\nsharp minimax testing rate based on the Bernstein width for nonparametric\ntwo-sample tests and show that our proposed test statistics is minimax optimal.\nIn addition, a data-adaptive tuning criterion is developed to choose the\npenalty parameter. Simulations and real applications demonstrate that the\nproposed test outperforms the conventional approaches under various scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 02:40:35 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 23:05:21 GMT"}, {"version": "v3", "created": "Sun, 5 Jan 2020 19:57:41 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 19:34:53 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Xing", "Xin", ""], ["Shang", "Zuofeng", ""], ["Du", "Pang", ""], ["Ma", "Ping", ""], ["Zhong", "Wenxuan", ""], ["Liu", "Jun S.", ""]]}, {"id": "1911.02192", "submitter": "Hang Li", "authors": "Hang Li, Enrique Del Castillo", "title": "Optimal Design of Experiments on Riemannian Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of optimal design of experiments has been traditionally developed\non an Euclidean space. In this paper, new theoretical results and an algorithm\nfor finding the optimal design of an experiment located on a Riemannian\nmanifold are provided. It is shown that analogously to the results in Euclidean\nspaces, D-optimal and G-optimal designs are equivalent on manifolds, and we\nprovide a lower bound for the maximum prediction variance of the response\nevaluated over the manifold. In addition, a converging algorithm that finds the\noptimal experimental design on manifold data is proposed. Numerical experiments\ndemonstrate the importance of considering the manifold structure in a designed\nexperiment when present, and the superiority of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 04:06:59 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 19:14:59 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Li", "Hang", ""], ["Del Castillo", "Enrique", ""]]}, {"id": "1911.02197", "submitter": "Junni Zhang", "authors": "Junni L. Zhang and Per Johansson", "title": "A Comparison of Methods of Inference in Randomized Experiments from a\n  Restricted Set of Allocations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rerandomization is a strategy of increasing efficiency as compared to\ncomplete randomization. The idea with rerandomization is that of removing\nallocations with imbalance in the observed covariates and then randomizing\nwithin the set of allocations with balance in these covariates. Standard\nasymptotic inference based on mean difference estimator is however conservative\nafter rerandomization. Given a Mahalanobis distance criterion for removing\nimbalanced allocations, Li et al. (2018) derived the asymptotic distribution of\nthe mean difference estimator and suggested a consistent estimator of its\nvariance. This paper discusses several alternative methods of inference under\nrerandomization, and compare their performance with that of the method in Li et\nal. (2018) through a large Monte Carlo simulation. We conclude that some of the\nmethods work better for small or moderate sample sized experiments than the\nmethod in Li et al. (2018).\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 04:17:14 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Zhang", "Junni L.", ""], ["Johansson", "Per", ""]]}, {"id": "1911.02205", "submitter": "Richard Chen", "authors": "Richard Y. Chen", "title": "The Fourier Transform Method for Volatility Functional Inference by\n  Asynchronous Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the volatility functional inference by Fourier transforms. This\nspectral framework is advantageous in that it harnesses the power of harmonic\nanalysis to handle missing data and asynchronous observations without any\nartificial time alignment nor data imputation. Under conditions, this spectral\napproach is consistent and we provide limit distributions using irregular and\nasynchronous observations. When observations are synchronous, the Fourier\ntransform method for volatility functionals attains both the optimal\nconvergence rate and the efficient bound in the sense of Le Cam and H\\'ajek.\nAnother finding is asynchronicity or missing data as a form of noise produces\n\"interference\" in the spectrum estimation and impacts on the convergence rate\nof volatility functional estimators. This new methodology extends previous\napplications of volatility functionals, including principal component analysis,\ngeneralized method of moments, continuous-time linear regression models et\ncetera, to high-frequency datasets of which asynchronicity is a prevailing\nfeature.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 05:14:53 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Chen", "Richard Y.", ""]]}, {"id": "1911.02249", "submitter": "Ghulam Abdul Qadir", "authors": "Ghulam A. Qadir, Ying Sun, Sebastian Kurtek", "title": "Estimation of Spatial Deformation for Nonstationary Processes via\n  Variogram Alignment", "comments": null, "journal-ref": "Technometrics 2021", "doi": "10.1080/00401706.2021.1883481", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modeling spatial processes, a second-order stationarity assumption is\noften made. However, for spatial data observed on a vast domain, the covariance\nfunction often varies over space, leading to a heterogeneous spatial dependence\nstructure, therefore requiring nonstationary modeling. Spatial deformation is\none of the main methods for modeling nonstationary processes, assuming the\nnonstationary process has a stationary counterpart in the deformed space. The\nestimation of the deformation function poses severe challenges. Here, we\nintroduce a novel approach for nonstationary geostatistical modeling, using\nspace deformation, when a single realization of the spatial process is\nobserved. Our method is based, at a fundamental level, on aligning regional\nvariograms, where warping variability of the distance from each subregion\nexplains the spatial nonstationarity. We propose to use multi-dimensional\nscaling to map the warped distances to spatial locations. We asses the\nperformance of our new method using multiple simulation studies. Additionally,\nwe illustrate our methodology on precipitation data to estimate the\nheterogeneous spatial dependence and to perform spatial predictions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 08:33:28 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 09:01:11 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Qadir", "Ghulam A.", ""], ["Sun", "Ying", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1911.02258", "submitter": "Ghulam Qadir", "authors": "Ghulam A. Qadir, Ying Sun", "title": "Semiparametric Estimation of Cross-covariance Functions for Multivariate\n  Random Fields", "comments": null, "journal-ref": "Biometric Methodology 2020", "doi": "10.1111/biom.13323", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of spatially referenced multivariate data has impelled\nresearchers to develop a procedure for the joint modeling of multiple spatial\nprocesses. This ordinarily involves modeling marginal and cross-process\ndependence for any arbitrary pair of locations using a multivariate spatial\ncovariance function. However, building a flexible multivariate spatial\ncovariance function that is nonnegative definite is challenging. Here, we\npropose a semiparametric approach for multivariate spatial covariance function\nestimation with approximate Mat\\'ern marginals and highly flexible\ncross-covariance functions via their spectral representations. The flexibility\nin our cross-covariance function arises due to B-spline based specification of\nthe underlying coherence functions, which in turn allows us to capture\nnon-trivial cross-spectral features. We then develop a likelihood-based\nestimation procedure and perform multiple simulation studies to demonstrate the\nperformance of our method, especially on the coherence function estimation.\nFinally, we analyze particulate matter concentrations ($\\text{PM}_{2.5}$) and\nwind speed data over the North-Eastern region of the United States, where we\nillustrate that our proposed method outperforms the commonly used full\nbivariate Mat\\'ern model and the linear model of coregionalization for spatial\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 08:53:31 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Qadir", "Ghulam A.", ""], ["Sun", "Ying", ""]]}, {"id": "1911.02457", "submitter": "Hadis Anahideh", "authors": "Hadis Anahideh, Jay Rosenberger, Victoria Chen", "title": "High-dimensional Black-box Optimization Under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing expensive black-box systems with limited data is an extremely\nchallenging problem. As a resolution, we present a new surrogate optimization\napproach by addressing two gaps in prior research -- unimportant input\nvariables and inefficient treatment of uncertainty associated with the\nblack-box output. We first design a new flexible non-interpolating parsimonious\nsurrogate model using a partitioning-based multivariate adaptive regression\nsplines approach, Tree Knot MARS (TK-MARS). The proposed model is specifically\ndesigned for optimization by capturing the structure of the function, bending\nat near-optimal locations, and is capable of screening unimportant input\nvariables. Furthermore, we develop a novel replication approach called\nSmart-Replication, to overcome the uncertainty associated with the black-box\noutput. The Smart-Replication approach identifies promising input points to\nreplicate and avoids unnecessary evaluations of other data points.\nSmart-Replication is agnostic to the choice of a surrogate and can adapt itself\nto an unknown noise level. Finally to demonstrate the effectiveness of our\nproposed approaches we consider different complex global optimization test\nfunctions from the surrogate optimization literature. The results indicate that\nTK-MARS outperforms original MARS within a surrogate optimization algorithm and\nsuccessfully detects important variables. The results also show that although\nnon-interpolating surrogates can mitigate uncertainty, replication is still\nbeneficial for optimizing highly complex black-box functions. The robustness\nand the quality of the final optimum solution found through Smart-Replication\nare competitive with that using no replications in environments with low levels\nof noise and using a fixed number of replications in highly noisy environments.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 16:13:36 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 03:47:27 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 17:01:46 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 15:55:32 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Anahideh", "Hadis", ""], ["Rosenberger", "Jay", ""], ["Chen", "Victoria", ""]]}, {"id": "1911.02629", "submitter": "Peter Marcy", "authors": "Peter W. Marcy, Scott A. Vander Wiel, Curtis B. Storlie, Veronica\n  Livescu, Curt A. Bronkhorst", "title": "Modeling Material Stress Using Integrated Gaussian Markov Random Fields", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2019.1686131", "report-no": "LA-UR-16-28920", "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The equations of a physical constitutive model for material stress within\ntantalum grains were solved numerically using a tetrahedrally meshed volume.\nThe resulting output included a scalar vonMises stress for each of the more\nthan 94,000 tetrahedra within the finite element discretization. In this paper,\nwe define an intricate statistical model for the spatial field of vonMises\nstress which uses the given grain geometry in a fundamental way. Our model\nrelates the three-dimensional field to integrals of latent stochastic processes\ndefined on the vertices of the one- and two-dimensional grain boundaries. An\nintuitive neighborhood structure of said boundary nodes suggested the use of a\nlatent Gaussian Markov random field (GMRF). However, despite the potential for\ncomputational gains afforded by GMRFs, the integral nature of our model and the\nsheer number of data points pose substantial challenges for a full Bayesian\nanalysis. To overcome these problems and encourage efficient exploration of the\nposterior distribution, a number of techniques are now combined: parallel\ncomputing, sparse matrix methods, and a modification of a block update strategy\nwithin the sampling routine. In addition, we use an auxiliary variables\napproach to accommodate the presence of outliers in the data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 21:07:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Marcy", "Peter W.", ""], ["Wiel", "Scott A. Vander", ""], ["Storlie", "Curtis B.", ""], ["Livescu", "Veronica", ""], ["Bronkhorst", "Curt A.", ""]]}, {"id": "1911.02720", "submitter": "Eric Kawaguchi", "authors": "Eric S. Kawaguchi, Jenny I. Shen, Marc A. Suchard, and Gang Li", "title": "Scalable Algorithms for Large Competing Risks Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops two orthogonal contributions to scalable sparse\nregression for competing risks time-to-event data. First, we study and\naccelerate the broken adaptive ridge method (BAR), an $\\ell_0$-based\niteratively reweighted $\\ell_2$-penalization algorithm that achieves sparsity\nin its limit, in the context of the Fine-Gray (1999) proportional\nsubdistributional hazards (PSH) model. In particular, we derive a new algorithm\nfor BAR regression, named cycBAR, that performs cyclic update of each\ncoordinate using an explicit thresholding formula. The new cycBAR algorithm\neffectively avoids fitting multiple reweighted $\\ell_2$-penalizations and thus\nyields impressive speedups over the original BAR algorithm. Second, we address\na pivotal computational issue related to fitting the PSH model. Specifically,\nthe computation costs of the log-pseudo likelihood and its derivatives for PSH\nmodel grow at the rate of $O(n^2)$ with the sample size $n$ in current\nimplementations. We propose a novel forward-backward scan algorithm that\nreduces the computation costs to $O(n)$. The proposed method applies to both\nunpenalized and penalized estimation for the PSH model and has exhibited\ndrastic speedups over current implementations. Finally, combining the two\nalgorithms can yields $>1,000$ fold speedups over the original BAR algorithm.\nIllustrations of the impressive scalability of our proposed algorithm for large\ncompeting risks data are given using both simulations and a United States Renal\nData System data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:24:25 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kawaguchi", "Eric S.", ""], ["Shen", "Jenny I.", ""], ["Suchard", "Marc A.", ""], ["Li", "Gang", ""]]}, {"id": "1911.02741", "submitter": "Jaewon Chung", "authors": "Jaewon Chung, Bijan Varjavand, Jesus Arroyo, Anton Alyakin, Joshua\n  Agterberg, Minh Tang, Joshua T. Vogelstein, Carey E. Priebe", "title": "Valid Two-Sample Graph Testing via Optimal Transport Procrustes and\n  Multiscale Graph Correlation: Applications in Connectomics", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing whether two graphs come from the same distribution is of interest in\nmany real world scenarios, including brain network analysis. Under the random\ndot product graph model, the nonparametric hypothesis testing framework\nconsists of embedding the graphs using the adjacency spectral embedding (ASE),\nfollowed by aligning the embeddings using the median flip heuristic, and\nfinally applying the nonparametric maximum mean discrepancy (MMD) test to\nobtain a p-value. Using synthetic data generated from Drosophila brain\nnetworks, we show that the median flip heuristic results in an invalid test,\nand demonstrate that optimal transport Procrustes (OTP) for alignment resolves\nthe invalidity. We further demonstrate that substituting the MMD test with\nmultiscale graph correlation (MGC) test leads to a more powerful test both in\nsynthetic and in simulated data. Lastly, we apply this valid and more powerful\ntest to the right and left hemispheres of the larval Drosophila mushroom body\nbrain networks, and conclude that there is not enough evidence to reject the\nnull hypothesis that the two hemispheres are equally distributed.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 03:35:42 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 01:51:10 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2020 14:56:02 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 03:37:51 GMT"}, {"version": "v5", "created": "Mon, 15 Mar 2021 06:39:40 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chung", "Jaewon", ""], ["Varjavand", "Bijan", ""], ["Arroyo", "Jesus", ""], ["Alyakin", "Anton", ""], ["Agterberg", "Joshua", ""], ["Tang", "Minh", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1911.02748", "submitter": "Hyungsuk Tak", "authors": "Hyungsuk Tak, Kisung You, Sujit K. Ghosh, Bingyue Su, Joseph Kelly", "title": "Data transforming augmentation for heteroscedastic models", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2019.1704295", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation (DA) turns seemingly intractable computational problems\ninto simple ones by augmenting latent missing data. In addition to\ncomputational simplicity, it is now well-established that DA equipped with a\ndeterministic transformation can improve the convergence speed of iterative\nalgorithms such as an EM algorithm or Gibbs sampler. In this article, we\noutline a framework for the transformation-based DA, which we call data\ntransforming augmentation (DTA), allowing augmented data to be a deterministic\nfunction of latent and observed data, and unknown parameters. Under this\nframework, we investigate a novel DTA scheme that turns heteroscedastic models\ninto homoscedastic ones to take advantage of simpler computations typically\navailable in homoscedastic cases. Applying this DTA scheme to fitting linear\nmixed models, we demonstrate simpler computations and faster convergence rates\nof resulting iterative algorithms, compared with those under a\nnon-transformation-based DA scheme. We also fit a Beta-Binomial model using the\nproposed DTA scheme, which enables sampling approximate marginal posterior\ndistributions that are available only under homoscedasticity. An R package,\nRdta, is publicly available at CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 04:14:01 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 02:53:11 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Tak", "Hyungsuk", ""], ["You", "Kisung", ""], ["Ghosh", "Sujit K.", ""], ["Su", "Bingyue", ""], ["Kelly", "Joseph", ""]]}, {"id": "1911.02768", "submitter": "Vitor Hadad", "authors": "Vitor Hadad, David A. Hirshberg, Ruohan Zhan, Stefan Wager, Susan\n  Athey", "title": "Confidence Intervals for Policy Evaluation in Adaptive Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive experiment designs can dramatically improve statistical efficiency\nin randomized trials, but they also complicate statistical inference. For\nexample, it is now well known that the sample mean is biased in adaptive\ntrials. Inferential challenges are exacerbated when our parameter of interest\ndiffers from the parameter the trial was designed to target, such as when we\nare interested in estimating the value of a sub-optimal treatment after running\na trial to determine the optimal treatment using a stochastic bandit design. In\nthis context, typical estimators that use inverse propensity weighting to\neliminate sampling bias can be problematic: their distributions become skewed\nand heavy-tailed as the propensity scores decay to zero. In this paper, we\npresent a class of estimators that overcome these issues. Our approach is to\nadaptively reweight the terms of an augmented inverse propensity weighting\nestimator to control the contribution of each term to the estimator's variance.\nThis adaptive weighting scheme prevents estimates from becoming heavy-tailed,\nensuring asymptotically correct coverage. It also reduces variance, allowing us\nto test hypotheses with greater power - especially hypotheses that were not\ntargeted by the experimental design. We validate the accuracy of the resulting\nestimates and their confidence intervals in numerical experiments and show our\nmethods compare favorably to existing alternatives in terms of RMSE and\ncoverage.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 06:15:52 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 17:44:37 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 18:09:03 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 20:03:50 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Hadad", "Vitor", ""], ["Hirshberg", "David A.", ""], ["Zhan", "Ruohan", ""], ["Wager", "Stefan", ""], ["Athey", "Susan", ""]]}, {"id": "1911.02982", "submitter": "Max Westphal", "authors": "Max Westphal, Antonia Zapf, Werner Brannath", "title": "A multiple testing framework for diagnostic accuracy studies with\n  co-primary endpoints", "comments": "32 pages, 5 figures; v2: minor linguistic revision, no\n  content-related changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major advances have been made regarding the utilization of artificial\nintelligence in health care. In particular, deep learning approaches have been\nsuccessfully applied for automated and assisted disease diagnosis and prognosis\nbased on complex and high-dimensional data. However, despite all justified\nenthusiasm, overoptimistic assessments of predictive performance are still\ncommon. Automated medical testing devices based on machine-learned prediction\nmodels should thus undergo a throughout evaluation before being implemented\ninto clinical practice. In this work, we propose a multiple testing framework\nfor (comparative) phase III diagnostic accuracy studies with sensitivity and\nspecificity as co-primary endpoints. Our approach challenges the frequent\nrecommendation to strictly separate model selection and evaluation, i.e. to\nonly assess a single diagnostic model in the evaluation study. We show that our\nparametric simultaneous test procedure asymptotically allows strong control of\nthe family-wise error rate. Moreover, we demonstrate in extensive simulation\nstudies that our multiple testing strategy on average leads to a better final\ndiagnostic model and increased statistical power. To plan such studies, we\npropose a Bayesian approach to determine the optimal number of models to\nevaluate. For this purpose, our algorithm optimizes the expected final model\nperformance given previous (hold-out) data from the model development phase. We\nconclude that an assessment of multiple promising diagnostic models in the same\nevaluation study has several advantages when suitable adjustments for multiple\ncomparisons are implemented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 01:58:54 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 22:38:32 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Westphal", "Max", ""], ["Zapf", "Antonia", ""], ["Brannath", "Werner", ""]]}, {"id": "1911.03017", "submitter": "Marius Hofert", "authors": "Erik Hintz, Marius Hofert, Christiane Lemieux", "title": "Normal variance mixtures: Distribution, density and parameter estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normal variance mixtures are a class of multivariate distributions that\ngeneralize the multivariate normal by randomizing (or mixing) the covariance\nmatrix via multiplication by a non-negative random variable W. The multivariate\nt distribution is an example of such mixture, where W has an inverse-gamma\ndistribution. Algorithms to compute the joint distribution function and perform\nparameter estimation for the multivariate normal and t (with integer degrees of\nfreedom) can be found in the literature and are implemented in, e.g., the R\npackage mvtnorm. In this paper, efficient algorithms to perform these tasks in\nthe general case of a normal variance mixture are proposed. In addition to the\nabove two tasks, the evaluation of the joint (logarithmic) density function of\na general normal variance mixture is tackled as well, as it is needed for\nparameter estimation and does not always exist in closed form in this more\ngeneral setup. For the evaluation of the joint distribution function, the\nproposed algorithms apply randomized quasi-Monte Carlo (RQMC) methods in a way\nthat improves upon existing methods proposed for the multivariate normal and t\ndistributions. An adaptive RQMC algorithm that similarly exploits the superior\nconvergence properties of RQMC methods is presented for the task of evaluating\nthe joint log-density function. This allows the parameter estimation task to be\naccomplished via an EM-like algorithm where all weights and log-densities are\nnumerically estimated. It is demonstrated through numerical examples that the\nsuggested algorithms are quite fast; even for high dimensions around 1000 the\ndistribution function can be estimated with moderate accuracy using only a few\nseconds of run time. Even log-densities around -100 can be estimated accurately\nand quickly. An implementation of all algorithms presented in this work is\navailable in the R package nvmix (version >= 0.0.4).\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:34:21 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 03:25:00 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 16:48:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hintz", "Erik", ""], ["Hofert", "Marius", ""], ["Lemieux", "Christiane", ""]]}, {"id": "1911.03071", "submitter": "Fredrik S\\\"avje", "authors": "Christopher Harshaw and Fredrik S\\\"avje and Daniel Spielman and Peng\n  Zhang", "title": "Balancing covariates in randomized experiments with the Gram-Schmidt\n  Walk design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of experiments involves a compromise between covariate balance and\nrobustness. This paper introduces an experimental design that admits precise\ncontrol over this trade-off. The design is specified by a parameter that bounds\nthe worst-case mean square error of an estimator of the average treatment\neffect. Subject to the experimenter's desired level of robustness, the design\naims to simultaneously balance all linear functions of the covariates. The\nachieved level of balance is considerably better than what a fully random\nassignment would produce, and it is close to optimal given the desired level of\nrobustness. We show that the mean square error of the estimator is bounded by\nthe minimum of the loss function of a ridge regression of the potential\noutcomes on the covariates. One may thus interpret the approach as regression\nadjustment by design. Finally, we provide non-asymptotic tail bounds for the\nestimator, which facilitate the construction of conservative confidence\nintervals.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 06:09:36 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 21:10:43 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 02:32:30 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Harshaw", "Christopher", ""], ["S\u00e4vje", "Fredrik", ""], ["Spielman", "Daniel", ""], ["Zhang", "Peng", ""]]}, {"id": "1911.03224", "submitter": "Zachary del Rosario", "authors": "Zachary del Rosario and Matthias Rupp and Yoolhee Kim and Erin Antono\n  and Julia Ling", "title": "Assessing the Frontier: Active Learning, Model Accuracy, and\n  Multi-objective Materials Discovery and Optimization", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering novel materials can be greatly accelerated by iterative machine\nlearning-informed proposal of candidates---active learning. However, standard\n\\emph{global-scope error} metrics for model quality are not predictive of\ndiscovery performance, and can be misleading. We introduce the notion of\n\\emph{Pareto shell-scope error} to help judge the suitability of a model for\nproposing material candidates. Further, through synthetic cases and a\nthermoelectric dataset, we probe the relation between acquisition function\nfidelity and active learning performance. Results suggest novel diagnostic\ntools, as well as new insights for acquisition function design.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 17:24:35 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 22:42:41 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 18:06:43 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["del Rosario", "Zachary", ""], ["Rupp", "Matthias", ""], ["Kim", "Yoolhee", ""], ["Antono", "Erin", ""], ["Ling", "Julia", ""]]}, {"id": "1911.03549", "submitter": "Mevin Hooten", "authors": "Mevin B. Hooten, Xinyi Lu, Martha J. Garlick, James A. Powell", "title": "Animal Movement Models with Mechanistic Selection Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A suite of statistical methods are used to study animal movement. Most of\nthese methods treat animal telemetry data in one of three ways: as discrete\nprocesses, as continuous processes, or as point processes. We briefly review\neach of these approaches and then focus in on the latter. In the context of\npoint processes, so-called resource selection analyses are among the most\ncommon way to statistically treat animal telemetry data. However, most resource\nselection analyses provide inference based on approximations of point process\nmodels. The forms of these models have been limited to a few types of\nspecifications that provide inference about relative resource use and, less\ncommonly, probability of use. For more general spatio-temporal point process\nmodels, the most common type of analysis often proceeds with a data\naugmentation approach that is used to create a binary data set that can be\nanalyzed with conditional logistic regression. We show that the conditional\nlogistic regression likelihood can be generalized to accommodate a variety of\nalternative specifications related to resource selection. We then provide an\nexample of a case where a spatio-temporal point process model coincides with\nthat implied by a mechanistic model for movement expressed as a partial\ndifferential equation derived from first principles of movement. We demonstrate\nthat inference from this form of point process model is intuitive (and could be\nuseful for management and conservation) by analyzing a set of telemetry data\nfrom a mountain lion in Colorado, USA, to understand the effects of spatially\nexplicit environmental conditions on movement behavior of this species.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 21:23:23 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:57:41 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Hooten", "Mevin B.", ""], ["Lu", "Xinyi", ""], ["Garlick", "Martha J.", ""], ["Powell", "James A.", ""]]}, {"id": "1911.03662", "submitter": "Kristoffer H. Hellton", "authors": "Kristoffer H. Hellton, Camilla Lingj{\\ae}rde and Riccardo De Bin", "title": "Influence of single observations on the choice of the penalty parameter\n  in ridge regression", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression methods, such as ridge regression, heavily rely on the\nchoice of a tuning, or penalty, parameter, which is often computed via\ncross-validation. Discrepancies in the value of the penalty parameter may lead\nto substantial differences in regression coefficient estimates and predictions.\nIn this paper, we investigate the effect of single observations on the optimal\nchoice of the tuning parameter, showing how the presence of influential points\ncan dramatically change it. We distinguish between points as \"expanders\" and\n\"shrinkers\", based on their effect on the model complexity. Our approach\nsupplies a visual exploratory tool to identify influential points, naturally\nimplementable for high-dimensional data where traditional approaches usually\nfail. Applications to real data examples, both low- and high-dimensional, and a\nsimulation study are presented.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 10:53:13 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Hellton", "Kristoffer H.", ""], ["Lingj\u00e6rde", "Camilla", ""], ["De Bin", "Riccardo", ""]]}, {"id": "1911.03685", "submitter": "Linda Altieri", "authors": "Linda Altieri, Daniela Cocchi and Giulia Roli", "title": "Estimation of entropy measures for categorical variables with spatial\n  correlation", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy is a measure of heterogeneity widely used in applied sciences, often\nwhen data are collected over space. Recently, a number of approaches has been\nproposed to include spatial information in entropy. The aim of entropy is to\nsynthesize the observed data in a single, interpretable number. In other\nstudies the objective is, instead, to use data for entropy estimation; several\nproposals can be found in the literature, which basically are corrections of\nthe estimator based on substituting the involved probabilities with\nproportions. In this case, independence is assumed and spatial correlation is\nnot considered. We propose a path for spatial entropy estimation: instead of\ncorrecting the global entropy estimator, we focus on improving the estimation\nof its components, i.e. the probabilities, in order to account for spatial\neffects. Once probabilities are suitably evaluated, estimating entropy is\nstraightforward since it is a deterministic function of the distribution.\nFollowing a Bayesian approach, we derive the posterior probabilities of a\nmultinomial distribution for categorical variables, accounting for spatial\ncorrelation. A posterior distribution for entropy can be obtained, which may be\nsynthesized as wished and displayed as an entropy surface for the area under\nstudy.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 13:16:22 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Altieri", "Linda", ""], ["Cocchi", "Daniela", ""], ["Roli", "Giulia", ""]]}, {"id": "1911.03764", "submitter": "Ruoxuan Xiong", "authors": "Ruoxuan Xiong, Susan Athey, Mohsen Bayati, Guido Imbens", "title": "Optimal Experimental Design for Staggered Rollouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimentation has become an increasingly prevalent tool for guiding\ndecision-making and policy choices. A common hurdle in designing experiments is\nthe lack of statistical power. In this paper, we study the optimal multi-period\nexperimental design under the constraint that the treatment cannot be easily\nremoved once implemented; for example, a government might implement a public\nhealth intervention in different geographies at different times, where the\ntreatment cannot be easily removed due to practical constraints. The treatment\ndesign problem is to select which geographies (referred by units) to treat at\nwhich time, intending to test hypotheses about the effect of the treatment.\nWhen the potential outcome is a linear function of unit and time effects, and\ndiscrete observed/latent covariates, we provide an analytically feasible\nsolution to the optimal treatment design problem where the variance of the\ntreatment effect estimator is at most 1+O(1/N^2) times the variance using the\noptimal treatment design, where N is the number of units. This solution assigns\nunits in a staggered treatment adoption pattern - if the treatment only affects\none period, the optimal fraction of treated units in each period increases\nlinearly in time; if the treatment affects multiple periods, the optimal\nfraction increases non-linearly in time, smaller at the beginning and larger at\nthe end. In the general setting where outcomes depend on latent covariates, we\nshow that historical data can be utilized in designing experiments. We propose\na data-driven local search algorithm to assign units to treatment times. We\ndemonstrate that our approach improves upon benchmark experimental designs via\nsynthetic interventions on the influenza occurrence rate and synthetic\nexperiments on interventions for in-home medical services and grocery\nexpenditure.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 19:46:29 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 02:19:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Xiong", "Ruoxuan", ""], ["Athey", "Susan", ""], ["Bayati", "Mohsen", ""], ["Imbens", "Guido", ""]]}, {"id": "1911.03783", "submitter": "Li Chen", "authors": "Li Chen, Jie Zhou, Lizhen Lin", "title": "Hypothesis testing for populations of networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become an increasingly common practice for scientists in modern\nscience and engineering to collect samples of multiple network data in which a\nnetwork serves as a basic data object. The increasing prevalence of multiple\nnetwork data calls for developments of models and theory that can deal with\ninference problems for populations of networks. In this work, we propose a\ngeneral procedure for hypothesis testing of networks and in particular, for\ndifferentiating distributions of two samples of networks. We consider a very\ngeneral framework which allows us to perform tests on large and sparse\nnetworks. Our contribution is two-fold: (1) We propose a test statistics based\non the singular value of a generalized Wigner matrix. The asymptotic null\ndistribution of the statistics is shown to follow the Tracy--Widom distribution\nas the number of nodes tends to infinity. The test also yields asymptotic power\nguarantee with the power tending to one under the alternative; (2) The test\nprocedure is adapted for change-point detection in dynamic networks which is\nproven to be consistent in detecting the change-points. In addition to\ntheoretical guarantees, another appealing feature of this adapted procedure is\nthat it provides a principled and simple method for selecting the threshold\nthat is also allowed to vary with time. Extensive simulation studies and real\ndata analyses demonstrate the superior performance of our procedure with\ncompetitors.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 21:47:02 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 16:11:06 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Chen", "Li", ""], ["Zhou", "Jie", ""], ["Lin", "Lizhen", ""]]}, {"id": "1911.03804", "submitter": "Anru Zhang", "authors": "Anru Zhang, Yuetian Luo, Garvesh Raskutti, Ming Yuan", "title": "ISLET: Fast and Optimal Low-rank Tensor Regression via Importance\n  Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we develop a novel procedure for low-rank tensor regression,\nnamely \\emph{\\underline{I}mportance \\underline{S}ketching \\underline{L}ow-rank\n\\underline{E}stimation for \\underline{T}ensors} (ISLET). The central idea\nbehind ISLET is \\emph{importance sketching}, i.e., carefully designed sketches\nbased on both the responses and low-dimensional structure of the parameter of\ninterest. We show that the proposed method is sharply minimax optimal in terms\nof the mean-squared error under low-rank Tucker assumptions and under\nrandomized Gaussian ensemble design. In addition, if a tensor is low-rank with\ngroup sparsity, our procedure also achieves minimax optimality. Further, we\nshow through numerical study that ISLET achieves comparable or better\nmean-squared error performance to existing state-of-the-art methods while\nhaving substantial storage and run-time advantages including capabilities for\nparallel and distributed computing. In particular, our procedure performs\nreliable estimation with tensors of dimension $p = O(10^8)$ and is $1$ or $2$\norders of magnitude faster than baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 23:36:13 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 05:36:30 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zhang", "Anru", ""], ["Luo", "Yuetian", ""], ["Raskutti", "Garvesh", ""], ["Yuan", "Ming", ""]]}, {"id": "1911.03873", "submitter": "Min Tsao Dr.", "authors": "Min Tsao", "title": "A constrained minimum criterion for model selection", "comments": "19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hypothesis test based model selection criterion for the best\nsubset selection of sparse linear models. We show it is consistent in that the\nprobability of its choosing the true model approaches one and the parameter\nvalues of its chosen model converge in probability to that of the true model as\nthe sample size goes to infinity. This criterion is capable of controlling the\nbalance between the false active rate and false inactive rate of the selected\nmodel, and it can be applied with other methods of model selection such as the\nlasso. We also demonstrate its accuracy and advantages with a numerical\ncomparison and an application.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 07:49:03 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 18:50:50 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Tsao", "Min", ""]]}, {"id": "1911.03985", "submitter": "Hyunseung Kang", "authors": "Nan Bi, Hyunseung Kang, Jonathan Taylor", "title": "Inference After Selecting Plausibly Valid Instruments with Application\n  to Mendelian Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) is a popular method in genetic epidemiology to\nestimate the effect of an exposure on an outcome by using genetic instruments.\nThese instruments are often selected from a combination of prior knowledge from\ngenome wide association studies (GWAS) and data-driven instrument selection\nprocedures or tests. Unfortunately, when testing for the exposure effect, the\ninstrument selection process done a priori is not accounted for. This paper\nstudies and highlights the bias resulting from not accounting for the\ninstrument selection process by focusing on a recent data-driven instrument\nselection procedure, sisVIVE, as an example. We introduce a conditional\ninference approach that conditions on the instrument selection done a priori\nand leverage recent advances in selective inference to derive conditional null\ndistributions of popular test statistics for the exposure effect in MR. The\nnull distributions can be characterized with individual-level or summary-level\ndata in MR. We show that our conditional confidence intervals derived from\nconditional null distributions attain the desired nominal level while typical\nconfidence intervals computed in MR do not. We conclude by reanalyzing the\neffect of BMI on diastolic blood pressure using summary-level data from the\nUKBiobank that accounts for instrument selection.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 19:28:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Bi", "Nan", ""], ["Kang", "Hyunseung", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1911.04056", "submitter": "Quefeng Li", "authors": "Quefeng Li and Lexin Li", "title": "Integrative Factor Regression and Its Inference for Multimodal Data\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal data, where different types of data are collected from the same\nsubjects, are fast emerging in a large variety of scientific applications.\nFactor analysis is commonly used in integrative analysis of multimodal data,\nand is particularly useful to overcome the curse of high dimensionality and\nhigh correlations. However, there is little work on statistical inference for\nfactor analysis based supervised modeling of multimodal data. In this article,\nwe consider an integrative linear regression model that is built upon the\nlatent factors extracted from multimodal data. We address three important\nquestions: how to infer the significance of one data modality given the other\nmodalities in the model; how to infer the significance of a combination of\nvariables from one modality or across different modalities; and how to quantify\nthe contribution, measured by the goodness-of-fit, of one data modality given\nthe others. When answering each question, we explicitly characterize both the\nbenefit and the extra cost of factor analysis. Those questions, to our\nknowledge, have not yet been addressed despite wide use of factor analysis in\nintegrative multimodal analysis, and our proposal bridges an important gap. We\nstudy the empirical performance of our methods through simulations, and further\nillustrate with a multimodal neuroimaging analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 03:24:47 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 02:45:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Quefeng", ""], ["Li", "Lexin", ""]]}, {"id": "1911.04062", "submitter": "Junjie Liang", "authors": "Junjie Liang, Dongkuan Xu, Yiwei Sun and Vasant Honavar", "title": "LMLFM: Longitudinal Multi-Level Factorization Machine", "comments": "Thirty-Fourth AAAI Conference on Artificial Intelligence, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning predictive models from longitudinal data,\nconsisting of irregularly repeated, sparse observations from a set of\nindividuals over time. Such data often exhibit {\\em longitudinal correlation}\n(LC) (correlations among observations for each individual over time), {\\em\ncluster correlation} (CC) (correlations among individuals that have similar\ncharacteristics), or both. These correlations are often accounted for using\n{\\em mixed effects models} that include {\\em fixed effects} and {\\em random\neffects}, where the fixed effects capture the regression parameters that are\nshared by all individuals, whereas random effects capture those parameters that\nvary across individuals. However, the current state-of-the-art methods are\nunable to select the most predictive fixed effects and random effects from a\nlarge number of variables, while accounting for complex correlation structure\nin the data and non-linear interactions among the variables. We propose\nLongitudinal Multi-Level Factorization Machine (LMLFM), to the best of our\nknowledge, the first model to address these challenges in learning predictive\nmodels from longitudinal data. We establish the convergence properties, and\nanalyze the computational complexity, of LMLFM. We present results of\nexperiments with both simulated and real-world longitudinal data which show\nthat LMLFM outperforms the state-of-the-art methods in terms of predictive\naccuracy, variable selection ability, and scalability to data with large number\nof variables. The code and supplemental material is available at\n\\url{https://github.com/junjieliang672/LMLFM}.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 03:45:39 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 19:06:26 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Liang", "Junjie", ""], ["Xu", "Dongkuan", ""], ["Sun", "Yiwei", ""], ["Honavar", "Vasant", ""]]}, {"id": "1911.04090", "submitter": "Steven Pav", "authors": "Steven Pav", "title": "A post hoc test on the Sharpe ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a post hoc test for the Sharpe ratio, analogous to Tukey's test\nfor pairwise equality of means. The test can be applied after rejection of the\nhypothesis that all population Signal-Noise ratios are equal. The test is\napplicable under a simple correlation structure among asset returns.\nSimulations indicate the test maintains nominal type I rate under a wide range\nof conditions and is moderately powerful under reasonable alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 05:47:30 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Pav", "Steven", ""]]}, {"id": "1911.04109", "submitter": "Yiping Hong", "authors": "Yiping Hong, Sameh Abdulah, Marc G. Genton, Ying Sun", "title": "Efficiency Assessment of Approximated Spatial Predictions for Large\n  Datasets", "comments": "43 pages + 8 pages of Supplementary Material, 8 figures, 8 tables + 8\n  tables in Supplementary Material. The Abstract is slightly abridged compared\n  to the article. Corrected the affiliation of Sameh Abdulah", "journal-ref": "Spatial Statistics, 43, 100517 (2021)", "doi": "10.1016/j.spasta.2021.100517", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the well-known computational showstopper of the exact Maximum\nLikelihood Estimation (MLE) for large geospatial observations, a variety of\napproximation methods have been proposed in the literature, which usually\nrequire tuning certain inputs. For example, the recently developed Tile\nLow-Rank approximation (TLR) method involves many tuning parameters, including\nnumerical accuracy. To properly choose the tuning parameters, it is crucial to\nadopt a meaningful criterion for the assessment of the prediction efficiency\nwith different inputs, which the most commonly-used Mean Square Prediction\nError (MSPE) criterion and the Kullback-Leibler Divergence criterion cannot\nfully describe. In this paper, we present three other criteria, the Mean Loss\nof Efficiency (MLOE), Mean Misspecification of the Mean Square Error (MMOM),\nand Root mean square MOM (RMOM), and show numerically that, in comparison with\nthe common MSPE criterion and the Kullback-Leibler Divergence criterion, our\ncriteria are more informative, and thus more adequate to assess the loss of the\nprediction efficiency by using the approximated or misspecified covariance\nmodels. Hence, our suggested criteria are more useful for the determination of\ntuning parameters for sophisticated approximation methods of spatial model\nfitting. To illustrate this, we investigate the trade-off between the execution\ntime, estimation accuracy, and prediction efficiency for the TLR method with\nextensive simulation studies and suggest proper settings of the TLR tuning\nparameters. We then apply the TLR method to a large spatial dataset of soil\nmoisture in the area of the Mississippi River basin, and compare the TLR with\nthe Gaussian predictive process and the composite likelihood method, showing\nthat our suggested criteria can successfully be used to choose the tuning\nparameters that can keep the estimation or the prediction accuracy in\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 06:39:57 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 15:38:13 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 13:33:03 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hong", "Yiping", ""], ["Abdulah", "Sameh", ""], ["Genton", "Marc G.", ""], ["Sun", "Ying", ""]]}, {"id": "1911.04285", "submitter": "Patrick Flaherty", "authors": "Patrick Flaherty, Pitchaya Wiratchotisatian, Ji Ah Lee, Zhou Tang,\n  Andrew C. Trapp", "title": "MAP Clustering under the Gaussian Mixture Model via Mixed Integer\n  Nonlinear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a global optimization approach for solving the maximum\na-posteriori (MAP) clustering problem under the Gaussian mixture model.Our\napproach can accommodate side constraints and it preserves the combinatorial\nstructure of the MAP clustering problem by formulating it asa mixed-integer\nnonlinear optimization problem (MINLP). We approximate the MINLP through a\nmixed-integer quadratic program (MIQP) transformation that improves\ncomputational aspects while guaranteeing $\\epsilon$-global optimality. An\nimportant benefit of our approach is the explicit quantification of the degree\nof suboptimality, via the optimality gap, en route to finding the globally\noptimal MAP clustering. Numerical experiments comparing our method to other\napproaches show that our method finds a better solution than standard\nclustering methods. Finally, we cluster a real breast cancer gene expression\ndata set incorporating intrinsic subtype information; the induced constraints\nsubstantially improve the computational performance and produce more coherent\nand bio-logically meaningful clusters.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 15:53:26 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 02:51:11 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Flaherty", "Patrick", ""], ["Wiratchotisatian", "Pitchaya", ""], ["Lee", "Ji Ah", ""], ["Tang", "Zhou", ""], ["Trapp", "Andrew C.", ""]]}, {"id": "1911.04337", "submitter": "Samuel I. Berchuck", "authors": "Samuel I. Berchuck, Mark Janko, Felipe A. Medeiros, William Pan, Sayan\n  Mukherjee", "title": "Bayesian Non-Parametric Factor Analysis for Longitudinal Spatial\n  Surfaces", "comments": "This is a preprint of an article submitted for publication in the\n  Journal of the American Statistical Association. The article contains 35\n  pages, 5 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian non-parametric spatial factor analysis model with\nspatial dependency induced through a prior on factor loadings. For each column\nof the loadings matrix, spatial dependency is encoded using a probit\nstick-breaking process (PSBP) and a multiplicative gamma process shrinkage\nprior is used across columns to adaptively determine the number of latent\nfactors. By encoding spatial information into the loadings matrix, meaningful\nfactors are learned that respect the observed neighborhood dependencies, making\nthem useful for assessing rates over space. Furthermore, the spatial PSBP prior\ncan be used for clustering temporal trends, allowing users to identify regions\nwithin the spatial domain with similar temporal trajectories, an important task\nin many applied settings. In the manuscript, we illustrate the model's\nperformance in simulated data, but also in two real-world examples:\nlongitudinal monitoring of glaucoma and malaria surveillance across the\nPeruvian Amazon. The R package spBFA, available on CRAN, implements the method.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 15:29:40 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Berchuck", "Samuel I.", ""], ["Janko", "Mark", ""], ["Medeiros", "Felipe A.", ""], ["Pan", "William", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1911.04341", "submitter": "Mathias M{\\o}rck Ljungdahl", "authors": "Mathias M{\\o}rck Ljungdahl and Mark Podolskij", "title": "A Minimal Contrast Estimator for the Linear Fractional Stable Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an estimator for the three-dimensional parameter\n$(\\sigma, \\alpha, H)$ of the linear fractional stable motion, where $H$\nrepresents the self-similarity parameter, and $(\\sigma, \\alpha)$ are the\nscaling and stability parameters of the driving symmetric L\\'evy process $L$.\nOur approach is based upon a minimal contrast method associated with the\nempirical characteristic function combined with a ratio type estimator for the\nselfsimilarity parameter $H$. The main result investigates the strong\nconsistency and weak limit theorems for the resulting estimator. Furthermore,\nwe propose several ideas to obtain feasible confidence regions in various\nparameter settings. Our work is mainly related to [16, 18], in which parameter\nestimation for the linear fractional stable motion and related L\\'evy moving\naverage processes has been studied.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 15:58:43 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 08:34:23 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Ljungdahl", "Mathias M\u00f8rck", ""], ["Podolskij", "Mark", ""]]}, {"id": "1911.04561", "submitter": "Elizabeth Eisenhauer", "authors": "Elizabeth Eisenhauer and Ephraim Hanks", "title": "A Lattice and Random Intermediate Point Sampling Design for Animal\n  Movement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal movement studies have become ubiquitous in animal ecology for\nestimation of space use and analysis of movement behavior. In these studies,\nanimal movement data are primarily collected at regular time intervals. We\npropose an irregular sampling design which could lead to greater efficiency and\ninformation gain in animal movement studies. Our novel sampling design, called\nlattice and random intermediate point (LARI), combines samples at regular and\nrandom time intervals. We compare the LARI sampling design to regular sampling\ndesigns in an example with common black carpenter ant location data, an example\nwith guppy location data, and a simulation study of movement with a point of\nattraction. We modify a general stochastic differential equation model to allow\nfor irregular time intervals and use this framework to compare sampling\ndesigns. When parameters are estimated reasonably well, regular sampling\nresults in greater precision and accuracy in prediction of missing data.\nHowever, in each of the data and simulation examples explored in this paper,\nLARI sampling results in more accurate and precise parameter estimation, and\nthus better prediction of missing data as well. This result suggests that\nresearchers might gain greater insight into underlying animal movement\nprocesses by choosing LARI sampling over regular sampling.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 20:56:19 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Eisenhauer", "Elizabeth", ""], ["Hanks", "Ephraim", ""]]}, {"id": "1911.04602", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung, Benjamin Haaland, Youngdeok Hwang, Siyuan Lu", "title": "A clustered Gaussian process model for computer experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Gaussian process has been one of the important approaches for emulating\ncomputer simulations. However, the stationarity assumption for a Gaussian\nprocess and the intractability for large-scale dataset limit its availability\nin practice. In this article, we propose a clustered Gaussian process model\nwhich segments the input data into multiple clusters, in each of which a\nGaussian process model is performed. The stochastic expectation-maximization is\nemployed to efficiently fit the model. In our simulations as well as a real\napplication to solar irradiance emulation, our proposed method had smaller mean\nsquare errors than its main competitors, with competitive computation time, and\nprovides valuable insights from data by discovering the clusters. An R package\nfor the proposed methodology is provided in an open repository.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 23:30:28 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 02:56:42 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 05:05:23 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Sung", "Chih-Li", ""], ["Haaland", "Benjamin", ""], ["Hwang", "Youngdeok", ""], ["Lu", "Siyuan", ""]]}, {"id": "1911.04663", "submitter": "Qian Guan", "authors": "Qian Guan, Shu Yang", "title": "A Unified Framework for Causal Inference with Multiple Imputation Using\n  Martingale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation is widely used to handle confounders missing at random in\ncausal inference. Although Rubin's combining rule is simple, it is not clear\nwhether or not the standard multiple imputation inference is consistent when\ncoupled with the commonly-used average causal effect (ACE) estimators. This\narticle establishes a unified martingale representation for the average causal\neffect (ACE) estimators after multiple imputation. This representation invokes\nthe wild bootstrap inference to provide consistent variance estimation. Our\nframework applies to asymptotically normal ACE estimators, including the\nregression imputation, weighting, and matching estimators. We extend to the\nscenarios when both outcome and confounders are subject to missingness and when\nthe data are missing not at random.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 04:07:45 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 01:20:15 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Guan", "Qian", ""], ["Yang", "Shu", ""]]}, {"id": "1911.04927", "submitter": "Rebecka J\\\"ornsten", "authors": "Jonatan Kallus, Patrik Johansson, Sven Nelander, Rebecka J\\\"ornsten", "title": "MM-PCA: Integrative Analysis of Multi-group and Multi-view Data", "comments": "Manuscript+Supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration is the problem of combining multiple data groups (studies,\ncohorts) and/or multiple data views (variables, features). This task is\nbecoming increasingly important in many disciplines due to the prevalence of\nlarge and heterogeneous data sets. Data integration commonly aims to identify\nstructure that is consistent across multiple cohorts and feature sets. While\nsuch joint analyses can boost information from single data sets, it is also\npossible that a globally restrictive integration of heterogeneous data may\nobscure signal of interest.\n  Here, we therefore propose a data adaptive integration method, allowing for\nstructure in data to be shared across an a priori unknown \\emph{subset of\ncohorts and views}. The method, Multi-group Multi-view Principal Component\nAnalysis (MM-PCA), identifies partially shared, sparse low-rank components.\nThis also results in an integrative bi-clustering across cohorts and views. The\nstrengths of MM-PCA are illustrated on simulated data and on 'omics data from\nThe Cancer Genome Atlas. MM-PCA is available as an R-package.\n  Key words: Data integration, Multi-view, Multi-group, Bi-clustering\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 15:18:35 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Kallus", "Jonatan", ""], ["Johansson", "Patrik", ""], ["Nelander", "Sven", ""], ["J\u00f6rnsten", "Rebecka", ""]]}, {"id": "1911.04974", "submitter": "Benjamin Lengerich", "authors": "Benjamin Lengerich, Sarah Tan, Chun-Hao Chang, Giles Hooker, Rich\n  Caruana", "title": "Purifying Interaction Effects with the Functional ANOVA: An Efficient\n  Algorithm for Recovering Identifiable Additive Models", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models which estimate main effects of individual variables alongside\ninteraction effects have an identifiability challenge: effects can be freely\nmoved between main effects and interaction effects without changing the model\nprediction. This is a critical problem for interpretability because it permits\n\"contradictory\" models to represent the same function. To solve this problem,\nwe propose pure interaction effects: variance in the outcome which cannot be\nrepresented by any smaller subset of features. This definition has an\nequivalence with the Functional ANOVA decomposition. To compute this\ndecomposition, we present a fast, exact algorithm that transforms any\npiecewise-constant function (such as a tree-based model) into a purified,\ncanonical representation. We apply this algorithm to Generalized Additive\nModels with interactions trained on several datasets and show large disparity,\nincluding contradictions, between the effects before and after purification.\nThese results underscore the need to specify data distributions and ensure\nidentifiability before interpreting model parameters.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 16:06:21 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 20:20:28 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 21:45:40 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lengerich", "Benjamin", ""], ["Tan", "Sarah", ""], ["Chang", "Chun-Hao", ""], ["Hooker", "Giles", ""], ["Caruana", "Rich", ""]]}, {"id": "1911.05125", "submitter": "William Aeberhard", "authors": "William H. Aeberhard, Eva Cantoni, Giampiero Marra, Rosalba Radice", "title": "Robust Fitting for Generalized Additive Models for Location, Scale and\n  Shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The validity of estimation and smoothing parameter selection for the wide\nclass of generalized additive models for location, scale and shape (GAMLSS)\nrelies on the correct specification of a likelihood function. Deviations from\nsuch assumption are known to mislead any likelihood-based inference and can\nhinder penalization schemes meant to ensure some degree of smoothness for\nnon-linear effects. We propose a general approach to achieve robustness in\nfitting GAMLSSs by limiting the contribution of observations with low\nlog-likelihood values. Robust selection of the smoothing parameters can be\ncarried out either by minimizing information criteria that naturally arise from\nthe robustified likelihood or via an extended Fellner-Schall method. The latter\nallows for automatic smoothing parameter selection and is particularly\nadvantageous in applications with multiple smoothing parameters. We also\naddress the challenge of tuning robust estimators for models with non-linear\neffects by proposing a novel median downweighting proportion criterion. This\nenables a fair comparison with existing robust estimators for the special case\nof generalized additive models, where our estimator competes favorably. The\noverall good performance of our proposal is illustrated by further simulations\nin the GAMLSS setting and by an application to functional magnetic resonance\nbrain imaging using bivariate smoothing splines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 20:14:57 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Aeberhard", "William H.", ""], ["Cantoni", "Eva", ""], ["Marra", "Giampiero", ""], ["Radice", "Rosalba", ""]]}, {"id": "1911.05272", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Mean and Variance of Brownian Motion with Given Final Value, Maximum and\n  ArgMax: Extended Version", "comments": "31 pages, 29 figures", "journal-ref": "Stochastic Models, 2021", "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional expectation and conditional variance of Brownian motion is\nconsidered given the argmax, B(t|argmax), as well as those with additional\ninformation: B(t|close, argmax), B(t|max, argmax), B(t|close, max, argmax)\nwhere the close is the final value: B(t=1)=c and t in [0,1]. We compute the\nexpectation and variance of a Brownian meander in time. By splicing together\ntwo Brownian meanders, the mean and variance of the constrained process are\ncalculated. Computational results displaying both the expectation and variance\nin time are presented. Comparison of the simulation with theoretical values are\nshown when the close and argmax are given.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 03:35:57 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 04:31:20 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 04:41:48 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1911.05280", "submitter": "Kurt Riedel", "authors": "Kurt S Riedel", "title": "The Value of the High, Low and Close in the Estimation of Brownian\n  Motion: Extended Version", "comments": "40 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional density of Brownian motion is considered given the max,\nB(t|\\max), as well as those with additional information: B(t|close, max),\nB(t|close, max, min) and B(t|max, min) where the close is the final value:\nB(t=1)=c and t in [0,1]. The conditional expectation and conditional variance\nof Brownian motion are evaluated subject to one or more of the the close (final\nvalue), the high (maximum), the low (minimum). Computational results displaying\nboth the expectation and variance in time are presented and compared with the\ntheoretical values. We tabulate the time averaged variance of Brownian motion\nconditional on knowing various extremal properties of the motion. The final\ntable shows that knowing the high is more useful than knowing the final value\namong other results. Knowing the open, high, low and close reduces the time\naveraged variance to 42% of the value of knowing only the open and close\n(Brownian bridge).\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 04:09:25 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 02:34:45 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 19:53:05 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Riedel", "Kurt S", ""]]}, {"id": "1911.05307", "submitter": "Brendan Beare", "authors": "Brendan K. Beare and Juwon Seo", "title": "Randomization tests of copula symmetry", "comments": null, "journal-ref": "Econom. Theory 36 (2020) 1025-1063", "doi": "10.1017/S0266466619000410", "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New nonparametric tests of copula exchangeability and radial symmetry are\nproposed. The novel aspect of the tests is a resampling procedure that exploits\ngroup invariance conditions associated with the relevant symmetry hypothesis.\nThey may be viewed as feasible versions of randomization tests of symmetry, the\nlatter being inapplicable due to the unobservability of margins. Our tests are\nsimple to compute, control size asymptotically, consistently detect arbitrary\nforms of asymmetry, and do not require the specification of a tuning parameter.\nSimulations indicate excellent small sample properties compared to existing\nprocedures involving the multiplier bootstrap.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 06:03:13 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Beare", "Brendan K.", ""], ["Seo", "Juwon", ""]]}, {"id": "1911.05376", "submitter": "Caitriona Ryan", "authors": "Caitr\\'iona M. Ryan and Andrew Parnell and Catherine Mahoney", "title": "Real-Time Anomaly Detection for Advanced Manufacturing: Improving on\n  Twitter's State of the Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of anomalies in real time is paramount to maintain performance\nand efficiency across a wide range of applications including web services and\nsmart manufacturing. This paper presents a novel algorithm to detect anomalies\nin streaming time series data via statistical learning. We adapt the\ngeneralised extreme studentised deviate test [1] to streaming data by using a\nsliding window approach. This is made computationally feasible by recursive\nupdates of the Grubbs test statistic [2]. Moreover, a priority queue [3] is\nemployed to reduce memory requirements, where subsets of the required data\nstreaming window are maintained in the algorithm rather than the full list. Our\nmethod is statistically principled. It is suitable for streaming data and it\noutperforms the AnomalyDetection software package, recently released by Twitter\nInc. (Twitter) [4] and used by multiple teams at Twitter as their state of the\nart on a daily basis [5]. The methodology is demonstrated using an example of\nunlabelled data from the Twitter AnomalyDetection GitHub repository and using a\nreal manufacturing example with labelled anomalies.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 10:03:52 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 10:57:48 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Ryan", "Caitr\u00edona M.", ""], ["Parnell", "Andrew", ""], ["Mahoney", "Catherine", ""]]}, {"id": "1911.05522", "submitter": "Tyler McCormick", "authors": "Wesley Lee and Tyler H. McCormick and Joshua Neil and Cole Sodja and\n  Yanran Cui", "title": "Anomaly Detection in Large Scale Networks with Latent Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a real-time anomaly detection algorithm for directed activity on\nlarge, sparse networks. We model the propensity for future activity using a\ndynamic logistic model with interaction terms for sender- and receiver-specific\nlatent factors in addition to sender- and receiver-specific popularity scores;\ndeviations from this underlying model constitute potential anomalies. Latent\nnodal attributes are estimated via a variational Bayesian approach and may\nchange over time, representing natural shifts in network activity. Estimation\nis augmented with a case-control approximation to take advantage of the\nsparsity of the network and reduces computational complexity from $O(N^2)$ to\n$O(E)$, where $N$ is the number of nodes and $E$ is the number of observed\nedges. We run our algorithm on network event records collected from an\nenterprise network of over 25,000 computers and are able to identify a red team\nattack with half the detection rate required of the model without latent\ninteraction terms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 14:57:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 18:20:46 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Lee", "Wesley", ""], ["McCormick", "Tyler H.", ""], ["Neil", "Joshua", ""], ["Sodja", "Cole", ""], ["Cui", "Yanran", ""]]}, {"id": "1911.05538", "submitter": "Frank R\\\"ottger", "authors": "Ulrike Gra{\\ss}hoff, Heinz Holling, Frank R\\\"ottger and Rainer Schwabe", "title": "Optimality regions for designs in multiple linear regression models with\n  correlated random coefficients", "comments": "16 pages, 7 figures", "journal-ref": "Journal of Statistical Planning and Inference (2020)", "doi": "10.1016/j.jspi.2020.04.004", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies optimal designs for linear regression models with\ncorrelated effects for single responses. We introduce the concept of rhombic\ndesign to reduce the computational complexity and find a semi-algebraic\ndescription for the D-optimality of a rhombic design via the Kiefer-Wolfowitz\nequivalence theorem. Subsequently, we show that the structure of an optimal\nrhombic design depends directly on the correlation structure of the random\ncoefficients.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 15:17:34 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Gra\u00dfhoff", "Ulrike", ""], ["Holling", "Heinz", ""], ["R\u00f6ttger", "Frank", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1911.05592", "submitter": "Haiyan Zheng", "authors": "Haiyan Zheng, Lisa V. Hampson, Thomas Jaki", "title": "A Bayesian hierarchical model for bridging across patient subgroups in\n  phase I clinical trials with animal data", "comments": "The main paper has 20 pages, 4 figures and 1 table", "journal-ref": "Statistical Methods in Medical Research 2021", "doi": "10.1177/0962280220986580", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating preclinical animal data, which can be regarded as a special\nkind of historical data, into phase I clinical trials can improve decision\nmaking when very little about human toxicity is known. In this paper, we\ndevelop a robust hierarchical modelling approach to leverage animal data into\nnew phase I clinical trials, where we bridge across non-overlapping,\npotentially heterogeneous patient subgroups. Translation parameters are used to\nbring both historical and contemporary data onto a common dosing scale. This\nleads to feasible exchangeability assumptions that the parameter vectors, which\nunderpin the dose-toxicity relationship per study, are assumed to be drawn from\na common distribution. Moreover, human dose-toxicity parameter vectors are\nassumed to be exchangeable either with the standardised, animal study-specific\nparameter vectors, or between themselves. Possibility of non-exchangeability\nfor each parameter vector is considered to avoid inferences for extreme\nsubgroups being overly influenced by the other. We illustrate the proposed\napproach with several trial data examples, and evaluate the operating\ncharacteristics of our model compared with several alternatives in a simulation\nstudy. Numerical results show that our approach yields robust inferences in\ncircumstances, where data from multiple sources are inconsistent and/or the\nbridging assumptions are incorrect.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 16:37:01 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zheng", "Haiyan", ""], ["Hampson", "Lisa V.", ""], ["Jaki", "Thomas", ""]]}, {"id": "1911.05643", "submitter": "Sandra E. Safo", "authors": "Sandra E. Safo, Eun Jeong Min, and Lillian Haine", "title": "Sparse Linear Discriminant Analysis for Multi-view Structured Data", "comments": "40 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification methods that leverage the strengths of data from multiple\nsources (multi-view data) simultaneously have enormous potential to yield more\npowerful findings than two step methods: association followed by\nclassification. We propose two methods, sparse integrative discriminant\nanalysis (SIDA) and SIDA with incorporation of network information (SIDANet),\nfor joint association and classification studies. The methods consider the\noverall association between multi-veiw data, and the separation within each\nview in choosing discriminant vectors that are associated and optimally\nseparate subjects into different classes. SIDANet is among the first methods to\nincorporate prior structural information in joint association and\nclassification studies. It uses the normalized Laplacian of a graph to smooth\ncoefficients of predictor variables, thus encouraging selection of predictors\nthat are connected and behave similarly. We demonstrate the effectiveness of\nour methods on a set of synthetic and real datasets. Our findings underscore\nthe benefit of joint association and classification methods if the goal is to\ncorrelate multi-view data and to perform classification.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 17:18:29 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 17:11:36 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Safo", "Sandra E.", ""], ["Min", "Eun Jeong", ""], ["Haine", "Lillian", ""]]}, {"id": "1911.05684", "submitter": "Lili Wang", "authors": "Lili Wang (1), Xiaodong Luo (2), Cheng Zheng (2) ((1) Department of\n  Biostatistics, University of Michigan, Ann Arbor, Michigan, U.S.A. (2)\n  Department of Biostatistics and Programming, Research and Development, Sanofi\n  US, Bridgewater, New Jersey, U.S.A.)", "title": "A Simulation-free Group Sequential Design with Max-combo Tests in the\n  Presence of Non-proportional Hazards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-proportional hazards (NPH) have been observed recently in many\nimmuno-oncology clinical trials. Weighted log-rank tests (WLRT) with suitably\nchosen weights can be used to improve the power of detecting the difference of\nthe two survival curves in the presence of NPH. However, it is not easy to\nchoose a proper WLRT in practice when both robustness and efficiency are\nconsidered. A versatile maxcombo test was proposed to achieve the balance of\nrobustness and efficiency and has received increasing attentions in both\nmethodology development and application. However, survival trials often warrant\ninterim analyses due to its high cost and long duration. The integration and\napplication of maxcombo tests in interim analyses often require extensive\nsimulation studies. In this paper, we propose a simulation-free approach for\ngroup sequential design with maxcombo test in survival trials. The simulation\nresults support that the proposed approaches successfully control both the type\nI error rate and offer great accuracy and flexibility in estimating sample\nsizes, at the expense of light computation burden. Notably, our methods display\na strong robustness towards various model misspecifications, and have been\nimplemented in an R package for free access online.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 18:04:40 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 02:13:41 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 21:04:49 GMT"}, {"version": "v4", "created": "Mon, 6 Jan 2020 01:37:01 GMT"}, {"version": "v5", "created": "Wed, 19 Feb 2020 04:13:32 GMT"}, {"version": "v6", "created": "Fri, 28 Aug 2020 01:12:13 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Wang", "Lili", ""], ["Luo", "Xiaodong", ""], ["Zheng", "Cheng", ""]]}, {"id": "1911.05691", "submitter": "Reynaldo Martina", "authors": "Reynaldo Martina, Keith Abrams, Sylwia Bujkiewicz, David Jenkins,\n  Pascale Dequen, Michael Lees, Frank A. Corvino, and Jessica Davies", "title": "The use of registry data to extrapolate overall survival results from\n  randomised controlled trials", "comments": "21 Pages, 7 named figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Pre-marketing authorisation estimates of survival are generally\nrestricted to those observed directly in randomised controlled trials (RCTs).\nHowever, for regulatory and Health Technology Assessment (HTA) decision-making\na longer time horizon is often required than is studied in RCTs. Therefore,\nextrapolation is required to estimate long-term treatment effect. Registry data\ncan provide evidence to support extrapolation of treatment effects from RCTs,\nwhich are considered the main sources of evidence of effect for new drug\napplications. A number of methods are available to extrapolate survival data,\nsuch as Exponential, Weibull, Gompertz, log-logistic or log-normal parametric\nmodels. The different methods have varying functional forms and can result in\ndifferent survival estimates.\n  Methods: The aim of this paper was to use registry data to supplement the\nrelatively short term RCT data to obtain long term estimates of effect. No\nformal hypotheses were tested. We explore the above parametric regression\nmodels as well as a nonparametric regression model based on local linear\n(parametric) regression. We also explore a Bayesian model constrained to the\nlong term estimate of survival reported in literature, a Bayesian power prior\napproach on the variability observed from published literature, and a Bayesian\nModel Averaging (BMA) approach. The methods were applied to extrapolate overall\nsurvival of a RCT in metastatic melanoma.\n  Results: The results showed that the BMA approach was able to fit the RCT\ndata well, with the lowest variability of the area under the curve up to 72\nmonths with or without the SEER Medicare registry.\n  Conclusion: the BMA approach is a viable approach to extrapolate overall\nsurvival in the absence of long term data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:11:33 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Martina", "Reynaldo", ""], ["Abrams", "Keith", ""], ["Bujkiewicz", "Sylwia", ""], ["Jenkins", "David", ""], ["Dequen", "Pascale", ""], ["Lees", "Michael", ""], ["Corvino", "Frank A.", ""], ["Davies", "Jessica", ""]]}, {"id": "1911.05728", "submitter": "Owen Leete", "authors": "Owen E. Leete, Nathan Kallus, Michael G. Hudgens, Sonia Napravnik,\n  Michael R. Kosorok", "title": "Balanced Policy Evaluation and Learning for Right Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individualized treatment rules can lead to better health outcomes when\npatients have heterogeneous responses to treatment. Very few individualized\ntreatment rule estimation methods are compatible with a multi-treatment\nobservational study with right censored survival outcomes. In this paper we\nextend policy evaluation methods to the right censored data setting. Existing\napproaches either make restrictive assumptions about the structure of the data,\nor use inverse weighting methods that increase the variance of the estimator\nresulting in decreased performance. We propose a method which uses balanced\npolicy evaluation combined with an imputation approach to remove right\ncensoring. We show that the proposed imputation approach is compatible with a\nlarge number of existing survival models and can be used to extend any\nindividualized treatment rule estimation method to the right censored data\nsetting. We establish the rate at which the imputed values converge to the\nconditional expected survival times, as well as consistency guarantees and\nregret bounds for the combined balanced policy with imputation approach. In\nsimulation studies, we demonstrate the improved performance of our approach\ncompared to existing methods. We also apply our method to data from the\nUniversity of North Carolina Center for AIDS Research HIV Clinical Cohort.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 18:55:08 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Leete", "Owen E.", ""], ["Kallus", "Nathan", ""], ["Hudgens", "Michael G.", ""], ["Napravnik", "Sonia", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1911.05754", "submitter": "Arya Pourzanjani", "authors": "Arya A. Pourzanjani and Linda R. Petzold", "title": "Implicit Hamiltonian Monte Carlo for Sampling Multiscale Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) has been widely adopted in the statistics\ncommunity because of its ability to sample high-dimensional distributions much\nmore efficiently than other Metropolis-based methods. Despite this, HMC often\nperforms sub-optimally on distributions with high correlations or marginal\nvariances on multiple scales because the resulting stiffness forces the\nleapfrog integrator in HMC to take an unreasonably small stepsize. We provide\nintuition as well as a formal analysis showing how these multiscale\ndistributions limit the stepsize of leapfrog and we show how the implicit\nmidpoint method can be used, together with Newton-Krylov iteration, to\ncircumvent this limitation and achieve major efficiency gains. Furthermore, we\noffer practical guidelines for when to choose between implicit midpoint and\nleapfrog and what stepsize to use for each method, depending on the\ndistribution being sampled. Unlike previous modifications to HMC, our method is\ngenerally applicable to highly non-Gaussian distributions exhibiting multiple\nscales. We illustrate how our method can provide a dramatic speedup over\nleapfrog in the context of the No-U-Turn sampler (NUTS) applied to several\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 21:28:59 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 00:01:08 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Pourzanjani", "Arya A.", ""], ["Petzold", "Linda R.", ""]]}, {"id": "1911.05865", "submitter": "Pulong Ma", "authors": "Pulong Ma and Anindya Bhadra", "title": "Beyond Mat\\'ern: On A Class of Interpretable Confluent Hypergeometric\n  Covariance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mat\\'ern covariance function is a popular choice for prediction in\nspatial statistics and uncertainty quantification literature. A key benefit of\nthe Mat\\'ern class is that it is possible to get precise control over the\ndegree of differentiability of the process realizations. However, the Mat\\'ern\nclass possesses exponentially decaying tails, and thus may not be suitable for\nmodeling polynomially decaying dependence. This problem can be remedied using\npolynomial covariances; however one loses control over the degree of\nmean-square differentiability of corresponding processes, in that the random\nprocesses with polynomial covariances are either infinitely mean-square\ndifferentiable or nowhere mean-square differentiable at all. We construct a new\nfamily of covariance functions called the \\emph{Confluent Hypergeometric} (CH)\nclass using a scale mixture representation of the Mat\\'ern class where one\nobtains the benefits of both Mat\\'ern and polynomial covariances. The resultant\ncovariance contains two parameters: one controls the degree of mean-square\ndifferentiability near the origin and the other controls the tail heaviness,\nindependently of each other. Using a spectral representation, we derive\ntheoretical properties of this new covariance including equivalent measures and\nasymptotic behavior of the maximum likelihood estimators under infill\nasymptotics. The improved theoretical properties of the CH class are verified\nvia extensive simulations. Application using NASA's Orbiting Carbon\nObservatory-2 satellite data confirms the advantage of the CH class over the\nMat\\'ern class, especially in extrapolative settings.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 00:01:07 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 17:38:46 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 14:09:33 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 03:05:12 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Ma", "Pulong", ""], ["Bhadra", "Anindya", ""]]}, {"id": "1911.05970", "submitter": "Nikolaos Ignatiadis", "authors": "Nikolaos Ignatiadis, Sujayam Saha, Dennis L. Sun, Omkar Muralidharan", "title": "Empirical Bayes mean estimation with nonparametric errors via order\n  statistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study empirical Bayes estimation of the effect sizes of $N$ units from $K$\nnoisy observations on each unit. We show that it is possible to achieve\nnear-Bayes optimal mean squared error, without any assumptions or knowledge\nabout the effect size distribution or the noise. The noise distribution can be\nheteroskedastic and vary arbitrarily from unit to unit. Our proposal, which we\ncall Aurora, leverages the replication inherent in the $K$ observations per\nunit and recasts the effect size estimation problem as a general regression\nproblem. Aurora with linear regression provably matches the performance of a\nwide array of estimators including the sample mean, the trimmed mean, the\nsample median, as well as James-Stein shrunk versions thereof. Aurora automates\neffect size estimation for Internet-scale datasets, as we demonstrate on Google\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 07:20:53 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Ignatiadis", "Nikolaos", ""], ["Saha", "Sujayam", ""], ["Sun", "Dennis L.", ""], ["Muralidharan", "Omkar", ""]]}, {"id": "1911.06030", "submitter": "Eleanor Murray", "authors": "Eleanor J. Murray, Sonja A. Swanson, Miguel A. Hern\\'an", "title": "Guidelines for estimating causal effects in pragmatic randomized trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pragmatic randomized trials are designed to provide evidence for clinical\ndecision-making rather than regulatory approval. Common features of these\ntrials include the inclusion of heterogeneous or diverse patient populations in\na wide range of care settings, the use of active treatment strategies as\ncomparators, unblinded treatment assignment, and the study of long-term,\nclinically relevant outcomes. These features can greatly increase the\nusefulness of the trial results for patients, clinicians, and other\nstakeholders. However, these features also introduce an increased risk of\nnon-adherence, which reduces the value of the intention-to-treat effect as a\npatient-centered measure of causal effect. In these settings, the per-protocol\neffect provides useful complementary information for decision making.\nUnfortunately, there is little guidance for valid estimation of the\nper-protocol effect. Here, we present our full guidelines for analyses of\npragmatic trials that will result in more informative causal inferences for\nboth the intention-to-treat effect and the per-protocol effect.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 10:50:29 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 06:07:26 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Murray", "Eleanor J.", ""], ["Swanson", "Sonja A.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1911.06177", "submitter": "Thomas Lee", "authors": "Suofei Wu, Jan Hannig and Thomas C. M. Lee", "title": "Uncertainty Quantification in Ensembles of Honest Regression Trees using\n  Generalized Fiducial Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their accuracies, methods based on ensembles of regression trees are a\npopular approach for making predictions. Some common examples include Bayesian\nadditive regression trees, boosting and random forests. This paper focuses on\nhonest random forests, which add honesty to the original form of random forests\nand are proved to have better statistical properties. The main contribution is\na new method that quantifies the uncertainties of the estimates and predictions\nproduced by honest random forests. The proposed method is based on the\ngeneralized fiducial methodology, and provides a fiducial density function that\nmeasures how likely each single honest tree is the true model. With such a\ndensity function, estimates and predictions, as well as their\nconfidence/prediction intervals, can be obtained. The promising empirical\nproperties of the proposed method are demonstrated by numerical comparisons\nwith several state-of-the-art methods, and by applications to a few real data\nsets. Lastly, the proposed method is theoretically backed up by a strong\nasymptotic guarantee.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 15:33:07 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Wu", "Suofei", ""], ["Hannig", "Jan", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1911.06421", "submitter": "Jos\\'e-Miguel Ponciano PhD", "authors": "Mark L. Taper and Subhash R Lele and Jos\\'e-Miguel Ponciano, Brian\n  Dennis and Christopher L Jerde", "title": "Assessing the global and local uncertainty in scientific evidence in the\n  presence of model misspecification", "comments": "30 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scientists need to compare the support for models based on observed\nphenomena. The main goal of the evidential paradigm is to quantify the strength\nof evidence in the data for a reference model relative to an alternative model.\nThis is done via an evidence function, such as $\\Delta SIC$, an estimator of\nthe sample size scaled difference of divergences between the generating\nmechanism and the competing models. To use evidence, either for decision making\nor as a guide to the accumulation of knowledge, an understanding of the\nuncertainty in the evidence is needed. This uncertainty is well characterized\nby the standard statistical theory of estimation. Unfortunately, the standard\ntheory breaks down if the models are misspecified, as it is normally the case\nin scientific studies. We develop non-parametric bootstrap methodologies for\nestimating the sampling distribution of the evidence estimator under model\nmisspecification. This sampling distribution allows us to determine how secure\nwe are in our evidential statement. We characterize this uncertainty in the\nstrength of evidence with two different types of confidence intervals, which we\nterm \"global\" and \"local\". We discuss how evidence uncertainty can be used to\nimprove scientific inference and illustrate this with a reanalysis of the model\nidentification problem in a prominent landscape ecology study (Grace and\nKeeley, 2006) using structural equations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 00:08:45 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 22:22:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Taper", "Mark L.", ""], ["Lele", "Subhash R", ""], ["Ponciano", "Jos\u00e9-Miguel", ""], ["Dennis", "Brian", ""], ["Jerde", "Christopher L", ""]]}, {"id": "1911.06564", "submitter": "Peiliang Xu", "authors": "Peiliang Xu", "title": "Akaike's Bayesian information criterion (ABIC) or not ABIC for\n  geophysical inversion", "comments": "7", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Akaike's Bayesian information criterion (ABIC) has been widely used in\ngeophysical inversion and beyond. However, little has been done to investigate\nits statistical aspects. We present an alternative derivation of the marginal\ndistribution of measurements, whose maximization directly leads to the\ninvention of ABIC by Akaike. We show that ABIC is to statistically estimate the\nvariance of measurements and the prior variance by maximizing the marginal\ndistribution of measurements. The determination of the regularization parameter\non the basis of ABIC is actually equivalent to estimating the relative\nweighting factor between the variance of measurements and the prior variance\nfor geophysical inverse problems. We show that if the noise level of\nmeasurements is unknown, ABIC tends to produce a substantially biased estimate\nof the variance of measurements. In particular, since the prior mean is\ngenerally unknown but arbitrarily treated as zero in geophysical inversion,\nABIC does not produce a reasonable estimate for the prior variance either.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 11:01:11 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Xu", "Peiliang", ""]]}, {"id": "1911.06583", "submitter": "Mari Myllym\\\"aki", "authors": "Mari Myllym\\\"aki and Tom\\'a\\v{s} Mrkvi\\v{c}ka", "title": "GET: Global envelopes in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes the R package GET that implements global envelopes for a\ngeneral set of $d$-dimensional vectors $T$ in various applications. A\n$100(1-\\alpha)$% global envelope is a band bounded by two vectors such that the\nprobability that $T$ falls outside this envelope in any of the $d$ points is\nequal to $\\alpha$. Global means that the probability is controlled\nsimultaneously for all the $d$ elements of the vectors. The global envelopes\ncan be employed for central regions of functional or multivariate data, for\ngraphical Monte Carlo and permutation tests where the test statistic is\nmultivariate or functional, and for global confidence and prediction bands.\nIntrinsic graphical interpretation property is introduced for global envelopes,\nand the global envelopes included in the GET package that have the property are\ndescribed. Examples of different uses of global envelopes and their\nimplementation in the GET package are presented, including global envelopes for\nsingle and several one- or two-dimensional functions, Monte Carlo\ngoodness-of-fit tests for simple and composite hypotheses, comparison of\ndistributions, graphical functional analysis of variance (ANOVA), and general\nlinear model (GLM), and confidence bands in polynomial regression.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 12:18:14 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 16:11:40 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Myllym\u00e4ki", "Mari", ""], ["Mrkvi\u010dka", "Tom\u00e1\u0161", ""]]}, {"id": "1911.06683", "submitter": "Heng Xiao", "authors": "Carlos A. Michel\\'en Str\\\"ofer, Xinlei Zhang, Heng Xiao, Olivier\n  Coutier-Delgosha", "title": "Enforcing Boundary Conditions on Physical Fields in Bayesian Inversion", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.113097", "report-no": null, "categories": "physics.comp-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems in computational mechanics consist of inferring physical\nfields that are latent in the model describing some observable fields.\n  For instance, an inverse problem of interest is inferring the Reynolds stress\nfield in the Navier--Stokes equations describing mean fluid velocity and\npressure.\n  The physical nature of the latent fields means they have their own set of\nphysical constraints, including boundary conditions.\n  The inherent ill-posedness of inverse problems, however, means that there\nexist many possible latent fields that do not satisfy their physical\nconstraints while still resulting in a satisfactory agreement in the\nobservation space.\n  These physical constraints must therefore be enforced through the problem\nformulation.\n  So far there has been no general approach to enforce boundary conditions on\nlatent fields in inverse problems in computational mechanics, with these\nconstraints often simply ignored.\n  In this work we demonstrate how to enforce boundary conditions in Bayesian\ninversion problems by choice of the statistical model for the latent fields.\n  Specifically, this is done by modifying the covariance kernel to guarantee\nthat all realizations satisfy known values or derivatives at the boundary.\n  As a test case the problem of inferring the eddy viscosity in the\nReynolds-averaged Navier--Stokes equations is considered.\n  The results show that enforcing these constraints results in similar\nimprovements in the output fields but with latent fields that behave as\nexpected at the boundaries.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 15:07:05 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Str\u00f6fer", "Carlos A. Michel\u00e9n", ""], ["Zhang", "Xinlei", ""], ["Xiao", "Heng", ""], ["Coutier-Delgosha", "Olivier", ""]]}, {"id": "1911.06708", "submitter": "Emanuele Aliverti", "authors": "Emanuele Aliverti, Jeff Tilson, Dayne Filer, Benjamin Babcock,\n  Alejandro Colaneri, Jennifer Ocasio, Timothy R. Gershon, Kirk C. Wilhelmsen\n  and David B. Dunson", "title": "Projected $t$-SNE for batch correction", "comments": "16 pages, 3 figures", "journal-ref": "Bioinformatics, 2020, 1-6", "doi": "10.1093/bioinformatics/btaa189", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical research often produces high-dimensional data confounded by batch\neffects such as systematic experimental variations, different protocols and\nsubject identifiers. Without proper correction, low-dimensional representation\nof high-dimensional data might encode and reproduce the same systematic\nvariations observed in the original data, and compromise the interpretation of\nthe results. In this article, we propose a novel procedure to remove batch\neffects from low-dimensional embeddings obtained with t-SNE dimensionality\nreduction. The proposed methods are based on linear algebra and constrained\noptimization, leading to efficient algorithms and fast computation in many\nhigh-dimensional settings. Results on artificial single-cell transcription\nprofiling data show that the proposed procedure successfully removes multiple\nbatch effects from t-SNE embeddings, while retaining fundamental information on\ncell types. When applied to single-cell gene expression data to investigate\nmouse medulloblastoma, the proposed method successfully removes batches related\nwith mice identifiers and the date of the experiment, while preserving clusters\nof oligodendrocytes, astrocytes, and endothelial cells and microglia, which are\nexpected to lie in the stroma within or adjacent to the tumors.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 15:36:58 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 16:01:09 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Aliverti", "Emanuele", ""], ["Tilson", "Jeff", ""], ["Filer", "Dayne", ""], ["Babcock", "Benjamin", ""], ["Colaneri", "Alejandro", ""], ["Ocasio", "Jennifer", ""], ["Gershon", "Timothy R.", ""], ["Wilhelmsen", "Kirk C.", ""], ["Dunson", "David B.", ""]]}, {"id": "1911.06722", "submitter": "Max Hinne", "authors": "Max Hinne, Marcel A.J. van Gerven, Luca Ambrogioni", "title": "Causal inference using Bayesian non-parametric quasi-experimental design", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The de facto standard for causal inference is the randomized controlled\ntrial, where one compares an manipulated group with a control group in order to\ndetermine the effect of an intervention. However, this research design is not\nalways realistically possible due to pragmatic or ethical concerns. In these\nsituations, quasi-experimental designs may provide a solution, as these allow\nfor causal conclusions at the cost of additional design assumptions. In this\npaper, we provide a framework for quasi-experimental design using Bayesian\nmodel comparison. We provide a theoretical motivation for a Gaussian process\nbased approach, and demonstrate its convenient use in a number of simulations.\nFinally, we apply the framework to determine the effect the 2005 smoking ban in\nSicily on the number of acute coronary events, and of the effect of an alleged\nhistorical phantom border in the Netherlands on Dutch voting behaviour.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:10:00 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 15:17:40 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hinne", "Max", ""], ["van Gerven", "Marcel A. J.", ""], ["Ambrogioni", "Luca", ""]]}, {"id": "1911.06723", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok,\n  and Suttipong Thajchayapong", "title": "A nonparametric framework for inferring orders of categorical data from\n  category-real ordered pairs", "comments": "The R package can be found at https://github.com/DarkEyes/EDOIF", "journal-ref": "Heliyon, Volume 6, Issue 11, 2020, e05435", "doi": "10.1016/j.heliyon.2020.e05435", "report-no": null, "categories": "stat.ME cs.CY math.ST physics.data-an stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset of careers and incomes, how large a difference of income\nbetween any pair of careers would be? Given a dataset of travel time records,\nhow long do we need to spend more when choosing a public transportation mode\n$A$ instead of $B$ to travel? In this paper, we propose a framework that is\nable to infer orders of categories as well as magnitudes of difference of real\nnumbers between each pair of categories using Estimation statistics framework.\nNot only reporting whether an order of categories exists, but our framework\nalso reports the magnitude of difference of each consecutive pairs of\ncategories in the order. In large dataset, our framework is scalable well\ncompared with the existing framework. The proposed framework has been applied\nto two real-world case studies: 1) ordering careers by incomes based on\ninformation of 350,000 households living in Khon Kaen province, Thailand, and\n2) ordering sectors by closing prices based on 1060 companies' closing prices\nof NASDAQ stock markets between years 2000 and 2016. The results of careers\nordering show income inequality among different careers. The stock market\nresults illustrate dynamics of sector domination that can change over time. Our\napproach is able to be applied in any research area that has category-real\nordered pairs. Our proposed \"Dominant-Distribution Network\" provides a novel\napproach to gain new insight of analyzing category orders. The software of this\nframework is available for researchers or practitioners within R package:\nEDOIF.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:10:27 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""], ["Surasvadi", "Navaporn", ""], ["Plangprasopchok", "Anon", ""], ["Thajchayapong", "Suttipong", ""]]}, {"id": "1911.06726", "submitter": "Alessandro Casa", "authors": "Alessandro Casa, Luca Scrucca, Giovanna Menardi", "title": "Better than the best? Answers via model ensemble in density-based\n  clustering", "comments": null, "journal-ref": "Advances in Data Analysis and Classification (2020): 1-25", "doi": "10.1007/s11634-020-00423-6", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent growth in data availability and complexity, and the\nassociated outburst of elaborate modelling approaches, model selection tools\nhave become a lifeline, providing objective criteria to deal with this\nincreasingly challenging landscape. In fact, basing predictions and inference\non a single model may be limiting if not harmful; ensemble approaches, which\ncombine different models, have been proposed to overcome the selection step,\nand proven fruitful especially in the supervised learning framework.\nConversely, these approaches have been scantily explored in the unsupervised\nsetting. In this work we focus on the model-based clustering formulation, where\na plethora of mixture models, with different number of components and\nparametrizations, is typically estimated. We propose an ensemble clustering\napproach that circumvents the single best model paradigm, while improving\nstability and robustness of the partitions. A new density estimator, being a\nconvex linear combination of the density estimates in the ensemble, is\nintroduced and exploited for group assignment. As opposed to the standard case,\nwhere clusters are typically associated to the components of the selected\nmixture model, we define partitions by borrowing the modal, or nonparametric,\nformulation of the clustering problem, where groups are linked with\nhigh-density regions. Staying in the density-based realm we thus show how\nblending together parametric and nonparametric approaches may be beneficial\nfrom a clustering perspective.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:22:52 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 10:26:08 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Casa", "Alessandro", ""], ["Scrucca", "Luca", ""], ["Menardi", "Giovanna", ""]]}, {"id": "1911.06743", "submitter": "Augusto Fasano", "authors": "Augusto Fasano, Daniele Durante and Giacomo Zanella", "title": "Scalable and Accurate Variational Bayes for High-Dimensional Binary\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern methods for Bayesian regression beyond the Gaussian response setting\nare computationally impractical or inaccurate in high dimensions. As discussed\nin recent literature, bypassing this trade-off is still an open problem even in\nbasic binary regression models, and there is limited theory on the quality of\nvariational approximations in high-dimensional settings. To address this gap,\nwe study the approximation accuracy of routine-use mean-field variational Bayes\nin high-dimensional probit regression with Gaussian priors, obtaining new and\npractically relevant results on the pathological behavior of this strategy in\nuncertainty quantification, estimation and prediction, that also suggest\ncaution against maximum a posteriori estimates when p>n. Motivated by these\nresults, we develop a new partially-factorized variational approximation for\nthe posterior distribution of the probit coefficients that leverages a\nrepresentation with global and local variables but, unlike for classical\nmean-field assumptions, it avoids a fully factorized approximation, and instead\nassumes a factorization only for local variables. We prove that the resulting\napproximation belongs to a tractable class of unified skew-normal densities\nthat incorporates skewness and, unlike for state-of-the-art mean-field\nsolutions, converges to the exact posterior density as p goes to infinity. To\nsolve the variational optimization problem, we derive a tractable CAVI\nalgorithm that easily scales to p in tens of thousands, and provably requires a\nnumber of iterations converging to 1 as p goes to infinity. Such findings are\nalso illustrated in extensive empirical studies where our new solution is shown\nto improve the accuracy of mean-field variational Bayes for any n and p, with\nthe magnitude of these gains being remarkable in those high-dimensional p>n\nsettings where state-of-the-art methods are computationally impractical.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:51:29 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 14:33:54 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 13:56:59 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 17:04:49 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Fasano", "Augusto", ""], ["Durante", "Daniele", ""], ["Zanella", "Giacomo", ""]]}, {"id": "1911.06868", "submitter": "Haodi Liang", "authors": "Haodi Liang, Cecilia Cotton", "title": "Causal inference with recurrent data via inverse probability treatment\n  weighting method (IPTW)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score methods are increasingly being used to reduce estimation\nbias of treatment effects for observational studies. Previous research has\nshown that propensity score methods consistently estimate the marginal hazard\nratio for time to event data. However, recurrent data frequently arise in the\nbiomedical literature and there is a paucity of research into the use of\npropensity score methods when data are recurrent in nature. The objective of\nthis paper is to extend the existing propensity score methods to recurrent data\nsetting. We illustrate our methods through a series of Monte Carlo simulations.\nThe simulation results indicate that without the presence of censoring, the\nIPTW estimators allow us to consistently estimate the marginal hazard ratio for\neach event. Under administrative censoring regime, the stabilized IPTW\nestimator yields biased estimate of the marginal hazard ratio, and the degree\nof bias depends on the proportion of subjects being censored. For variance\nestimation, the na\\\"ive variance estimator often tends to substantially\nunderestimate the variance of the IPTW estimator, while the robust variance\nestimator significantly reduces the estimation bias of the variance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 20:47:39 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Liang", "Haodi", ""], ["Cotton", "Cecilia", ""]]}, {"id": "1911.06869", "submitter": "Srijan Sengupta", "authors": "Somnath Bhadra, Kaustav Chakraborty, Srijan Sengupta, and Soumendra\n  Lahiri", "title": "A Bootstrap-based Inference Framework for Testing Similarity of Paired\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in an interconnected world where network valued data arises in many\ndomains, and, fittingly, statistical network analysis has emerged as an active\narea in the literature. However, the topic of inference in networks has\nreceived relatively less attention. In this work, we consider the paired\nnetwork inference problem where one is given two networks on the same set of\nnodes, and the goal is to test whether the given networks are stochastically\nsimilar in terms of some notion of similarity. We develop a general inferential\nframework based on parametric bootstrap to address this problem. Under this\nsetting, we address two specific and important problems: the equality problem,\ni.e., whether the two networks are generated from the same random graph model,\nand the scaling problem, i.e., whether the underlying probability matrices of\nthe two random graph models are scaled versions of each other.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 20:50:22 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 00:27:54 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Bhadra", "Somnath", ""], ["Chakraborty", "Kaustav", ""], ["Sengupta", "Srijan", ""], ["Lahiri", "Soumendra", ""]]}, {"id": "1911.06888", "submitter": "George Leckie", "authors": "George Leckie, William Browne, Harvey Goldstein, Juan Merlo, Peter\n  Austin", "title": "Variance partitioning in multilevel models for count data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A first step when fitting multilevel models to continuous responses is to\nexplore the degree of clustering in the data. Researchers fit\nvariance-component models and then report the proportion of variation in the\nresponse that is due to systematic differences between clusters. Equally they\nreport the response correlation between units within a cluster. These\nstatistics are popularly referred to as variance partition coefficients (VPCs)\nand intraclass correlation coefficients (ICCs). When fitting multilevel models\nto categorical (binary, ordinal, or nominal) and count responses, these\nstatistics prove more challenging to calculate. For categorical response\nmodels, researchers appeal to their latent response formulations and report\nVPCs/ICCs in terms of latent continuous responses envisaged to underly the\nobserved categorical responses. For standard count response models, however,\nthere are no corresponding latent response formulations. More generally, there\nis a paucity of guidance on how to partition the variation. As a result,\napplied researchers are likely to avoid or inadequately report and discuss the\nsubstantive importance of clustering and cluster effects in their studies. A\nrecent article drew attention to a little-known exact algebraic expression for\nthe VPC/ICC for the special case of the two-level random-intercept Poisson\nmodel. In this article, we make a substantial new contribution. First, we\nderive exact VPC/ICC expressions for more flexible negative binomial models\nthat allows for overdispersion, a phenomenon which often occurs in practice.\nThen we derive exact VPC/ICC expressions for three-level and random-coefficient\nextensions to these models. We illustrate our work with an application to\nstudent absenteeism.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 21:41:59 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 21:32:15 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Leckie", "George", ""], ["Browne", "William", ""], ["Goldstein", "Harvey", ""], ["Merlo", "Juan", ""], ["Austin", "Peter", ""]]}, {"id": "1911.06911", "submitter": "Yunan Yang", "authors": "Bj\u007forn Engquist, Kui Ren, Yunan Yang", "title": "The quadratic Wasserstein metric for inverse data matching", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": "10.1088/1361-6420/ab7e04", "report-no": null, "categories": "math.NA cs.NA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work characterizes, analytically and numerically, two major effects of\nthe quadratic Wasserstein ($W_2$) distance as the measure of data discrepancy\nin computational solutions of inverse problems. First, we show, in the\ninfinite-dimensional setup, that the $W_2$ distance has a smoothing effect on\nthe inversion process, making it robust against high-frequency noise in the\ndata but leading to a reduced resolution for the reconstructed objects at a\ngiven noise level. Second, we demonstrate that for some finite-dimensional\nproblems, the $W_2$ distance leads to optimization problems that have better\nconvexity than the classical $L^2$ and $H^{-1}$ distances, making it a more\npreferred distance to use when solving such inverse matching problems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 23:29:27 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 21:22:15 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 19:24:29 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Engquist", "Bj\u007forn", ""], ["Ren", "Kui", ""], ["Yang", "Yunan", ""]]}, {"id": "1911.06944", "submitter": "Donghui Yan", "authors": "Ke Alexander Wang, Xinran Bian, Pan Liu, Donghui Yan", "title": "$DC^2$: A Divide-and-conquer Algorithm for Large-scale Kernel Learning\n  with Application to Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divide-and-conquer is a general strategy to deal with large scale problems.\nIt is typically applied to generate ensemble instances, which potentially\nlimits the problem size it can handle. Additionally, the data are often divided\nby random sampling which may be suboptimal. To address these concerns, we\npropose the $DC^2$ algorithm. Instead of ensemble instances, we produce\nstructure-preserving signature pieces to be assembled and conquered. $DC^2$\nachieves the efficiency of sampling-based large scale kernel methods while\nenabling parallel multicore or clustered computation. The data partition and\nsubsequent compression are unified by recursive random projections. Empirically\ndividing the data by random projections induces smaller mean squared\napproximation errors than conventional random sampling. The power of $DC^2$ is\ndemonstrated by our clustering algorithm $rpfCluster^+$, which is as accurate\nas some fastest approximate spectral clustering algorithms while maintaining a\nrunning time close to that of K-means clustering. Analysis on $DC^2$ when\napplied to spectral clustering shows that the loss in clustering accuracy due\nto data division and reduction is upper bounded by the data approximation error\nwhich would vanish with recursive random projections. Due to its easy\nimplementation and flexibility, we expect $DC^2$ to be applicable to general\nlarge scale learning problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 03:10:36 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Wang", "Ke Alexander", ""], ["Bian", "Xinran", ""], ["Liu", "Pan", ""], ["Yan", "Donghui", ""]]}, {"id": "1911.06955", "submitter": "Randall Reese", "authors": "Randall Reese", "title": "Marginal and Interactive Feature Screening of Ultra-high Dimensional\n  Feature Spaces with Multivariate Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the number of features exponentially outnumbers the number of samples,\nfeature screening plays a pivotal role in reducing the dimension of the feature\nspace and developing models based on such data. While most extant feature\nscreening approaches are only applicable to data having univariate response, we\npropose a new method (GenCorr) that admits a multivariate response. Such an\napproach allows us to more appropriately model multiple responses as a single\nunit, rather than as unrelated entities, which avails more robust analyses in\nrelation to complex traits embedded in the covariance structure of multiple\nresponses. The GenCorr framework allows for the screening of both marginal as\nwell as interactive features. It is demonstrated that GenCorr possesses the\ndesirable property of strong sure screening. In the marginal case, we examine\nthe superior numerical performance of GenCorr in comparison to two current\nmethods for multivariate marginal screening via an assortment of empirical\nsimulations. We also present several simulations inspecting GenCorr's\nperformance in multivariate interaction screening. A culminating real data\nanalysis demonstrates the performance of our method on GWAS data.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 04:48:32 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Reese", "Randall", ""]]}, {"id": "1911.06999", "submitter": "Morteza Raeisi", "authors": "Morteza Raeisi, Florent Bonneu and Edith Gabriel", "title": "A spatio-temporal multi-scale model for Geyer saturation point process:\n  application to forest fire occurrences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because most natural phenomena exhibit dependence at multiple scales like\nlocations of earthquakes or forest fire occurrences, spatio-temporal\nsingle-scale point process models are unrealistic in many applications. This\nmotivates us to construct generalizations of classical Gibbs models. In this\npaper, we extend the Geyer saturation point process model to the\nspatio-temporal multi-scale framework. The simulation process is carried out\nthrough a birth-death Metropolis-Hastings algorithm. In a simulation study, we\ncompare two common methods for statistical inference in Gibbs models: the\npseudo-likelihood and logistic likelihood approaches that we tailor to this\nmodel. Finally, we illustrate this new model on forest fire occurrences\nmodeling in Southern France.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 09:18:19 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 13:18:19 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Raeisi", "Morteza", ""], ["Bonneu", "Florent", ""], ["Gabriel", "Edith", ""]]}, {"id": "1911.07049", "submitter": "Haotian Xu", "authors": "St\\'ephane Guerrier, Juan Jurado, Mehran Khaghani, Gaetan Bakalli,\n  Mucyo Karemera, Roberto Molinari, Samuel Orso, John Raquet, Christine M.\n  Schubert Kabban, Jan Skaloud, Haotian Xu, Yuming Zhang", "title": "Wavelet-Based Moment-Matching Techniques for Inertial Sensor Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of inertial sensor calibration has required the development of\nvarious techniques to take into account the sources of measurement error coming\nfrom such devices. The calibration of the stochastic errors of these sensors\nhas been the focus of increasing amount of research in which the method of\nreference has been the so-called \"Allan variance slope method\" which, in\naddition to not having appropriate statistical properties, requires a\nsubjective input which makes it prone to mistakes. To overcome this, recent\nresearch has started proposing \"automatic\" approaches where the parameters of\nthe probabilistic models underlying the error signals are estimated by matching\nfunctions of the Allan variance or Wavelet Variance with their model-implied\ncounterparts. However, given the increased use of such techniques, there has\nbeen no study or clear direction for practitioners on which approach is optimal\nfor the purpose of sensor calibration. This paper formally defines the class of\nestimators based on this technique and puts forward theoretical and applied\nresults that, comparing with estimators in this class, suggest the use of the\nGeneralized Method of Wavelet Moments as an optimal choice.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 15:49:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Jurado", "Juan", ""], ["Khaghani", "Mehran", ""], ["Bakalli", "Gaetan", ""], ["Karemera", "Mucyo", ""], ["Molinari", "Roberto", ""], ["Orso", "Samuel", ""], ["Raquet", "John", ""], ["Kabban", "Christine M. Schubert", ""], ["Skaloud", "Jan", ""], ["Xu", "Haotian", ""], ["Zhang", "Yuming", ""]]}, {"id": "1911.07085", "submitter": "Michael Leung", "authors": "Michael P. Leung", "title": "Causal Inference Under Approximate Neighborhood Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies causal inference in randomized experiments under network\ninterference. Commonly used models of interference posit that treatments\nassigned to alters beyond a certain network distance from the ego have no\neffect on the ego's response. However, this assumption is violated in many\nmodels of social interactions. We propose a substantially weaker model of\n\"approximate neighborhood interference\" (ANI) under which treatments assigned\nto alters further from the ego have smaller, but potentially nonzero, effects\non the ego's response. For well-known models of social interactions, we can\nformally verify that ANI holds. We also prove that, under ANI and restrictions\non the network topology, standard inverse-probability weighting estimators\nconsistently estimate useful exposure effects and are asymptotically normal\nunder asymptotics taking the network size large. For inference, we consider a\nnetwork HAC variance estimator. Under a finite population model, we show that\nthe estimator is biased but that the bias can be interpreted as the variance of\nunit-level exposure effects. This generalizes Neyman's well-known result on\nconservative variance estimation to settings with interference.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 19:51:36 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 20:46:28 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 00:05:33 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Leung", "Michael P.", ""]]}, {"id": "1911.07099", "submitter": "Barbara Engelhardt", "authors": "Isabella N Grabski, Roberta De Vito, Barbara E Engelhardt", "title": "Bayesian Ordinal Quantile Regression with a Partially Collapsed Gibbs\n  Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlike standard linear regression, quantile regression captures the\nrelationship between covariates and the conditional response distribution as a\nwhole, rather than only the relationship between covariates and the expected\nvalue of the conditional response. However, while there are well-established\nquantile regression methods for continuous variables and some forms of discrete\ndata, there is no widely accepted method for ordinal variables, despite their\nimportance in many medical contexts. In this work, we describe two existing\nordinal quantile regression methods and demonstrate their weaknesses. We then\npropose a new method, Bayesian ordinal quantile regression with a partially\ncollapsed Gibbs sampler (BORPS). We show superior results using BORPS versus\nexisting methods on an extensive set of simulations. We further illustrate the\nbenefits of our method by applying BORPS to the Fragile Families and Child\nWellbeing Study data to tease apart associations with early puberty among both\ngenders. Software is available at: GitHub.com/igrabski/borps.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 21:02:50 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Grabski", "Isabella N", ""], ["De Vito", "Roberta", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1911.07106", "submitter": "Michael Leung", "authors": "Michael P. Leung", "title": "Inference in Models of Discrete Choice with Social Interactions Using\n  Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies inference in models of discrete choice with social\ninteractions when the data consists of a single large network. We provide\ntheoretical justification for the use of spatial and network HAC variance\nestimators in applied work, the latter constructed by using network path\ndistance in place of spatial distance. Toward this end, we prove new central\nlimit theorems for network moments in a large class of social interactions\nmodels. The results are applicable to discrete games on networks and dynamic\nmodels where social interactions enter through lagged dependent variables. We\nillustrate our results in an empirical application and simulation study.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 22:16:56 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Leung", "Michael P.", ""]]}, {"id": "1911.07111", "submitter": "Yue Wang", "authors": "Yue Wang, Joseph Ibrahim and Hongtu Zhu", "title": "Partial Least Squares for Functional Joint Models", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biomedical studies have identified important imaging biomarkers that are\nassociated with both repeated clinical measures and a survival outcome. The\nfunctional joint model (FJM) framework, proposed in Li and Luo (2017),\ninvestigates the association between repeated clinical measures and survival\ndata, while adjusting for both high-dimensional images and low-dimensional\ncovariates based upon the functional principal component analysis (FPCA). In\nthis paper, we propose a novel algorithm for the estimation of FJM based on the\nfunctional partial least squares (FPLS). Our numerical studies demonstrate\nthat, compared to FPCA, the proposed FPLS algorithm can yield more accurate and\nrobust estimation and prediction performance in many important scenarios. We\napply the proposed FPLS algorithm to the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) study. Data used in the preparation of this article were\nobtained from the ADNI database.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 23:05:51 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Wang", "Yue", ""], ["Ibrahim", "Joseph", ""], ["Zhu", "Hongtu", ""]]}, {"id": "1911.07172", "submitter": "Chencheng Cai", "authors": "Chencheng Cai and Rong Chen", "title": "State Space Emulation and Annealed Sequential Monte Carlo for High\n  Dimensional Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high dimensional optimization problems can be reformulated into a\nproblem of finding theoptimal state path under an equivalent state space model\nsetting. In this article, we present a general emulation strategy for\ndeveloping a state space model whose likelihood function (or posterior\ndistribution) shares the same general landscape as the objective function of\nthe original optimization problem. Then the solution of the optimization\nproblem is the same as the optimal state path that maximizes the likelihood\nfunction or the posterior distribution under the emulated system. To find such\nan optimal path, we adapt a simulated annealing approach by inserting a\ntemperature control into the emulated dynamic system and propose a novel\nannealed Sequential Monte Carlo (SMC) method that effectively generating Monte\nCarlo sample paths utilizing samples obtained on the high temperature scale.\nCompared to the vanilla simulated annealing implementation, annealed SMC is an\niterative algorithm for state space model optimization that directly generates\nstate paths from the equilibrium distributions with a decreasing sequence of\ntemperatures through sequential importance sampling which does not require\nburn-in or mixing iterations to ensure quasi-equilibrium condition. Several\napplications of state space model emulation and the corresponding annealed SMC\nresults are demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 07:26:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""]]}, {"id": "1911.07196", "submitter": "Seongoh Park", "authors": "Hyejeong Choi, Johan Lim, Minjung Kwak, Seongoh Park", "title": "Testing for Stochastic Order in Interval-Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a procedure to test the stochastic order of two samples of\ninterval-valued data. We propose a test statistic which belongs to U-statistic\nand derive its asymptotic distribution under the null hypothesis. We compare\nthe performance of the newly proposed method with the existing one-sided\nbivariate Kolmogorov-Smirnov test using real data and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 09:35:31 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 13:47:22 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Choi", "Hyejeong", ""], ["Lim", "Johan", ""], ["Kwak", "Minjung", ""], ["Park", "Seongoh", ""]]}, {"id": "1911.07248", "submitter": "Thomas Jaki", "authors": "Chi Chang, Thomas Jaki, Muhammad Saad Sadiq, Alena A. Kuhlemeier,\n  Daniel Feaster, Nathan Cole, Andrea Lamont, Daniel Oberski, Yasin Desai (The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium) M. Lee Van Horn", "title": "A Permutation Test for Assessing the Presence of Individual Differences\n  in Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One size fits all approaches to medicine have become a thing of the past as\nthe understanding of individual differences grows. The paper introduces a test\nfor the presence of heterogeneity in treatment effects in a clinical trial.\nHeterogeneity is assessed on the basis of the predicted individual treatment\neffects (PITE) framework and a permutation test is utilized to establish if\nsignificant heterogeneity is present. We first use the novel test to show that\nheterogeneity in the effects of interventions exists in the Amyotrophic Lateral\nSclerosis Clinical Trials. We then show, using two different predictive models\n(linear regression model and Random Forests) that the test has adequate type I\nerror control. Next, we use the ALS data as the basis for simulations to\ndemonstrate the ability of the permutation test to find heterogeneity in\ntreatment effects as a function of both effect size and sample size. We find\nthat the proposed test has good power to detected heterogeneity in treatment\neffects when the heterogeneity was due primarily to a single predictor, or when\nit was spread across the predictors. The predictive model, on the other hand is\nof secondary importance to detect heterogeneity. The non-parametric property of\nthe permutation test can be applied with any predictive method and requires no\nadditional assumptions to obtain PITEs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 14:45:46 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chang", "Chi", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Jaki", "Thomas", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Sadiq", "Muhammad Saad", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Kuhlemeier", "Alena A.", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Feaster", "Daniel", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Cole", "Nathan", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Lamont", "Andrea", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Oberski", "Daniel", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Desai", "Yasin", "", "The\n  Pooled Resource Open-Access ALS Clinical Trials Consortium"], ["Van Horn", "M. Lee", ""]]}, {"id": "1911.07285", "submitter": "Zhehui Chen", "authors": "Zhehui Chen, Simon Mak, C. F. Jeff Wu", "title": "A hierarchical expected improvement method for Bayesian optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expected Improvement (EI) method, proposed by Jones et al. (1998), is a\nwidely-used Bayesian optimization method, which makes use of a fitted Gaussian\nprocess model for efficient black-box optimization. However, one key drawback\nof EI is that it is overly greedy in exploiting the fitted Gaussian process\nmodel for optimization, which results in suboptimal solutions even with large\nsample sizes. To address this, we propose a new hierarchical EI (HEI)\nframework, which makes use of a hierarchical Gaussian process model. HEI\npreserves a closed-form acquisition function, and corrects the over-greediness\nof EI by encouraging exploration of the optimization space. We then introduce\nhyperparameter estimation methods which allow HEI to mimic a fully Bayesian\noptimization procedure, while avoiding expensive Markov-chain Monte Carlo\nsampling steps. We prove the global convergence of HEI over a broad function\nspace, and establish near-minimax convergence rates under certain prior\nspecifications. Numerical experiments show the improvement of HEI over existing\nBayesian optimization methods, for synthetic functions and a semiconductor\nmanufacturing optimization problem.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 17:04:42 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 22:37:16 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Zhehui", ""], ["Mak", "Simon", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1911.07319", "submitter": "Ming Yu", "authors": "Ming Yu, Varun Gupta, Mladen Kolar", "title": "Constrained High Dimensional Statistical Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In typical high dimensional statistical inference problems, confidence\nintervals and hypothesis tests are performed for a low dimensional subset of\nmodel parameters under the assumption that the parameters of interest are\nunconstrained. However, in many problems, there are natural constraints on\nmodel parameters and one is interested in whether the parameters are on the\nboundary of the constraint or not. e.g. non-negativity constraints for\ntransmission rates in network diffusion. In this paper, we provide algorithms\nto solve this problem of hypothesis testing in high-dimensional statistical\nmodels under constrained parameter space. We show that following our testing\nprocedure we are able to get asymptotic designed Type I error under the null.\nNumerical experiments demonstrate that our algorithm has greater power than the\nstandard algorithms where the constraints are ignored. We demonstrate the\neffectiveness of our algorithms on two real datasets where we have\n{\\emph{intrinsic}} constraint on the parameters.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 19:12:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yu", "Ming", ""], ["Gupta", "Varun", ""], ["Kolar", "Mladen", ""]]}, {"id": "1911.07494", "submitter": "Yi Yu", "authors": "Oscar Hernan Madrid Padilla, Yi Yu and Carey E. Priebe", "title": "Change point localization in dependent dynamic nonparametric random dot\n  product graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the change point localization problem in a sequence\nof dependent nonparametric random dot product graphs. To be specific, assume\nthat at every time point, a network is generated from a nonparametric random\ndot product graph model (see e.g. Athreya et al., 2017), where the latent\npositions are generated from unknown underlying distributions. The underlying\ndistributions are piecewise constant in time and change at unknown locations,\ncalled change points. Most importantly, we allow for dependence among networks\ngenerated between two consecutive change points. This setting incorporates\nedge-dependence within networks and temporal dependence between networks, which\nis the most flexible setting in the published literature.\n  To accomplish the task of consistently localizing change points, we propose a\nnovel change point detection algorithm, consisting of two steps. First, we\nestimate the latent positions of the random dot product model, our theoretical\nresult being a refined version of the state-of-the-art results, allowing the\ndimension of the latent positions to grow unbounded. Subsequently, we construct\na nonparametric version of the CUSUM statistic (Page, 1954, Padilla et al.,\n2019) that allows for temporal dependence. Consistent localization is proved\ntheoretically and supported by extensive numerical experiments, which\nillustrate state-of-the-art performance. We also provide in depth discussion of\npossible extensions to give more understanding and insights.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 09:18:02 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 17:28:10 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Yu", "Yi", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1911.07553", "submitter": "Lizhen Lin", "authors": "Lizhen Lin, Brian St. Thomas, Walter W. Piegorsch, James Scott and\n  Carlos Carvalho", "title": "A projection approach for multiple monotone regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape-constrained inference has wide applicability in bioassay, medicine,\neconomics, risk assessment, and many other fields. Although there has been a\nlarge amount of work on monotone-constrained univariate curve estimation,\nmultivariate shape-constrained problems are much more challenging, and fewer\nadvances have been made in this direction. With a focus on monotone regression\nwith multiple predictors, this current work proposes a projection approach to\nestimate a multiple monotone regression function. An initial unconstrained\nestimator -- such as a local polynomial estimator or spline estimator -- is\nfirst obtained, which is then projected onto the shape-constrained space. A\nshape-constrained estimate (with multiple predictors) is obtained by\nsequentially projecting an (adjusted) initial estimator along each univariate\ndirection. Compared to the initial unconstrained estimator, the projection\nestimate results in a reduction of estimation error in terms of both $L^p$\n($p\\geq 1$) distance and supremum distance. We also derive the asymptotic\ndistribution of the projection estimate. Simple computational algorithms are\navailable for implementing the projection in both the unidimensional and higher\ndimensional cases. Our work provides a simple recipe for practitioners to use\nin real applications, and is illustrated with a joint-action example from\nenvironmental toxicology.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 11:30:29 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Lin", "Lizhen", ""], ["Thomas", "Brian St.", ""], ["Piegorsch", "Walter W.", ""], ["Scott", "James", ""], ["Carvalho", "Carlos", ""]]}, {"id": "1911.07556", "submitter": "Kukush Alexander", "authors": "Alexander Kukush, Igor Mandel", "title": "Does Regression Approximate the Influence of the Covariates or Just\n  Measurement Errors? A Model Validity Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A criterion is proposed for testing hypothesis about the nature of the error\nvariance in the dependent variable in linear model, which separates correctly\nand incorrectly specified models. In the former only measurement errors\ndetermine the variance (i.e., dependent variable is correctly explained by\nindependent ones, up to measurement errors), while in the latter the model\nlacks some independent covariates (or has nonlinear structure). The proposed\nMEM-V (Measurement Error Model Validity) test checks the validity of the model\nwhen both dependent and independent covariates are measured with errors. The\ncriterion has asymptotic character, but numerical simulations outlined\napproximate boundaries where estimates make sense. A practical example of the\nimplementation of the test is discussed in detail; it shows ability of the test\nto detect wrong specification even in seemingly perfect models. This type of\nrelation between measurement errors and model specification has not been\nstudied before, and the proposed criterion may stimulate future research in\nthis important area.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 11:34:38 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kukush", "Alexander", ""], ["Mandel", "Igor", ""]]}, {"id": "1911.07762", "submitter": "Jun Li", "authors": "Lingjun Li, Jun Li", "title": "Online Change-Point Detection in High-Dimensional Covariance Structure\n  with Application to Dynamic Networks", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an online change-point detection procedure in the\ncovariance structure of high-dimensional data. A new stopping rule is proposed\nto terminate the process as early as possible when a change in covariance\nstructure occurs. The stopping rule allows temporal dependence and can be\napplied to non-Gaussian data. An explicit expression for the average run length\n(ARL) is derived, so that the level of threshold in the stopping rule can be\neasily obtained with no need to run time-consuming Monte Carlo simulations. We\nalso establish an upper bound for the expected detection delay (EDD), the\nexpression of which demonstrates the impact of data dependence and magnitude of\nchange in the covariance structure. Simulation studies are provided to confirm\naccuracy of the theoretical results. The practical usefulness of the proposed\nprocedure is illustrated by detecting the change of brain's covariance network\nin a resting-state fMRI dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:53:53 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 00:10:58 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Li", "Lingjun", ""], ["Li", "Jun", ""]]}, {"id": "1911.07827", "submitter": "\\'Alvaro Briz-Red\\'on", "authors": "\\'Alvaro Briz-Red\\'on and Francisco Mart\\'inez-Ruiz and Francisco\n  Montes", "title": "DRHotNet: An R package for detecting differential risk hotspots on a\n  linear network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common applications of spatial data analysis is detecting\nzones, at a certain investigation level, where a point-referenced event under\nstudy is especially concentrated. The detection of this kind of zones, which\nare usually referred to as hotspots, is essential in certain fields such as\ncriminology, epidemiology or traffic safety. Traditionally, hotspot detection\nprocedures have been developed over areal units of analysis. Although working\nat this spatial scale can be suitable enough for many research or practical\npurposes, detecting hotspots at a more accurate level (for instance, at the\nroad segment level) may be more convenient sometimes. Furthermore, it is\ntypical that hotspot detection procedures are entirely focused on the\ndetermination of zones where an event is (overall) highly concentrated. It is\nless common, by far, that such procedures prioritize the location of zones\nwhere a specific type of event is overrepresented in relation to the other\ntypes observed, which have been denoted as differential risk hotspots. The R\npackage DRHotNet provides several functionalities to facilitate the detection\nof differential risk hotspots along a linear network. In this paper, DRHotNet\nis depicted and its usage in the R console is shown through a detailed analysis\nof a crime dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 18:43:04 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Briz-Red\u00f3n", "\u00c1lvaro", ""], ["Mart\u00ednez-Ruiz", "Francisco", ""], ["Montes", "Francisco", ""]]}, {"id": "1911.08022", "submitter": "Timothy Pollington MSc", "authors": "Timothy M. Pollington (1 and 3), Michael J. Tildesley (2), T.\n  D\\'eirdre Hollingsworth (3) and Lloyd A. C. Chapman (4) ((1) MathSys CDT,\n  University of Warwick, UK, (2) Zeeman Institute (SBIDER), School of Life\n  Sciences and Mathematics Institute, University of Warwick, UK, (3) Big Data\n  Institute, Li Ka Shing Centre for Health Information and Discovery,\n  University of Oxford, UK, (4) London School of Hygiene & Tropical Medicine,\n  UK)", "title": "Developments in statistical inference when assessing spatiotemporal\n  disease clustering with the tau statistic", "comments": "Corresponding author is Timothy M. Pollington. Equal contributions by\n  T. D\\'eirdre Hollingsworth and Lloyd A. C. Chapman. Accepted by Spatial\n  Statistics (7 Mch 2020). 43 pp, 4180 words, 11 figs, 1 graphical abstract.\n  Changes: This is our post-print after refereeing prior to proof; Title;\n  Clarity in Methods esp. spatial bootstrapping; Non-essentials moved to\n  appendices; Public GitHub repo; Figs", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100438", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tau statistic $\\tau$ uses geolocation and, usually, symptom onset time to\nassess global spatiotemporal clustering from epidemiological data. We test\ndifferent factors that could affect graphical hypothesis tests of clustering or\nbias clustering range estimates based on the statistic, by comparison with a\nbaseline analysis of an open access measles dataset.\n  From re-analysing this data we find that the spatial bootstrap sampling\nmethod used to construct the confidence interval for the tau estimate and\nconfidence interval (CI) type can bias clustering range estimates. We suggest\nthat the bias-corrected and accelerated (BCa) CI is essential for asymmetric\nsample bootstrap distributions of tau estimates.\n  We also find evidence against no spatiotemporal clustering, $p$-value $\\in$\n[0,0.014] (global envelope test). We develop a tau-specific modification of the\nLoh & Stein spatial bootstrap sampling method, which gives more precise\nbootstrapped tau estimates and a 20% higher estimated clustering endpoint than\npreviously published (36.0m; 95% BCa CI (14.9, 46.6), vs 30m) and an equivalent\nincrease in the clustering area of elevated disease odds by 44%. What appears a\nmodest radial bias in the range estimate is more than doubled on the areal\nscale, which public health resources are proportional to. This difference could\nhave important consequences for control.\n  Correct practice of hypothesis testing of no clustering and clustering range\nestimation of the tau statistic are illustrated in the Graphical abstract. We\nadvocate proper implementation of this useful statistic, ultimately to reduce\ninaccuracies in control policy decisions made during disease clustering\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 00:54:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 09:17:38 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 13:50:17 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2020 16:40:34 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Pollington", "Timothy M.", "", "1 and 3"], ["Tildesley", "Michael J.", ""], ["Hollingsworth", "T. D\u00e9irdre", ""], ["Chapman", "Lloyd A. C.", ""]]}, {"id": "1911.08048", "submitter": "Yixuan Qiu", "authors": "Yixuan Qiu, Jing Lei, and Kathryn Roeder", "title": "Gradient-based Sparse Principal Component Analysis with Extensions to\n  Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (PCA) is an important technique for\ndimensionality reduction of high-dimensional data. However, most existing\nsparse PCA algorithms are based on non-convex optimization, which provide\nlittle guarantee on the global convergence. Sparse PCA algorithms based on a\nconvex formulation, for example the Fantope projection and selection (FPS),\novercome this difficulty, but are computationally expensive. In this work we\nstudy sparse PCA based on the convex FPS formulation, and propose a new\nalgorithm that is computationally efficient and applicable to large and\nhigh-dimensional data sets. Nonasymptotic and explicit bounds are derived for\nboth the optimization error and the statistical accuracy, which can be used for\ntesting and inference problems. We also extend our algorithm to online learning\nproblems, where data are obtained in a streaming fashion. The proposed\nalgorithm is applied to high-dimensional gene expression data for the detection\nof functional gene groups.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 02:17:09 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Qiu", "Yixuan", ""], ["Lei", "Jing", ""], ["Roeder", "Kathryn", ""]]}, {"id": "1911.08061", "submitter": "Xin Li", "authors": "Xin Li, Dongya Wu, Chong Li, Jinhua Wang, Jen-Chih Yao", "title": "Sparse recovery via nonconvex regularized $M$-estimators over\n  $\\ell_q$-balls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyse the recovery properties of nonconvex regularized\n$M$-estimators, under the assumption that the true parameter is of soft\nsparsity. In the statistical aspect, we establish the recovery bound for any\nstationary point of the nonconvex regularized $M$-estimator, under restricted\nstrong convexity and some regularity conditions on the loss function and the\nregularizer, respectively. In the algorithmic aspect, we slightly decompose the\nobjective function and then solve the nonconvex optimization problem via the\nproximal gradient method, which is proved to achieve a linear convergence rate.\nIn particular, we note that for commonly-used regularizers such as SCAD and\nMCP, a simpler decomposition is applicable thanks to our assumption on the\nregularizer, which helps to construct the estimator with better recovery\nperformance. Finally, we demonstrate our theoretical consequences and the\nadvantage of the assumption by several numerical experiments on the corrupted\nerrors-in-variables linear regression model. Simulation results show remarkable\nconsistency with our theory under high-dimensional scaling.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 02:56:35 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Li", "Xin", ""], ["Wu", "Dongya", ""], ["Li", "Chong", ""], ["Wang", "Jinhua", ""], ["Yao", "Jen-Chih", ""]]}, {"id": "1911.08103", "submitter": "Chenhe Zhang", "authors": "Yutong Nie and Chenhe Zhang", "title": "A Normal Approximation Method for Statistics in Knockouts", "comments": "18 pages, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The authors give an approximation method for Bayesian inference in arena\nmodel, which is focused on paired comparisons with eliminations and\nbifurcations. The approximation method simplifies the inference by reducing\nparameters and introducing normal distribution functions into the computation\nof posterior distribution, which is largely based on an important property of\nnormal random variables. Maximum a posteriori probability (MAP) and Bayesian\nprediction are then used to mine the information from the past pairwise\ncomparison data, such as an individual's strength or volatility and his\npossible future results. We conduct a simulation to show the accuracy and\nstability of the approximation method and demonstrate the algorithm on\nnonlinear parameter inference as well as prediction problem arising in the FIFA\nWorld Cup.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 05:35:36 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Nie", "Yutong", ""], ["Zhang", "Chenhe", ""]]}, {"id": "1911.08171", "submitter": "Christophe Ley", "authors": "Sladana Babic and Laetitia Gelbgras and Marc Hallin and Christophe Ley", "title": "Optimal tests for elliptical symmetry: specified and unspecified\n  location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the assumption of elliptical symmetry is quite common in\nmultivariate analysis and widespread in a number of applications, the problem\nof testing the null hypothesis of ellipticity so far has not been addressed in\na fully satisfactory way. Most of the literature in the area indeed addresses\nthe null hypothesis of elliptical symmetry with specified location and actually\naddresses location rather than non-elliptical alternatives. In this paper, we\nare proposing new classes of testing procedures, both for specified and\nunspecified location. The backbone of our construction is Le Cam's asymptotic\ntheory of statistical experiments, and optimality is to be understood locally\nand asymptotically within the family of generalized skew-elliptical\ndistributions. The tests we are proposing are meeting all the desired\nproperties of a``good'' test of elliptical symmetry: they have a simple\nasymptotic distribution under the entire null hypothesis of elliptical symmetry\nwith unspecified radial density and shape parameter; they are affine-invariant,\ncomputationally fast, intuitively understandable, and not too demanding in\nterms of moments. While achieving optimality against generalized\nskew-elliptical alternatives, they remain quite powerful under a much broader\nclass of non-elliptical distributions and significantly outperform the\navailable competitors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 09:27:59 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Babic", "Sladana", ""], ["Gelbgras", "Laetitia", ""], ["Hallin", "Marc", ""], ["Ley", "Christophe", ""]]}, {"id": "1911.08429", "submitter": "Sara Hamis", "authors": "Sara Hamis and Stanislav Stratiev and Gibin G Powathil", "title": "Uncertainty and Sensitivity Analyses Methods for Agent-Based\n  Mathematical Models: An Introductory Review", "comments": "41 pages", "journal-ref": "The Physics of Cancer, Research Advances, Chapter 1, Ed: Bernard S\n  Gerstman, January 2021", "doi": "10.1142/9789811223495_0001", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiscale, agent-based mathematical models of biological systems are often\nassociated with model uncertainty and sensitivity to parameter perturbations.\nHere, three uncertainty and sensitivity analyses methods, that are suitable to\nuse when working with agent-based models, are discussed. These methods are\nnamely Consistency Analysis, Robustness Analysis and Latin Hypercube Analysis.\nThis introductory review discusses origins, conventions, implementation and\nresult interpretation of the aforementioned methods. Information on how to\nimplement the discussed methods in MATLAB is included.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 17:51:34 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 13:30:45 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 10:06:32 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Hamis", "Sara", ""], ["Stratiev", "Stanislav", ""], ["Powathil", "Gibin G", ""]]}, {"id": "1911.08438", "submitter": "Elea McDonnell Feit", "authors": "Ron Berman and Elea McDonnell Feit", "title": "Principal Stratification for Advertising Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertising experiments often suffer from noisy responses making precise\nestimation of the average treatment effect (ATE) and evaluating ROI difficult.\nWe develop a principal stratification model that improves the precision of the\nATE by dividing the customers into three strata - those who buy regardless of\nad exposure, those who buy only if exposed to ads and those who do not buy\nregardless. The method decreases the variance of the ATE by separating out the\ntypically large share of customers who never buy and therefore have individual\ntreatment effects that are exactly zero. Applying the procedure to 5 catalog\nmailing experiments with sample sizes around 140,000 shows a reduction of\n36-57% in the variance of the estimate. When we include pre-randomization\ncovariates that predict stratum membership, we find that estimates of\ncustomers' past response to similar advertising are a good predictor of stratum\nmembership, even if such estimates are biased because past advertising was\ntargeted. Customers who have not purchased recently are also more likely to be\nin the \"never purchase\" stratum. We provide simple summary statistics that\nfirms can compute from their own experiment data to determine if the procedure\nis expected to be beneficial before applying it.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 18:02:11 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Berman", "Ron", ""], ["Feit", "Elea McDonnell", ""]]}, {"id": "1911.08504", "submitter": "Yan Li", "authors": "Yan Li, Matthew Sperrin, Glen P. Martin, Darren M Ashcroft, Tjeerd\n  Pieter van Staa", "title": "Examining the impact of data quality and completeness of electronic\n  health records on predictions of patients risks of cardiovascular disease", "comments": "2 tables 4 figures in the main manuscript, 1 table 3 figure in\n  appendix. Online published in IJMI, license CC-BY-NC-ND", "journal-ref": "https://doi.org/10.1016/j.ijmedinf.2019.104033", "doi": "10.1016/j.ijmedinf.2019.104033", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective is to assess the extent of variation of data quality and\ncompleteness of electronic health records and impact on the robustness of risk\npredictions of incident cardiovascular disease (CVD) using a risk prediction\ntool that is based on routinely collected data (QRISK3). The study design is a\nlongitudinal cohort study with a setting of 392 general practices (including\n3.6 million patients) linked to hospital admission data. Variation in data\nquality was assessed using Saez stability metrics quantifying outlyingness of\neach practice. Statistical frailty models evaluated whether accuracy of QRISK3\npredictions on individual predictions and effects of overall risk factors\n(linear predictor) varied between practices. There was substantial\nheterogeneity between practices in CVD incidence unaccounted for by QRISK3. In\nthe lowest quintile of statistical frailty, a QRISK3 predicted risk of 10% for\nfemale was in a range between 7.1% and 9.0% when incorporating practice\nvariability into the statistical frailty models; for the highest quintile, this\nwas 10.9%-16.4%. Data quality (using Saez metrics) and completeness were\ncomparable across different levels of statistical frailty. For example,\nrecording of missing information on ethnicity was 55.7%, 62.7%, 57.8%, 64.8%\nand 62.1% for practices from lowest to highest quintiles of statistical frailty\nrespectively. The effects of risk factors did not vary between practices with\nlittle statistical variation of beta coefficients. In conclusion, the\nconsiderable unmeasured heterogeneity in CVD incidence between practices was\nnot explained by variations in data quality or effects of risk factors. QRISK3\nrisk prediction should be supplemented with clinical judgement and evidence of\nadditional risk factors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:02:44 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Li", "Yan", ""], ["Sperrin", "Matthew", ""], ["Martin", "Glen P.", ""], ["Ashcroft", "Darren M", ""], ["van Staa", "Tjeerd Pieter", ""]]}, {"id": "1911.08531", "submitter": "Adrienne Mendrik", "authors": "Adri\\\"enne M. Mendrik and Stephen R. Aylward", "title": "A Framework for Challenge Design: Insight and Deployment Challenges to\n  Address Medical Image Analysis Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we aim to refine the concept of grand challenges in medical\nimage analysis, based on statistical principles from quantitative and\nqualitative experimental research. We identify two types of challenges based on\ntheir generalization objective: 1) a deployment challenge and 2) an insight\nchallenge. A deployment challenge's generalization objective is to find\nalgorithms that solve a medical image analysis problem, which thereby requires\nthe use of a quantitative experimental design. An insight challenge's\ngeneralization objective is to gain a broad understanding of what class of\nalgorithms might be effective for a class of medical image analysis problems,\nin which case a qualitative experimental design is sufficient. Both challenge\ntypes are valuable, but problems arise when a challenge's design and objective\nare inconsistent, as is often the case when a challenge does not carefully\nconsider these concepts. Therefore, in this paper, we propose a theoretical\nframework, based on statistical principles, to guide researchers in challenge\ndesign, documentation, and assessment. Experimental results are given that\nexplore the factors that effect the practical implementation of this\ntheoretical framework.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:49:39 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Mendrik", "Adri\u00ebnne M.", ""], ["Aylward", "Stephen R.", ""]]}, {"id": "1911.08628", "submitter": "Emi Tanaka", "authors": "Emi Tanaka and Francis K. C. Hui", "title": "Symbolic Formulae for Linear Mixed Models", "comments": null, "journal-ref": null, "doi": "10.1007/978-981-15-1960-4_1", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical model is a mathematical representation of an often simplified\nor idealised data-generating process. In this paper, we focus on a particular\ntype of statistical model, called linear mixed models (LMMs), that is widely\nused in many disciplines e.g.~agriculture, ecology, econometrics, psychology.\nMixed models, also commonly known as multi-level, nested, hierarchical or panel\ndata models, incorporate a combination of fixed and random effects, with LMMs\nbeing a special case. The inclusion of random effects in particular gives LMMs\nconsiderable flexibility in accounting for many types of complex correlated\nstructures often found in data. This flexibility, however, has given rise to a\nnumber of ways by which an end-user can specify the precise form of the LMM\nthat they wish to fit in statistical software. In this paper, we review the\nsoftware design for specification of the LMM (and its special case, the linear\nmodel), focusing in particular on the use of high-level symbolic model formulae\nand two popular but contrasting R-packages in lme4 and asreml.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 23:30:17 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Tanaka", "Emi", ""], ["Hui", "Francis K. C.", ""]]}, {"id": "1911.08662", "submitter": "Kenichiro McAlinn", "authors": "K\\=osaku Takanashi and Kenichiro McAlinn", "title": "Predictions with dynamic Bayesian predictive synthesis are exact minimax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the combination of multiple predictive distributions for time\nseries data when all forecasts are misspecified. We show that a specific\ndynamic form of Bayesian predictive synthesis -- a general and coherent\nBayesian framework for ensemble methods -- produces exact minimax predictive\ndensities with regard to Kullback-Leibler loss, providing theoretical support\nfor finite sample predictive performance over existing ensemble methods. A\nsimulation study that highlights this theoretical result is presented, showing\nthat dynamic Bayesian predictive synthesis is superior to other ensemble\nmethods using multiple metrics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 01:46:24 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 05:35:43 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 05:16:37 GMT"}, {"version": "v4", "created": "Sat, 3 Jul 2021 15:13:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Takanashi", "K\u014dsaku", ""], ["McAlinn", "Kenichiro", ""]]}, {"id": "1911.08682", "submitter": "Haema Nilakanta", "authors": "Haema Nilakanta, Zack W. Almquist, and Galin L. Jones", "title": "Ensuring Reliable Monte Carlo Estimates of Network Properties", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature in social network analysis has largely focused on methods and\nmodels which require complete network data; however there exist many networks\nwhich can only be studied via sampling methods due to the scale or complexity\nof the network, access limitations, or the population of interest is hard to\nreach. In such cases, the application of random walk-based Markov chain Monte\nCarlo (MCMC) methods to estimate multiple network features is common. However,\nthe reliability of these estimates has been largely ignored. We consider and\nfurther develop multivariate MCMC output analysis methods in the context of\nnetwork sampling to directly address the reliability of the multivariate\nestimation. This approach yields principled, computationally efficient, and\nbroadly applicable methods for assessing the Monte Carlo estimation procedure.\nIn particular, with respect to two random-walk algorithms, a simple random walk\nand a Metropolis-Hastings random walk, we construct and compare network\nparameter estimates, effective sample sizes, coverage probabilities, and\nstopping rules, all of which speaks to the estimation reliability.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 03:29:05 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 01:55:59 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Nilakanta", "Haema", ""], ["Almquist", "Zack W.", ""], ["Jones", "Galin L.", ""]]}, {"id": "1911.08703", "submitter": "Kaito Shimamura", "authors": "Kaito Shimamura, Shuichi Kawano", "title": "Bayesian sparse convex clustering via global-local shrinkage priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse convex clustering is to cluster observations and conduct variable\nselection simultaneously in the framework of convex clustering. Although a\nweighted $L_1$ norm is usually employed for the regularization term in sparse\nconvex clustering, its use increases the dependence on the data and reduces the\nestimation accuracy if the sample size is not sufficient. To tackle these\nproblems, this paper proposes a Bayesian sparse convex clustering method based\non the ideas of Bayesian lasso and global-local shrinkage priors. We introduce\nGibbs sampling algorithms for our method using scale mixtures of normal\ndistributions. The effectiveness of the proposed methods is shown in simulation\nstudies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 04:41:05 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 17:25:18 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Shimamura", "Kaito", ""], ["Kawano", "Shuichi", ""]]}, {"id": "1911.08830", "submitter": "Ruiqi Liu", "authors": "Ruiqi Liu, Ben Boukai and Zuofeng Shang", "title": "Statistical Inference on Partially Linear Panel Model under Unobserved\n  Linearity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new statistical procedure, based on a modified spline basis, is proposed to\nidentify the linear components in the panel data model with fixed effects.\nUnder some mild assumptions, the proposed procedure is shown to consistently\nestimate the underlying regression function, correctly select the linear\ncomponents, and effectively conduct the statistical inference. When compared to\nexisting methods for detection of linearity in the panel model, our approach is\ndemonstrated to be theoretically justified as well as practically convenient.\nWe provide a computational algorithm that implements the proposed procedure\nalong with a path-based solution method for linearity detection, which avoids\nthe burden of selecting the tuning parameter for the penalty term. Monte Carlo\nsimulations are conducted to examine the finite sample performance of our\nproposed procedure with detailed findings that confirm our theoretical results\nin the paper. Applications to Aggregate Production and Environmental Kuznets\nCurve data also illustrate the necessity for detecting linearity in the\npartially linear panel model.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 11:16:00 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Liu", "Ruiqi", ""], ["Boukai", "Ben", ""], ["Shang", "Zuofeng", ""]]}, {"id": "1911.08955", "submitter": "Saverio Ranciati", "authors": "Saverio Ranciati, Veronica Vinciotti, Ernst C. Wit, Giuliano\n  Galimberti", "title": "Mixtures of multivariate generalized linear models with overlapping\n  clusters", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of ubiquitous monitoring and measurement protocols, studies\nhave started to focus more and more on complex, multivariate and heterogeneous\ndatasets. In such studies, multivariate response variables are drawn from a\nheterogeneous population often in the presence of additional covariate\ninformation. In order to deal with this intrinsic heterogeneity, regression\nanalyses have to be clustered for different groups of units. Up until now,\nmixture model approaches assigned units to distinct and non-overlapping groups.\nHowever, not rarely these units exhibit more complex organization and\nclustering. It is our aim to define a mixture of generalized linear models with\noverlapping clusters of units. This involves crucially an overlap function,\nthat maps the coefficients of the parent clusters into the the coefficient of\nthe multiple allocation units. We present a computationally efficient MCMC\nscheme that samples the posterior distribution of the parameters in the model.\nAn example on a two-mode network study shows details of the implementation in\nthe case of a multivariate probit regression setting. A simulation study shows\nthe overall performance of the method, whereas an illustration of the voting\nbehaviour on the US supreme court shows how the 9 justices split in two\noverlapping sets of justices.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:03:06 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ranciati", "Saverio", ""], ["Vinciotti", "Veronica", ""], ["Wit", "Ernst C.", ""], ["Galimberti", "Giuliano", ""]]}, {"id": "1911.09006", "submitter": "Gilles Kratzer", "authors": "Gilles Kratzer, Fraser Iain Lewis, Arianna Comin, Marta Pittavino,\n  Reinhard Furrer", "title": "Additive Bayesian Network Modelling with the R Package abn", "comments": "37 pages, 14 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package abn is designed to fit additive Bayesian models to\nobservational datasets. It contains routines to score Bayesian networks based\non Bayesian or information theoretic formulations of generalized linear models.\nIt is equipped with exact search and greedy search algorithms to select the\nbest network. It supports a possible blend of continuous, discrete and count\ndata and input of prior knowledge at a structural level. The Bayesian\nimplementation supports random effects to control for one-layer clustering. In\nthis paper, we give an overview of the methodology and illustrate the package's\nfunctionalities using a veterinary dataset about respiratory diseases in\ncommercial swine production.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 16:22:22 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Kratzer", "Gilles", ""], ["Lewis", "Fraser Iain", ""], ["Comin", "Arianna", ""], ["Pittavino", "Marta", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1911.09012", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Parsimonious Mixtures of Matrix Variate Bilinear Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, data have become increasingly higher dimensional, which has\nprompted an increased need for dimension reduction techniques. This is perhaps\nespecially true for clustering (unsupervised classification) as well as\nsemi-supervised and supervised classification. Many methods have been proposed\nin the literature for two-way (multivariate) data and quite recently methods\nhave been presented for three-way (matrix variate) data. One such such method\nis the mixtures of matrix variate bilinear factor analyzers (MMVBFA) model.\nHerein, we propose of total of 64 parsimonious MMVBFA models. Simulated and\nreal data are used for illustration.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 16:30:31 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1911.09049", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "Sharp hypotheses and bispatial inference", "comments": "Corrected, rewritten and extended. *Final version*", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental class of inferential problems are those characterised by there\nhaving been a substantial degree of pre-data (or prior) belief that the value\nof a model parameter was equal or lay close to a specified value, which may,\nfor example, be the value that indicates the absence of an effect. Standard\nways of tackling problems of this type, including the Bayesian method, are\noften highly inadequate in practice. To address this issue, an inferential\nframework called bispatial inference is put forward, which can be viewed as\nboth a generalisation and radical reinterpretation of existing approaches to\ninference that are based on P values. It is shown that to obtain an appropriate\npost-data density function for a given parameter, it is often convenient to\ncombine a special type of bispatial inference, which is constructed around\none-sided P values, with a previously outlined form of fiducial inference.\nFinally, by using what are called post-data opinion curves, this\nbispatial-fiducial theory is naturally extended to deal with the general\nscenario in which any number of parameters may be unknown. The application of\nthe theory is illustrated in various examples, which are especially relevant to\nthe analysis of clinical trial data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:24:29 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 18:28:26 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "1911.09128", "submitter": "Jean-Jacques Forneron", "authors": "Jean-Jacques Forneron", "title": "A Scrambled Method of Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo (qMC) methods are a powerful alternative to classical\nMonte-Carlo (MC) integration. Under certain conditions, they can approximate\nthe desired integral at a faster rate than the usual Central Limit Theorem,\nresulting in more accurate estimates. This paper explores these methods in a\nsimulation-based estimation setting with an emphasis on the scramble of Owen\n(1995). For cross-sections and short-panels, the resulting Scrambled Method of\nMoments simply replaces the random number generator with the scramble\n(available in most softwares) to reduce simulation noise. Scrambled Indirect\nInference estimation is also considered. For time series, qMC may not apply\ndirectly because of a curse of dimensionality on the time dimension. A simple\nalgorithm and a class of moments which circumvent this issue are described.\nAsymptotic results are given for each algorithm. Monte-Carlo examples\nillustrate these results in finite samples, including an income process with\n\"lots of heterogeneity.\"\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 19:01:04 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Forneron", "Jean-Jacques", ""]]}, {"id": "1911.09171", "submitter": "Siyu Heng", "authors": "Siyu Heng, Bo Zhang, Xu Han, Scott A. Lorch, Dylan S. Small", "title": "Instrumental Variables: To Strengthen or Not to Strengthen?", "comments": "86 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables (IVs) are extensively used to estimate treatment\neffects when the treatment and outcome are confounded by unmeasured\nconfounders; however, weak IVs are often encountered in empirical studies and\nmay cause problems. Many studies have considered building a stronger IV from\nthe original, possibly weak, IV in the design stage of a matched study at the\ncost of not using some of the samples in the analysis. It is widely accepted\nthat strengthening an IV tends to render nonparametric tests more powerful and\nwill increase the power of sensitivity analyses in large samples. In this\narticle, we re-evaluate this conventional wisdom to bring new insights into\nthis topic. We consider matched observational studies from three perspectives.\nFirst, we evaluate the trade-off between IV strength and sample size on\nnonparametric tests assuming the IV is valid and exhibit conditions under which\nstrengthening an IV increases power and conversely conditions under which it\ndecreases power. Second, we derive a necessary condition for a valid\nsensitivity analysis model with continuous doses. We show that the $\\Gamma$\nsensitivity analysis model, which has been previously used to come to the\nconclusion that strengthening an IV increases the power of sensitivity analyses\nin large samples, does not apply to the continuous IV setting and thus this\npreviously reached conclusion may be invalid. Third, we quantify the bias of\nthe Wald estimator with a possibly invalid IV under an oracle and leverage it\nto develop a valid sensitivity analysis framework; under this framework, we\nshow that strengthening an IV may amplify or mitigate the bias of the\nestimator, and may or may not increase the power of sensitivity analyses. We\nalso discuss how to better adjust for the observed covariates when building an\nIV in matched studies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 21:07:16 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 02:44:46 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Heng", "Siyu", ""], ["Zhang", "Bo", ""], ["Han", "Xu", ""], ["Lorch", "Scott A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1911.09200", "submitter": "Wesley Tansey", "authors": "Jackson H. Loper, Lihua Lei, William Fithian, Wesley Tansey", "title": "Smoothed Nested Testing on Directed Acyclic Graphs", "comments": "Revised with genetic interaction maps application and new theory of\n  PRDS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multiple hypothesis testing when there is a\nlogical nested structure to the hypotheses. When one hypothesis is nested\ninside another, the outer hypothesis must be false if the inner hypothesis is\nfalse. We model the nested structure as a directed acyclic graph, including\nchain and tree graphs as special cases. Each node in the graph is a hypothesis\nand rejecting a node requires also rejecting all of its ancestors. We propose a\ngeneral framework for adjusting node-level test statistics using the known\nlogical constraints. Within this framework, we study a smoothing procedure that\ncombines each node with all of its descendants to form a more powerful\nstatistic. We prove a broad class of smoothing strategies can be used with\nexisting selection procedures to control the familywise error rate, false\ndiscovery exceedance rate, or false discovery rate, so long as the original\ntest statistics are independent under the null. When the null statistics are\nnot independent but are derived from positively-correlated normal observations,\nwe prove control for all three error rates when the smoothing method is\narithmetic averaging of the observations. Simulations and an application to a\nreal biology dataset demonstrate that smoothing leads to substantial power\ngains.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 22:40:30 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 20:04:58 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Loper", "Jackson H.", ""], ["Lei", "Lihua", ""], ["Fithian", "William", ""], ["Tansey", "Wesley", ""]]}, {"id": "1911.09248", "submitter": "Sida Peng", "authors": "Sida Peng and Yang Ning", "title": "Regression Discontinuity Design under Self-selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Regression Discontinuity (RD) design, self-selection leads to different\ndistributions of covariates on two sides of the policy intervention, which\nessentially violates the continuity of potential outcome assumption. The\nstandard RD estimand becomes difficult to interpret due to the existence of\nsome indirect effect, i.e. the effect due to self selection. We show that the\ndirect causal effect of interest can still be recovered under a class of\nestimands. Specifically, we consider a class of weighted average treatment\neffects tailored for potentially different target populations. We show that a\nspecial case of our estimands can recover the average treatment effect under\nthe conditional independence assumption per Angrist and Rokkanen (2015), and\nanother example is the estimand recently proposed in Fr\\\"olich and Huber\n(2018). We propose a set of estimators through a weighted local linear\nregression framework and prove the consistency and asymptotic normality of the\nestimators. Our approach can be further extended to the fuzzy RD case. In\nsimulation exercises, we compare the performance of our estimator with the\nstandard RD estimator. Finally, we apply our method to two empirical data sets:\nthe U.S. House elections data in Lee (2008) and a novel data set from Microsoft\nBing on Generalized Second Price (GSP) auction.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 02:28:59 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Peng", "Sida", ""], ["Ning", "Yang", ""]]}, {"id": "1911.09254", "submitter": "Yujie Wu", "authors": "Yujie Wu, Mitchell H. Gail, Stephanie A. Smith-Warner, Regina G.\n  Ziegler, Molin Wang", "title": "Spline Analysis of Biomarker Data Pooled From Multiple Matched/Nested\n  Case-Control Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling biomarker data across multiple studies enables researchers to get\nmore precise estimates of the association between biomarker exposure\nmeasurements and disease risks due to increased sample sizes. However,\nbiomarker measurements vary significantly across different assays and\nlaboratories, and therefore calibration of the local laboratory measurements to\na reference laboratory is necessary before pooling data. We propose two methods\nthat can estimate a nonlinear relationship between biomarker exposure\nmeasurements and disease risks using spline functions with a nested\ncase-control study design: full calibration and internalized calibration. The\nfull calibration method calibrates all observations using a study-specific\ncalibration model while the internalized calibration method only calibrates\nobservations that do not have reference laboratory measurements available. We\ncompare the two methods with a naive method whereby data are pooled without\ncalibration. We find that: (1) Internalized and full calibration methods have\nsubstantially better performance than the naive method in terms of average\nrelative bias and coverage rate. (2) Full calibration is more robust than\ninternalized calibration when the size of calibration subsets varies. We apply\nour methods to a pooling project with nested case-control study design to\nestimate the association of circulating Vitamin D levels with the risk of\ncolorectal cancer.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 02:50:10 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wu", "Yujie", ""], ["Gail", "Mitchell H.", ""], ["Smith-Warner", "Stephanie A.", ""], ["Ziegler", "Regina G.", ""], ["Wang", "Molin", ""]]}, {"id": "1911.09260", "submitter": "Yifan Cui", "authors": "Yifan Cui, Eric Tchetgen Tchetgen", "title": "A semiparametric instrumental variable approach to optimal treatment\n  regimes under endogeneity", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a fast-growing literature on estimating optimal treatment regimes\nbased on randomized trials or observational studies under a key identifying\ncondition of no unmeasured confounding. Because confounding by unmeasured\nfactors cannot generally be ruled out with certainty in observational studies\nor randomized trials subject to noncompliance, we propose a general\ninstrumental variable approach to learning optimal treatment regimes under\nendogeneity. Specifically, we establish identification of both value function\n$E[Y_{\\mathcal{D}(L)}]$ for a given regime $\\mathcal{D}$ and optimal regimes\n$\\text{argmax}_{\\mathcal{D}} E[Y_{\\mathcal{D}(L)}]$ with the aid of a binary\ninstrumental variable, when no unmeasured confounding fails to hold. We also\nconstruct novel multiply robust classification-based estimators. Furthermore,\nwe propose to identify and estimate optimal treatment regimes among those who\nwould comply to the assigned treatment under a standard monotonicity\nassumption. In this latter case, we establish the somewhat surprising result\nthat complier optimal regimes can be consistently estimated without directly\ncollecting compliance information and therefore without the complier average\ntreatment effect itself being identified. Our approach is illustrated via\nextensive simulation studies and a data application on the effect of child\nrearing on labor participation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:10:08 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 00:22:59 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 16:52:58 GMT"}, {"version": "v4", "created": "Thu, 21 May 2020 20:24:14 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 16:09:39 GMT"}, {"version": "v6", "created": "Thu, 25 Jun 2020 20:43:31 GMT"}, {"version": "v7", "created": "Sat, 4 Jul 2020 01:13:19 GMT"}, {"version": "v8", "created": "Mon, 10 Aug 2020 19:35:06 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Cui", "Yifan", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1911.09274", "submitter": "Pulong Ma", "authors": "Pulong Ma and Anirban Mondal and Bledar Konomi and Jonathan Hobbs and\n  Joon Song and Emily Kang", "title": "Computer Model Emulation with High-Dimensional Functional Output in\n  Large-Scale Observing System Uncertainty Experiments", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing system uncertainty experiments (OSUEs) have been recently proposed\nas a cost-effective way to perform probabilistic assessment of retrievals for\nNASA's Orbiting Carbon Observatory-2 (OCO-2) mission. One important component\nin the OCO-2 retrieval algorithm is a full-physics forward model that describes\nthe mathematical relationship between atmospheric variables such as carbon\ndioxide and radiances measured by the remote sensing instrument. This forward\nmodel is complicated and computationally expensive but large-scale OSUEs\nrequire evaluation of this model numerous times, which makes it infeasible for\ncomprehensive experiments. To tackle this issue, we develop a statistical\nemulator to facilitate large-scale OSUEs in the OCO-2 mission with independent\nemulation. Within each distinct spectral band, the emulator represents\nradiances output at irregular wavelengths via a linear combination of basis\nfunctions and random coefficients. These random coefficients are then modeled\nwith nearest-neighbor Gaussian processes with built-in input dimension\nreduction via active subspace. The proposed emulator reduces dimensionality in\nboth input space and output space, so that fast computation is achieved within\na fully Bayesian inference framework. Validation experiments demonstrate that\nthis emulator outperforms other competing statistical methods and a reduced\norder model that approximates the full-physics forward model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:58:27 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 17:33:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ma", "Pulong", ""], ["Mondal", "Anirban", ""], ["Konomi", "Bledar", ""], ["Hobbs", "Jonathan", ""], ["Song", "Joon", ""], ["Kang", "Emily", ""]]}, {"id": "1911.09343", "submitter": "Ke Zhu", "authors": "Guochang Wang, Ke Zhu, Guodong Li, Wai Keung Li", "title": "Hybrid quantile estimation for asymmetric power GARCH models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric power GARCH models have been widely used to study the higher order\nmoments of financial returns, while their quantile estimation has been rarely\ninvestigated. This paper introduces a simple monotonic transformation on its\nconditional quantile function to make the quantile regression tractable. The\nasymptotic normality of the resulting quantile estimators is established under\neither stationarity or non-stationarity. Moreover, based on the estimation\nprocedure, new tests for strict stationarity and asymmetry are also\nconstructed. This is the first try of the quantile estimation for\nnon-stationary ARCH-type models in the literature. The usefulness of the\nproposed methodology is illustrated by simulation results and real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 08:45:30 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wang", "Guochang", ""], ["Zhu", "Ke", ""], ["Li", "Guodong", ""], ["Li", "Wai Keung", ""]]}, {"id": "1911.09408", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Yan Lu, and Irini Moustaki", "title": "Detection of Two-Way Outliers in Multivariate Data and Application to\n  Cheating Detection in Educational Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a new latent variable model for the simultaneous (two-way)\ndetection of outlying individuals and items for item-response-type data. The\nproposed model is a synergy between a factor model for binary responses and\ncontinuous response times that captures normal item response behaviour and a\nlatent class model that captures the outlying individuals and items. A\nstatistical decision framework is developed under the proposed model that\nprovides compound decision rules for controlling local false\ndiscovery/nondiscovery rates of outlier detection. Statistical inference is\ncarried out under a Bayesian framework, for which a Markov chain Monte Carlo\nalgorithm is developed. The proposed method is applied to the detection of\ncheating in educational tests due to item leakage using a case study of a\ncomputer-based nonadaptive licensure assessment. The performance of the\nproposed method is evaluated by simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 11:08:16 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 21:54:15 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 10:40:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chen", "Yunxiao", ""], ["Lu", "Yan", ""], ["Moustaki", "Irini", ""]]}, {"id": "1911.09442", "submitter": "Uri Keich", "authors": "Kristen Emery and Uri Keich", "title": "Controlling the FDR in variable selection via multiple knockoffs", "comments": "Fixed minor linguistic errors in the original submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Barber and Candes recently introduced a feature selection method called\nknockoff+ that controls the false discovery rate (FDR) among the selected\nfeatures in the classical linear regression problem. Knockoff+ uses the\ncompetition between the original features and artificially created knockoff\nfeatures to control the FDR [1]. We generalize Barber and Candes' knockoff\nconstruction to generate multiple knockoffs and use those in conjunction with a\nrecently developed general framework for multiple competition-based FDR control\n[9].\n  We prove that using our initial multiple-knockoff construction the combined\nprocedure rigorously controls the FDR in the finite sample setting. Because\nthis construction has a somewhat limited utility we introduce a heuristic we\ncall \"batching\" which significantly improves the power of our multiple-knockoff\nprocedures.\n  Finally, we combine the batched knockoffs with a new context-dependent\nresampling scheme that replaces the generic resampling scheme used in the\ngeneral multiple-competition setup. We show using simulations that the\nresulting \"multi-knockoff-select\" procedure empirically controls the FDR in the\nfinite setting of the variable selection problem while often delivering\nsubstantially more power than knockoff+.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 12:41:48 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 03:16:18 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Emery", "Kristen", ""], ["Keich", "Uri", ""]]}, {"id": "1911.09511", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Nicolas Idrobo, Rocio Titiunik", "title": "A Practical Introduction to Regression Discontinuity Designs:\n  Foundations", "comments": null, "journal-ref": null, "doi": "10.1017/9781108684606", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Element and its accompanying Element, Matias D. Cattaneo, Nicolas\nIdrobo, and Rocio Titiunik provide an accessible and practical guide for the\nanalysis and interpretation of Regression Discontinuity (RD) designs that\nencourages the use of a common set of practices and facilitates the\naccumulation of RD-based empirical evidence. In this Element, the authors\ndiscuss the foundations of the canonical Sharp RD design, which has the\nfollowing features: (i) the score is continuously distributed and has only one\ndimension, (ii) there is only one cutoff, and (iii) compliance with the\ntreatment assignment is perfect. In the accompanying Element, the authors\ndiscuss practical and conceptual extensions to the basic RD setup.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 14:58:18 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Idrobo", "Nicolas", ""], ["Titiunik", "Rocio", ""]]}, {"id": "1911.09625", "submitter": "Lionel Barnett", "authors": "A. J. Gutknecht and L. Barnett", "title": "Sampling distribution for single-regression Granger causality estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show for the first time that, under the null hypothesis of vanishing\nGranger causality, the single-regression Granger-Geweke estimator converges to\na generalised $\\chi^2$ distribution, which may be well approximated by a\n$\\Gamma$ distribution. We show that this holds too for Geweke's spectral\ncausality averaged over a given frequency band, and derive explicit expressions\nfor the generalised $\\chi^2$ and $\\Gamma$-approximation parameters in both\ncases. We present an asymptotically valid Neyman-Pearson test based on the\nsingle-regression estimators, and discuss in detail how it may be usefully\nemployed in realistic scenarios where autoregressive model order is unknown or\ninfinite. We outline how our analysis may be extended to the conditional case,\npoint-frequency spectral Granger causality, state-space Granger causality, and\nthe Granger causality $F$-test statistic. Finally, we discuss approaches to\napproximating the distribution of the single-regression estimator under the\nalternative hypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 17:34:24 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 10:54:03 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gutknecht", "A. J.", ""], ["Barnett", "L.", ""]]}, {"id": "1911.09638", "submitter": "Ruilin Li", "authors": "Ruilin Li, Robert Tibshirani", "title": "On the Use of C-index for Stratified and Cross-Validated Cox Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a baseline-adjusted C-index to evaluate fitted Cox proportional\nhazard models. This metric is particularly useful in evaluating stratified Cox\nmodels, as well as model selection using cross validation. Our metric is able\nto compare all pairs of comparable individuals in strata or cross validation\nfolds, as opposed to only pairs within the same stratum/folds. We demonstrate\nthrough simulations and real data applications that the baseline adjusted\nC-index is more stable and that it selects better model in high-dimensional\n$L^1$ penalized Cox regression.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 17:50:54 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Li", "Ruilin", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1911.09641", "submitter": "Brennan Bean", "authors": "Brennan Bean, Yan Sun, and Marc Maguire", "title": "Interval-Valued Kriging Models for Geostatistical Mapping with Imprecise\n  Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many geosciences data are imprecise due to various limitations and\nuncertainties in the measuring process. One way to preserve this imprecision in\na geostatistical mapping framework is to characterize the measurements as\nintervals rather than single values. To effectively analyze the interval-valued\ndata, this paper proposes and develops interval-valued kriging models based on\nthe theory of random sets and a generalized L2 metric. These models overcome\nthe mathematical difficulties of a previous development and are computationally\nmore feasible. Numerical implementation of our interval-valued kriging is\nprovided using a penalty-based constrained optimization algorithm. An\napplication to the prediction of design ground snow loads in Utah, USA, is\npresented that demonstrates the advantages of the proposed models in preserving\ncrucial sources of uncertainty towards a more efficient risk-based designing\nscheme.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 17:54:38 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Bean", "Brennan", ""], ["Sun", "Yan", ""], ["Maguire", "Marc", ""]]}, {"id": "1911.09656", "submitter": "Mike West", "authors": "Mike West", "title": "Bayesian forecasting of multivariate time series: Scalability, structure\n  uncertainty and decisions", "comments": "2018 Akaike Memorial Lecture Award paper", "journal-ref": null, "doi": "10.1007/s10463-019-00741-3", "report-no": null, "categories": "stat.ME eess.SP math.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I overview recent research advances in Bayesian state-space modeling of\nmultivariate time series. A main focus is on the decouple/recouple concept that\nenables application of state-space models to increasingly large-scale data,\napplying to continuous or discrete time series outcomes. The scope includes\nlarge-scale dynamic graphical models for forecasting and multivariate\nvolatility analysis in areas such as economics and finance, multi-scale\napproaches for forecasting discrete/count time series in areas such as\ncommercial sales and demand forecasting, and dynamic network flow models for\nareas including internet traffic monitoring. In applications, explicit\nforecasting, monitoring and decision goals are paramount and should factor into\nmodel assessment and comparison, a perspective that is highlighted.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:35:12 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:57:39 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["West", "Mike", ""]]}, {"id": "1911.09761", "submitter": "Xin Xing", "authors": "Xin Xing, Zhigen Zhao, Jun S. Liu", "title": "Controlling False Discovery Rate Using Gaussian Mirrors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously finding multiple influential variables and controlling the\nfalse discovery rate (FDR) for linear regression models is a fundamental\nproblem. We here propose the Gaussian Mirror (GM) method, which creates for\neach predictor variable a pair of mirror variables by adding and subtracting a\nrandomly generated Gaussian perturbation, and proceeds with a certain\nregression method, such as the ordinary least-square or the Lasso (the mirror\nvariables can also be created after selection). The mirror variables naturally\nlead to test statistics effective for controlling the FDR. Under a mild\nassumption on the dependence among the covariates, we show that the FDR can be\ncontrolled at any designated level asymptotically. We also demonstrate through\nextensive numerical studies that the GM method is more powerful than many\nexisting methods for selecting relevant variables subject to FDR control,\nespecially for cases when the covariates are highly correlated and the\ninfluential variables are not overly sparse.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 21:33:35 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 04:58:09 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 17:19:37 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Xing", "Xin", ""], ["Zhao", "Zhigen", ""], ["Liu", "Jun S.", ""]]}, {"id": "1911.09779", "submitter": "Melissa Humphries Dr", "authors": "Lachlann McArthur, Melissa A. Humphries", "title": "Multi-model mimicry for model selection according to generalised\n  goodness-of-fit criteria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-model mimicry (MMM) is a flexible model selection technique for\ncomparison of multiple, non-nested models on any desired goodness-of-fit\ncriteria. Applicable to any set of candidate models that are 1) able to be fit\nto observed data, 2) can simulate new sets of data under the models, and 3)\nhave a metric by which a dataset's goodness-of-fit to the model can be\ncalculated, MMM has a much broader range of applicability than many standard\nmodel selection techniques. This manuscript highlights the previous literature\nwhilst presenting the theoretical framework underpinning MMM. The scope of\napplicability is broadened through presentation of generalised criteria for\ncomparison and the effectiveness of the method is demonstrated. Clear\ninstruction for the application of MMM and the classification techniques\nrequired for model selection are also included.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 23:04:35 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2019 23:48:44 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["McArthur", "Lachlann", ""], ["Humphries", "Melissa A.", ""]]}, {"id": "1911.09802", "submitter": "Ting Ye", "authors": "Ting Ye, Jun Shao, Hyunseung Kang", "title": "Debiased Inverse-Variance Weighted Estimator in Two-Sample Summary-Data\n  Mendelian Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) has become a popular approach to study the\neffect of a modifiable exposure on an outcome by using genetic variants as\ninstrumental variables. A challenge in MR is that each genetic variant explains\na relatively small proportion of variance in the exposure and there are many\nsuch variants, a setting known as many weak instruments. To this end, we\nprovide a theoretical characterization of the statistical properties of two\npopular estimators in MR, the inverse-variance weighted (IVW) estimator and the\nIVW estimator with screened instruments using an independent selection dataset,\nunder many weak instruments. We then propose a debiased IVW estimator, a simple\nmodification of the IVW estimator, that is robust to many weak instruments and\ndoesn't require screening. Additionally, we present two instrument selection\nmethods to improve the efficiency of the new estimator when a selection dataset\nis available. An extension of the debiased IVW estimator to handle balanced\nhorizontal pleiotropy is also discussed. We conclude by demonstrating our\nresults in simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 01:21:11 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 15:52:24 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ye", "Ting", ""], ["Shao", "Jun", ""], ["Kang", "Hyunseung", ""]]}, {"id": "1911.09879", "submitter": "Saurabh Khanna", "authors": "Saurabh Khanna and Vincent Y. F. Tan", "title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger\n  Causality", "comments": "A new RNN architecture for inferring nonlinear Granger causality from\n  time series data with emphasis on learning time-localized predictive features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality is a widely-used criterion for analyzing interactions in\nlarge-scale networks. As most physical interactions are inherently nonlinear,\nwe consider the problem of inferring the existence of pairwise Granger\ncausality between nonlinearly interacting stochastic processes from their time\nseries measurements. Our proposed approach relies on modeling the embedded\nnonlinearities in the measurements using a component-wise time series\nprediction model based on Statistical Recurrent Units (SRUs). We make a case\nthat the network topology of Granger causal relations is directly inferrable\nfrom a structured sparse estimate of the internal parameters of the SRU\nnetworks trained to predict the processes$'$ time series measurements. We\npropose a variant of SRU, called economy-SRU, which, by design has considerably\nfewer trainable parameters, and therefore less prone to overfitting. The\neconomy-SRU computes a low-dimensional sketch of its high-dimensional hidden\nstate in the form of random projections to generate the feedback for its\nrecurrent processing. Additionally, the internal weight parameters of the\neconomy-SRU are strategically regularized in a group-wise manner to facilitate\nthe proposed network in extracting meaningful predictive features that are\nhighly time-localized to mimic real-world causal events. Extensive experiments\nare carried out to demonstrate that the proposed economy-SRU based time series\nprediction model outperforms the MLP, LSTM and attention-gated CNN-based time\nseries models considered previously for inferring Granger causality.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 06:40:07 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 04:48:30 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Khanna", "Saurabh", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1911.09937", "submitter": "Jin Liu", "authors": "Jin Liu, Robert A. Perera, Le Kang, Roy T. Sabo, Robert M. Kirkpatrick", "title": "Hybridizing two-step growth mixture model and exploratory factor\n  analysis to examine heterogeneity in nonlinear trajectories", "comments": "Draft version 1.6, 08/08/2020. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical researchers are usually interested in investigating the impacts of\nbaseline covariates have when uncovering sample heterogeneity and separating\nsamples into more homogeneous groups. However, a considerable number of studies\nin the structural equation modeling (SEM) framework usually start with vague\nhypotheses in terms of heterogeneity and possible reasons. It suggests that (1)\nthe determination and specification of a proper model with covariates is not\nstraightforward, and (2) the exploration process may be computational intensive\ngiven that a model in the SEM framework is usually complicated and the pool of\ncandidate covariates is usually huge in the psychological and educational\ndomain where the SEM framework is widely employed. Following\n\\citet{Bakk2017two}, this article presents a two-step growth mixture model\n(GMM) that examines the relationship between latent classes of nonlinear\ntrajectories and baseline characteristics. Our simulation studies demonstrate\nthat the proposed model is capable of clustering the nonlinear change patterns,\nand estimating the parameters of interest unbiasedly, precisely, as well as\nexhibiting appropriate confidence interval coverage. Considering the pool of\ncandidate covariates is usually huge and highly correlated, this study also\nproposes implementing exploratory factor analysis (EFA) to reduce the dimension\nof covariate space. We illustrate how to use the hybrid method, the two-step\nGMM and EFA, to efficiently explore the heterogeneity of nonlinear trajectories\nof longitudinal mathematics achievement data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:17:47 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 22:54:51 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 02:44:23 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 22:20:27 GMT"}, {"version": "v5", "created": "Tue, 26 May 2020 13:32:35 GMT"}, {"version": "v6", "created": "Mon, 10 Aug 2020 01:31:41 GMT"}, {"version": "v7", "created": "Sun, 16 Aug 2020 19:07:45 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Jin", ""], ["Perera", "Robert A.", ""], ["Kang", "Le", ""], ["Sabo", "Roy T.", ""], ["Kirkpatrick", "Robert M.", ""]]}, {"id": "1911.09939", "submitter": "Jin Liu", "authors": "Jin Liu, Robert A. Perera, Le Kang, Robert M. Kirkpatrick, Roy T. Sabo", "title": "Obtaining interpretable parameters from reparameterizing longitudinal\n  models: transformation matrices between growth factors in two\n  parameter-spaces", "comments": "Draft version 1.6, 07/28/2020. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear spline growth model (LSGM), which approximates complex patterns\nusing at least two linear segments, is a popular tool for examining nonlinear\nchange patterns. Among such models, the linear-linear piecewise change pattern\nis the most straightforward one. An earlier study has proved that other than\nthe intercept and slopes, the knot (or change-point), at which two linear\nsegments join together, can be estimated as a growth factor in a\nreparameterized longitudinal model in the latent growth curve modeling\nframework. However, the reparameterized coefficients were no longer directly\nrelated to the underlying developmental process and therefore lacked\nmeaningful, substantive interpretation, although they were simple functions of\nthe original parameters. This study proposes transformation matrices between\nparameters in the original and reparameterized models so that the interpretable\ncoefficients directly related to the underlying change pattern can be derived\nfrom reparameterized ones. Additionally, the study extends the existing\nlinear-linear piecewise model to allow for individual measurement occasions,\nand investigates predictors for the individual-differences in change patterns.\nWe present the proposed methods with simulation studies and a real-world data\nanalysis. Our simulation studies demonstrate that the proposed method can\ngenerally provide an unbiased and consistent estimation of model parameters of\ninterest and confidence intervals with satisfactory coverage probabilities. An\nempirical example using longitudinal mathematics achievement scores shows that\nthe model can estimate the growth factor coefficients and path coefficients\ndirectly related to the underlying developmental process, thereby providing\nmeaningful interpretation. For easier implementation, we also provide the\ncorresponding code for the proposed models.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:20:53 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 22:52:48 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 02:42:07 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 22:23:47 GMT"}, {"version": "v5", "created": "Thu, 7 May 2020 01:01:53 GMT"}, {"version": "v6", "created": "Tue, 26 May 2020 13:30:05 GMT"}, {"version": "v7", "created": "Wed, 29 Jul 2020 01:58:27 GMT"}, {"version": "v8", "created": "Sun, 16 Aug 2020 19:05:52 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Jin", ""], ["Perera", "Robert A.", ""], ["Kang", "Le", ""], ["Kirkpatrick", "Robert M.", ""], ["Sabo", "Roy T.", ""]]}, {"id": "1911.09980", "submitter": "Jonathan Bartlett", "authors": "Jonathan W. Bartlett, Rachael A. Hughes", "title": "Bootstrap Inference for Multiple Imputation under Uncongeniality and\n  Misspecification", "comments": "Updated (fixed) reference based simulation results. Now included\n  tables which were previously not included as they were in supplementary\n  information document. Swapped order of the two simulation studies. Added\n  acknowledgement and funding statements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation has become one of the most popular approaches for\nhandling missing data in statistical analyses. Part of this success is due to\nRubin's simple combination rules. These give frequentist valid inferences when\nthe imputation and analysis procedures are so called congenial and the complete\ndata analysis is valid, but otherwise may not. Roughly speaking, congeniality\ncorresponds to whether the imputation and analysis models make different\nassumptions about the data. In practice imputation and analysis procedures are\noften not congenial, such that tests may not have the correct size and\nconfidence interval coverage deviates from the advertised level. We examine a\nnumber of recent proposals which combine bootstrapping with multiple\nimputation, and determine which are valid under uncongeniality and model\nmisspecification. Imputation followed by bootstrapping generally does not\nresult in valid variance estimates under uncongeniality or misspecification,\nwhereas bootstrapping followed by imputation does. We recommend a particular\ncomputationally efficient variant of bootstrapping followed by imputation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 11:37:26 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 16:52:51 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Bartlett", "Jonathan W.", ""], ["Hughes", "Rachael A.", ""]]}, {"id": "1911.10089", "submitter": "Michael Genin", "authors": "Mohamed-Salem Ahmed, Lionel Cucala, Michael Genin", "title": "Spatial Autoregressive Models for Scan Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial scan statistics are well-known methods for cluster detection and are\nwidely used in epidemiology and medical studies for detecting and evaluating\nthe statistical significance of disease hotspots. For the sake of simplicity,\nthe classical spatial scan statistic assumes that the observations of the\noutcome variable in different locations are independent, while in practice the\ndata may exhibit a spatial correlation. In this article, we use spatial\nautoregressive (SAR) models to account the spatial correlation in\nparametric/non-parametric scan statistic. Firstly, the correlation parameter is\nestimated in the SAR model to transform the outcome into a new independent\noutcome over all locations. Secondly, we propose an adapted spatial scan\nstatistic based on this independent outcome for cluster detection. A simulation\nstudy highlights the better performance of the proposed methods than the\nclassical one in presence of spatial correlation in the data. The latter shows\na sharp increase in Type I error and false-positive rate but also decreases the\ntrue-positive rate when spatial correlation increases. Besides, our methods\nretain the Type I error and have stable true and false positive rates with\nrespect to the spatial correlation. The proposed methods are illustrated using\na spatial economic dataset of the median income in Paris city. In this\napplication, we show that taking spatial correlation into account leads to the\nidentification of more concentrated clusters than those identified by the\nclassical spatial scan statistic.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 15:38:05 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Ahmed", "Mohamed-Salem", ""], ["Cucala", "Lionel", ""], ["Genin", "Michael", ""]]}, {"id": "1911.10117", "submitter": "Miguel Juarez Dr", "authors": "James Sharpe and Miguel A Juarez", "title": "Calibration of the Pareto and related distributions -a\n  reference-intrinsic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two Bayesian (Reference Intrinsic and Jeffreys prior) and two\nfrequentist (MLE and PWM) approaches to calibrating the Pareto and related\ndistributions. Three of these approaches are compared in a simulation study and\nall four to investigate how much equity risk capital banks subject to Basel II\nbanking regulations must hold. The Reference Intrinsic approach, which is\ninvariant under one-to-one transformations of the data and parameter, performs\nbetter when fitting a generalised Pareto distribution to data simulated from a\nPareto distribution and is competitive in the case study on equity capital\nrequirements\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:17:46 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Sharpe", "James", ""], ["Juarez", "Miguel A", ""]]}, {"id": "1911.10142", "submitter": "Bingxin Zhao", "authors": "Bingxin Zhao and Hongtu Zhu", "title": "Cross-trait prediction accuracy of high-dimensional ridge-type\n  estimators in genome-wide association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal association summary statistics have attracted great attention in\nstatistical genetics, mainly because the primary results of most genome-wide\nassociation studies (GWAS) are produced by marginal screening. In this paper,\nwe study the prediction accuracy of marginal estimator in dense (or sparsity\nfree) high-dimensional settings with $(n,p,m) \\to \\infty$, $m/n \\to \\gamma \\in\n(0,\\infty)$, and $p/n \\to \\omega \\in (0,\\infty)$. We consider a general\ncorrelation structure among the $p$ features and allow an unknown subset $m$ of\nthem to be signals. As the marginal estimator can be viewed as a ridge\nestimator with regularization parameter $\\lambda \\to \\infty$, we further\ninvestigate a class of ridge-type estimators in a unifying framework, including\nthe popular best linear unbiased prediction (BLUP) in genetics. We find that\nthe influence of $\\lambda$ on out-of-sample prediction accuracy heavily depends\non $\\omega$. Though selecting an optimal $\\lambda$ can be important when $p$\nand $n$ are comparable, it turns out that the out-of-sample $R^2$ of ridge-type\nestimators becomes near-optimal for any $\\lambda \\in (0,\\infty)$ as $\\omega$\nincreases. For example, when features are independent, the out-of-sample $R^2$\nis always bounded by $1/\\omega$ from above and is largely invariant to\n$\\lambda$ given large $\\omega$ (say, $\\omega>5$). We also find that in-sample\n$R^2$ has completely different patterns and depends much more on $\\lambda$ than\nout-of-sample $R^2$. In practice, our analysis delivers useful messages for\ngenome-wide polygenic risk prediction and computation-accuracy trade-off in\ndense high-dimensions. We numerically illustrate our results in simulation\nstudies and a real data example.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 17:05:07 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhao", "Bingxin", ""], ["Zhu", "Hongtu", ""]]}, {"id": "1911.10191", "submitter": "Sen Tian", "authors": "Sen Tian, Clifford M. Hurvich, Jeffrey S. Simonoff", "title": "On the Use of Information Criteria for Subset Selection in Least Squares\n  Regression", "comments": "Code to reproduce the results in this paper, the complete set of\n  simulation results, and an R package 'BOSSreg' (also available on CRAN), are\n  publicly available at https://github.com/sentian/BOSSreg", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least squares (LS)-based subset selection methods are popular in linear\nregression modeling. Best subset selection (BS) is known to be NP-hard and has\na computational cost that grows exponentially with the number of predictors.\nRecently, Bertsimas (2016) formulated BS as a mixed integer optimization (MIO)\nproblem and largely reduced the computation overhead by using a well-developed\noptimization solver, but the current methodology is not scalable to very large\ndatasets. In this paper, we propose a novel LS-based method, the best\northogonalized subset selection (BOSS) method, which performs BS upon an\northogonalized basis of ordered predictors and scales easily to large problem\nsizes. Another challenge in applying LS-based methods in practice is the\nselection rule to choose the optimal subset size k. Cross-validation (CV)\nrequires fitting a procedure multiple times, and results in a selected k that\nis random across repeated application to the same dataset. Compared to CV,\ninformation criteria only require fitting a procedure once, but they require\nknowledge of the effective degrees of freedom for the fitting procedure, which\nis generally not available analytically for complex methods. Since BOSS uses\northogonalized predictors, we first explore a connection for orthogonal\nnon-random predictors between BS and its Lagrangian formulation (i.e.,\nminimization of the residual sum of squares plus the product of a\nregularization parameter and k), and based on this connection propose a\nheuristic degrees of freedom (hdf) for BOSS that can be estimated via an\nanalytically-based expression. We show in both simulations and real data\nanalysis that BOSS using a proposed Kullback-Leibler based information\ncriterion AICc-hdf has the strongest performance of all of the LS-based methods\nconsidered and is competitive with regularization methods, with the\ncomputational effort of a single ordinary LS fit.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 18:54:43 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 05:00:42 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 03:46:41 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tian", "Sen", ""], ["Hurvich", "Clifford M.", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "1911.10246", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz and Oleksander Savenkov and Hooman Kamel", "title": "Non-parametric targeted Bayesian estimation of class proportions in\n  unlabeled data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel Bayesian estimator for the class proportion in an\nunlabeled dataset, based on the targeted learning framework. Our procedure\nrequires the specification of a prior (and outputs a posterior) only for the\ntarget of inference, instead of the prior (and posterior) on the full-data\ndistribution employed by classical non-parametric Bayesian methods .When the\nscientific question can be characterized by a low-dimensional parameter\nfunctional, focus on such a prior and posterior distributions is more aligned\nwith Bayesian subjectivism, compared to focus on entire data distributions. We\nprove a Bernstein-von Mises-type result for our proposed Bayesian procedure,\nwhich guarantees that the posterior distribution converges to the distribution\nof an efficient, asymptotically linear estimator. In particular, the posterior\nis Gaussian, doubly robust, and efficient in the limit, under the only\nassumption that certain nuisance parameters are estimated at slow rates. We\nperform numerical studies illustrating the frequentist properties of the\nmethod. We also illustrate their use in a motivating application to estimate\nthe proportion of embolic strokes of undetermined source arising from occult\ncardiac sources or large-artery atherosclerotic lesions. Though we focus on the\nmotivating example of the proportion of cases in an unlabeled dataset, the\nprocedure is general and can be adapted to estimate any pathwise differentiable\nparameter in a non-parametric model.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 20:54:29 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Savenkov", "Oleksander", ""], ["Kamel", "Hooman", ""]]}, {"id": "1911.10434", "submitter": "Yuedong Wang", "authors": "Danqing Xu and Yuedong Wang", "title": "Low Rank Approximation for Smoothing Spline via Eigensystem Truncation", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothing splines provide a powerful and flexible means for nonparametric\nestimation and inference. With a cubic time complexity, fitting smoothing\nspline models to large data is computationally prohibitive. In this paper, we\nuse the theoretical optimal eigenspace to derive a low rank approximation of\nthe smoothing spline estimates. We develop a method to approximate the\neigensystem when it is unknown and derive error bounds for the approximate\nestimates. The proposed methods are easy to implement with existing software.\nExtensive simulations show that the new methods are accurate, fast, and\ncompares favorably against existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 23:50:29 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 17:04:32 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Xu", "Danqing", ""], ["Wang", "Yuedong", ""]]}, {"id": "1911.10540", "submitter": "Haipeng Xing", "authors": "Haipeng Xing, Yingru Wu, Yong Chen, Michael Zhang", "title": "Deciphering hierarchical organization of topologically associated\n  domains through change-point testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The nucleus of eukaryotic cells spatially packages chromosomes\ninto a hierarchical and distinct segregation that plays critical roles in\nmaintaining transcription regulation. High-throughput methods of chromosome\nconformation capture, such as Hi-C, have revealed topologically associating\ndomains (TADs) that are defined by biased chromatin interactions within them.\n  Results: Here, we introduce a novel method, HiCKey, to decipher hierarchical\nTAD structures in Hi-C data and compare them across samples. We first derive a\ngeneralized likelihood-ratio (GLR) test for detecting change-points in an\ninteraction matrix that follows a negative binomial distribution or general\nmixture distribution. We then employ several optimal search strategies to\ndecipher hierarchical TADs with p-values calculated by the GLR test.\nLarge-scale validations of simulation data show that HiCKey has good precision\nin recalling known TADs and is robust against random collision noise of\nchromatin interactions. By applying HiCKey to Hi-C data of seven human cell\nlines, we identified multiple layers of TAD organization among them, but the\nvast majority had no more than four layers. In particular, we found that TAD\nboundaries are significantly enriched in active chromosomal regions compared to\nrepressed regions, indicating finer hierarchical architectures in active\nregions for precise gene transcription regulation.\n  Conclusions: HiCKey is optimized for processing large matrices constructed\nfrom high-resolution Hi-C experiments. The method and theoretical result of the\nGLR test provide a general framework for significance testing of similar\nexperimental chromatin interaction data that may not fully follow negative\nbinomial distributions but rather more general mixture distributions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 14:48:26 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 11:51:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xing", "Haipeng", ""], ["Wu", "Yingru", ""], ["Chen", "Yong", ""], ["Zhang", "Michael", ""]]}, {"id": "1911.10604", "submitter": "Rong Ma", "authors": "Rong Ma, T. Tony Cai and Hongzhe Li", "title": "Optimal Permutation Recovery in Permuted Monotone Matrix Model", "comments": null, "journal-ref": "Journal of the American Statistical Association, 2020", "doi": "10.1080/01621459.2020.1713794", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent research on quantifying bacterial growth dynamics based\non genome assemblies, we consider a permuted monotone matrix model\n$Y=\\Theta\\Pi+Z$, where the rows represent different samples, the columns\nrepresent contigs in genome assemblies and the elements represent log-read\ncounts after preprocessing steps and Guanine-Cytosine (GC) adjustment. In this\nmodel, $\\Theta$ is an unknown mean matrix with monotone entries for each row,\n$\\Pi$ is a permutation matrix that permutes the columns of $\\Theta$, and $Z$ is\na noise matrix. This paper studies the problem of estimation/recovery of $\\Pi$\ngiven the observed noisy matrix $Y$. We propose an estimator based on the best\nlinear projection, which is shown to be minimax rate-optimal for both exact\nrecovery, as measured by the 0-1 loss, and partial recovery, as quantified by\nthe normalized Kendall's tau distance. Simulation studies demonstrate the\nsuperior empirical performance of the proposed estimator over alternative\nmethods. We demonstrate the methods using a synthetic metagenomics dataset of\n45 closely related bacterial species and a real metagenomic dataset to compare\nthe bacterial growth dynamics between the responders and the non-responders of\nthe IBD patients after 8 weeks of treatment.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 20:36:42 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 21:30:45 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ma", "Rong", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1911.10633", "submitter": "Leonhard Held", "authors": "Leonhard Held", "title": "The harmonic mean $\\chi^2$ test to substantiate scientific findings", "comments": "Final version, to be published in JRSSC", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methodology plays a crucial role in drug regulation. Decisions by\nthe FDA or EMA are typically made based on multiple primary studies testing the\nsame medical product, where the two-trials rule is the standard requirement,\ndespite a number of shortcomings. A new approach is proposed for this task\nbased on the (weighted) harmonic mean of the squared study-specific test\nstatistics. Appropriate scaling ensures that, for any number of independent\nstudies, the null distribution is a $\\chi^2$-distribution with one degree of\nfreedom. This gives rise to a new method for combining one-sided $p$-values and\ncalculating confidence intervals for the overall treatment effect. Further\nproperties are discussed and a comparison with the two-trials rule is made, as\nwell as with alternative research synthesis methods. An attractive feature of\nthe new approach is that a claim of success requires each study to be\nconvincing on its own to a certain degree depending on the overall significance\nlevel and the number of studies. A real example with 5 clinical trials\ninvestigating the effect of Carvedilol for the treatment of patients with\nmoderate to severe heart failure patients is used to illustrate the\nmethodology.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 22:44:44 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 14:07:41 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 16:47:11 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Held", "Leonhard", ""]]}, {"id": "1911.10640", "submitter": "Aria Khademi", "authors": "Aria Khademi and Vasant Honavar", "title": "Algorithmic Bias in Recidivism Prediction: A Causal Perspective", "comments": "Accepted for publication at the Thirty Fourth AAAI conference on\n  Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ProPublica's analysis of recidivism predictions produced by Correctional\nOffender Management Profiling for Alternative Sanctions (COMPAS) software tool\nfor the task, has shown that the predictions were racially biased against\nAfrican American defendants. We analyze the COMPAS data using a causal\nreformulation of the underlying algorithmic fairness problem. Specifically, we\nassess whether COMPAS exhibits racial bias against African American defendants\nusing FACT, a recently introduced causality grounded measure of algorithmic\nfairness. We use the Neyman-Rubin potential outcomes framework for causal\ninference from observational data to estimate FACT from COMPAS data. Our\nanalysis offers strong evidence that COMPAS exhibits racial bias against\nAfrican American defendants. We further show that the FACT estimates from\nCOMPAS data are robust in the presence of unmeasured confounding.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 23:47:50 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Khademi", "Aria", ""], ["Honavar", "Vasant", ""]]}, {"id": "1911.10644", "submitter": "Edilberto Cepeda-Cuervo", "authors": "Mar\\'ia Victoria Cifuentes-Amado and Edilberto Cepeda-Cuervo", "title": "The Tilted Beta Binomial Linear Regression Model: a Bayesian Approach", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new linear regression models to deal with overdispersed\nbinomial datasets. These new models, called tilted beta binomial regression\nmodels, are defined from the tilted beta binomial distribution, proposed\nassuming that the parameter of the binomial distribution follows a tilted beta\ndistribution. As a particular case of this regression models, we propose the\nbeta rectangular binomial regression models, defined from the binomial\ndistribution assuming that their parameters follow a beta rectangular\ndistribution. These new linear regression models, defined assuming that the\nparameters of these new distributions follow regression structures, are fitted\napplying Bayesian methods and using the OpenBUGS software. The proposed\nregression models are fitted to an overdispersed binomial dataset of the number\nof seeds that germinate depending on the type of chosen seed androot.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 00:12:19 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Cifuentes-Amado", "Mar\u00eda Victoria", ""], ["Cepeda-Cuervo", "Edilberto", ""]]}, {"id": "1911.10682", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan", "title": "Analysis of odds, probability, and hazard ratios: From 2 by 2 tables to\n  two-sample survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of 2 by 2 tables and two-sample survival data has been widely used.\nExact calculation is computational intractable for conditional likelihood\ninference in odds ratio models with large marginals in 2 by 2 tables, or\npartial likelihood inference in Cox's proportional hazards models with\nconsiderable tied event times. Approximate methods are often employed, but\ntheir statistical properties have not been formally studied while taking into\naccount the approximation involved. We develop new methods and theory by\nconstructing suitable estimating functions while leveraging knowledge from\nconditional or partial likelihood inference. We propose a weighted\nMantel--Haenszel estimator in an odds ratio model such as Cox's discrete-time\nproportional hazards model. Moreover, we consider a probability ratio model,\nand derive as a consistent estimator the Breslow--Peto estimator, which has\nbeen regarded as an approximation to partial likelihood estimation in the odds\nratio model. We study both model-based and model-robust variance estimation.\nFor the Breslow--Peto estimator, our new model-based variance estimator is no\ngreater than the commonly reported variance estimator. We present numerical\nstudies which support the theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 03:22:17 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tan", "Zhiqiang", ""]]}, {"id": "1911.10800", "submitter": "Timothy Cannings", "authors": "Timothy I. Cannings", "title": "Random projections: data perturbation for classification problems", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projections offer an appealing and flexible approach to a wide range\nof large-scale statistical problems. They are particularly useful in\nhigh-dimensional settings, where we have many covariates recorded for each\nobservation. In classification problems there are two general techniques using\nrandom projections. The first involves many projections in an ensemble -- the\nidea here is to aggregate the results after applying different random\nprojections, with the aim of achieving superior statistical accuracy. The\nsecond class of methods include hashing and sketching techniques, which are\nstraightforward ways to reduce the complexity of a problem, perhaps therefore\nwith a huge computational saving, while approximately preserving the\nstatistical efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 10:13:56 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Cannings", "Timothy I.", ""]]}, {"id": "1911.10955", "submitter": "Bruno Ebner", "authors": "Philip D\\\"orr, Bruno Ebner, Norbert Henze", "title": "A new test of multivariate normality by a double estimation in a\n  characterizing PDE", "comments": "16 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with testing for nondegenerate normality of a $d$-variate\nrandom vector $X$ based on a random sample $X_1,\\ldots,X_n$ of $X$. The\nrationale of the test is that the characteristic function $\\psi(t) =\n\\exp(-\\|t\\|^2/2)$ of the standard normal distribution in $\\mathbb{R}^d$ is the\nonly solution of the partial differential equation $\\Delta f(t) =\n(\\|t\\|^2-d)f(t)$, $t \\in \\mathbb{R}^d$, subject to the condition $f(0) = 1$. By\ncontrast with a recent approach that bases a test for multivariate normality on\nthe difference $\\Delta \\psi_n(t)-(\\|t\\|^2-d)\\psi(t)$, where $\\psi_n(t)$ is the\nempirical characteristic function of suitably scaled residuals of\n$X_1,\\ldots,X_n$, we consider a weighted $L^2$-statistic that employs $\\Delta\n\\psi_n(t)-(\\|t\\|^2-d)\\psi_n(t)$. We derive asymptotic properties of the test\nunder the null hypothesis and alternatives. The test is affine invariant and\nconsistent against general alternatives, and it exhibits high power when\ncompared with prominent competitors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 14:56:59 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["D\u00f6rr", "Philip", ""], ["Ebner", "Bruno", ""], ["Henze", "Norbert", ""]]}, {"id": "1911.11043", "submitter": "Yunan Wu", "authors": "Yunan Wu and Lan Wang", "title": "Resampling-based Confidence Intervals for Model-free Robust Inference on\n  Optimal Treatment Regimes", "comments": "59 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new procedure for inference on optimal treatment regimes in the\nmodel-free setting, which does not require to specify an outcome regression\nmodel. Existing model-free estimators for optimal treatment regimes are usually\nnot suitable for the purpose of inference, because they either have nonstandard\nasymptotic distributions or do not necessarily guarantee consistent estimation\nof the parameter indexing the Bayes rule due to the use of surrogate loss. We\nfirst study a smoothed robust estimator that directly targets the parameter\ncorresponding to the Bayes decision rule for optimal treatment regimes\nestimation. This estimator is shown to have an asymptotic normal distribution.\nFurthermore, we verify that a resampling procedure provides asymptotically\naccurate inference for both the parameter indexing the optimal treatment regime\nand the optimal value function. A new algorithm is developed to calculate the\nproposed estimator with substantially improved speed and stability. Numerical\nresults demonstrate the satisfactory performance of the new methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 16:55:40 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 04:10:23 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Wu", "Yunan", ""], ["Wang", "Lan", ""]]}, {"id": "1911.11221", "submitter": "Justin Williams", "authors": "Justin R. Williams, Hyung-Woo Kim, Catherine M. Crespi", "title": "Modeling Variables with a Detection Limit using a Truncated Normal\n  Distribution with Censoring", "comments": null, "journal-ref": "BMC Medical Research Methodology 20 (2020)", "doi": "10.1186/s12874-020-01032-9", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data are collected subject to a detection limit, observations below the\ndetection limit may be considered censored. In addition, the domain of such\nobservations may be restricted; for example, values may be required to be\nnon-negative. We propose a regression method for censored observations that\nalso accounts for domain restriction. The method finds maximum likelihood\nestimates assuming an underlying truncated normal distribution. We show that\nour method, tcensReg, outperforms other methods commonly used for data with\ndetection limits such as Tobit regression and single imputation of the\ndetection limit or half detection limit with respect to bias and mean squared\nerror under a range of simulation settings. We apply our method to analyze\nvision quality data collected from ophthalmology clinical trials comparing\ndifferent types of intraocular lenses implanted during cataract surgery.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 20:36:04 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Williams", "Justin R.", ""], ["Kim", "Hyung-Woo", ""], ["Crespi", "Catherine M.", ""]]}, {"id": "1911.11345", "submitter": "Abhishek Chakrabortty", "authors": "Abhishek Chakrabortty, Jiarui Lu, T. Tony Cai and Hongzhe Li", "title": "High Dimensional M-Estimation with Missing Outcomes: A Semi-Parametric\n  Framework", "comments": "34 pages, 4 tables; (Supplement: 58 pages, 10 tables);", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high dimensional $M$-estimation in settings where the response\n$Y$ is possibly missing at random and the covariates $\\mathbf{X} \\in\n\\mathbb{R}^p$ can be high dimensional compared to the sample size $n$. The\nparameter of interest $\\boldsymbol{\\theta}_0 \\in \\mathbb{R}^d$ is defined as\nthe minimizer of the risk of a convex loss, under a fully non-parametric model,\nand $\\boldsymbol{\\theta}_0$ itself is high dimensional which is a key\ndistinction from existing works. Standard high dimensional regression and\nseries estimation with possibly misspecified models and missing $Y$ are\nincluded as special cases, as well as their counterparts in causal inference\nusing 'potential outcomes'.\n  Assuming $\\boldsymbol{\\theta}_0$ is $s$-sparse ($s \\ll n$), we propose an\n$L_1$-regularized debiased and doubly robust (DDR) estimator of\n$\\boldsymbol{\\theta}_0$ based on a high dimensional adaptation of the\ntraditional double robust (DR) estimator's construction. Under mild tail\nassumptions and arbitrarily chosen (working) models for the propensity score\n(PS) and the outcome regression (OR) estimators, satisfying only some\nhigh-level conditions, we establish finite sample performance bounds for the\nDDR estimator showing its (optimal) $L_2$ error rate to be $\\sqrt{s (\\log d)/\nn}$ when both models are correct, and its consistency and DR properties when\nonly one of them is correct. Further, when both the models are correct, we\npropose a desparsified version of our DDR estimator that satisfies an\nasymptotic linear expansion and facilitates inference on low dimensional\ncomponents of $\\boldsymbol{\\theta}_0$. Finally, we discuss various of choices\nof high dimensional parametric/semi-parametric working models for the PS and OR\nestimators. All results are validated via detailed simulations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 05:11:44 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Chakrabortty", "Abhishek", ""], ["Lu", "Jiarui", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1911.11463", "submitter": "Mark De Rooij", "authors": "Mark de Rooij, Bunga Citra Pratiwi, Marjolein Fokkema, Elise\n  Dusseldorp, Henk Kelderman", "title": "The Early Roots of Statistical Learning in the Psychometric Literature:\n  A review and two new results", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine and Statistical learning techniques become more and more important\nfor the analysis of psychological data. Four core concepts of machine learning\nare the bias variance trade-off, cross-validation, regularization, and basis\nexpansion. We present some early psychometric papers, from almost a century\nago, that dealt with cross-validation and regularization. From this review it\nis safe to conclude that the origins of these lie partly in the field of\npsychometrics. From our historical review, two new ideas arose which we\ninvestigated further: The first is about the relationship between reliability\nand predictive validity; the second is whether optimal regression weights\nshould be estimated by regularizing their values towards equality or shrinking\ntheir values towards zero. In a simulation study we show that the reliability\nof a test score does not influence the predictive validity as much as is\nusually written in psychometric textbooks. Using an empirical example we show\nthat regularization towards equal regression coefficients is beneficial in\nterms of prediction error.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:29:56 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["de Rooij", "Mark", ""], ["Pratiwi", "Bunga Citra", ""], ["Fokkema", "Marjolein", ""], ["Dusseldorp", "Elise", ""], ["Kelderman", "Henk", ""]]}, {"id": "1911.11467", "submitter": "Jorge Sicacha-Parada", "authors": "J. Sicacha-Parada, I. Steinsland, B. Cretois, J. Borgelt", "title": "Accounting for spatial varying sampling effort due to accessibility in\n  Citizen Science data: A case study of moose in Norway", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citizen Scientists together with an increasing access to technology provide\nlarge datasets that can be used to study e.g. ecology and biodiversity. Unknown\nand varying sampling effort is a major issue when making inference based on\ncitizen science data. In this paper we propose a modeling approach for\naccounting for variation in sampling effort due to accessibility. The paper is\nbased on a illustrative case study using citizen science data of moose\noccurrence in Hedmark, Norway. The aim is to make inference about the\nimportance of two geographical properties known to influence moose occurrence;\nterrain ruggedness index and solar radiation. Explanatory analysis show that\nmoose occurrences are overrepresented close to roads, and we use distance to\nroads as a proxy for accessibility. We propose a model based on a Bayesian\nLog-Gaussian Cox Process specification for occurrence. The model accounts for\naccessibility through a distance sampling approach. This approach can be seen\nas a thinning process where probability of thinning, i.e. not observing,\nincreases with increasing distances. For the moose case study distance to roads\nare used. Computationally efficient full Bayesian inference is performed using\nthe Integrated Nested Laplace Approximation and the Stochastic Partial\nDifferential Equation approach for spatial modeling. The proposed model as well\nas the consequences of not accounting for varying sampling effort due to\naccessibility are studied through a simulation study based on the case study.\nConsiderable biases are found in estimates for the effect of radiation on moose\noccurrence when accessibility is not considered in the model.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:38:21 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sicacha-Parada", "J.", ""], ["Steinsland", "I.", ""], ["Cretois", "B.", ""], ["Borgelt", "J.", ""]]}, {"id": "1911.11470", "submitter": "Claudio Durastanti Dr.", "authors": "Alessia Caponera, Claudio Durastanti, and Anna Vidotto", "title": "LASSO estimation for spherical autoregressive processes", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of the present paper is to investigate on a class of spherical\nfunctional autoregressive processes in order to introduce and study LASSO\n(Least Absolute Shrinkage and Selection Operator) type estimators for the\ncorresponding autoregressive kernels, defined in the harmonic domain by means\nof their spectral decompositions. Some crucial properties for these estimators\nare proved, in particular, consistency and oracle inequalities.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:44:37 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 07:55:01 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Caponera", "Alessia", ""], ["Durastanti", "Claudio", ""], ["Vidotto", "Anna", ""]]}, {"id": "1911.11476", "submitter": "Timothy Pollington MSc", "authors": "Timothy M. Pollington (1 and 3), Michael J. Tildesley (2), T.\n  D\\'eirdre Hollingsworth (3), Lloyd A. C. Chapman (4) ((1) MathSys CDT,\n  University of Warwick, UK, (2) Zeeman Institute (SBIDER), School of Life\n  Sciences and Mathematics Institute, University of Warwick, UK, (3) Big Data\n  Institute, Li Ka Shing Centre for Health Information and Discovery,\n  University of Oxford, UK, (4) London School of Hygiene & Tropical Medicine,\n  UK)", "title": "The spatiotemporal tau statistic: a review", "comments": "Corresponding author is Timothy M. Pollington. Equal contributions by\n  T. D\\'eirdre Hollingsworth and Lloyd A. C. Chapman. 42 pgs, 7866 words, 1\n  fig., 1 table. Right-to-reply date added, refs' DoI links shortened", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction\n  The tau statistic is a recent second-order correlation function that can\nassess the magnitude and range of global spatiotemporal clustering from\nepidemiological data containing geolocations of individual cases and, usually,\ndisease onset times. This is the first review of its use, and the aspects of\nits computation and presentation that could affect inferences drawn and bias\nestimates of the statistic.\n  Methods\n  Using Google Scholar we searched papers or preprints that cited the papers\nthat first defined/reformed the statistic. We tabulated their key\ncharacteristics to understand the statistic's development since 2012.\n  Results\n  Only half of the 16 studies found were considered to be using true tau\nstatistics, but their inclusion in the review still provided important insights\ninto their analysis motivations. All papers that used graphical hypothesis\ntesting and parameter estimation used incorrect methods. There is a lack of\nclarity over how to choose the time-relatedness interval to relate cases and\nthe distance band set, that are both required to calculate the statistic. Some\nstudies demonstrated nuanced applications of the tau statistic in settings with\nunusual data or time relation variables, which enriched understanding of its\npossibilities. A gap was noticed in the estimators available to account for\nvariable person-time at risk.\n  Discussion\n  Our review comprehensively covers current uses of the tau statistic for\ndescriptive analysis, graphical hypothesis testing, and parameter estimation of\nspatiotemporal clustering. We also define a new estimator of the tau statistic\nfor disease rates. For the tau statistic there are still open questions on its\nimplementation which we hope this review inspires others to research.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:53:20 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 13:21:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pollington", "Timothy M.", "", "1 and 3"], ["Tildesley", "Michael J.", ""], ["Hollingsworth", "T. D\u00e9irdre", ""], ["Chapman", "Lloyd A. C.", ""]]}, {"id": "1911.11488", "submitter": "Riccardo Rastelli", "authors": "Laleh Tafakori, Armin Pourkhanali, Riccardo Rastelli", "title": "Measuring systemic risk and contagion in the European financial network", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel framework to study default dependence and\nsystemic risk in a financial network that evolves over time. We analyse several\nindicators of risk, and develop a new latent space model to assess the health\nof key European banks before, during, and after the recent financial crises.\nFirst, we adopt the measure of CoRisk to determine the impact of such crises on\nthe financial network. Then, we use minimum spanning trees to analyse the\ncorrelation structure and the centrality of the various banks. Finally, we\npropose a new statistical model that permits a latent space visualisation of\nthe financial system. This provides a clear and interpretable model-based\nsummary of the interaction data, and it gives a new perspective on the topology\nstructure of the network. Crucially, the methodology provides a new approach to\nassess and understand the systemic risk associated with a financial system, and\nto study how debt may spread between institutions. Our dynamic framework\nprovides an interpretable map that illustrates the default dependencies between\ninstitutions, highlighting the possible patterns of contagion and the\ninstitutions that may pose systemic threats.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 12:15:45 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 14:38:21 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Tafakori", "Laleh", ""], ["Pourkhanali", "Armin", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "1911.11646", "submitter": "Bin Luo", "authors": "Bin Luo and Xiaoli Gao", "title": "A High-dimensional M-estimator Framework for Bi-level Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data analysis, bi-level sparsity is often assumed when\ncovariates function group-wisely and sparsity can appear either at the group\nlevel or within certain groups. In such case, an ideal model should be able to\nencourage the bi-level variable selection consistently. The bi-level variable\nselection has become even more challenging when data have heavy-tailed\ndistribution or outliers exist in random errors and covariates. In this paper,\nwe study a framework of high-dimensional M-estimation for bi-level variable\nselection. This framework encourages bi-level sparsity through a\ncomputationally efficient two-stage procedure. It produces strong robust\nparameter estimators if nonconvex redescending loss functions are applied. In\ntheory, we provide sufficient conditions under which our two-stage penalized\nM-estimator possesses simultaneous local estimation consistency and the\nbi-level variable selection consistency if certain nonconvex penalty functions\nare used at the group level. Both our simulation studies and real data analysis\ndemonstrate satisfactory finite sample performance of the proposed estimators\nunder different irregular settings.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 15:38:02 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Luo", "Bin", ""], ["Gao", "Xiaoli", ""]]}, {"id": "1911.11709", "submitter": "Ana Fernandez Vidal", "authors": "Ana F. Vidal, Valentin De Bortoli, Marcelo Pereyra and Alain Durmus", "title": "Maximum likelihood estimation of regularisation parameters in\n  high-dimensional inverse problems: an empirical Bayesian approach. Part I:\n  Methodology and Experiments", "comments": "37 pages - SIIMS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many imaging problems require solving an inverse problem that is\nill-conditioned or ill-posed. Imaging methods typically address this difficulty\nby regularising the estimation problem to make it well-posed. This often\nrequires setting the value of the so-called regularisation parameters that\ncontrol the amount of regularisation enforced. These parameters are notoriously\ndifficult to set a priori, and can have a dramatic impact on the recovered\nestimates. In this work, we propose a general empirical Bayesian method for\nsetting regularisation parameters in imaging problems that are convex w.r.t.\nthe unknown image. Our method calibrates regularisation parameters directly\nfrom the observed data by maximum marginal likelihood estimation, and can\nsimultaneously estimate multiple regularisation parameters. Furthermore, the\nproposed algorithm uses the same basic operators as proximal optimisation\nalgorithms, namely gradient and proximal operators, and it is therefore\nstraightforward to apply to problems that are currently solved by using\nproximal optimisation techniques. Our methodology is demonstrated with a range\nof experiments and comparisons with alternative approaches from the literature.\nThe considered experiments include image denoising, non-blind image\ndeconvolution, and hyperspectral unmixing, using synthesis and analysis priors\ninvolving the L1, total-variation, total-variation and L1, and\ntotal-generalised-variation pseudo-norms. A detailed theoretical analysis of\nthe proposed method is presented in the companion paper arXiv:2008.05793.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:31:00 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 09:17:14 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 10:44:18 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Vidal", "Ana F.", ""], ["De Bortoli", "Valentin", ""], ["Pereyra", "Marcelo", ""], ["Durmus", "Alain", ""]]}, {"id": "1911.11715", "submitter": "Tony Tohme", "authors": "Tony Tohme, Kevin Vanslette, Kamal Youcef-Toumi", "title": "A Generalized Bayesian Approach to Model Calibration", "comments": null, "journal-ref": "Reliability Engineering & System Safety, December 2020, Volume\n  204, 107141", "doi": "10.1016/j.ress.2020.107141", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model development, model calibration and validation play complementary\nroles toward learning reliable models. In this article, we expand the Bayesian\nValidation Metric framework to a general calibration and validation framework\nby inverting the validation mathematics into a generalized Bayesian method for\nmodel calibration and regression. We perform Bayesian regression based on a\nuser's definition of model-data agreement. This allows for model selection on\nany type of data distribution, unlike Bayesian and standard regression\ntechniques, that \"fail\" in some cases. We show that our tool is capable of\nrepresenting and combining least squares, likelihood-based, and Bayesian\ncalibration techniques in a single framework while being able to generalize\naspects of these methods. This tool also offers new insights into the\ninterpretation of the predictive envelopes (also known as confidence bands)\nwhile giving the analyst more control over these envelopes. We demonstrate the\nvalidity of our method by providing three numerical examples to calibrate\ndifferent models, including a model for energy dissipation in lap joints under\nimpact loading. By calibrating models with respect to the validation metrics\none desires a model to ultimately pass, reliability and safety metrics may be\nintegrated into and automatically adopted by the model in the calibration\nphase.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:39:58 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 03:59:51 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 22:34:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Tohme", "Tony", ""], ["Vanslette", "Kevin", ""], ["Youcef-Toumi", "Kamal", ""]]}, {"id": "1911.11774", "submitter": "Chencheng Cai", "authors": "Chencheng Cai and Rong Chen and Han Xiao", "title": "Matrix Completion using Kronecker Product Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matrix completion problem is to recover the missing entries in a partially\nobserved matrix. Most of the existing matrix completion methods assume a low\nrank structure of the underlying complete matrix. In this paper, we introduce\nan alternative and more general form of the underlying complete matrix, which\nassumes a low Kronecker rank instead of a low regular rank, but includes the\nlatter as a special case. The extra flexibility allows for a much more\nparsimonious representation of the underlying matrix, but also raises the\nchallenge of determining the proper Kronecker product configuration to be used.\nWe find that the configuration can be identified using the mean squared error\ncriterion as well as a modified cross-validation criterion. We establish the\nconsistency of this procedure under suitable conditions on the signal-to-noise\nratio. A aggregation procedure is also proposed to deal with special missing\npatterns and complex underlying structures. Both numerical and empirical\nstudies are carried out to demonstrate the performance of the new method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 16:48:31 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 03:04:00 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 18:18:31 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""], ["Xiao", "Han", ""]]}, {"id": "1911.11864", "submitter": "Paromita Dubey", "authors": "Paromita Dubey, Hans-Georg M\\\"uller", "title": "Fr\\'echet Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to infer the presence and location of change-points in\nthe distribution of a sequence of independent data taking values in a general\nmetric space, where change-points are viewed as locations at which the\ndistribution of the data sequence changes abruptly in terms of either its\nFr\\'echet mean or Fr\\'echet variance or both. The proposed method is based on\ncomparisons of Fr\\'echet variances before and after putative change-point\nlocations and does not require a tuning parameter except for the specification\nof cut-off intervals near the endpoints where change-points are assumed not to\noccur. Our results include theoretical guarantees for consistency of the test\nunder contiguous alternatives when a change-point exists and also for\nconsistency of the estimated location of the change-point if it exists, where\nunder the null hypothesis of no change-point the limit distribution of the\nproposed scan function is the square of a standardized Brownian Bridge. These\nconsistency results are applicable for a broad class of metric spaces under\nmild entropy conditions. Examples include the space of univariate probability\ndistributions and the space of graph Laplacians for networks. Simulation\nstudies demonstrate the effectiveness of the proposed methods, both for\ninferring the presence of a change-point and estimating its location. We also\ndevelop theory that justifies bootstrap-based inference and illustrate the new\napproach with sequences of maternal fertility distributions and communication\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 22:20:39 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 03:47:40 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Dubey", "Paromita", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1911.11890", "submitter": "Soosan Beheshti", "authors": "Soosan Beheshti, Edward Nidoy, and Faizan Rahman", "title": "K-MACE and Kernel K-MACE Clustering", "comments": "13 pages, 4 Tables, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the correct number of clusters (CNC) is an important task in data\nclustering and has a critical effect on finalizing the partitioning results.\nK-means is one of the popular methods of clustering that requires CNC. Validity\nindex methods use an additional optimization procedure to estimate the CNC for\nK-means. We propose an alternative validity index approach denoted by\nk-minimizing Average Central Error (KMACE). K-means is one of the popular\nmethods of clustering that requires CNC. Validity ACE is the average error\nbetween the true unavailable cluster center and the estimated cluster center\nfor each sample data. Kernel K-MACE is kernel K-means that is equipped with an\nefficient CNC estimator. In addition, kernel K_MACE includes the first\nautomatically tuned procedure for choosing the Gaussian kernel parameters.\nSimulation results for both synthetic and read data show superiority of K_MACE\nand kernel K-MACE over the conventional clustering methods not only in CNC\nestimation but also in the partitioning procedure.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 00:14:05 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Beheshti", "Soosan", ""], ["Nidoy", "Edward", ""], ["Rahman", "Faizan", ""]]}, {"id": "1911.11922", "submitter": "Anton Alyakin A", "authors": "Anton Alyakin, Yichen Qin, Carey E. Priebe", "title": "LqRT: Robust Hypothesis Testing of Location Parameters using\n  Lq-Likelihood-Ratio-Type Test in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A t-test is considered a standard procedure for inference on population means\nand is widely used in scientific discovery. However, as a special case of a\nlikelihood-ratio test, t-test often shows drastic performance degradation due\nto the deviations from its hard-to-verify distributional assumptions.\nAlternatively, in this article, we propose a new two-sample\nLq-likelihood-ratio-type test (LqRT) along with an easy-to-use Python package\nfor implementation. LqRT preserves high power when the distributional\nassumption is violated, and maintains the satisfactory performance when the\nassumption is valid. As numerical studies suggest, LqRT dominates many other\nrobust tests in power, such as Wilcoxon test and sign test, while maintaining a\nvalid size. To the extent that the robustness of the Wilcoxon test (minimum\nasymptotic relative efficiency (ARE) of the Wilcoxon test vs the t-test is\n0.864) suggests that the Wilcoxon test should be the default test of choice\n(rather than \"use Wilcoxon if there is evidence of non-normality\", the default\nposition should be \"use Wilcoxon unless there is good reason to believe the\nnormality assumption\"), the results in this article suggest that the LqRT is\npotentially the new default go-to test for practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 02:37:11 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Alyakin", "Anton", ""], ["Qin", "Yichen", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1911.11993", "submitter": "Jun Lu", "authors": "Lu Lin and Jun Lu", "title": "A race-DC in Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strategy of divide-and-combine (DC) has been widely used in the area of\nbig data. Bias-correction is crucial in the DC procedure for validly\naggregating the locally biased estimators, especial for the case when the\nnumber of batches of data is large. This paper establishes a race-DC through a\nresidual-adjustment composition estimate (race). The race-DC applies to various\ntypes of biased estimators, which include but are not limited to Lasso\nestimator, Ridge estimator and principal component estimator in linear\nregression, and least squares estimator in nonlinear regression. The resulting\nglobal estimator is strictly unbiased under linear model, and is acceleratingly\nbias-reduced in nonlinear model, and can achieve the theoretical optimality,\nfor the case when the number of batches of data is large. Moreover, the race-DC\nis computationally simple because it is a least squares estimator in a pro\nforma linear regression. Detailed simulation studies demonstrate that the\nresulting global estimator is significantly bias-corrected, and the behavior is\ncomparable with the oracle estimation and is much better than the competitors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:24:58 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Lin", "Lu", ""], ["Lu", "Jun", ""]]}, {"id": "1911.12160", "submitter": "Abhik Ghosh PhD", "authors": "Tuhin Majumder, Ayanendranath Basu, Abhik Ghosh", "title": "On Robust Pseudo-Bayes Estimation for the Independent Non-homogeneous\n  Set-up", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ordinary Bayes estimator based on the posterior density suffers from the\npotential problems of non-robustness under data contamination or outliers. In\nthis paper, we consider the general set-up of independent but non-homogeneous\n(INH) observations and study a robustified pseudo-posterior based estimation\nfor such parametric INH models. In particular, we focus on the\n$R^{(\\alpha)}$-posterior developed by Ghosh and Basu (2016) for IID data and\nlater extended by Ghosh and Basu (2017) for INH set-up, where its usefulness\nand desirable properties have been numerically illustrated. In this paper, we\ninvestigate the detailed theoretical properties of this robust pseudo Bayes\n$R^{(\\alpha)}$-posterior and associated $R^{(\\alpha)}$-Bayes estimate under the\ngeneral INH set-up with applications to fixed-design regressions. We derive a\nBernstein von-Mises types asymptotic normality results and Laplace type\nasymptotic expansion of the $R^{(\\alpha)}$-posterior as well as the asymptotic\ndistributions of the expected $R^{(\\alpha)}$-posterior estimators. The required\nconditions and the asymptotic results are simplified for linear regressions\nwith known or unknown error variance and logistic regression models with fixed\ncovariates. The robustness of the $R^{(\\alpha)}$-posterior and associated\nestimators are theoretically examined through appropriate influence function\nanalyses under general INH set-up; illustrations are provided for the case of\nlinear regression. A high breakdown point result is derived for the expected\n$R^{(\\alpha)}$-posterior estimators of the location parameter under a\nlocation-scale type model. Some interesting real life data examples illustrate\npossible applications.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:05:40 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Majumder", "Tuhin", ""], ["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""]]}, {"id": "1911.12185", "submitter": "Bret Zeldow", "authors": "Bret Zeldow and Laura A. Hatfield", "title": "Confounding and Regression Adjustment in Difference-in-Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difference-in-differences (diff-in-diff) is a study design that compares\noutcomes of two groups (treated and comparison) at two time points (pre- and\npost-treatment) and is widely used in evaluating new policy implementations.\nFor instance, diff-in-diff has been used to estimate the effect that increasing\nminimum wage has on employment rates and to assess the Affordable Care Act's\neffect on health outcomes. Although diff-in-diff appears simple, potential\npitfalls lurk. In this paper, we discuss one such complication: time-varying\nconfounding. We provide rigorous definitions for confounders in diff-in-diff\nstudies and explore regression strategies to adjust for confounding. In\nsimulations, we show how and when regression adjustment can ameliorate\nconfounding for both time-invariant and time-varying covariates. We compare our\nregression approach to those models commonly fit in applied literature, which\noften fail to address the time-varying nature of confounding in diff-in-diff.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:39:26 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Zeldow", "Bret", ""], ["Hatfield", "Laura A.", ""]]}, {"id": "1911.12198", "submitter": "Florencia Leonardi", "authors": "Florencia Leonardi, Rodrigo R.S. Carvalho and Iara M. Frondana", "title": "Strong structure recovery for partially observed discrete Markov random\n  fields on graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized pseudo-likelihood criterion to estimate the graph of\nconditional dependencies in a discrete Markov random field that can be\npartially observed. We prove the convergence of the estimator in the case of a\nfinite or countable infinite set of variables. In the finite case the\nunderlying graph can be recovered with probability one, while in the countable\ninfinite case we can recover any finite sub-graph with probability one, by\nallowing the candidate neighborhoods to grow with the sample size n and\nprovided the penalizing constant is sufficiently large. Our method requires\nminimal assumptions on the probability distribution and contrary to other\napproaches in the literature, the usual positivity condition is not needed. We\nevaluate the performance of the estimator on simulated data and we apply the\nmethodology to a real dataset of stock index markets in different countries.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:56:51 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 12:09:04 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 19:29:58 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Leonardi", "Florencia", ""], ["Carvalho", "Rodrigo R. S.", ""], ["Frondana", "Iara M.", ""]]}, {"id": "1911.12204", "submitter": "Olayid\\'e Boussari", "authors": "Olayid\\'e Boussari, Laurent Bordes, Ga\\\"elle Romain, Marc Colonna,\n  Nadine Bossard, Laurent Remontet, and Val\\'erie Jooste", "title": "Modeling excess hazard with time--to--cure as a parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cure models have been widely developed to estimate the cure fraction when\nsome subjects never experience the event of interest. However these models were\nrarely focused on the estimation of the time-to-cure i.e. the delay elapsed\nbetween the diagnosis and \"the time from which cure is reached\", an important\nindicator, for instance to address the question of access to insurance or loans\nfor subjects with personal history of cancer. We propose a new excess hazard\nregression model that includes the time-to-cure as a covariate dependent\nparameter to be estimated. The model is written similarly to a Beta probability\ndistribution function and is shown to be a particular case of the non-mixture\ncure models. Parameters are estimated through a maximum likelihood approach and\nsimulation studies demonstrate good performance of the model. Illustrative\napplications to two cancer data sets are provided and some limitations as well\nas possible extensions of the model are discussed. The proposed model offers a\nsimple and comprehensive way to estimate more accurately the time-to-cure. Key\nwords: Cancer; Cure model; Cure time; Net survival; Right to be forgotten.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:01:17 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Boussari", "Olayid\u00e9", ""], ["Bordes", "Laurent", ""], ["Romain", "Ga\u00eblle", ""], ["Colonna", "Marc", ""], ["Bossard", "Nadine", ""], ["Remontet", "Laurent", ""], ["Jooste", "Val\u00e9rie", ""]]}, {"id": "1911.12284", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "The bivariate $K$-finite normal mixture \"blanket\" copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist many bivariate parametric copulas to model bivariate data with\ndifferent dependence features. We propose a new bivariate parametric copula\nfamily that cannot only handle various dependence patterns that appear in the\nexisting parametric bivariate copula families, but also provides a more\nenriched dependence structure. The proposed copula construction exploits finite\nmixtures of bivariate normal distributions. The mixing operation, the distinct\ncorrelation and mean parameters at each mixture component introduce quite a\nflexible dependence. The new parametric copula is theoretically investigated,\ncompared with a set of classical bivariate parametric copulas and illustrated\non two empirical examples from astrophysics and agriculture where some of the\nvariables have peculiar and asymmetric dependence, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 16:54:18 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 06:14:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1911.12295", "submitter": "Raanju Ragavendar Sundararajan", "authors": "Raanju Ragavendar Sundararajan, Ron D. Frostig, Hernando Ombao", "title": "Modeling Spectral Properties in Stationary Processes of Varying\n  Dimensions with Applications to Brain Local Field Potential Signals", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common class of methods for analyzing of multivariate time series,\nstationary and nonstationary, decomposes the observed series into latent\nsources. Methods such as principal compoment analysis (PCA), independent\ncomponent analysis (ICA) and Stationary Subspace Analysis (SSA) assume the\nobserved multivariate process is generated by latent sources that are\nstationary or nonstationary. We develop a method that tracks changes in the\ncomplexity of a 32-channel local field potential (LFP) signal from a rat\nfollowing an experimentally induced stroke. We study complexity through the\nlatent sources and their dimensions that can change across epochs due to an\ninduced shock to the cortical system. Our method compares the spread of\nspectral information in several multivariate stationary processes with\ndifferent dimensions. A frequency specific spectral ratio (FS-ratio) statistic\nis proposed and its asymptotic properties are derived. The FS-ratio is blind to\nthe dimension of the stationary process and captures the proportion of spectral\ninformation in various (user-specified) frequency bands. We apply our method to\nstudy differences in complexity and structure of the LFP before and after\nsystem shock. The analysis indicates that spectral information in the beta\nfrequency band (12-30 Hertz) demonstrated the greatest change in structure and\ncomplexity due to the stroke.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 17:18:28 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 15:12:17 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Sundararajan", "Raanju Ragavendar", ""], ["Frostig", "Ron D.", ""], ["Ombao", "Hernando", ""]]}, {"id": "1911.12321", "submitter": "Ali Eshragh", "authors": "Ali Eshragh, Fred Roosta, Asef Nazari, Michael W. Mahoney", "title": "LSAR: Efficient Leverage Score Sampling Algorithm for the Analysis of\n  Big Time Series Data", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply methods from randomized numerical linear algebra (RandNLA) to\ndevelop improved algorithms for the analysis of large-scale time series data.\nWe first develop a new fast algorithm to estimate the leverage scores of an\nautoregressive (AR) model in big data regimes. We show that the accuracy of\napproximations lies within $(1+\\mathcal{O}(\\varepsilon))$ of the true leverage\nscores with high probability. These theoretical results are subsequently\nexploited to develop an efficient algorithm, called LSAR, for fitting an\nappropriate AR model to big time series data. Our proposed algorithm is\nguaranteed, with high probability, to find the maximum likelihood estimates of\nthe parameters of the underlying true AR model and has a worst case running\ntime that significantly improves those of the state-of-the-art alternatives in\nbig data regimes. Empirical results on large-scale synthetic as well as real\ndata highly support the theoretical results and reveal the efficacy of this new\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 17:57:12 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 21:36:05 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Eshragh", "Ali", ""], ["Roosta", "Fred", ""], ["Nazari", "Asef", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1911.12426", "submitter": "Adam Sandler", "authors": "Adam Sandler, Diego Klabjan, Yuan Luo", "title": "Conditional Hierarchical Bayesian Tucker Decomposition", "comments": "20 pages, added model evaluation and log-likelihood sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research focuses on studying and developing methods for reducing the\ndimensionality of large datasets, common in biomedical applications. A major\nproblem when learning information about patients based on genetic sequencing\ndata is that there are often more feature variables (genetic data) than\nobservations (patients). This makes direct supervised learning difficult. One\nway of reducing the feature space is to use latent Dirichlet allocation in\norder to group genetic variants in an unsupervised manner. Latent Dirichlet\nallocation is a common model in natural language processing, which describes a\ndocument as a mixture of topics, each with a probability of generating certain\nwords. This can be generalized as a Bayesian tensor decomposition to account\nfor multiple feature variables. While we made some progress improving and\nmodifying these methods, our significant contributions are with hierarchical\ntopic modeling. We developed distinct methods of incorporating hierarchical\ntopic modeling, based on nested Chinese restaurant processes and Pachinko\nAllocation Machine, into Bayesian tensor decompositions. We apply these models\nto predict whether or not patients have autism spectrum disorder based on\ngenetic sequencing data. We examine a dataset from National Database for Autism\nResearch consisting of paired siblings -- one with autism, and the other\nwithout -- and counts of their genetic variants. Additionally, we linked the\ngenes with their Reactome biological pathways. We combine this information into\na tensor of patients, counts of their genetic variants, and the membership of\nthese genes in pathways. Once we decompose this tensor, we use logistic\nregression on the reduced features in order to predict if patients have autism.\nWe also perform a similar analysis of a dataset of patients with one of four\ncommon types of cancer (breast, lung, prostate, and colorectal).\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 21:22:04 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 04:18:25 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Sandler", "Adam", ""], ["Klabjan", "Diego", ""], ["Luo", "Yuan", ""]]}, {"id": "1911.12430", "submitter": "Shuhan Tang", "authors": "Shuhan Tang, Shu Yang, Tongrong Wang, Zhanglin Cui, Li Li, and Douglas\n  E. Faries", "title": "Causal inference of hazard ratio based on propensity score matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score matching is commonly used to draw causal inference from\nobservational survival data. However, there is no gold standard approach to\nanalyze survival data after propensity score matching, and variance estimation\nafter matching is open to debate. We derive the statistical properties of the\npropensity score matching estimator of the marginal causal hazard ratio based\non matching with replacement and a fixed number of matches. We also propose a\ndouble-resampling technique for variance estimation that takes into account the\nuncertainty due to propensity score estimation prior to matching.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 21:34:48 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 19:42:28 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 14:31:49 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Tang", "Shuhan", ""], ["Yang", "Shu", ""], ["Wang", "Tongrong", ""], ["Cui", "Zhanglin", ""], ["Li", "Li", ""], ["Faries", "Douglas E.", ""]]}, {"id": "1911.12445", "submitter": "Jonas Moss", "authors": "Jonas Moss, Riccardo De Bin", "title": "Modelling publication bias and p-hacking", "comments": "21 pager, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Publication bias and p-hacking are two well-known phenomena that strongly\naffect the scientific literature and cause severe problems in meta-analyses.\nDue to these phenomena, the assumptions of meta-analyses are seriously violated\nand the results of the studies cannot be trusted. While publication bias is\nalmost perfectly captured by the weighting function selection model, p-hacking\nis much harder to model and no definitive solution has been found yet. In this\npaper we propose to model both publication bias and p-hacking with selection\nmodels. We derive some properties for these models, and we compare them\nformally and through simulations. Finally, two real data examples are used to\nshow how the models work in practice.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 22:09:35 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 20:10:11 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Moss", "Jonas", ""], ["De Bin", "Riccardo", ""]]}, {"id": "1911.12474", "submitter": "Mouloud Belbahri", "authors": "Mouloud Belbahri, Alejandro Murua, Olivier Gandouet, Vahid Partovi Nia", "title": "Qini-based Uplift Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uplift models provide a solution to the problem of isolating the marketing\neffect of a campaign. For customer churn reduction, uplift models are used to\nidentify the customers who are likely to respond positively to a retention\nactivity only if targeted, and to avoid wasting resources on customers that are\nvery likely to switch to another company. We introduce a Qini-based uplift\nregression model to analyze a large insurance company's retention marketing\ncampaign. Our approach is based on logistic regression models. We show that a\nQini-optimized uplift model acts as a regularizing factor for uplift, much as a\npenalized likelihood model does for regression. This results in interpretable\nparsimonious models with few relevant xplanatory variables. Our results show\nthat performing Qini-based parameters estimation significantly improves the\nuplift models performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 01:09:42 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 18:29:18 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Belbahri", "Mouloud", ""], ["Murua", "Alejandro", ""], ["Gandouet", "Olivier", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "1911.12516", "submitter": "Rong Ma", "authors": "Rong Ma, T. Tony Cai and Hongzhe Li", "title": "Optimal Estimation of Bacterial Growth Rates Based on Permuted Monotone\n  Matrix", "comments": null, "journal-ref": "Biometrika (2020)", "doi": "10.1093/biomet/asaa082", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of estimating the bacterial growth rates for genome\nassemblies from shotgun metagenomic data, we consider the permuted monotone\nmatrix model $Y=\\Theta\\Pi+Z$, where $Y\\in \\mathbb{R}^{n\\times p}$ is observed,\n$\\Theta\\in \\mathbb{R}^{n\\times p}$ is an unknown approximately rank-one signal\nmatrix with monotone rows, $\\Pi \\in \\mathbb{R}^{p\\times p}$ is an unknown\npermutation matrix, and $Z\\in \\mathbb{R}^{n\\times p}$ is the noise matrix. This\npaper studies the estimation of the extreme values associated to the signal\nmatrix $\\Theta$, including its first and last columns, as well as their\ndifference. Treating these estimation problems as compound decision problems,\nminimax rate-optimal estimators are constructed using the spectral column\nsorting method. Numerical experiments through simulated and synthetic\nmicrobiome metagenomic data are presented, showing the superiority of the\nproposed methods over the alternatives. The methods are illustrated by\ncomparing the growth rates of gut bacteria between inflammatory bowel disease\npatients and normal controls.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 03:51:42 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 17:49:17 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ma", "Rong", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1911.12624", "submitter": "Clemence Leyrat", "authors": "Clemence Leyrat, James R Carpenter, Sebastien Bailly, Elizabeth J\n  Willamson", "title": "A review and evaluation of standard methods to handle missing data on\n  time-varying confounders in marginal structural models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models (MSMs) are commonly used to estimate causal\nintervention effects in longitudinal non-randomised studies. A common issue\nwhen analysing data from observational studies is the presence of incomplete\nconfounder data, which might lead to bias in the intervention effect estimates\nif they are not handled properly in the statistical analysis. However, there is\ncurrently no recommendation on how to address missing data on covariates in\nMSMs under a variety of missingness mechanisms encountered in practice. We\nreviewed existing methods to handling missing data in MSMs and performed a\nsimulation study to compare the performance of complete case (CC) analysis, the\nlast observation carried forward (LOCF), the missingness pattern approach\n(MPA), multiple imputation (MI) and inverse-probability-of-missingness\nweighting (IPMW). We considered three mechanisms for non-monotone missing data\nwhich are common in observational studies using electronic health record data.\nWhereas CC analysis lead to biased estimates of the intervention effect in\nalmost all scenarios, the performance of the other approaches varied across\nscenarios. The LOCF approach led to unbiased estimates only under a specific\nnon-random mechanism in which confounder values were missing when their values\nremained unchanged since the previous measurement. In this scenario, MI, the\nMPA and IPMW were biased. MI and IPMW led to the estimation of unbiased effects\nwhen data were missing at random, given the covariates or the treatment but\nonly MI was unbiased when the outcome was a predictor of missingness.\nFurthermore, IPMW generally lead to very large standard errors. Lastly,\nregardless of the missingness mechanism, the MPA led to unbiased estimates only\nwhen the failure to record a confounder at a given time-point modified the\nsubsequent relationships between the partially observed covariate and the\noutcome.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 10:21:52 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Leyrat", "Clemence", ""], ["Carpenter", "James R", ""], ["Bailly", "Sebastien", ""], ["Willamson", "Elizabeth J", ""]]}, {"id": "1911.12724", "submitter": "Dimitar Ninevski", "authors": "Dimitar Ninevski and Paul O'Leary", "title": "Detection of Derivative Discontinuities in Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to the detection of discontinuities in the\nn-th derivative of observational data. This is achieved by performing two\npolynomial approximations at each interstitial point. The polynomials are\ncoupled by constraining their coefficients to ensure continuity of the model up\nto the (n-1)-th derivative; while yielding an estimate for the discontinuity of\nthe n-th derivative. The coefficients of the polynomials correspond directly to\nthe derivatives of the approximations at the interstitial points through the\nprudent selection of a common coordinate system. The approximation residual and\nextrapolation errors are investigated as measures for detecting discontinuity.\nThis is necessary since discrete observations of continuous systems are\ndiscontinuous at every point. It is proven, using matrix algebra, that positive\nextrema in the combined approximation-extrapolation error correspond exactly to\nextrema in the difference of the Taylor coefficients. This provides a relative\nmeasure for the severity of the discontinuity in the observational data. The\nmatrix algebraic derivations are provided for all aspects of the methods\npresented here; this includes a solution for the covariance propagation through\nthe computation. The performance of the method is verified with a Monte Carlo\nsimulation using synthetic piecewise polynomial data with known\ndiscontinuities. It is also demonstrated that the discontinuities are suitable\nas knots for B-spline modelling of data. For completeness, the results of\napplying the method to sensor data acquired during the monitoring of heavy\nmachinery are presented.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 14:29:18 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ninevski", "Dimitar", ""], ["O'Leary", "Paul", ""]]}, {"id": "1911.12882", "submitter": "Daniel Xu", "authors": "Daniel Xu, Pamela Shaw, and Ian Barnett", "title": "Novel Non-Negative Variance Estimator for (Modified) Within-Cluster\n  Resampling", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel variance estimator for within-cluster\nresampling (WCR) and modified within-cluster resampling (MWCR) - two existing\nmethods for analyzing longitudinal data. WCR is a simple but computationally\nintensive method, in which a single observation is randomly sampled from each\ncluster to form a new dataset. This process is repeated numerous times, and in\neach resampled dataset (or outputation), we calculate beta using a generalized\nlinear model. The final resulting estimator is an average across estimates from\nall outputations. MWCR is an extension of WCR that can account for the\nwithin-cluster correlation of the dataset; consequently, there are two\nnoteworthy differences: 1) in MWCR, each resampled dataset is formed by\nrandomly sampling multiple observations without replacement from each cluster\nand 2) generalized estimating equations (GEEs) are used to estimate the\nparameter of interest. While WCR and MWCR are relatively simple to implement, a\nkey challenge is that the proposed moment-based estimator is often times\nnegative in practice. Our modified variance estimator is not only strictly\npositive, but simulations show that it preserves the type I error and allows\nstatistical power gains associated with MWCR to be realized.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 22:27:53 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Xu", "Daniel", ""], ["Shaw", "Pamela", ""], ["Barnett", "Ian", ""]]}, {"id": "1911.13122", "submitter": "Solenne Gaucher", "authors": "Solenne Gaucher (LMO), Olga Klopp (CREST), Genevi\\`eve Robin (ENPC,\n  MATHERIALS)", "title": "Outliers Detection in Networks with Missing Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outliers arise in networks due to different reasons such as fraudulent\nbehavior of malicious users or default in measurement instruments and can\nsignificantly impair network analyses. In addition, real-life networks are\nlikely to be incompletely observed, with missing links due to individual\nnon-response or machine failures. Identifying outliers in the presence of\nmissing links is therefore a crucial problem in network analysis. In this work,\nwe introduce a new algorithm to detect outliers in a network that\nsimultaneously predicts the missing links. The proposed method is statistically\nsound: we prove that, under fairly general assumptions, our algorithm exactly\ndetects the outliers, and achieves the best known error for the prediction of\nmissing links with polynomial computation cost. It is also computationally\nefficient: we prove sub-linear convergence of our algorithm. We provide a\nsimulation study which demonstrates the good behavior of the algorithm in terms\nof outliers detection and prediction of the missing links. We also illustrate\nthe method with an application in epidemiology, and with the analysis of a\npolitical Twitter network. The method is freely available as an R package on\nthe Comprehensive R Archive Network.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:33:00 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 14:19:55 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Gaucher", "Solenne", "", "LMO"], ["Klopp", "Olga", "", "CREST"], ["Robin", "Genevi\u00e8ve", "", "ENPC,\n  MATHERIALS"]]}, {"id": "1911.13136", "submitter": "Hector Rodriguez-Deniz", "authors": "Hector Rodriguez-Deniz, Mattias Villani and Augusto Voltes-Dorta", "title": "A Multilayered Block Network Model to Forecast Large Dynamic\n  Transportation Graphs: an Application to US Air Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic transportation networks have been analyzed for years by means of\nstatic graph-based indicators in order to study the temporal evolution of\nrelevant network components, and to reveal complex dependencies that would not\nbe easily detected by a direct inspection of the data. This paper presents a\nstate-of-the-art latent network model to forecast multilayer dynamic graphs\nthat are increasingly common in transportation and proposes a community-based\nextension to reduce the computational burden. Flexible time series analysis is\nobtained by modeling the probability of edges between vertices through latent\nGaussian processes. The models and Bayesian inference are illustrated on a\nsample of 10-year data from four major airlines within the US air\ntransportation system. Results show how the estimated latent parameters from\nthe models are related to the airline's connectivity dynamics, and their\nability to project the multilayer graph into the future for out-of-sample full\nnetwork forecasts, while stochastic blockmodeling allows for the identification\nof relevant communities. Reliable network predictions would allow policy-makers\nto better understand the dynamics of the transport system, and help in their\nplanning on e.g. route development, or the deployment of new regulations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:03:05 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 07:07:09 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 12:44:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rodriguez-Deniz", "Hector", ""], ["Villani", "Mattias", ""], ["Voltes-Dorta", "Augusto", ""]]}, {"id": "1911.13142", "submitter": "Mohammad Ghorbani Dr.", "authors": "Ottmar Cronie, Mohammad Ghorbani, Jorge Mateu and Jun Yu", "title": "Functional marked point processes -- A natural structure to unify\n  spatio-temporal frameworks and to analyse dependent functional data", "comments": "44 pages, 3 figures with 9 plots", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper treats functional marked point processes (FMPPs), which are\ndefined as marked point processes where the marks are random elements in some\n(Polish) function space. Such marks may represent e.g. spatial paths or\nfunctions of time. To be able to consider e.g. multivariate FMPPs, we also\nattach an additional, Euclidean, mark to each point. We indicate how FMPPs\nquite naturally connect the point process framework with both the functional\ndata analysis framework and the geostatistical framework. We further show that\nvarious existing models fit well into the FMPP framework. In addition, we\nintroduce a new family of summary statistics, weighted marked reduced moment\nmeasures, together with their non-parametric estimators, in order to study\nfeatures of the functional marks. We further show how they generalise other\nsummary statistics and we finally apply these tools to analyse population\nstructures, such as demographic evolution and sex ratio over time, in Spanish\nprovinces.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:10:50 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Cronie", "Ottmar", ""], ["Ghorbani", "Mohammad", ""], ["Mateu", "Jorge", ""], ["Yu", "Jun", ""]]}, {"id": "1911.13272", "submitter": "Christian Hennig", "authors": "Christian Hennig", "title": "Minkowski distances and standardisation for clustering and\n  classification of high dimensional data", "comments": "Preliminary version; final version to be published by Springer, using\n  Springer's svmult LATEX style", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many distance-based methods for classification and clustering, and\nfor data with a high number of dimensions and a lower number of observations,\nprocessing distances is computationally advantageous compared to the raw data\nmatrix. Euclidean distances are used as a default for continuous multivariate\ndata, but there are alternatives. Here the so-called Minkowski distances, $L_1$\n(city block)-, $L_2$ (Euclidean)-, $L_3$-, $L_4$-, and maximum distances are\ncombined with different schemes of standardisation of the variables before\naggregating them. Boxplot transformation is proposed, a new transformation\nmethod for a single variable that standardises the majority of observations but\nbrings outliers closer to the main bulk of the data. Distances are compared in\nsimulations for clustering by partitioning around medoids, complete and average\nlinkage, and classification by nearest neighbours, of data with a low number of\nobservations but high dimensionality. The $L_1$-distance and the boxplot\ntransformation show good results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:19:03 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 23:29:09 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Hennig", "Christian", ""]]}]