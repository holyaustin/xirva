[{"id": "1407.0044", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Frank Wood", "title": "Infinite Structured Hidden Semi-Markov Models", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent advances in Bayesian nonparametric techniques for\nconstructing and performing inference in infinite hidden Markov models. We\nfocus on variants of Bayesian nonparametric hidden Markov models that enhance a\nposteriori state-persistence in particular. This paper also introduces a new\nBayesian nonparametric framework for generating left-to-right and other\nstructured, explicit-duration infinite hidden Markov models that we call the\ninfinite structured hidden semi-Markov model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 20:18:18 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Wood", "Frank", ""]]}, {"id": "1407.0050", "submitter": "Barbara Engelhardt", "authors": "David Mimno and David M Blei and Barbara E Engelhardt", "title": "Posterior predictive checks to quantify lack-of-fit in admixture models\n  of latent population structure", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1412301112", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Admixture models are a ubiquitous approach to capture latent population\nstructure in genetic samples. Despite the widespread application of admixture\nmodels, little thought has been devoted to the quality of the model fit or the\naccuracy of the estimates of parameters of interest for a particular study.\nHere we develop methods for validating admixture models based on posterior\npredictive checks (PPCs), a Bayesian method for assessing the quality of a\nstatistical model. We develop PPCs for five population-level statistics of\ninterest: within-population genetic variation, background linkage\ndisequilibrium, number of ancestral populations, between-population genetic\nvariation, and the downstream use of admixture parameters to correct for\npopulation structure in association studies. Using PPCs, we evaluate the\nquality of the model estimates for four qualitatively different population\ngenetic data sets: the POPRES European individuals, the HapMap phase 3\nindividuals, continental Indians, and African American individuals. We found\nthat the same model fitted to different genomic studies resulted in highly\nstudy-specific results when evaluated using PPCs, illustrating the utility of\nPPCs for model-based analyses in large genomic studies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 20:36:54 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Mimno", "David", ""], ["Blei", "David M", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1407.0064", "submitter": "James Sweeney Dr.", "authors": "James Sweeney, John Haslett and Andrew C. Parnell", "title": "The zero & $N$-inflated binomial distribution with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the distribution arising when two zero-inflated\nPoisson count processes are constrained by their sum total, resulting in a\nnovel zero & $N$-inflated binomial distribution. This result motivates a\ngeneral class of model for applications in which a sum-constrained count\nresponse is subject to multiple sources of heterogeneity, principally an excess\nof zeroes and $N$'s in the underlying count generating process. Two examples\nfrom the ecological regression literature are used to illustrate the wide\napplicability of the proposed model, and serve to detail its substantial\nsuperiority in modelling performance as compared to competing models. We also\npresent an extension to the modelling framework for more complex cases,\nconsidering a gender study dataset which is overdispersed relative to the new\nlikelihood, and conclude the article with the description of a general\nframework for a zero & $N$-inflated multinomial distribution.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 21:33:11 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 09:18:55 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 18:45:04 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2016 05:13:43 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Sweeney", "James", ""], ["Haslett", "John", ""], ["Parnell", "Andrew C.", ""]]}, {"id": "1407.0185", "submitter": "Lilun Du", "authors": "Lilun Du, Chunming Zhang", "title": "Single-index modulated multiple testing", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1222 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1262-1311", "doi": "10.1214/14-AOS1222", "report-no": "IMS-AOS-AOS1222", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of large-scale multiple testing, hypotheses are often\naccompanied with certain prior information. In this paper, we present a\nsingle-index modulated (SIM) multiple testing procedure, which maintains\ncontrol of the false discovery rate while incorporating prior information, by\nassuming the availability of a bivariate $p$-value, $(p_1,p_2)$, for each\nhypothesis, where $p_1$ is a preliminary $p$-value from prior information and\n$p_2$ is the primary $p$-value for the ultimate analysis. To find the optimal\nrejection region for the bivariate $p$-value, we propose a criteria based on\nthe ratio of probability density functions of $(p_1,p_2)$ under the true null\nand nonnull. This criteria in the bivariate normal setting further motivates us\nto project the bivariate $p$-value to a single-index, $p(\\theta)$, for a wide\nrange of directions $\\theta$. The true null distribution of $p(\\theta)$ is\nestimated via parametric and nonparametric approaches, leading to two\nprocedures for estimating and controlling the false discovery rate. To derive\nthe optimal projection direction $\\theta$, we propose a new approach based on\npower comparison, which is further shown to be consistent under some mild\nconditions. Simulation evaluations indicate that the SIM multiple testing\nprocedure improves the detection power significantly while controlling the\nfalse discovery rate. Analysis of a real dataset will be illustrated.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 10:57:15 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Du", "Lilun", ""], ["Zhang", "Chunming", ""]]}, {"id": "1407.0230", "submitter": "Nathan Uyttendaele", "authors": "Nathan Uyttendaele", "title": "Nested Archimedean copulas: a new class of nonparametric tree structure\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any nested Archimedean copula is defined starting from a rooted phylogenetic\ntree, for which a new class of nonparametric estimators is presented. An\nestimator from this new class relies on a two-step procedure where first a\nbinary tree is built and second is collapsed if necessary to give an estimate\nof the target tree structure. Several examples of estimators from this class\nare given and the performance of each of these estimators, as well as of the\nonly known comparable estimator, is assessed by means of a simulation study\ninvolving target structures in various dimensions, showing that the new\nestimators, besides being faster, usually offer better performance as well.\nFurther, among the given examples of estimators from the new class, one of the\nbest performing one is applied on three datasets: 482 students and their\nresults to various examens, 26 European countries in 1979 and the percentage of\nworkers employed in different economic activities, and 104 countries in 2002\nfor which various health-related variables are available. The resulting\nestimated trees offer valuable insights on the analyzed data. The future of\nnested Archimedean copulas in general is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 13:29:05 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Uyttendaele", "Nathan", ""]]}, {"id": "1407.0316", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama, Felipe Llinares L\\'opez, Niklas Kasenburg, Karsten M.\n  Borgwardt", "title": "Significant Subgraph Mining with Multiple Testing Correction", "comments": "18 pages, 5 figure, accepted to the 2015 SIAM International\n  Conference on Data Mining (SDM15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding itemsets that are statistically significantly enriched\nin a class of transactions is complicated by the need to correct for multiple\nhypothesis testing. Pruning untestable hypotheses was recently proposed as a\nstrategy for this task of significant itemset mining. It was shown to lead to\ngreater statistical power, the discovery of more truly significant itemsets,\nthan the standard Bonferroni correction on real-world datasets. An open\nquestion, however, is whether this strategy of excluding untestable hypotheses\nalso leads to greater statistical power in subgraph mining, in which the number\nof hypotheses is much larger than in itemset mining. Here we answer this\nquestion by an empirical investigation on eight popular graph benchmark\ndatasets. We propose a new efficient search strategy, which always returns the\nsame solution as the state-of-the-art approach and is approximately two orders\nof magnitude faster. Moreover, we exploit the dependence between subgraphs by\nconsidering the effective number of tests and thereby further increase the\nstatistical power.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 16:53:51 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2014 13:39:21 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 16:11:17 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Sugiyama", "Mahito", ""], ["L\u00f3pez", "Felipe Llinares", ""], ["Kasenburg", "Niklas", ""], ["Borgwardt", "Karsten M.", ""]]}, {"id": "1407.0873", "submitter": "Denis Belomestny", "authors": "Denis Belomestny and John Schoenmakers", "title": "Statistical Skorohod embedding problem and its generalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a L\\'evy process $L$, we consider the so-called statistical Skorohod\nembedding problem of recovering the distribution of an independent random time\n$T$ based on i.i.d. sample from $L_{T}.$ Our approach is based on the genuine\nuse of the Mellin and Laplace transforms. We propose a consistent estimator for\nthe density of $T,$ derive its convergence rates and prove their optimality. It\nturns out that the convergence rates heavily depend on the decay of the Mellin\ntransform of $T.$ We also consider the application of our results to the\nproblem of statistical inference for variance-mean mixture models and for\ntime-changed L\\'evy processes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 11:40:34 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Belomestny", "Denis", ""], ["Schoenmakers", "John", ""]]}, {"id": "1407.0886", "submitter": "Tobias Michael Erhardt", "authors": "Tobias Michael Erhardt, Claudia Czado and Ulf Schepsmeier", "title": "Spatial composite likelihood inference using local C-vines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a vine copula based composite likelihood approach to model spatial\ndependencies, which allows to perform prediction at arbitrary locations. This\napproach combines established methods to model (spatial) dependencies. On the\none hand the geostatistical concept utilizing spatial differences between the\nvariable locations to model the extend of spatial dependencies is applied. On\nthe other hand the flexible class of C-vine copulas is utilized to model the\nspatial dependency structure locally. These local C-vine copulas are\nparametrized jointly, exploiting an existing relationship between the copula\nparameters and the respective spatial distances and elevation differences, and\nare combined in a composite likelihood approach. The new methodology called\nspatial local C-vine composite likelihood (S-LCVCL) method benefits from the\nfact that it is able to capture non-Gaussian dependency structures. The\ndevelopment and validation of the new methodology is illustrated using a data\nset of daily mean temperatures observed at 73 observation stations spread over\nGermany. For validation continuous ranked probability scores are utilized.\nComparison with two other approaches of spatial dependency modeling introduced\nin yet unpublished work of Erhardt, Czado and Schepsmeier (2014) shows a\npreference for the local C-vine composite likelihood approach.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 12:26:51 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Erhardt", "Tobias Michael", ""], ["Czado", "Claudia", ""], ["Schepsmeier", "Ulf", ""]]}, {"id": "1407.1070", "submitter": "{\\O}ystein S{\\o}rensen", "authors": "{\\O}ystein S{\\o}rensen, Arnoldo Frigessi, Magne Thoresen", "title": "Covariate Selection in High-Dimensional Generalized Linear Models With\n  Measurement Error", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2018.1425626", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems involving generalized linear models, the covariates are\nsubject to measurement error. When the number of covariates p exceeds the\nsample size n, regularized methods like the lasso or Dantzig selector are\nrequired. Several recent papers have studied methods which correct for\nmeasurement error in the lasso or Dantzig selector for linear models in the p>n\nsetting. We study a correction for generalized linear models based on Rosenbaum\nand Tsybakov's matrix uncertainty selector. By not requiring an estimate of the\nmeasurement error covariance matrix, this generalized matrix uncertainty\nselector has a great practical advantage in problems involving high-dimensional\ndata. We further derive an alternative method based on the lasso, and develop\nefficient algorithms for both methods. In our simulation studies of logistic\nand Poisson regression with measurement error, the proposed methods outperform\nthe standard lasso and Dantzig selector with respect to covariate selection, by\nreducing the number of false positives considerably. We also consider\nclassification of patients on the basis of gene expression data with noisy\nmeasurements.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 21:43:15 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["S\u00f8rensen", "\u00d8ystein", ""], ["Frigessi", "Arnoldo", ""], ["Thoresen", "Magne", ""]]}, {"id": "1407.1079", "submitter": "Daniell Toth Ph.D.", "authors": "Daniell Toth and John Eltinge", "title": "Building Consistent Regression Trees From Complex Sample Data", "comments": null, "journal-ref": "Daniell Toth and John L. Eltinge (2011): Building Consistent\n  Regression Trees From Complex Sample Data, Journal of the American\n  Statistical Association, 106:496, 1626-1636", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In the past several years a wide range of methods for the construction of\nregression trees and other estimators based on the recursive partitioning of\nsamples have appeared in the statistics literature. Many applications involve\ndata collected through a complex sample design. At present, however, relatively\nlittle is known regarding the properties of these methods under complex\ndesigns. This article proposes a method for incorporating information about the\ncomplex sample design when building a regression tree using a recursive\npartitioning algorithm. Sufficient conditions are established for asymptotic\ndesign L 2 consistency of these regression trees as estimators for an arbitrary\nregression function. The proposed method is illustrated with Occupational\nEmployment Statistics establishment survey data linked to Quarterly Census of\nEmployment and Wage payroll data of the Bureau of Labor Statistics. Performance\nof the nonparametric estimator is investigated through a simulation study based\non this example.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 22:26:39 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Toth", "Daniell", ""], ["Eltinge", "John", ""]]}, {"id": "1407.1158", "submitter": "Sanvesh Srivastava", "authors": "Sanvesh Srivastava, Barbara E. Engelhardt, and David B. Dunson", "title": "Expandable Factor Analysis", "comments": "28 pages, 4 figures", "journal-ref": "Biometrika. vol. 104. number 3. pp. 649-663. 2017", "doi": "10.1093/biomet/asx030", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian sparse factor models have proven useful for characterizing\ndependence in multivariate data, but scaling computation to large numbers of\nsamples and dimensions is problematic. We propose expandable factor analysis\nfor scalable inference in factor models when the number of factors is unknown.\nThe method relies on a continuous shrinkage prior for efficient maximum a\nposteriori estimation of a low-rank and sparse loadings matrix. The structure\nof the prior leads to an estimation algorithm that accommodates uncertainty in\nthe number of factors. We propose an information criterion to select the\nhyperparameters of the prior. Expandable factor analysis has better false\ndiscovery rates and true positive rates than its competitors across diverse\nsimulations. We apply the proposed approach to a gene expression study of aging\nin mice, illustrating superior results relative to four competing methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 09:09:55 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 10:59:13 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 03:07:43 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Srivastava", "Sanvesh", ""], ["Engelhardt", "Barbara E.", ""], ["Dunson", "David B.", ""]]}, {"id": "1407.1181", "submitter": "Reza Hosseini", "authors": "Reza Hosseini, Akimichi Takemura, Kiros Berhane", "title": "A framework for fitting sparse data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a framework for fitting functions with domains in the\nEuclidean space, when data are sparse but a slow variation allows for a useful\nfit. We measure the variation by Lipschitz Bound (LB). Functions which admit\nsmaller LB are considered to vary more slowly. Since most functions in practice\nare wiggly and do not admit a small LB, we extend this framework by\napproximating a wiggly function, f, by ones which admit a smaller LB and do not\ndeviate from f by more than a specified Bound Deviation (BD). In fact for any\npositive LB, one can find such a BD, thus defining a trade-off function (LB-BD\nfunction) between the variation measure (LB) and the deviation measure (BD). We\nshow that the LB-BD function satisfies nice properties: it is non-increasing\nand convex. We also present a method to obtain it using convex optimization.\nFor a function with given LB and BD, we find the optimal fit and present\ndeterministic bounds for the prediction error of various methods. Given the\nLB-BD function, we discuss picking an appropriate LB-BD pair for fitting and\ncalculating the prediction errors. The developed methods can naturally\naccommodate an extra assumption of periodicity to obtain better prediction\nerrors. Finally we present the application of this framework to air pollution\ndata with sparse observations over time.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 10:37:55 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Hosseini", "Reza", ""], ["Takemura", "Akimichi", ""], ["Berhane", "Kiros", ""]]}, {"id": "1407.1347", "submitter": "Gael Martin Prof", "authors": "Gael M. Martin, Kanchana Nadarajah and D.S. Poskitt", "title": "Issues in the Estimation of Mis-Specified Models of Fractionally\n  Integrated Processes", "comments": "This is an extensive revision of an earlier paper with the same name", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a comprehensive set of new results on the impact of mis-specifying\nthe short run dynamics in fractionally integrated processes. We show that four\nalternative parametric estimators - frequency domain maximum likelihood,\nWhittle, time domain maximum likelihood and conditional sum of squares -\nconverge to the same pseudo-true value under common mis-specification, and that\nthey possess a common asymptotic distribution. The results are derived assuming\na completely general parametric specification for the short run dynamics of the\nestimated (mis-specified) fractional model, and with long memory, short memory\nand antipersistence in both the model and the true data generating process\naccommodated. As well as providing new theoretical insights, we undertake an\nextensive set of numerical explorations, beginning with the numerical\nevaluation, and implementation, of the (common) asymptotic distribution that\nholds under the most extreme form of mis-specification. Simulation experiments\nare then conducted to assess the relative finite sample performance of all four\nmis-specified estimators, initially under the assumption of a known mean, as\naccords with the theoretical derivations. The importance of the known mean\nassumption is illustrated via the production of an alternative set of bias and\nmean squared error results, in which the estimators are applied to demeaned\ndata. The paper concludes with a discussion of open problems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jul 2014 00:07:49 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 23:19:52 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Martin", "Gael M.", ""], ["Nadarajah", "Kanchana", ""], ["Poskitt", "D. S.", ""]]}, {"id": "1407.1624", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic, Jean-Fran\\c{c}ois Quessy and Tom Rohmer", "title": "Testing the constancy of Spearman's rho in multivariate time series", "comments": "42 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of tests for change-point detection designed to be particularly\nsensitive to changes in the cross-sectional rank correlation of multivariate\ntime series is proposed. The derived procedures are based on several\nmultivariate extensions of Spearman's rho. Two approaches to carry out the\ntests are studied: the first one is based on resampling, the second one\nconsists of estimating the asymptotic null distribution. The asymptotic\nvalidity of both techniques is proved under the null for strongly mixing\nobservations. A procedure for estimating a key bandwidth parameter involved in\nboth approaches is proposed, making the derived tests parameter-free. Their\nfinite-sample behavior is investigated through Monte Carlo experiments.\nPractical recommendations are made and an illustration on trivariate financial\ndata is finally presented.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 08:42:46 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 18:43:39 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Quessy", "Jean-Fran\u00e7ois", ""], ["Rohmer", "Tom", ""]]}, {"id": "1407.1682", "submitter": "Klaus Holst K", "authors": "Klaus K. Holst and Thomas H. Scheike and Jacob B. Hjelmborg", "title": "The Liability Threshold Model for Censored Twin Data", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.01.014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Family studies provide an important tool for understanding etiology of\ndiseases, with the key aim of discovering evidence of family aggregation and to\ndetermine if such aggregation can be attributed to genetic components.\nHeritability and concordance estimates are routinely calculated in twin studies\nof diseases, as a way of quantifying such genetic contribution. The endpoint in\nthese studies are typically defined as occurrence of a disease versus death\nwithout the disease. However, a large fraction of the subjects may still be\nalive at the time of follow-up without having experienced the disease thus\nstill being at risk. Ignoring this right-censoring can lead to severely biased\nestimates. We propose to extend the classical liability threshold model with\ninverse probability of censoring weighting of complete observations. This leads\nto a flexible way of modeling twin concordance and obtaining consistent\nestimates of heritability. We apply the method in simulations and to data from\nthe population based Danish twin cohort where we describe the dependence in\nprostate cancer occurrence in twins.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 12:24:31 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 14:09:06 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Holst", "Klaus K.", ""], ["Scheike", "Thomas H.", ""], ["Hjelmborg", "Jacob B.", ""]]}, {"id": "1407.1751", "submitter": "Marco Enea", "authors": "Marco Enea and Gianfranco Lovison", "title": "A penalized approach to the bivariate logistic regression model for the\n  association between ordinal responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bivariate ordered logistic models (BOLMs) are appealing to jointly model the\nmarginal distribution of two ordered responses and their association, given a\nset of covariates. When the number of categories of the responses increases,\nthe number of global odds ratios (or their re-parametrizations) to be estimated\nalso increases and estimating the association structure becomes crucial for\nthis type of data. In fact, such data could be too \"rich\" to be fully modelled\nwith an ordinary BOLM while, sometimes, the well-known Dale's model could be\ntoo parsimonious to provide a good fit. In addition, when the cross-tabulation\nof the responses contains some zeros, for a number of model configurations,\nincluding the bivariate version of the partial proportional odds model (PPOM),\nestimation of a BOLM by the Fisher-scoring algorithm may either fail or\nestimate a too \"irregular\" association structure. In this work, we propose to\nuse a nonparametric approach for the maximum likelihood estimation of a BOLM.\nWe apply penalties to the differences between adjacent row and column effects.\nAs a result, estimation is less demanding than an ordinary BOLM, permitting the\nfit of PPOMs and/or the smoothing of the marginal and association parameters by\npolynomial curves and surfaces, with scores chosen by the data. Model selection\nis based on the penalized log-likelihood ratio, whose limiting distribution has\nbeen studied through simulations, and AIC. Our proposal is compared to the\nGoodman's model and the Dale's model, in terms of goodness-of-fit and\nparsimony, on a literature data set. Finally, an application on an original\ndata set of liver disease patients is proposed.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 15:52:29 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Enea", "Marco", ""], ["Lovison", "Gianfranco", ""]]}, {"id": "1407.1778", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh", "title": "Robust Estimation of Bivariate Tail Dependence Coefficient", "comments": "Pre-Print, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the coefficient of bivariate tail dependence is\nconsidered here from the robustness point of view; it combines two apparently\ncontradictory theories of robust statistics and extreme value statistics. The\nusual maximum likelihood based or the moment type estimators of tail dependence\ncoefficient are highly sensitive to the presence of outlying observations in\ndata. This paper proposes some alternative robust estimators obtained by\nminimizing the density power divergence with suitable model assumptions; their\nrobustness properties are examined through the classical influence function\nanalysis. The performance of the proposed estimators is illustrated through an\nextensive empirical study considering several important bivariate extreme value\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 17:20:51 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Ghosh", "Abhik", ""]]}, {"id": "1407.1991", "submitter": "Mohamed Chaouch", "authors": "Mohamed Chaouch, Naamane Lai\u007fb and Djamal Louani", "title": "Rate of uniform consistency for a class of mode regression on functional\n  stationary ergodic data. Application to electricity consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to study the asymptotic properties of a class of\nkernel conditional mode estimates whenever functional stationary ergodic data\nare considered. To be more precise on the matter, in the ergodic data setting,\nwe consider a random element $(X, Z)$ taking values in some semi-metric\nabstract space $E\\times F$. For a real function $\\varphi$ defined on the space\n$F$ and $x\\in E$, we consider the conditional mode of the real random variable\n$\\varphi(Z)$ given the event $``X=x\"$. While estimating the conditional mode\nfunction, say $\\theta_\\varphi(x)$, using the well-known kernel estimator, we\nestablish the strong consistency with rate of this estimate uniformly over\nVapnik-Chervonenkis classes of functions $\\varphi$. Notice that the ergodic\nsetting offers a more general framework than the usual mixing structure. Two\napplications to energy data are provided to illustrate some examples of the\nproposed approach in time series forecasting framework. The first one consists\nin forecasting the {\\it daily peak} of electricity demand in France (measured\nin Giga-Watt). Whereas the second one deals with the short-term forecasting of\nthe electrical {\\it energy} (measured in Giga-Watt per Hour) that may be\nconsumed over some time intervals that cover the peak demand.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 08:35:31 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Chaouch", "Mohamed", ""], ["Lai\u007fb", "Naamane", ""], ["Louani", "Djamal", ""]]}, {"id": "1407.2165", "submitter": "Nirian Mart\\'in", "authors": "\\'Angel Felipe, Nirian Mart\\'in, Pedro Miranda, Leandro Pardo", "title": "Phi-Divergence test statistics for testing the validity of latent class\n  models for binary data", "comments": "arXiv admin note: text overlap with arXiv:1406.0109", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to present new families of test statistics\nfor studying the problem of goodness-of-fit of some data to a latent class\nmodel for binary data. The families of test statistics introduced are based on\nphi-divergence measures, a natural extension of maximum likelihood. We also\ntreat the problem of testing a nested sequence of latent class models for\nbinary data. For these statistics, we obtain their asymptotic distribution. We\nshall consider consistent estimators introduced in Felipe et al (2014) for\nsolving the problem of estimation. Finally, a simulation study is carried out\nin order to compare the efficiency, in the sense of the level and the power, of\nthe new statistics considered in this paper for sample sizes that are not big\nenough to apply the asymptotical results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 16:46:29 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Felipe", "\u00c1ngel", ""], ["Mart\u00edn", "Nirian", ""], ["Miranda", "Pedro", ""], ["Pardo", "Leandro", ""]]}, {"id": "1407.2192", "submitter": "Debasish Roy", "authors": "Mamatha Venugopal, Ram Mohan Vasu and Debasish Roy", "title": "An ensemble Kushner-Stratonovich-Poisson filter for recursive estimation\n  in nonlinear dynamical systems", "comments": "12 pages, 16 figures, submitted to a refereed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the numerous applications that may be expeditiously modelled by\ncounting processes, stochastic filtering strategies involving Poisson-type\nobservations still remain somewhat poorly developed. In this work, we propose a\nMonte Carlo stochastic filter for recursive estimation in the context of\nlinear/nonlinear dynamical systems with Poisson-type measurements. A key aspect\nof the present development is the filter-update scheme, derived from an\nensemble approximation of the time-discretized nonlinear filtering equation,\nmodified to account for Poisson-type measurements. Specifically, the additive\nupdate through a gain-like correction term, empirically approximated from the\ninnovation integral in the filtering equation, eliminates the problem of\nparticle collapse encountered in many conventional particle filters. Through a\nfew numerical demonstrations, the versatility of the proposed filter is brought\nforth, first with application to filtering problems with diffusive or\nPoisson-type measurements and then to an automatic control problem wherein the\nextremization of the associated cost functional is achieved simply by an\nappropriate redefinition of the innovation process.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 18:15:50 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Venugopal", "Mamatha", ""], ["Vasu", "Ram Mohan", ""], ["Roy", "Debasish", ""]]}, {"id": "1407.2219", "submitter": "Catia  Scricciolo", "authors": "Catia Scricciolo", "title": "Bayesian adaptation", "comments": "20 pages, Propositions 3 and 5 added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the need for low assumption inferential methods in infinite-dimensional\nsettings, Bayesian adaptive estimation via a prior distribution that does not\ndepend on the regularity of the function to be estimated nor on the sample size\nis valuable. We elucidate relationships among the main approaches followed to\ndesign priors for minimax-optimal rate-adaptive estimation meanwhile shedding\nlight on the underlying ideas.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 19:08:58 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 13:43:08 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Scricciolo", "Catia", ""]]}, {"id": "1407.2235", "submitter": "Ryan Adams", "authors": "Barbara E. Engelhardt, Ryan P. Adams", "title": "Bayesian Structured Sparsity from Gaussian Fields", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial research on structured sparsity has contributed to analysis of\nmany different applications. However, there have been few Bayesian procedures\namong this work. Here, we develop a Bayesian model for structured sparsity that\nuses a Gaussian process (GP) to share parameters of the sparsity-inducing prior\nin proportion to feature similarity as defined by an arbitrary positive\ndefinite kernel. For linear regression, this sparsity-inducing prior on\nregression coefficients is a relaxation of the canonical spike-and-slab prior\nthat flattens the mixture model into a scale mixture of normals. This prior\nretains the explicit posterior probability on inclusion parameters---now with\nGP probit prior distributions---but enables tractable computation via\nelliptical slice sampling for the latent Gaussian field. We motivate\ndevelopment of this prior using the genomic application of association mapping,\nor identifying genetic variants associated with a continuous trait. Our\nBayesian structured sparsity model produced sparse results with substantially\nimproved sensitivity and precision relative to comparable methods. Through\nsimulations, we show that three properties are key to this improvement: i)\nmodeling structure in the covariates, ii) significance testing using the\nposterior probabilities of inclusion, and iii) model averaging. We present\nresults from applying this model to a large genomic dataset to demonstrate\ncomputational tractability.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 19:50:25 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Engelhardt", "Barbara E.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1407.2267", "submitter": "Rob Wang", "authors": "Rob J. Wang and Peter W. Glynn", "title": "Measuring the Initial Transient: Reflected Brownian Motion", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the convergence to equilibrium of one-dimensional reflected\nBrownian motion (RBM) and compute a number of related initial transient\nformulae. These formulae are of interest as approximations to the initial\ntransient for queueing systems in heavy traffic, and help us to identify\nsettings in which initialization bias is significant. We conclude with a\ndiscussion of mean square error for RBM. Our analysis supports the view that\ninitial transient effects for RBM and related models are typically of modest\nsize relative to the intrinsic stochastic variability, unless one chooses an\nespecially poor initialization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 20:45:11 GMT"}, {"version": "v2", "created": "Sun, 15 Feb 2015 20:18:07 GMT"}, {"version": "v3", "created": "Sat, 21 Feb 2015 08:28:59 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Wang", "Rob J.", ""], ["Glynn", "Peter W.", ""]]}, {"id": "1407.2451", "submitter": "Preetam Nandy", "authors": "Preetam Nandy, Marloes H. Maathuis and Thomas S. Richardson", "title": "Estimating the effect of joint interventions from observational data in\n  sparse high-dimensional settings", "comments": "30 pages, 3 figures, 45 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of joint causal effects from observational data.\nIn particular, we propose new methods to estimate the effect of multiple\nsimultaneous interventions (e.g., multiple gene knockouts), under the\nassumption that the observational data come from an unknown linear structural\nequation model with independent errors. We derive asymptotic variances of our\nestimators when the underlying causal structure is partly known, as well as\nhigh-dimensional consistency when the causal structure is fully unknown and the\njoint distribution is multivariate Gaussian. We also propose a generalization\nof our methodology to the class of nonparanormal distributions. We evaluate the\nestimators in simulation studies and also illustrate them on data from the\nDREAM4 challenge.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 12:14:00 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 16:50:13 GMT"}, {"version": "v3", "created": "Thu, 10 Mar 2016 00:41:14 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Nandy", "Preetam", ""], ["Maathuis", "Marloes H.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1407.2635", "submitter": "Lee Dicker", "authors": "Lee H. Dicker and Sihai D. Zhao", "title": "Nonparametric empirical Bayes and maximum likelihood estimation for\n  high-dimensional data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric empirical Bayes methods provide a flexible and attractive\napproach to high-dimensional data analysis. One particularly elegant empirical\nBayes methodology, involving the Kiefer-Wolfowitz nonparametric maximum\nlikelihood estimator (NPMLE) for mixture models, has been known for decades.\nHowever, implementation and theoretical analysis of the Kiefer-Wolfowitz NPMLE\nare notoriously difficult. A fast algorithm was recently proposed that makes\nNPMLE-based procedures feasible for use in large-scale problems, but the\nalgorithm calculates only an approximation to the NPMLE. In this paper we make\ntwo contributions. First, we provide upper bounds on the convergence rate of\nthe approximate NPMLE's statistical error, which have the same order as the\nbest known bounds for the true NPMLE. This suggests that the approximate NPMLE\nis just as effective as the true NPMLE for statistical applications. Second, we\nillustrate the promise of NPMLE procedures in a high-dimensional binary\nclassification problem. We propose a new procedure and show that it vastly\noutperforms existing methods in experiments with simulated data. In real data\nanalyses involving cancer survival and gene expression data, we show that it is\nvery competitive with several recently proposed methods for regularized linear\ndiscriminant analysis, another popular approach to high-dimensional\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 20:50:23 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Dicker", "Lee H.", ""], ["Zhao", "Sihai D.", ""]]}, {"id": "1407.3010", "submitter": "Eric Bair", "authors": "Qian Liu, Guanhua Chen, Michael R. Kosorok, and Eric Bair", "title": "Biclustering Via Sparse Clustering", "comments": "40 pages, 8 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations it is desirable to identify clusters that differ with\nrespect to only a subset of features. Such clusters may represent homogeneous\nsubgroups of patients with a disease, such as cancer or chronic pain. We define\na bicluster to be a submatrix U of a larger data matrix X such that the\nfeatures and observations in U differ from those not contained in U. For\nexample, the observations in U could have different means or variances with\nrespect to the features in U. We propose a general framework for biclustering\nbased on the sparse clustering method of Witten and Tibshirani (2010). We\ndevelop a method for identifying features that belong to biclusters. This\nframework can be used to identify biclusters that differ with respect to the\nmeans of the features, the variance of the features, or more general\ndifferences. We apply these methods to several simulated and real-world data\nsets and compare the results of our method with several previously published\nmethods. The results of our method compare favorably with existing methods with\nrespect to both predictive accuracy and computing time.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 02:56:40 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Liu", "Qian", ""], ["Chen", "Guanhua", ""], ["Kosorok", "Michael R.", ""], ["Bair", "Eric", ""]]}, {"id": "1407.3079", "submitter": "Ying Sun", "authors": "Ying Sun, Prabhu Babu, and Daniel P. Palomar", "title": "Regularized Tyler's Scatter Estimator: Existence, Uniqueness, and\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2348944", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the regularized Tyler's scatter estimator for elliptical\ndistributions, which has received considerable attention recently. Various\ntypes of shrinkage Tyler's estimators have been proposed in the literature and\nproved work effectively in the \"small n large p\" scenario. Nevertheless, the\nexistence and uniqueness properties of the estimators are not thoroughly\nstudied, and in certain cases the algorithms may fail to converge. In this\nwork, we provide a general result that analyzes the sufficient condition for\nthe existence of a family of shrinkage Tyler's estimators, which quantitatively\nshows that regularization indeed reduces the number of required samples for\nestimation and the convergence of the algorithms for the estimators. For two\nspecific shrinkage Tyler's estimators, we also proved that the condition is\nnecessary and the estimator is unique. Finally, we show that the two estimators\nare actually equivalent. Numerical algorithms are also derived based on the\nmajorization-minimization framework, under which the convergence is analyzed\nsystematically.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 09:20:51 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Sun", "Ying", ""], ["Babu", "Prabhu", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1407.3089", "submitter": "Marie-Colette van Lieshout", "authors": "O. Cronie, M.N.M. van Lieshout", "title": "Summary statistics for inhomogeneous marked point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new summary statistics for intensity-reweighted moment stationary\nmarked point processes with particular emphasis on discrete marks. The new\nstatistics are based on the n-point correlation functions and reduce to cross\nJ- and D-functions when stationarity holds. We explore the relationships\nbetween the various functions and discuss their explicit forms under specific\nmodel assumptions. We derive ratio-unbiased minus sampling estimators for our\nstatistics and illustrate their use on a data set of wildfires.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 09:53:55 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Cronie", "O.", ""], ["van Lieshout", "M. N. M.", ""]]}, {"id": "1407.3152", "submitter": "Tao Yu Dr", "authors": "Tao Yu, Pengfei Li and Jing Qin", "title": "Maximum Smoothed Likelihood Component Density Estimation in Mixture\n  Models with Known Mixing Proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a maximum smoothed likelihood method to estimate\nthe component density functions of mixture models, in which the mixing\nproportions are known and may differ among observations. The proposed estimates\nmaximize a smoothed log likelihood function and inherit all the important\nproperties of probability density functions. A majorization-minimization\nalgorithm is suggested to compute the proposed estimates numerically. In\ntheory, we show that starting from any initial value, this algorithm increases\nthe smoothed likelihood function and further leads to estimates that maximize\nthe smoothed likelihood function. This indicates the convergence of the\nalgorithm. Furthermore, we theoretically establish the asymptotic convergence\nrate of our proposed estimators. An adaptive procedure is suggested to choose\nthe bandwidths in our estimation procedure. Simulation studies show that the\nproposed method is more efficient than the existing method in terms of\nintegrated squared errors. A real data example is further analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 13:30:14 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Yu", "Tao", ""], ["Li", "Pengfei", ""], ["Qin", "Jing", ""]]}, {"id": "1407.3206", "submitter": "Flore Harl\\'e", "authors": "Flore Harl\\'e, Florent Chatelain, C\\'edric Gouy-Pailler and Sophie\n  Achard", "title": "Bayesian Model for Multiple Change-points Detection in Multivariate Time\n  Series", "comments": "29 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of detecting change-points in multivariate\ntime series. The proposed approach differs from existing counterparts by making\nonly weak assumptions on both the change-points structure across series, and\nthe statistical signal distributions. Specifically change-points are not\nassumed to occur at simultaneous time instants across series, and no specific\ndistribution is assumed on the individual signals. It relies on the combination\nof a local robust statistical test acting on individual time segments, with a\nglobal Bayesian framework able to optimize configurations from multiple local\nstatistics (from segments of a unique time series or multiple time series).\nUsing an extensive experimental set-up, our algorithm is shown to perform well\non Gaussian data, with the same results in term of recall and precision as\nclassical approaches, such as the fused lasso and the Bernoulli Gaussian model.\nFurthermore, it outperforms the reference models in the case of non normal data\nwith outliers. The control of the False Discovery Rate by an acceptance level\nis confirmed. In the case of multivariate data, the probabilities that\nsimultaneous change-points are shared by some specific time series are learned.\nWe finally illustrate our algorithm with real datasets from energy monitoring\nand genomic. Segmentations are compared to state-of-the-art approaches based on\nfused lasso and group fused lasso.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 16:11:58 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Harl\u00e9", "Flore", ""], ["Chatelain", "Florent", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Achard", "Sophie", ""]]}, {"id": "1407.3252", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and Sebastian Lerch", "title": "Log-normal distribution based EMOS models for probabilistic wind speed\n  forecasting", "comments": "24 pages, 10 figures", "journal-ref": "Quarterly Journal of the Royal Meteorological Society 141 (2015),\n  no. 691, 2289-2299", "doi": "10.1002/qj.2521", "report-no": null, "categories": "stat.ME physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of forecasts are obtained from multiple runs of numerical weather\nforecasting models with different initial conditions and typically employed to\naccount for forecast uncertainties. However, biases and dispersion errors often\noccur in forecast ensembles, they are usually under-dispersive and uncalibrated\nand require statistical post-processing. We present an Ensemble Model Output\nStatistics (EMOS) method for calibration of wind speed forecasts based on the\nlog-normal (LN) distribution, and we also show a regime-switching extension of\nthe model which combines the previously studied truncated normal (TN)\ndistribution with the LN.\n  Both presented models are applied to wind speed forecasts of the eight-member\nUniversity of Washington mesoscale ensemble, of the fifty-member ECMWF ensemble\nand of the eleven-member ALADIN-HUNEPS ensemble of the Hungarian Meteorological\nService, and their predictive performances are compared to those of the TN and\ngeneral extreme value (GEV) distribution based EMOS methods and to the TN-GEV\nmixture model. The results indicate improved calibration of probabilistic and\naccuracy of point forecasts in comparison to the raw ensemble and to\nclimatological forecasts. Further, the TN-LN mixture model outperforms the\ntraditional TN method and its predictive performance is able to keep up with\nthe models utilizing the GEV distribution without assigning mass to negative\nvalues.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 18:57:58 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Lerch", "Sebastian", ""]]}, {"id": "1407.3414", "submitter": "Kristin Linn", "authors": "Kristin A. Linn, Eric B. Laber, Leonard A. Stefanski", "title": "Interactive Q-learning for Probabilities and Quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic treatment regime is a sequence of decision rules in which each\ndecision rule recommends treatment based on features of patient medical history\nsuch as past treatments and outcomes. Existing methods for estimating optimal\ndynamic treatment regimes from data optimize the mean of a response variable.\nHowever, the mean may not always be the most appropriate summary of\nperformance. We derive estimators of decision rules for optimizing\nprobabilities and quantiles computed with respect to the response distribution\nfor two-stage, binary treatment settings. This enables estimation of dynamic\ntreatment regimes that optimize the cumulative distribution function of the\nresponse at a prespecified point or a prespecified quantile of the response\ndistribution such as the median. The proposed methods perform favorably in\nsimulation experiments. We illustrate our approach with data from a\nsequentially randomized trial where the primary outcome is remission of\ndepression symptoms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 19:09:43 GMT"}, {"version": "v2", "created": "Thu, 21 May 2015 18:20:49 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Linn", "Kristin A.", ""], ["Laber", "Eric B.", ""], ["Stefanski", "Leonard A.", ""]]}, {"id": "1407.3463", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Antti Solonen, Tiangang Cui, James Martin, Luis\n  Tenorio, and Youssef Marzouk", "title": "Optimal low-rank approximations of Bayesian linear inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bayesian approach to inverse problems, data are often informative,\nrelative to the prior, only on a low-dimensional subspace of the parameter\nspace. Significant computational savings can be achieved by using this subspace\nto characterize and approximate the posterior distribution of the parameters.\nWe first investigate approximation of the posterior covariance matrix as a\nlow-rank update of the prior covariance matrix. We prove optimality of a\nparticular update, based on the leading eigendirections of the matrix pencil\ndefined by the Hessian of the negative log-likelihood and the prior precision,\nfor a broad class of loss functions. This class includes the F\\\"{o}rstner\nmetric for symmetric positive definite matrices, as well as the\nKullback-Leibler divergence and the Hellinger distance between the associated\ndistributions. We also propose two fast approximations of the posterior mean\nand prove their optimality with respect to a weighted Bayes risk under\nsquared-error loss. These approximations are deployed in an offline-online\nmanner, where a more costly but data-independent offline calculation is\nfollowed by fast online evaluations. As a result, these approximations are\nparticularly useful when repeated posterior mean evaluations are required for\nmultiple data sets. We demonstrate our theoretical results with several\nnumerical examples, including high-dimensional X-ray tomography and an inverse\nheat conduction problem. In both of these examples, the intrinsic\nlow-dimensional structure of the inference problem can be exploited while\nproducing results that are essentially indistinguishable from solutions\ncomputed in the full space.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jul 2014 13:23:50 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 14:54:58 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Spantini", "Alessio", ""], ["Solonen", "Antti", ""], ["Cui", "Tiangang", ""], ["Martin", "James", ""], ["Tenorio", "Luis", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1407.3824", "submitter": "Emmanuel J. Cand\\`{e}s", "authors": "Ma{\\l}gorzata Bogdan, Ewout van den Berg, Chiara Sabatti, Weijie Su,\n  Emmanuel J. Cand\\`es", "title": "SLOPE - Adaptive variable selection via convex optimization", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS842 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1103-1140", "doi": "10.1214/15-AOAS842", "report-no": "IMS-AOAS-AOAS842", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new estimator for the vector of coefficients $\\beta$ in the\nlinear model $y=X\\beta+z$, where $X$ has dimensions $n\\times p$ with $p$\npossibly larger than $n$. SLOPE, short for Sorted L-One Penalized Estimation,\nis the solution to \\[\\min_{b\\in\\mathbb{R}^p}\\frac{1}{2}\\Vert y-Xb\\Vert\n_{\\ell_2}^2+\\lambda_1\\vert b\\vert _{(1)}+\\lambda_2\\vert\nb\\vert_{(2)}+\\cdots+\\lambda_p\\vert b\\vert_{(p)},\\] where\n$\\lambda_1\\ge\\lambda_2\\ge\\cdots\\ge\\lambda_p\\ge0$ and $\\vert\nb\\vert_{(1)}\\ge\\vert b\\vert_{(2)}\\ge\\cdots\\ge\\vert b\\vert_{(p)}$ are the\ndecreasing absolute values of the entries of $b$. This is a convex program and\nwe demonstrate a solution algorithm whose computational complexity is roughly\ncomparable to that of classical $\\ell_1$ procedures such as the Lasso. Here,\nthe regularizer is a sorted $\\ell_1$ norm, which penalizes the regression\ncoefficients according to their rank: the higher the rank - that is, stronger\nthe signal - the larger the penalty. This is similar to the Benjamini and\nHochberg [J. Roy. Statist. Soc. Ser. B 57 (1995) 289-300] procedure (BH) which\ncompares more significant $p$-values with more stringent thresholds. One\nnotable choice of the sequence $\\{\\lambda_i\\}$ is given by the BH critical\nvalues $\\lambda_{\\mathrm {BH}}(i)=z(1-i\\cdot q/2p)$, where $q\\in(0,1)$ and\n$z(\\alpha)$ is the quantile of a standard normal distribution. SLOPE aims to\nprovide finite sample guarantees on the selected model; of special interest is\nthe false discovery rate (FDR), defined as the expected proportion of\nirrelevant regressors among all selected predictors. Under orthogonal designs,\nSLOPE with $\\lambda_{\\mathrm{BH}}$ provably controls FDR at level $q$.\nMoreover, it also appears to have appreciable inferential properties under more\ngeneral designs $X$ while having substantial power, as demonstrated in a series\nof experiments running on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 21:29:23 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 13:38:12 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Bogdan", "Ma\u0142gorzata", ""], ["Berg", "Ewout van den", ""], ["Sabatti", "Chiara", ""], ["Su", "Weijie", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1407.3939", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (DI-ENS, INRIA Paris - Rocquencourt), Robin Genuer\n  (ISPED, INRIA Bordeaux - Sud-Ouest)", "title": "Analysis of purely random forests bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are a very effective and commonly used statistical method, but\ntheir full theoretical analysis is still an open problem. As a first step,\nsimplified models such as purely random forests have been introduced, in order\nto shed light on the good performance of random forests. In this paper, we\nstudy the approximation error (the bias) of some purely random forest models in\na regression framework, focusing in particular on the influence of the number\nof trees in the forest. Under some regularity assumptions on the regression\nfunction, we show that the bias of an infinite forest decreases at a faster\nrate (with respect to the size of each tree) than a single tree. As a\nconsequence, infinite forests attain a strictly better risk rate (with respect\nto the sample size) than single trees. Furthermore, our results allow to derive\na minimum number of trees sufficient to reach the same rate as an infinite\nforest. As a by-product of our analysis, we also show a link between the bias\nof purely random forests and the bias of some kernel estimators.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 11:12:54 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Arlot", "Sylvain", "", "DI-ENS, INRIA Paris - Rocquencourt"], ["Genuer", "Robin", "", "ISPED, INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1407.3961", "submitter": "Abhik Ghosh", "authors": "Avijit Maji, Abhik Ghosh, Ayanendranath Basu", "title": "The Logarithmic Super Divergence and its use in Statistical Inference", "comments": "Pre-print, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new superfamily of divergences that is similar in\nspirit to the S-divergence family introduced by Ghosh et al. (2013). This new\nfamily serves as an umbrella that contains the logarithmic power divergence\nfamily (Renyi, 1961; Maji, Chakraborty and Basu 2014) and the logarithmic\ndensity power divergence family (Jones et al., 2001) as special cases. Various\nproperties of this new family and the corresponding minimum distance procedures\nare discussed with particular emphasis on the robustness issue; these\nproperties are demonstrated through simulation studies. In particular the\nmethod demonstrates the limitation of the first order influence function in\nassessing the robustness of the corresponding minimum distance procedures.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 12:33:27 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Maji", "Avijit", ""], ["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1407.4139", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega", "title": "Subjectivity, Bayesianism, and Causality", "comments": "21 pages, 21 figures. Submitted to Special Issue of Pattern\n  Recognition Letters on \"Philosophical aspects of pattern recognition\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian probability theory is one of the most successful frameworks to model\nreasoning under uncertainty. Its defining property is the interpretation of\nprobabilities as degrees of belief in propositions about the state of the world\nrelative to an inquiring subject. This essay examines the notion of\nsubjectivity by drawing parallels between Lacanian theory and Bayesian\nprobability theory, and concludes that the latter must be enriched with causal\ninterventions to model agency. The central contribution of this work is an\nabstract model of the subject that accommodates causal interventions in a\nmeasure-theoretic formalisation. This formalisation is obtained through a\ngame-theoretic Ansatz based on modelling the inside and outside of the subject\nas an extensive-form game with imperfect information between two players.\nFinally, I illustrate the expressiveness of this model with an example of\ncausal induction.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 20:16:10 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 03:51:42 GMT"}, {"version": "v3", "created": "Mon, 16 Feb 2015 21:27:16 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2015 19:59:32 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Ortega", "Pedro A.", ""]]}, {"id": "1407.4184", "submitter": "Lixing Zhu", "authors": "Lu Lin, Lixing Zhu and Yujie Gai", "title": "Inference for biased models: a quasi-instrumental variable approach", "comments": "33 pages. arXiv admin note: substantial text overlap with\n  arXiv:1112.0712, arXiv:1008.1345", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  For linear regression models who are not exactly sparse in the sense that the\ncoefficients of the insignificant variables are not exactly zero, the working\nmodels obtained by a variable selection are often biased. Even in sparse cases,\nafter a variable selection, when some significant variables are missing, the\nworking models are biased as well. Thus, under such situations, root-n\nconsistent estimation and accurate prediction could not be expected. In this\npaper, a novel remodelling method is proposed to produce an unbiased model when\nquasi-instrumental variables are introduced. The root-n estimation consistency\nand the asymptotic normality can be achieved, and the prediction accuracy can\nbe promoted as well. The performance of the new method is examined through\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 02:32:15 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Lin", "Lu", ""], ["Zhu", "Lixing", ""], ["Gai", "Yujie", ""]]}, {"id": "1407.4413", "submitter": "Tamar Sofer", "authors": "Tamar Sofer, Marilyn C. Cornelis, Peter Kraft, and Eric J. Tchetgen\n  Tchetgen", "title": "Control Function Assisted IPW Estimation with a Secondary Outcome in\n  Case-Control Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case-control studies are designed towards studying associations between risk\nfactors and a single, primary outcome. Information about additional, secondary\noutcomes is also collected, but association studies targeting such secondary\noutcomes should account for the case-control sampling scheme, or otherwise\nresults may be biased. Often, one uses inverse probability weighted (IPW)\nestimators to estimate population effects in such studies. However, these\nestimators are inefficient relative to estimators that make additional\nassumptions about the data generating mechanism. We propose a class of\nestimators for the effect of risk factors on a secondary outcome in\ncase-control studies, when the mean is modeled using either the identity or the\nlog link. The proposed estimator combines IPW with a mean zero control function\nthat depends explicitly on a model for the primary disease outcome. The\nefficient estimator in our class of estimators reduces to standard IPW when the\nmodel for the primary disease outcome is unrestricted, and is more efficient\nthan standard IPW when the model is either parametric or semiparametric.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:23:59 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Sofer", "Tamar", ""], ["Cornelis", "Marilyn C.", ""], ["Kraft", "Peter", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1407.4578", "submitter": "Giles Hooker", "authors": "Giles Hooker and Steven Roberts", "title": "Maximal Autocorrelation Functions in Functional Data Analysis", "comments": "10 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new factor rotation for the context of functional\nprincipal components analysis. This rotation seeks to re-represent a functional\nsubspace in terms of directions of decreasing smoothness as represented by a\ngeneralized smoothing metric. The rotation can be implemented simply and we\nshow on two examples that this rotation can improve the interpretability of the\nleading components.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 06:57:36 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Hooker", "Giles", ""], ["Roberts", "Steven", ""]]}, {"id": "1407.4703", "submitter": "Karla Diaz-Ordaz Dr", "authors": "Karla Diaz-Ordaz, Michael G. Kenward, Manuel Gomes, Richard Grieve", "title": "A comparison of multiple imputation methods for bivariate hierarchical\n  outcomes", "comments": "15 pages, 4 figures. Supplementary tables and other material\n  available from first author upon request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing observations are common in cluster randomised trials. Approaches\ntaken to handling such missing data include: complete case analysis,\nsingle-level multiple imputation that ignores the clustering, multiple\nimputation with a fixed effect for each cluster and multilevel multiple\nimputation.\n  We conducted a simulation study to assess the performance of these\napproaches, in terms of confidence interval coverage and empirical bias in the\nestimated treatment effects. Missing-at-random clustered data scenarios were\nsimulated following a full-factorial design. An Analysis of Variance was\ncarried out to study the influence of the simulation factors on each\nperformance measure.\n  When the randomised treatment arm was associated with missingness, complete\ncase analysis resulted in biased treatment effect estimates. Across all the\nmissing data mechanisms considered, the multiple imputation methods provided\nestimators with negligible bias. Confidence interval coverage was generally in\nexcess of nominal levels (up to 99.8%) following fixed-effects multiple\nimputation, and too low following single-level multiple imputation. Multilevel\nmultiple imputation led to coverage levels of approximately 95% throughout.\n  The approach to handling missing data was the most influential factor on the\nbias and coverage. Within each method, the most important factors were the\nnumber and size of clusters, and the intraclass correlation coefficient.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 15:12:45 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Diaz-Ordaz", "Karla", ""], ["Kenward", "Michael G.", ""], ["Gomes", "Manuel", ""], ["Grieve", "Richard", ""]]}, {"id": "1407.4729", "submitter": "Yin Lou", "authors": "Yin Lou, Jacob Bien, Rich Caruana, Johannes Gehrke", "title": "Sparse Partially Linear Additive Models", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized partially linear additive model (GPLAM) is a flexible and\ninterpretable approach to building predictive models. It combines features in\nan additive manner, allowing each to have either a linear or nonlinear effect\non the response. However, the choice of which features to treat as linear or\nnonlinear is typically assumed known. Thus, to make a GPLAM a viable approach\nin situations in which little is known $a~priori$ about the features, one must\novercome two primary model selection challenges: deciding which features to\ninclude in the model and determining which of these features to treat\nnonlinearly. We introduce the sparse partially linear additive model (SPLAM),\nwhich combines model fitting and $both$ of these model selection challenges\ninto a single convex optimization problem. SPLAM provides a bridge between the\nlasso and sparse additive models. Through a statistical oracle inequality and\nthorough simulation, we demonstrate that SPLAM can outperform other methods\nacross a broad spectrum of statistical regimes, including the high-dimensional\n($p\\gg N$) setting. We develop efficient algorithms that are applied to real\ndata sets with half a million samples and over 45,000 features with excellent\npredictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 16:27:36 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 19:17:59 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 19:02:45 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Lou", "Yin", ""], ["Bien", "Jacob", ""], ["Caruana", "Rich", ""], ["Gehrke", "Johannes", ""]]}, {"id": "1407.4916", "submitter": "Andre Beinrucker", "authors": "Andre Beinrucker, \\\"Ur\\\"un Dogan, Gilles Blanchard", "title": "Extensions of stability selection using subsamples of observations and\n  covariates", "comments": "accepted for publication in Statistics and Computing", "journal-ref": "Statistics and Computing 26 (5): 1059-1077 (2016)", "doi": "10.1007/s11222-015-9589-y", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce extensions of stability selection, a method to stabilise\nvariable selection methods introduced by Meinshausen and B\\\"uhlmann (J R Stat\nSoc 72:417-473, 2010). We propose to apply a base selection method repeatedly\nto random observation subsamples and covariate subsets under scrutiny, and to\nselect covariates based on their selection frequency. We analyse the effects\nand benefits of these extensions. Our analysis generalizes the theoretical\nresults of Meinshausen and B\\\"uhlmann (J R Stat Soc 72:417-473, 2010) from the\ncase of half-samples to subsamples of arbitrary size. We study, in a\ntheoretical manner, the effect of taking random covariate subsets using a\nsimplified score model. Finally we validate these extensions on numerical\nexperiments on both synthetic and real datasets, and compare the obtained\nresults in detail to the original stability selection method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 08:52:41 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 16:53:44 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2015 08:21:08 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Beinrucker", "Andre", ""], ["Dogan", "\u00dcr\u00fcn", ""], ["Blanchard", "Gilles", ""]]}, {"id": "1407.4971", "submitter": "Kosuke Morikawa", "authors": "Kosuke Morikawa and Yutaka Kano", "title": "Statistical Inference with Different Missing-data Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data are missing due to at most one cause from some time to next time,\nwe can make sampling distribution inferences about the parameter of the data by\nmodeling the missing-data mechanism correctly. Proverbially, in case its\nmechanism is missing at random (MAR), it can be ignored, but in case not\nmissing at random (NMAR), it can not be. There are no methods, however, to\nanalyze when missing of the data can occur because of several causes despite of\nthere being many such data in practice. Hence the aim of this paper is to\npropose how to inference on such data. Concretely, we extend the missing-data\nindicator from usual binary random vectors to discrete random vectors, define\nmissing-data mechanism for every causes and research ignorability of a mixture\nof missing-data mechanisms such as \"MAR & MAR\" and \"MAR & NMAR\". In particular,\nwhen the combination of mechanisms is \"MAR & NMAR\", generally the component of\nMAR can not be ignored, but in special case, it can be.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 12:38:56 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Morikawa", "Kosuke", ""], ["Kano", "Yutaka", ""]]}, {"id": "1407.4981", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, Jukka Corander", "title": "Likelihood-free inference via classification", "comments": "Accepted for publication in Statistics and Computing (Feb 13, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly complex generative models are being used across disciplines as\nthey allow for realistic characterization of data, but a common difficulty with\nthem is the prohibitively large computational cost to evaluate the likelihood\nfunction and thus to perform likelihood-based statistical inference. A\nlikelihood-free inference framework has emerged where the parameters are\nidentified by finding values that yield simulated data resembling the observed\ndata. While widely applicable, a major difficulty in this framework is how to\nmeasure the discrepancy between the simulated and observed data. Transforming\nthe original problem into a problem of classifying the data into simulated\nversus observed, we find that classification accuracy can be used to assess the\ndiscrepancy. The complete arsenal of classification methods becomes thereby\navailable for inference of intractable generative models. We validate our\napproach using theory and simulations for both point estimation and Bayesian\ninference, and demonstrate its use on real data by inferring an\nindividual-based epidemiological model for bacterial infections in child care\ncenters.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 13:17:30 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2015 12:09:27 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 11:14:02 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Dutta", "Ritabrata", ""], ["Kaski", "Samuel", ""], ["Corander", "Jukka", ""]]}, {"id": "1407.5014", "submitter": "Yakov Nikitin", "authors": "M. Jovanovic, B. Milosevic, Ya. Yu. Nikitin, M. Obradovic, K. Yu.\n  Volkova", "title": "Tests of exponentiality based on Arnold-Villasenor characterization, and\n  their efficiencies", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two families of scale-free exponentiality tests based on the\nrecent characterization of exponentiality by Arnold and Villasenor. The test\nstatistics are based on suitable functionals of U-empirical distribution\nfunctions. The family of integral statistics can be reduced to V- or\nU-statistics with relatively simple non-degenerate kernels. They are\nasymptotically normal and have reasonably high local Bahadur efficiency under\ncommon alternatives. This efficiency is compared with simulated powers of new\ntests. On the other hand, the Kolmogorov type tests demonstrate very low local\nBahadur efficiency and rather moderate power for common alternatives,and can\nhardly be recommended to practitioners. We also explore the conditions of local\nasymptotic optimality of new tests and describe for both families special \"most\nfavorable\" alternatives for which the tests are fully efficient.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 14:41:54 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Jovanovic", "M.", ""], ["Milosevic", "B.", ""], ["Nikitin", "Ya. Yu.", ""], ["Obradovic", "M.", ""], ["Volkova", "K. Yu.", ""]]}, {"id": "1407.5055", "submitter": "Enming Luo", "authors": "Enming Luo, Stanley H. Chan, Truong Q. Nguyen", "title": "Adaptive Image Denoising by Targeted Databases", "comments": "15 pages, 13 figures, 2 tables, journal", "journal-ref": null, "doi": "10.1109/TIP.2015.2414873", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-dependent denoising procedure to restore noisy images.\nDifferent from existing denoising algorithms which search for patches from\neither the noisy image or a generic database, the new algorithm finds patches\nfrom a database that contains only relevant patches. We formulate the denoising\nproblem as an optimal filter design problem and make two contributions. First,\nwe determine the basis function of the denoising filter by solving a group\nsparsity minimization problem. The optimization formulation generalizes\nexisting denoising algorithms and offers systematic analysis of the\nperformance. Improvement methods are proposed to enhance the patch search\nprocess. Second, we determine the spectral coefficients of the denoising filter\nby considering a localized Bayesian prior. The localized prior leverages the\nsimilarity of the targeted database, alleviates the intensive Bayesian\ncomputation, and links the new method to the classical linear minimum mean\nsquared error estimation. We demonstrate applications of the proposed method in\na variety of scenarios, including text images, multiview images and face\nimages. Experimental results show the superiority of the new algorithm over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 22:39:56 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 03:58:16 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 05:27:58 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Luo", "Enming", ""], ["Chan", "Stanley H.", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1407.5185", "submitter": "Pavlo Mozharovskyi", "authors": "Pavlo Mozharovskyi, Karl Mosler, Tatjana Lange", "title": "Classifying real-world data with the $DD\\alpha$-procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The $DD\\alpha$-classifier, a nonparametric fast and very robust procedure, is\ndescribed and applied to fifty classification problems regarding a broad\nspectrum of real-world data. The procedure first transforms the data from their\noriginal property space into a depth space, which is a low-dimensional unit\ncube, and then separates them by a projective invariant procedure, called\n$\\alpha$-procedure. To each data point the transformation assigns its depth\nvalues with respect to the given classes. Several alternative depth notions\n(spatial depth, Mahalanobis depth, projection depth, and Tukey depth, the\nlatter two being approximated by univariate projections) are used in the\nprocedure, and compared regarding their average error rates. With the Tukey\ndepth, which fits the distributions' shape best and is most robust,\n`outsiders', that is data points having zero depth in all classes, need an\nadditional treatment for classification. Evidence is also given about the\ndimension of the extended feature space needed for linear separation. The\n$DD\\alpha$-procedure is available as an R-package.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2014 13:55:47 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 15:20:39 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Mozharovskyi", "Pavlo", ""], ["Mosler", "Karl", ""], ["Lange", "Tatjana", ""]]}, {"id": "1407.5241", "submitter": "Wanjie Wang", "authors": "Jiashun Jin and Wanjie Wang", "title": "Influential Feature PCA for high dimensional clustering", "comments": "62 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a clustering problem where we observe feature vectors $X_i \\in\nR^p$, $i = 1, 2, \\ldots, n$, from $K$ possible classes. The class labels are\nunknown and the main interest is to estimate them. We are primarily interested\nin the modern regime of $p \\gg n$, where classical clustering methods face\nchallenges.\n  We propose Influential Features PCA (IF-PCA) as a new clustering procedure.\nIn IF-PCA, we select a small fraction of features with the largest\nKolmogorov-Smirnov (KS) scores, where the threshold is chosen by adapting the\nrecent notion of Higher Criticism, obtain the first $(K-1)$ left singular\nvectors of the post-selection normalized data matrix, and then estimate the\nlabels by applying the classical k-means to these singular vectors. It can be\nseen that IF-PCA is a tuning free clustering method.\n  We apply IF-PCA to $10$ gene microarray data sets. The method has competitive\nperformance in clustering. Especially, in three of the data sets, the error\nrates of IF-PCA are only $29\\%$ or less of the error rates by other methods. We\nhave also rediscovered a phenomenon on empirical null by \\cite{Efron} on\nmicroarray data.\n  With delicate analysis, especially post-selection eigen-analysis, we derive\ntight probability bounds on the Kolmogorov-Smirnov statistics and show that\nIF-PCA yields clustering consistency in a broad context. The clustering problem\nis connected to the problems of sparse PCA and low-rank matrix recovery, but it\nis different in important ways. We reveal an interesting phase transition\nphenomenon associated with these problems and identify the range of interest\nfor each.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 03:41:25 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 20:24:28 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2015 04:05:06 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Jin", "Jiashun", ""], ["Wang", "Wanjie", ""]]}, {"id": "1407.5290", "submitter": "Mathias Raschke -", "authors": "Mathias Raschke", "title": "Event-controlled constructions of random fields of maxima with\n  non-max-stable dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable random fields can be constructed according to Schlather (2002)\nwith a random function or a stationary process and a kind of random event\nmagnitude. These are applied for the modelling of natural hazards. We simply\nextend these event-controlled constructions to random fields of maxima with\nnon-max-stable dependence structure (copula). The theory for the variant with a\nstationary process is obvious; the parameter(s) of its correlation function\nis/are determined by the event magnitude. The introduced variant with random\nfunctions can only be researched numerically. The scaling of the random\nfunction is exponentially determined by the event magnitude. The location\nparameter of the Gumbel margins depends only on this exponential function in\nthe researched examples; the scale parameter of the margins is normalized. In\naddition, we propose a method for the parameter estimation for such\nconstructions by using Kendall's tau. The spatial dependence in relation to the\nblock size is considered therein. Finally, we briefly discuss some issues like\nthe sampling.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 13:46:50 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Raschke", "Mathias", ""]]}, {"id": "1407.5363", "submitter": "Marcos Prates O", "authors": "Marcos O. Prates and Erica C. Rodrigues and Renato M. Assun\\c{c}\\~ao", "title": "Where geography lives? A projection approach for spatial confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial confounding between the spatial random effects and fixed effects\ncovariates has been recently discovered and showed that it may bring misleading\ninterpretation to the model results. Solutions to alleviate this problem are\nbased on decomposing the spatial random effect and fitting a restricted spatial\nregression. In this paper, we propose a different approach: a transformation of\nthe geographic space to ensure that the unobserved spatial random effect added\nto the regression is orthogonal to the fixed effects covariates. Our approach,\nnamed SPOCK, has the additional benefit of providing a fast and simple\ncomputational method to estimate the parameters. Furthermore, it does not\nconstrain the distribution class assumed for the spatial error term. A\nsimulation study and a real data analysis are presented to better understand\nthe advantages of the new method in comparison with the existing ones.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 02:44:07 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 17:54:23 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Prates", "Marcos O.", ""], ["Rodrigues", "Erica C.", ""], ["Assun\u00e7\u00e3o", "Renato M.", ""]]}, {"id": "1407.5412", "submitter": "Rahul Biswas", "authors": "Rahul Biswas, Koulik Khamaru, Kaushik Majumdar", "title": "A Peak Synchronization Measure for Multiple Signals", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol.62, no.17,\n  pp.4390-4398, Sept.1, 2014", "doi": "10.1109/TSP.2014.2333568", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peaks signify important events in a signal. In a pair of signals how peaks\nare occurring with mutual correspondence may offer us significant insights into\nthe mutual interdependence between the two signals based on important events.\nIn this work we proposed a novel synchronization measure between two signals,\ncalled peak synchronization, which measures the simultaneity of occurrence of\npeaks in the signals. We subsequently generalized it to more than two signals.\nWe showed that our measure of synchronization is largely independent of the\nunderlying parameter values. A time complexity analysis of the algorithm has\nalso been presented. We applied the measure on intracranial EEG signals of\nepileptic patients and found that the enhanced synchronization during an\nepileptic seizure can be modeled better by the new peak synchronization measure\nthan the classical amplitude correlation method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 08:34:17 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 20:17:06 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Biswas", "Rahul", ""], ["Khamaru", "Koulik", ""], ["Majumdar", "Kaushik", ""]]}, {"id": "1407.5472", "submitter": "Veerabhadran Baladandayuthapani", "authors": "Subharup Guha and Veerabhadran Baladandayuthapani", "title": "Nonparametric Variable Selection, Clustering and Prediction for\n  High-Dimensional Regression", "comments": "Note: this version has been substantially revised and please see new\n  version of the article at the following link: [arXiv:1604.03615]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of parsimonious models for reliable inference and prediction\nof responses in high-dimensional regression settings is often challenging due\nto relatively small sample sizes and the presence of complex interaction\npatterns between a large number of covariates. We propose an efficient,\nnonparametric framework for simultaneous variable selection, clustering and\nprediction in high-throughput regression settings with continuous or discrete\noutcomes, called VariScan. The VariScan model utilizes the sparsity induced by\nPoisson-Dirichlet processes (PDPs) to group the covariates into\nlower-dimensional latent clusters consisting of covariates with similar\npatterns among the samples. The data are permitted to direct the choice of a\nsuitable cluster allocation scheme, choosing between PDPs and their special\ncase, a Dirichlet process. Subsequently, the latent clusters are used to build\na nonlinear prediction model for the responses using an adaptive mixture of\nlinear and nonlinear elements, thus achieving a balance between model parsimony\nand flexibility. We investigate theoretical properties of the VariScan\nprocedure that differentiate the allocations patterns of PDPs and Dirichlet\nprocesses both in terms of the number and relative sizes of their clusters.\nAdditional theoretical results guarantee the high accuracy of the model-based\nclustering procedure, and establish model selection and prediction consistency.\nThrough simulation studies and analyses of benchmark data sets, we demonstrate\nthe reliability of VariScan's clustering mechanism and show that the technique\ncompares favorably to, and often outperforms, existing methodologies in terms\nof the prediction accuracies of the subject-specific responses.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 12:41:32 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 15:49:50 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 02:04:25 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Guha", "Subharup", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1407.5509", "submitter": "John Matthews", "authors": "J.N.S. Matthews and Nuri H. Badi", "title": "Inconsistent treatment estimates from mis-specified logistic regression\n  analyses of randomized trials", "comments": "18 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the difference between treatments in a clinical trial is estimated by a\ndifference in means, then it is well known that randomization ensures unbiassed\nestimation, even if no account is taken of important baseline covariates.\nHowever, when the treatment effect is assessed by other summaries, e.g. by an\nodds ratio if the outcome is binary, then bias can arise if some covariates are\nomitted, regardless of the use of randomization for treatment allocation or the\nsize of the trial. We present accurate closed-form approximations for this\nasymptotic bias when important Normally distributed covariates are omitted from\na logistic regression. We compare this approximation with ones in the\nliterature and derive more convenient forms for some of these existing results.\nThe expressions give insight into the form of the bias, which simulations show\nis usable for distributions other than the Normal. The key result applies even\nwhen there are additional binary covariates in the model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 14:30:56 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Matthews", "J. N. S.", ""], ["Badi", "Nuri H.", ""]]}, {"id": "1407.5515", "submitter": "Lilun Du", "authors": "Wei Lan and Lilun Du", "title": "A Factor-Adjusted Multiple Testing Procedure with Application to Mutual\n  Fund Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a factor-adjusted multiple testing (FAT)\nprocedure based on factor-adjusted p-values in a linear factor model involving\nsome observable and unobservable factors, for the purpose of selecting skilled\nfunds in empirical finance. The factor-adjusted p-values were obtained after\nextracting the latent common factors by the principal component method. Under\nsome mild conditions, the false discovery proportion can be consistently\nestimated even if the idiosyncratic errors are allowed to be weakly correlated\nacross units. Furthermore, by appropriately setting a sequence of threshold\nvalues approaching zero, the proposed FAT procedure enjoys model selection\nconsistency. Extensive simulation studies and a real data analysis for\nselecting skilled funds in the U.S. financial market are presented to\nillustrate the practical utility of the proposed method. Supplementary\nmaterials for this article are available online.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 14:47:32 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 03:49:32 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Lan", "Wei", ""], ["Du", "Lilun", ""]]}, {"id": "1407.5525", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet, Jun Li, Prakash Balachandran, Steven Rosenberg and\n  Eric D. Kolaczyk", "title": "Hypothesis Testing For Network Data in Functional Neuroimaging", "comments": "34 pages. 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, it has become common practice in neuroscience to use\nnetworks to summarize relational information in a set of measurements,\ntypically assumed to be reflective of either functional or structural\nrelationships between regions of interest in the brain. One of the most basic\ntasks of interest in the analysis of such data is the testing of hypotheses, in\nanswer to questions such as \"Is there a difference between the networks of\nthese two groups of subjects?\" In the classical setting, where the unit of\ninterest is a scalar or a vector, such questions are answered through the use\nof familiar two-sample testing strategies. Networks, however, are not Euclidean\nobjects, and hence classical methods do not directly apply. We address this\nchallenge by drawing on concepts and techniques from geometry, and\nhigh-dimensional statistical inference. Our work is based on a precise\ngeometric characterization of the space of graph Laplacian matrices and a\nnonparametric notion of averaging due to Fr\\'echet. We motivate and illustrate\nour resulting methodologies for testing in the context of networks derived from\nfunctional neuroimaging data on human subjects from the 1000 Functional\nConnectomes Project. In particular, we show that this global test is more\nstatistical powerful, than a mass-univariate approach. In addition, we have\nalso provided a method for visualizing the individual contribution of each edge\nto the overall test statistic.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 15:20:34 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 16:40:50 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 15:15:47 GMT"}, {"version": "v4", "created": "Thu, 2 Feb 2017 15:49:34 GMT"}, {"version": "v5", "created": "Fri, 3 Feb 2017 10:53:01 GMT"}, {"version": "v6", "created": "Fri, 17 Mar 2017 16:41:13 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["Li", "Jun", ""], ["Balachandran", "Prakash", ""], ["Rosenberg", "Steven", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "1407.5809", "submitter": "Steffen Lauritzen", "authors": "Peter G. M. Forbes and Steffen Lauritzen and Jesper M{\\o}ller", "title": "Fingerprint Analysis with Marked Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for fingerprint matching based on marked point process\nmodels. An efficient Monte Carlo algorithm is developed to calculate the\nmarginal likelihood ratio for the hypothesis that two observed prints originate\nfrom the same finger against the hypothesis that they originate from different\nfingers. Our model achieves good performance on an NIST-FBI fingerprint\ndatabase of 258 matched fingerprint pairs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 10:19:00 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Forbes", "Peter G. M.", ""], ["Lauritzen", "Steffen", ""], ["M\u00f8ller", "Jesper", ""]]}, {"id": "1407.6033", "submitter": "Catia  Scricciolo", "authors": "Sophie Donnet, Vincent Rivoirard, Judith Rousseau, Catia Scricciolo", "title": "Posterior concentration rates for counting processes with Aalen\n  multiplicative intensities", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide general conditions to derive posterior concentration rates for\nAalen counting processes. The conditions are designed to resemble those\nproposed in the literature for the problem of density estimation, for instance\nin Ghosal et al. (2000), so that existing results on density estimation can be\nadapted to the present setting. We apply the general theorem to some prior\nmodels including Dirichlet process mixtures of uniform densities to estimate\nmonotone non-increasing intensities and log-splines.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 20:42:29 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Donnet", "Sophie", ""], ["Rivoirard", "Vincent", ""], ["Rousseau", "Judith", ""], ["Scricciolo", "Catia", ""]]}, {"id": "1407.6084", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Wei Luo, Svetha Venkatesh", "title": "Stabilized Sparse Ordinal Regression for Medical Risk Stratification", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-014-0740-4", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent wide adoption of Electronic Medical Records (EMR) presents great\nopportunities and challenges for data mining. The EMR data is largely temporal,\noften noisy, irregular and high dimensional. This paper constructs a novel\nordinal regression framework for predicting medical risk stratification from\nEMR. First, a conceptual view of EMR as a temporal image is constructed to\nextract a diverse set of features. Second, ordinal modeling is applied for\npredicting cumulative or progressive risk. The challenges are building a\ntransparent predictive model that works with a large number of weakly\npredictive features, and at the same time, is stable against resampling\nvariations. Our solution employs sparsity methods that are stabilized through\ndomain-specific feature interaction networks. We introduces two indices that\nmeasure the model stability against data resampling. Feature networks are used\nto generate two multivariate Gaussian priors with sparse precision matrices\n(the Laplacian and Random Walk). We apply the framework on a large short-term\nsuicide risk prediction problem and demonstrate that our methods outperform\nclinicians to a large-margin, discover suicide risk factors that conform with\nmental health knowledge, and produce models with enhanced stability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 01:19:47 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Luo", "Wei", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6383", "submitter": "Armin Schwartzman", "authors": "Armin Schwartzman", "title": "Lognormal Distributions and Geometric Averages of Positive Definite\n  Matrices", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives a formal definition of a lognormal family of probability\ndistributions on the set of symmetric positive definite (PD) matrices, seen as\na matrix-variate extension of the univariate lognormal family of distributions.\nTwo forms of this distribution are obtained as the large sample limiting\ndistribution via the central limit theorem of two types of geometric averages\nof i.i.d. PD matrices: the log-Euclidean average and the canonical geometric\naverage. These averages correspond to two different geometries imposed on the\nset of PD matrices. The limiting distributions of these averages are used to\nprovide large-sample confidence regions for the corresponding population means.\nThe methods are illustrated on a voxelwise analysis of diffusion tensor imaging\ndata, permitting a comparison between the various average types from the point\nof view of their sampling variability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 20:38:53 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 17:20:39 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Schwartzman", "Armin", ""]]}, {"id": "1407.6484", "submitter": "Dominik Liebl", "authors": "Oualid Bada and Dominik Liebl", "title": "The R-package phtt: Panel Data Analysis with Heterogeneous Time Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R-package phtt provides estimation procedures for panel data with large\ndimensions n, T, and general forms of unobservable heterogeneous effects.\nParticularly, the estimation procedures are those of Bai (2009) and Kneip,\nSickles, and Song (2012), which complement one another very well: both models\nassume the unobservable heterogeneous effects to have a factor structure. Kneip\net al. (2012) considers the case in which the time varying common factors have\nrelatively smooth patterns including strongly positive auto-correlated\nstationary as well as non-stationary factors, whereas the method of Bai (2009)\nfocuses on stochastic bounded factors such as ARMA processes. Additionally, the\nphtt package provides a wide range of dimensionality criteria in order to\nestimate the number of the unobserved factors simultaneously with the remaining\nmodel parameters.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 08:30:32 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Bada", "Oualid", ""], ["Liebl", "Dominik", ""]]}, {"id": "1407.6800", "submitter": "Paul Kabaila", "authors": "Waruni Abeysekera and Paul Kabaila", "title": "Optimized recentered confidence spheres for the multivariate normal mean", "comments": "arXiv admin note: text overlap with arXiv:1306.2419", "journal-ref": "(2017) Electronic Journal of Statistics, 11, 1798-1826", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casella and Hwang, 1983, JASA, introduced a broad class of recentered\nconfidence spheres for the mean $\\boldsymbol{\\theta}$ of a multivariate normal\ndistribution with covariance matrix $\\sigma^2 \\boldsymbol{I}$, for $\\sigma^2$\nknown. Both the center and radius functions of these confidence spheres are\nflexible functions of the data. For the particular case of confidence spheres\ncentered on the positive-part James-Stein estimator and with radius determined\nby empirical Bayes considerations, they show numerically that these confidence\nspheres have the desired minimum coverage probability $1-\\alpha$ and dominate\nthe usual confidence sphere in terms of scaled volume. We shift the focus from\nthe scaled volume to the scaled expected volume of the recentered confidence\nsphere. Since both the coverage probability and the scaled expected volume are\nfunctions of the Euclidean norm of $\\boldsymbol{\\theta}$, it is feasible to\noptimize the performance of the recentered confidence sphere by numerically\ncomputing both the center and radius functions so as to optimize some clearly\nspecified criterion. We suppose that we have uncertain prior information that\n$\\boldsymbol{\\theta}= \\boldsymbol{0}$. This motivates us to determine the\ncenter and radius functions of the confidence sphere by numerical minimization\nof the scaled expected volume of the confidence sphere at $\\boldsymbol{\\theta}=\n\\boldsymbol{0}$, subject to the constraints that (a) the coverage probability\nnever falls below $1-\\alpha$ and (b) the radius never exceeds the radius of the\nstandard $1-\\alpha$ confidence sphere. Our results show that, by focusing on\nthis clearly specified criterion, significant gains in performance (in terms of\nthis criterion) can be achieved. We also present analogous results for the much\nmore difficult case that $\\sigma^2$ is unknown.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 07:44:14 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 23:45:49 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 04:26:41 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2015 02:47:11 GMT"}, {"version": "v5", "created": "Fri, 13 Oct 2017 00:37:59 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Abeysekera", "Waruni", ""], ["Kabaila", "Paul", ""]]}, {"id": "1407.6895", "submitter": "Stephanie Thiemichen", "authors": "Stephanie Thiemichen, Nial Friel, Alberto Caimo, G\\\"oran Kauermann", "title": "Bayesian Exponential Random Graph Models with Nodal Random Effects", "comments": "23 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the well-known and widely used Exponential Random Graph Model\n(ERGM) by including nodal random effects to compensate for heterogeneity in the\nnodes of a network. The Bayesian framework for ERGMs proposed by Caimo and\nFriel (2011) yields the basis of our modelling algorithm. A central question in\nnetwork models is the question of model selection and following the Bayesian\nparadigm we focus on estimating Bayes factors. To do so we develop an\napproximate but feasible calculation of the Bayes factor which allows one to\npursue model selection. Two data examples and a small simulation study\nillustrate our mixed model approach and the corresponding model selection.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 13:53:07 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 14:30:28 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Thiemichen", "Stephanie", ""], ["Friel", "Nial", ""], ["Caimo", "Alberto", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1407.7118", "submitter": "Vladimir Filimonov", "authors": "Spencer Wheatley, Vladimir Filimonov, Didier Sornette", "title": "Estimation of the Hawkes Process With Renewal Immigration Using the EM\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Hawkes process with renewal immigration and make its\nstatistical estimation possible with two Expectation Maximization (EM)\nalgorithms. The standard Hawkes process introduces immigrant points via a\nPoisson process, and each immigrant has a subsequent cluster of associated\noffspring of multiple generations. We generalize the immigration to come from a\nRenewal process; introducing dependence between neighbouring clusters, and\nallowing for over/under dispersion in cluster locations. This complicates\nevaluation of the likelihood since one needs to know which subset of the\nobserved points are immigrants. Two EM algorithms enable estimation here: The\nfirst is an extension of an existing algorithm that treats the entire branching\nstructure - which points are immigrants, and which point is the parent of each\noffspring - as missing data. The second considers only if a point is an\nimmigrant or not as missing data and can be implemented with linear time\ncomplexity. Both algorithms are found to be consistent in simulation studies.\nFurther, we show that misspecifying the immigration process introduces\nsignficant bias into model estimation-- especially the branching ratio, which\nquantifies the strength of self excitation. Thus, this extended model provides\na valuable alternative model in practice.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 10:22:06 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Wheatley", "Spencer", ""], ["Filimonov", "Vladimir", ""], ["Sornette", "Didier", ""]]}, {"id": "1407.7152", "submitter": "Aditya Vempaty", "authors": "Aditya Vempaty, Hao He, Biao Chen, and Pramod K. Varshney", "title": "On Quantizer Design for Distributed Bayesian Estimation in Sensor\n  Networks", "comments": "15 pages, 3 figures, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2014.2350964", "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed estimation under the Bayesian\ncriterion and explore the design of optimal quantizers in such a system. We\nshow that, for a conditionally unbiased and efficient estimator at the fusion\ncenter and when local observations have identical distributions, it is optimal\nto partition the local sensors into groups, with all sensors within a group\nusing the same quantization rule. When all the sensors use identical number of\ndecision regions, use of identical quantizers at the sensors is optimal. When\nthe network is constrained by the capacity of the wireless multiple access\nchannel over which the sensors transmit their quantized observations, we show\nthat binary quantizers at the local sensors are optimal under certain\nconditions. Based on these observations, we address the location parameter\nestimation problem and present our optimal quantizer design approach. We also\nderive the performance limit for distributed location parameter estimation\nunder the Bayesian criterion and find the conditions when the widely used\nthreshold quantizer achieves this limit. We corroborate this result using\nsimulations. We then relax the assumption of conditionally independent\nobservations and derive the optimality conditions of quantizers for\nconditionally dependent observations. Using counter-examples, we also show that\nthe previous results do not hold in this setting of dependent observations and,\ntherefore, identical quantizers are not optimal.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 19:42:41 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Vempaty", "Aditya", ""], ["He", "Hao", ""], ["Chen", "Biao", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1407.7247", "submitter": "Jesse Shore", "authors": "Jesse Shore and Benjamin Lubin", "title": "Spectral goodness of fit for network models", "comments": null, "journal-ref": "Social Networks (2015) Vol. 43", "doi": null, "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new statistic, 'spectral goodness of fit' (SGOF) to measure\nhow well a network model explains the structure of an observed network. SGOF\nprovides an absolute measure of fit, analogous to the standard R-squared in\nlinear regression. Additionally, as it takes advantage of the properties of the\nspectrum of the graph Laplacian, it is suitable for comparing network models of\ndiverse functional forms, including both fitted statistical models and\nalgorithmic generative models of networks. After introducing, defining, and\nproviding guidance for interpreting SGOF, we illustrate the properties of the\nstatistic with a number of examples and comparisons to existing techniques. We\nshow that such a spectral approach to assessing model fit fills gaps left by\nearlier methods and can be widely applied.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 15:35:41 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Shore", "Jesse", ""], ["Lubin", "Benjamin", ""]]}, {"id": "1407.7297", "submitter": "Jing Kong", "authors": "Jing Kong, Sijian Wang and Grace Wahba", "title": "Using distance covariance for improved variable selection with\n  applications to genetic risk models", "comments": "14 pages and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is of increasing importance to address the difficulties of\nhigh dimensionality in many scientific areas. In this paper, we demonstrate a\nproperty for distance covariance, which is incorporated in a novel feature\nscreening procedure together with the use of distance correlation. The approach\nmakes no distributional assumptions for the variables and does not require the\nspecification of a regression model, and hence is especially attractive in\nvariable selection given an enormous number of candidate attributes without\nmuch information about the true model with the response. The method is applied\nto two genetic risk problems, where issues including uncertainty of variable\nselection via cross validation, subgroup of hard-to-classify cases and the\napplication of a reject option are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 00:30:58 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 17:00:13 GMT"}, {"version": "v3", "created": "Tue, 2 Sep 2014 14:15:38 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Kong", "Jing", ""], ["Wang", "Sijian", ""], ["Wahba", "Grace", ""]]}, {"id": "1407.7308", "submitter": "Dylan Small", "authors": "Dylan Small, Zhiqiang Tan, Scott Lorch and Alan Brookhart", "title": "Instrumental Variable Estimation When Compliance is not Deterministic:\n  The Stochastic Monotonicity Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instrumental variables (IV) method is a method for making causal\ninferences about the effect of a treatment based on an observational study in\nwhich there are unmeasured confounding variables. The method requires a valid\nIV, a variable that is independent of the unmeasured confounding variables and\nis associated with the treatment but which has no effect on the outcome beyond\nits effect on the treatment. An additional assumption that is often made for\nthe IV method is deterministic monotonicity, which is an assumption that for\neach subject, the level of the treatment that a subject would take if given a\nlevel of the IV is a monotonic increasing function of the level of the IV.\nUnder deterministic monotonicity, the IV method identifies the average\ntreatment effect for the compliers (those subject who would take the treatment\nif encouraged to do so by the IV and not take the treatment if not encouraged).\nHowever, deterministic monotonicity is sometimes not realistic. We introduce a\nstochastic monotonicity condition which relaxes deterministic monotonicity in\nthat it does not require that a monotonic increasing relationship hold within\nsubjects between the levels of the IV and the level of the treatment that the\nsubject would take if given a level of the IV, but only that a monotonic\nincreasing relationship hold across subjects between the IV and the treatment\nin a certain manner. We show that under stochastic monotonicity, the IV method\nidentifies a weighted average of treatment effects with greater weight on\nsubgroups of subjects on whom the IV has a stronger effect. We provide bounds\non the global average treatment effect under stochastic monotonicity and a\nsensitivity analysis for violations of the stochastic monotonicity assumption.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 02:09:29 GMT"}, {"version": "v2", "created": "Sat, 16 Aug 2014 02:46:58 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Small", "Dylan", ""], ["Tan", "Zhiqiang", ""], ["Lorch", "Scott", ""], ["Brookhart", "Alan", ""]]}, {"id": "1407.7479", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley, Scott H. Holan, Christopher K. Wikle", "title": "Mixed Effects Modeling for Areal Data that Exhibit\n  Multivariate-Spatio-Temporal Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many data sources available that report related variables of\ninterest that are also referenced over geographic regions and time; however,\nthere are relatively few general statistical methods that one can readily use\nthat incorporate these multivariate-spatio-temporal dependencies. As such, we\nintroduce the multivariate-spatio-temporal mixed effects model (MSTM) to\nanalyze areal data with multivariate-spatio-temporal dependencies. The proposed\nMSTM extends the notion of Moran's I basis functions to the\nmultivariate-spatio-temporal setting. This extension leads to several\nmethodological contributions including extremely effective dimension reduction,\na dynamic linear model for multivariate-spatio-temporal areal processes, and\nthe reduction of a high-dimensional parameter space using a novel parameter\nmodel. Several examples are used to demonstrate that the MSTM provides an\nextremely viable solution to many important problems found in different and\ndistinct corners of the spatio-temporal statistics literature including:\nmodeling nonseparable and nonstationary covariances, combing data from multiple\nrepeated surveys, and analyzing massive multivariate-spatio-temporal datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 17:37:47 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 14:54:58 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1407.7614", "submitter": "Julie Josse", "authors": "Julie Josse and Stefan Wager and Fran\\c{c}ois Husson", "title": "Confidence Areas for Fixed-Effects PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PCA is often used to visualize data when the rows and the columns are both of\ninterest. In such a setting there is a lack of inferential methods on the PCA\noutput. We study the asymptotic variance of a fixed-effects model for PCA, and\npropose several approaches to assessing the variability of PCA estimates: a\nmethod based on a parametric bootstrap, a new cell-wise jackknife, as well as a\ncomputationally cheaper approximation to the jackknife. We visualize the\nconfidence regions by Procrustes rotation. Using a simulation study, we compare\nthe proposed methods and highlight the strengths and drawbacks of each method\nas we vary the number of rows, the number of columns, and the strength of the\nrelationships between variables.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 01:54:51 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Josse", "Julie", ""], ["Wager", "Stefan", ""], ["Husson", "Fran\u00e7ois", ""]]}, {"id": "1407.7697", "submitter": "Hidehiko Ichimura", "authors": "Yoichi Arai and Hidehiko Ichimura", "title": "Simultaneous Selection of Optimal Bandwidths for the Sharp Regression\n  Discontinuity Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new bandwidth selection rule that uses different bandwidths for the local\nlinear regression estimators on the left and the right of the cut-off point is\nproposed for the sharp regression discontinuity estimator of the mean program\nimpact at the cut-off point. The asymptotic mean squared error of the estimator\nusing the proposed bandwidth selection rule is shown to be smaller than other\nbandwidth selection rules proposed in the literature. An extensive simulation\nstudy shows that the proposed method's performances for the sample sizes 500,\n2000, and 5000 closely match the theoretical predictions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 11:29:17 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 07:24:07 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Arai", "Yoichi", ""], ["Ichimura", "Hidehiko", ""]]}, {"id": "1407.7738", "submitter": "Peter Martey Addo", "authors": "Peter Martey Addo", "title": "Multivariate Self-Exciting Threshold Autoregressive Models with\n  eXogenous Input", "comments": "This is a preliminary version of the paper-- please do not quote", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.CP q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study defines a multivariate Self--Exciting Threshold Autoregressive\nwith eXogenous input (MSETARX) models and present an estimation procedure for\nthe parameters. The conditions for stationarity of the nonlinear MSETARX models\nis provided. In particular, the efficiency of an adaptive parameter estimation\nalgorithm and LSE (least squares estimate) algorithm for this class of models\nis then provided via simulations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 14:30:25 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Addo", "Peter Martey", ""]]}, {"id": "1407.7783", "submitter": "Nanny Wermuth", "authors": "Nanny Wermuth and D.R. Cox", "title": "Graphical Markov models: overview", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe how graphical Markov models started to emerge in the last 40\nyears, based on three essential concepts that had been developed independently\nmore than a century ago. Sequences of joint or single regressions and their\nregression graphs are singled out as being best suited for analyzing\nlongitudinal data and for tracing developmental pathways. Interpretations are\nillustrated using two sets of data and some of the more recent, important\nresults for sequences of regressions are summarized.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 17:21:37 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 11:50:15 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Wermuth", "Nanny", ""], ["Cox", "D. R.", ""]]}, {"id": "1407.7795", "submitter": "Harrison Quick", "authors": "Harrison Quick, Scott H. Holan, Christopher K. Wikle, and Jerome P.\n  Reiter", "title": "Bayesian Marked Point Process Modeling for Generating Fully Synthetic\n  Public Use Data with Point-Referenced Geography", "comments": null, "journal-ref": "Spatial Statistics, 14 (2015), 439-451", "doi": "10.1016/j.spasta.2015.07.008", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data stewards collect confidential data that include fine geography.\nWhen sharing these data with others, data stewards strive to disseminate data\nthat are informative for a wide range of spatial and non-spatial analyses while\nsimultaneously protecting the confidentiality of data subjects' identities and\nattributes. Typically, data stewards meet this challenge by coarsening the\nresolution of the released geography and, as needed, perturbing the\nconfidential attributes. When done with high intensity, these redaction\nstrategies can result in released data with poor analytic quality. We propose\nan alternative dissemination approach based on fully synthetic data. We\ngenerate data using marked point process models that can maintain both the\nstatistical properties and the spatial dependence structure of the confidential\ndata. We illustrate the approach using data consisting of mortality records\nfrom Durham, North Carolina.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 17:43:10 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Quick", "Harrison", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1407.7820", "submitter": "Rui Song", "authors": "Runchao Jiang, Wenbin Lu, Rui Song, and Marie Davidian", "title": "On Estimation of Optimal Treatment Regimes For Maximizing t-Year\n  Survival Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A treatment regime is a deterministic function that dictates personalized\ntreatment based on patients' individual prognostic information. There is a\nfast-growing interest in finding optimal treatment regimes to maximize expected\nlong-term clinical outcomes of patients for complex diseases, such as cancer\nand AIDS. For many clinical studies with survival time as a primary endpoint, a\nmain goal is to maximize patients' survival probabilities given treatments. In\nthis article, we first propose two nonparametric estimators for survival\nfunction of patients following a given treatment regime. Then, we derive the\nestimation of the optimal treatment regime based on a value-based searching\nalgorithm within a set of treatment regimes indexed by parameters. The\nasymptotic properties of the proposed estimators for survival probabilities\nunder derived optimal treatment regimes are established under suitable\nregularity conditions. Simulations are conducted to evaluate the numerical\nperformance of the proposed estimators under various scenarios. An application\nto an AIDS clinical trial data is also given to illustrate the methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 18:38:47 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Jiang", "Runchao", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""], ["Davidian", "Marie", ""]]}, {"id": "1407.8038", "submitter": "Xiang Wan", "authors": "Xiang Wan, Wenqian Wang, Jiming Liu, and Tiejun Tong", "title": "Estimating the sample mean and standard deviation from the sample size,\n  median, range and/or interquartile range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In systematic reviews and meta-analysis, researchers often pool the results\nof the sample mean and standard deviation from a set of similar clinical\ntrials. A number of the trials, however, reported the study using the median,\nthe minimum and maximum values, and/or the first and third quartiles. Hence, in\norder to combine results, one may have to estimate the sample mean and standard\ndeviation for such trials. In this paper, we propose to improve the existing\nliterature in several directions. First, we show that the sample standard\ndeviation estimation in Hozo et al. (2005) has some serious limitations and is\nalways less satisfactory in practice. Inspired by this, we propose a new\nestimation method by incorporating the sample size. Second, we systematically\nstudy the sample mean and standard deviation estimation problem under more\ngeneral settings where the first and third quartiles are also available for the\ntrials. Through simulation studies, we demonstrate that the proposed methods\ngreatly improve the existing methods and enrich the literature. We conclude our\nwork with a summary table that serves as a comprehensive guidance for\nperforming meta-analysis in different situations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 13:36:42 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 12:10:00 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 02:14:57 GMT"}, {"version": "v4", "created": "Wed, 5 Oct 2016 01:32:44 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Wan", "Xiang", ""], ["Wang", "Wenqian", ""], ["Liu", "Jiming", ""], ["Tong", "Tiejun", ""]]}, {"id": "1407.8119", "submitter": "Radu Craiu", "authors": "Avideh Sabeti, Mian Wei and Radu V. Craiu", "title": "Additive Models for Conditional Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional copulas are flexible statistical tools that couple joint\nconditional and marginal conditional distributions. In a linear regression\nsetting with more than one covariate and two dependent outcomes, we propose the\nuse of additive models for conditional bivariate copula models and discuss\ncomputation and model selection tools for performing Bayesian inference. The\nmethod is illustrated using simulations and a real example.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 16:41:25 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Sabeti", "Avideh", ""], ["Wei", "Mian", ""], ["Craiu", "Radu V.", ""]]}, {"id": "1407.8151", "submitter": "Fabio Cuzzolin", "authors": "Fabio Cuzzolin", "title": "Consistent transformations of belief functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistent belief functions represent collections of coherent or\nnon-contradictory pieces of evidence, but most of all they are the counterparts\nof consistent knowledge bases in belief calculus. The use of consistent\ntransformations cs[.] in a reasoning process to guarantee coherence can\ntherefore be desirable, and generalizes similar techniques in classical logic.\nTransformations can be obtained by minimizing an appropriate distance measure\nbetween the original belief function and the collection of consistent ones. We\nfocus here on the case in which distances are measured using classical Lp\nnorms, in both the \"mass space\" and the \"belief space\" representation of belief\nfunctions. While mass consistent approximations reassign the mass not focussed\non a chosen element of the frame either to the whole frame or to all supersets\nof the element on an equal basis, approximations in the belief space do\ndistinguish these focal elements according to the \"focussed consistent\ntransformation\" principle. The different approximations are interpreted and\ncompared, with the help of examples.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 18:25:31 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Cuzzolin", "Fabio", ""]]}, {"id": "1407.8219", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle", "title": "Detecting duplicates in a homicide registry using a Bayesian\n  partitioning approach", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS779 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2404-2434", "doi": "10.1214/14-AOAS779", "report-no": "IMS-AOAS-AOAS779", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding duplicates in homicide registries is an important step in keeping an\naccurate account of lethal violence. This task is not trivial when unique\nidentifiers of the individuals are not available, and it is especially\nchallenging when records are subject to errors and missing values. Traditional\napproaches to duplicate detection output independent decisions on the\ncoreference status of each pair of records, which often leads to nontransitive\ndecisions that have to be reconciled in some ad-hoc fashion. The task of\nfinding duplicate records in a data file can be alternatively posed as\npartitioning the data file into groups of coreferent records. We present an\napproach that targets this partition of the file as the parameter of interest,\nthereby ensuring transitive decisions. Our Bayesian implementation allows us to\nincorporate prior information on the reliability of the fields in the data\nfile, which is especially useful when no training data are available, and it\nalso provides a proper account of the uncertainty in the duplicate detection\ndecisions. We present a study to detect killings that were reported multiple\ntimes to the United Nations Truth Commission for El Salvador.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 21:21:50 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 10:47:56 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Sadinle", "Mauricio", ""]]}, {"id": "1407.8225", "submitter": "Ryan Martin", "authors": "Chuanhai Liu and Ryan Martin", "title": "Frameworks for prior-free posterior probabilistic inference", "comments": "14 pages, 1 figure; to appear in WIREs Computational Statistics", "journal-ref": "WIREs Computational Statistics, volume 7, pages 77--85, 2015", "doi": "10.1002/wics.1329", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of statistical methods for valid and efficient probabilistic\ninference without prior distributions has a long history. Fisher's fiducial\ninference is perhaps the most famous of these attempts. We argue that, despite\nits seemingly prior-free formulation, fiducial and its various extensions are\nnot prior-free and, therefore, do not meet the requirements for prior-free\nprobabilistic inference. In contrast, the inferential model (IM) framework is\ngenuinely prior-free and is shown to be a promising new method for generating\nboth valid and efficient probabilistic inference. With a brief introduction to\nthe two fundamental principles, namely, the validity and efficiency principles,\nthe three-step construction of the basic IM framework is discussed in the\ncontext of the validity principle. Efficient IM methods, based on conditioning\nand marginalization are illustrated with two benchmark examples, namely, the\nbivariate normal with unknown correlation coefficient and the Behrens--Fisher\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 21:51:27 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Liu", "Chuanhai", ""], ["Martin", "Ryan", ""]]}, {"id": "1407.8430", "submitter": "Ioanna Manolopoulou", "authors": "P. Richard Hahn, Jared S. Murray and Ioanna Manolopoulou", "title": "A Bayesian partial identification approach to inferring the prevalence\n  of accounting misconduct", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the use of flexible Bayesian regression models for\nestimating a partially identified probability function. Our approach permits\nefficient sensitivity analysis concerning the posterior impact of priors on the\npartially identified component of the regression model. The new methodology is\nillustrated on an important problem where only partially observed data is\navailable - inferring the prevalence of accounting misconduct among publicly\ntraded U.S. businesses.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 14:24:11 GMT"}, {"version": "v2", "created": "Fri, 1 Aug 2014 16:54:05 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 23:20:39 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Hahn", "P. Richard", ""], ["Murray", "Jared S.", ""], ["Manolopoulou", "Ioanna", ""]]}]