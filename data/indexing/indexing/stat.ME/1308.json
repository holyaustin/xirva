[{"id": "1308.0049", "submitter": "Won Chang", "authors": "Won Chang, Murali Haran, Roman Olson and Klaus Keller", "title": "A composite likelihood approach to computer model calibration using\n  high-dimensional spatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models are used to model complex processes in various disciplines.\nOften, a key source of uncertainty in the behavior of complex computer models\nis uncertainty due to unknown model input parameters. Statistical computer\nmodel calibration is the process of inferring model parameter values, along\nwith associated uncertainties, from observations of the physical process and\nfrom model outputs at various parameter settings. Observations and model\noutputs are often in the form of high-dimensional spatial fields, especially in\nthe environmental sciences. Sound statistical inference may be computationally\nchallenging in such situations. Here we introduce a composite likelihood-based\napproach to perform computer model calibration with high-dimensional spatial\ndata. While composite likelihood has been studied extensively in the context of\nspatial statistics, computer model calibration using composite likelihood poses\nseveral new challenges. We propose a computationally efficient approach for\nBayesian computer model calibration using composite likelihood. We also develop\na methodology based on asymptotic theory for adjusting the composite likelihood\nposterior distribution so that it accurately represents posterior\nuncertainties. We study the application of our new approach in the context of\ncalibration for a climate model.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 22:16:39 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Chang", "Won", ""], ["Haran", "Murali", ""], ["Olson", "Roman", ""], ["Keller", "Klaus", ""]]}, {"id": "1308.0248", "submitter": "Rui Gon\\c{c}alves", "authors": "Rui Gon\\c{c}alves", "title": "Nearly Gaussian random variables and application to meteorology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a nearly Gaussian random variable $X$ (see \\cite{Lefebvre}) that,\nafter a power transformation, the variable $X^c$ where $c=\\{(2k+1)/(2j+1)\\}$,\n$k,j=\\{0,1,\\dots\\}$, is approximately Gaussian. This transformation is useful\nto model errors in temperature forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 15:45:18 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Gon\u00e7alves", "Rui", ""]]}, {"id": "1308.0641", "submitter": "Subhadeep Mukhopadhyay", "authors": "Emanuel Parzen, Subhadeep Mukhopadhyay", "title": "United Statistical Algorithm, Small and Big Data: Future OF Statistician", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides the role of big idea statisticians in future of Big\nData Science. We describe the `United Statistical Algorithms' framework for\ncomprehensive unification of traditional and novel statistical methods for\nmodeling Small Data and Big Data, especially mixed data (discrete, continuous).\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 23:54:44 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Parzen", "Emanuel", ""], ["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1308.0642", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay and Emanuel Parzen", "title": "Nonlinear Time Series Modeling: A Unified Perspective, Algorithm, and\n  Application", "comments": "Major restructuring has been done", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new comprehensive approach to nonlinear time series analysis and modeling\nis developed in the present paper. We introduce novel data-specific\nmid-distribution based Legendre Polynomial (LP) like nonlinear transformations\nof the original time series Y(t) that enables us to adapt all the existing\nstationary linear Gaussian time series modeling strategy and made it applicable\nfor non-Gaussian and nonlinear processes in a robust fashion. The emphasis of\nthe present paper is on empirical time series modeling via the algorithm\nLPTime. We demonstrate the effectiveness of our theoretical framework using\ndaily S&P 500 return data between Jan/2/1963 - Dec/31/2009. Our proposed LPTime\nalgorithm systematically discovers all the `stylized facts' of the financial\ntime series automatically all at once, which were previously noted by many\nresearchers one at a time.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 00:04:00 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2013 18:59:35 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 23:03:40 GMT"}, {"version": "v4", "created": "Sun, 24 Dec 2017 01:40:19 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Parzen", "Emanuel", ""]]}, {"id": "1308.0657", "submitter": "Alireza Mahani", "authors": "Alireza S. Mahani, Mansour T.A. Sharabiani", "title": "Metropolis-Hastings Sampling Using Multivariate Gaussian Tangents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MH-MGT, a multivariate technique for sampling from\ntwice-differentiable, log-concave probability density functions. MH-MGT is\nMetropolis-Hastings sampling using asymmetric, multivariate Gaussian proposal\nfunctions constructed from Taylor-series expansion of the log-density function.\nThe mean of the Gaussian proposal function represents the full Newton step, and\nthus MH-MGT is the stochastic counterpart to Newton optimization. Convergence\nanalysis shows that MH-MGT is well suited for sampling from\ncomputationally-expensive log-densities with contributions from many\nindependent observations. We apply the technique to Gibbs sampling analysis of\na Hierarchical Bayesian marketing effectiveness model built for a large US\nfoodservice distributor. Compared to univariate slice sampling, MH-MGT shows 6x\nimprovement in sampling efficiency, measured in terms of `function evaluation\nequivalents per independent sample'. To facilitate wide applicability of MH-MGT\nto statistical models, we prove that log-concavity of a twice-differentiable\ndistribution is invariant with respect to 'linear-projection' transformations\nincluding, but not restricted to, generalized linear models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 03:34:03 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Mahani", "Alireza S.", ""], ["Sharabiani", "Mansour T. A.", ""]]}, {"id": "1308.0691", "submitter": "Pasquale Erto", "authors": "Pasquale Erto, Giuliana Pallotta and Christina M. Mastrangelo", "title": "A semi-empirical Bayesian chart to monitor Weibull percentiles", "comments": "21 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a Bayesian control chart for the percentiles of the\nWeibull distribution, when both its in-control and out-of-control parameters\nare unknown. The Bayesian approach enhances parameter estimates for small\nsample sizes that occur when monitoring rare events as in high-reliability\napplications or genetic mutations. The chart monitors the parameters of the\nWeibull distribution directly, instead of transforming the data as most\nWeibull-based charts do in order to comply with their normality assumption. The\nchart uses the whole accumulated knowledge resulting from the likelihood of the\ncurrent sample combined with the information given by both the initial prior\nknowledge and all the past samples. The chart is adapting since its control\nlimits change (e.g. narrow) during the Phase I. An example is presented and\ngood Average Run Length properties are demonstrated. In addition, the paper\ngives insights into the nature of monitoring Weibull processes by highlighting\nthe relationship between distribution and process parameters.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 12:39:00 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Erto", "Pasquale", ""], ["Pallotta", "Giuliana", ""], ["Mastrangelo", "Christina M.", ""]]}, {"id": "1308.0764", "submitter": "Rajarshi Mukherjee", "authors": "Rajarshi Mukherjee, Natesh S. Pillai, Xihong Lin", "title": "Hypothesis testing for high-dimensional sparse binary regression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1279 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 352-381", "doi": "10.1214/14-AOS1279", "report-no": "IMS-AOS-AOS1279", "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the detection boundary for minimax hypothesis testing\nin the context of high-dimensional, sparse binary regression models. Motivated\nby genetic sequencing association studies for rare variant effects, we\ninvestigate the complexity of the hypothesis testing problem when the design\nmatrix is sparse. We observe a new phenomenon in the behavior of detection\nboundary which does not occur in the case of Gaussian linear regression. We\nderive the detection boundary as a function of two components: a design matrix\nsparsity index and signal strength, each of which is a function of the sparsity\nof the alternative. For any alternative, if the design matrix sparsity index is\ntoo high, any test is asymptotically powerless irrespective of the magnitude of\nsignal strength. For binary design matrices with the sparsity index that is not\ntoo high, our results are parallel to those in the Gaussian case. In this\ncontext, we derive detection boundaries for both dense and sparse regimes. For\nthe dense regime, we show that the generalized likelihood ratio is rate\noptimal; for the sparse regime, we propose an extended Higher Criticism Test\nand show it is rate optimal and sharp. We illustrate the finite sample\nproperties of the theoretical results using simulation studies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 01:06:15 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 02:42:31 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 10:30:23 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Mukherjee", "Rajarshi", ""], ["Pillai", "Natesh S.", ""], ["Lin", "Xihong", ""]]}, {"id": "1308.0777", "submitter": "James D. Wilson", "authors": "James D. Wilson, Simi Wang, Peter J. Mucha, Shankar Bhamidi, Andrew B.\n  Nobel", "title": "A testing based extraction algorithm for identifying significant\n  communities in networks", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS760 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1853-1891", "doi": "10.1214/14-AOAS760", "report-no": "IMS-AOAS-AOAS760", "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common and important problem arising in the study of networks is how to\ndivide the vertices of a given network into one or more groups, called\ncommunities, in such a way that vertices of the same community are more\ninterconnected than vertices belonging to different ones. We propose and\ninvestigate a testing based community detection procedure called Extraction of\nStatistically Significant Communities (ESSC). The ESSC procedure is based on\n$p$-values for the strength of connection between a single vertex and a set of\nvertices under a reference distribution derived from a conditional\nconfiguration network model. The procedure automatically selects both the\nnumber of communities in the network and their size. Moreover, ESSC can handle\noverlapping communities and, unlike the majority of existing methods,\nidentifies \"background\" vertices that do not belong to a well-defined\ncommunity. The method has only one parameter, which controls the stringency of\nthe hypothesis tests. We investigate the performance and potential use of ESSC\nand compare it with a number of existing methods, through a validation study\nusing four real network data sets. In addition, we carry out a simulation study\nto assess the effectiveness of ESSC in networks with various types of community\nstructure, including networks with overlapping communities and those with\nbackground vertices. These results suggest that ESSC is an effective\nexploratory tool for the discovery of relevant community structure in complex\nnetwork systems. Data and software are available at\n\\urlhttp://www.unc.edu/~jameswd/research.html.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 04:40:26 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2013 22:14:51 GMT"}, {"version": "v3", "created": "Thu, 19 Jun 2014 20:52:37 GMT"}, {"version": "v4", "created": "Wed, 3 Dec 2014 10:32:06 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Wilson", "James D.", ""], ["Wang", "Simi", ""], ["Mucha", "Peter J.", ""], ["Bhamidi", "Shankar", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1308.0868", "submitter": "John Aston", "authors": "Pantelis Z. Hadjipantelis, John A. D. Aston, Hans-Georg M\\\"uller and\n  Jonathan P. Evans", "title": "Unifying Amplitude and Phase Analysis: A Compositional Data Approach to\n  Functional Multivariate Mixed-Effects Modeling of Mandarin Chinese", "comments": "49 pages, 13 figures, small changes to discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mandarin Chinese is characterized by being a tonal language; the pitch (or\n$F_0$) of its utterances carries considerable linguistic information. However,\nspeech samples from different individuals are subject to changes in amplitude\nand phase which must be accounted for in any analysis which attempts to provide\na linguistically meaningful description of the language. A joint model for\namplitude, phase and duration is presented which combines elements from\nFunctional Data Analysis, Compositional Data Analysis and Linear Mixed Effects\nModels. By decomposing functions via a functional principal component analysis,\nand connecting registration functions to compositional data analysis, a joint\nmultivariate mixed effect model can be formulated which gives insights into the\nrelationship between the different modes of variation as well as their\ndependence on linguistic and non-linguistic covariates. The model is applied to\nthe COSPRO-1 data set, a comprehensive database of spoken Taiwanese Mandarin,\ncontaining approximately 50 thousand phonetically diverse sample $F_0$ contours\n(syllables), and reveals that phonetic information is jointly carried by both\namplitude and phase variation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 01:35:13 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2013 13:05:09 GMT"}, {"version": "v3", "created": "Wed, 24 Sep 2014 05:31:28 GMT"}, {"version": "v4", "created": "Sun, 28 Dec 2014 09:24:47 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Hadjipantelis", "Pantelis Z.", ""], ["Aston", "John A. D.", ""], ["M\u00fcller", "Hans-Georg", ""], ["Evans", "Jonathan P.", ""]]}, {"id": "1308.1114", "submitter": "Noel Erp van", "authors": "H.R.N. van Erp, R.O. Linger, P.H.A.J.M. van Gelder", "title": "Deriving Proper Uniform Priors for Regression Coefficients, Part II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a relatively well-known fact that in problems of Bayesian model\nselection improper priors should, in general, be avoided. In this paper we\nderive a proper and parsimonious uniform prior for regression coefficients. We\nthen use this prior to derive the corresponding evidence values of the\nregression models under consideration. By way of these evidence values one may\nproceed to compute the posterior probabilities of the competing regression\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 20:40:25 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["van Erp", "H. R. N.", ""], ["Linger", "R. O.", ""], ["van Gelder", "P. H. A. J. M.", ""]]}, {"id": "1308.1136", "submitter": "Vinayak Rao", "authors": "Vinayak Rao, Ryan P. Adams and David B. Dunson", "title": "Bayesian inference for Mat\\'ern repulsive processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications involving point pattern data, the Poisson process\nassumption is unrealistic, with the data exhibiting a more regular spread. Such\na repulsion between events is exhibited by trees for example, because of\ncompetition for light and nutrients. Other examples include the locations of\nbiological cells and cities, and the times of neuronal spikes. Given the many\napplications of repulsive point processes, there is a surprisingly limited\nliterature developing flexible, realistic and interpretable models, as well as\nefficient inferential methods. We address this gap by developing a modelling\nframework around the Mat\\'ern type-III repulsive process. We consider a number\nof extensions of the original Mat\\'ern type-III process for both the\nhomogeneous and inhomogeneous cases. We also derive the probability density of\nthis generalized Mat\\'ern process. This allows us to characterize the posterior\ndistribution of the various latent variables, and leads to a novel and\nefficient Markov chain Monte Carlo algorithm. We apply our ideas to datasets\ninvolving the spatial locations of trees, nerve fiber cells and Greyhound bus\nstations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 22:51:27 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 22:10:33 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Rao", "Vinayak", ""], ["Adams", "Ryan P.", ""], ["Dunson", "David B.", ""]]}, {"id": "1308.1237", "submitter": "Stefan Fremdt", "authors": "Stefan Fremdt", "title": "Page's Sequential Procedure for Change-Point Detection in Time Series\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of different settings cumulative sum (CUSUM) procedures have\nbeen applied for the sequential detection of structural breaks in the\nparameters of stochastic models. Yet their performance depends strongly on the\ntime of change and is best under early-change scenarios. For later changes\ntheir finite sample behavior is rather questionable. We therefore propose\nmodified CUSUM procedures for the detection of abrupt changes in the regression\nparameter of multiple time series regression models, that show a higher\nstability with respect to the time of change than ordinary CUSUM procedures.\nThe asymptotic distributions of the test statistics and the consistency of the\nprocedures are provided. In a simulation study it is shown that the proposed\nprocedures behave well in finite samples. Finally the procedures are applied to\na set of capital asset pricing data related to the Fama-French extension of the\ncapital asset pricing model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 11:08:40 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Fremdt", "Stefan", ""]]}, {"id": "1308.1241", "submitter": "Stefan Fremdt", "authors": "Stefan Fremdt", "title": "Asymptotic Distribution of the Delay Time in Page's Sequential Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the asymptotic distribution of the stopping time in Page's\nsequential cumulative sum (CUSUM) procedure is presented. Page as well as\nordinary cumulative sums are considered as detectors for changes in the mean of\nobservations satisfying a weak invariance principle. The main results on the\nstopping times derived from these detectors extend a series of results on the\nasymptotic normality of stopping times of CUSUM-type procedures. In particular\nthe results quantify the superiority of the Page CUSUM procedure to ordinary\nCUSUM procedures in late change scenarios. The theoretical results are\nillustrated by a small simulation study, including a comparison of the\nperformance of ordinary and Page CUSUM detectors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 11:21:33 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Fremdt", "Stefan", ""]]}, {"id": "1308.1313", "submitter": "Georg  Stadler Omar Ghattas", "authors": "Tan Bui-Thanh, Omar Ghattas, James Martin and Georg Stadler", "title": "A computational framework for infinite-dimensional Bayesian inverse\n  problems. Part I: The linearized case, with application to global seismic\n  inversion", "comments": "30 pages; to appear in SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational framework for estimating the uncertainty in the\nnumerical solution of linearized infinite-dimensional statistical inverse\nproblems. We adopt the Bayesian inference formulation: given observational data\nand their uncertainty, the governing forward problem and its uncertainty, and a\nprior probability distribution describing uncertainty in the parameter field,\nfind the posterior probability distribution over the parameter field. The prior\nmust be chosen appropriately in order to guarantee well-posedness of the\ninfinite-dimensional inverse problem and facilitate computation of the\nposterior. Furthermore, straightforward discretizations may not lead to\nconvergent approximations of the infinite-dimensional problem. And finally,\nsolution of the discretized inverse problem via explicit construction of the\ncovariance matrix is prohibitive due to the need to solve the forward problem\nas many times as there are parameters. Our computational framework builds on\nthe infinite-dimensional formulation proposed by Stuart (A. M. Stuart, Inverse\nproblems: A Bayesian perspective, Acta Numerica, 19 (2010), pp. 451-559), and\nincorporates a number of components aimed at ensuring a convergent\ndiscretization of the underlying infinite-dimensional inverse problem. The\nframework additionally incorporates algorithms for manipulating the prior,\nconstructing a low rank approximation of the data-informed component of the\nposterior covariance operator, and exploring the posterior that together ensure\nscalability of the entire framework to very high parameter dimensions. We\ndemonstrate this computational framework on the Bayesian solution of an inverse\nproblem in 3D global seismic wave propagation with hundreds of thousands of\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 15:39:11 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Bui-Thanh", "Tan", ""], ["Ghattas", "Omar", ""], ["Martin", "James", ""], ["Stadler", "Georg", ""]]}, {"id": "1308.1359", "submitter": "David Ginsbourger", "authors": "David Ginsbourger (IMSV), Olivier Roustant (- M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques, DEMO-ENSMSE), Nicolas\n  Durrande", "title": "Invariances of random fields paths, with applications in Gaussian\n  Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study pathwise invariances of centred random fields that can be controlled\nthrough the covariance. A result involving composition operators is obtained in\nsecond-order settings, and we show that various path properties including\nadditivity boil down to invariances of the covariance kernel. These results are\nextended to a broader class of operators in the Gaussian case, via the Lo\\`eve\nisometry. Several covariance-driven pathwise invariances are illustrated,\nincluding fields with symmetric paths, centred paths, harmonic paths, or sparse\npaths. The proposed approach delivers a number of promising results and\nperspectives in Gaussian process regression.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 17:56:52 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Ginsbourger", "David", "", "IMSV"], ["Roustant", "Olivier", "", "- M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques, DEMO-ENSMSE"], ["Durrande", "Nicolas", ""]]}, {"id": "1308.1559", "submitter": "Ruth Heller", "authors": "Ruth Heller, Yair Heller, Shachar Kaufman, and Malka Gorfine", "title": "Consistent distribution-free tests of association between univariate\n  random variables", "comments": "The paper has been withdrawn, since we submitted a new manuscript\n  arXiv:1410.6758 that includes this work but is far more general, thus it\n  included also many new results, and therefore should be read instead of this\n  work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing whether pairs of univariate random\nvariables are associated. Few tests of independence exist that are consistent\nagainst all dependent alternatives and are distribution free. We propose novel\ntests that are consistent, distribution free, and have excellent power\nproperties. The tests have simple form, and are surprisingly computationally\nefficient thanks to accompanying innovative algorithms we develop. Moreover, we\nshow that one of the test statistics is a consistent estimator of the mutual\ninformation. We demonstrate the good power properties in simulations, and apply\nthe tests to a microarray study where many pairs of genes are examined\nsimultaneously for co-dependence.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 13:05:35 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2014 11:05:41 GMT"}, {"version": "v3", "created": "Fri, 14 Mar 2014 07:59:13 GMT"}, {"version": "v4", "created": "Mon, 8 Dec 2014 09:44:06 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Heller", "Ruth", ""], ["Heller", "Yair", ""], ["Kaufman", "Shachar", ""], ["Gorfine", "Malka", ""]]}, {"id": "1308.1822", "submitter": "Huijun Jiang", "authors": "Huijun Jiang, Mingfeng Pu, Zhonghuai Hou", "title": "An Efficient Self-optimized Sampling Method for Rare Events in\n  Nonequilibrium Systems", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s11426-013-5009-3", "report-no": null, "categories": "stat.ME cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare events such as nucleation processes are of ubiquitous importance in real\nsystems. The most popular method for nonequilibrium systems, forward flux\nsampling (FFS), samples rare events by using interfaces to partition the whole\ntransition process into sequence of steps along an order parameter connecting\nthe initial and final states. FFS usually suffers from two main difficulties:\nlow computational efficiency due to bad interface locations and even being not\napplicable when trapping into unknown intermediate metastable states. In the\npresent work, we propose an approach to overcome these difficulties, by\nself-adaptively locating the interfaces on the fly in an optimized manner.\nContrary to the conventional FFS which set the interfaces with euqal distance\nof the order parameter, our approach determines the interfaces with equal\ntransition probability which is shown to satisfy the optimization condition.\nThis is done by firstly running long local trajectories starting from the\ncurrent interface $\\l_i$ to get the conditional probability distribution $P_c$,\nand then determining $\\l_{i+1}$ by equalling $P_c$ to a give value $p_0$. With\nthese optimized interfaces, FFS can be run in a much efficient way. In\naddition, our approach can conveniently find the intermediate metastable states\nby monitoring some special long trajectories that nither end at the initial\nstate nor reach the next interface, the number of which will increase sharply\nfrom zero if such metastable states are encountered. We apply our approach to a\nmodel two-state system and a two-dimensional lattice gas Ising model. Our\napproach is shown to be much more efficient than the conventional FFS method\nwithout losing accuracy, and it can also well reproduce the two-step nucleation\nscenario of the Ising model with easy identification of the intermidiate\nmetastable state.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 11:48:16 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Jiang", "Huijun", ""], ["Pu", "Mingfeng", ""], ["Hou", "Zhonghuai", ""]]}, {"id": "1308.2329", "submitter": "Maxwell Grazier G'Sell", "authors": "Max Grazier G'Sell, Shai S. Shen-Orr, Robert Tibshirani", "title": "Sensitivity Analysis for Inference with Partially Identifiable\n  Covariance Matrices", "comments": "19 pages, 8 figures. Submitted to Computational Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some multivariate problems with missing data, pairs of variables exist\nthat are never observed together. For example, some modern biological tools can\nproduce data of this form. As a result of this structure, the covariance matrix\nis only partially identifiable, and point estimation requires that identifying\nassumptions be made. These assumptions can introduce an unknown and potentially\nlarge bias into the inference. This paper presents a method based on\nsemidefinite programming for automatically quantifying this potential bias by\ncomputing the range of possible equal-likelihood inferred values for convex\nfunctions of the covariance matrix. We focus on the bias of missing value\nimputation via conditional expectation and show that our method can give an\naccurate assessment of the true error in cases where estimates based on\nsampling uncertainty alone are overly optimistic.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2013 18:24:51 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["G'Sell", "Max Grazier", ""], ["Shen-Orr", "Shai S.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1308.2403", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "CDfdr: A Comparison Density Approach to Local False Discovery Rate\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efron et al. (2001) proposed empirical Bayes formulation of the frequentist\nBenjamini and Hochbergs False Discovery Rate method (Benjamini and\nHochberg,1995). This article attempts to unify the `two cultures' using\nconcepts of comparison density and distribution function. We have also shown\nhow almost all of the existing local fdr methods can be viewed as proposing\nvarious model specification for comparison density - unifies the vast\nliterature of false discovery methods under one concept and notation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 15:46:36 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1308.2719", "submitter": "Trevor Hastie", "authors": "Michael Lim and Trevor Hastie", "title": "Learning interactions through hierarchical group-lasso regularization", "comments": "35 pages, about 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for learning pairwise interactions in a manner that\nsatisfies strong hierarchy: whenever an interaction is estimated to be nonzero,\nboth its associated main effects are also included in the model. We motivate\nour approach by modeling pairwise interactions for categorical variables with\narbitrary numbers of levels, and then show how we can accommodate continuous\nvariables and mixtures thereof. Our approach allows us to dispense with\nexplicitly applying constraints on the main effects and interactions for\nidentifiability, which results in interpretable interaction models. We compare\nour method with existing approaches on both simulated and real data, including\na genome wide association study, all using our R package glinternet.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 23:46:13 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Lim", "Michael", ""], ["Hastie", "Trevor", ""]]}, {"id": "1308.2771", "submitter": "Nicolas Stadler N.S.", "authors": "Nicolas St\\\"adler and Sach Mukherjee", "title": "Network-based multivariate gene-set testing", "comments": "23 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of predefined groups of genes (\"gene-sets\") which are\ndifferentially expressed between two conditions (\"gene-set analysis\", or GSA)\nis a very popular analysis in bioinformatics. GSA incorporates biological\nknowledge by aggregating over genes that are believed to be functionally\nrelated. This can enhance statistical power over analyses that consider only\none gene at a time. However, currently available GSA approaches are all based\non univariate two-sample comparison of single genes. This means that they\ncannot test for differences in covariance structure between the two conditions.\nYet interplay between genes is a central aspect of biological investigation and\nit is likely that such interplay may differ between conditions. This paper\nproposes a novel approach for gene-set analysis that allows for truly\nmultivariate hypotheses, in particular differences in gene-gene networks\nbetween conditions. Testing hypotheses concerning networks is challenging due\nthe nature of the underlying estimation problem. Our starting point is a\nrecent, general approach for high-dimensional two-sample testing. We refine the\napproach and show how it can be used to perform multivariate, network-based\ngene-set testing. We validate the approach in simulated examples and show\nresults using high-throughput data from several studies in cancer biology.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 07:12:58 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["St\u00e4dler", "Nicolas", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1308.2791", "submitter": "Nicholas Lewis", "authors": "Nicholas Lewis", "title": "Modification of Bayesian Updating where Continuous Parameters have\n  Differing Relationships with New and Existing Data", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian analyses are often performed using so-called noninformative priors,\nwith a view to achieving objective inference about unknown parameters on which\navailable data depends. Noninformative priors depend on the relationship of the\ndata to the parameters over the sample space. Combining Bayesian updating -\nmultiplying an existing posterior density for parameters being estimated by a\nlikelihood function derived from independent new data that depend on those\nparameters and renormalizing - with use of noninformative priors gives rise to\ninconsistency where existing and new data depend on continuous parameters in\ndifferent ways. In such cases, noninformative priors for inference from only\nthe existing and from only the new data would differ, so Bayesian updating\nwould give different final posterior densities depending on which set of data\nwas used to derive an initial posterior and which was used to update that\nposterior. I propose a revised Bayesian updating method, which resolves this\ninconsistency by updating the prior as well as the likelihood function, and\ninvolves only a single application of Bayes' theorem. The revised method is\nalso applicable where actual prior information as to parameter values exists\nand inference that objectively reflects the existing information as well as new\ndata is sought. I demonstrate by numerical testing the probability-matching\nsuperiority of the proposed revised updating method, in two cases.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 09:08:10 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Lewis", "Nicholas", ""]]}, {"id": "1308.3015", "submitter": "Nisar Ahmed", "authors": "Nisar Ahmed, Tsung-Lin Yang, Mark Campbell", "title": "On Generalized Bayesian Data Fusion with Complex Models in Large Scale\n  Networks", "comments": "Revised version of paper submitted to 2013 Workshop on Wireless\n  Intelligent Sensor Networks (WISeNET 2013) at Duke University, June 5, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.SY stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in communications, mobile computing, and artificial\nintelligence have greatly expanded the application space of intelligent\ndistributed sensor networks. This in turn motivates the development of\ngeneralized Bayesian decentralized data fusion (DDF) algorithms for robust and\nefficient information sharing among autonomous agents using probabilistic\nbelief models. However, DDF is significantly challenging to implement for\ngeneral real-world applications requiring the use of dynamic/ad hoc network\ntopologies and complex belief models, such as Gaussian mixtures or hybrid\nBayesian networks. To tackle these issues, we first discuss some new key\nmathematical insights about exact DDF and conservative approximations to DDF.\nThese insights are then used to develop novel generalized DDF algorithms for\ncomplex beliefs based on mixture pdfs and conditional factors. Numerical\nexamples motivated by multi-robot target search demonstrate that our methods\nlead to significantly better fusion results, and thus have great potential to\nenhance distributed intelligent reasoning in sensor networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 02:30:40 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Ahmed", "Nisar", ""], ["Yang", "Tsung-Lin", ""], ["Campbell", "Mark", ""]]}, {"id": "1308.3020", "submitter": "Jonathan Taylor", "authors": "Jonathan Taylor and Joshua Loftus and Ryan Tibshirani", "title": "Tests in adaptive regression via the Kac-Rice formula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an exact p-value for testing a global null hypothesis in a general\nadaptive regression problem. The general approach uses the Kac-Rice formula, as\ndescribed in (Adler & Taylor 2007). The resulting formula is exact in finite\nsamples, requiring only Gaussianity of the errors. We apply the formula to the\nlasso, group lasso, and principal components and matrix completion problems. In\nthe case of the lasso, the new test relates closely to the recently proposed\ncovariance test of Lockhart et al. (2013).\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 03:01:05 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2013 22:48:10 GMT"}, {"version": "v3", "created": "Thu, 15 May 2014 23:11:25 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Taylor", "Jonathan", ""], ["Loftus", "Joshua", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1308.3201", "submitter": "Ulrike Schneider", "authors": "Ulrike Schneider", "title": "Confidence Sets Based on Thresholding Estimators in High-Dimensional\n  Gaussian Regression Models", "comments": "Section 1 and 2 rewritten, small numerical study added, minor\n  corrections", "journal-ref": "Economet. Rev. 35 (2016), 1412-1455", "doi": "10.1080/07474938.2015.1092798", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study confidence intervals based on hard-thresholding, soft-thresholding,\nand adaptive soft-thresholding in a linear regression model where the number of\nregressors $k$ may depend on and diverge with sample size $n$. In addition to\nthe case of known error variance, we define and study versions of the\nestimators when the error variance is unknown. In the known variance case, we\nprovide an exact analysis of the coverage properties of such intervals in\nfinite samples. We show that these intervals are always larger than the\nstandard interval based on the least-squares estimator. Asymptotically, the\nintervals based on the thresholding estimators are larger even by an order of\nmagnitude when the estimators are tuned to perform consistent variable\nselection. For the unknown-variance case, we provide non-trivial lower bounds\nfor the coverage probabilities in finite samples and conduct an asymptotic\nanalysis where the results from the known-variance case can be shown to carry\nover asymptotically if the number of degrees of freedom $n-k$ tends to infinity\nfast enough in relation to the thresholding parameter.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 18:25:49 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 15:00:44 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Schneider", "Ulrike", ""]]}, {"id": "1308.3416", "submitter": "Yixin Fang", "authors": "Yixin Fang, Binhuan Wang, and Yang Feng", "title": "Tuning Parameter Selection in Regularized Estimations of Large\n  Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently many regularized estimators of large covariance matrices have been\nproposed, and the tuning parameters in these estimators are usually selected\nvia cross-validation. However, there is no guideline on the number of folds for\nconducting cross-validation and there is no comparison between cross-validation\nand the methods based on bootstrap. Through extensive simulations, we suggest\n10-fold cross-validation (nine-tenths for training and one-tenth for\nvalidation) be appropriate when the estimation accuracy is measured in the\nFrobenius norm, while 2-fold cross-validation (half for training and half for\nvalidation) or reverse 3-fold cross-validation (one-third for training and\ntwo-thirds for validation) be appropriate in the operator norm. We also suggest\nthe \"optimal\" cross-validation be more appropriate than the methods based on\nbootstrap for both types of norm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 14:38:12 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Fang", "Yixin", ""], ["Wang", "Binhuan", ""], ["Feng", "Yang", ""]]}, {"id": "1308.3467", "submitter": "Artur Lemonte", "authors": "Tiago M. Vargas, Silvia L.P. Ferrari, Artur J. Lemonte", "title": "Improved likelihood inference in generalized linear models", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of performing testing inference in generalized linear\nmodels when the sample size is small. This class of models provides a\nstraightforward way of modeling normal and non-normal data and has been widely\nused in several practical situations. The likelihood ratio, Wald and score\nstatistics, and the recently proposed gradient statistic provide the basis for\ntesting inference on the parameters in these models. We focus on the\nsmall-sample case, where the reference chi-squared distribution gives a poor\napproximation to the true null distribution of these test statistics. We derive\na general Bartlett-type correction factor in matrix notation for the gradient\ntest which reduces the size distortion of the test, and numerically compare the\nproposed test with the usual likelihood ratio, Wald, score and gradient tests,\nand with the Bartlett-corrected likelihood ratio and score tests. Our\nsimulation results suggest that the corrected test we propose can be an\ninteresting alternative to the other tests since it leads to very accurate\ninference even for very small samples. We also present an empirical application\nfor illustrative purposes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 18:04:46 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Vargas", "Tiago M.", ""], ["Ferrari", "Silvia L. P.", ""], ["Lemonte", "Artur J.", ""]]}, {"id": "1308.3568", "submitter": "Camille Charbonnier", "authors": "Camille Charbonnier, Nicolas Verzelen (MISTEA), Fanny Villers (LPMA)", "title": "A Global Homogeneity Test for High-Dimensional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by the comparison of genetic networks based on\nmicroarray samples. The aim is to test whether the differences observed between\ntwo inferred Gaussian graphical models come from real differences or arise from\nestimation uncertainties. Adopting a neighborhood approach, we consider a\ntwo-sample linear regression model with random design and propose a procedure\nto test whether these two regressions are the same. Relying on multiple testing\nand variable selection strategies, we develop a testing procedure that applies\nto high-dimensional settings where the number of covariates $p$ is larger than\nthe number of observations $n_1$ and $n_2$ of the two samples. Both type I and\ntype II errors are explicitely controlled from a non-asymptotic perspective and\nthe test is proved to be minimax adaptive to the sparsity. The performances of\nthe test are evaluated on simulated data. Moreover, we illustrate how this\nprocedure can be used to compare genetic networks on Hess \\emph{et al} breast\ncancer microarray dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 07:39:52 GMT"}, {"version": "v2", "created": "Thu, 19 Jun 2014 19:10:35 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Charbonnier", "Camille", "", "MISTEA"], ["Verzelen", "Nicolas", "", "MISTEA"], ["Villers", "Fanny", "", "LPMA"]]}, {"id": "1308.3600", "submitter": "Jens Malmros", "authors": "Jens Malmros, Naoki Masuda, and Tom Britton", "title": "Random Walks on Directed Networks: Inference and Respondent-driven\n  Sampling", "comments": "31 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent driven sampling (RDS) is a method often used to estimate\npopulation properties (e.g. sexual risk behavior) in hard-to-reach populations.\nIt combines an effective modified snowball sampling methodology with an\nestimation procedure that yields unbiased population estimates under the\nassumption that the sampling process behaves like a random walk on the social\nnetwork of the population. Current RDS estimation methodology assumes that the\nsocial network is undirected, i.e. that all edges are reciprocal. However,\nempirical social networks in general also have non-reciprocated edges. To\naccount for this fact, we develop a new estimation method for RDS in the\npresence of directed edges on the basis of random walks on directed networks.\nWe distinguish directed and undirected edges and consider the possibility that\nthe random walk returns to its current position in two steps through an\nundirected edge. We derive estimators of the selection probabilities of\nindividuals as a function of the number of outgoing edges of sampled\nindividuals. We evaluate the performance of the proposed estimators on\nartificial and empirical networks to show that they generally perform better\nthan existing methods. This is in particular the case when the fraction of\ndirected edges in the network is large.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 11:00:23 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Malmros", "Jens", ""], ["Masuda", "Naoki", ""], ["Britton", "Tom", ""]]}, {"id": "1308.3915", "submitter": "Suprateek Kundu", "authors": "Suprateek Kundu, Veera Baladandayuthapani and Bani K. Mallick", "title": "Bayes Regularized Graphical Model Estimation in High Dimensions", "comments": "42 Pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an intense development of Bayes graphical model estimation\napproaches over the past decade - however, most of the existing methods are\nrestricted to moderate dimensions. We propose a novel approach suitable for\nhigh dimensional settings, by decoupling model fitting and covariance\nselection. First, a full model based on a complete graph is fit under novel\nclass of continuous shrinkage priors on the precision matrix elements, which\ninduces shrinkage under an equivalence with Cholesky-based regularization while\nenabling conjugate updates of entire precision matrices. Subsequently, we\npropose a post-fitting graphical model estimation step which proceeds using\npenalized joint credible regions to perform neighborhood selection sequentially\nfor each node. The posterior computation proceeds using straightforward fully\nGibbs sampling, and the approach is scalable to high dimensions. The proposed\napproach is shown to be asymptotically consistent in estimating the graph\nstructure for fixed $p$ when the truth is a Gaussian graphical model.\nSimulations show that our approach compares favorably with Bayesian competitors\nboth in terms of graphical model estimation and computational efficiency. We\napply our methods to high dimensional gene expression and microRNA datasets in\ncancer genomics.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 03:20:59 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Kundu", "Suprateek", ""], ["Baladandayuthapani", "Veera", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1308.3942", "submitter": "Jialiang Li", "authors": "Ming-Yen Cheng, Toshio Honda, Jialiang Li, Heng Peng", "title": "Nonparametric independence screening and structure identification for\n  ultra-high dimensional longitudinal data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1236 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 5, 1819-1849", "doi": "10.1214/14-AOS1236", "report-no": "IMS-AOS-AOS1236", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra-high dimensional longitudinal data are increasingly common and the\nanalysis is challenging both theoretically and methodologically. We offer a new\nautomatic procedure for finding a sparse semivarying coefficient model, which\nis widely accepted for longitudinal data analysis. Our proposed method first\nreduces the number of covariates to a moderate order by employing a screening\nprocedure, and then identifies both the varying and constant coefficients using\na group SCAD estimator, which is subsequently refined by accounting for the\nwithin-subject correlation. The screening procedure is based on working\nindependence and B-spline marginal models. Under weaker conditions than those\nin the literature, we show that with high probability only irrelevant variables\nwill be screened out, and the number of selected variables can be bounded by a\nmoderate order. This allows the desirable sparsity and oracle properties of the\nsubsequent structure identification step. Note that existing methods require\nsome kind of iterative screening in order to achieve this, thus they demand\nheavy computational effort and consistency is not guaranteed. The refined\nsemivarying coefficient model employs profile least squares, local linear\nsmoothing and nonparametric covariance estimation, and is semiparametric\nefficient. We also suggest ways to implement the proposed methods, and to\nselect the tuning parameters. An extensive simulation study is summarized to\ndemonstrate its finite sample performance and the yeast cell cycle data is\nanalyzed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 07:16:25 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2014 03:24:36 GMT"}, {"version": "v3", "created": "Tue, 23 Sep 2014 13:31:46 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Cheng", "Ming-Yen", ""], ["Honda", "Toshio", ""], ["Li", "Jialiang", ""], ["Peng", "Heng", ""]]}, {"id": "1308.3968", "submitter": "Heather Battey", "authors": "Heather Battey and Han Liu", "title": "Smooth projected density estimation", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyse a new nonparametric estimator of a multi-dimensional\ndensity. Our smooth projection estimator (SPE) is defined by a least squares\nprojection of the sample onto an infinite dimensional mixture class via an\nundersmoothed nonparametric pilot estimate, which acts as a structural filter\nto regularise the solution. The undersmoothing is required to optimise the\nconvergence rate of the SPE, which is jointly determined by that of the pilot\nestimator to the true density in squared $\\mathbb{L}_{2}$ norm, and by that of\nthe pilot distribution function to the empirical distribution function in\nuniform norm. Our procedure was conceived with a view to exploiting well known\nresults in convex analysis and their connection to mixture densities. In the\ncontext of our work, this translates to the observation that the infinite\ndimensional minimisation problem, implicit in the construction of the SPE,\npossesses a solution of dimension at most $n+1$, where $n$ is the sample size.\nThe SPE thus enjoys practical advantages such as computational efficiency, ease\nof storage and rapid evaluation at a new data point.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 09:53:41 GMT"}, {"version": "v2", "created": "Sun, 23 Nov 2014 20:35:49 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Battey", "Heather", ""], ["Liu", "Han", ""]]}, {"id": "1308.4022", "submitter": "Nina Golyandina E.", "authors": "Nina Golyandina, Alex Shlemov", "title": "Variations of singular spectrum analysis for separability improvement:\n  non-orthogonal decompositions of time series", "comments": null, "journal-ref": "Statistics and Its Interface, 2015, V.8, N3, P.277-294", "doi": "10.4310/SII.2015.v8.n3.a3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular spectrum analysis (SSA) as a nonparametric tool for decomposition of\nan observed time series into sum of interpretable components such as trend,\noscillations and noise is considered. The separability of these series\ncomponents by SSA means the possibility of such decomposition. Two variations\nof SSA, which weaken the separability conditions, are proposed. Both proposed\napproaches consider inner products corresponding to oblique coordinate systems\ninstead of the conventional Euclidean inner product. One of the approaches\nperforms iterations to obtain separating inner products. The other method\nchanges contributions of the components by involving the series derivative to\navoid component mixing. Performance of the suggested methods is demonstrated on\nsimulated and real-life data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 13:47:25 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 21:08:50 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Golyandina", "Nina", ""], ["Shlemov", "Alex", ""]]}, {"id": "1308.4084", "submitter": "Alen Alexanderian", "authors": "Alen Alexanderian, Noemi Petra, Georg Stadler, Omar Ghattas", "title": "A-optimal design of experiments for infinite-dimensional Bayesian linear\n  inverse problems with regularized $\\ell_0$-sparsification", "comments": "27 pages, accepted for publication in SIAM Journal on Scientific\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient method for computing A-optimal experimental designs\nfor infinite-dimensional Bayesian linear inverse problems governed by partial\ndifferential equations (PDEs). Specifically, we address the problem of\noptimizing the location of sensors (at which observational data are collected)\nto minimize the uncertainty in the parameters estimated by solving the inverse\nproblem, where the uncertainty is expressed by the trace of the posterior\ncovariance. Computing optimal experimental designs (OEDs) is particularly\nchallenging for inverse problems governed by computationally expensive PDE\nmodels with infinite-dimensional (or, after discretization, high-dimensional)\nparameters. To alleviate the computational cost, we exploit the problem\nstructure and build a low-rank approximation of the parameter-to-observable\nmap, preconditioned with the square root of the prior covariance operator. This\nrelieves our method from expensive PDE solves when evaluating the optimal\nexperimental design objective function and its derivatives. Moreover, we employ\na randomized trace estimator for efficient evaluation of the OED objective\nfunction. We control the sparsity of the sensor configuration by employing a\nsequence of penalty functions that successively approximate the\n$\\ell_0$-\"norm\"; this results in binary designs that characterize optimal\nsensor locations. We present numerical results for inference of the initial\ncondition from spatio-temporal observations in a time-dependent\nadvection-diffusion problem in two and three space dimensions. We find that an\noptimal design can be computed at a cost, measured in number of forward PDE\nsolves, that is independent of the parameter and sensor dimensions. We\ndemonstrate numerically that $\\ell_0$-sparsified experimental designs obtained\nvia a continuation method outperform $\\ell_1$-sparsified designs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 17:59:23 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 22:52:32 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Alexanderian", "Alen", ""], ["Petra", "Noemi", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1308.4128", "submitter": "Min Wang", "authors": "Min Wang", "title": "A new three-parameter lifetime distribution and associated inference", "comments": "19 pages, 3 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new three-parameter lifetime distribution is introduced and\nmany of its standard properties are discussed. These include shape of the\nprobability density function, hazard rate function and its shape, quantile\nfunction, limiting distributions of order statistics, and the moments. The\nunknown parameters are estimated by the maximum likelihood estimation\nprocedure. We develop an EM algorithm to find the maximum likelihood estimates\nof the parameters, because they are not available in closed form. The Fisher\ninformation matrix is also obtained and it can be used for constructing the\nasymptotic confidence intervals. Finally, a real-data application is given to\ndemonstrate the performance of the new distribution.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 16:39:40 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Wang", "Min", ""]]}, {"id": "1308.4206", "submitter": "Lingsong Zhang Lingsong Zhang", "authors": "Lingsong Zhang and J. S. Marron and Shu Lu", "title": "Nested Nonnegative Cone Analysis", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the analysis of nonnegative data objects, a novel Nested\nNonnegative Cone Analysis (NNCA) approach is proposed to overcome some\ndrawbacks of existing methods. The application of traditional PCA/SVD method to\nnonnegative data often cause the approximation matrix leave the nonnegative\ncone, which leads to non-interpretable and sometimes nonsensical results. The\nnonnegative matrix factorization (NMF) approach overcomes this issue, however\nthe NMF approximation matrices suffer several drawbacks: 1) the factorization\nmay not be unique, 2) the resulting approximation matrix at a specific rank may\nnot be unique, and 3) the subspaces spanned by the approximation matrices at\ndifferent ranks may not be nested. These drawbacks will cause troubles in\ndetermining the number of components and in multi-scale (in ranks)\ninterpretability. The NNCA approach proposed in this paper naturally generates\na nested structure, and is shown to be unique at each rank. Simulations are\nused in this paper to illustrate the drawbacks of the traditional methods, and\nthe usefulness of the NNCA method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 01:59:49 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 02:50:54 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Zhang", "Lingsong", ""], ["Marron", "J. S.", ""], ["Lu", "Shu", ""]]}, {"id": "1308.4462", "submitter": "Adam Persing", "authors": "Adam Persing and Ajay Jasra", "title": "Twisting the Alive Particle Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on sampling from hidden Markov models (Cappe et al, 2005)\nwhose observations have intractable density functions. We develop a new\nsequential Monte Carlo (Doucet et al, 2000 and Gordon et al, 1993) algorithm\nand a new particle marginal Metropolis-Hastings (Andrieu et al, 2010) algorithm\nfor these purposes. We build from Jasra, et al (2013) and Whiteley, et al\n(2013) to construct the sequential Monte Carlo (SMC) algorithm (which we call\nthe alive twisted particle filter). Like the alive particle filter of Jasra, et\nal (2013), our new SMC algorithm adopts an approximate Bayesian computation\n(Tavare et al, 1997) estimate of the HMM. Our alive twisted particle filter\nalso uses a twisted proposal as in Whiteley, et al (2013) to obtain a\nlow-variance estimate of the HMM normalising constant. We demonstrate via\nnumerical examples that, in some scenarios, this estimate has a much lower\nvariance than that of the estimate obtained via the alive particle filter. The\nlow variance of this normalising constant estimate encourages the\nimplementation of our SMC algorithm within a particle marginal\nMetropolis-Hastings (PMMH) scheme, and we call the resulting methodology\n``alive twisted PMMH''. We numerically demonstrate on a stochastic volatility\nmodel how our alive twisted PMMH can converge faster than the standard alive\nPMMH of Jasra, et al (2013).\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 01:23:52 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Persing", "Adam", ""], ["Jasra", "Ajay", ""]]}, {"id": "1308.4747", "submitter": "Emily B. Fox", "authors": "Emily B. Fox, Michael C. Hughes, Erik B. Sudderth, Michael I. Jordan", "title": "Joint modeling of multiple time series via the beta process with\n  application to motion capture segmentation", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS742 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). arXiv admin note: text\n  overlap with arXiv:1111.4226", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1281-1313", "doi": "10.1214/14-AOAS742", "report-no": "IMS-AOAS-AOAS742", "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric approach to the problem of jointly\nmodeling multiple related time series. Our model discovers a latent set of\ndynamical behaviors shared among the sequences, and segments each time series\ninto regions defined by a subset of these behaviors. Using a beta process\nprior, the size of the behavior set and the sharing pattern are both inferred\nfrom data. We develop Markov chain Monte Carlo (MCMC) methods based on the\nIndian buffet process representation of the predictive distribution of the beta\nprocess. Our MCMC inference algorithm efficiently adds and removes behaviors\nvia novel split-merge moves as well as data-driven birth and death proposals,\navoiding the need to consider a truncated model. We demonstrate promising\nresults on unsupervised segmentation of human motion capture data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 00:52:02 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 05:55:04 GMT"}, {"version": "v3", "created": "Thu, 13 Nov 2014 10:11:57 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Fox", "Emily B.", ""], ["Hughes", "Michael C.", ""], ["Sudderth", "Erik B.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1308.4756", "submitter": "Silvia Montagna", "authors": "Silvia Montagna and Surya T. Tokdar", "title": "Computer emulation with non-stationary Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) models are widely used to emulate propagation\nuncertainty in computer experiments. GP emulation sits comfortably within an\nanalytically tractable Bayesian framework. Apart from propagating uncertainty\nof the input variables, a GP emulator trained on finitely many runs of the\nexperiment also offers error bars for response surface estimates at unseen\ninput values. This helps select future input values where the experiment should\nbe run to minimize the uncertainty in the response surface estimation. However,\ntraditional GP emulators use stationary covariance functions, which perform\npoorly and lead to sub-optimal selection of future input points when the\nresponse surface has sharp local features, such as a jump discontinuity or an\nisolated tall peak. We propose an easily implemented non-stationary GP\nemulator, based on two stationary GPs, one nested into the other, and\ndemonstrate its superior ability in handling local features and selecting\nfuture input points from the boundaries of such features.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 03:29:08 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 15:38:25 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Montagna", "Silvia", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1308.5036", "submitter": "Yang Feng", "authors": "Yang Feng, Tengfei Li, Zhiliang Ying", "title": "Likelihood Adaptively Modified Penalties", "comments": "42 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new family of penalty functions, adaptive to likelihood, is introduced for\nmodel selection in general regression models. It arises naturally through\nassuming certain types of prior distribution on the regression parameters. To\nstudy stability properties of the penalized maximum likelihood estimator, two\ntypes of asymptotic stability are defined. Theoretical properties, including\nthe parameter estimation consistency, model selection consistency, and\nasymptotic stability, are established under suitable regularity conditions. An\nefficient coordinate-descent algorithm is proposed. Simulation results and real\ndata analysis show that the proposed method has competitive performance in\ncomparison with existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 03:30:31 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Feng", "Yang", ""], ["Li", "Tengfei", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1308.5115", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer, Leonhard Held", "title": "Power-law models for infectious disease spread", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS743 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1612-1639", "doi": "10.1214/14-AOAS743", "report-no": "IMS-AOAS-AOAS743", "categories": "stat.ME physics.data-an physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-time human travel behaviour can be described by a power law with\nrespect to distance. We incorporate this information in space-time models for\ninfectious disease surveillance data to better capture the dynamics of disease\nspread. Two previously established model classes are extended, which both\ndecompose disease risk additively into endemic and epidemic components: a\nspatio-temporal point process model for individual-level data and a\nmultivariate time-series model for aggregated count data. In both frameworks, a\npower-law decay of spatial interaction is embedded into the epidemic component\nand estimated jointly with all other unknown parameters using (penalised)\nlikelihood inference. Whereas the power law can be based on Euclidean distance\nin the point process model, a novel formulation is proposed for count data\nwhere the power law depends on the order of the neighbourhood of discrete\nspatial units. The performance of the new approach is investigated by a\nreanalysis of individual cases of invasive meningococcal disease in Germany\n(2002-2008) and count data on influenza in 140 administrative districts of\nSouthern Germany (2001-2008). In both applications, the power law substantially\nimproves model fit and predictions, and is reasonably close to alternative\nqualitative formulations, where distance and order of neighbourhood,\nrespectively, are treated as a factor. Implementation in the R package\nsurveillance allows the approach to be applied in other settings.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 12:45:25 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 15:30:09 GMT"}, {"version": "v3", "created": "Mon, 24 Nov 2014 07:52:37 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Meyer", "Sebastian", ""], ["Held", "Leonhard", ""]]}, {"id": "1308.5390", "submitter": "Yi Yu", "authors": "Yang Feng and Yi Yu", "title": "The restricted consistency property of leave-$n_v$-out cross-validation\n  for high-dimensional variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) methods are popular for selecting the tuning parameter\nin the high-dimensional variable selection problem. We show the mis-alignment\nof the CV is one possible reason of its over-selection behavior. To fix this\nissue, we propose a version of leave-$n_v$-out cross-validation (CV($n_v$)),\nfor selecting the optimal model among the restricted candidate model set for\nhigh-dimensional generalized linear models. By using the same candidate model\nsequence and a proper order of construction sample size $n_c$ in each CV split,\nCV($n_v$) avoids the potential hurdles in developing theoretical properties.\nCV($n_v$) is shown to enjoy the restricted model selection consistency property\nunder mild conditions. Extensive simulations and real data analysis support the\ntheoretical results and demonstrate the performances of CV($n_v$) in terms of\nboth model selection and prediction.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2013 08:58:12 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 16:33:40 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 07:16:30 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Feng", "Yang", ""], ["Yu", "Yi", ""]]}, {"id": "1308.5595", "submitter": "Corwin Zigler", "authors": "Corwin M. Zigler", "title": "The central role of Bayes theorem for joint estimation of causal effects\n  and propensity scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although propensity scores have been central to the estimation of causal\neffects for over 30 years, only recently has the statistical literature begun\nto consider in detail methods for Bayesian estimation of propensity scores and\ncausal effects. Underlying this recent body of literature on Bayesian\npropensity score estimation is an implicit discordance between the goal of the\npropensity score and the use of Bayes theorem. The propensity score condenses\nmultivariate covariate information into a scalar to allow estimation of causal\neffects without specifying a model for how each covariate relates to the\noutcome. Avoiding specification of a detailed model for the outcome response\nsurface is valuable for robust estimation of causal effects, but this strategy\nis at odds with the use of Bayes theorem, which presupposes a full probability\nmodel for the observed data. The goal of this paper is to explicate this\nfundamental feature of Bayesian estimation of causal effects with propensity\nscores in order to provide context for the existing literature and for future\nwork on this important topic.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 14:24:56 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 18:49:21 GMT"}, {"version": "v3", "created": "Tue, 8 Apr 2014 19:36:01 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Zigler", "Corwin M.", ""]]}, {"id": "1308.5736", "submitter": "Lucas Janson", "authors": "Lucas Janson and Bala Rajaratnam", "title": "A Methodology for Robust Multiproxy Paleoclimate Reconstructions and\n  Modeling of Temperature Conditional Quantiles", "comments": null, "journal-ref": "Journal of the American Statistical Association 109 (2014) 63-77", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great strides have been made in the field of reconstructing past temperatures\nbased on models relating temperature to temperature-sensitive paleoclimate\nproxies. One of the goals of such reconstructions is to assess if current\nclimate is anomalous in a millennial context. These regression based approaches\nmodel the conditional mean of the temperature distribution as a function of\npaleoclimate proxies (or vice versa). Some of the recent focus in the area has\nconsidered methods which help reduce the uncertainty inherent in such\nstatistical paleoclimate reconstructions, with the ultimate goal of improving\nthe confidence that can be attached to such endeavors. A second important\nscientific focus in the subject area is the area of forward models for proxies,\nthe goal of which is to understand the way paleoclimate proxies are driven by\ntemperature and other environmental variables. In this paper we introduce novel\nstatistical methodology for (1) quantile regression with autoregressive\nresidual structure, (2) estimation of corresponding model parameters, (3)\ndevelopment of a rigorous framework for specifying uncertainty estimates of\nquantities of interest, yielding (4) statistical byproducts that address the\ntwo scientific foci discussed above. Our statistical methodology demonstrably\nproduces a more robust reconstruction than is possible by using\nconditional-mean-fitting methods. Our reconstruction shares some of the common\nfeatures of past reconstructions, but also gains useful insights. More\nimportantly, we are able to demonstrate a significantly smaller uncertainty\nthan that from previous regression methods. In addition, the quantile\nregression component allows us to model, in a more complete and flexible way\nthan least squares, the conditional distribution of temperature given proxies.\nThis relationship can be used to inform forward models relating how proxies are\ndriven by temperature.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 01:55:17 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Janson", "Lucas", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1308.5836", "submitter": "Roland Langrock", "authors": "Roland Langrock, Th\\'eo Michelot, Alexander Sohn, Thomas Kneib", "title": "Semiparametric stochastic volatility modelling using penalized splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic volatility (SV) models mimic many of the stylized facts attributed\nto time series of asset returns, while maintaining conceptual simplicity. The\ncommonly made assumption of conditionally normally distributed or\nStudent-t-distributed returns, given the volatility, has however been\nquestioned. In this manuscript, we introduce a novel maximum penalized\nlikelihood approach for estimating the conditional distribution in an SV model\nin a nonparametric way, thus avoiding any potentially critical assumptions on\nthe shape. The considered framework exploits the strengths both of the powerful\nhidden Markov model machinery and of penalized B-splines, and constitutes a\npowerful and flexible alternative to recently developed Bayesian approaches to\nsemiparametric SV modelling. We demonstrate the feasibility of the approach in\na simulation study before outlining its potential in applications to three\nseries of returns on stocks and one series of stock index returns.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 12:06:16 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2013 12:29:04 GMT"}, {"version": "v3", "created": "Tue, 17 Jun 2014 21:53:41 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Langrock", "Roland", ""], ["Michelot", "Th\u00e9o", ""], ["Sohn", "Alexander", ""], ["Kneib", "Thomas", ""]]}, {"id": "1308.5865", "submitter": "Pili Hu", "authors": "Pili Hu and Wing Cheong Lau", "title": "A Survey and Taxonomy of Graph Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sampling is a technique to pick a subset of vertices and/ or edges from\noriginal graph. It has a wide spectrum of applications, e.g. survey hidden\npopulation in sociology [54], visualize social graph [29], scale down Internet\nAS graph [27], graph sparsification [8], etc. In some scenarios, the whole\ngraph is known and the purpose of sampling is to obtain a smaller graph. In\nother scenarios, the graph is unknown and sampling is regarded as a way to\nexplore the graph. Commonly used techniques are Vertex Sampling, Edge Sampling\nand Traversal Based Sampling. We provide a taxonomy of different graph sampling\nobjectives and graph sampling approaches. The relations between these\napproaches are formally argued and a general framework to bridge theoretical\nanalysis and practical implementation is provided. Although being smaller in\nsize, sampled graphs may be similar to original graphs in some way. We are\nparticularly interested in what graph properties are preserved given a sampling\nprocedure. If some properties are preserved, we can estimate them on the\nsampled graphs, which gives a way to construct efficient estimators. If one\nalgorithm relies on the perserved properties, we can expect that it gives\nsimilar output on original and sampled graphs. This leads to a systematic way\nto accelerate a class of graph algorithms. In this survey, we discuss both\nclassical text-book type properties and some advanced properties. The landscape\nis tabularized and we see a lot of missing works in this field. Some\ntheoretical studies are collected in this survey and simple extensions are\nmade. Most previous numerical evaluation works come in an ad hoc fashion, i.e.\nevaluate different type of graphs, different set of properties, and different\nsampling algorithms. A systematical and neutral evaluation is needed to shed\nlight on further graph sampling studies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 14:28:06 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Hu", "Pili", ""], ["Lau", "Wing Cheong", ""]]}, {"id": "1308.6013", "submitter": "John Storey", "authors": "Neo Christopher Chung and John D. Storey", "title": "Statistical significance of variables driving systematic variation", "comments": "35 pages, 1 table, 6 main figures, 7 supplementary figures", "journal-ref": "Bioinformatics (2015) 31 (4): 545-554", "doi": "10.1093/bioinformatics/btu674", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a number of well-established methods such as principal components\nanalysis (PCA) for automatically capturing systematic variation due to latent\nvariables in large-scale genomic data. PCA and related methods may directly\nprovide a quantitative characterization of a complex biological variable that\nis otherwise difficult to precisely define or model. An unsolved problem in\nthis context is how to systematically identify the genomic variables that are\ndrivers of systematic variation captured by PCA. Principal components (and\nother estimates of systematic variation) are directly constructed from the\ngenomic variables themselves, making measures of statistical significance\nartificially inflated when using conventional methods due to over-fitting. We\nintroduce a new approach called the jackstraw that allows one to accurately\nidentify genomic variables that are statistically significantly associated with\nany subset or linear combination of principal components (PCs). The proposed\nmethod can greatly simplify complex significance testing problems encountered\nin genomics and can be utilized to identify the genomic variables significantly\nassociated with latent variables. Using simulation, we demonstrate that our\nmethod attains accurate measures of statistical significance over a range of\nrelevant scenarios. We consider yeast cell-cycle gene expression data, and show\nthat the proposed method can be used to straightforwardly identify\nstatistically significant genes that are cell-cycle regulated. We also analyze\ngene expression data from post-trauma patients, allowing the gene expression\ndata to provide a molecularly-driven phenotype. We find a greater enrichment\nfor inflammatory-related gene sets compared to using a clinically defined\nphenotype. The proposed method provides a useful bridge between large-scale\nquantifications of systematic variation and gene-level significance analyses.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 23:37:11 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Chung", "Neo Christopher", ""], ["Storey", "John D.", ""]]}, {"id": "1308.6069", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang, Jin Li", "title": "Compound Poisson Processes, Latent Shrinkage Priors and Bayesian\n  Nonconvex Penalization", "comments": "Published at http://dx.doi.org/10.1214/14-BA892 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 247-274", "doi": "10.1214/14-BA892", "report-no": "VTeX-BA-BA892", "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss Bayesian nonconvex penalization for sparse learning\nproblems. We explore a nonparametric formulation for latent shrinkage\nparameters using subordinators which are one-dimensional L\\'{e}vy processes. We\nparticularly study a family of continuous compound Poisson subordinators and a\nfamily of discrete compound Poisson subordinators. We exemplify four specific\nsubordinators: Gamma, Poisson, negative binomial and squared Bessel\nsubordinators. The Laplace exponents of the subordinators are Bernstein\nfunctions, so they can be used as sparsity-inducing nonconvex penalty\nfunctions. We exploit these subordinators in regression problems, yielding a\nhierarchical model with multiple regularization parameters. We devise ECME\n(Expectation/Conditional Maximization Either) algorithms to simultaneously\nestimate regression coefficients and regularization parameters. The empirical\nevaluation of simulated data shows that our approach is feasible and effective\nin high-dimensional data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 06:05:42 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 08:34:53 GMT"}, {"version": "v3", "created": "Fri, 15 May 2015 06:01:46 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Zhang", "Zhihua", ""], ["Li", "Jin", ""]]}, {"id": "1308.6221", "submitter": "Noemi Petra", "authors": "Noemi Petra, James Martin, Georg Stadler, Omar Ghattas", "title": "A computational framework for infinite-dimensional Bayesian inverse\n  problems: Part II. Stochastic Newton MCMC with application to ice sheet flow\n  inverse problems", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the numerical solution of infinite-dimensional inverse problems in\nthe framework of Bayesian inference. In the Part I companion to this paper\n(arXiv.org:1308.1313), we considered the linearized infinite-dimensional\ninverse problem. Here in Part II, we relax the linearization assumption and\nconsider the fully nonlinear infinite-dimensional inverse problem using a\nMarkov chain Monte Carlo (MCMC) sampling method. To address the challenges of\nsampling high-dimensional pdfs arising from Bayesian inverse problems governed\nby PDEs, we build on the stochastic Newton MCMC method. This method exploits\nproblem structure by taking as a proposal density a local Gaussian\napproximation of the posterior pdf, whose construction is made tractable by\ninvoking a low-rank approximation of its data misfit component of the Hessian.\nHere we introduce an approximation of the stochastic Newton proposal in which\nwe compute the low-rank-based Hessian at just the MAP point, and then reuse\nthis Hessian at each MCMC step. We compare the performance of the proposed\nmethod to the original stochastic Newton MCMC method and to an independence\nsampler. The comparison of the three methods is conducted on a synthetic ice\nsheet inverse problem. For this problem, the stochastic Newton MCMC method with\na MAP-based Hessian converges at least as rapidly as the original stochastic\nNewton MCMC method, but is far cheaper since it avoids recomputing the Hessian\nat each step. On the other hand, it is more expensive per sample than the\nindependence sampler; however, its convergence is significantly more rapid, and\nthus overall it is much cheaper. Finally, we present extensive analysis and\ninterpretation of the posterior distribution, and classify directions in\nparameter space based on the extent to which they are informed by the prior or\nthe observations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 17:14:29 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 15:40:14 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Petra", "Noemi", ""], ["Martin", "James", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1308.6303", "submitter": "Noel Erp van", "authors": "H.R.N. van Erp", "title": "Uncovering the Specific Product Rule for the Lattice of Questions", "comments": "arXiv admin note: text overlap with arXiv:0909.3684, arXiv:1009.5161\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give here the specific product rule for the lattice of questions. This\nproduct rule differs from the product rule for the lattice of statements, hence\nthe qualifier `specific'. This is because the elements in the lattice of\nstatements are ordered by way of implication, an upper context, whereas the\nelements in the lattice of questions are ordered by way of relevancy, a lower\ncontext.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 20:29:44 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2013 15:47:43 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["van Erp", "H. R. N.", ""]]}, {"id": "1308.6315", "submitter": "Paul McNicholas", "authors": "Katherine Morris and Paul D. McNicholas", "title": "Clustering, Classification, Discriminant Analysis, and Dimension\n  Reduction via Generalized Hyperbolic Mixtures", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.10.008", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for dimension reduction with clustering, classification, or\ndiscriminant analysis is introduced. This mixture model-based approach is based\non fitting generalized hyperbolic mixtures on a reduced subspace within the\nparadigm of model-based clustering, classification, or discriminant analysis. A\nreduced subspace of the data is derived by considering the extent to which\ngroup means and group covariances vary. The members of the subspace arise\nthrough linear combinations of the original data, and are ordered by importance\nvia the associated eigenvalues. The observations can be projected onto the\nsubspace, resulting in a set of variables that captures most of the clustering\ninformation available. The use of generalized hyperbolic mixtures gives a\nrobust framework capable of dealing with skewed clusters. Although dimension\nreduction is increasingly in demand across many application areas, the authors\nare most familiar with biological applications and so two of the five real data\nexamples are within that sphere. Simulated data are also used for illustration.\nThe approach introduced herein can be considered the most general such approach\navailable, and so we compare results to three special and limiting cases.\nComparisons with several well established techniques illustrate its promising\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 21:24:50 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 22:35:18 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 22:20:50 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Morris", "Katherine", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1308.6753", "submitter": "Ioannis Ntzoufras", "authors": "Silia Vitoratou and Ioannis Ntzoufras", "title": "Thermodynamic assessment of probability distribution divergencies and\n  Bayesian model comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within path sampling framework, we show that probability distribution\ndivergences, such as the Chernoff information, can be estimated via\nthermodynamic integration. The Boltzmann-Gibbs distribution pertaining to\ndifferent Hamiltonians is implemented to derive tempered transitions along the\npath, linking the distributions of interest at the endpoints. Under this\nperspective, a geometric approach is feasible, which prompts intuition and\nfacilitates tuning the error sources. Additionally, there are direct\napplications in Bayesian model evaluation. Existing marginal likelihood and\nBayes factor estimators are reviewed here along with their stepping-stone\nsampling analogues. New estimators are presented and the use of compound paths\nis introduced.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 14:22:12 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2013 11:09:19 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Vitoratou", "Silia", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "1308.6780", "submitter": "Leonhard Held", "authors": "Leonhard Held, Daniel Saban\\'es Bov\\'e, Isaac Gravestock", "title": "Approximate Bayesian Model Selection with the Deviance Statistic", "comments": "Published at http://dx.doi.org/10.1214/14-STS510 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 242-257", "doi": "10.1214/14-STS510", "report-no": "IMS-STS-STS510", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection poses two main challenges: the specification of\nparameter priors for all models, and the computation of the resulting Bayes\nfactors between models. There is now a large literature on automatic and\nobjective parameter priors in the linear model. One important class are\n$g$-priors, which were recently extended from linear to generalized linear\nmodels (GLMs). We show that the resulting Bayes factors can be approximated by\ntest-based Bayes factors (Johnson [Scand. J. Stat. 35 (2008) 354-368]) using\nthe deviance statistics of the models. To estimate the hyperparameter $g$, we\npropose empirical and fully Bayes approaches and link the former to minimum\nBayes factors and shrinkage estimates from the literature. Furthermore, we\ndescribe how to approximate the corresponding posterior distribution of the\nregression coefficients based on the standard GLM output. We illustrate the\napproach with the development of a clinical prediction model for 30-day\nsurvival in the GUSTO-I trial using logistic regression.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 15:51:43 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 08:20:23 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2015 04:57:55 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Held", "Leonhard", ""], ["Bov\u00e9", "Daniel Saban\u00e9s", ""], ["Gravestock", "Isaac", ""]]}]