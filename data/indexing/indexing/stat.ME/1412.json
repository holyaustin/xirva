[{"id": "1412.0048", "submitter": "Peter D. Hoff", "authors": "Peter D. Hoff", "title": "Multilinear tensor regression for longitudinal relational data", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS839 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1169-1193", "doi": "10.1214/15-AOAS839", "report-no": "IMS-AOAS-AOAS839", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental aspect of relational data, such as from a social network, is\nthe possibility of dependence among the relations. In particular, the relations\nbetween members of one pair of nodes may have an effect on the relations\nbetween members of another pair. This article develops a type of regression\nmodel to estimate such effects in the context of longitudinal and multivariate\nrelational data, or other data that can be represented in the form of a tensor.\nThe model is based on a general multilinear tensor regression model, a special\ncase of which is a tensor autoregression model in which the tensor of relations\nat one time point are parsimoniously regressed on relations from previous time\npoints. This is done via a separable, or Kronecker-structured, regression\nparameter along with a separable covariance model. In the context of an\nanalysis of longitudinal multivariate relational data, it is shown how the\nmultilinear tensor regression model can represent patterns that often appear in\nrelational and network data, such as reciprocity and transitivity.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 23:05:02 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 06:59:50 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1412.0242", "submitter": "Michael Lopez", "authors": "Michael Lopez, Roee Gutman", "title": "Estimating the average treatment effects of nutritional label use using\n  subclassification with regression adjustment", "comments": "Statistical Methods in Medical Research (online first, November 2014)", "journal-ref": null, "doi": "10.1177/0962280214560046", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score methods are common for estimating a binary treatment effect\nwhen treatment assignment is not randomized. When exposure is measured on an\nordinal scale (i.e., low - medium - high), however, propensity score inference\nrequires extensions which have received limited attention. Estimands of\npossible interest with an ordinal exposure are the average treatment effects\nbetween each pair of exposure levels. Using these estimands, it is possible to\ndetermine an optimal exposure level. Traditional methods, including\ndichotomization of the exposure or a series of binary propensity score\ncomparisons across exposure pairs, are generally inadequate for identification\nof optimal levels.We combine subclassification with regression adjustment to\nestimate transitive, unbiased average causal effects across an ordered\nexposure, and apply our method on the 2005-06 National Health and Nutrition\nExamination Survey to estimate the effects of nutritional label use on body\nmass index.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 16:48:08 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Lopez", "Michael", ""], ["Gutman", "Roee", ""]]}, {"id": "1412.0367", "submitter": "Valerie Poynor", "authors": "Valerie Poynor and Athanasios Kottas", "title": "Bayesian nonparametric modeling for mean residual life regression", "comments": "arXiv admin note: text overlap with arXiv:1411.7481", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean residual life function is a key functional for a survival\ndistribution. It has practically useful interpretation as the expected\nremaining lifetime given survival up to a particular time point, and it also\ncharacterizes the survival distribution. However, it has received limited\nattention in terms of inference methods under a probabilistic modeling\nframework. In this paper, we seek to provide general inference methodology for\nmean residual life regression. Survival data often include a set of predictor\nvariables for the survival response distribution, and in many cases it is\nnatural to include the covariates as random variables into the modeling. We\nthus propose a Dirichlet process mixture modeling approach for the joint\nstochastic mechanism of the covariates and survival responses. This approach\nimplies a flexible model structure for the mean residual life of the\nconditional response distribution, allowing general shapes for mean residual\nlife as a function of covariates given a specific time point, as well as a\nfunction of time given particular values of the covariate vector. To expand the\nscope of the modeling framework, we extend the mixture model to incorporate\ndependence across experimental groups, such as treatment and control groups.\nThis extension is built from a dependent Dirichlet process prior for the\ngroup-specific mixing distributions, with common locations and weights that\nvary across groups through latent bivariate beta distributed random variables.\nWe develop properties of the proposed regression models, and discuss methods\nfor prior specification and posterior inference. The different components of\nthe methodology are illustrated with simulated data sets. Moreover, the\nmodeling approach is applied to a data set comprising right censored survival\ntimes of patients with small cell lung cancer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 07:43:37 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 22:44:36 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Poynor", "Valerie", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1412.0561", "submitter": "Joshua Habiger", "authors": "Joshua D Habiger", "title": "Multiple Test Functions and Adjusted p-Values for Test Statistics with\n  Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized $p$-value, (nonrandomized) mid-$p$-value and abstract\nrandomized $p$-value have all been recommended for testing a null hypothesis\nwhenever the test statistic has a discrete distribution. This paper provides a\nunifying framework for these approaches and extends it to the multiple testing\nsetting. In particular, multiplicity adjusted versions of the aforementioned\n$p$-values and multiple test functions are developed. It is demonstrated that,\nwhenever the usual nonrandomized and randomized decisions to reject or retain\nthe null hypothesis may differ, the (adjusted) abstract randomized $p$-value\nand test function should be reported, especially when the number of tests is\nlarge. It is shown that the proposed approach dominates the traditional\nrandomized and nonrandomized approaches in terms of bias and variability. Tools\nfor plotting adjusted abstract randomized $p$-values and for computing multiple\ntest functions are developed. Examples are used to illustrate the method and to\nmotivate a new type of multiplicity adjusted mid-$p$-value.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 17:43:15 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Habiger", "Joshua D", ""]]}, {"id": "1412.0645", "submitter": "Joshua Habiger D", "authors": "Joshua D. Habiger", "title": "Adaptive False Discovery Rate Control for Heterogeneous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efforts to develop more efficient multiple hypothesis testing procedures for\nfalse discovery rate (FDR) control have focused on incorporating an estimate of\nthe proportion of true null hypotheses (such procedures are called adaptive) or\nexploiting heterogeneity across tests via some optimal weighting scheme. This\npaper combines these approaches using a weighted adaptive multiple decision\nfunction (WAMDF) framework. Optimal weights for a flexible random effects model\nare derived and a WAMDF that controls the FDR for arbitrary weighting schemes\nwhen test statistics are independent under the null hypotheses is given.\nAsymptotic and numerical assessment reveals that, under weak dependence, the\nproposed WAMDFs provide more efficient FDR control even if optimal weights are\nmisspecified. The robustness and flexibility of the proposed methodology\nfacilitates the development of more efficient, yet practical, FDR procedures\nfor heterogeneous data. To illustrate, two different weighted adaptive FDR\nmethods for heterogeneous sample sizes are developed and applied to data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 20:54:01 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 15:54:26 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Habiger", "Joshua D.", ""]]}, {"id": "1412.0753", "submitter": "Gourab Mukherjee", "authors": "Peter Radchenko, Gourab Mukherjee", "title": "Convex clustering via $\\ell_1$ fusion penalization", "comments": "final journal version with supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the large sample behavior of a convex clustering framework, which\nminimizes the sample within cluster sum of squares under an~$\\ell_1$ fusion\nconstraint on the cluster centroids. This recently proposed approach has been\ngaining in popularity, however, its asymptotic properties have remained mostly\nunknown. Our analysis is based on a novel representation of the sample\nclustering procedure as a sequence of cluster splits determined by a sequence\nof maximization problems. We use this representation to provide a simple and\nintuitive formulation for the population clustering procedure. We then\ndemonstrate that the sample procedure consistently estimates its population\nanalog, and derive the corresponding rates of convergence. The proof conducts a\ncareful simultaneous analysis of a collection of M-estimation problems, whose\ncardinality grows together with the sample size. Based on the new perspectives\ngained from the asymptotic investigation, we propose a key post-processing\nmodification of the original clustering framework. We show, both theoretically\nand empirically, that the resulting approach can be successfully used to\nestimate the number of clusters in the population. Using simulated data, we\ncompare the proposed method with existing number of clusters and modality\nassessment approaches, and obtain encouraging results. We also demonstrate the\napplicability of our clustering method for the detection of cellular\nsubpopulations in a single-cell virology study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 01:10:11 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 03:30:15 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 01:26:51 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2016 20:46:40 GMT"}, {"version": "v5", "created": "Wed, 28 Dec 2016 01:42:44 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Radchenko", "Peter", ""], ["Mukherjee", "Gourab", ""]]}, {"id": "1412.0778", "submitter": "Philip Reiss", "authors": "Philip T. Reiss, Lei Huang, Huaihou Chen, and Stan Colcombe", "title": "Varying-smoother models for functional responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies estimation of a smooth function $f(t,s)$ when we are given\nfunctional responses of the form $f(t,\\cdot)$ + error, but scientific interest\ncenters on the collection of functions $f(\\cdot,s)$ for different $s$. The\nmotivation comes from studies of human brain development, in which $t$ denotes\nage whereas $s$ refers to brain locations. Analogously to varying-coefficient\nmodels, in which the mean response is linear in $t$, the \"varying-smoother\"\nmodels that we consider exhibit nonlinear dependence on $t$ that varies\nsmoothly with $s$. We discuss three approaches to estimating varying-smoother\nmodels: (a) methods that employ a tensor product penalty; (b) an approach based\non smoothed functional principal component scores; and (c) two-step methods\nconsisting of an initial smooth with respect to $t$ at each $s$, followed by a\npostprocessing step. For the first approach, we derive an exact expression for\na penalty proposed by Wood, and an adaptive penalty that allows smoothness to\nvary more flexibly with $s$. We also develop \"pointwise degrees of freedom,\" a\nnew tool for studying the complexity of estimates of $f(\\cdot,s)$ at each $s$.\nThe three approaches to varying-smoother models are compared in simulations and\nwith a diffusion tensor imaging data set.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 04:08:52 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Reiss", "Philip T.", ""], ["Huang", "Lei", ""], ["Chen", "Huaihou", ""], ["Colcombe", "Stan", ""]]}, {"id": "1412.0838", "submitter": "Anne Sabourin", "authors": "Anne Sabourin (LTCI)", "title": "Semi-parametric modeling of excesses above high multivariate thresholds\n  with censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to include censored data in a statistical analysis is a recur-rent issue\nin statistics. In multivariate extremes, the dependence structure of large\nobservations can be characterized in terms of a non parametric angular measure,\nwhile marginal excesses above asymptotically large thresholds have a parametric\ndistribution. In this work, a flexible semi-parametric Dirichlet mix-ture model\nfor angular measures is adapted to the context of censored data and missing\ncomponents. One major issue is to take into account censoring intervals\noverlapping the extremal threshold, without knowing whether the correspond-ing\nhidden data is actually extreme. Further, the censored likelihood needed for\nBayesian inference has no analytic expression. The first issue is tackled using\na Poisson process model for extremes, whereas a data augmentation scheme avoids\nmultivariate integration of the Poisson process intensity over both the\ncensored intervals and the failure region above threshold. The implemented MCMC\nalgorithm allows simultaneous estimation of marginal and dependence parameters,\nso that all sources of uncertainty other than model bias are cap-tured by\nposterior credible intervals. The method is illustrated on simulated and real\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 10:11:49 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Sabourin", "Anne", "", "LTCI"]]}, {"id": "1412.0948", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "Copulas from Order Statistics", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of copulas based on order statistics was introduced by Baker\n(2008). Here, further properties of the bivariate and multivariate copulas are\ndescribed, such as that of likelihood ratio dominance (LRD), and further\nbivariate copulas are introduced that generalize the earlier work. One of the\nnew copulas is an integral of a product of Bessel functions of imaginary\nargument, and can attain the Fr\\'echet bound. The use of these copulas for\nfitting data is described, and illustrated with examples. It was found\nempirically that the multivariate copulas previously proposed are not flexible\nenough to be generally useful in data fitting, and further development is\nneeded in this area.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 15:46:10 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "1412.0952", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "Application of some new heavy-tailed survival distributions", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some new survival distributions are introduced based on a generalised\nexponential function. This class of distributions includes heavy-tailed\ngeneralisations of exponential, Weibull and gamma distributions. Properties of\nthe distributions are described, and R code is available for computation of\npdf, quantiles, inverse quantiles, random numbers, etc. A use of these\ndistributions for robust inference is suggested, and this is exemplified with a\nMonte-Carlo study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 15:56:23 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "1412.1181", "submitter": "Vered Madar", "authors": "Vered Madar", "title": "Direct formulation to Cholesky decomposition of a general nonsingular\n  correlation matrix", "comments": "Accepted to Statistics and Probability Letters, March 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two novel, explicit representations of Cholesky factor of a\nnonsingular correlation matrix. The first representation uses semi-partial\ncorrelation coefficients as its entries. The second, uses an equivalent form of\nthe square roots of the differences between two ratios of successive\ndeterminants. Each of the two new forms enjoys parsimony of notations and\noffers a simpler alternative to both spherical factorization and the\nmultiplicative partial correlation Cholesky matrix (Cooke et al 2011). Two\nrelevant applications are offered for each form: a simple $t$-test for\nassessing the independence of a single variable in a multivariate normal\nstructure, and a straightforward algorithm for generating random\npositive-definite correlation matrix. The second representation is also\nextended to any nonsingular hermitian matrix.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 04:08:14 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 00:57:44 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2015 17:21:21 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Madar", "Vered", ""]]}, {"id": "1412.1344", "submitter": "Fouedjio Francky", "authors": "Francky Fouedjio, Nicolas Desassis, Thomas Romary", "title": "Estimation of Space Deformation Model for Non-stationary Random\n  Functions", "comments": "17 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary Random Functions have been successfully applied in geostatistical\napplications for decades. In some instances, the assumption of a homogeneous\nspatial dependence structure across the entire domain of interest is\nunrealistic. A practical approach for modelling and estimating non-stationary\nspatial dependence structure is considered. This consists in transforming a\nnon-stationary Random Function into a stationary and isotropic one via a\nbijective continuous deformation of the index space. So far, this approach has\nbeen successfully applied in the context of data from several independent\nrealizations of a Random Function. In this work, we propose an approach for\nnon-stationary geostatistical modelling using space deformation in the context\nof a single realization with possibly irregularly spaced data. The estimation\nmethod is based on a non-stationary variogram kernel estimator which serves as\na dissimilarity measure between two locations in the geographical space. The\nproposed procedure combines aspects of kernel smoothing, weighted non-metric\nmulti-dimensional scaling and thin-plate spline radial basis functions. On a\nsimulated data, the method is able to retrieve the true deformation.\nPerformances are assessed on both synthetic and real datasets. It is shown in\nparticular that our approach outperforms the stationary approach. Beyond the\nprediction, the proposed method can also serve as a tool for exploratory\nanalysis of the non-stationarity.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 14:42:13 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Fouedjio", "Francky", ""], ["Desassis", "Nicolas", ""], ["Romary", "Thomas", ""]]}, {"id": "1412.1373", "submitter": "Fouedjio Francky", "authors": "Francky Fouedjio, Nicolas Desassis, Jacques Rivoirard", "title": "A Generalized Convolution Model and Estimation for Non-stationary Random\n  Functions", "comments": "24 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard geostatistical models assume second order stationarity of the\nunderlying Random Function. In some instances, there is little reason to expect\nthe spatial dependence structure to be stationary over the whole region of\ninterest. In this paper, we introduce a new model for second order\nnon-stationary Random Functions as a convolution of an orthogonal random\nmeasure with a spatially varying random weighting function. This new model is a\ngeneralization of the common convolution model where a non-random weighting\nfunction is used. The resulting class of non-stationary covariance functions is\nvery general, flexible and allows to retrieve classes of closed-form\nnon-stationary covariance functions known from the literature, for a suitable\nchoices of the random weighting functions family. Under the framework of a\nsingle realization and local stationarity, we develop parameter inference\nprocedure of these explicit classes of non-stationary covariance functions.\nFrom a local variogram non-parametric kernel estimator, a weighted local\nleast-squares approach in combination with kernel smoothing method is developed\nto estimate the parameters. Performances are assessed on two real datasets:\nsoil and rainfall data. It is shown in particular that the proposed approach\noutperforms the stationary one, according to several criteria. Beyond the\nspatial predictions, we also show how conditional simulations can be carried\nout in this non-stationary framework.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 15:47:34 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Fouedjio", "Francky", ""], ["Desassis", "Nicolas", ""], ["Rivoirard", "Jacques", ""]]}, {"id": "1412.1392", "submitter": "John Harlim", "authors": "John Harlim, Hoon Hong, and Jacob L. Robbins", "title": "An algebraic method for constructing stable and consistent\n  autoregressive filters", "comments": "10 figures", "journal-ref": "J. Comput. Phys. 283, 241-257, 2015", "doi": "10.1016/j.jcp.2014.12.004", "report-no": null, "categories": "stat.ME math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an algebraic method to construct stable and\nconsistent univariate autoregressive (AR) models of low order for filtering and\npredicting nonlinear turbulent signals with memory depth. By stable, we refer\nto the classical stability condition for the AR model. By consistent, we refer\nto the classical consistency constraints of Adams-Bashforth methods of\norder-two. One attractive feature of this algebraic method is that the model\nparameters can be obtained without directly knowing any training data set as\nopposed to many standard, regression-based parameterization methods. It takes\nonly long-time average statistics as inputs. The proposed method provides a\ndiscretization time step interval which guarantees the existence of stable and\nconsistent AR model and simultaneously produces the parameters for the AR\nmodels. In our numerical examples with two chaotic time series with different\ncharacteristics of decaying time scales, we find that the proposed AR models\nproduce significantly more accurate short-term predictive skill and comparable\nfiltering skill relative to the linear regression-based AR models. These\nencouraging results are robust across wide ranges of discretization times,\nobservation times, and observation noise variances. Finally, we also find that\nthe proposed model produces an improved short-time prediction relative to the\nlinear regression-based AR-models in forecasting a data set that characterizes\nthe variability of the Madden-Julian Oscillation, a dominant tropical\natmospheric wave pattern.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 12:56:57 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Harlim", "John", ""], ["Hong", "Hoon", ""], ["Robbins", "Jacob L.", ""]]}, {"id": "1412.1394", "submitter": "Peter Bubenik", "authors": "Violeta Kovacev-Nikolic, Peter Bubenik, Dragan Nikoli\\'c, Giseon Heo", "title": "Using persistent homology and dynamical distances to analyze protein\n  binding", "comments": "27 pages, various improvements based on referees' comments", "journal-ref": "Statistical Applications in Genetics and Molecular Biology.\n  January 2016, Volume 15, Issue 1, Pages 19-38", "doi": "10.1515/sagmb-2015-0057", "report-no": null, "categories": "stat.ME math.AT q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology captures the evolution of topological features of a model\nas a parameter changes. The most commonly used summary statistics of persistent\nhomology are the barcode and the persistence diagram. Another summary\nstatistic, the persistence landscape, was recently introduced by Bubenik. It is\na functional summary, so it is easy to calculate sample means and variances,\nand it is straightforward to construct various test statistics. Implementing a\npermutation test we detect conformational changes between closed and open forms\nof the maltose-binding protein, a large biomolecule consisting of 370 amino\nacid residues. Furthermore, persistence landscapes can be applied to machine\nlearning methods. A hyperplane from a support vector machine shows the clear\nseparation between the closed and open proteins conformations. Moreover,\nbecause our approach captures dynamical properties of the protein our results\nmay help in identifying residues susceptible to ligand binding; we show that\nthe majority of active site residues and allosteric pathway residues are\nlocated in the vicinity of the most persistent loop in the corresponding\nfiltered Vietoris-Rips complex. This finding was not observed in the classical\nanisotropic network model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 16:43:02 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 21:39:04 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Kovacev-Nikolic", "Violeta", ""], ["Bubenik", "Peter", ""], ["Nikoli\u0107", "Dragan", ""], ["Heo", "Giseon", ""]]}, {"id": "1412.1414", "submitter": "Matthias De Lozzo", "authors": "Matthias De Lozzo (DER), Amandine Marrel (DER)", "title": "New improvements in the use of dependence measures for sensitivity\n  analysis and screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical phenomena are commonly modeled by numerical simulators. Such codes\ncan take as input a high number of uncertain parameters and it is important to\nidentify their influences via a global sensitivity analysis (GSA). However,\nthese codes can be time consuming which prevents a GSA based on the classical\nSobol' indices, requiring too many simulations. This is especially true as the\nnumber of inputs is important. To address this limitation, we consider recent\nadvances in dependence measures, focusing on the distance correlation and the\nHilbert-Schmidt independence criterion (HSIC). Our objective is to study these\nindices and use them for a screening purpose. Numerical tests reveal some\ndifferences between dependence measures and classical Sobol' indices, and\npreliminary answers to \"What sensitivity indices to what situation?\" are\nderived. Then, two approaches are proposed to use the dependence measures for a\nscreening purpose. The first one directly uses these indices with independence\ntests; asymptotic tests and their spectral extensions exist and are detailed.\nFor a higher accuracy in presence of small samples, we propose a non-asymptotic\nversion based on bootstrap sampling. The second approach is based on a linear\nmodel associating two simulations, which explains their output difference as a\nweighed sum of their input differences. From this, a bootstrap method is\nproposed for the selection of the influential inputs. We also propose a\nheuristic approach for the calibration of the HSIC Lasso method. Numerical\nexperiments are performed and show the potential of these approaches for\nscreening when many inputs are not influential.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 17:48:00 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["De Lozzo", "Matthias", "", "DER"], ["Marrel", "Amandine", "", "DER"]]}, {"id": "1412.1530", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Strength of Connections in a Random Graph: Definition, Characterization,\n  and Estimation", "comments": "18 pages, 6 Figures. Third version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can the `affinity' or `strength' of ties of a random graph be\ncharacterized and compactly represented? How can concepts like Fourier and\ninverse-Fourier like transform be developed for graph data? To do so, we\nintroduce a new graph-theoretic function called `Graph Correlation Density\nField' (or in short GraField), which differs from the traditional edge\nprobability density-based approaches, to completely characterize tie-strength\nbetween graph nodes. Our approach further allows frequency domain analysis,\napplicable for both directed and undirected random graphs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 01:08:39 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 00:40:50 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2015 22:29:27 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1412.1553", "submitter": "Li-Xin Zhang", "authors": "Li-Xin Zhang", "title": "Response-adaptive randomization: an overview of designs and asymptotic\n  theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an overview of important research works on\nresponse-adaptive randomization completed in the past decades.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 04:38:55 GMT"}, {"version": "v2", "created": "Sat, 31 Jan 2015 14:44:51 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Zhang", "Li-Xin", ""]]}, {"id": "1412.1559", "submitter": "Qing Zhou", "authors": "Yuliya Marchetti and Qing Zhou", "title": "Iterative Subsampling in Solution Path Clustering of Noisy Big Data", "comments": "17 pages, 7 figures", "journal-ref": "Statistics and Its Interface, 9: 415-431 (2016)", "doi": "10.4310/SII.2016.v9.n4.a2", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an iterative subsampling approach to improve the computational\nefficiency of our previous work on solution path clustering (SPC). The SPC\nmethod achieves clustering by concave regularization on the pairwise distances\nbetween cluster centers. This clustering method has the important capability to\nrecognize noise and to provide a short path of clustering solutions; however,\nit is not sufficiently fast for big datasets. Thus, we propose a method that\niterates between clustering a small subsample of the full data and sequentially\nassigning the other data points to attain orders of magnitude of computational\nsavings. The new method preserves the ability to isolate noise, includes a\nsolution selection mechanism that ultimately provides one clustering solution\nwith an estimated number of clusters, and is shown to be able to extract small\ntight clusters from noisy data. The method's relatively minor losses in\naccuracy are demonstrated through simulation studies, and its ability to handle\nlarge datasets is illustrated through applications to gene expression datasets.\nAn R package, SPClustering, for the SPC method with iterative subsampling is\navailable at http://www.stat.ucla.edu/~zhou/Software.html.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 06:05:59 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 19:09:58 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Marchetti", "Yuliya", ""], ["Zhou", "Qing", ""]]}, {"id": "1412.1597", "submitter": "Iain Johnston", "authors": "Iain G. Johnston, Benjamin C. Rickett, Nick S. Jones", "title": "Explicit tracking of uncertainty increases the power of quantitative\n  rule-of-thumb reasoning in cell biology", "comments": "8 pages, 3 figures", "journal-ref": "Biophys. J. 107 2612 (2014)", "doi": "10.1016/j.bpj.2014.08.040", "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Back-of-the-envelope\" or \"rule-of-thumb\" calculations involving rough\nestimates of quantities play a central scientific role in developing intuition\nabout the structure and behaviour of physical systems, for example in so-called\n`Fermi problems' in the physical sciences. Such calculations can be used to\npowerfully and quantitatively reason about biological systems, particularly at\nthe interface between physics and biology. However, substantial uncertainties\nare often associated with values in cell biology, and performing calculations\nwithout taking this uncertainty into account may limit the extent to which\nresults can be interpreted for a given problem. We present a means to\nfacilitate such calculations where uncertainties are explicitly tracked through\nthe line of reasoning, and introduce a `probabilistic calculator' called\nCaladis, a web tool freely available at www.caladis.org, designed to perform\nthis tracking. This approach allows users to perform more statistically robust\ncalculations in cell biology despite having uncertain values, and to identify\nwhich quantities need to be measured more precisely in order to make confident\nstatements, facilitating efficient experimental design. We illustrate the use\nof our tool for tracking uncertainty in several example biological\ncalculations, showing that the results yield powerful and interpretable\nstatistics on the quantities of interest. We also demonstrate that the outcomes\nof calculations may differ from point estimates when uncertainty is accurately\ntracked. An integral link between Caladis and the Bionumbers repository of\nbiological quantities further facilitates the straightforward location,\nselection, and use of a wealth of experimental data in cell biological\ncalculations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 09:29:48 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Johnston", "Iain G.", ""], ["Rickett", "Benjamin C.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1412.1649", "submitter": "Xuenan Feng", "authors": "Xuenan Feng", "title": "A Class of Conjugate Priors Defined on the Unit Simplex", "comments": "20 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet distribution and Dirichlet process as its infinite dimensional\ngeneralization are primarily used conjugate prior of categorical and\nmultinomial distributions in Bayesian statistics. Extensions have been proposed\nto broaden applications for different purposes. In this article, we explore a\nclass of prior distributions closely related to Dirichlet distribution\nincorporating additional information on the data generating mechanism. Examples\nare given to show potential use of the models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 12:53:28 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Feng", "Xuenan", ""]]}, {"id": "1412.1684", "submitter": "Yang Feng", "authors": "Diego Franco Saldana, Yi Yu, and Yang Feng", "title": "How Many Communities Are There?", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic blockmodels and variants thereof are among the most widely used\napproaches to community detection for social networks and relational data. A\nstochastic blockmodel partitions the nodes of a network into disjoint sets,\ncalled communities. The approach is inherently related to clustering with\nmixture models; and raises a similar model selection problem for the number of\ncommunities. The Bayesian information criterion (BIC) is a popular solution,\nhowever, for stochastic blockmodels, the conditional independence assumption\ngiven the communities of the endpoints among different edges is usually\nviolated in practice. In this regard, we propose composite likelihood BIC\n(CL-BIC) to select the number of communities, and we show it is robust against\npossible misspecifications in the underlying stochastic blockmodel assumptions.\nWe derive the requisite methodology and illustrate the approach using both\nsimulated and real data. Supplementary materials containing the relevant\ncomputer code are available online.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 14:47:47 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 18:56:30 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Saldana", "Diego Franco", ""], ["Yu", "Yi", ""], ["Feng", "Yang", ""]]}, {"id": "1412.1716", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, Larry\n  Wasserman", "title": "Nonparametric modal regression", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1373 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 2, 489-514", "doi": "10.1214/15-AOS1373", "report-no": "IMS-AOS-AOS1373", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modal regression estimates the local modes of the distribution of $Y$ given\n$X=x$, instead of the mean, as in the usual regression sense, and can hence\nreveal important structure missed by usual regression methods. We study a\nsimple nonparametric method for modal regression, based on a kernel density\nestimate (KDE) of the joint distribution of $Y$ and $X$. We derive asymptotic\nerror bounds for this method, and propose techniques for constructing\nconfidence sets and prediction sets. The latter is used to select the smoothing\nbandwidth of the underlying KDE. The idea behind modal regression is connected\nto many others, such as mixture regression and density ridge estimation, and we\ndiscuss these ties as well.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 16:18:42 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 00:50:10 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 11:54:46 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Tibshirani", "Ryan J.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1412.1914", "submitter": "Martin Schlather", "authors": "Martin Schlather", "title": "A parametric variogram model bridging between stationary and\n  intrinsically stationary processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple variogram model with two parameters is presented that includes the\npower variogram for the fractional Brownian motion, a modified De Wijsian\nmodel, the generalized Cauchy model and the multiquadrics model. One parameter\ncontrols the smoothness of the process. The other parameter allows for a smooth\nparametrization between stationary and intrinsically stationary second order\nprocesses in a Gaussian framework, or between mixing and non-ergodic max-stable\nprocesses when modeling spatial extremes by a Brown-Resnick process.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 08:00:09 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Schlather", "Martin", ""]]}, {"id": "1412.1920", "submitter": "Eustasio del Barrio", "authors": "Pedro C. Alvarez-Esteban, Eustasio del Barrio, Juan A. Cuesta-Albertos\n  and Carlos Matran", "title": "A contamination model for approximate stochastic order: extended version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic ordering among distributions has been considered in a variety of\nscenarios. Economic studies often involve research about the ordering of\ninvestment strategies or social welfare. However, as noted in the literature,\nstochastic orderings are often a too strong assumption which is not supported\nby the data even in cases in which the researcher tends to believe that a\ncertain variable is somehow smaller than other. Instead of considering this\nrigid model of stochastic order we propose to look at a more flexible version\nin which two distributions are said to satisfy an approximate stochastic order\nrelation if they are slightly contaminated versions of distributions which do\nsatisfy the stochastic ordering. The minimal level of contamination that makes\nthis approximate model hold can be used as a measure of the deviation of the\noriginal distributions from the exact stochastic order model. Our approach is\nbased on the use of trimmings of probability measures. We discuss the\nconnection between them and the approximate stochastic order model and provide\ntheoretical support for its use in data analysis. We also provide simulation\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 08:47:26 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Alvarez-Esteban", "Pedro C.", ""], ["del Barrio", "Eustasio", ""], ["Cuesta-Albertos", "Juan A.", ""], ["Matran", "Carlos", ""]]}, {"id": "1412.1927", "submitter": "Sylvain Sardy", "authors": "Jairo Diaz-Rodriguez and Sylvain Sardy", "title": "Quantile universal threshold: model selection at the detection edge for\n  high-dimensional linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To estimate a sparse linear model from data with Gaussian noise, consilience\nfrom lasso and compressed sensing literatures is that thresholding estimators\nlike lasso and the Dantzig selector have the ability in some situations to\nidentify with high probability part of the significant covariates\nasymptotically, and are numerically tractable thanks to convexity.\n  Yet, the selection of a threshold parameter $\\lambda$ remains crucial in\npractice. To that aim we propose Quantile Universal Thresholding, a selection\nof $\\lambda$ at the detection edge. We show with extensive simulations and real\ndata that an excellent compromise between high true positive rate and low false\ndiscovery rate is achieved, leading also to good predictive risk.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 09:18:31 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Diaz-Rodriguez", "Jairo", ""], ["Sardy", "Sylvain", ""]]}, {"id": "1412.2041", "submitter": "Daniel Bartz", "authors": "Daniel Bartz and Johannes H\\\"ohne and Klaus-Robert M\\\"uller", "title": "Multi-Target Shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein showed that the multivariate sample mean is outperformed by \"shrinking\"\nto a constant target vector. Ledoit and Wolf extended this approach to the\nsample covariance matrix and proposed a multiple of the identity as shrinkage\ntarget. In a general framework, independent of a specific estimator, we extend\nthe shrinkage concept by allowing simultaneous shrinkage to a set of targets.\nApplication scenarios include settings with (A) additional data sets from\npotentially similar distributions, (B) non-stationarity, (C) a natural grouping\nof the data or (D) multiple alternative estimators which could serve as\ntargets.\n  We show that this Multi-Target Shrinkage can be translated into a quadratic\nprogram and derive conditions under which the estimation of the shrinkage\nintensities yields optimal expected squared error in the limit. For the sample\nmean and the sample covariance as specific instances, we derive conditions\nunder which the optimality of MTS is applicable. We consider two asymptotic\nsettings: the large dimensional limit (LDL), where the dimensionality and the\nnumber of observations go to infinity at the same rate, and the finite\nobservations large dimensional limit (FOLDL), where only the dimensionality\ngoes to infinity while the number of observations remains constant. We then\nshow the effectiveness in extensive simulations and on real world data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 16:02:50 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Bartz", "Daniel", ""], ["H\u00f6hne", "Johannes", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1412.2044", "submitter": "Christian P. Robert", "authors": "Kaniav Kamary (Universit\\'e Paris-Dauphine), Kerrie Mengersen (QUT),\n  Christian P. Robert (Universit\\'e Paris-Dauphine and University of Warwick),\n  and Judith Rousseau (University of Oxford and Universit\\'e Paris-Dauphine)", "title": "Testing hypotheses via a mixture estimation model", "comments": "25 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel paradigm for Bayesian testing of hypotheses and Bayesian\nmodel comparison. Our alternative to the traditional construction of posterior\nprobabilities that a given hypothesis is true or that the data originates from\na specific model is to consider the models under comparison as components of a\nmixture model. We therefore replace the original testing problem with an\nestimation one that focus on the probability weight of a given model within a\nmixture model. We analyze the sensitivity on the resulting posterior\ndistribution on the weights of various prior modeling on the weights. We stress\nthat a major appeal in using this novel perspective is that generic improper\npriors are acceptable, while not putting convergence in jeopardy. Among other\nfeatures, this allows for a resolution of the Lindley-Jeffreys paradox. When\nusing a reference Beta B(a,a) prior on the mixture weights, we note that the\nsensitivity of the posterior estimations of the weights to the choice of a\nvanishes with the sample size increasing and avocate the default choice a=0.5,\nderived from Rousseau and Mengersen (2011). Another feature of this easily\nimplemented alternative to the classical Bayesian solution is that the speeds\nof convergence of the posterior mean of the weight and of the corresponding\nposterior probability are quite similar.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 16:04:03 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 12:47:34 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2018 14:51:04 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Kamary", "Kaniav", "", "Universit\u00e9 Paris-Dauphine"], ["Mengersen", "Kerrie", "", "QUT"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine and University of Warwick"], ["Rousseau", "Judith", "", "University of Oxford and Universit\u00e9 Paris-Dauphine"]]}, {"id": "1412.2048", "submitter": "Krzysztof Fornalski", "authors": "Krzysztof Wojciech Fornalski", "title": "Alternative statistical methods for cytogenetic radiation biological\n  dosimetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.bio-ph physics.data-an physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents alternative statistical methods for biological dosimetry,\nsuch as the Bayesian and Monte Carlo method. The classical Gaussian and robust\nBayesian fit algorithms for the linear, linear-quadratic as well as saturated\nand critical calibration curves are described. The Bayesian model selection\nalgorithm for those curves is also presented. In addition, five methods of dose\nestimation for a mixed neutron and gamma irradiation field were described: two\nclassical methods, two Bayesian methods and one Monte Carlo method. Bayesian\nmethods were also enhanced and generalized for situations with many types of\nmixed radiation. All algorithms were presented in easy-to-use form, which can\nbe applied to any computational programming language. The presented algorithm\nis universal, although it was originally dedicated to cytogenetic biological\ndosimetry of victims of a nuclear reactor accident.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 16:24:32 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Fornalski", "Krzysztof Wojciech", ""]]}, {"id": "1412.2149", "submitter": "Sihai Zhao", "authors": "Sihai Dave Zhao, T. Tony Cai, and Hongzhe Li", "title": "Optimal detection of weak positive latent dependence between two\n  sequences of multiple tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is frequently of interest to jointly analyze two paired sequences of\nmultiple tests. This paper studies the problem of detecting whether there are\nmore pairs of tests that are significant in both sequences than would be\nexpected by chance. The detection boundary is derived in terms of parameters\nsuch as the sparsity of the latent significance indicators of the tests in each\nsequence, the effect sizes of the non-null tests, and the magnitude of the\ndependence between the two sequences. A new test for detecting weak dependence\nis also proposed, shown to be asymptotically adaptively optimal, studied in\nsimulations, and applied to study of genetic pleiotropy in 10 pediatric\nautoimmune diseases.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 21:29:54 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 16:51:52 GMT"}, {"version": "v3", "created": "Fri, 23 Jun 2017 16:37:54 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Zhao", "Sihai Dave", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1412.2150", "submitter": "Shengchun Kong", "authors": "Shengchun Kong and Bin Nan", "title": "Semiparametric Approach for Regression with Covariate Subject to Limit\n  of Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider generalized linear regression analysis with left-censored\ncovariate due to the lower limit of detection. Complete case analysis by\neliminating observations with values below limit of detection yields valid\nestimates for regression coefficients, but loses efficiency; substitution\nmethods are biased; maximum likelihood method relies on parametric models for\nthe unobservable tail probability distribution of such covariate, thus may\nsuffer from model misspecification. To obtain robust and more efficient\nresults, we propose a semiparametric likelihood-based approach for the\nestimation of regression parameters using an accelerated failure time model for\nthe covariate subject to limit of detection. A two-stage estimation procedure\nis considered, where the conditional distribution of the covariate with limit\nof detection given other variables is estimated prior to maximizing the\nlikelihood function. The proposed method outperforms the complete case analysis\nand the substitution methods as well in simulation studies. Technical\nconditions for desirable asymptotic properties are provided.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 21:32:12 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Kong", "Shengchun", ""], ["Nan", "Bin", ""]]}, {"id": "1412.2183", "submitter": "Pengfei Zang", "authors": "Richard A. Davis, Pengfei Zang, Tian Zheng", "title": "Reduced-Rank Covariance Estimation in Vector Autoregressive Modeling", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reduced-rank modeling of the white noise covariance matrix in a\nlarge dimensional vector autoregressive (VAR) model. We first propose the\nreduced-rank covariance estimator under the setting where independent\nobservations are available. We derive the reduced-rank estimator based on a\nlatent variable model for the vector observation and give the analytical form\nof its maximum likelihood estimate. Simulation results show that the\nreduced-rank covariance estimator outperforms two competing covariance\nestimators for estimating large dimensional covariance matrices from\nindependent observations. Then we describe how to integrate the proposed\nreduced-rank estimator into the fitting of large dimensional VAR models, where\nwe consider two scenarios that require different model fitting procedures. In\nthe VAR modeling context, our reduced-rank covariance estimator not only\nprovides interpretable descriptions of the dependence structure of VAR\nprocesses but also leads to improvement in model-fitting and forecasting over\nunrestricted covariance estimators. Two real data examples are presented to\nillustrate these fitting procedures.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 23:53:40 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Davis", "Richard A.", ""], ["Zang", "Pengfei", ""], ["Zheng", "Tian", ""]]}, {"id": "1412.2282", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Jerome P. Reiter, Quanli Wang", "title": "Dirichlet Process Mixture Models for Modeling and Generating Synthetic\n  Versions of Nested Categorical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian model for estimating the joint distribution of\nmultivariate categorical data when units are nested within groups. Such data\narise frequently in social science settings, for example, people living in\nhouseholds. The model assumes that (i) each group is a member of a group-level\nlatent class, and (ii) each unit is a member of a unit-level latent class\nnested within its group-level latent class. This structure allows the model to\ncapture dependence among units in the same group. It also facilitates\nsimultaneous modeling of variables at both group and unit levels. We develop a\nversion of the model that assigns zero probability to groups and units with\nphysically impossible combinations of variables. We apply the model to estimate\nmultivariate relationships in a subset of the American Community Survey. Using\nthe estimated model, we generate synthetic household data that could be\ndisseminated as redacted public use files with high analytic validity and low\ndisclosure risks. Supplementary materials for this article are available\nonline.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 21:15:22 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 01:56:22 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2015 14:00:30 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2015 01:13:47 GMT"}, {"version": "v5", "created": "Fri, 13 May 2016 19:13:05 GMT"}, {"version": "v6", "created": "Fri, 28 Oct 2016 02:43:26 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Hu", "Jingchen", ""], ["Reiter", "Jerome P.", ""], ["Wang", "Quanli", ""]]}, {"id": "1412.2315", "submitter": "Rudolf Beran", "authors": "Rudolf Beran", "title": "Nonparametric Estimation of Trend in Directional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider measured positions of the paleomagnetic north pole over time. Each\nmeasured position may be viewed as a direction, expressed as a unit vector in\nthree dimensions and incorporating some error. In this sequence, the true\ndirections are expected to be close to one another at nearby times. A simple\ntrend estimator that respects the geometry of the sphere is to compute a\nrunning average over the time-ordered observed direction vectors, then\nnormalize these average vectors to unit length. This paper treats a\nconsiderably richer class of competing directional trend estimators that\nrespect spherical geometry. The analysis relies on a nonparametric error model\nfor directional data in $R^q$ that imposes no symmetry or other shape\nrestrictions on the error distributions. Good trend estimators are selected by\ncomparing estimated risks of competing estimators under the error model.\nUniform laws of large numbers, from empirical process theory, establish when\nthese estimated risks are trustworthy surrogates for the corresponding unknown\nrisks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 04:25:50 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Beran", "Rudolf", ""]]}, {"id": "1412.2548", "submitter": "Holger Dette", "authors": "Holger Dette and Viatcheslav B. Melas and Roman Guchenko", "title": "Bayesian T-optimal discriminating designs", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of constructing Bayesian optimal discriminating designs for a\nclass of regression models with respect to the T-optimality criterion\nintroduced by Atkinson and Fedorov (1975a) is considered. It is demonstrated\nthat the discretization of the integral with respect to the prior distribution\nleads to locally T-optimal discrimination designs can only deal with a few\ncomparisons, but the discretization of the Bayesian prior easily yields to\ndiscrimination design problems for more than 100 competing models. A new\nefficient method is developed to deal with problems of this type. It combines\nsome features of the classical exchange type algorithm with the gradient\nmethods. Convergence is proved and it is demonstrated that the new method can\nfind Bayesian optimal discriminating designs in situations where all currently\navailable procedures fail.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 13:05:04 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Dette", "Holger", ""], ["Melas", "Viatcheslav B.", ""], ["Guchenko", "Roman", ""]]}, {"id": "1412.2798", "submitter": "Rikke Ingebrigtsen", "authors": "Rikke Ingebrigtsen, Finn Lindgren, Ingelin Steinsland, Sara Martino", "title": "Estimation of a non-stationary model for annual precipitation in\n  southern Norway using replicates of the spatial field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of stationary dependence structure parameters using only a single\nrealisation of the spatial process, typically leads to inaccurate estimates and\npoorly identified parameters. A common way to handle this is to fix some of the\nparameters, or within the Bayesian framework, impose prior knowledge. In many\napplied settings, stationary models are not flexible enough to model the\nprocess of interest, thus non-stationary spatial models are used. However, more\nflexible models usually means more parameters, and the identifiability problem\nbecomes even more challenging. We investigate aspects of estimation of a\nBayesian non-stationary spatial model for annual precipitation using\nobservations from multiple years. The model contains replicates of the spatial\nfield, which increases precision of the estimates and makes them less prior\nsensitive. Using R-INLA, we analyse precipitation data from southern Norway,\nand investigate statistical properties of the replicate model in a simulation\nstudy. The non-stationary spatial model we explore belongs to a recently\nintroduced class of stochastic partial differential equation (SPDE) based\nspatial models. This model class allows for non-stationary models with\nexplanatory variables in the dependence structure. We derive conditions to\nfacilitate prior specification for these types of non-stationary spatial\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 22:18:46 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 08:21:22 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Ingebrigtsen", "Rikke", ""], ["Lindgren", "Finn", ""], ["Steinsland", "Ingelin", ""], ["Martino", "Sara", ""]]}, {"id": "1412.2851", "submitter": "Vikram Garg PhD.", "authors": "Vikram V. Garg, Luis Tenorio, Karen Willcox", "title": "Minimum Local Distance Density Estimation", "comments": null, "journal-ref": null, "doi": "10.1080/03610926.2014.988260", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a local density estimator based on first order statistics. To\nestimate the density at a point, $x$, the original sample is divided into\nsubsets and the average minimum sample distance to $x$ over all such subsets is\nused to define the density estimate at $x$. The tuning parameter is thus the\nnumber of subsets instead of the typical bandwidth of kernel or histogram-based\ndensity estimators. The proposed method is similar to nearest-neighbor density\nestimators but it provides smoother estimates. We derive the asymptotic\ndistribution of this minimum sample distance statistic to study globally\noptimal values for the number and size of the subsets. Simulations are used to\nillustrate and compare the convergence properties of the estimator. The results\nshow that the method provides good estimates of a wide variety of densities\nwithout changes of the tuning parameter, and that it offers competitive\nconvergence performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 04:04:37 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Garg", "Vikram V.", ""], ["Tenorio", "Luis", ""], ["Willcox", "Karen", ""]]}, {"id": "1412.2887", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet", "title": "A note on the consistency of the Narain-Horvitz-Thompson estimator", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the Narain-Horvitz-Thompson estimator to have usual asymptotic properties\nsuch as consistency, some conditions on the sampling design and on the variable\nof interest are needed. Cardot et al. (2010) give some sufficient conditions\nfor the mean square consistency, but one of them is usually difficult to prove\nor does not hold for some unequal probability sampling designs. We propose\nalternative conditions for the mean square consistency of the\nNarain-Horvitz-Thompson estimator. A specific result is also proved in case\nwhen a martingale sampling algorithm is used, which implies consistency under a\nfast algorithm for the cube method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 08:45:40 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Chauvet", "Guillaume", ""]]}, {"id": "1412.3000", "submitter": "Lixing Zhu", "authors": "Lu Lin, Ping Dong, Yunquan Song and Lixing Zhu", "title": "Upper expectation parametric regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every observation may follow a distribution that is randomly selected in a\nclass of distributions. It is called the distribution uncertainty. This is a\nfact acknowledged in some research fields such as financial risk measure. Thus,\nthe classical expectation is not identifiable in general.In this paper, a\ndistribution uncertainty is defined, and then an upper expectation regression\nis proposed, which can describe the relationship between extreme events and\nrelevant covariates under the framework of distribution uncertainty. As there\nare no classical methods available to estimate the parameters in the upper\nexpectation regression, a two-step penalized maximum least squares procedure is\nproposed to estimate the mean function and the upper expectation of the error.\nThe resulting estimators are consistent and asymptotically normal in a certain\nsense.Simulation studies and a real data example are conducted to show that the\nclassical least squares estimation does not work and the penalized maximum\nleast squares performs well.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 15:39:50 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Lin", "Lu", ""], ["Dong", "Ping", ""], ["Song", "Yunquan", ""], ["Zhu", "Lixing", ""]]}, {"id": "1412.3050", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis and Magnus Rattray", "title": "A Bayesian model selection approach for identifying differentially\n  expressed transcripts from RNA-Seq data", "comments": "Revised version of arXiv:1412.3050v3", "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics), 2017", "doi": "10.1111/rssc.12213", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in molecular biology allow the quantification of the\ntranscriptome and scoring transcripts as differentially or equally expressed\nbetween two biological conditions. Although these two tasks are closely linked,\nthe available inference methods treat them separately: a primary model is used\nto estimate expression and its output is post-processed using a differential\nexpression model. In this paper, both issues are simultaneously addressed by\nproposing the joint estimation of expression levels and differential\nexpression: the unknown relative abundance of each transcript can either be\nequal or not between two conditions. A hierarchical Bayesian model builds upon\nthe BitSeq framework and the posterior distribution of transcript expression\nand differential expression is inferred using Markov Chain Monte Carlo (MCMC).\nIt is shown that the proposed model enjoys conjugacy for fixed dimension\nvariables, thus the full conditional distributions are analytically derived.\nTwo samplers are constructed, a reversible jump MCMC sampler and a collapsed\nGibbs sampler, and the latter is found to perform best. A cluster\nrepresentation of the aligned reads to the transcriptome is introduced,\nallowing parallel estimation of the marginal posterior distribution of subsets\nof transcripts under reasonable computing time. The proposed algorithm is\nbenchmarked against alternative methods using synthetic datasets and applied to\nreal RNA-sequencing data. Source code is available online\n(https://github.com/mqbssppe/cjBitSeq).\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 18:42:22 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 07:10:28 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2015 08:07:21 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 10:32:45 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Papastamoulis", "Panagiotis", ""], ["Rattray", "Magnus", ""]]}, {"id": "1412.3230", "submitter": "Anna Kiriliouk", "authors": "Michel Denuit and Anna Kiriliouk and Johan Segers", "title": "Max-factor individual risk models with application to credit portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual risk models need to capture possible correlations as failing to do\nso typically results in an underestimation of extreme quantiles of the\naggregate loss. Such dependence modelling is particularly important for\nmanaging credit risk, for instance, where joint defaults are a major cause of\nconcern. Often, the dependence between the individual loss occurrence\nindicators is driven by a small number of unobservable factors. Conditional\nloss probabilities are then expressed as monotone functions of linear\ncombinations of these hidden factors. However, combining the factors in a\nlinear way allows for some compensation between them. Such diversification\neffects are not always desirable and this is why the present work proposes a\nnew model replacing linear combinations with maxima. These max-factor models\ngive more insight into which of the factors is dominant.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 09:04:42 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Denuit", "Michel", ""], ["Kiriliouk", "Anna", ""], ["Segers", "Johan", ""]]}, {"id": "1412.3242", "submitter": "Amit Meir", "authors": "Yoav Benjamini and Amit Meir", "title": "Selective Correlations - the conditional estimators", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Voodoo correlations is recognized in neuroimaging as the\nproblem of estimating quantities of interest from the same data that was used\nto select them as interesting. In statistical terminology, the problem of\ninference following selection from the same data is that of selective\ninference. Motivated by the unwelcome side-effects of the recommended remedy-\nsplitting the data. A method for constructing confidence intervals based on the\ncorrect post-selection distribution of the observations has been suggested\nrecently. We utilize a similar approach in order to provide point estimates\nthat account for a large part of the selection bias. We show via extensive\nsimulations that the proposed estimator has favorable properties, namely, that\nit is likely to reduce estimation bias and the mean squared error compared to\nthe direct estimator without sacrificing power to detect non-zero correlation\nas in the case of the data splitting approach. We show that both point\nestimates and confidence intervals are needed in order to get a full assessment\nof the uncertainty in the point estimates as both are integrated into the\nConfidence Calibration Plots proposed recently.\n  The computation of the estimators is implemented in an accompanying software\npackage.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 10:04:36 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Benjamini", "Yoav", ""], ["Meir", "Amit", ""]]}, {"id": "1412.3502", "submitter": "Luke Prendergast", "authors": "Luke A. Prendergast and Robert G. Staudte", "title": "Meta-analysis of ratios of sample variances", "comments": null, "journal-ref": null, "doi": "10.1002/sim.6838", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When conducting a meta-analysis of standardized mean differences (SMDs), it\nis common to assume equal variances in the two arms of each study. This leads\nto Cohen's $d$ estimates for which interpretation is simple. However, this\nsimplicity should not be used as a justification for the assumption of equal\nvariances in situations where evidence may suggest that it is incorrect. Until\nnow, researchers have either used an $F$-test for each individual study as a\njustification for the equality of variances or perhaps even conveniently\nignored such tools altogether. In this paper we propose using a meta-analysis\nof F-test statistics to estimate the ratio of variances prior to the\ncombination of SMD's. This procedure allows some studies to be included that\nmight otherwise be omitted by individual fixed level tests for unequal\nvariances, sometimes occur even when the assumption of equal variances holds.\nThe estimated ratio of variances, as well as associated confidence intervals,\ncan be used as guidance as to whether the assumption of equal variances is\nviolated. The estimators considered include variance stabilization\ntransformations (VST) of the $F$-test statistics as well as MLE estimators. The\nVST approaches enable the use of QQ-plots to visually inspect for violations of\nequal variances while the MLE estimator easily allows for the introduction of a\nrandom effect. When there is evidence of unequal variances, this work provides\na means to formally justify the use of less common methods such as log ratio of\nmeans when studies are measured on a different scale.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 00:05:03 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2015 22:13:16 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Prendergast", "Luke A.", ""], ["Staudte", "Robert G.", ""]]}, {"id": "1412.3565", "submitter": "David Robinson", "authors": "David Robinson", "title": "broom: An R Package for Converting Statistical Analysis Objects Into\n  Tidy Data Frames", "comments": "A reproducible version of the manuscript can be found at\n  https://github.com/dgrtwo/broom_paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The concept of \"tidy data\" offers a powerful framework for structuring data\nto ease manipulation, modeling and visualization. However, most R functions,\nboth those built-in and those found in third-party packages, produce output\nthat is not tidy, and that is therefore difficult to reshape, recombine, and\notherwise manipulate. Here I introduce the broom package, which turns the\noutput of model objects into tidy data frames that are suited to further\nanalysis, manipulation, and visualization with input-tidy tools. Broom defines\nthe \"tidy\", \"augment\" and \"glance\" generics, which arrange a model into three\nlevels of tidy output respectively: the component level, the observation level,\nand the model level. I provide examples to demonstrate how these generics work\nwith tidy tools to allow analysis and modeling of data that is divided into\nsubsets, to recombine results from bootstrap replicates, and to perform\nsimulations that investigate the effect of varying input parameters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 08:07:03 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 18:32:07 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Robinson", "David", ""]]}, {"id": "1412.3730", "submitter": "Thijs van Ommen", "authors": "Peter Gr\\\"unwald and Thijs van Ommen", "title": "Inconsistency of Bayesian Inference for Misspecified Linear Models, and\n  a Proposal for Repairing It", "comments": "70 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically show that Bayesian inference can be inconsistent under\nmisspecification in simple linear regression problems, both in a model\naveraging/selection and in a Bayesian ridge regression setting. We use the\nstandard linear model, which assumes homoskedasticity, whereas the data are\nheteroskedastic, and observe that the posterior puts its mass on ever more\nhigh-dimensional models as the sample size increases. To remedy the problem, we\nequip the likelihood in Bayes' theorem with an exponent called the learning\nrate, and we propose the Safe Bayesian method to learn the learning rate from\nthe data. SafeBayes tends to select small learning rates as soon the standard\nposterior is not `cumulatively concentrated', and its results on our data are\nquite encouraging.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 17:16:06 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 16:23:39 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 13:14:47 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Gr\u00fcnwald", "Peter", ""], ["van Ommen", "Thijs", ""]]}, {"id": "1412.4170", "submitter": "Ritwik Mitra", "authors": "Ritwik Mitra and Cun-Hui Zhang", "title": "The Benefit of Group Sparsity in Group Inference with De-biased Scaled\n  Group Lasso", "comments": "39 Pages, 2 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study confidence regions and approximate chi-squared tests for variable\ngroups in high-dimensional linear regression. When the size of the group is\nsmall, low-dimensional projection estimators for individual coefficients can be\ndirectly used to construct efficient confidence regions and p-values for the\ngroup. However, the existing analyses of low-dimensional projection estimators\ndo not directly carry through for chi-squared-based inference of a large group\nof variables without inflating the sample size by a factor of the group size.\nWe propose to de-bias a scaled group Lasso for chi-squared-based statistical\ninference for potentially very large groups of variables. We prove that the\nproposed methods capture the benefit of group sparsity under proper conditions,\nfor statistical inference of the noise level and variable groups, large and\nsmall. Such benefit is especially strong when the group size is large.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 00:41:22 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 00:48:11 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Mitra", "Ritwik", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1412.4222", "submitter": "Jairo Cugliari", "authors": "Anestis Antoniadis (LJK), Xavier Brossat, Jairo Cugliari (ERIC),\n  Jean-Michel Poggi (LM-Orsay, INRIA Saclay - Ile de France)", "title": "A prediction interval for a function-valued forecast model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the information contained in the shape of the load curves, we\nhave proposed a flexible nonparametric function-valued fore-cast model called\nKWF (Kernel+Wavelet+Functional) well suited to handle nonstationary series. The\npredictor can be seen as a weighted average of futures of past situations,\nwhere the weights increase with the similarity between the past situations and\nthe actual one. In addi-tion, this strategy provides with a simultaneous\nmultiple horizon pre-diction. These weights induce a probability distribution\nthat can be used to produce bootstrap pseudo predictions. Prediction intervals\nare constructed after obtaining the corresponding bootstrap pseudo pre-diction\nresiduals. We develop two propositions following directly the KWF strategy and\ncompare it to two alternative ways coming from proposals of econometricians.\nThey construct simultaneous prediction intervals using multiple comparison\ncorrections through the control of the family wise error (FWE) or the false\ndiscovery rate. Alternatively, such prediction intervals can be constructed\nbootstrapping joint prob-ability regions. In this work we propose to obtain\nprediction intervals for the KWF model that are simultaneously valid for the H\npredic-tion horizons that corresponds with the corresponding path forecast,\nmaking a connection between functional time series and the econome-tricians'\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 10:31:31 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Antoniadis", "Anestis", "", "LJK"], ["Brossat", "Xavier", "", "ERIC"], ["Cugliari", "Jairo", "", "ERIC"], ["Poggi", "Jean-Michel", "", "LM-Orsay, INRIA Saclay - Ile de France"]]}, {"id": "1412.4260", "submitter": "Richard Warr", "authors": "Richard L. Warr, Brandon M. Greenwell", "title": "A Hierarchical Nonparametric Bayesian Model that Integrates Multiple\n  Sources of Lifetime Information to Model Large-Scale System Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The need for new large-scale reliability models is becoming apparent as the\namount of available data is expanding at a dramatic rate. Often complex systems\nhave thousands of components. Each of these components and their respective\nsubsystems could have many, few, or no test data. The large number of\ncomponents creates a massive estimation project that challenges the\ncomputational feasibility of traditional reliability models. The solution\npresented in this work suggests a hierarchical nonparametric Bayesian\nframework, using beta-Stacy processes. In this Bayesian framework,\ntime-to-event distributions are estimated from sample data (which may be\nrandomly right censored), and possible expert opinion. These estimates can be\nused to compute and predict system reliability.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 17:25:47 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Warr", "Richard L.", ""], ["Greenwell", "Brandon M.", ""]]}, {"id": "1412.4355", "submitter": "David Woods", "authors": "Timothy W. Waite and David C. Woods", "title": "Designs for generalized linear models with random block effects via\n  information matrix approximations", "comments": null, "journal-ref": "Biometrika, 2015, 102, 677-693", "doi": "10.1093/biomet/asv005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of optimal designs for generalized linear mixed models is\ncomplicated by the fact that the Fisher information matrix, on which most\noptimality criteria depend, is computationally expensive to evaluate. Our focus\nis on the design of experiments for likelihood estimation of parameters in the\nconditional model. We provide two novel approximations that substantially\nreduce the computational cost of evaluating the information matrix by complete\nenumeration of response outcomes, or Monte Carlo approximations thereof: (i) an\nasymptotic approximation which is accurate when there is strong dependence\nbetween observations in the same block; (ii) an approximation via Kriging\ninterpolators. For logistic random intercept models, we show how interpolation\ncan be especially effective for finding pseudo-Bayesian designs that\nincorporate uncertainty in the values of the model parameters. The new results\nare used to provide the first evaluation of the efficiency, for estimating\nconditional models, of optimal designs from closed-form approximations to the\ninformation matrix derived from marginal models. It is found that correcting\nfor the marginal attenuation of parameters in binary-response models yields\nmuch improved designs, typically with very high efficiencies. However, in some\nexperiments exhibiting strong dependence, designs for marginal models may still\nbe inefficient for conditional modelling. Our asymptotic results provide some\ntheoretical insights into why such inefficiencies occur.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 13:28:34 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Waite", "Timothy W.", ""], ["Woods", "David C.", ""]]}, {"id": "1412.4384", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Robert Pich\\'e", "title": "Bayesian Hierarchical Model of Total Variation Regularisation for Image\n  Deblurring", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian hierarchical model for total variation regularisation is presented\nin this paper. All the parameters of an inverse problem, including the\n\"regularisation parameter\", are estimated simultaneously from the data in the\nmodel. The model is based on the characterisation of the Laplace density prior\nas a scale mixture of Gaussians. With different priors on the mixture variable,\nother total variation like regularisations e.g. a prior that is related to\nt-distribution, are also obtained. An approximation of the resulting posterior\nmean is found using a variational Bayes method. In addition, an iterative\nalternating sequential algorithm for computing the maximum a posteriori\nestimate is presented. The methods are illustrated with examples of image\ndeblurring. Results show that the proposed model can be used for automatic\nedge-preserving inversion in the case of image deblurring. Despite promising\nresults, some difficulties with the model were encountered and are subject to\nfuture work.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 17:43:53 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Pich\u00e9", "Robert", ""]]}, {"id": "1412.4428", "submitter": "Timothy Christensen", "authors": "Timothy Christensen", "title": "Nonparametric Stochastic Discount Factor Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.EC q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic discount factor (SDF) processes in dynamic economies admit a\npermanent-transitory decomposition in which the permanent component\ncharacterizes pricing over long investment horizons. This paper introduces an\nempirical framework to analyze the permanent-transitory decomposition of SDF\nprocesses. Specifically, we show how to estimate nonparametrically the solution\nto the Perron-Frobenius eigenfunction problem of Hansen and Scheinkman (2009).\nOur empirical framework allows researchers to (i) recover the time series of\nthe estimated permanent and transitory components and (ii) estimate the yield\nand the change of measure which characterize pricing over long investment\nhorizons. We also introduce nonparametric estimators of the continuation value\nfunction in a class of models with recursive preferences by reinterpreting the\nvalue function recursion as a nonlinear Perron-Frobenius problem. We establish\nconsistency and convergence rates of the eigenfunction estimators and\nasymptotic normality of the eigenvalue estimator and estimators of related\nfunctionals. As an application, we study an economy where the representative\nagent is endowed with recursive preferences, allowing for general (nonlinear)\nconsumption and earnings growth dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 00:00:15 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 16:44:08 GMT"}, {"version": "v3", "created": "Sun, 23 Apr 2017 15:44:55 GMT"}, {"version": "v4", "created": "Sun, 30 Apr 2017 16:32:40 GMT"}, {"version": "v5", "created": "Fri, 19 May 2017 18:28:46 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Christensen", "Timothy", ""]]}, {"id": "1412.4533", "submitter": "Ludovic Delchambre Mr.", "authors": "Ludovic Delchambre", "title": "Weighted principal component analysis: a weighted covariance\n  eigendecomposition approach", "comments": "12 pages, 9 figures", "journal-ref": "Monthly Notices of the Royal Astronomical Society 2014 446 (2):\n  3545-3555", "doi": "10.1093/mnras/stu2219", "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new straightforward principal component analysis (PCA) method\nbased on the diagonalization of the weighted variance-covariance matrix through\ntwo spectral decomposition methods: power iteration and Rayleigh quotient\niteration. This method allows one to retrieve a given number of orthogonal\nprincipal components amongst the most meaningful ones for the case of problems\nwith weighted and/or missing data. Principal coefficients are then retrieved by\nfitting principal components to the data while providing the final\ndecomposition. Tests performed on real and simulated cases show that our method\nis optimal in the identification of the most significant patterns within data\nsets. We illustrate the usefulness of this method by assessing its quality on\nthe extrapolation of Sloan Digital Sky Survey quasar spectra from measured\nwavelengths to shorter and longer wavelengths. Our new algorithm also benefits\nfrom a fast and flexible implementation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 11:00:42 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Delchambre", "Ludovic", ""]]}, {"id": "1412.4605", "submitter": "Benedikt M. P\\\"otscher", "authors": "Fran\\c{c}ois Bachoc, Hannes Leeb, and Benedikt M. P\\\"otscher", "title": "Valid confidence intervals for post-model-selection predictors", "comments": "Some material added. Some restructuring of the paper. Some minor\n  errors corrected", "journal-ref": "Annals of Statistics 47 (2019), 1475-1504", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference post-model-selection in linear regression. In this\nsetting, Berk et al.(2013) recently introduced a class of confidence sets, the\nso-called PoSI intervals, that cover a certain non-standard quantity of\ninterest with a user-specified minimal coverage probability, irrespective of\nthe model selection procedure that is being used. In this paper, we generalize\nthe PoSI intervals to post-model-selection predictors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 14:19:05 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 14:36:34 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2017 12:01:41 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bachoc", "Fran\u00e7ois", ""], ["Leeb", "Hannes", ""], ["P\u00f6tscher", "Benedikt M.", ""]]}, {"id": "1412.4643", "submitter": "Simon DeDeo", "authors": "Simon DeDeo", "title": "Wrong side of the tracks: Big Data and Protected Categories", "comments": "12 pages. Extended discussion", "journal-ref": "in Cassidy R. Sugimoto, Hamid R. Ekbia and Michael Mattioli\n  (Eds.), Big Data Is Not a Monolith (pp. 31-42). MIT Press. 2016", "doi": null, "report-no": null, "categories": "cs.IT cs.CY math.IT physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we use machine learning for public policy, we find that many useful\nvariables are associated with others on which it would be ethically problematic\nto base decisions. This problem becomes particularly acute in the Big Data era,\nwhen predictions are often made in the absence of strong theories for\nunderlying causal mechanisms. We describe the dangers to democratic\ndecision-making when high-performance algorithms fail to provide an explicit\naccount of causation. We then demonstrate how information theory allows us to\ndegrade predictions so that they decorrelate from protected variables with\nminimal loss of accuracy. Enforcing total decorrelation is at best a near-term\nsolution, however. The role of causal argument in ethical debate urges the\ndevelopment of new, interpretable machine-learning algorithms that reference\ncausal mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 15:48:04 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 03:37:34 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 02:14:48 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["DeDeo", "Simon", ""]]}, {"id": "1412.4681", "submitter": "Yoann Altmann", "authors": "Yoann Altmann and Marcelo Pereyra and Stephen McLaughlin", "title": "Bayesian nonlinear hyperspectral unmixing with spatial residual\n  component analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Bayesian model and algorithm for nonlinear unmixing\nof hyperspectral images. The model proposed represents the pixel reflectances\nas linear combinations of the endmembers, corrupted by nonlinear (with respect\nto the endmembers) terms and additive Gaussian noise. Prior knowledge about the\nproblem is embedded in a hierarchical model that describes the dependence\nstructure between the model parameters and their constraints. In particular, a\ngamma Markov random field is used to model the joint distribution of the\nnonlinear terms, which are expected to exhibit significant spatial\ncorrelations. An adaptive Markov chain Monte Carlo algorithm is then proposed\nto compute the Bayesian estimates of interest and perform Bayesian inference.\nThis algorithm is equipped with a stochastic optimisation adaptation mechanism\nthat automatically adjusts the parameters of the gamma Markov random field by\nmaximum marginal likelihood estimation. Finally, the proposed methodology is\ndemonstrated through a series of experiments with comparisons using synthetic\nand real data and with competing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 17:14:48 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 12:33:49 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Altmann", "Yoann", ""], ["Pereyra", "Marcelo", ""], ["McLaughlin", "Stephen", ""]]}, {"id": "1412.4841", "submitter": "Jordan Yoder", "authors": "Jordan Yoder and Carey E. Priebe", "title": "A Model-based Semi-Supervised Clustering Methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an extension of model-based clustering to the semi-supervised\ncase, where some of the data are pre-labeled. We provide a derivation of the\nBayesian Information Criterion (BIC) approximation to the Bayes factor in this\nsetting. We then use the BIC to the select number of clusters and the variables\nuseful for clustering. We demonstrate the efficacy of this adaptation of the\nmodel-based clustering paradigm through two simulation examples and a fly\nlarvae behavioral dataset in which lines of neurons are clustered into\nbehavioral groups.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 00:26:09 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 22:31:01 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Yoder", "Jordan", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1412.4845", "submitter": "Ernest Ryu", "authors": "Ernest K. Ryu and Stephen P. Boyd", "title": "Adaptive Importance Sampling via Stochastic Convex Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the variance of the Monte Carlo estimator that is importance\nsampled from an exponential family is a convex function of the natural\nparameter of the distribution. With this insight, we propose an adaptive\nimportance sampling algorithm that simultaneously improves the choice of\nsampling distribution while accumulating a Monte Carlo estimate. Exploiting\nconvexity, we prove that the method's unbiased estimator has variance that is\nasymptotically optimal over the exponential family.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 00:30:36 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 00:44:09 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Ryu", "Ernest K.", ""], ["Boyd", "Stephen P.", ""]]}, {"id": "1412.4857", "submitter": "Jing Lei", "authors": "Jing Lei", "title": "A goodness-of-fit test for stochastic block models", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1370 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 1, 401-424", "doi": "10.1214/15-AOS1370", "report-no": "IMS-AOS-AOS1370", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model is a popular tool for studying community\nstructures in network data. We develop a goodness-of-fit test for the\nstochastic block model. The test statistic is based on the largest singular\nvalue of a residual matrix obtained by subtracting the estimated block mean\neffect from the adjacency matrix. Asymptotic null distribution is obtained\nusing recent advances in random matrix theory. The test is proved to have full\npower against alternative models with finer structures. These results naturally\nlead to a consistent sequential testing estimate of the number of communities.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 02:07:57 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 14:30:13 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Lei", "Jing", ""]]}, {"id": "1412.4869", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Andrew Gelman, Tuomas Sivula, Pasi Jyl\\\"anki, Dustin\n  Tran, Swupnil Sahai, Paul Blomstedt, John P. Cunningham, David Schiminovich,\n  Christian Robert", "title": "Expectation propagation as a way of life: A framework for Bayesian\n  inference on partitioned data", "comments": "Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common divide-and-conquer approach for Bayesian computation with big data\nis to partition the data, perform local inference for each piece separately,\nand combine the results to obtain a global posterior approximation. While being\nconceptually and computationally appealing, this method involves the\nproblematic need to also split the prior for the local inferences; these\nweakened priors may not provide enough regularization for each separate\ncomputation, thus eliminating one of the key advantages of Bayesian methods. To\nresolve this dilemma while still retaining the generalizability of the\nunderlying local inference method, we apply the idea of expectation propagation\n(EP) as a framework for distributed Bayesian inference. The central idea is to\niteratively update approximations to the local likelihoods given the state of\nthe other approximations and the prior. The present paper has two roles: we\nreview the steps that are needed to keep EP algorithms numerically stable, and\nwe suggest a general approach, inspired by EP, for approaching data\npartitioning problems in a way that achieves the computational benefits of\nparallelism while allowing each local update to make use of relevant\ninformation from the other sites. In addition, we demonstrate how the method\ncan be applied in a hierarchical context to make use of partitioning of both\ndata and parameters. The paper describes a general algorithmic framework,\nrather than a specific algorithm, and presents an example implementation for\nit.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 03:47:38 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 13:06:17 GMT"}, {"version": "v3", "created": "Sat, 10 Mar 2018 21:52:41 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 19:33:01 GMT"}, {"version": "v5", "created": "Sat, 30 Nov 2019 14:11:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""], ["Sivula", "Tuomas", ""], ["Jyl\u00e4nki", "Pasi", ""], ["Tran", "Dustin", ""], ["Sahai", "Swupnil", ""], ["Blomstedt", "Paul", ""], ["Cunningham", "John P.", ""], ["Schiminovich", "David", ""], ["Robert", "Christian", ""]]}, {"id": "1412.5000", "submitter": "Peng Ding", "authors": "Peng Ding, Avi Feller, and Luke Miratrix", "title": "Randomization Inference for Treatment Effect Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied researchers are increasingly interested in whether and how treatment\neffects vary in randomized evaluations, especially variation not explained by\nobserved covariates. We propose a model-free approach for testing for the\npresence of such unexplained variation. To use this randomization-based\napproach, we must address the fact that the average treatment effect, generally\nthe object of interest in randomized experiments, actually acts as a nuisance\nparameter in this setting. We explore potential solutions and advocate for a\nmethod that guarantees valid tests in finite samples despite this nuisance. We\nalso show how this method readily extends to testing for heterogeneity beyond a\ngiven model, which can be useful for assessing the sufficiency of a given\nscientific theory. We finally apply our method to the National Head Start\nImpact Study, a large-scale randomized evaluation of a Federal preschool\nprogram, finding that there is indeed significant unexplained treatment effect\nvariation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 13:57:41 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Ding", "Peng", ""], ["Feller", "Avi", ""], ["Miratrix", "Luke", ""]]}, {"id": "1412.5139", "submitter": "Ryan Martin", "authors": "Ryan Martin, Huiping Xu, Zuoyi Zhang, Chuanhai Liu", "title": "Valid uncertainty quantification about the model in a linear regression\n  setting", "comments": "24 pages, 4 figures, 2 pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scientific applications, there often are several competing models that\ncould be fit to the observed data, so quantification of the model uncertainty\nis of fundamental importance. In this paper, we develop an inferential model\n(IM) approach for simultaneously valid probabilistic inference over a\ncollection of assertions of interest without requiring any prior input. Our\nconstruction guarantees that the approach is optimal in the sense that it is\nthe most efficient among those which are valid. Connections between the IM's\nsimultaneous validity and post-selection inference are also made. We apply the\ngeneral results to obtain valid uncertainty quantification about the set of\npredictor variables to be included in a linear regression model.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 19:53:35 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 18:59:48 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Martin", "Ryan", ""], ["Xu", "Huiping", ""], ["Zhang", "Zuoyi", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1412.5250", "submitter": "Ines Wilms", "authors": "William B. Nicholson, Ines Wilms, Jacob Bien, David S. Matteson", "title": "High Dimensional Forecasting via Interpretable Vector Autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector autoregression (VAR) is a fundamental tool for modeling multivariate\ntime series. However, as the number of component series is increased, the VAR\nmodel becomes overparameterized. Several authors have addressed this issue by\nincorporating regularized approaches, such as the lasso in VAR estimation.\nTraditional approaches address overparameterization by selecting a low lag\norder, based on the assumption of short range dependence, assuming that a\nuniversal lag order applies to all components. Such an approach constrains the\nrelationship between the components and impedes forecast performance. The\nlasso-based approaches work much better in high-dimensional situations but do\nnot incorporate the notion of lag order selection.\n  We propose a new class of hierarchical lag structures (HLag) that embed the\nnotion of lag selection into a convex regularizer. The key modeling tool is a\ngroup lasso with nested groups which guarantees that the sparsity pattern of\nlag coefficients honors the VAR's ordered structure. The HLag framework offers\nthree structures, which allow for varying levels of flexibility. A simulation\nstudy demonstrates improved performance in forecasting and lag order selection\nover previous approaches, and a macroeconomic application further highlights\nforecasting improvements as well as HLag's convenient, interpretable output.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 03:36:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 16:55:02 GMT"}, {"version": "v3", "created": "Sat, 8 Sep 2018 17:08:29 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 18:18:55 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nicholson", "William B.", ""], ["Wilms", "Ines", ""], ["Bien", "Jacob", ""], ["Matteson", "David S.", ""]]}, {"id": "1412.5262", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Rheanna Mainzer and Davide Farchione", "title": "The impact of a Hausman pretest, applied to panel data, on the coverage\n  probability of confidence intervals", "comments": "The exposition has been improved", "journal-ref": "The impact of a Hausman pretest, applied to panel data, on the\n  coverage probability of confidence intervals. Economics Letters, 131, 12-15\n  (2015)", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of panel data that includes a time-varying covariate, a\nHausman pretest is commonly used to decide whether subsequent inference is made\nusing the random effects model or the fixed effects model. We consider the\neffect of this pretest on the coverage probability of a confidence interval for\nthe slope parameter. We prove three new finite sample theorems that make it\neasy to assess, for a wide variety of circumstances, the effect of the Hausman\npretest on the minimum coverage probability of this confidence interval. Our\nresults show that for the small levels of significance of the Hausman pretest\ncommonly used in applications, the minimum coverage probability of the\nconfidence interval for the slope parameter can be far below nominal.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 05:50:49 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 05:02:51 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Mainzer", "Rheanna", ""], ["Farchione", "Davide", ""]]}, {"id": "1412.5376", "submitter": "Holger Dette", "authors": "Axel B\\\"ucher and Michael Hoffmann and Mathias Vetter and Holger Dette", "title": "Nonparametric tests for detecting breaks in the jump behaviour of a\n  time-continuous process", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with tests for changes in the jump behaviour of a\ntime-continuous process. Based on results on weak convergence of a sequential\nempirical tail integral process, asymptotics of certain tests statistics for\nbreaks in the jump measure of an Ito semimartingale are constructed. Whenever\nlimiting distributions depend in a complicated way on the unknown jump measure,\nempirical quantiles are obtained using a multiplier bootstrap scheme. An\nextensive simulation study shows a good performance of our tests in finite\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 13:15:55 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Hoffmann", "Michael", ""], ["Vetter", "Mathias", ""], ["Dette", "Holger", ""]]}, {"id": "1412.5647", "submitter": "Ivan Fernandez-Val", "authors": "Mingli Chen, Iv\\'an Fern\\'andez-Val, and Martin Weidner", "title": "Nonlinear Factor Models for Network and Panel Data", "comments": "49 pages, 6 tables, the changes in v4 include numerical results with\n  more simulations and minor edits in the main text and appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor structures or interactive effects are convenient devices to\nincorporate latent variables in panel data models. We consider fixed effect\nestimation of nonlinear panel single-index models with factor structures in the\nunobservables, which include logit, probit, ordered probit and Poisson\nspecifications. We establish that fixed effect estimators of model parameters\nand average partial effects have normal distributions when the two dimensions\nof the panel grow large, but might suffer of incidental parameter bias. We show\nhow models with factor structures can also be applied to capture important\nfeatures of network data such as reciprocity, degree heterogeneity, homophily\nin latent variables and clustering. We illustrate this applicability with an\nempirical example to the estimation of a gravity equation of international\ntrade between countries using a Poisson model with multiple factors.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:10:01 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 16:05:12 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 22:39:36 GMT"}, {"version": "v4", "created": "Tue, 15 Oct 2019 16:13:59 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Chen", "Mingli", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Weidner", "Martin", ""]]}, {"id": "1412.5843", "submitter": "Francisco Louzada Professor", "authors": "Pedro L. Ramos, Francisco Louzada", "title": "A Modified Reference Prior for the Generalized Gamma Distribution", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": "ArVix-FL-2014-02", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an objective Bayesian estimation approach for the\nparameters of the generalized gamma distribution. Various reference priors are\nobtained, but showing that they lead to improper posterior distributions. We\novercome this problem by proposing a modification in a reference priori\ndistribution, allowing for a proper posterior distribution for the parameters\nof the generalized gamma distribution. We perform a simulation study in order\nto study the efficiency of the proposed methodology, which is also fully\nillustrated on a real data set.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 13:00:59 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Ramos", "Pedro L.", ""], ["Louzada", "Francisco", ""]]}, {"id": "1412.5848", "submitter": "Francisco Louzada Professor", "authors": "Taciana K. O. Shimizu, Francisco Louzada, Adriano K. Suzuki", "title": "Analyzing Volleyball Data on a Compositional Regression Model Approach:\n  An Application to the Brazilian Men's Volleyball Super League 2011/2012 Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": "ArVix-FL-2014-03a", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volleyball has become a competitive sport with high physical and technical\nperformance. Matches results are based on the players and teams'skills as\ntechnical and tactical strategies to succeed in a championship. At this point,\nsome studies are carried out on the performance analysis of different match\nelements, contributing to the development of this sport. In this paper, we\nproposed a new approach to analyze volleyball data. The study is based on the\ncompositional data methodology modeling in regression model. The parameters are\nobtained through the maximum likelihood. We performed a simulation study to\nevaluate the estimation procedure in compositional regression model and we\nillustrated the proposed methodology considering real data set of volleyball.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 13:18:25 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Shimizu", "Taciana K. O.", ""], ["Louzada", "Francisco", ""], ["Suzuki", "Adriano K.", ""]]}, {"id": "1412.5870", "submitter": "Hamed Haselimashhadi", "authors": "Hamed Haselimashhadi and Veronica Vinciotti", "title": "Penalised inference for autoregressive moving average models with\n  time-dependent predictors", "comments": null, "journal-ref": null, "doi": "10.1007/s40300-017-0121-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear models that contain a time-dependent response and explanatory\nvariables have attracted much interest in recent years. The most general form\nof the existing approaches is of a linear regression model with autoregressive\nmoving average residuals. The addition of the moving average component results\nin a complex model with a very challenging implementation. In this paper, we\npropose to account for the time dependency in the data by explicitly adding\nautoregressive terms of the response variable in the linear model. In addition,\nwe consider an autoregressive process for the errors in order to capture\ncomplex dynamic relationships parsimoniously. To broaden the application of the\nmodel, we present an $l_1$ penalized likelihood approach for the estimation of\nthe parameters and show how the adaptive lasso penalties lead to an estimator\nwhich enjoys the oracle property. Furthermore, we prove the consistency of the\nestimators with respect to the mean squared prediction error in\nhigh-dimensional settings, an aspect that has not been considered by the\nexisting time-dependent regression models. A simulation study and real data\nanalysis show the successful applications of the model on financial data on\nstock indexes.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 14:25:42 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 14:38:09 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Haselimashhadi", "Hamed", ""], ["Vinciotti", "Veronica", ""]]}, {"id": "1412.6049", "submitter": "Qipeng Liu", "authors": "Qipeng Liu, Jiuhua Zhao, and Xiaofan Wang", "title": "Distributed Detection via Bayesian Updates and Consensus", "comments": "6 pages, 3 figures. This paper has been submitted to Chinese Control\n  Conference 2015 at Hangzhou, People's Republic of China", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.MA cs.SY physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss a class of distributed detection algorithms which\ncan be viewed as implementations of Bayes' law in distributed settings. Some of\nthe algorithms are proposed in the literature most recently, and others are\nfirst developed in this paper. The common feature of these algorithms is that\nthey all combine (i) certain kinds of consensus protocols with (ii) Bayesian\nupdates. They are different mainly in the aspect of the type of consensus\nprotocol and the order of the two operations. After discussing their\nsimilarities and differences, we compare these distributed algorithms by\nnumerical examples. We focus on the rate at which these algorithms detect the\nunderlying true state of an object. We find that (a) The algorithms with\nconsensus via geometric average is more efficient than that via arithmetic\naverage; (b) The order of consensus aggregation and Bayesian update does not\napparently influence the performance of the algorithms; (c) The existence of\ncommunication delay dramatically slows down the rate of convergence; (d) More\ncommunication between agents with different signal structures improves the rate\nof convergence.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 00:35:25 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 07:00:54 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Liu", "Qipeng", ""], ["Zhao", "Jiuhua", ""], ["Wang", "Xiaofan", ""]]}, {"id": "1412.6231", "submitter": "Kengo Kamatani", "authors": "Kengo Kamatani", "title": "Efficient strategy for the Markov chain Monte Carlo in high-dimension\n  with heavy-tailed target probability distribution", "comments": "30pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce a new Markov chain Monte Carlo\nmethod and exhibit its efficiency by simulation and high-dimensional asymptotic\ntheory. Key fact is that our algorithm has a reversible proposal transition\nkernel, which is designed to have a heavy-tailed invariant probability\ndistribution. The high-dimensional asymptotic theory is studied for a class of\nheavy-tailed target probability distribution. As the number of dimension of the\nstate space goes to infinity, we will show that our algorithm has a much better\nconvergence rate than that of the preconditioned Crank Nicolson (pCN) algorithm\nand the random-walk Metropolis (RWM) algorithm. We also show that our algorithm\nis at least as good as the pCN algorithm and better than the RWM algorithm for\nlight-tailed target probability distribution.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 06:55:24 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Kamatani", "Kengo", ""]]}, {"id": "1412.6316", "submitter": "Lorenzo Hern\\'andez", "authors": "Lorenzo Hern\\'andez, Jorge Tejero, Jaime Vinuesa", "title": "Maximum Likelihood Estimation of the correlation parameters for\n  elliptical copulas", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to obtain the maximum likelihood estimates of the\ncorrelation parameters of elliptical copulas. Previously existing methods for\nthis task were either fast but only approximate or exact but very\ntime-consuming, especially for high-dimensional problems. Our proposal combines\nthe advantages of both, since it obtains the exact estimates and its\nperformance makes it suitable for most practical applications. The algorithm is\ngiven with explicit expressions for the Gaussian and Student's t copulas.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 12:32:25 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Hern\u00e1ndez", "Lorenzo", ""], ["Tejero", "Jorge", ""], ["Vinuesa", "Jaime", ""]]}, {"id": "1412.6370", "submitter": "Jan Palczewski", "authors": "Blazej Miasojedow, Wojciech Niemiro, Jan Palczewski, Wojciech Rejchel", "title": "Adaptive Monte Carlo Maximum Likelihood", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-18781-5_14", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Monte Carlo approximations to the maximum likelihood estimator in\nmodels with intractable norming constants. This paper deals with adaptive Monte\nCarlo algorithms, which adjust control parameters in the course of simulation.\nWe examine asymptotics of adaptive importance sampling and a new algorithm,\nwhich uses resampling and MCMC. This algorithm is designed to reduce problems\nwith degeneracy of importance weights. Our analysis is based on martingale\nlimit theorems. We also describe how adaptive maximization algorithms of\nNewton-Raphson type can be combined with the resampling techniques. The paper\nincludes results of a small scale simulation study in which we compare the\nperformance of adaptive and non-adaptive Monte Carlo maximum likelihood\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:12:37 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Miasojedow", "Blazej", ""], ["Niemiro", "Wojciech", ""], ["Palczewski", "Jan", ""], ["Rejchel", "Wojciech", ""]]}, {"id": "1412.6371", "submitter": "Jan Palczewski", "authors": "Blazej Miasojedow, Wojciech Niemiro, Jan Palczewski, Wojciech Rejchel", "title": "Asymptotics of Monte Carlo maximum likelihood estimators", "comments": null, "journal-ref": "Probability and Mathematical Statistics, 36(2), 2016", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Monte Carlo approximation to the maximum likelihood estimator in\nmodels with intractable norming constants and explanatory variables. We\nconsider both sources of randomness (due to the initial sample and to Monte\nCarlo simulations) and prove asymptotical normality of the estimator.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:13:10 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Miasojedow", "Blazej", ""], ["Niemiro", "Wojciech", ""], ["Palczewski", "Jan", ""], ["Rejchel", "Wojciech", ""]]}, {"id": "1412.6586", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mohammad Javad Shafiee, Parthipan Siva, and Xiao Yu\n  Wang", "title": "A deep-structured fully-connected random field model for structured\n  inference", "comments": "Accepted, 13 pages", "journal-ref": "IEEE Access Journal, vol. 3, pp. 469-477, 2015", "doi": "10.1109/ACCESS.2015.2425304", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant interest in the use of fully-connected graphical\nmodels and deep-structured graphical models for the purpose of structured\ninference. However, fully-connected and deep-structured graphical models have\nbeen largely explored independently, leaving the unification of these two\nconcepts ripe for exploration. A fundamental challenge with unifying these two\ntypes of models is in dealing with computational complexity. In this study, we\ninvestigate the feasibility of unifying fully-connected and deep-structured\nmodels in a computationally tractable manner for the purpose of structured\ninference. To accomplish this, we introduce a deep-structured fully-connected\nrandom field (DFRF) model that integrates a series of intermediate sparse\nauto-encoding layers placed between state layers to significantly reduce\ncomputational complexity. The problem of image segmentation was used to\nillustrate the feasibility of using the DFRF for structured inference in a\ncomputationally tractable manner. Results in this study show that it is\nfeasible to unify fully-connected and deep-structured models in a\ncomputationally tractable manner for solving structured inference problems such\nas image segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 03:02:32 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 21:34:22 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 23:05:49 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wong", "Alexander", ""], ["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Wang", "Xiao Yu", ""]]}, {"id": "1412.6592", "submitter": "Hua Zhou", "authors": "Xiang Zhang and Lexin Li and Hua Zhou and Dinggang Shen and the\n  Alzheimer's Disease Neuroimaging Initiative", "title": "Tensor Generalized Estimating Equations for Longitudinal Imaging\n  Analysis", "comments": "40 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In an increasing number of neuroimaging studies, brain images, which are in\nthe form of multidimensional arrays (tensors), have been collected on multiple\nsubjects at multiple time points. Of scientific interest is to analyze such\nmassive and complex longitudinal images to diagnose neurodegenerative disorders\nand to identify disease relevant brain regions. In this article, we treat those\nproblems in a unifying regression framework with image predictors, and propose\ntensor generalized estimating equations (GEE) for longitudinal imaging\nanalysis. The GEE approach takes into account intra-subject correlation of\nresponses, whereas a low rank tensor decomposition of the coefficient array\nenables effective estimation and prediction with limited sample size. We\npropose an efficient estimation algorithm, study the asymptotics in both fixed\n$p$ and diverging $p$ regimes, and also investigate tensor GEE with\nregularization that is particularly useful for region selection. The efficacy\nof the proposed tensor GEE is demonstrated on both simulated data and a real\ndata set from the Alzheimer's Disease Neuroimaging Initiative (ADNI).\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 03:48:59 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Zhang", "Xiang", ""], ["Li", "Lexin", ""], ["Zhou", "Hua", ""], ["Shen", "Dinggang", ""], ["Initiative", "the Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1412.6608", "submitter": "Chaoxu Zhou", "authors": "Kani Chen, Yuanyuan Lin, Yuan Yao, Chaoxu Zhou", "title": "Regression Analysis with Response-biased Sampling", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response-biased sampling, in which samples are drawn from a popula- tion\naccording to the values of the response variable, is common in biomedical,\nepidemiological, economic and social studies. In particular, the complete\nobser- vations in data with censoring, truncation or missing covariates can be\nregarded as response-biased sampling under certain conditions. This paper\nproposes to use transformation models, known as the generalized accelerated\nfailure time model in econometrics, for regression analysis with\nresponse-biased sampling. With unknown error distribution, the transformation\nmodels are broad enough to cover linear re- gression models, the Cox's model\nand the proportional odds model as special cases. To the best of our knowledge,\nexcept for the case-control logistic regression, there is no report in the\nliterature that a prospective estimation approach can work for biased sampling\nwithout any modification. We prove that the maximum rank corre- lation\nestimation is valid for response-biased sampling and establish its consistency\nand asymptotic normality. Unlike the inverse probability methods, the proposed\nmethod of estimation does not involve the sampling probabilities, which are\noften difficult to obtain in practice. Without the need of estimating the\nunknown trans- formation function or the error distribution, the proposed\nmethod is numerically easy to implement with the Nelder-Mead simplex algorithm,\nwhich does not require convexity or continuity. We propose an inference\nprocedure using random weight- ing to avoid the complication of density\nestimation when using the plug-in rule for variance estimation. Numerical\nstudies with supportive evidence are presented. Applications are illustrated\nwith the Forbes Global 2000 data and the Stanford heart transplant data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:17:56 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Chen", "Kani", ""], ["Lin", "Yuanyuan", ""], ["Yao", "Yuan", ""], ["Zhou", "Chaoxu", ""]]}, {"id": "1412.6921", "submitter": "Mohamed Kayid", "authors": "M. Kayid, S. Izadkhah and H. Alhalees", "title": "Combination Mean Residual Life Order with Reliability Applications", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce, study and analyze a new stochastic\norder which lies in the framework of the mean residual life and the combination\nconvexity orders. Several preservation properties of the new order under\nreliability operations of monotone transformation, mixture, weighted\ndistributions and shock models are discussed. In addition, two characterization\nproperties of the new order based on the concept of residual life at random\ntime and the concept of excess lifetime in renewal processes are given.\nFinally, we highlight some new applications of this order in the context of\nreliability and survival analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 10:55:20 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 11:27:15 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Kayid", "M.", ""], ["Izadkhah", "S.", ""], ["Alhalees", "H.", ""]]}, {"id": "1412.6966", "submitter": "Franck Picard", "authors": "S. Ivanoff, F. Picard, V. Rivoirard", "title": "Adaptive Lasso and group-Lasso for functional Poisson regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional Poisson regression has become a standard framework for the\nanalysis of massive counts datasets. In this work we estimate the intensity\nfunction of the Poisson regression model by using a dictionary approach, which\ngeneralizes the classical basis approach, combined with a Lasso or a\ngroup-Lasso procedure. Selection depends on penalty weights that need to be\ncalibrated. Standard methodologies developed in the Gaussian framework can not\nbe directly applied to Poisson models due to heteroscedasticity. Here we\nprovide data-driven weights for the Lasso and the group-Lasso derived from\nconcentration inequalities adapted to the Poisson case. We show that the\nassociated Lasso and group-Lasso procedures are theoretically optimal in the\noracle approach. Simulations are used to assess the empirical performance of\nour procedure, and an original application to the analysis of Next Generation\nSequencing data is provided.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 13:00:36 GMT"}, {"version": "v2", "created": "Fri, 26 Dec 2014 20:13:11 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Ivanoff", "S.", ""], ["Picard", "F.", ""], ["Rivoirard", "V.", ""]]}, {"id": "1412.7096", "submitter": "Emmanuel Bacry", "authors": "Emmanuel Bacry and Thibault Jaisson and Jean-Francois Muzy", "title": "Estimation of slowly decreasing Hawkes kernels: Application to high\n  frequency order book modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.TR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a modified version of the non parametric Hawkes kernel estimation\nprocedure studied in arXiv:1401.0903 that is adapted to slowly decreasing\nkernels. We show on numerical simulations involving a reasonable number of\nevents that this method allows us to estimate faithfully a power-law decreasing\nkernel over at least 6 decades. We then propose a 8-dimensional Hawkes model\nfor all events associated with the first level of some asset order book.\nApplying our estimation procedure to this model, allows us to uncover the main\nproperties of the coupled dynamics of trade, limit and cancel orders in\nrelationship with the mid-price variations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 18:58:03 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Bacry", "Emmanuel", ""], ["Jaisson", "Thibault", ""], ["Muzy", "Jean-Francois", ""]]}, {"id": "1412.7138", "submitter": "Ning Hao", "authors": "Ning Hao and Hao Helen Zhang", "title": "A Note on High Dimensional Linear Regression with Interactions", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of interaction selection has recently caught much attention in\nhigh dimensional data analysis. This note aims to address and clarify several\nfundamental issues in interaction selection for linear regression models,\nespecially when the input dimension p is much larger than the sample size n. We\nfirst discuss issues such as a valid way of defining importance for the main\neffects and interaction effects, the invariance principle, and the strong\nheredity condition. Then we focus on two-stage methods, which are\ncomputationally attractive for large p problems but regarded heuristic in the\nliterature. We will revisit the counterexample of Turlach (2004) and provide\nnew insight to justify two-stage methods from a theoretical perspective. In the\nend, we suggest some new strategies for interaction selection under the\nmarginality principle, which is followed by a numerical example.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:34:01 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 16:47:47 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Hao", "Ning", ""], ["Zhang", "Hao Helen", ""]]}, {"id": "1412.7299", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Chris Sherlock and Paul Fearnhead", "title": "Particle Metropolis-adjusted Langevin algorithms", "comments": "Accepted to Biometrika. Main text: 22 pages and 3 figures.\n  Supplementary material: 18 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new sampling scheme based on Langevin dynamics that is\napplicable within pseudo-marginal and particle Markov chain Monte Carlo\nalgorithms. We investigate this algorithm's theoretical properties under\nstandard asymptotics, which correspond to an increasing dimension of the\nparameters, $n$. Our results show that the behaviour of the algorithm depends\ncrucially on how accurately one can estimate the gradient of the log target\ndensity. If the error in the estimate of the gradient is not sufficiently\ncontrolled as dimension increases, then asymptotically there will be no\nadvantage over the simpler random-walk algorithm. However, if the error is\nsufficiently well-behaved, then the optimal scaling of this algorithm will be\n$O(n^{-1/6})$ compared to $O(n^{-1/2})$ for the random walk. Our theory also\ngives guidelines on how to tune the number of Monte Carlo samples in the\nlikelihood estimate and the proposal step-size.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 09:53:09 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 16:29:52 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 12:52:55 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Nemeth", "Christopher", ""], ["Sherlock", "Chris", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1412.7332", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti, Caterina May and Chiara Tommasi", "title": "Best estimation of functional linear models", "comments": "the best information from the two samples of functions and\n  derivatives: a strong version of the Gauss-Markov theorem. Relaxed an hidden\n  hypothesis on linear independence of the Riesz representation of the\n  Karhunen-Loeve base", "journal-ref": "J.Multivariate Anal. 151 (2016) 54-68", "doi": "10.1016/j.jmva.2016.07.005", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations which are realizations from some continuous process are frequent\nin sciences, engineering, economics, and other fields. We consider linear\nmodels, with possible random effects, where the responses are random functions\nin a suitable Sobolev space. The processes cannot be observed directly. With\nsmoothing procedures from the original data, both the response curves and their\nderivatives can be reconstructed, even separately. From both these samples of\nfunctions, just one sample of representatives is obtained to estimate the\nvector of functional parameters. A simulation study shows the benefits of this\napproach over the common method of using information either on curves or\nderivatives. The main theoretical result is a strong functional version of the\nGauss-Markov theorem. This ensures that the proposed functional estimator is\nmore efficient than the best linear unbiased estimator based only on curves or\nderivatives.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 11:54:47 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 12:13:07 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 12:30:42 GMT"}, {"version": "v4", "created": "Wed, 27 Apr 2016 12:56:24 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Aletti", "Giacomo", ""], ["May", "Caterina", ""], ["Tommasi", "Chiara", ""]]}, {"id": "1412.7468", "submitter": "Yang Feng", "authors": "Pallavi Basu, Yang Feng and Jinchi Lv", "title": "Model Selection in High-Dimensional Misspecified Models", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is indispensable to high-dimensional sparse modeling in\nselecting the best set of covariates among a sequence of candidate models. Most\nexisting work assumes implicitly that the model is correctly specified or of\nfixed dimensions. Yet model misspecification and high dimensionality are common\nin real applications. In this paper, we investigate two classical\nKullback-Leibler divergence and Bayesian principles of model selection in the\nsetting of high-dimensional misspecified models. Asymptotic expansions of these\nprinciples reveal that the effect of model misspecification is crucial and\nshould be taken into account, leading to the generalized AIC and generalized\nBIC in high dimensions. With a natural choice of prior probabilities, we\nsuggest the generalized BIC with prior probability which involves a logarithmic\nfactor of the dimensionality in penalizing model complexity. We further\nestablish the consistency of the covariance contrast matrix estimator in a\ngeneral setting. Our results and new method are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 18:49:19 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Basu", "Pallavi", ""], ["Feng", "Yang", ""], ["Lv", "Jinchi", ""]]}, {"id": "1412.7778", "submitter": "Sairam Rayaprolu", "authors": "Sairam Rayaprolu and Zhiyi Chi", "title": "False Discovery Variance Reduction in Large Scale Simultaneous\n  Hypothesis Tests", "comments": "22 pages, 5 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical dependence between hypotheses poses a significant challenge to\nthe stability of large scale multiple hypotheses testing. Ignoring it often\nresults in an unacceptably large spread in the false positive proportion even\nthough the average value is acceptable [21, 39, 40, 49]. However, the\nstatistical dependence structure of data is often unknown. Using a generic\nsignalprocessing model, Bayesian multiple testing, and simulations, we\ndemonstrate that the variance of the false positive proportion can be\nsubstantially reduced even under unknown short range dependence. We do this by\nmodeling the data generating process as a stationary ergodic binary signal\nprocess embedded in noisy observations. We derive conditional probabilities\nneeded for the Bayesian multiple testing by incorporating nearby observations\ninto a second order Taylor series approximation. Simulations under general\nconditions are carried out to assess the validity and the variance reduction of\nthe approach. Along the way, we address the problem of sampling a random Markov\nmatrix with specified stationary distribution and lower bounds on the top\nabsolute eigenvalues, which is of interest in its own right.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 03:05:13 GMT"}, {"version": "v2", "created": "Sat, 9 Apr 2016 21:00:32 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 00:02:14 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Rayaprolu", "Sairam", ""], ["Chi", "Zhiyi", ""]]}, {"id": "1412.8285", "submitter": "Piotr Zwiernik", "authors": "Mathias Drton, Shaowei Lin, Luca Weihs and Piotr Zwiernik", "title": "Marginal likelihood and model selection for Gaussian latent tree and\n  forest models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian latent tree models, or more generally, Gaussian latent forest models\nhave Fisher-information matrices that become singular along interesting\nsubmodels, namely, models that correspond to subforests. For these\nsingularities, we compute the real log-canonical thresholds (also known as\nstochastic complexities or learning coefficients) that quantify the\nlarge-sample behavior of the marginal likelihood in Bayesian inference. This\nprovides the information needed for a recently introduced generalization of the\nBayesian information criterion. Our mathematical developments treat the general\nsetting of Laplace integrals whose phase functions are sums of squared\ndifferences between monomials and constants. We clarify how in this case real\nlog-canonical thresholds can be computed using polyhedral geometry, and we show\nhow to apply the general theory to the Laplace integrals associated with\nGaussian latent tree and forest models. In simulations and a data example, we\ndemonstrate how the mathematical knowledge can be applied in model selection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 09:10:17 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 02:19:35 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Drton", "Mathias", ""], ["Lin", "Shaowei", ""], ["Weihs", "Luca", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1412.8594", "submitter": "Mohamed Kayid", "authors": "M. Kayid and S. Izadkhah", "title": "A New Extended Mixture Model of Residual Lifetime Distributions", "comments": "Accepted. appears in Operations Research Letters, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first propose a new extended mixture model of residual\nlifetime distributions. We show that this model is suitable in modeling\nresidual lifetime in some practical situations. Several closure properties of\nsome well-known dependence concepts, stochastic orders and aging notions under\nthe formation of this model, are obtained. Finally, preservation properties of\nsome stochastic orders under the formation of the model are discussed and some\nexamples of interest are presented.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 09:48:12 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Kayid", "M.", ""], ["Izadkhah", "S.", ""]]}, {"id": "1412.8695", "submitter": "Nikolas Kantas", "authors": "Nikolas Kantas, Arnaud Doucet, Sumeetpal S. Singh, Jan Maciejowski,\n  Nicolas Chopin", "title": "On Particle Methods for Parameter Estimation in State-Space Models", "comments": "Published at http://dx.doi.org/10.1214/14-STS511 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 3, 328-351", "doi": "10.1214/14-STS511", "report-no": "IMS-STS-STS511", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear non-Gaussian state-space models are ubiquitous in statistics,\neconometrics, information engineering and signal processing. Particle methods,\nalso known as Sequential Monte Carlo (SMC) methods, provide reliable numerical\napproximations to the associated state inference problems. However, in most\napplications, the state-space model of interest also depends on unknown static\nparameters that need to be estimated from the data. In this context, standard\nparticle methods fail and it is necessary to rely on more sophisticated\nalgorithms. The aim of this paper is to present a comprehensive review of\nparticle methods that have been proposed to perform static parameter estimation\nin state-space models. We discuss the advantages and limitations of these\nmethods and illustrate their performance on simple models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 17:21:00 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 12:02:31 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Kantas", "Nikolas", ""], ["Doucet", "Arnaud", ""], ["Singh", "Sumeetpal S.", ""], ["Maciejowski", "Jan", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1412.8762", "submitter": "Marija Vucelja", "authors": "Marija Vucelja", "title": "Lifting -- A nonreversible Markov chain Monte Carlo Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech physics.comp-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo algorithms are invaluable tools for exploring\nstationary properties of physical systems, especially in situations where\ndirect sampling is unfeasible. Common implementations of Monte Carlo algorithms\nemploy reversible Markov chains. Reversible chains obey detailed balance and\nthus ensure that the system will eventually relax to equilibrium. Detailed\nbalance is not necessary for convergence to equilibrium. We review\nnonreversible Markov chains, which violate detailed balance, and yet still\nrelax to a given target stationary distribution. In particular cases,\nnonreversible Markov chains are substantially better at sampling than the\nconventional reversible Markov chains with up to a square root improvement in\nthe convergence time to the steady state. One kind of nonreversible Markov\nchain is constructed from the reversible ones by enlarging the state space and\nby modifying and adding extra transition rates to create non-reversible moves.\nBecause of the augmentation of the state space, such chains are often referred\nto as lifted Markov Chains. We illustrate the use of lifted Markov chains for\nefficient sampling for several examples. The examples include sampling on a\nring, sampling on a torus, the Ising model on a complete graph, and the\none-dimensional Ising model. We also provide a pseudocode implementation,\nreview related work, and discuss the applicability of such methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 20:48:56 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 07:16:55 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2016 23:22:13 GMT"}, {"version": "v4", "created": "Tue, 26 Apr 2016 16:14:34 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Vucelja", "Marija", ""]]}]