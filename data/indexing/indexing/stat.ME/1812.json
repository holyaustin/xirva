[{"id": "1812.00126", "submitter": "Jeffrey Rosenthal", "authors": "Jeffrey S. Rosenthal", "title": "Simple Confidence Intervals for MCMC Without CLTs", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note argues that 95% confidence intervals for MCMC estimates can\nbe obtained even without establishing a CLT, by multiplying their widths by\n2.3.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 02:22:32 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1812.00215", "submitter": "Bo Zhang", "authors": "Bo Zhang and Dylan Small", "title": "A calibrated sensitivity analysis for matched observational studies with\n  application to the effect of second-hand smoke exposure on blood lead levels\n  in U.S. children", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matched observational studies are commonly used to study treatment effects in\nnon-randomized data. After matching for observed confounders, there could\nremain bias from unobserved confounders. A standard way to address this problem\nis to do a sensitivity analysis. A sensitivity analysis asks how sensitive the\nresult is to a hypothesized unmeasured confounder U. One method, known as\nsimultaneous sensitivity analysis, has two sensitivity parameters: one relating\nU to treatment assignment and the other to response. This method assumes that\nin each matched set, U is distributed to make the bias worst. This approach has\ntwo concerning features. First, this worst case distribution of U in each\nmatched set does not correspond to a realistic distribution of U in the\npopulation. Second, sensitivity parameters are in absolute scales which are\nhard to compare to observed covariates. We address these concerns by\nintroducing a method that endows U with a probability distribution in the\npopulation and calibrates the unmeasured confounder to the observed covariates.\nWe compare our method to simultaneous sensitivity analysis in simulations and\nin a study of the effect of second-hand smoke exposure on blood lead levels in\nU.S. children.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 15:25:44 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 20:44:28 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 23:45:33 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhang", "Bo", ""], ["Small", "Dylan", ""]]}, {"id": "1812.00250", "submitter": "Wenge Guo", "authors": "Zhiying Qiu, Li Yu, Wenge Guo", "title": "A Family-based Graphical Approach for Testing Hierarchically Ordered\n  Families of Hypotheses", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.23109.29929", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In applications of clinical trials, tested hypotheses are often grouped as\nmultiple hierarchically ordered families. To test such structured hypotheses,\nvarious gatekeeping strategies have been developed in the literature, such as\nseries gatekeeping, parallel gatekeeping, tree-structured gatekeeping\nstrategies, etc. However, these gatekeeping strategies are often either\nnon-intuitive or less flexible when addressing increasingly complex logical\nrelationships among families of hypotheses. In order to overcome the issue, in\nthis paper, we develop a new family-based graphical approach, which can easily\nderive and visualize different gatekeeping strategies. In the proposed\napproach, a directed and weighted graph is used to represent the generated\ngatekeeping strategy where each node corresponds to a family of hypotheses and\ntwo simple updating rules are used for updating the critical value of each\nfamily and the transition coefficient between any two families. Theoretically,\nwe show that the proposed graphical approach strongly controls the overall\nfamilywise error rate at a pre-specified level. Through some case studies and a\nreal clinical example, we demonstrate simplicity and flexibility of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 19:38:14 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Qiu", "Zhiying", ""], ["Yu", "Li", ""], ["Guo", "Wenge", ""]]}, {"id": "1812.00258", "submitter": "Wenge Guo", "authors": "Wenge Guo, Gavin Lynch, Joseph P. Romano", "title": "A New Approach for Large Scale Multiple Testing with Application to FDR\n  Control for Graphically Structured Hypotheses", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many large scale multiple testing applications, the hypotheses often have\na known graphical structure, such as gene ontology in gene expression data.\nExploiting this graphical structure in multiple testing procedures can improve\npower as well as aid in interpretation. However, incorporating the structure\ninto large scale testing procedures and proving that an error rate, such as the\nfalse discovery rate (FDR), is controlled can be challenging. In this paper, we\nintroduce a new general approach for large scale multiple testing, which can\naid in developing new procedures under various settings with proven control of\ndesired error rates. This approach is particularly useful for developing FDR\ncontrolling procedures, which is simplified as the problem of developing\nper-family error rate (PFER) controlling procedures. Specifically, for testing\nhypotheses with a directed acyclic graph (DAG) structure, by using the general\napproach, under the assumption of independence, we first develop a specific\nPFER controlling procedure and based on this procedure, then develop a new FDR\ncontrolling procedure, which can preserve the desired DAG structure among the\nrejected hypotheses. Through a small simulation study and a real data analysis,\nwe illustrate nice performance of the proposed FDR controlling procedure for\nDAG-structured hypotheses.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 20:23:19 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Guo", "Wenge", ""], ["Lynch", "Gavin", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1812.00260", "submitter": "Andrea Arf\\`e", "authors": "Andrea Arf\\`e, Stefano Peluso, Pietro Muliere", "title": "The semi-Markov beta-Stacy process: a Bayesian non-parametric prior for\n  semi-Markov processes", "comments": "Accepted for publication in the journal Statistical Inference for\n  Stochastic Processes on July 23, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on Bayesian methods for the analysis of discrete-time\nsemi-Markov processes is sparse. In this paper, we introduce the semi-Markov\nbeta-Stacy process, a stochastic process useful for the Bayesian non-parametric\nanalysis of semi-Markov processes. The semi-Markov beta-Stacy process is\nconjugate with respect to data generated by a semi-Markov process, a property\nwhich makes it easy to obtain probabilistic forecasts. Its predictive\ndistributions are characterized by a reinforced random walk on a system of\nurns.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 20:36:54 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 13:53:59 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Arf\u00e8", "Andrea", ""], ["Peluso", "Stefano", ""], ["Muliere", "Pietro", ""]]}, {"id": "1812.00331", "submitter": "Yuehan Yang", "authors": "Yuehan Yang and Ji Zhu and Edward I. George", "title": "MSP: A Multi-step Screening Procedure for Sparse Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Multi-step Screening Procedure (MSP) for the recovery of sparse\nlinear models in high-dimensional data. This method is based on a repeated\nsmall penalty strategy that quickly converges to an estimate within a few\niterations. Specifically, in each iteration, an adaptive lasso regression with\na small penalty is fit within the reduced feature space obtained from the\nprevious step, rendering its computational complexity roughly comparable with\nthe Lasso. MSP is shown to select the true model under complex correlation\nstructures among the predictors and response, even when the irrepresentable\ncondition fails. Further, under suitable regularity conditions, MSP achieves\nthe optimal minimax rate $(q \\log n /n)^{1/2}$ for the upper bound of\n$l_2$-norm error. Numerical comparisons show that the method works effectively\nboth in model selection and estimation, and the MSP fitted model is stable over\na range of small tuning parameter values, eliminating the need to choose the\ntuning parameter by cross-validation. We also apply MSP to financial data and\nshow that MSP is successful in asset allocation selection.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 05:25:57 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 12:21:59 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Yang", "Yuehan", ""], ["Zhu", "Ji", ""], ["George", "Edward I.", ""]]}, {"id": "1812.00492", "submitter": "Linh Nghiem", "authors": "Linh Nghiem, Michael Byrd, Cornelis Potgieter", "title": "Estimation in linear errors-in-variables models with unknown error\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation in linear errors-in-variables models typically requires\nthat the measurement error distribution be known (or estimable from replicate\ndata). A generalized method of moments approach can be used to estimate model\nparameters in the absence of knowledge of the error distributions, but requires\nthe existence of a large number of model moments. In this paper, parameter\nestimation based on the phase function, a normalized version of the\ncharacteristic function, is considered. This approach requires the model\ncovariates to have asymmetric distributions, while the error distributions are\nsymmetric. Parameter estimation is then based on minimizing a distance function\nbetween the empirical phase functions of the noisy covariates and the outcome\nvariable. No knowledge of the measurement error distribution is required to\ncalculate this estimator. Both the asymptotic and finite sample properties of\nthe estimator are considered. The connection between the phase function\napproach and method of moments is also discussed. The estimation of standard\nerrors is also considered and a modified bootstrap algorithm is proposed for\nfast computation. The newly proposed estimator is competitive when compared to\ngeneralized method of moments, even while making fewer model assumptions on the\nmeasurement error. Finally, the proposed method is applied to a real dataset\nconcerning the measurement of air pollution.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 00:12:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Nghiem", "Linh", ""], ["Byrd", "Michael", ""], ["Potgieter", "Cornelis", ""]]}, {"id": "1812.00532", "submitter": "Yiming Sun", "authors": "Yiming Sun, Yige Li, Amy Kuceyeski, Sumanta Basu", "title": "Large Spectral Density Matrix Estimation by Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral density matrix estimation of multivariate time series is a classical\nproblem in time series and signal processing. In modern neuroscience, spectral\ndensity based metrics are commonly used for analyzing functional connectivity\namong brain regions. In this paper, we develop a non-asymptotic theory for\nregularized estimation of high-dimensional spectral density matrices of\nGaussian and linear processes using thresholded versions of averaged\nperiodograms. Our theoretical analysis ensures that consistent estimation of\nspectral density matrix of a $p$-dimensional time series using $n$ samples is\npossible under high-dimensional regime $\\log p / n \\rightarrow 0$ as long as\nthe true spectral density is approximately sparse. A key technical component of\nour analysis is a new concentration inequality of average periodogram around\nits expectation, which is of independent interest. Our estimation consistency\nresults complement existing results for shrinkage based estimators of\nmultivariate spectral density, which require no assumption on sparsity but only\nensure consistent estimation in a regime $p^2/n \\rightarrow 0$. In addition,\nour proposed thresholding based estimators perform consistent and automatic\nedge selection when learning coherence networks among the components of a\nmultivariate time series. We demonstrate the advantage of our estimators using\nsimulation studies and a real data application on functional connectivity\nanalysis with fMRI data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 03:04:08 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Sun", "Yiming", ""], ["Li", "Yige", ""], ["Kuceyeski", "Amy", ""], ["Basu", "Sumanta", ""]]}, {"id": "1812.00538", "submitter": "Cai Li", "authors": "Cai Li, Luo Xiao, Sheng Luo", "title": "Fast Covariance Estimation for Multivariate Sparse Functional Data", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance estimation is essential yet underdeveloped for analyzing\nmultivariate functional data. We propose a fast covariance estimation method\nfor multivariate sparse functional data using bivariate penalized splines. The\ntensor-product B-spline formulation of the proposed method enables a simple\nspectral decomposition of the associated covariance operator and explicit\nexpressions of the resulting eigenfunctions as linear combinations of B-spline\nbases, thereby dramatically facilitating subsequent principal component\nanalysis. We derive a fast algorithm for selecting the smoothing parameters in\ncovariance smoothing using leave-one-subject-out cross-validation. The method\nis evaluated with extensive numerical studies and applied to an Alzheimer's\ndisease study with multiple longitudinal outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 03:20:44 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 21:25:34 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Cai", ""], ["Xiao", "Luo", ""], ["Luo", "Sheng", ""]]}, {"id": "1812.00641", "submitter": "David Zucker PhD", "authors": "David M. Zucker and Malka Gorfine", "title": "An improved fully nonparametric estimator of the marginal survival\n  function based on case-control clustered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A case-control family study is a study where individuals with a disease of\ninterest (case probands) and individuals without the disease (control probands)\nare randomly sampled from a well-defined population. Possibly right-censored\nage at onset and disease status are observed for both probands and their\nrelatives. Correlation among the outcomes within a family is induced by factors\nsuch as inherited genetic susceptibility, shared environment, and common\nbehavior patterns. For this setting, we present a nonparametric estimator of\nthe marginal survival function, based on local linear estimation of conditional\nsurvival functions. Asymptotic theory for the estimator is provided, and\nsimulation results are presented showing that the method performs well. The\nmethod is illustrated on data from a prostate cancer study.\n  Keywords: case-control; family study; multivariate survival; nonparametric\nestimator; local linear\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 10:16:50 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zucker", "David M.", ""], ["Gorfine", "Malka", ""]]}, {"id": "1812.00789", "submitter": "Rex Cheung", "authors": "Rex C. Y. Cheung, Alexander Aue, Seungyong Hwang, Thomas C. M. Lee", "title": "Simultaneous Detection of Multiple Change Points and Community\n  Structures in Time Series of Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many complex systems, networks and graphs arise in a natural manner.\nOften, time evolving behavior can be easily found and modeled using time-series\nmethodology. Amongst others, two common research problems in network analysis\nare community detection and change-point detection. Community detection aims at\nfinding specific sub-structures within the networks, and change-point detection\ntries to find the time points at which sub-structures change. We propose a\nnovel methodology to detect both community structures and change points\nsimultaneously based on a model selection framework in which the Minimum\nDescription Length Principle (MDL) is utilized as minimizing objective\ncriterion. The promising practical performance of the proposed method is\nillustrated via a series of numerical experiments and real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:02:06 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 18:22:14 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Cheung", "Rex C. Y.", ""], ["Aue", "Alexander", ""], ["Hwang", "Seungyong", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1812.00924", "submitter": "Alexandre Emerick", "authors": "Alexandre A. Emerick", "title": "Analysis of Geometric Selection of the Data-Error Covariance Inflation\n  for ES-MDA", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ensemble smoother with multiple data assimilation (ES-MDA) is becoming a\npopular assisted history matching method. In its standard form, the method\nrequires the specification of the number of iterations in advance. If the\nselected number of iterations is not enough, the entire data assimilation must\nbe restarted. Moreover, ES-MDA also requires the selection of data-error\ncovariance inflations. The typical choice is to select constant values.\nHowever, previous works indicate that starting with large inflation and\ngradually decreasing during the data assimilation steps may improve the quality\nof the final models.\n  This paper presents an analysis of the use of geometrically decreasing\nsequences of the data-error covariance inflations. In particular, the paper\ninvestigates a recently introduced procedure based on the singular values of a\nsensitivity matrix computed from the prior ensemble. The paper also introduces\na novel procedure to select the inflation factors. The performance of the data\nassimilation schemes is evaluated in three reservoir history-matching problems\nwith increasing level of complexity. The first problem is a small synthetic\ncase which illustrates that the standard ES-MDA scheme with constant inflation\nmay result in overcorrection of the permeability field and that a geometric\nsequence can alleviate this problem. The second problem is a recently published\nbenchmark and the third one is a field case with real production data. The data\nassimilation schemes are compared in terms of a data-mismatch and a\nmodel-change norm. The first norm evaluates the ability of the models to\nreproduce the observed data. The second norm evaluates the amount of changes in\nthe prior model. The results indicate that geometric inflations can generate\nsolutions with good balance between the two norms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 17:28:43 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Emerick", "Alexandre A.", ""]]}, {"id": "1812.01172", "submitter": "Longyang Wu", "authors": "Longyang Wu, Chengguo Weng, Xu Wang, Kesheng Wang, and Xuefeng Liu", "title": "Test of Covariance and Correlation Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a generalized cosine measure between two symmetric matrices, we\npropose a general framework for one-sample and two-sample tests of covariance\nand correlation matrices. We also develop a set of associated permutation\nalgorithms for some common one-sample tests, such as the tests of sphericity,\nidentity and compound symmetry, and the $K$-sample tests of multivariate\nequality of covariance or correlation matrices. The proposed method is very\nflexible in the sense that it does not assume any underlying distributions and\ndata generation models. Moreover, it allows data to have different marginal\ndistributions in both the one-sample identity and $K$-sample tests. Through\nreal datasets and extensive simulations, we demonstrate that the proposed\nmethod performs well in terms of empirical type I error and power in a variety\nof hypothesis testing situations in which data of different sizes and\ndimensions are generated using different distributions and generation models.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 02:32:29 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Wu", "Longyang", ""], ["Weng", "Chengguo", ""], ["Wang", "Xu", ""], ["Wang", "Kesheng", ""], ["Liu", "Xuefeng", ""]]}, {"id": "1812.01188", "submitter": "Yilin Zhang", "authors": "Yilin Zhang, Karl Rohe, Sebastien Roch", "title": "Reducing Seed Bias in Respondent-Driven Sampling by Estimating Block\n  Transition Probabilities", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.PR stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a popular approach to study marginalized\nor hard-to-reach populations. It collects samples from a networked population\nby incentivizing participants to refer their friends into the study. One major\nchallenge in analyzing RDS samples is seed bias. Seed bias refers to the fact\nthat when the social network is divided into multiple communities (or blocks),\nthe RDS sample might not provide a balanced representation of the different\ncommunities in the population, and such unbalance is correlated with the\ninitial participant (or the seed). In this case, the distributions of\nestimators are typically non-trivial mixtures, which are determined (1) by the\nseed and (2) by how the referrals transition from one block to another. This\npaper shows that (1) block-transition probabilities are easy to estimate with\nhigh accuracy, and (2) we can use these estimated block-transition\nprobabilities to estimate the stationary distribution over blocks and thus, an\nestimate of the block proportions. This stationary distribution on blocks has\npreviously been used in the RDS literature to evaluate whether the sampling\nprocess has appeared to `mix'. We use these estimated block proportions in a\nsimple post-stratified (PS) estimator that greatly diminishes seed bias. By\naggregating over the blocks/strata in this way, we prove that the PS estimator\nis $\\sqrt{n}$-consistent under a Markov model, even when other estimators are\nnot. Simulations show that the PS estimator has smaller Root Mean Square Error\n(RMSE) compared to the state-of-the-art estimators.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 03:08:38 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Zhang", "Yilin", ""], ["Rohe", "Karl", ""], ["Roch", "Sebastien", ""]]}, {"id": "1812.01322", "submitter": "Karla DiazOrdaz", "authors": "Karla DiazOrdaz and James Carpenter", "title": "Local average treatment effects estimation via substantive model\n  compatible multiple imputation", "comments": "24 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-adherence to assigned treatment is common in randomised controlled trials\n(RCTs). Recently, there has been an increased interest in estimating causal\neffects of treatment received, for example the so-called local average\ntreatment effect (LATE). Instrumental variables (IV) methods can be used for\nidentification, with estimation proceeding either via fully parametric mixture\nmodels or two-stage least squares (TSLS). TSLS is popular but can be\nproblematic for binary outcomes where the estimand of interest is a causal odds\nratio. Mixture models are rarely used in practice, perhaps because of their\nperceived complexity and need for specialist software. Here, we propose using\nmultiple imputation (MI) to impute the latent compliance class appearing in the\nmixture models. Since such models include an interaction term between\ncompliance class and randomised treatment, we use `substantive model\ncompatible' MI (SMC MIC), which can also address other missing data, before\nfitting the mixture models via maximum likelihood to the MI datasets and\ncombining results via Rubin's rules. We use simulations to compare the\nperformance of SMC MIC to existing approaches and also illustrate the methods\nby re-analysing a RCT in UK primary health. We show that SMC MIC can be more\nefficient than full Bayesian estimation when auxiliary variables are\nincorporated, and is superior to two-stage methods, especially for binary\noutcomes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 10:45:23 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["DiazOrdaz", "Karla", ""], ["Carpenter", "James", ""]]}, {"id": "1812.01328", "submitter": "Karla DiazOrdaz", "authors": "Schadrac C. Agbla, Bianca De Stavola and Karla DiazOrdaz", "title": "Estimating cluster-level local average treatment effects in cluster\n  randomised trials with non-adherence", "comments": "21 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-adherence to assigned treatment is a common issue in cluster randomised\ntrials (CRTs). In these settings, the efficacy estimand may be also of\ninterest. Many methodological contributions in recent years have advocated\nusing instrumental variables to identify and estimate the local average\ntreatment effect (LATE). However, the clustered nature of randomisation in CRTs\nadds to the complexity of such analyses.\n  In this paper, we show that under certain assumptions, the LATE can be\nestimated via two-stage least squares (TSLS) using cluster-level summaries of\noutcomes and treatment received. Implementation needs to account for this, as\nwell as the possible heteroscedasticity, to obtain valid inferences.\n  We use simulations to assess the performance of TSLS of cluster-level\nsummaries under cluster-level or individual-level non-adherence, with and\nwithout weighting and robust standard errors. We also explore the impact of\nadjusting for cluster-level covariates and of appropriate degrees of freedom\ncorrection for inference.\n  We find that TSLS estimation using cluster-level summaries provides estimates\nwith small to negligible bias and coverage close to nominal level, provided\nsmall sample degrees of freedom correction is used for inference, with\nappropriate use of robust standard errors. We illustrate the methods by\nre-analysing a CRT in UK primary health settings.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 10:58:54 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Agbla", "Schadrac C.", ""], ["De Stavola", "Bianca", ""], ["DiazOrdaz", "Karla", ""]]}, {"id": "1812.01412", "submitter": "Amit Sharma", "authors": "Amit Sharma", "title": "Necessary and Probably Sufficient Test for Finding Valid Instrumental\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can instrumental variables be found from data? While instrumental variable\n(IV) methods are widely used to identify causal effect, testing their validity\nfrom observed data remains a challenge. This is because validity of an IV\ndepends on two assumptions, exclusion and as-if-random, that are largely\nbelieved to be untestable from data. In this paper, we show that under certain\nconditions, testing for instrumental variables is possible. We build upon prior\nwork on necessary tests to derive a test that characterizes the odds of being a\nvalid instrument, thus yielding the name \"necessary and probably sufficient\".\nThe test works by defining the class of invalid-IV and valid-IV causal models\nas Bayesian generative models and comparing their marginal likelihood based on\nobserved data. When all variables are discrete, we also provide a method to\nefficiently compute these marginal likelihoods.\n  We evaluate the test on an extensive set of simulations for binary data,\ninspired by an open problem for IV testing proposed in past work. We find that\nthe test is most powerful when an instrument follows monotonicity---effect on\ntreatment is either non-decreasing or non-increasing---and has moderate-to-weak\nstrength; incidentally, such instruments are commonly used in observational\nstudies. Among as-if-random and exclusion, it detects exclusion violations with\nhigher power. Applying the test to IVs from two seminal studies on instrumental\nvariables and five recent studies from the American Economic Review shows that\nmany of the instruments may be flawed, at least when all variables are\ndiscretized. The proposed test opens the possibility of data-driven validation\nand search for instrumental variables.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 13:53:21 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Sharma", "Amit", ""]]}, {"id": "1812.01774", "submitter": "Ningshan Zhang", "authors": "Ningshan Zhang and Jeffrey S. Simonoff", "title": "Joint Latent Class Trees: A Tree-Based Approach to Modeling\n  Time-to-event and Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semiparametric, tree based joint latent class\nmodeling approach (JLCT) to model the joint behavior of longitudinal and\ntime-to-event data. Existing joint latent class modeling approaches are\nparametric and can suffer from high computational cost. The most common\nparametric approach, the joint latent class model (JLCM), further restricts\nanalysis to using time-invariant covariates in modeling survival risks and\nlatent class memberships. Instead, the proposed JLCT is fast to fit, and can\nuse time-varying covariates in all of its modeling components. We demonstrate\nthe prognostic value of using time-varying covariates, and therefore the\nadvantage of JLCT over JLCM on simulated data. We further apply JLCT to the\nPAQUID data set and confirm its superior prediction performance and\norders-of-magnitude speedup over JLCM.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 01:36:49 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 21:15:08 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 22:42:08 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 03:09:56 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Zhang", "Ningshan", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "1812.01786", "submitter": "Ran Yang", "authors": "Ran Yang, Daniel Apley, Jeremy Staum, David Ruppert", "title": "Density Deconvolution with Additive Measurement Errors using Quadratic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution estimation for noisy data via density deconvolution is a\nnotoriously difficult problem for typical noise distributions like Gaussian. We\ndevelop a density deconvolution estimator based on quadratic programming (QP)\nthat can achieve better estimation than kernel density deconvolution methods.\nThe QP approach appears to have a more favorable regularization tradeoff\nbetween oversmoothing vs. oscillation, especially at the tails of the\ndistribution. An additional advantage is that it is straightforward to\nincorporate a number of common density constraints such as nonnegativity,\nintegration-to-one, unimodality, tail convexity, tail monotonicity, and support\nconstraints. We demonstrate that the QP approach has outstanding estimation\nperformance relative to existing methods. Its performance is superior when only\nthe universally applicable nonnegativity and integration-to-one constraints are\nincorporated, and incorporating additional common constraints when applicable\n(e.g., nonnegative support, unimodality, tail monotonicity or convexity, etc.)\ncan further substantially improve the estimation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 02:21:35 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yang", "Ran", ""], ["Apley", "Daniel", ""], ["Staum", "Jeremy", ""], ["Ruppert", "David", ""]]}, {"id": "1812.01831", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin and Fang Yao", "title": "Intrinsic Riemannian Functional Data Analysis", "comments": "46 pages, 3 figures, 3 tables", "journal-ref": "The Annals of Statistics 2019, Vol. 47, No. 6, 3533-3577", "doi": "10.1214/18-AOS1787", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a novel and foundational framework for analyzing\ngeneral Riemannian functional data, in particular a new development of tensor\nHilbert spaces along curves on a manifold. Such spaces enable us to derive\nKarhunen-Loeve expansion for Riemannian random processes. This framework also\nfeatures an approach to compare objects from different tensor Hilbert spaces,\nwhich paves the way for asymptotic analysis in Riemannian functional data\nanalysis. Built upon intrinsic geometric concepts such as vector field,\nLevi-Civita connection and parallel transport on Riemannian manifolds, the\ndeveloped framework applies to not only Euclidean submanifolds but also\nmanifolds without a natural ambient space. As applications of this framework,\nwe develop intrinsic Riemannian functional principal component analysis\n(iRFPCA) and intrinsic Riemannian functional linear regression (iRFLR) that are\ndistinct from their traditional and ambient counterparts. We also provide\nestimation procedures for iRFPCA and iRFLR, and investigate their asymptotic\nproperties within the intrinsic geometry. Numerical performance is illustrated\nby simulated and real examples.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 06:46:20 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 13:12:57 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Lin", "Zhenhua", ""], ["Yao", "Fang", ""]]}, {"id": "1812.01882", "submitter": "Henning Omre", "authors": "Henning Omre and Kjartan Rimstad", "title": "Bayesian Spatial Inversion and Conjugate Selection Gaussian Prior Models", "comments": "53 pages, 12 figures, 1 appendix, 5 supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of conjugate prior models for a given likelihood\nfunction in Bayesian spatial inversion. The conjugate class of prior models can\nbe selection extended and still remain conjugate. We demonstrate the generality\nof selection Gaussian prior models, representing multi-modality, skewness and\nheavy-tailedness. For Gauss-linear likelihood functions, the posterior model is\nalso selection Gaussian. The model parameters of the posterior pdf are\nexplisite functions of the model parameters of the likelihood and prior models\n- and the actual observations, of course. Efficient algorithms for simulation\nof and prediction for the selection Gaussian posterior pdf are defined.\nInference of the model parameters in the selection Gaussian prior pdf, based on\none training image of the spatial variable, can be reliably made by a maximum\nlikelihood criterion and numerical optimization. Lastly, a seismic inversion\ncase study is presented, and improvements of $ 20$-$40\\%$ in prediction\nmean-square-error, relative to traditional Gaussian inversion, are found.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:58:24 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Omre", "Henning", ""], ["Rimstad", "Kjartan", ""]]}, {"id": "1812.01938", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis, David Firth", "title": "Jeffreys-prior penalty, finiteness and shrinkage in binomial-response\n  generalized linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalization of the likelihood by Jeffreys' invariant prior, or by a positive\npower thereof, is shown to produce finite-valued maximum penalized likelihood\nestimates in a broad class of binomial generalized linear models. The class of\nmodels includes logistic regression, where the Jeffreys-prior penalty is known\nadditionally to reduce the asymptotic bias of the maximum likelihood estimator;\nand also models with other commonly used link functions such as probit and\nlog-log. Shrinkage towards equiprobability across observations, relative to the\nmaximum likelihood estimator, is established theoretically and is studied\nthrough illustrative examples. Some implications of finiteness and shrinkage\nfor inference are discussed, particularly when inference is based on Wald-type\nprocedures. A widely applicable procedure is developed for computation of\nmaximum penalized likelihood estimates, by using repeated maximum likelihood\nfits with iteratively adjusted binomial responses and totals. These theoretical\nresults and methods underpin the increasingly widespread use of reduced-bias\nand similarly penalized binomial regression models in many applied fields.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 12:02:39 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 16:28:10 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 17:38:59 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2020 20:17:37 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Kosmidis", "Ioannis", ""], ["Firth", "David", ""]]}, {"id": "1812.02127", "submitter": "Konstantinos Spiliopoulos", "authors": "Konstantinos Spiliopoulos", "title": "Information geometry for approximate Bayesian computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to explore the basic Approximate Bayesian\nComputation (ABC) algorithm via the lens of information theory. ABC is a widely\nused algorithm in cases where the likelihood of the data is hard to work with\nor intractable, but one can simulate from it. We use relative entropy ideas to\nanalyze the behavior of the algorithm as a function of the threshold parameter\nand of the size of the data. Relative entropy here is data driven as it depends\non the values of the observed statistics. Relative entropy also allows us to\nexplore the effect of the distance metric and sets up a mathematical framework\nfor sensitivity analysis allowing to find important directions which could lead\nto lower computational cost of the algorithm for the same level of accuracy. In\naddition, we also investigate the bias of the estimators for generic\nobservables as a function of both the threshold parameters and the size of the\ndata. Our analysis provides error bounds on performance for positive tolerances\nand finite sample sizes. Simulation studies complement and illustrate the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:30:03 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 19:35:19 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1812.02130", "submitter": "Cheng Zheng", "authors": "Ran Dai, Cheng Zheng, Mei-Jie Zhang", "title": "On High Dimensional Covariate Adjustment for Estimating Causal Effects\n  in Randomized Trials with Survival Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to improve the efficiency in estimating the\naverage causal effect (ACE) on the survival scale where right-censoring exists\nand high-dimensional covariate information is available. We propose new\nestimators using regularized survival regression and survival random forests\n(SRF) to make the adjustment for the high dimensional covariates to improve\nefficiency. We study the behavior of the adjusted estimator under mild\nassumptions and show theoretical guarantees that the proposed estimators are\nmore efficient than the unadjusted ones asymptotically when using SRF for\nadjustment. In addition, these adjusted estimators are $\\sqrt{n}$- consistent\nand asymptotically normally distributed. The finite sample behavior of our\nmethods are studied by simulation, and the results are in agreement with the\ntheoretical results. We also illustrate our methods by analyzing the real data\nfrom transplant research to identify the relative effectiveness of identical\nsibling donors compared to unrelated donors with the adjustment of cytogenetic\nabnormalities.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:35:48 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 20:56:09 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 20:08:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Dai", "Ran", ""], ["Zheng", "Cheng", ""], ["Zhang", "Mei-Jie", ""]]}, {"id": "1812.02149", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "On nonparametric estimation of a mixing density via the predictive\n  recursion algorithm", "comments": "22 pages, 5 figures. Comments welcome at\n  https://www.researchers.one/article/2018-12-5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of a mixing density based on observations from the\ncorresponding mixture is a challenging statistical problem. This paper surveys\nthe literature on a fast, recursive estimator based on the predictive recursion\nalgorithm. After introducing the algorithm and giving a few examples, I\nsummarize the available asymptotic convergence theory, describe an important\nsemiparametric extension, and highlight two interesting applications. I\nconclude with a discussion of several recent developments in this area and some\nopen problems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 18:31:38 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1812.02227", "submitter": "Marco Morucci", "authors": "Marco Morucci, Md. Noor-E-Alam, Cynthia Rudin", "title": "A robust approach to quantifying uncertainty in matching problems of\n  causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unquantified sources of uncertainty in observational causal analyses can\nbreak the integrity of the results. One would never want another analyst to\nrepeat a calculation with the same dataset, using a seemingly identical\nprocedure, only to find a different conclusion. However, as we show in this\nwork, there is a typical source of uncertainty that is essentially never\nconsidered in observational causal studies: the choice of match assignment for\nmatched groups, that is, which unit is matched to which other unit before a\nhypothesis test is conducted. The choice of match assignment is anything but\ninnocuous, and can have a surprisingly large influence on the causal\nconclusions. Given that a vast number of causal inference studies test\nhypotheses on treatment effects after treatment cases are matched with similar\ncontrol cases, we should find a way to quantify how much this extra source of\nuncertainty impacts results. What we would really like to be able to report is\nthat \\emph{no matter} which match assignment is made, as long as the match is\nsufficiently good, then the hypothesis test result still holds. In this paper,\nwe provide methodology based on discrete optimization to create robust tests\nthat explicitly account for this possibility. We formulate robust tests for\nbinary and continuous data based on common test statistics as integer linear\nprograms solvable with common methodologies. We study the finite-sample\nbehavior of our test statistic in the discrete-data case. We apply our methods\nto simulated and real-world datasets and show that they can produce useful\nresults in practical applied settings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 21:11:21 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 16:26:46 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 19:06:59 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Morucci", "Marco", ""], ["Noor-E-Alam", "Md.", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1812.02337", "submitter": "Qihui Chen", "authors": "Qihui Chen, Zheng Fang", "title": "Improved Inference on the Rank of a Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general framework for conducting inference on the rank\nof an unknown matrix $\\Pi_0$. A defining feature of our setup is the null\nhypothesis of the form $\\mathrm H_0: \\mathrm{rank}(\\Pi_0)\\le r$. The problem is\nof first order importance because the previous literature focuses on $\\mathrm\nH_0': \\mathrm{rank}(\\Pi_0)= r$ by implicitly assuming away\n$\\mathrm{rank}(\\Pi_0)<r$, which may lead to invalid rank tests due to\nover-rejections. In particular, we show that limiting distributions of test\nstatistics under $\\mathrm H_0'$ may not stochastically dominate those under\n$\\mathrm{rank}(\\Pi_0)<r$. A multiple test on the nulls\n$\\mathrm{rank}(\\Pi_0)=0,\\ldots,r$, though valid, may be substantially\nconservative. We employ a testing statistic whose limiting distributions under\n$\\mathrm H_0$ are highly nonstandard due to the inherent irregular natures of\nthe problem, and then construct bootstrap critical values that deliver size\ncontrol and improved power. Since our procedure relies on a tuning parameter, a\ntwo-step procedure is designed to mitigate concerns on this nuisance. We\nadditionally argue that our setup is also important for estimation. We\nillustrate the empirical relevance of our results through testing\nidentification in linear IV models that allows for clustered data and inference\non sorting dimensions in a two-sided matching model with transferrable utility.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 03:46:11 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 13:46:29 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Chen", "Qihui", ""], ["Fang", "Zheng", ""]]}, {"id": "1812.02401", "submitter": "Wessel van Wieringen", "authors": "Diederik S. Laman Trip, Wessel N. van Wieringen", "title": "A parallel algorithm for penalized learning of the multivariate\n  exponential family from data of mixed types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational efficient evaluation of penalized estimators of multivariate\nexponential family distributions is sought. These distributions encompass among\nothers Markov random fields with variates of mixed type (e.g. binary and\ncontinuous) as special case of interest. The model parameter is estimated by\nmaximization of the pseudo-likelihood augmented with a convex penalty. The\nestimator is shown to be consistent. With a world of multi-core computers in\nmind, a computationally efficient parallel Newton-Raphson algorithm is\npresented for numerical evaluation of the estimator alongside conditions for\nits convergence. Parallelization comprises the division of the parameter vector\ninto subvectors that are estimated simultaneously and subsequently aggregated\nto form an estimate of the original parameter. This approach may also enable\nefficient numerical evaluation of other high-dimensional estimators. The\nperformance of the proposed estimator and algorithm are evaluated and compared\nin a simulation study. Finally, the paper concludes with an illustration of the\npresented methodology in the reconstruction of the conditional independence\nnetwork from data of an integrative omics study.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 08:37:18 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 09:32:34 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Trip", "Diederik S. Laman", ""], ["van Wieringen", "Wessel N.", ""]]}, {"id": "1812.02409", "submitter": "Justin Chown", "authors": "Justin Chown, Nicolai Bissantz and Holger Dette", "title": "Goodness-of-fit testing the error distribution in multivariate indirect\n  regression", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a goodness-of-fit test for the distribution of errors from a\nmultivariate indirect regression model. The test statistic is based on the\nKhmaladze transformation of the empirical process of standardized residuals.\nThis goodness-of-fit test is consistent at the root-n rate of convergence, and\nthe test can maintain power against local alternatives converging to the null\nat a root-n rate.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 09:11:32 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Chown", "Justin", ""], ["Bissantz", "Nicolai", ""], ["Dette", "Holger", ""]]}, {"id": "1812.02609", "submitter": "Emilia Pompe", "authors": "Emilia Pompe, Chris Holmes, Krzysztof {\\L}atuszy\\'nski", "title": "A Framework for Adaptive MCMC Targeting Multimodal Distributions", "comments": "65 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Monte Carlo method for sampling from multimodal\ndistributions. The idea of this technique is based on splitting the task into\ntwo: finding the modes of a target distribution $\\pi$ and sampling, given the\nknowledge of the locations of the modes. The sampling algorithm relies on steps\nof two types: local ones, preserving the mode; and jumps to regions associated\nwith different modes. Besides, the method learns the optimal parameters of the\nalgorithm while it runs, without requiring user intervention. Our technique\nshould be considered as a flexible framework, in which the design of moves can\nfollow various strategies known from the broad MCMC literature.\n  In order to design an adaptive scheme that facilitates both local and jump\nmoves, we introduce an auxiliary variable representing each mode and we define\na new target distribution $\\tilde{\\pi}$ on an augmented state space\n$\\mathcal{X}~\\times~\\mathcal{I}$, where $\\mathcal{X}$ is the original state\nspace of $\\pi$ and $\\mathcal{I}$ is the set of the modes. As the algorithm runs\nand updates its parameters, the target distribution $\\tilde{\\pi}$ also keeps\nbeing modified. This motivates a new class of algorithms, Auxiliary Variable\nAdaptive MCMC. We prove general ergodic results for the whole class before\nspecialising to the case of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:36:54 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 14:34:36 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Pompe", "Emilia", ""], ["Holmes", "Chris", ""], ["\u0141atuszy\u0144ski", "Krzysztof", ""]]}, {"id": "1812.02633", "submitter": "Pierre-Alexandre Mattei", "authors": "Pierre-Alexandre Mattei and Jes Frellsen", "title": "MIWAE: Deep Generative Modelling and Imputation of Incomplete Data", "comments": "A short version of this paper was presented at the 3rd NeurIPS\n  workshop on Bayesian Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of handling missing data with deep latent variable\nmodels (DLVMs). First, we present a simple technique to train DLVMs when the\ntraining set contains missing-at-random data. Our approach, called MIWAE, is\nbased on the importance-weighted autoencoder (IWAE), and maximises a\npotentially tight lower bound of the log-likelihood of the observed data.\nCompared to the original IWAE, our algorithm does not induce any additional\ncomputational overhead due to the missing data. We also develop Monte Carlo\ntechniques for single and multiple imputation using a DLVM trained on an\nincomplete data set. We illustrate our approach by training a convolutional\nDLVM on a static binarisation of MNIST that contains 50% of missing pixels.\nLeveraging multiple imputation, a convolutional network trained on these\nincomplete digits has a test performance similar to one trained on complete\ndata. On various continuous and binary data sets, we also show that MIWAE\nprovides accurate single imputations, and is highly competitive with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:14:17 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 18:06:43 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Mattei", "Pierre-Alexandre", ""], ["Frellsen", "Jes", ""]]}, {"id": "1812.02794", "submitter": "Boya Zhang", "authors": "Boya Zhang, D. Austin Cole, Robert B. Gramacy", "title": "Distance-distributed design for Gaussian process surrogates", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common challenge in computer experiments and related fields is to\nefficiently explore the input space using a small number of samples, i.e., the\nexperimental design problem. Much of the recent focus in the computer\nexperiment literature, where modeling is often via Gaussian process (GP)\nsurrogates, has been on space-filling designs, via maximin distance, Latin\nhypercube, etc. However, it is easy to demonstrate empirically that such\ndesigns disappoint when the model hyperparameterization is unknown, and must be\nestimated from data observed at the chosen design sites. This is true even when\nthe performance metric is prediction-based, or when the target of interest is\ninherently or eventually sequential in nature, such as in blackbox (Bayesian)\noptimization. Here we expose such inefficiencies, showing that in many cases\npurely random design is superior to higher-powered alternatives. We then\npropose a family of new schemes by reverse engineering the qualities of the\nrandom designs which give the best estimates of GP lengthscales. Specifically,\nwe study the distribution of pairwise distances between design elements, and\ndevelop a numerical scheme to optimize those distances for a given sample size\nand dimension. We illustrate how our distance-based designs, and their hybrids\nwith more conventional space-filling schemes, outperform in both static\n(one-shot design) and sequential settings.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 20:37:08 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 20:38:45 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zhang", "Boya", ""], ["Cole", "D. Austin", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "1812.02829", "submitter": "Katrina Devick", "authors": "Katrina L. Devick, Linda Valeri, Jarvis Chen, Alejandro Jara,\n  Marie-Ab\\`ele Bind, and Brent A. Coull", "title": "The Role of Body Mass Index at Diagnosis on Black-White Disparities in\n  Colorectal Cancer Survival: A Density Regression Mediation Approach", "comments": "15 pages, 2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of racial/ethnic inequalities in health is important to reduce the\nuneven burden of disease. In the case of colorectal cancer (CRC), disparities\nin survival among non-Hispanic Whites and Blacks are well documented, and\nmechanisms leading to these disparities need to be studied formally. It has\nalso been established that body mass index (BMI) is a risk factor for\ndeveloping CRC, and recent literature shows BMI at diagnosis of CRC is\nassociated with survival. Since BMI varies by racial/ethnic group, a question\nthat arises is whether disparities in BMI is partially responsible for observed\nracial/ethnic disparities in CRC survival. This paper presents new methodology\nto quantify the impact of the hypothetical intervention that matches the BMI\ndistribution in the Black population to a potentially complex distributional\nform observed in the White population on racial/ethnic disparities in survival.\nWe perform a simulation that shows our proposed Bayesian density regression\napproach performs as well as or better than current methodology allowing for a\nshift in the mean of the distribution only, and that standard practice of\ncategorizing BMI leads to large biases. When applied to motivating data from\nthe Cancer Care Outcomes Research and Surveillance (CanCORS) Consortium, our\napproach suggests the proposed intervention is potentially beneficial for\nelderly and low income Black patients, yet harmful for young and high income\nBlack populations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:21:27 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Devick", "Katrina L.", ""], ["Valeri", "Linda", ""], ["Chen", "Jarvis", ""], ["Jara", "Alejandro", ""], ["Bind", "Marie-Ab\u00e8le", ""], ["Coull", "Brent A.", ""]]}, {"id": "1812.02884", "submitter": "Jami Mulgrave", "authors": "Jami J. Mulgrave, Subhashis Ghosal", "title": "Bayesian Analysis of Nonparanormal Graphical Models Using\n  Rank-Likelihood", "comments": "arXiv admin note: text overlap with arXiv:1812.04442", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gaussian graphical models, where it is assumed that the variables of interest\njointly follow a multivariate normal distribution with a sparse precision\nmatrix, have been used to study intrinsic dependence among variables, but the\nnormality assumption may be restrictive in many settings. A nonparanormal\ngraphical model is a semiparametric generalization of a Gaussian graphical\nmodel for continuous variables where it is assumed that the variables follow a\nGaussian graphical model only after some unknown smooth monotone\ntransformation. We consider a Bayesian approach for the nonparanormal graphical\nmodel using a rank-likelihood which remains invariant under monotone\ntransformations, thereby avoiding the need to put a prior on the transformation\nfunctions. On the underlying precision matrix of the transformed variables, we\nconsider a horseshoe prior on its Cholesky decomposition and use an efficient\nposterior Gibbs sampling scheme. We present a posterior consistency result for\nthe precision matrix based on the rank-based likelihood. We study the numerical\nperformance of the proposed method through a simulation study and apply it on a\nreal dataset.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 02:48:01 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 03:53:08 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 02:18:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Mulgrave", "Jami J.", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1812.03428", "submitter": "Pauline Y. O'Shaughnessy", "authors": "P.Y. O'Shaughnessy, Francis Hui, Samuel Muller and A.H. Welsh", "title": "Bootstrapping F test for testing Random Effects in Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Hui et al. (2018) use F tests for testing a subset of random effect,\ndemonstrating its computational simplicity and exactness when the first two\nmoment of the random effects are specified. We extended the investigation of\nthe F test in the following two aspects: firstly, we examined the power of the\nF test under non-normality of the errors. Secondly, we consider bootstrap\ncounterparts to the F test, which offer improvement for the cases with small\ncluster size or for the cases with non-normal errors.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 04:41:27 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["O'Shaughnessy", "P. Y.", ""], ["Hui", "Francis", ""], ["Muller", "Samuel", ""], ["Welsh", "A. H.", ""]]}, {"id": "1812.03432", "submitter": "Richard Minkah", "authors": "Richard Minkah and Tertius de Wet", "title": "Constant versus Covariate Dependent Threshold in the Peaks-Over\n  Threshold Method", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Peaks-Over Threshold is a fundamental method in the estimation of rare\nevents such as small exceedance probabilities, extreme quantiles and return\nperiods. The main problem with the Peaks-Over Threshold method relates to the\nselection of threshold above and below which the asymptotic results are valid\nfor large and small observations respectively. In addition, the main assumption\nleading to the asymptotic results is that the observations are independent and\nidentically distributed. However, in practice, many real life processes yield\ndata that are non-stationary and/or related to some covariate variables. As a\nresult, threshold selection gets complicated as it may depend on the\ncovariates. Strong arguments have been made against the use of constant\nthreshold as observation that is considered extreme at some covariate level may\nnot qualify as an extreme observation at another covariate level. Some authors\nhave attempted to obtain covariate dependent thresholds in different ways: the\nmost appealing one relies on quantile regression. In this paper, we propose a\ncovariate dependent threshold based on expectiles. We compare this threshold\nwith the constant and the quantile regression in a simulation study for\nestimating the tail index of the Generalised Pareto distribution. As may be\nexpected, no threshold is universally the best. However, certain general\nobservations can be made for the exponential growth data considered. Firstly,\nwe find that the expectile threshold outperforms the others when the response\nvariable has smaller to medium values. Secondly, for larger values of the\nresponse variable, the constant threshold is generally the best method. The\nthreshold selection methods are illustrated in the estimation of the tail index\nof an insurance claims data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 05:17:51 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Minkah", "Richard", ""], ["de Wet", "Tertius", ""]]}, {"id": "1812.03449", "submitter": "Daniela Marella", "authors": "Pier Luigi Conti and Alberto Di Iorio and Alessio Guandalini and\n  Daniela Marella and Paola Vicard and Vincenzina Vitale", "title": "On the estimation of the Lorenz curve under complex sampling designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the estimation of the concentration curve of a finite\npopulation, when data are collected according to a complex sampling design with\ndifferent inclusion probabilities. A (design-based) Hajek type estimator for\nthe Lorenz curve is proposed, and its asymptotic properties are studied. Then,\na resampling scheme able to approximate the asymptotic law of the Lorenz curve\nestimator is constructed. Applications are given to the construction of (i) a\nconfidence band for the Lorenz curve, (ii) confidence intervals for the Gini\nconcentration ratio, and (iii) a test for Lorenz dominance. The merits of the\nproposed resampling procedure are evaluated through a simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 09:02:32 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Conti", "Pier Luigi", ""], ["Di Iorio", "Alberto", ""], ["Guandalini", "Alessio", ""], ["Marella", "Daniela", ""], ["Vicard", "Paola", ""], ["Vitale", "Vincenzina", ""]]}, {"id": "1812.03510", "submitter": "Natsuki Kariya", "authors": "Natsuki Kariya, and Sumio Watanabe", "title": "Asymptotic Analysis of the Bayesian Likelihood Ratio for Testing\n  Homogeneity in Normal Mixture Models", "comments": "29 pages, 4 figures, Section 5 is added to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we use the normal mixture model, the optimal number of the components\ndescribing the data should be determined. Testing homogeneity is good for this\npurpose; however, to construct its theory is challenging, since the test\nstatistic does not converge to the $\\chi^{2}$ distribution even asymptotically.\nThe reason for such asymptotic behavior is that the parameter set describing\nthe null hypothesis (N.H.) contains singularities in the space of the\nalternative hypothesis (A.H.). Recently, a $\\it{Bayesian}$ theory for singular\nmodels was developed, and it has elucidated various problems of statistical\ninference. However, its application to hypothesis tests for singular models has\nbeen limited. In this paper, we introduce a scaling technique that greatly\nsimplifies the derivation and study testing of homogeneity for the first time\nthe basis of Bayesian theory. We derive the asymptotic distributions of the\nmarginal likelihood ratios in three cases:\n  (1) only the mixture ratio is a variable in the A.H. ;\n  (2) the mixture ratio and the mean of the mixed distribution are variables;\n  And (3) the mixture ratio, the mean, and the variance of the mixed\ndistribution are variables.; In all cases, the results are complex, but can be\ndescribed as functions of random variables obeying normal distributions. A\ntesting scheme based on them was constructed, and their validity was confirmed\nthrough numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 16:06:48 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 15:00:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kariya", "Natsuki", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1812.03555", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley, Christopher K. Wikle, Scott H. Holan", "title": "Spatio-Temporal Models for Big Multinomial Data using the Conditional\n  Multivariate Logit-Beta Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian approach for analyzing high-dimensional multinomial\ndata that are referenced over space and time. In particular, the proportions\nassociated with multinomial data are assumed to have a logit link to a latent\nspatio-temporal mixed effects model. This strategy allows for covariances that\nare nonstationarity in both space and time, asymmetric, and parsimonious. We\nalso introduce the use of the conditional multivariate logit-beta distribution\ninto the dependent multinomial data setting, which leads to conjugate\nfull-conditional distributions for use in a collapsed Gibbs sampler. We refer\nto this model as the multinomial spatio-temporal mixed effects model (MN-STM).\nAdditionally, we provide methodological developments including: the derivation\nof the associated full-conditional distributions, a relationship with a latent\nGaussian process model, and the stability of the non-stationary vector\nautoregressive model. We illustrate the MN-STM through simulations and through\na demonstration with public-use Quarterly Workforce Indicators (QWI) data from\nthe Longitudinal Employer Household Dynamics (LEHD) program of the U.S. Census\nBureau.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 20:30:51 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Wikle", "Christopher K.", ""], ["Holan", "Scott H.", ""]]}, {"id": "1812.03568", "submitter": "Sumanta Basu", "authors": "Sumanta Basu, Xianqi Li and George Michailidis", "title": "Low Rank and Structured Modeling of High-dimensional Vector\n  Autoregressions", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2887401", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network modeling of high-dimensional time series data is a key learning task\ndue to its widespread use in a number of application areas, including\nmacroeconomics, finance and neuroscience. While the problem of sparse modeling\nbased on vector autoregressive models (VAR) has been investigated in depth in\nthe literature, more complex network structures that involve low rank and group\nsparse components have received considerably less attention, despite their\npresence in data. Failure to account for low-rank structures results in\nspurious connectivity among the observed time series, which may lead\npractitioners to draw incorrect conclusions about pertinent scientific or\npolicy questions. In order to accurately estimate a network of Granger causal\ninteractions after accounting for latent effects, we introduce a novel approach\nfor estimating low-rank and structured sparse high-dimensional VAR models. We\nintroduce a regularized framework involving a combination of nuclear norm and\nlasso (or group lasso) penalty. Further, and subsequently establish\nnon-asymptotic upper bounds on the estimation error rates of the low-rank and\nthe structured sparse components. We also introduce a fast estimation algorithm\nand finally demonstrate the performance of the proposed modeling framework over\nstandard sparse VAR estimates through numerical experiments on synthetic and\nreal data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 22:40:42 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Basu", "Sumanta", ""], ["Li", "Xianqi", ""], ["Michailidis", "George", ""]]}, {"id": "1812.03644", "submitter": "Sangwon Hyun", "authors": "Sangwon Hyun, Kevin Lin, Max G'Sell, Ryan J. Tibshirani", "title": "Post-Selection Inference for Changepoint Detection Algorithms with\n  Application to Copy Number Variation Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changepoint detection methods are used in many areas of science and\nengineering, e.g., in the analysis of copy number variation data, to detect\nabnormalities in copy numbers along the genome. Despite the broad array of\navailable tools, methodology for quantifying our uncertainty in the strength\n(or presence) of given changepoints, post-detection, are lacking.\nPost-selection inference offers a framework to fill this gap, but the most\nstraightforward application of these methods results in low-powered tests and\nleaves open several important questions about practical usability. In this\nwork, we carefully tailor post-selection inference methods towards changepoint\ndetection, focusing as our main scientific application on copy number variation\ndata. As for changepoint algorithms, we study binary segmentation, and two of\nits most popular variants, wild and circular, and the fused lasso. We implement\nsome of the latest developments in post-selection inference theory: we use\nauxiliary randomization to improve power, which requires implementations of\nMCMC algorithms (importance sampling and hit-and-run sampling) to carry out our\ntests. We also provide recommendations for improving practical useability,\ndetailed simulations, and an example analysis on array comparative genomic\nhybridization (CGH) data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 06:37:32 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hyun", "Sangwon", ""], ["Lin", "Kevin", ""], ["G'Sell", "Max", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1812.03648", "submitter": "Weichang Yu", "authors": "Weichang Yu, Lamiae Azizi, John T. Ormerod", "title": "Variational Nonparametric Discriminant Analysis", "comments": null, "journal-ref": "Computational Statistics and Data Analysis, 142 (106817) (2020)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection and classification are common objectives in the analysis\nof high-dimensional data. Most such methods make distributional assumptions\nthat may not be compatible with the diverse families of distributions data can\ntake. A novel Bayesian nonparametric discriminant analysis model that performs\nboth variable selection and classification within a seamless framework is\nproposed. P{\\'o}lya tree priors are assigned to the unknown group-conditional\ndistributions to account for their uncertainty, and allow prior beliefs about\nthe distributions to be incorporated simply as hyperparameters. The adoption of\ncollapsed variational Bayes inference in combination with a chain of functional\napproximations led to an algorithm with low computational cost. The resultant\ndecision rules carry heuristic interpretations and are related to an existing\ntwo-sample Bayesian nonparametric hypothesis test. By an application to some\nsimulated and publicly available real datasets, the proposed method exhibits\ngood performance when compared to current state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:01:17 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 05:17:38 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 06:03:39 GMT"}, {"version": "v4", "created": "Mon, 29 Apr 2019 06:35:33 GMT"}, {"version": "v5", "created": "Tue, 27 Aug 2019 04:37:29 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Yu", "Weichang", ""], ["Azizi", "Lamiae", ""], ["Ormerod", "John T.", ""]]}, {"id": "1812.03659", "submitter": "Lili Zheng", "authors": "Lili Zheng and Garvesh Raskutti", "title": "Testing for high-dimensional network parameters in auto-regressive\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional auto-regressive models provide a natural way to model\ninfluence between $M$ actors given multi-variate time series data for $T$ time\nintervals. While there has been considerable work on network estimation, there\nis limited work in the context of inference and hypothesis testing. In\nparticular, prior work on hypothesis testing in time series has been restricted\nto linear Gaussian auto-regressive models. From a practical perspective, it is\nimportant to determine suitable statistical tests for connections between\nactors that go beyond the Gaussian assumption. In the context of\n\\emph{high-dimensional} time series models, confidence intervals present\nadditional estimators since most estimators such as the Lasso and Dantzig\nselectors are biased which has led to \\emph{de-biased} estimators. In this\npaper we address these challenges and provide convergence in distribution\nresults and confidence intervals for the multi-variate AR(p) model with\nsub-Gaussian noise, a generalization of Gaussian noise that broadens\napplicability and presents numerous technical challenges. The main technical\nchallenge lies in the fact that unlike Gaussian random vectors, for\nsub-Gaussian vectors zero correlation does not imply independence. The proof\nrelies on using an intricate truncation argument to develop novel concentration\nbounds for quadratic forms of dependent sub-Gaussian random variables. Our\nconvergence in distribution results hold provided $T = \\Omega((s \\vee \\rho)^2\n\\log^2 M)$, where $s$ and $\\rho$ refer to sparsity parameters which matches\nexisted results for hypothesis testing with i.i.d. samples. We validate our\ntheoretical results with simulation results for both block-structured and\nchain-structured networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:43:35 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 20:19:50 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Zheng", "Lili", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1812.03662", "submitter": "Aviv Navon", "authors": "Aviv Navon, Saharon Rosset", "title": "Capturing Between-Tasks Covariance and Similarities Using Multivariate\n  Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting several response variables using the\nsame set of explanatory variables. This setting naturally induces a group\nstructure over the coefficient matrix, in which every explanatory variable\ncorresponds to a set of related coefficients. Most of the existing methods that\nutilize this group formation assume that the similarities between related\ncoefficients arise solely through a joint sparsity structure. In this paper, we\npropose a procedure for constructing an estimator of a multivariate regression\ncoefficient matrix that directly models and captures the within-group\nsimilarities, by employing a multivariate linear mixed model formulation, with\na joint estimation of covariance matrices for coefficients and errors via\npenalized likelihood. Our approach, which we term Multivariate random\nRegression with Covariance Estimation (MrRCE) encourages structured similarity\nin parameters, in which coefficients for the same variable in related tasks\nsharing the same sign and similar magnitude. We illustrate the benefits of our\napproach in synthetic and real examples, and show that the proposed method\noutperforms natural competitors and alternative estimators under several model\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:52:15 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 12:19:59 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Navon", "Aviv", ""], ["Rosset", "Saharon", ""]]}, {"id": "1812.03775", "submitter": "Jia Zhang", "authors": "Xin Chen, Jingjing Wu, Zhigang Yao, Jia Zhang", "title": "Sufficient Dimension Reduction for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sufficient dimension reduction approach designed\ndeliberately for high-dimensional classification. This novel method is named\nmaximal mean variance (MMV), inspired by the mean variance index first proposed\nby Cui, Li and Zhong (2015), which measures the dependence between a\ncategorical random variable with multiple classes and a continuous random\nvariable. Our method requires reasonably mild restrictions on the predicting\nvariables and keeps the model-free advantage without the need to estimate the\nlink function. The consistency of the MMV estimator is established under\nregularity conditions for both fixed and diverging dimension (p) cases and the\nnumber of the response classes can also be allowed to diverge with the sample\nsize n. We also construct the asymptotic normality for the estimator when the\ndimension of the predicting vector is fixed. Furthermore, our method works\npretty well when n < p. The surprising classification efficiency gain of the\nproposed method is demonstrated by simulation studies and real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:01:02 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Chen", "Xin", ""], ["Wu", "Jingjing", ""], ["Yao", "Zhigang", ""], ["Zhang", "Jia", ""]]}, {"id": "1812.03970", "submitter": "Caterina May", "authors": "Nancy Flournoy, Caterina May, Chiara Tommasi", "title": "The Effects of Adaptation on Inference for Non-Linear Regression Models\n  with Normal Errors", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the properties of the maximum likelihood estimator (MLE) of\na non-linear model with Gaussian errors and multidimensional parameter. The\nobservations are collected in a two-stage experimental design and are dependent\nsince the second stage design is determined by the observations at the first\nstage; the MLE maximizes the total likelihood. Differently from the most of the\nliterature, the first stage sample size is small, and hence asymptotic\napproximation is used only in the second stage. It is proved that the MLE is\nconsistent and that its asymptotic distribution is a specific Gaussian mixture,\nvia stable convergence. Finally, a simulation study is provided in the case of\na dose-response Emax model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 18:39:42 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 10:08:13 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 16:13:00 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Flournoy", "Nancy", ""], ["May", "Caterina", ""], ["Tommasi", "Chiara", ""]]}, {"id": "1812.04063", "submitter": "Shu Li", "authors": "Shu Li, Peter B\\\"uhlmann", "title": "Estimating heterogeneous treatment effects in nonstationary time series\n  with state-space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized trials and observational studies, more often than not, run over a\ncertain period of time. The treatment effect evolves during this period which\nprovides crucial insights into the treatment response and the long-term\neffects. Many conventional methods for estimating treatment effects are limited\nto the i.i.d. setting and are not suited for inferring the time dynamics of the\ntreatment effect. The time series encountered in these settings are highly\ninformative but often nonstationary due to the changing effects of treatment.\nThis increases the difficulty, since stationarity, a common assumption in time\nseries analysis, cannot be reasonably assumed. Another challenge is the\nheterogeneity of the treatment effect when the treatment affects units\ndifferently. The task of estimating heterogeneous treatment effects from\nnonstationary and, in particular, interventional time series is highly relevant\nbut has remained unexplored yet. We propose Causal Transfer, a method which\ncombines regression to adjust for confounding with time series modelling to\nlearn the effect of the treatment and how it evolves over time. Causal Transfer\ndoes not assume the data to be stationary and can be applied to randomized\ntrials and observational studies in which treatment is confounded. Causal\nTransfer adjusts the effect for possible confounders and transfers the learned\neffect to other time series and, thereby, estimates various forms of treatment\neffects, such as the average treatment effect (ATE) or the conditional average\ntreatment effect (CATE). By learning the time dynamics of the effect, Causal\nTransfer can also predict the treatment effect for unobserved future time\npoints and determine the long-term consequences of treatment.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:04:05 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 10:09:06 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 07:29:48 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Li", "Shu", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1812.04187", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn, Veronika Rockova, Enakshi Saha", "title": "Dynamic Sparse Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Its conceptual appeal and effectiveness has made latent factor modeling an\nindispensable tool for multivariate analysis. Despite its popularity across\nmany fields, there are outstanding methodological challenges that have hampered\npractical deployments. One major challenge is the selection of the number of\nfactors, which is exacerbated for dynamic factor models, where factors can\ndisappear, emerge, and/or reoccur over time. Existing tools that assume a fixed\nnumber of factors may provide a misguided representation of the data mechanism,\nespecially when the number of factors is crudely misspecified. Another\nchallenge is the interpretability of the factor structure, which is often\nregarded as an unattainable objective due to the lack of identifiability.\nMotivated by a topical macroeconomic application, we develop a flexible\nBayesian method for dynamic factor analysis (DFA) that can simultaneously\naccommodate a time-varying number of factors and enhance interpretability\nwithout strict identifiability constraints. To this end, we turn to dynamic\nsparsity by employing Dynamic Spike-and-Slab (DSS) priors within DFA. Scalable\nBayesian EM estimation is proposed for fast posterior mode identification via\nrotations to sparsity, enabling Bayesian data analysis at scales that would\nhave been previously time-consuming. We study a large-scale balanced panel of\nmacroeconomic variables covering multiple facets of the US economy, with a\nfocus on the Great Recession, to highlight the efficacy and usefulness of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 02:08:38 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["McAlinn", "Kenichiro", ""], ["Rockova", "Veronika", ""], ["Saha", "Enakshi", ""]]}, {"id": "1812.04195", "submitter": "Kyungchul Song", "authors": "Xiaoqi He, Kyungchul Song", "title": "Measuring Diffusion over a Large Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a measure of diffusion of binary outcomes over a large,\nsparse network. The measure captures the aggregated spillover effect of the\noutcomes in the first period on their neighboring outcomes in the second\nperiod. We associate the network with a set of conditional independence\nrestrictions, and show that when there is an observed proxy network that\nsatisfies these conditional independence restrictions, the measure of diffusion\nis identified as a spatio-temporal dependence measure of observed outcomes.\nWhen the proxy network does not satisfy the restrictions but the spillover\neffect is nonnegative, the spatio-temporal dependence measure serves as a lower\nbound for the diffusion. Using this, we propose a confidence lower bound for\ndiffusion and establish its asymptotic validity. Our Monte Carlo simulation\nstudies demonstrate the finite sample stability of the inference across a range\nof network configurations. We apply the method to Indian village data to\nmeasure the diffusion of microfinancing decisions over social networks of\nhouseholds and find that the diffusion parameter is significantly different\nfrom zero at 1% level.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 02:51:27 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 11:33:07 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 03:31:41 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["He", "Xiaoqi", ""], ["Song", "Kyungchul", ""]]}, {"id": "1812.04324", "submitter": "Soghra Bohlourihajjar", "authors": "Soghra Bohlourihajjar, Soleiman Khazaei", "title": "Bayesian Nonparametric Model for Weighted Data Using Mixture of Burr XII\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet process mixture model (DPMM) is a popular Bayesian nonparametric\nmodel. In this paper, we apply this model to weighted data and then estimate\nthe un-weighted distribution from the corresponding weighted distribution using\nthe metropolis-Hastings algorithm. We then apply the DPMM with different\nkernels to simulated and real data sets. In particular, we work with lifetime\ndata in the presence of censored data and then calculate estimated density and\nsurvival values.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 10:45:53 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Bohlourihajjar", "Soghra", ""], ["Khazaei", "Soleiman", ""]]}, {"id": "1812.04442", "submitter": "Jami Mulgrave", "authors": "Jami J. Mulgrave, Subhashis Ghosal", "title": "Regression-Based Bayesian Estimation and Structure Learning for\n  Nonparanormal Graphical Models", "comments": "arXiv admin note: text overlap with arXiv:1812.02884", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A nonparanormal graphical model is a semiparametric generalization of a\nGaussian graphical model for continuous variables in which it is assumed that\nthe variables follow a Gaussian graphical model only after some unknown smooth\nmonotone transformations. We consider a Bayesian approach to inference in a\nnonparanormal graphical model in which we put priors on the unknown\ntransformations through a random series based on B-splines. We use a regression\nformulation to construct the likelihood through the Cholesky decomposition on\nthe underlying precision matrix of the transformed variables and put shrinkage\npriors on the regression coefficients. We apply a plug-in variational Bayesian\nalgorithm for learning the sparse precision matrix and compare the performance\nto a posterior Gibbs sampling scheme in a simulation study. We finally apply\nthe proposed methods to a real data set. KEYWORDS:\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 05:34:32 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 18:57:11 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mulgrave", "Jami J.", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1812.04680", "submitter": "Rahul Ghosal", "authors": "Rahul Ghosal, Arnab Maity", "title": "A Score Based Test for Functional Linear Concurrent Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for testing the null hypothesis of no effect of a\ncovariate on the response in the context of functional linear concurrent\nregression. We establish an equivalent random effects formulation of our\nfunctional regression model under which our testing problem reduces to testing\nfor zero variance component for random effects. For this purpose, we use a\none-sided score test approach, which is an extension of the classical score\ntest. We provide theoretical justification as to why our testing procedure has\nthe right levels (asymptotically) under null using standard assumptions. Using\nnumerical simulations, we show that our testing method has the desired type I\nerror rate and gives higher power compared to a bootstrapped F test currently\nexisting in the literature. Our model and testing procedure are shown to give\ngood performances even when the data is sparsely observed, and the covariate is\ncontaminated with noise. Applications of the proposed testing method are\ndemonstrated on gait study and a dietary calcium absorption data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 20:35:51 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 19:02:11 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Ghosal", "Rahul", ""], ["Maity", "Arnab", ""]]}, {"id": "1812.04774", "submitter": "Xiongtao Dai", "authors": "Xiongtao Dai, Zhenhua Lin, Hans-Georg M\\\"uller", "title": "Modeling Longitudinal Data on Riemannian Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering functional principal component analysis for sparsely\nobserved longitudinal data that take values on a nonlinear manifold, a major\nchallenge is how to handle the sparse and irregular observations that are\ncommonly encountered in longitudinal studies. Addressing this challenge, we\nprovide theory and implementations for a manifold version of the principal\nanalysis by conditional expectation (PACE) procedure that produces\nrepresentations intrinsic to the manifold, extending a well-established version\nof functional principal component analysis targeting sparsely sampled\nlongitudinal data in linear spaces. Key steps are local linear smoothing\nmethods for the estimation of a Fr\\'echet mean curve, mapping the observed\nmanifold-valued longitudinal data to tangent spaces around the estimated mean\ncurve, and applying smoothing methods to obtain the covariance structure of the\nmapped data. Dimension reduction is achieved via representations based on the\nfirst few leading principal components. A finitely truncated representation of\nthe original manifold-valued data is then obtained by mapping these tangent\nspace representations to the manifold. We show that the proposed estimates of\nmean curve and covariance structure achieve state-of-the-art convergence rates.\nFor longitudinal emotional well-being data for unemployed workers as an example\nof time-dynamic compositional data that are located on a sphere, we demonstrate\nthat our methods lead to interpretable eigenfunctions and principal component\nscores. In a second example, we analyze the body shapes of wallabies by mapping\nthe relative size of their body parts onto a spherical pre-shape space.\nCompared to standard functional principal component analysis, which is based on\nEuclidean geometry, the proposed approach leads to improved trajectory recovery\nfor sparsely sampled data on nonlinear manifolds.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 01:52:51 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Dai", "Xiongtao", ""], ["Lin", "Zhenhua", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1812.04808", "submitter": "Hedi Xia", "authors": "Hedi Xia, Hector D. Ceniceros", "title": "Kernel Treelets", "comments": null, "journal-ref": null, "doi": "10.1142/S2424922X19500062", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for hierarchical clustering is presented. It combines treelets,\na particular multiscale decomposition of data, with a projection on a\nreproducing kernel Hilbert space. The proposed approach, called kernel treelets\n(KT), effectively substitutes the correlation coefficient matrix used in\ntreelets with a symmetric, positive semi-definite matrix efficiently\nconstructed from a kernel function. Unlike most clustering methods, which\nrequire data sets to be numeric, KT can be applied to more general data and\nyield a multi-resolution sequence of basis on the data directly in feature\nspace. The effectiveness and potential of KT in clustering analysis is\nillustrated with some examples.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 04:55:24 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Xia", "Hedi", ""], ["Ceniceros", "Hector D.", ""]]}, {"id": "1812.04928", "submitter": "Lars Holden", "authors": "Lars Holden and Kristoffer Hellton", "title": "Multiple Model-Free Knockoffs", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free knockoffs is a recently proposed technique for identifying\ncovariates that is likely to have an effect on a response variable. The method\nis an efficient method to control the false discovery rate in hypothesis tests\nfor separate covariates. This paper presents a generalisation of the technique\nusing multiple sets of model-free knockoffs. This is formulated as an open\nquestion in Candes et al. [4]. With multiple knockoffs, we are able to reduce\nthe randomness in the knockoffs, making the result stronger. Since we use the\nsame structure for generating all the knockoffs, the computational resources is\nfar smaller than proportional with the number of knockoffs. We prove a bound on\nthe asymptotic false discovery rate when the number of sets increases that is\nbetter then the published bounds for one set.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 13:10:12 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 13:31:42 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 15:02:52 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Holden", "Lars", ""], ["Hellton", "Kristoffer", ""]]}, {"id": "1812.04933", "submitter": "Sumit Kumar", "authors": "Harsh Tripathi, Abhimanyu Singh Yadav, Mahendra Saha, Sumit Kumar", "title": "Generalized inverse xgamma distribution: A non-monotone hazard rate\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a generalized inverse xgamma distribution (GIXGD) has been\nintroduced as the generalized version of the inverse xgamma distribution. The\nproposed model exhibits the pattern of non-monotone hazard rate and belongs to\nfamily of positively skewed models. The explicit expressions of some\ndistributional properties, such as, moments, inverse moments, conditional\nmoments, mean deviation, quantile function have been derived. The maximum\nlikelihood estimation procedure has been used to estimate the unknown model\nparameters as well as survival characteristics of GIXGD. The practical\napplicability of the proposed model has been illustrated through a survival\ndata of guinea pigs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 14:22:43 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Tripathi", "Harsh", ""], ["Yadav", "Abhimanyu Singh", ""], ["Saha", "Mahendra", ""], ["Kumar", "Sumit", ""]]}, {"id": "1812.04990", "submitter": "Youjin Lee", "authors": "Elizabeth L. Ogburn, Ilya Shpitser, Youjin Lee", "title": "Causal inference, social networks, and chain graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, statistical and causal inference on human subjects rely on the\nassumption that individuals are independently affected by treatments or\nexposures. However, recently there has been increasing interest in settings,\nsuch as social networks, where individuals may interact with one another such\nthat treatments may spill over from the treated individual to their social\ncontacts and outcomes may be contagious. Existing models proposed for causal\ninference using observational data from networks of interacting individuals\nhave two major shortcomings. First, they often require a level of granularity\nin the data that is practically infeasible to collect in most settings, and\nsecond, the models are high-dimensional and often too big to fit to the\navailable data. In this paper we illustrate and justify a parsimonious\nparameterization for network data with interference and contagion. Our\nparameterization corresponds to a particular family of graphical models known\nas chain graphs. We argue that, in some settings, chain graph models\napproximate the marginal distribution of a snapshot of a longitudinal data\ngenerating process on interacting units. We illustrate the use of chain graphs\nfor causal inference about collective decision making in social networks using\ndata from U.S. Supreme Court decisions between 1994 and 2004 and in\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 15:36:55 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 00:19:51 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ogburn", "Elizabeth L.", ""], ["Shpitser", "Ilya", ""], ["Lee", "Youjin", ""]]}, {"id": "1812.05068", "submitter": "Tijana Zrnic", "authors": "Tijana Zrnic, Aaditya Ramdas, Michael I. Jordan", "title": "Asynchronous Online Testing of Multiple Hypotheses", "comments": "36 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of asynchronous online testing, aimed at providing\ncontrol of the false discovery rate (FDR) during a continual stream of data\ncollection and testing, where each test may be a sequential test that can start\nand stop at arbitrary times. This setting increasingly characterizes real-world\napplications in science and industry, where teams of researchers across large\norganizations may conduct tests of hypotheses in a decentralized manner. The\noverlap in time and space also tends to induce dependencies among test\nstatistics, a challenge for classical methodology, which either assumes (overly\noptimistically) independence or (overly pessimistically) arbitrary dependence\nbetween test statistics. We present a general framework that addresses both of\nthese issues via a unified computational abstraction that we refer to as\n\"conflict sets.\" We show how this framework yields algorithms with formal FDR\nguarantees under a more intermediate, local notion of dependence. We illustrate\nour algorithms in simulations by comparing to existing algorithms for online\nFDR control.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 18:12:55 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 19:15:28 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zrnic", "Tijana", ""], ["Ramdas", "Aaditya", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1812.05170", "submitter": "Nathan Sandholtz", "authors": "Nathan Sandholtz and Luke Bornn", "title": "Markov Decision Processes with Dynamic Transition Probabilities: An\n  Analysis of Shooting Strategies in Basketball", "comments": "32 pages, 9 Figures, code available at\n  https://github.com/nsandholtz/nba_replay", "journal-ref": "Ann. Appl. Stat. 14(3), (2020) 1122-1145", "doi": "10.1214/20-AOAS1348", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we model basketball plays as episodes from team-specific\nnon-stationary Markov decision processes (MDPs) with shot clock dependent\ntransition probabilities. Bayesian hierarchical models are employed in the\nmodeling and parametrization of the transition probabilities to borrow strength\nacross players and through time. To enable computational feasibility, we\ncombine lineup-specific MDPs into team-average MDPs using a novel transition\nweighting scheme. Specifically, we derive the dynamics of the team-average\nprocess such that the expected transition count for an arbitrary state-pair is\nequal to the weighted sum of the expected counts of the separate\nlineup-specific MDPs.\n  We then utilize these non-stationary MDPs in the creation of a basketball\nplay simulator with uncertainty propagated via posterior samples of the model\ncomponents. After calibration, we simulate seasons both on-policy and under\naltered policies and explore the net changes in efficiency and production under\nthe alternate policies. Additionally, we discuss the game-theoretic\nramifications of testing alternative decision policies.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 21:48:53 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 00:52:09 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Sandholtz", "Nathan", ""], ["Bornn", "Luke", ""]]}, {"id": "1812.05188", "submitter": "Chi Song", "authors": "XIaoyu Cai and Lo-Bin Chang and Chi Song", "title": "Association Analysis of Common and Rare SNVs using Adaptive Fisher\n  Method to Detect Dense and Sparse Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of next generation sequencing (NGS) technology and genotype\nimputation methods enabled researchers to measure both common and rare variants\nin genome-wide association studies (GWAS). Statistical methods have been\nproposed to test a set of genomic variants together to detect if any of them is\nassociated with the phenotype or disease. In practice, within the set of\nvariants, there is an unknown proportion of variants truly causal or associated\nwith the disease. Because most developed methods are sensitive to either the\ndense scenario, where a large proportion of the variants are associated, or the\nsparse scenario, where only a small proportion of the variants are associated,\nthere is a demand of statistical methods with high power in both scenarios. In\nthis paper, we propose a new association test (weighted Adaptive Fisher, wAF)\nthat can adapt to both the dense and sparse scenario by adding weights to the\nAdaptive Fisher (AF) method we developed before. Using both simulation and the\nGenetic Analysis Workshop 16 (GAW16) data, we have shown that the new method\nenjoys comparable or better power to popular methods such as sequence kernel\nassociation test (SKAT and SKAT-O) and adaptive SPU (aSPU) test.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 23:08:57 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Cai", "XIaoyu", ""], ["Chang", "Lo-Bin", ""], ["Song", "Chi", ""]]}, {"id": "1812.05202", "submitter": "Lin Wang", "authors": "Lin Wang, Hongquan Xu", "title": "A Class of Multilevel Nonregular Designs for Studying Quantitative\n  Factors", "comments": null, "journal-ref": "Statistica Sinica 2021", "doi": "10.5705/ss.202020.0223", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional factorial designs are widely used for designing screening\nexperiments. Nonregular fractional factorial designs can have better properties\nthan regular designs, but their construction is challenging. Current research\non the construction of nonregular designs focuses on two-level designs. We\nprovide a novel class of multilevel nonregular designs by permuting levels of\nregular designs. We develop a theory illustrating how levels can be permuted\nwithout computer search and accordingly propose a sequential method for\nconstructing nonregular designs. Compared to regular designs, these nonregular\ndesigns can provide more accurate estimations on factorial effects and more\nefficient screening for experiments with quantitative factors. We further\nexplore the space-filling property of the obtained designs and demonstrate\ntheir superiority.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 00:05:42 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 16:04:24 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Lin", ""], ["Xu", "Hongquan", ""]]}, {"id": "1812.05245", "submitter": "Ravi Goyal", "authors": "Ravi Goyal and Victor De Gruttola", "title": "Dynamic Network Prediction", "comments": null, "journal-ref": "Net Sci 8 (2020) 574-595", "doi": "10.1017/nws.2020.24", "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a statistical framework for generating predicted dynamic networks\nbased on the observed evolution of social relationships in a population. The\nframework includes a novel and flexible procedure to sample dynamic networks\ngiven a probability distribution on evolving network properties; it permits the\nuse of a broad class of approaches to model trends, seasonal variability,\nuncertainty, and changes in population composition. Current methods do not\naccount for the variability in the observed historical networks when predicting\nthe network structure; the proposed method provides a principled approach to\nincorporate uncertainty in prediction. This advance aids in the designing of\nnetwork-based interventions, as development of such interventions often\nrequires prediction of the network structure in the presence and absence of the\nintervention. Two simulation studies are conducted to demonstrate the\nusefulness of generating predicted networks when designing network-based\ninterventions. The framework is also illustrated by investigating results of\npotential interventions on bill passage rates using a dynamic network that\nrepresents the sponsor/co-sponsor relationships among senators derived from\nbills introduced in the US Senate from 2003-2016.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 03:05:17 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 18:11:52 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Goyal", "Ravi", ""], ["De Gruttola", "Victor", ""]]}, {"id": "1812.05334", "submitter": "Kevin Burke Dr", "authors": "Kevin Burke, Valentin Patilea", "title": "A likelihood-based approach for cure regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new likelihood-based approach for estimation, inference and\nvariable selection for parametric cure regression models in time-to-event\nanalysis under random right-censoring. In this context, it often happens that\nsome subjects are \"cured\", i.e., they will never experience the event of\ninterest. Then, the sample of censored observations is an unlabeled mixture of\ncured and \"susceptible\" subjects. Using inverse probability censoring weighting\n(IPCW), we propose a likelihood-based estimation procedure for the cure\nregression model without making assumptions about the distribution of survival\ntimes for the susceptible subjects. The IPCW approach does require a\npreliminary estimate of the censoring distribution, for which general\nparametric, semi- or non-parametric approaches can be used. The incorporation\nof a penalty term in our estimation procedure is straightforward; in\nparticular, we propose L1-type penalties for variable selection. Our\ntheoretical results are derived under mild assumptions. Simulation experiments\nand real data analysis illustrate the effectiveness of the new approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 09:38:47 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 15:23:12 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 09:10:26 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 20:50:00 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Burke", "Kevin", ""], ["Patilea", "Valentin", ""]]}, {"id": "1812.05507", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad and Jelle J. Goeman and Erik W. van Zwet", "title": "Simultaneous Confidence Intervals for Ranks With Application to Ranking\n  Institutions", "comments": "Working paper 20 pages to be submitted soon. This paper differs from\n  our paper \"An improvement of Tukey's HSD with application to ranking\n  institutions\". The sequential Tukey is not there, and a new practical method\n  is proposed. arXiv admin note: substantial text overlap with arXiv:1708.02428", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a ranking of institutions such as medical centers or universities is\nbased on an indicator provided with a standard error, confidence intervals\nshould be calculated to assess the quality of these ranks. We consider the\nproblem of constructing simultaneous confidence intervals for the ranks of\nmeans based on an observed sample. For this aim, the only available method from\nthe literature uses Monte-Carlo simulations and is highly anticonservative\nespecially when the means are close to each other or have ties. We present a\nnovel method based on Tukey's honest significant difference test (HSD). Our new\nmethod is on the contrary conservative when there are no ties. By properly\nrescaling these two methods to the nominal confidence level, they surprisingly\nperform very similarly. The Monte-Carlo method is however unscalable when the\nnumber of institutions is large than 30 to 50 and stays thus anticonservative.\nWe provide extensive simulations to support our claims and the two methods are\ncompared in terms of their simultaneous coverage and their efficiency. We\nprovide a data analysis for 64 hospitals in the Netherlands and compare both\nmethods. Software for our new methods is available online in package ICRanks\ndownloadable from CRAN. Supplementary materials include supplementary R code\nfor the simulations and proofs of the propositions presented in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 21:13:03 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Mohamad", "Diaa Al", ""], ["Goeman", "Jelle J.", ""], ["van Zwet", "Erik W.", ""]]}, {"id": "1812.05531", "submitter": "Fabrizio Leisen", "authors": "Laurentiu Catalin Hinoveanu, Fabrizio Leisen, Cristiano Villa", "title": "A Loss-Based Prior for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models play an important role in various areas such as\ngenetics, finance, statistical physics and others. They are a powerful\nmodelling tool which allows one to describe the relationships among the\nvariables of interest. From the Bayesian perspective, there are two sources of\nrandomness: one is related to the multivariate distribution and the quantities\nthat may parametrise the model, the other has to do with the underlying graph,\n$G$, equivalent to describing the conditional independence structure of the\nmodel under consideration. In this paper, we propose a prior on G based on two\nloss components. One considers the loss in information one would incur in\nselecting the wrong graph, while the second penalises for large number of\nedges, favouring sparsity. We illustrate the prior on simulated data and on\nreal datasets, and compare the results with other priors on $G$ used in the\nliterature. Moreover, we present a default choice of the prior as well as\ndiscuss how it can be calibrated so as to reflect available prior information.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 17:26:19 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 18:57:32 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hinoveanu", "Laurentiu Catalin", ""], ["Leisen", "Fabrizio", ""], ["Villa", "Cristiano", ""]]}, {"id": "1812.05553", "submitter": "Holger Dette", "authors": "Holger Dette, Maria Konstantinou, Kirsten Schorning", "title": "Optimal designs for series estimation in nonparametric regression with\n  correlated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of designing experiments for series\nestimators in nonparametric regression models with correlated observations. We\nuse projection based estimators to derive an explicit solution of the best\nlinear oracle estimator in the continuous time model for all Markovian-type\nerror processes. These solutions are then used to construct estimators, which\ncan be calculated from the available data along with their corresponding\noptimal design points. Our results are illustrated by means of a simulation\nstudy, which demonstrates that the new series estimator has a better\nperformance than the commonly used techniques based on the optimal linear\nunbiased estimators. Moreover, we show that the performance of the estimators\nproposed in this paper can be further improved by choosing the design points\nappropriately.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 18:08:50 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Dette", "Holger", ""], ["Konstantinou", "Maria", ""], ["Schorning", "Kirsten", ""]]}, {"id": "1812.05678", "submitter": "Anthony Christidis", "authors": "Anthony Christidis, Stefan Van Aelst and Ruben Zamar", "title": "Split regression modeling", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we study the benefits of splitting variables variables for\nreducing the variance of linear functions of the regression coefficient\nestimate. We show that splitting combined with shrinkage can result in\nestimators with smaller mean squared error compared to popular shrinkage\nestimators such as Lasso, ridge regression and garrote.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 20:36:38 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 09:17:07 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Christidis", "Anthony", ""], ["Van Aelst", "Stefan", ""], ["Zamar", "Ruben", ""]]}, {"id": "1812.05697", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke, Koushiki Bose, Jianqing Fan", "title": "Higher Moment Estimation for Elliptically-distributed Data: Is it\n  Necessary to Use a Sledgehammer to Crack an Egg?", "comments": "47 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate elliptically-contoured distributions are widely used for\nmodeling economic and financial data. We study the problem of estimating moment\nparameters of a semi-parametric elliptical model in a high-dimensional setting.\nSuch estimators are useful for financial data analysis and quadratic\ndiscriminant analysis. For low-dimensional elliptical models, efficient moment\nestimators can be obtained by plugging in an estimate of the precision matrix.\nNatural generalizations of the plug-in estimator to high-dimensional settings\nperform unsatisfactorily, due to estimating a large precision matrix. Do we\nreally need a sledgehammer to crack an egg? Fortunately, we discover that\nmoment parameters can be efficiently estimated without estimating the precision\nmatrix in high-dimension. We propose a marginal aggregation estimator (MAE) for\nmoment parameters. The MAE only requires estimating the diagonal of covariance\nmatrix and is convenient to implement. With mild sparsity on the covariance\nstructure, we prove that the asymptotic variance of MAE is the same as the\nideal plug-in estimator which knows the true precision matrix, so MAE is\nasymptotically efficient. We also extend MAE to a block-wise aggregation\nestimator (BAE) when estimates of diagonal blocks of covariance matrix are\navailable. The performance of our methods is validated by extensive simulations\nand an application to financial returns.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 21:31:10 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Bose", "Koushiki", ""], ["Fan", "Jianqing", ""]]}, {"id": "1812.05723", "submitter": "Malgorzata Bogdan", "authors": "Patrick J. C. Tardivel and Malgorzata Bogdan", "title": "On the sign recovery by LASSO, thresholded LASSO and thresholded Basis\n  Pursuit Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the regression model, when the number of observations is smaller\nthan the number of explicative variables. It is well known that the popular\nLeast Absolute Shrinkage and Selection Operator (LASSO) can recover the sign of\nregression coefficients only if a very stringent irrepresentable condition is\nsatisfied. We extend this result by providing a tight upper bound for the\nprobability of LASSO sign recovery. The bound depends on the tuning parameter\nand is attained when non-null components of the vector of regression\ncoefficients tend to infinity. In this situation it can be used to select the\nvalue of the tuning parameter so as to control the probability of at least one\nfalse discovery. Next, we revisit properties of thresholded LASSO and\nthresholded Basis Pursuit Denoising (BPDN) and provide new theoretical results\nin the asymptotic setup under which the design matrix is fixed and the\nmagnitudes of nonzero regression coefficients tend to infinity. We formulate an\neasy identifiability condition which turns out to be sufficient and necessary\nfor thresholded LASSO and thresholded BPDN to recover the sign of the\nsufficiently large signal. Our simulation study illustrates the large\ndifference between the irrepresentability and the identifiability condition,\nespecially when the entries in each row of the design matrix are strongly\ncorrelated. Finally, we illustrate how the knockoff methodology allows to\nselect an appropriate threshold and that thresholded BPDN and thresholded LASSO\ncan recover the sign of the vector of regression coefficients with a larger\nprobability than adaptive LASSO.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 22:59:24 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 21:10:44 GMT"}, {"version": "v3", "created": "Sat, 22 Jun 2019 14:33:07 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Tardivel", "Patrick J. C.", ""], ["Bogdan", "Malgorzata", ""]]}, {"id": "1812.05741", "submitter": "Deborshee Sen", "authors": "Deborshee Sen, Sayan Patra, and David Dunson", "title": "Constrained inference through posterior projections", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian approaches are appealing for constrained inference problems in\nallowing a probabilistic characterization of uncertainty, while providing a\ncomputational machinery for incorporating complex constraints in hierarchical\nmodels. However, the usual Bayesian strategy of placing a prior on the\nconstrained space and conducting posterior computation with Markov chain Monte\nCarlo algorithms is often intractable. An alternative is to conduct inference\nfor a less constrained posterior and project samples to the constrained space\nthrough a minimal distance mapping. We formalize and provide a unifying\nframework for such posterior projections. For theoretical tractability, we\ninitially focus on constrained parameter spaces corresponding to closed and\nconvex subsets of the original space. We then consider non-convex Stiefel\nmanifolds. We provide a general formulation of the projected posterior and show\nthat it can be viewed as an update of a data-dependent prior with the\nlikelihood for particular classes of priors and likelihood functions. We also\nshow that asymptotic properties of the unconstrained posterior are transferred\nto the projected posterior. Posterior projections are illustrated through\nmultiple examples, both in simulation studies and real data applications.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 00:07:26 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 19:59:33 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 14:51:21 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Sen", "Deborshee", ""], ["Patra", "Sayan", ""], ["Dunson", "David", ""]]}, {"id": "1812.05893", "submitter": "Erwan Koch", "authors": "Erwan Koch and Christian Y. Robert", "title": "Stochastic derivative estimation for max-stable random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider expected performances based on max-stable random fields and we\nare interested in their derivatives with respect to the spatial dependence\nparameters of those fields. Max-stable fields, such as the Brown--Resnick and\nSmith fields, are very popular in spatial extremes. We focus on the two most\npopular unbiased stochastic derivative estimation approaches: the likelihood\nratio method (LRM) and the infinitesimal perturbation analysis (IPA). LRM\nrequires the multivariate density of the max-stable field to be explicit, and\nIPA necessitates the computation of the derivative with respect to the\nparameters for each simulated value. We propose convenient and tractable\nconditions ensuring the validity of LRM and IPA in the cases of the\nBrown--Resnick and Smith field, respectively. Obtaining such conditions is\nintricate owing to the very structure of max-stable fields. Then we focus on\nrisk and dependence measures, which constitute one of the several frameworks\nwhere our theoretical results can be useful. We perform a simulation study\nwhich shows that both LRM and IPA perform well in various configurations, and\nprovide a real case study that is valuable for the insurance industry.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 12:49:29 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 05:58:21 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 14:13:34 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Koch", "Erwan", ""], ["Robert", "Christian Y.", ""]]}, {"id": "1812.05915", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A multinomial quadrivariate D-vine copula mixed model for meta-analysis\n  of diagnostic studies in the presence of non-evaluable subjects", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.03685 and\n  text overlap with arXiv:1506.03920, arXiv:1805.09674, arXiv:1502.07505", "journal-ref": "Statistical Methods in Medical Research, 2020, 29 (10), 2988-3005", "doi": "10.1177/0962280220913898", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic test accuracy studies observe the result of a gold standard\nprocedure that defines the presence or absence of a disease and the result of a\ndiagnostic test. They typically report the number of true positives, false\npositives, true negatives and false negatives. However, diagnostic test\noutcomes can also be either non-evaluable positives or non-evaluable negatives.\nWe propose a novel model for meta-analysis of diagnostic studies in the\npresence of non-evaluable outcomes that assumes independent multinomial\ndistributions for the true and non-evaluable positives, and, the true and non\nevaluable negatives, conditional on the latent sensitivity, specificity,\nprobability of non-evaluable positives and probability of non-evaluable\nnegatives in each study. For the random effects distribution of the latent\nproportions, we employ a drawable vine copula that can successively model the\ndependence in the joint tails. Our methodology is demonstrated with an\nextensive simulation study and applied to data from diagnostic accuracy studies\nof coronary computed tomography angiography for the detection of coronary\nartery disease. The comparison of our method with the existing approaches\nyields findings in the real data application that change the current\nconclusions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 10:28:17 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 08:04:22 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1812.05928", "submitter": "Siva Rajesh Kasa", "authors": "Siva Rajesh Kasa, Vaibhav Rajan", "title": "Automatic Differentiation in Mixture Models", "comments": "19 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1301.1505, arXiv:1502.05767, arXiv:1503.06302 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we discuss two specific classes of models - Gaussian Mixture\nCopula models and Mixture of Factor Analyzers - and the advantages of doing\ninference with gradient descent using automatic differentiation. Gaussian\nmixture models are a popular class of clustering methods, that offers a\nprincipled statistical approach to clustering. However, the underlying\nassumption, that every mixing component is normally distributed, can often be\ntoo rigid for several real life datasets. In order to to relax the assumption\nabout the normality of mixing components, a new class of parametric mixture\nmodels that are based on Copula functions - Gaussian Mixuture Copula Models\nwere introduced. Estimating the parameters of the proposed Gaussian Mixture\nCopula Model (GMCM) through maximum likelihood has been intractable due to the\npositive semi-positive-definite constraints on the variance-covariance\nmatrices. Previous attempts were limited to maximizing a proxy-likelihood which\ncan be maximized using EM algorithm. These existing methods, even though easier\nto implement, does not guarantee any convergence nor monotonic increase of the\nGMCM Likelihood. In this paper, we use automatic differentiation tools to\nmaximize the exact likelihood of GMCM, at the same time avoiding any constraint\nequations or Lagrange multipliers. We show how our method leads a monotonic\nincrease in likelihood and converges to a (local) optimum value of likelihood.\n  In this paper, we also show how Automatic Differentiation can be used for\ninference with Mixture of Factor Analyzers and advantages of doing so. We also\ndiscuss how this method also has all the properties such as monotonic increase\nin likelihood and convergence to a local optimum.\n  Note that our work is also applicable to special cases of these two models -\nfor e.g. Simple Copula models, Factor Analyzer model, etc.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 16:33:35 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kasa", "Siva Rajesh", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "1812.06037", "submitter": "Keisuke Yano", "authors": "Keisuke Yano, Ryoya Kaneko, Fumiyasu Komaki", "title": "Minimax Predictive Density for Sparse Count Data", "comments": "49 pages; the supplement is included in pp. 32-49 Accepted for\n  publication in Bernoulli journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses predictive densities under the Kullback--Leibler loss\nfor high-dimensional Poisson sequence models under sparsity constraints.\nSparsity in count data implies zero-inflation. We present a class of Bayes\npredictive densities that attain asymptotic minimaxity in sparse Poisson\nsequence models. We also show that our class with an estimator of unknown\nsparsity level plugged-in is adaptive in the asymptotically minimax sense. For\napplication, we extend our results to settings with quasi-sparsity and with\nmissing-completely-at-random observations. The simulation studies as well as\napplication to real data illustrate the efficiency of the proposed Bayes\npredictive densities.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 17:21:12 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 02:12:39 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 09:40:38 GMT"}, {"version": "v4", "created": "Sat, 5 Sep 2020 14:04:24 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Yano", "Keisuke", ""], ["Kaneko", "Ryoya", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1812.06115", "submitter": "Jiraphan Suntornchost", "authors": "Partha Lahiri and Jiraphan Suntornchost", "title": "A General Bayesian Approach to Meet Different Inferential Goals in\n  Poverty Research for Small Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poverty mapping that displays spatial distribution of various poverty indices\nis most useful to policymakers and researchers when they are disaggregated into\nsmall geographic units, such as cities, municipalities or other administrative\npartitions of a country. Typically, national household surveys that contain\nwelfare variables such as income and expenditures provide limited or no data\nfor small areas. It is well-known that while direct survey-weighted estimates\nare quite reliable for national or large geographical areas they are unreliable\nfor small geographic areas. If the objective is to find areas with extreme\npoverty, these direct estimates will often select small areas due to the high\nvariabilities in the estimates. Empirical best prediction and Bayesian methods\nhave been proposed to improve on the direct point estimates. However, these\nestimates are not appropriate for different inferential purposes. For example,\nfor identifying areas with extreme poverty, these estimates would often select\nareas with large sample sizes. In this paper, using databases used by the\nChilean Ministry for their Small Area Estimation production, we illustrate how\nappropriate Bayesian methodology can be developed to address different\ninferential problems.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:31:38 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Lahiri", "Partha", ""], ["Suntornchost", "Jiraphan", ""]]}, {"id": "1812.06167", "submitter": "Ben Boukai", "authors": "Ben Boukai and Yue Zhang", "title": "Recycled Least Squares Estimation in Nonlinear Regression", "comments": "19 pages with 4 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a resampling scheme for parameters estimates in nonlinear\nregression models. We provide an estimation procedure which recycles, via\nrandom weighting, the relevant parameters estimates to construct consistent\nestimates of the sampling distribution of the various estimates. We establish\nthe asymptotic normality of the resampled estimates and demonstrate the\napplicability of the recycling approach in a small simulation study and via\nexample.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 21:12:17 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Boukai", "Ben", ""], ["Zhang", "Yue", ""]]}, {"id": "1812.06212", "submitter": "Jiacheng Wu", "authors": "Jiacheng Wu and Jian-Xun Wang and Shawn C. Shadden", "title": "Adding Constraints to Bayesian Inverse Problems", "comments": "Accepted by 2019 AAAI conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using observation data to estimate unknown parameters in computational models\nis broadly important. This task is often challenging because solutions are\nnon-unique due to the complexity of the model and limited observation data.\nHowever, the parameters or states of the model are often known to satisfy\nadditional constraints beyond the model. Thus, we propose an approach to\nimprove parameter estimation in such inverse problems by incorporating\nconstraints in a Bayesian inference framework. Constraints are imposed by\nconstructing a likelihood function based on fitness of the solution to the\nconstraints. The posterior distribution of the parameters conditioned on (1)\nthe observed data and (2) satisfaction of the constraints is obtained, and the\nestimate of the parameters is given by the maximum a posteriori estimation or\nposterior mean. Both equality and inequality constraints can be considered by\nthis framework, and the strictness of the constraints can be controlled by\nconstraint uncertainty denoting a confidence on its correctness. Furthermore,\nwe extend this framework to an approximate Bayesian inference framework in\nterms of the ensemble Kalman filter method, where the constraint is imposed by\nre-weighing the ensemble members based on the likelihood function. A synthetic\nmodel is presented to demonstrate the effectiveness of the proposed method and\nin both the exact Bayesian inference and ensemble Kalman filter scenarios,\nnumerical simulations show that imposing constraints using the method presented\nimproves identification of the true parameter solution among multiple local\nminima.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 00:57:11 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Wu", "Jiacheng", ""], ["Wang", "Jian-Xun", ""], ["Shadden", "Shawn C.", ""]]}, {"id": "1812.06309", "submitter": "Bruno Sudret", "authors": "C. Lataniotis, S. Marelli and B. Sudret", "title": "Extending classical surrogate modelling to high-dimensions through\n  supervised dimensionality reduction: a data-driven approach", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": "RSUQ-2018-008B", "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to their versatility, ease of deployment and high-performance,\nsurrogate models have become staple tools in the arsenal of uncertainty\nquantification (UQ). From local interpolants to global spectral decompositions,\nsurrogates are characterised by their ability to efficiently emulate complex\ncomputational models based on a small set of model runs used for training. An\ninherent limitation of many surrogate models is their susceptibility to the\ncurse of dimensionality, which traditionally limits their applicability to a\nmaximum of $\\mathcal{O}(10^2)$ input dimensions. We present a novel approach at\nhigh-dimensional surrogate modelling that is model-, dimensionality reduction-\nand surrogate model- agnostic (black box), and can enable the solution of high\ndimensional (i.e. up to $\\mathcal{O}(10^4)$) problems. After introducing the\ngeneral algorithm, we demonstrate its performance by combining Kriging and\npolynomial chaos expansions surrogates and kernel principal component analysis.\nIn particular, we compare the generalisation performance that the resulting\nsurrogates achieve to the classical sequential application of dimensionality\nreduction followed by surrogate modelling on several benchmark applications,\ncomprising an analytical function and two engineering applications of\nincreasing dimensionality and complexity.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 15:39:41 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 09:54:05 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 11:05:08 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Lataniotis", "C.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "1812.06406", "submitter": "Yubai Yuan", "authors": "Yubai Yuan and Annie Qu", "title": "Community Detection with Dependent Connectivity", "comments": "44 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network analysis, within-community members are more likely to be connected\nthan between-community members, which is reflected in that the edges within a\ncommunity are intercorrelated. However, existing probabilistic models for\ncommunity detection such as the stochastic block model (SBM) are not designed\nto capture the dependence among edges. In this paper, we propose a new\ncommunity detection approach to incorporate within-community dependence of\nconnectivities through the Bahadur representation. The proposed method does not\nrequire specifying the likelihood function, which could be intractable for\ncorrelated binary connectivities. In addition, the proposed method allows for\nheterogeneity among edges between different communities. In theory, we show\nthat incorporating correlation information can lower estimation bias and\naccelerate algorithm convergence. Our simulation studies show that the proposed\nalgorithm outperforms the popular variational EM algorithm assuming conditional\nindependence among edges. We also demonstrate the application of the proposed\nmethod to agricultural product trading networks from different countries.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 07:04:02 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 19:21:20 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Yuan", "Yubai", ""], ["Qu", "Annie", ""]]}, {"id": "1812.06551", "submitter": "Shinjini Nandi", "authors": "Shinjini Nandi and Sanat K. Sarkar", "title": "Adapting BH to One- and Two-Way Classified Structures of Hypotheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple testing literature contains ample research on controlling false\ndiscoveries for hypotheses classified according to one criterion, which we\nrefer to as one-way classified hypotheses. Although simultaneous classification\nof hypotheses according to two different criteria, resulting in two-way\nclassified hypotheses, do often occur in scientific studies, no such research\nhas taken place yet, as far as we know, under this structure. This article\nproduces procedures, both in their oracle and data-adaptive forms, for\ncontrolling the overall false discovery rate (FDR) across all hypotheses\neffectively capturing the underlying one- or two-way classification structure.\nThey have been obtained by using results associated with weighted\nBenjamini-Hochberg (BH) procedure in their more general forms providing\nguidance on how to adapt the original BH procedure to the underlying one- or\ntwo-way classification structure through an appropriate choice of the weights.\nThe FDR is maintained non-asymptotically by our proposed procedures in their\noracle forms under positive regression dependence on subset of null $p$-values\n(PRDS) and in their data-adaptive forms under independence of the $p$-values.\nPossible control of FDR for our data-adaptive procedures in certain scenarios\ninvolving dependent $p$-values have been investigated through simulations. The\nfact that our suggested procedures can be superior to contemporary practices\nhas been demonstrated through their applications in simulated scenarios and to\nreal-life data sets. While the procedures proposed here for two-way classified\nhypotheses are new, the data-adaptive procedure obtained for one-way classified\nhypotheses is alternative to and often more powerful than those proposed in Hu\net al. (2010).\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 22:44:47 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 03:33:04 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Nandi", "Shinjini", ""], ["Sarkar", "Sanat K.", ""]]}, {"id": "1812.06575", "submitter": "Xiao Wu", "authors": "Xiao Wu, Fabrizia Mealli, Marianthi-Anna Kioumourtzoglou, Francesca\n  Dominici, Danielle Braun", "title": "Matching on Generalized Propensity Scores with Continuous Exposures", "comments": "We create an R package, GPSmacthing, available at\n  https://github.com/wxwx1993/GPSmatching, to implement the proposed matching\n  approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness of statistical methods and interpretability of statistical\nanalysis from observational studies are central concerns in many applied\nfields, and robust causal inference methods have the potential to mitigate\nthese concerns. When estimating the causal effects of continuous exposure in\nobservational studies, generalized propensity scores (GPS) have been used to\nadjust for confounding bias. Existing GPS methods, either relying on weighting\nor regression, have certain limitations: a) they require a correctly specified\noutcome model; b) they are sensitive to extreme values of the estimated GPS; c)\nassessing covariate balance when using these approaches is not straightforward.\nMatching, a class of popular causal inference methods with binary treatments,\nhas not been extended to the continuous exposure setting, disregarding its many\nattractive features on method robustness and interpretability. In this paper,\nwe propose an innovative approach for GPS caliper matching in settings with\ncontinuous exposures. We first introduce an assumption of identifiability,\ncalled local weak unconfoundedness, that is less stringent than what is\ncurrently proposed in the literature. Under this assumption and mild smoothness\nconditions, we provide theoretical guarantees that our proposed matching\nestimators attain consistency and asymptotic normality. In simulations, our\nproposed matching estimator outperforms existing methods under settings of\nmodel misspecification and/or in the presence of extreme values of the\nestimated GPS in terms of bias reduction, root mean squared error, and overall\nachieves excellent covariate balance. We utilize the largest-to-date Medicare\nclaims data for the entire US from 2000 to 2016 to construct a continuous\ncausal exposure-response curve for long-term exposure to fine particles\n($PM_{2.5}$) on mortality.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 01:45:20 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 17:21:40 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 21:03:45 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wu", "Xiao", ""], ["Mealli", "Fabrizia", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Dominici", "Francesca", ""], ["Braun", "Danielle", ""]]}, {"id": "1812.06605", "submitter": "Weichang Yu", "authors": "Weichang Yu, John T. Ormerod, Michael Stewart", "title": "Variational Discriminant Analysis with Variable Selection", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast Bayesian method that seamlessly fuses classification and hypothesis\ntesting via discriminant analysis is developed. Building upon the original\ndiscriminant analysis classifier, modelling components are added to identify\ndiscriminative variables. A combination of cake priors and a novel form of\nvariational Bayes we call reverse collapsed variational Bayes gives rise to\nvariable selection that can be directly posed as a multiple hypothesis testing\napproach using likelihood ratio statistics. Some theoretical arguments are\npresented showing that Chernoff-consistency (asymptotically zero type I and\ntype II error) is maintained across all hypotheses. We apply our method on some\npublicly available genomics datasets and show that our method performs well in\npractice for its computational cost. An R package VaDA has also been made\navailable on Github.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 03:57:44 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 05:19:47 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 05:59:44 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2019 04:49:10 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Yu", "Weichang", ""], ["Ormerod", "John T.", ""], ["Stewart", "Michael", ""]]}, {"id": "1812.06688", "submitter": "Johannes Bracher", "authors": "Johannes Bracher", "title": "Comment on \"Under-reported data analysis with INAR-hidden Markov chains\"", "comments": "This is the pre-reviewing version of a letter published in Statistics\n  in Medicine 38(5), 893-898:\n  https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8032 Fernandez-Fontelo\n  et al published a reply which raises some interesting further points:\n  https://onlinelibrary.wiley.com/doi/10.1002/sim.8033 After a 12 month embargo\n  period the accepted version will be made available on arxiv", "journal-ref": "Statistics in Medicine 38(5):893-898 (2019)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Fernandez-Fontelo et al (Statis. Med. 2016, DOI 10.1002/sim.7026) hidden\ninteger-valued autoregressive (INAR) processes are used to estimate reporting\nprobabilities for various diseases. In this comment it is demonstrated that the\nPoisson INAR(1) model with time-homogeneous underreporting can be expressed\nequivalently as a completely observed INAR(inf) model with a geometric lag\nstructure. This implies that estimated reporting probabilities depend on the\nassumed lag structure of the latent process.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 10:48:35 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 09:45:33 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Bracher", "Johannes", ""]]}, {"id": "1812.06870", "submitter": "Jon Sporring", "authors": "Jon Sporring and Rasmus Waagepetersen and Stefan Sommer", "title": "Generalizations of Ripley's K-function with Application to Space Curves", "comments": "9 pages & 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intensity function and Ripley's K-function have been used extensively in\nthe literature to describe the first and second moment structure of spatial\npoint sets. This has many applications including describing the statistical\nstructure of synaptic vesicles. Some attempts have been made to extend Ripley's\nK-function to curve pieces. Such an extension can be used to describe the\nstatistical structure of muscle fibers and brain fiber tracks. In this paper,\nwe take a computational perspective and construct new and very general variants\nof Ripley's K-function for curves pieces, surface patches etc. We discuss the\nmethod from [Chiu, Stoyan, Kendall, & Mecke 2013] and compare it with our\ngeneralizations theoretically, and we give examples demonstrating the\ndifference in their ability to separate sets of curve pieces.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 16:16:06 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sporring", "Jon", ""], ["Waagepetersen", "Rasmus", ""], ["Sommer", "Stefan", ""]]}, {"id": "1812.06880", "submitter": "Piotr Fryzlewicz", "authors": "Piotr Fryzlewicz", "title": "Detecting possibly frequent change-points: Wild Binary Segmentation 2\n  and steepest-drop model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing procedures for detecting multiple change-points in data\nsequences fail in frequent-change-point scenarios. This article proposes a new\nchange-point detection methodology designed to work well in both infrequent and\nfrequent change-point settings. It is made up of two ingredients: one is \"Wild\nBinary Segmentation 2\" (WBS2), a recursive algorithm for producing what we call\na `complete' solution path to the change-point detection problem, i.e. a\nsequence of estimated nested models containing $0, \\ldots, T-1$ change-points,\nwhere $T$ is the data length. The other ingredient is a new model selection\nprocedure, referred to as \"Steepest Drop to Low Levels\" (SDLL). The SDLL\ncriterion acts on the WBS2 solution path, and, unlike many existing model\nselection procedures for change-point problems, it is not penalty-based, and\nonly uses thresholding as a certain discrete secondary check. The resulting\nWBS2.SDLL procedure, combining both ingredients, is shown to be consistent, and\nto significantly outperform the competition in the frequent change-point\nscenarios tested. WBS2.SDLL is fast, easy to code and does not require the\nchoice of a window or span parameter.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 16:38:11 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 20:44:22 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 16:48:40 GMT"}, {"version": "v4", "created": "Mon, 24 Feb 2020 18:37:36 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Fryzlewicz", "Piotr", ""]]}, {"id": "1812.06948", "submitter": "Paulo Serra", "authors": "Paulo Serra, Tatyana Krivobokova, Francisco Rosales", "title": "Adaptive Non-parametric Estimation of Mean and Autocovariance in\n  Regression with Dependent Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully automatic non-parametric approach to simultaneous\nestimation of mean and autocovariance functions in regression with dependent\nerrors. Our empirical Bayesian approach is adaptive, numerically efficient and\nallows for the construction of confidence sets for the regression function.\nConsistency of the estimators is shown and small sample performance is\ndemonstrated in simulations and real data analysis. The method is implemented\nin the R package eBsc that accompanies the paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 18:39:07 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Serra", "Paulo", ""], ["Krivobokova", "Tatyana", ""], ["Rosales", "Francisco", ""]]}, {"id": "1812.07153", "submitter": "Abbas Zaidi", "authors": "Abbas Zaidi, Sayan Mukherjee", "title": "Gaussian Process Mixtures for Estimating Heterogeneous Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Gaussian-process mixture model for heterogeneous treatment\neffect estimation that leverages the use of transformed outcomes. The approach\nwe will present attempts to improve point estimation and uncertainty\nquantification relative to past work that has used transformed variable related\nmethods as well as traditional outcome modeling. Earlier work on modeling\ntreatment effect heterogeneity using transformed outcomes has relied on tree\nbased methods such as single regression trees and random forests. Under the\numbrella of non-parametric models, outcome modeling has been performed using\nBayesian additive regression trees and various flavors of weighted single\ntrees. These approaches work well when large samples are available, but suffer\nin smaller samples where results are more sensitive to model misspecification -\nour method attempts to garner improvements in inference quality via a correctly\nspecified model rooted in Bayesian non-parametrics. Furthermore, while we begin\nwith a model that assumes that the treatment assignment mechanism is known, an\nextension where it is learnt from the data is presented for applications to\nobservational studies. Our approach is applied to simulated and real data to\ndemonstrate our theorized improvements in inference with respect to two causal\nestimands: the conditional average treatment effect and the average treatment\neffect. By leveraging our correctly specified model, we are able to more\naccurately estimate the treatment effects while reducing their variance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 03:43:41 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Zaidi", "Abbas", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1812.07173", "submitter": "Chen Zhang", "authors": "Chen Zhang, Zimu Chen, Zhanfeng Wang, Yaohua Wu", "title": "Robust functional ANOVA model with t-process", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation approaches are of fundamental importance for statistical\nmodelling. To reduce susceptibility to outliers, we propose a robust estimation\nprocedure with t-process under functional ANOVA model. Besides common mean\nstructure of the studied subjects, their personal characters are also\ninformative, especially for prediction. We develop a prediction method to\npredict the individual effect. Statistical properties, such as robustness and\ninformation consistency, are studied. Numerical studies including simulation\nand real data examples show that the proposed method performs well.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 05:13:04 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Zhang", "Chen", ""], ["Chen", "Zimu", ""], ["Wang", "Zhanfeng", ""], ["Wu", "Yaohua", ""]]}, {"id": "1812.07259", "submitter": "Gertraud Malsiner-Walli", "authors": "Gertraud Malsiner-Walli and Helga Wagner", "title": "Comparing Spike and Slab Priors for Bayesian Variable Selection", "comments": null, "journal-ref": "Austrian Journal of Statistics, Vol. 40 (2011), No. 4, 241--264", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in building regression models is to decide which regressors\nshould be included in the final model. In a Bayesian approach, variable\nselection can be performed using mixture priors with a spike and a slab\ncomponent for the effects subject to selection. As the spike is concentrated at\nzero, variable selection is based on the probability of assigning the\ncorresponding regression effect to the slab component. These posterior\ninclusion probabilities can be determined by MCMC sampling. In this paper we\ncompare the MCMC implementations for several spike and slab priors with regard\nto posterior inclusion probabilities and their sampling efficiency for\nsimulated data. Further, we investigate posterior inclusion probabilities\nanalytically for different slabs in two simple settings. Application of\nvariable selection with spike and slab priors is illustrated on a data set of\npsychiatric patients where the goal is to identify covariates affecting\nmetabolism.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 09:35:41 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Malsiner-Walli", "Gertraud", ""], ["Wagner", "Helga", ""]]}, {"id": "1812.07271", "submitter": "Luca Rossini", "authors": "Fabrizio Leisen, Rams\\'es H. Mena, Freddy Palma Mancilla, Luca Rossini", "title": "On a flexible construction of a negative binomial model", "comments": "Forthcoming in \"Statistics & Probability Letters\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a construction of stationary Markov models with\nnegative-binomial marginal distributions. A simple closed form expression for\nthe corresponding transition probabilities is given, linking the proposal to\nwell-known classes of birth and death processes and thus revealing interesting\ncharacterizations. The advantage of having such closed form expressions is\ntested on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 10:12:35 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 05:58:56 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Mena", "Rams\u00e9s H.", ""], ["Mancilla", "Freddy Palma", ""], ["Rossini", "Luca", ""]]}, {"id": "1812.07295", "submitter": "Matteo Grigoletto", "authors": "Luisa Bisaglia and Matteo Grigoletto", "title": "A new time-varying model for forecasting long-memory series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new class of long-memory models with time-varying\nfractional parameter. In particular, the dynamics of the long-memory\ncoefficient, $d$, is specified through a stochastic recurrence equation driven\nby the score of the predictive likelihood, as suggested by Creal et al. (2013)\nand Harvey (2013). We demonstrate the validity of the proposed model by a Monte\nCarlo experiment and an application to two real time series.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 11:03:11 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Bisaglia", "Luisa", ""], ["Grigoletto", "Matteo", ""]]}, {"id": "1812.07318", "submitter": "Vladim\\'ir Hol\\'y", "authors": "Francisco Blasques, Vladim\\'ir Hol\\'y and Petra Tomanov\\'a", "title": "Zero-Inflated Autoregressive Conditional Duration Model for Discrete\n  Trade Durations with Excessive Zeros", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finance, durations between successive transactions are usually modeled by\nthe autoregressive conditional duration model based on a continuous\ndistribution omitting zero values. Zero or close-to-zero durations can be\ncaused by either split transactions or independent transactions. We propose a\ndiscrete model allowing for excessive zero values based on the zero-inflated\nnegative binomial distribution with score dynamics. This model allows to\ndistinguish between the processes generating split and standard transactions.\nWe use the existing theory on score models to establish the invertibility of\nthe score filter and verify that sufficient conditions hold for the consistency\nand asymptotic normality of the maximum likelihood of the model parameters. In\nan empirical study of DJIA stocks, we find that split transactions cause on\naverage 63% of close-to-zero values. Furthermore, the loss of decimal places in\nthe proposed approach is less severe than incorrect treatment of close-to-zero\nvalues in continuous models.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 12:04:49 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 17:41:57 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Blasques", "Francisco", ""], ["Hol\u00fd", "Vladim\u00edr", ""], ["Tomanov\u00e1", "Petra", ""]]}, {"id": "1812.07408", "submitter": "Gustavo Pereira", "authors": "Gustavo H. A. Pereira, Juliana S. Rodrigues, Manoel Santos Neto,\n  Denise A. Botter, M\\^onica C. Sandoval", "title": "A residual for outlier identification in zero adjusted regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero adjusted regression models are used to fit variables that are discrete\nat zero and continuous at some interval of the positive real numbers.\nDiagnostic analysis in these models is usually performed using the randomized\nquantile residual, which is useful for checking the overall adequacy of a zero\nadjusted regression model. However, it may fail to identify some outliers. In\nthis work, we introduce a residual for outlier identification in zero adjusted\nregression models. Monte Carlo simulation studies and an application suggest\nthat the residual introduced here has good properties and detects outliers that\nare not identified by the randomized quantile residual.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 14:50:30 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Pereira", "Gustavo H. A.", ""], ["Rodrigues", "Juliana S.", ""], ["Neto", "Manoel Santos", ""], ["Botter", "Denise A.", ""], ["Sandoval", "M\u00f4nica C.", ""]]}, {"id": "1812.07435", "submitter": "Davide Pigoli", "authors": "Alessandra Menafoglio, Davide Pigoli, Piercesare Secchi", "title": "Kriging Riemannian Data via Random Domain Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data taking value on a Riemannian manifold and observed over a complex\nspatial domain are becoming more frequent in applications, e.g. in\nenvironmental sciences and in geoscience. The analysis of these data needs to\nrely on local models to account for the non stationarity of the generating\nrandom process, the non linearity of the manifold and the complex topology of\nthe domain. In this paper, we propose to use a random domain decomposition\napproach to estimate an ensemble of local models and then to aggregate the\npredictions of the local models through Fr\\'{e}chet averaging. The algorithm is\nintroduced in complete generality and is valid for data belonging to any smooth\nRiemannian manifold but it is then described in details for the case of the\nmanifold of positive definite matrices, the hypersphere and the Cholesky\nmanifold. The predictive performance of the method are explored via simulation\nstudies for covariance matrices and correlation matrices, where the Cholesky\nmanifold geometry is used. Finally, the method is illustrated on an\nenvironmental dataset observed over the Chesapeake Bay (USA).\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 15:35:10 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Menafoglio", "Alessandra", ""], ["Pigoli", "Davide", ""], ["Secchi", "Piercesare", ""]]}, {"id": "1812.07485", "submitter": "Michail Tsagris", "authors": "Yannis Pantazis, Michail Tsagris and Andrew T.A. Wood", "title": "Gaussian asymptotic limits for the $\\alpha$-transformation in the\n  analysis of compositional data", "comments": "This is a preprint of the original publication that is available at\n  https://link.springer.com/article/10.1007/s13171-018-00160-1", "journal-ref": null, "doi": "10.1007/s13171-018-00160-1", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data consists of vectors of proportions whose components sum to\n1. Such vectors lie in the standard simplex, which is a manifold with boundary.\nOne issue that has been rather controversial within the field of compositional\ndata analysis is the choice of metric on the simplex. One popular possibility\nhas been to use the metric implied by logtransforming the data, as proposed by\nAitchison [1, 2]; and another popular approach has been to use the standard\nEuclidean metric inherited from the ambient space. Tsagris et al. [21] proposed\na one-parameter family of power transformations, the $\\alpha$-transformations,\nwhich include both the metric implied by Aitchison's transformation and the\nEuclidean metric as particular cases. Our underlying philosophy is that, with\nmany datasets, it may make sense to use the data to help us determine a\nsuitable metric. A related possibility is to apply the $\\alpha$-transformations\nto a parametric family of distributions, and then estimate a along with the\nother parameters. However, as we shall see, when one follows this last approach\nwith the Dirichlet family, some care is needed in a certain limiting case which\narises $(\\alpha \\neq 0)$, as we found out when fitting this model to real and\nsimulated data. Specifically, when the maximum likelihood estimator of a is\nclose to 0, the other parameters tend to be large. The main purpose of the\npaper is to study this limiting case both theoretically and numerically and to\nprovide insight into these numerical findings.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 19:33:54 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 09:49:31 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Pantazis", "Yannis", ""], ["Tsagris", "Michail", ""], ["Wood", "Andrew T. A.", ""]]}, {"id": "1812.07488", "submitter": "Lei Sun", "authors": "Lei Sun and Matthew Stephens", "title": "Solving the Empirical Bayes Normal Means Problem with Correlated Noise", "comments": "27 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Normal Means problem plays a fundamental role in many areas of modern\nhigh-dimensional statistics, both in theory and practice. And the Empirical\nBayes (EB) approach to solving this problem has been shown to be highly\neffective, again both in theory and practice. However, almost all EB treatments\nof the Normal Means problem assume that the observations are independent. In\npractice correlations are ubiquitous in real-world applications, and these\ncorrelations can grossly distort EB estimates. Here, exploiting theory from\nSchwartzman (2010), we develop new EB methods for solving the Normal Means\nproblem that take account of unknown correlations among observations. We\nprovide practical software implementations of these methods, and illustrate\nthem in the context of large-scale multiple testing problems and False\nDiscovery Rate (FDR) control. In realistic numerical experiments our methods\ncompare favorably with other commonly-used multiple testing methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:15:58 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 16:55:16 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sun", "Lei", ""], ["Stephens", "Matthew", ""]]}, {"id": "1812.07496", "submitter": "Debasis Kundu Professor", "authors": "Swagata Nandi and Debasis Kundu", "title": "Estimating the fundamental frequency using modified Newton-Raphson\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a modified Newton-Raphson algorithm to estimate the\nfrequency parameter in the fundamental frequency model in presence of an\nadditive stationary error. The proposed estimator is super efficient in nature\nin the sense that its asymptotic variance is less than the asymptotic variance\nof the least squares estimator. With a proper step factor modification, the\nproposed modified Newton-Raphson algorithm produces an estimator with the rate\n$O_p(n^{-\\frac{3}{2}})$, the same rate as the least squares estimator.\nNumerical experiments are performed for different sample sizes, different error\nvariances and for different models. For illustrative purposes, two real data\nsets are analyzed using the fundamental frequency model and the estimators are\nobtained using the proposed algorithm. It is observed the model and the\nproposed algorithm work quite well in both cases.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 11:44:40 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Nandi", "Swagata", ""], ["Kundu", "Debasis", ""]]}, {"id": "1812.07497", "submitter": "Yusuke Kaino", "authors": "Yusuke Kaino, Shogo H. Nakakita, and Masayuki Uchida", "title": "Hybrid estimation for ergodic diffusion processes based on noisy\n  discrete observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parametric estimation for ergodic diffusion processes with noisy\nsampled data based on the hybrid method, that is, the multi-step estimation\nwith the initial Bayes type estimators. In order to select proper initial\nvalues for optimisation of the quasi likelihood function of ergodic diffusion\nprocesses with noisy observations, we construct the initial Bayes type\nestimator based on the local means of the noisy observations. The asymptotic\nproperties of the initial Bayes type estimators and the hybrid multi-step\nestimators with the initial Bayes type estimators are shown, and a concrete\nexample and the simulation results are given.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:06:54 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kaino", "Yusuke", ""], ["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1812.07673", "submitter": "Thomas Trikalinos", "authors": "Alexandra G. Ellis, Rowan Iskandar, Christopher H. Schmid, John B.\n  Wong, Thomas A. Trikalinos", "title": "Active learning for efficiently training emulators of computationally\n  expensive mathematical models", "comments": "Counting appendix materials: 31 pages, 7 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emulator is a fast-to-evaluate statistical approximation of a detailed\nmathematical model (simulator). When used in lieu of simulators, emulators can\nexpedite tasks that require many repeated evaluations, such as sensitivity\nanalyses, policy optimization, model calibration, and value-of-information\nanalyses. Emulators are developed using the output of simulators at specific\ninput values (design points). Developing an emulator that closely approximates\nthe simulator can require many design points, which becomes computationally\nexpensive. We describe a self-terminating active learning algorithm to\nefficiently develop emulators tailored to a specific emulation task, and\ncompare it with algorithms that optimize geometric criteria (random latin\nhypercube sampling and maximum projection designs) and other active learning\nalgorithms (treed Gaussian Processes that optimize typical active learning\ncriteria). We compared the algorithms' root mean square error (RMSE) and\nmaximum absolute deviation from the simulator (MAX) for seven benchmark\nfunctions and in a prostate cancer screening model. In the empirical analyses,\nin simulators with greatly-varying smoothness over the input domain, active\nlearning algorithms resulted in emulators with smaller RMSE and MAX for the\nsame number of design points. In all other cases, all algorithms performed\ncomparably. The proposed algorithm attained satisfactory performance in all\nanalyses, had smaller variability than the treed Gaussian Processes (it is\ndeterministic), and, on average, had similar or better performance as the treed\nGaussian Processes in 6 out of 7 benchmark functions and in the prostate cancer\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 22:31:31 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 18:56:38 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Ellis", "Alexandra G.", ""], ["Iskandar", "Rowan", ""], ["Schmid", "Christopher H.", ""], ["Wong", "John B.", ""], ["Trikalinos", "Thomas A.", ""]]}, {"id": "1812.07691", "submitter": "Jingjing Zou", "authors": "Jingjing Zou, David J. Lederer, Daniel Rabinowitz", "title": "Efficiency in Lung Transplant Allocation Strategies", "comments": "36 pages of main text, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently in the United States, lung transplantations are allocated to\ncandidates according to the candidates' Lung Allocation Score (LAS). The LAS is\nan ad-hoc ranking system for patients' priorities of transplantation. The goal\nof this study is to develop a framework for improving patients' life expectancy\nover the LAS based on a comprehensive modeling of the lung transplantation\nwaiting list. Patients and organs are modeled as arriving according to Poisson\nprocesses, patients' health status evolving a waiting time inhomogeneous Markov\nprocess until death or transplantation, with organ recipient's expected\npost-transplant residual life depending on waiting time and health status at\ntransplantation. Under allocation rules satisfying minimal fairness\nrequirements, the long-term average expected life converges, and its limit is a\nnatural standard for comparing allocation strategies. Via the\nHamilton-Jacobi-Bellman equations, upper bounds for the limiting average\nexpected life are derived as a function of organ availability. Corresponding to\neach upper bound is an allocable set of (time, state) pairs at which patients\nwould be optimally transplanted. The allocable set expands monotonically as\norgan availability increases, which motivates the development of an allocation\nstrategy that leads to long-term expected life close to the upper bound.\nSimulation studies are conducted with model parameters estimated from national\nlung transplantation data. Results suggest that compared to the LAS, the\nproposed allocation strategy could provide a 7.7% increase in average total\nlife. We further extended the results to the the allocation and matching of\nmultiple organ types.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:22:42 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 03:53:30 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Zou", "Jingjing", ""], ["Lederer", "David J.", ""], ["Rabinowitz", "Daniel", ""]]}, {"id": "1812.07694", "submitter": "Alexander Petersen", "authors": "Alexander Petersen and Hans-Georg M\\\"uller", "title": "Wasserstein Covariance for Multiple Random Densities", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common feature of methods for analyzing samples of probability density\nfunctions is that they respect the geometry inherent to the space of densities.\nOnce a metric is specified for this space, the Fr\\'echet mean is typically used\nto quantify and visualize the average density from the sample. For\none-dimensional densities, the Wasserstein metric is popular due to its\ntheoretical appeal and interpretive value as an optimal transport metric,\nleading to the Wasserstein-Fr\\'echet mean or barycenter as the mean density. We\nextend the existing methodology for samples of densities in two key directions.\nFirst, motivated by applications in neuroimaging, we consider dependent density\ndata, where a $p$-vector of univariate random densities is observed for each\nsampling unit. Second, we introduce a Wasserstein covariance measure and\npropose intuitively appealing estimators for both fixed and diverging $p$,\nwhere the latter corresponds to continuously-indexed densities. We also give\ntheory demonstrating consistency and asymptotic normality, while accounting for\nerrors introduced in the unavoidable preparatory density estimation step. The\nutility of the Wasserstein covariance matrix is demonstrated through\napplications to functional connectivity in the brain using functional magnetic\nresonance imaging data and to the secular evolution of mortality for various\ncountries.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:30:30 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Petersen", "Alexander", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1812.07706", "submitter": "Jun Yang", "authors": "Jun Yang and Zhou Zhou", "title": "Spectral Inference under Complex Temporal Dynamics", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop unified theory and methodology for the inference of evolutionary\nFourier power spectra for a general class of locally stationary and possibly\nnonlinear processes. In particular, simultaneous confidence regions (SCR) with\nasymptotically correct coverage rates are constructed for the evolutionary\nspectral densities on a nearly optimally dense grid of the joint time-frequency\ndomain. A simulation based bootstrap method is proposed to implement the SCR.\nThe SCR enables researchers and practitioners to visually evaluate the\nmagnitude and pattern of the evolutionary power spectra with asymptotically\naccurate statistical guarantee. The SCR also serves as a unified tool for a\nwide range of statistical inference problems in time-frequency analysis ranging\nfrom tests for white noise, stationarity and time-frequency separability to the\nvalidation for non-stationary linear models.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 00:28:19 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 06:14:00 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 01:29:41 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yang", "Jun", ""], ["Zhou", "Zhou", ""]]}, {"id": "1812.07728", "submitter": "Peter Cohen", "authors": "Peter L. Cohen, Matt A. Olson, Colin B. Fogarty", "title": "Multivariate one-sided testing in matched observational studies as an\n  adversarial game", "comments": "11 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multivariate one-sided sensitivity analysis for matched\nobservational studies, appropriate when the researcher has specified that a\ngiven causal mechanism should manifest itself in effects on multiple outcome\nvariables in a known direction. The test statistic can be thought of as the\nsolution to an adversarial game, where the researcher determines the best\nlinear combination of test statistics to combat nature's presentation of the\nworst-case pattern of hidden bias. The corresponding optimization problem is\nconvex, and can be solved efficiently even for reasonably sized observational\nstudies. Asymptotically the test statistic converges to a chi-bar-squared\ndistribution under the null, a common distribution in order restricted\nstatistical inference. The test attains the largest possible design sensitivity\nover a class of coherent test statistics, and facilitates one-sided sensitivity\nanalyses for individual outcome variables while maintaining familywise error\ncontrol through is incorporation into closed testing procedures.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 01:55:47 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 01:01:01 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Cohen", "Peter L.", ""], ["Olson", "Matt A.", ""], ["Fogarty", "Colin B.", ""]]}, {"id": "1812.07730", "submitter": "Budhi Arta Surya", "authors": "H. Frydman and B.A. Surya", "title": "The Mixture of Markov Jump Processes: Monte Carlo Method and the EM\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses tractable development and statistical estimation of a\ncontinuous time stochastic process with a finite state space having non-Markov\nproperty. The process is formed by a finite mixture of right-continuous Markov\njump processes moving at different speeds on the same finite state space,\nwhereas the speed regimes are assumed to be unobservable. The mixture was first\nproposed by Frydman (J. Am. Stat. Assoc., 100, 1046-1053, 2005) in 2005 and\nrecently generalized in Surya (Stoch. Syst. 8, 29-44, 2018), in which\ndistributional properties and explicit identities of the process are given in\nits full generality. The contribution of this paper is two fold. First, we\npresent Monte Carlo method for constructing the process and show distributional\nequivalence between the simulated process and the actual process. Secondly, we\nperform statistical inference on the distribution parameters of the process.\nUnder complete observation of the sample paths, maximum likelihood estimates\nare given in explicit form in terms of sufficient statistics of the process.\nEstimation under incomplete observation is performed using the EM algorithm.\nThe estimation results completely characterize the process in terms of the\ninitial probability of starting the process in any phase of the state space,\nintensity matrices of the underlying Markov jump processes, and the switching\nprobability matrix of the process. Some numerical examples are given to test\nthe performance of the developed method. The proposed estimation generalizes\nthe existing statistical inferences for the Markov model by Albert (Ann. Math.\nStatist., 38, p.727-753., 1961), the mover-stayer model by Frydman (J. Am.\nStat. Assoc.79, 632-638., 1984) and the Markov mixture model by Frydman (J. Am.\nStat. Assoc., 100, 1046-1053, 2005).\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 02:09:28 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 01:43:10 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Frydman", "H.", ""], ["Surya", "B. A.", ""]]}, {"id": "1812.07736", "submitter": "John Lewis", "authors": "John R. Lewis, Steven N. MacEachern, Yoonkyung Lee", "title": "Bayesian Restricted Likelihood Methods: Conditioning on Insufficient\n  Statistics in Bayesian Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods have proven themselves to be successful across a wide range\nof scientific problems and have many well-documented advantages over competing\nmethods. However, these methods run into difficulties for two major and\nprevalent classes of problems: handling data sets with outliers and dealing\nwith model misspecification. We outline the drawbacks of previous solutions to\nboth of these problems and propose a new method as an alternative. When working\nwith the new method, the data is summarized through a set of insufficient\nstatistics, targeting inferential quantities of interest, and the prior\ndistribution is updated with the summary statistics rather than the complete\ndata. By careful choice of conditioning statistics, we retain the main benefits\nof Bayesian methods while reducing the sensitivity of the analysis to features\nof the data not captured by the conditioning statistics. For reducing\nsensitivity to outliers, classical robust estimators (e.g., M-estimators) are\nnatural choices for conditioning statistics. A major contribution of this work\nis the development of a data augmented Markov chain Monte Carlo (MCMC)\nalgorithm for the linear model and a large class of summary statistics. We\ndemonstrate the method on simulated and real data sets containing outliers and\nsubject to model misspecification. Success is manifested in better predictive\nperformance for data points of interest as compared to competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 02:41:12 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Lewis", "John R.", ""], ["MacEachern", "Steven N.", ""], ["Lee", "Yoonkyung", ""]]}, {"id": "1812.07813", "submitter": "Xiaojun Mao", "authors": "Xiaojun Mao, Raymond K. W. Wong and Song Xi Chen", "title": "Matrix Completion under Low-Rank Missing Mechanism", "comments": "29 pages, 0 figures", "journal-ref": null, "doi": "10.5705/ss.202019.0196", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a modern missing data problem where both the missing\nstructure and the underlying parameter are high dimensional. Although missing\nstructure is a key component to any missing data problems, existing matrix\ncompletion methods often assume a simple uniform missing mechanism. In this\nwork, we study matrix completion from corrupted data under a novel low-rank\nmissing mechanism. The probability matrix of observation is estimated via a\nhigh dimensional low-rank matrix estimation procedure, and further used to\ncomplete the target matrix via inverse probabilities weighting. Due to both\nhigh dimensional and extreme (i.e., very small) nature of the true probability\nmatrix, the effect of inverse probability weighting requires careful study. We\nderive optimal asymptotic convergence rates of the proposed estimators for both\nthe observation probabilities and the target matrix.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:46:50 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 02:56:27 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Mao", "Xiaojun", ""], ["Wong", "Raymond K. W.", ""], ["Chen", "Song Xi", ""]]}, {"id": "1812.07935", "submitter": "Manoel Santos Neto", "authors": "Jeremias Le\\~ao and Marcelo Bourguignon and Manoel Santos-Neto and\n  Helton Saulo", "title": "The negative binomial beta prime regression model with cure rate", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a cure rate survival model by assuming that the time to\nthe event of interest follows a beta prime distribution and that the number of\ncompeting causes of the event of interest follows a negative binomial\ndistribution. This model provides a novel alternative to the existing cure rate\nregression models due to its flexibility, as the beta prime model can exhibit\ngreater levels of skewness and kurtosis than those of the gamma and inverse\nGaussian distributions. Moreover, the hazard rate of this model can have an\nupside-down bathtub or an increasing shape. We approach both parameter\nestimation and local influence based on likelihood methods. In special, three\nperturbation schemes are considered for local influence. Numerical evaluation\nof the proposed model is performed by Monte Carlo simulations. In order to\nillustrate the potential for practice of our model we apply it to a real data\nset.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 13:32:37 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Le\u00e3o", "Jeremias", ""], ["Bourguignon", "Marcelo", ""], ["Santos-Neto", "Manoel", ""], ["Saulo", "Helton", ""]]}, {"id": "1812.07955", "submitter": "Benedikt M. P\u00c3\u00b6tscher", "authors": "Hannes Leeb, Benedikt M. P\\\"otscher, and Danijel Kivaranovic", "title": "Discussion on \"Model Confidence Bounds for Variable Selection\" by Yang\n  Li, Yuetian Luo, Davide Ferrari, Xiaonan Hu, and Yichen Qin", "comments": null, "journal-ref": "Biometrics 75 (2019), 407-410", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment on the article mentioned in the title.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 14:10:37 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 15:32:57 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Leeb", "Hannes", ""], ["P\u00f6tscher", "Benedikt M.", ""], ["Kivaranovic", "Danijel", ""]]}, {"id": "1812.08147", "submitter": "Kevin Lin", "authors": "Kevin Z. Lin and Han Liu and Kathryn Roeder", "title": "Covariance-based sample selection for heterogeneous data: Applications\n  to gene expression and autism risk gene detection", "comments": "45 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk for autism can be influenced by genetic mutations in hundreds of genes.\nBased on findings showing that genes with highly correlated gene expressions\nare functionally interrelated, \"guilt by association\" methods such as DAWN have\nbeen developed to identify these autism risk genes. Previous research analyzes\nthe BrainSpan dataset, which contains gene expression of brain tissues from\nvarying regions and developmental periods. Since the spatiotemporal properties\nof brain tissue is known to affect the gene expression's covariance, previous\nresearch have focused only on a specific subset of samples to avoid the issue\nof heterogeneity. This leads to a potential loss of power when detecting risk\ngenes. In this article, we develop a new method called COBS (COvariance-Based\nsample Selection) to find a larger and more homogeneous subset of samples that\nshare the same population covariance matrix for the downstream DAWN analysis.\nTo demonstrate COBS's effectiveness, we utilize genetic risk scores from two\nsequential data freezes obtained in 2014 and 2019. We show COBS improves DAWN's\nability to predict risk genes detected in the newer data freeze when utilizing\nthe risk scores of the older data freeze as input.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 18:43:49 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 23:24:36 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Lin", "Kevin Z.", ""], ["Liu", "Han", ""], ["Roeder", "Kathryn", ""]]}, {"id": "1812.08217", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Qiao Hu, Cheng Liu, Cheng Yong Tang", "title": "Optimal covariance matrix estimation for high-dimensional noise in\n  high-frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional measurement errors with high-frequency data. Our\nfocus is on recovering the covariance matrix of the random errors with\noptimality. In this problem, not all components of the random vector are\nobserved at the same time and the measurement errors are latent variables,\nleading to major challenges besides high data dimensionality. We propose a new\ncovariance matrix estimator in this context with appropriate localization and\nthresholding. By developing a new technical device integrating the\nhigh-frequency data feature with the conventional notion of $\\alpha$-mixing,\nour analysis successfully accommodates the challenging serial dependence in the\nmeasurement errors. Our theoretical analysis establishes the minimax optimal\nconvergence rates associated with two commonly used loss functions. We then\nestablish cases when the proposed localized estimator with thresholding\nachieves the minimax optimal convergence rates. Considering that the variances\nand covariances can be small in reality, we conduct a second-order theoretical\nanalysis that further disentangles the dominating bias in the estimator. A\nbias-corrected estimator is then proposed to ensure its practical finite sample\nperformance. We illustrate the promising empirical performance of the proposed\nestimator with extensive simulation studies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 19:49:31 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 08:08:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chang", "Jinyuan", ""], ["Hu", "Qiao", ""], ["Liu", "Cheng", ""], ["Tang", "Cheng Yong", ""]]}, {"id": "1812.08233", "submitter": "Peter B\\\"uhlmann", "authors": "Peter B\\\"uhlmann", "title": "Invariance, Causality and Robustness", "comments": "36 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss recent work for causal inference and predictive robustness in a\nunifying way. The key idea relies on a notion of probabilistic invariance or\nstability: it opens up new insights for formulating causality as a certain risk\nminimization problem with a corresponding notion of robustness. The invariance\nitself can be estimated from general heterogeneous or perturbation data which\nfrequently occur with nowadays data collection. The novel methodology is\npotentially useful in many applications, offering more robustness and better\n`causal-oriented' interpretation than machine learning or estimation in\nstandard regression or classification frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 20:39:54 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["B\u00fchlmann", "Peter", ""]]}, {"id": "1812.08638", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Franz Nestmann and Matthias Schulte", "title": "Testing multivariate uniformity based on random geometric graphs", "comments": "36 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new families of goodness-of-fit tests of uniformity on a\nfull-dimensional set $W\\subset\\R^d$ based on statistics related to edge lengths\nof random geometric graphs. Asymptotic normality of these statistics is proven\nunder the null hypothesis as well as under fixed alternatives. The derived\ntests are consistent and their behaviour for some contiguous alternatives can\nbe controlled. A simulation study suggests that the procedures can compete with\nor are better than established goodness-of-fit tests. We show with a real data\nexample that the new tests can detect non-uniformity of a small sample data\nset, where most of the competitors fail.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 15:38:05 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 14:04:55 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ebner", "Bruno", ""], ["Nestmann", "Franz", ""], ["Schulte", "Matthias", ""]]}, {"id": "1812.08683", "submitter": "Yang Ning", "authors": "Yang Ning, Sida Peng, Kosuke Imai", "title": "Robust Estimation of Causal Effects via High-Dimensional Covariate\n  Balancing Propensity Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust method to estimate the average treatment\neffects in observational studies when the number of potential confounders is\npossibly much greater than the sample size. We first use a class of penalized\nM-estimators for the propensity score and outcome models. We then calibrate the\ninitial estimate of the propensity score by balancing a carefully selected\nsubset of covariates that are predictive of the outcome. Finally, the estimated\npropensity score is used to construct the inverse probability weighting\nestimator. We prove that the proposed estimator, which has the sample\nboundedness property, is root-n consistent, asymptotically normal, and\nsemiparametrically efficient when the propensity score model is correctly\nspecified and the outcome model is linear in covariates. More importantly, we\nshow that our estimator remains root-n consistent and asymptotically normal so\nlong as either the propensity score model or the outcome model is correctly\nspecified. We provide valid confidence intervals in both cases and further\nextend these results to the case where the outcome model is a generalized\nlinear model. In simulation studies, we find that the proposed methodology\noften estimates the average treatment effect more accurately than the existing\nmethods. We also present an empirical application, in which we estimate the\naverage causal effect of college attendance on adulthood political\nparticipation. Open-source software is available for implementing the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 16:33:08 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Ning", "Yang", ""], ["Peng", "Sida", ""], ["Imai", "Kosuke", ""]]}, {"id": "1812.08696", "submitter": "Eric Laber", "authors": "Eric B. Laber and Min Qian", "title": "Generalization error for decision problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this entry we review the generalization error for classification and\nsingle-stage decision problems. We distinguish three alternative definitions of\nthe generalization error which have, at times, been conflated in the statistics\nliterature and show that these definitions need not be equivalent even\nasymptotically. Because the generalization error is a non-smooth functional of\nthe underlying generative model, standard asymptotic approximations, e.g., the\nbootstrap or normal approximations, cannot guarantee correct frequentist\noperating characteristics without modification. We provide simple data-adaptive\nprocedures that can be used to construct asymptotically valid confidence sets\nfor the generalization error. We conclude the entry with a discussion of\nextensions and related problems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 16:54:50 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Laber", "Eric B.", ""], ["Qian", "Min", ""]]}, {"id": "1812.08824", "submitter": "Albert Vexler", "authors": "Li Zou, Albert Vexler, Jihnhee Yu, Hongzhi Wan", "title": "A Sequential Density-Based Empirical Likelihood Ratio Test for Treatment\n  Effects", "comments": "Statistics in Medicine, Accepted. Keywords: Empirical likelihood;\n  Density-based empirical likelihood; Entropy; Likelihood ratio; Paired data;\n  Sequential signed-rank test; Treatment effect; Ventilator-associated\n  pneumonia", "journal-ref": "Statistics in Medicine (2019)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In health-related experiments, treatment effects can be identified using\npaired data that consist of pre- and post-treatment measurements. In this\nframework, sequential testing strategies are widely accepted statistical tools\nin practice. Since performances of parametric sequential testing procedures\nvitally depend on the validity of the parametric assumptions regarding\nunderlying data distributions, we focus on distribution-free mechanisms for\nsequentially evaluating treatment effects. In fixed sample size designs, the\ndensity-based empirical likelihood (DBEL) methods provide powerful\nnonparametric approximations to optimal Neyman-Pearson type statistics. In this\narticle, we extend the DBEL methodology to develop a novel sequential DBEL\ntesting procedure for detecting treatment effects based on paired data. The\nasymptotic consistency of the proposed test is shown. An extensive Monte Carlo\nstudy confirms that the proposed test outperforms the conventional sequential\nWilcoxon signed-rank test across a variety of alternatives. The excellent\napplicability of the proposed method is exemplified using the\nventilator-associated pneumonia study that evaluates the effect of\nChlorhexidine Gluconate treatment in reducing oral colonization by pathogens in\nventilated patients.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 20:09:58 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Zou", "Li", ""], ["Vexler", "Albert", ""], ["Yu", "Jihnhee", ""], ["Wan", "Hongzhi", ""]]}, {"id": "1812.08916", "submitter": "Han Xiao", "authors": "Rong Chen, Han Xiao, Dan Yang", "title": "Autoregressive Models for Matrix-Valued Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finance, economics and many other fields, observations in a matrix form\nare often generated over time. For example, a set of key economic indicators\nare regularly reported in different countries every quarter. The observations\nat each quarter neatly form a matrix and are observed over many consecutive\nquarters. Dynamic transport networks with observations generated on the edges\ncan be formed as a matrix observed over time. Although it is natural to turn\nthe matrix observations into a long vector, and then use the standard vector\ntime series models for analysis, it is often the case that the columns and rows\nof the matrix represent different types of structures that are closely\ninterplayed. In this paper we follow the autoregressive structure for modeling\ntime series and propose a novel matrix autoregressive model in a bilinear form\nthat maintains and utilizes the matrix structure to achieve a greater\ndimensional reduction, as well as more interpretable results. Probabilistic\nproperties of the models are investigated. Estimation procedures with their\ntheoretical properties are presented and demonstrated with simulated and real\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 02:39:02 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 15:10:12 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Chen", "Rong", ""], ["Xiao", "Han", ""], ["Yang", "Dan", ""]]}, {"id": "1812.08924", "submitter": "Ilmun Kim", "authors": "Ilmun Kim", "title": "Multinomial Goodness-of-Fit Based on U-Statistics: High-Dimensional\n  Asymptotic and Minimax Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multinomial goodness-of-fit tests in the high-dimensional regime\nwhere the number of bins increases with the sample size. In this regime,\nPearson's chi-squared test can suffer from low power due to the substantial\nbias as well as high variance of its statistic. To resolve these issues, we\nintroduce a family of U-statistic for multinomial goodness-of-fit and study\ntheir asymptotic behaviors in high-dimensions. Specifically, we establish\nconditions under which the considered U-statistic is asymptotically Poisson or\nGaussian, and investigate its power function under each asymptotic regime.\nFurthermore, we introduce a class of weights for the U-statistic that results\nin minimax rate optimal tests.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 03:17:55 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Kim", "Ilmun", ""]]}, {"id": "1812.08927", "submitter": "Ilmun Kim", "authors": "Ilmun Kim, Ann B. Lee, Jing Lei", "title": "Global and Local Two-Sample Tests via Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample testing is a fundamental problem in statistics. Despite its long\nhistory, there has been renewed interest in this problem with the advent of\nhigh-dimensional and complex data. Specifically, in the machine learning\nliterature, there have been recent methodological developments such as\nclassification accuracy tests. The goal of this work is to present a regression\napproach to comparing multivariate distributions of complex data. Depending on\nthe chosen regression model, our framework can efficiently handle different\ntypes of variables and various structures in the data, with competitive power\nunder many practical scenarios. Whereas previous work has been largely limited\nto global tests which conceal much of the local information, our approach\nnaturally leads to a local two-sample testing framework in which we identify\nlocal differences between multivariate distributions with statistical\nconfidence. We demonstrate the efficacy of our approach both theoretically and\nempirically, under some well-known parametric and nonparametric regression\nmethods. Our proposed methods are applied to simulated data as well as a\nchallenging astronomy data set to assess their practical usefulness.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 03:29:14 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 21:57:42 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 14:46:59 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kim", "Ilmun", ""], ["Lee", "Ann B.", ""], ["Lei", "Jing", ""]]}, {"id": "1812.09158", "submitter": "Olivier Bouaziz", "authors": "Olivier Bouaziz (MAP5 - UMR 8145, UPD5, IUT - Paris Descartes, USPC),\n  Eva Lauridsen, Gr\\'egory Nuel (LPSM (UMR\\_8001))", "title": "Regression modelling of interval censored data based on the adaptive\n  ridge procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for the analysis of time to ankylosis complication on a dataset\nof replanted teeth is proposed. In this context of left-censored,\ninterval-censored and right-censored data, a Cox model with piecewise constant\nbaseline hazard is introduced. Estimation is carried out with the EM algorithm\nby treating the true event times as unobserved variables. This estimation\nprocedure is shown to produce a block diagonal Hessian matrix of the baseline\nparameters. Taking advantage of this interesting feature of the estimation\nmethod a L0 penalised likelihood method is implemented in order to\nautomatically determine the number and locations of the cuts of the baseline\nhazard. This procedure allows to detect specific areas of time where patients\nare at greater risks for ankylosis. The method can be directly extended to the\ninclusion of exact observations and to a cure fraction. Theoretical results are\nobtained which allow to derive statistical inference of the model parameters\nfrom asymptotic likelihood theory. Through simulation studies, the penalisation\ntechnique is shown to provide a good fit of the baseline hazard and precise\nestimations of the resulting regression parameters.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:49:02 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 13:54:01 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Bouaziz", "Olivier", "", "MAP5 - UMR 8145, UPD5, IUT - Paris Descartes, USPC"], ["Lauridsen", "Eva", "", "LPSM"], ["Nuel", "Gr\u00e9gory", "", "LPSM"]]}, {"id": "1812.09203", "submitter": "Xi Chen", "authors": "Xi Chen, Jin Xie, Qingcong Yuan", "title": "Pan-Cancer Epigenetic Biomarker Selection from Blood Samples Using SAS", "comments": "9 pages, MWSUG 2018", "journal-ref": "MWSUG 2018 conference proceedings", "doi": null, "report-no": "HS-45", "categories": "q-bio.GN stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A key focus in current cancer research is the discovery of cancer biomarkers\nthat allow earlier detection with high accuracy and lower costs for both\npatients and hospitals. Blood samples have long been used as a health status\nindicator, but DNA methylation signatures in blood have not been fully\nappreciated in cancer research. Historically, analysis of cancer has been\nconducted directly with the patient's tumor or related tissues. Such analyses\nallow physicians to diagnose a patient's health and cancer status; however,\nphysicians must observe certain symptoms that prompt them to use biopsies or\nimaging to verify the diagnosis. This is a post-hoc approach. Our study will\nfocus on epigenetic information for cancer detection, specifically information\nabout DNA methylation in human peripheral blood samples in cancer discordant\nmonozygotic twin-pairs. This information might be able to help us detect cancer\nmuch earlier, before the first symptom appears. Several other types of\nepigenetic data can also be used, but here we demonstrate the potential of\nblood DNA methylation data as a biomarker for pan-cancer using SAS 9.3 and SAS\nEM. We report that 55 methylation CpG sites measurable in blood samples can be\nused as biomarkers for early cancer detection and classification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 15:42:00 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Chen", "Xi", ""], ["Xie", "Jin", ""], ["Yuan", "Qingcong", ""]]}, {"id": "1812.09384", "submitter": "Dootika Vats", "authors": "Dootika Vats and Christina Knudson", "title": "Revisiting the Gelman-Rubin Diagnostic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gelman and Rubin's (1992) convergence diagnostic is one of the most popular\nmethods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the\nseminal paper, researchers have developed sophisticated methods for estimating\nvariance of Monte Carlo averages. We show that these estimators find immediate\nuse in the Gelman-Rubin statistic, a connection not previously established in\nthe literature. We incorporate these estimators to upgrade both the univariate\nand multivariate Gelman-Rubin statistics, leading to improved stability in MCMC\ntermination time. An immediate advantage is that our new Gelman-Rubin statistic\ncan be calculated for a single chain. In addition, we establish a one-to-one\nrelationship between the Gelman-Rubin statistic and effective sample size.\nLeveraging this relationship, we develop a principled termination criterion for\nthe Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved\ndiagnostic via examples.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 21:48:15 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 16:08:09 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 11:55:23 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Vats", "Dootika", ""], ["Knudson", "Christina", ""]]}, {"id": "1812.09424", "submitter": "Yuan-chin Ivan Chang", "authors": "Zhanfeng Wang and Yuan-chin Ivan Chang", "title": "Distributed sequential method for analyzing massive data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyse a very large data set containing lengthy variables, we adopt a\nsequential estimation idea and propose a parallel divide-and-conquer method. We\nconduct several conventional sequential estimation procedures separately, and\nproperly integrate their results while maintaining the desired statistical\nproperties. Additionally, using a criterion from the statistical experiment\ndesign, we adopt an adaptive sample selection, together with an adaptive\nshrinkage estimation method, to simultaneously accelerate the estimation\nprocedure and identify the effective variables. We confirm the cogency of our\nmethods through theoretical justifications and numerical results derived from\nsynthesized data sets. We then apply the proposed method to three real data\nsets, including those pertaining to appliance energy use and particulate matter\nconcentration.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 00:54:31 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1812.09481", "submitter": "Satoshi Goto", "authors": "Satoshi Goto and Mariko Takagishi and Hiroshi Yadohisa", "title": "Bi-clustering for time-varying relational count data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational count data are often obtained from sources such as simultaneous\npurchase in online shops and social networking service information.\nBi-clustering such relational count data reveals the latent structure of the\nrelationship between objects such as household items or people. When relational\ncount data observed at multiple time points are available, it is worthwhile\nincorporating the time structure into the bi-clustering result to understand\nhow objects move between the cluster over time. In this paper, we propose two\nbi-clustering methods for analyzing time-varying relational count data. The\nfirst model, the dynamic Poisson infinite relational model (dPIRM), handles\ntime-varying relational count data. In the second model, which we call the\ndynamic zero-inflated Poisson infinite relational model, we further extend the\ndPIRM so that it can handle zero-inflated data. Proposing both two models is\nimportant as zero-inflated data are often encountered, especially when the time\nintervals are short. In addition, by explicitly deriving the relevant full\nconditional distributions, we describe the features of the estimated parameters\nand, in turn, the relationship between the two models. We show the\neffectiveness of both models through a simulation study and a real data\nexample.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 09:07:05 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Goto", "Satoshi", ""], ["Takagishi", "Mariko", ""], ["Yadohisa", "Hiroshi", ""]]}, {"id": "1812.09590", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle", "title": "Bayesian Propagation of Record Linkage Uncertainty into Population Size\n  Estimation of Human Rights Violations", "comments": "Published as part of the Special Issue in memory of Stephen E.\n  Fienberg (https://projecteuclid.org/euclid.aoas/1532743462#toc), in\n  https://projecteuclid.org/euclid.aoas/1532743484 at the Annals of Applied\n  Statistics\n  (https://www.imstat.org/journals-and-publications/annals-of-applied-statistics/)\n  by the Institute of Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2018, Volume 12, Number 2, 1013-1038", "doi": "10.1214/18-AOAS1178", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-systems or capture-recapture estimation are common techniques for\npopulation size estimation, particularly in the quantitative study of human\nrights violations. These methods rely on multiple samples from the population,\nalong with the information of which individuals appear in which samples. The\ngoal of record linkage techniques is to identify unique individuals across\nsamples based on the information collected on them. Linkage decisions are\nsubject to uncertainty when such information contains errors and missingness,\nand when different individuals have very similar characteristics. Uncertainty\nin the linkage should be propagated into the stage of population size\nestimation. We propose an approach called linkage-averaging to propagate\nlinkage uncertainty, as quantified by some Bayesian record linkage\nmethodologies, into a subsequent stage of population size estimation.\nLinkage-averaging is a two-stage approach in which the results from the record\nlinkage stage are fed into the population size estimation stage. We show that\nunder some conditions the results of this approach correspond to those of a\nproper Bayesian joint model for both record linkage and population size\nestimation. The two-stage nature of linkage-averaging allows us to combine\ndifferent record linkage models with different capture-recapture models, which\nfacilitates model exploration. We present a case study from the Salvadoran\ncivil war, where we are interested in estimating the total number of civilian\nkillings using lists of witnesses' reports collected by different\norganizations. These lists contain duplicates, typographical and spelling\nerrors, missingness, and other inaccuracies that lead to uncertainty in the\nlinkage. We show how linkage-averaging can be used for transferring the\nuncertainty in the linkage of these lists into different models for population\nsize estimation.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 19:38:17 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sadinle", "Mauricio", ""]]}, {"id": "1812.09607", "submitter": "Yoav Zemel", "authors": "Bastian Galasso and Yoav Zemel and Miguel de Carvalho", "title": "Bayesian semiparametric modelling of phase-varying point processes", "comments": "30 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian semiparametric approach for registration of multiple\npoint processes. Our approach entails modelling the mean measures of the\nphase-varying point processes with a Bernstein-Dirichlet prior, which induces a\nprior on the space of all warp functions. Theoretical results on the support of\nthe induced priors are derived, and posterior consistency is obtained under\nmild conditions. Numerical experiments suggest a good performance of the\nproposed methods, and a climatology real-data example is used to showcase how\nthe method can be employed in practice.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 21:44:50 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 13:10:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Galasso", "Bastian", ""], ["Zemel", "Yoav", ""], ["de Carvalho", "Miguel", ""]]}, {"id": "1812.09758", "submitter": "Paul McNicholas", "authors": "Forrest Paton and Paul D. McNicholas", "title": "Detecting British Columbia Coastal Rainfall Patterns by Clustering\n  Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is a statistical framework where data are assumed to\nfollow some functional form. This method of analysis is commonly applied to\ntime series data, where time, measured continuously or in discrete intervals,\nserves as the location for a function's value. Gaussian processes are a\ngeneralization of the multivariate normal distribution to function space and,\nin this paper, they are used to shed light on coastal rainfall patterns in\nBritish Columbia (BC). Specifically, this work addressed the question over how\none should carry out an exploratory cluster analysis for the BC, or any\nsimilar, coastal rainfall data. An approach is developed for clustering\nmultiple processes observed on a comparable interval, based on how similar\ntheir underlying covariance kernel is. This approach provides interesting\ninsights into the BC data, and these insights can be framed in terms of El\nNi\\~{n}o and La Ni\\~{n}a; however, the result is not simply one cluster\nrepresenting El Ni\\~{n}o years and another for La Ni\\~{n}a years. From one\nperspective, the results show that clustering annual rainfall can potentially\nbe used to identify extreme weather patterns.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 19:15:36 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 22:54:47 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Paton", "Forrest", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1812.09885", "submitter": "Christian P. Robert", "authors": "Gilles Celeux (INRIA), Sylvia Fruewirth-Schnatter (WU), and Christian\n  P. Robert (PSL)", "title": "Model Selection for Mixture Models - Perspectives and Strategies", "comments": "This is a preprint of a chapter in Handbook of Mixture Analysis\n  (2018), edited by Gilles Celeux, Sylvia Fruewirth-Schnatter, and Christian P.\n  Robert", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the number G of components in a finite mixture distribution is an\nimportant and difficult inference issue. This is a most important question,\nbecause statistical inference about the resulting model is highly sensitive to\nthe value of G. Selecting an erroneous value of G may produce a poor density\nestimate. This is also a most difficult question from a theoretical perspective\nas it relates to unidentifiability issues of the mixture model. This is further\na most relevant question from a practical viewpoint since the meaning of the\nnumber of components G is strongly related to the modelling purpose of a\nmixture distribution. We distinguish in this chapter between selecting G as a\ndensity estimation problem in Section 2 and selecting G in a model-based\nclustering framework in Section 3. Both sections discuss frequentist as well as\nBayesian approaches. We present here some of the Bayesian solutions to the\ndifferent interpretations of picking the \"right\" number of components in a\nmixture, before concluding on the ill-posed nature of the question.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 10:45:09 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Celeux", "Gilles", "", "INRIA"], ["Fruewirth-Schnatter", "Sylvia", "", "WU"], ["Robert", "Christian P.", "", "PSL"]]}, {"id": "1812.09895", "submitter": "Maximilian Kurthen", "authors": "Maximilian Kurthen, Torsten A. En{\\ss}lin", "title": "A Bayesian Model for Bivariate Causal Inference", "comments": null, "journal-ref": "Entropy 2020, 22(1), 46", "doi": "10.3390/e22010046", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of two-variable causal inference without intervention.\nThis task is to infer an existing causal relation between two random variables,\ni.e. $X \\rightarrow Y$ or $Y \\rightarrow X$ , from purely observational data.\nAs the option to modify a potential cause is not given in many situations only\nstructural properties of the data can be used to solve this ill-posed problem.\nWe briefly review a number of state-of-the-art methods for this, including very\nrecent ones. A novel inference method is introduced, Bayesian Causal Inference\n(BCI), which assumes a generative Bayesian hierarchical model to pursue the\nstrategy of Bayesian model selection. In the adopted model the distribution of\nthe cause variable is given by a Poisson lognormal distribution, which allows\nto explicitly regard the discrete nature of datasets, correlations in the\nparameter spaces, as well as the variance of probability densities on\nlogarithmic scales. We assume Fourier diagonal Field covariance operators. The\nmodel itself is restricted to use cases where a direct causal relation $X\n\\rightarrow Y$ has to be decided against a relation $Y \\rightarrow X$ ,\ntherefore we compare it other methods for this exact problem setting. The\ngenerative model assumed provides synthetic causal data for benchmarking our\nmodel in comparison to existing State-of-the-art models, namely LiNGAM ,\nANM-HSIC , ANM-MML , IGCI and CGNN . We explore how well the above methods\nperform in case of high noise settings, strongly discretized data and very\nsparse data. BCI performs generally reliable with synthetic data as well as\nwith the real world TCEP benchmark set, with an accuracy comparable to\nstate-of-the-art algorithms. We discuss directions for the future development\nof BCI .\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 11:34:09 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 16:54:10 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Kurthen", "Maximilian", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1812.09970", "submitter": "Stefan Wager", "authors": "Dmitry Arkhangelsky, Susan Athey, David A. Hirshberg, Guido W. Imbens,\n  Stefan Wager", "title": "Synthetic Difference in Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new estimator for causal effects with panel data that builds on\ninsights behind the widely used difference in differences and synthetic control\nmethods. Relative to these methods we find, both theoretically and empirically,\nthat this \"synthetic difference in differences\" estimator has desirable\nrobustness properties, and that it performs well in settings where the\nconventional estimators are commonly used in practice. We study the asymptotic\nbehavior of the estimator when the systematic part of the outcome model\nincludes latent unit factors interacted with latent time factors, and we\npresent conditions for consistency and asymptotic normality.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 19:35:12 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 18:28:35 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 18:39:59 GMT"}, {"version": "v4", "created": "Fri, 2 Jul 2021 22:05:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Arkhangelsky", "Dmitry", ""], ["Athey", "Susan", ""], ["Hirshberg", "David A.", ""], ["Imbens", "Guido W.", ""], ["Wager", "Stefan", ""]]}, {"id": "1812.10018", "submitter": "Wenchuan Guo", "authors": "Wenchuan Guo, Xiao-hua Zhou, and Shujie Ma", "title": "Optimal Treatment Selection Using the Covariate-Specific Treatment\n  Effect Curve with High-dimensional Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new semi-parametric modeling strategy for\nheterogeneous treatment effect estimation and individualized treatment\nselection, which are two major goals in personalized medicine, with a large\nnumber of baseline covariates. We achieve the first goal through estimating a\ncovariate-specific treatment effect (CSTE) curve modeled as an unknown function\nof a weighted linear combination of all baseline covariates. The weight or the\ncoefficient for each covariate is estimated by fitting a sparse semi-parametric\nlogistic single-index coefficient model. The CSTE curve is estimated by a\nspline-backfitted kernel procedure, which enables us to further construct a\nsimultaneous confidence band (SCB) for the CSTE curve under a desired\nconfidence level. Based on the SCB, we find the subgroups of patients that\nbenefit from each treatment, so that we can make individualized treatment\nselection. The proposed method is quite flexible to depict both local and\nglobal associations between the treatment and baseline covariates, and thus is\nrobust against model mis-specification in the presence of high-dimensional\ncovariates. We also establish the theoretical properties of our proposed\nprocedure. They provide a sound basis for conducting statistical inference in\nmaking individualized treatment decisions. Our proposed method is further\nillustrated by simulation studies and analysis of a real data example.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 03:26:18 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Guo", "Wenchuan", ""], ["Zhou", "Xiao-hua", ""], ["Ma", "Shujie", ""]]}, {"id": "1812.10042", "submitter": "Subhradev Sen", "authors": "Subhradev Sen, Hazem Al-Mofleh, Sudhansu S. Maiti", "title": "On discrimination between the Lindley and xgamma distributions", "comments": "17 pages, Communicated Article", "journal-ref": "Annals of Data Science, 2020", "doi": "10.1007/s40745-020-00243-7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given data set the problem of selecting either Lindley or xgamma\ndistribution with unknown parameter is investigated in this article. Both these\ndistributions can be used quite effectively for analyzing skewed non-negative\ndata and in modeling time-to-event data sets. We have used the ratio of the\nmaximized likelihoods in choosing between the Lindley and xgamma distributions.\nAsymptotic distributions of the ratio of the maximized likelihoods are obtained\nand those are utilized to determine the minimum sample size required to\ndiscriminate between these two distributions for user specified probability of\ncorrect selection and tolerance limit.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 05:58:14 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Sen", "Subhradev", ""], ["Al-Mofleh", "Hazem", ""], ["Maiti", "Sudhansu S.", ""]]}, {"id": "1812.10208", "submitter": "Adam Peterson", "authors": "Adam Peterson and Brisa Sanchez", "title": "rstap: An R Package for Spatial Temporal Aggregated Predictor Models", "comments": "20 pages 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rstap package implements Bayesian spatial temporal aggregated predictor\nmodels in R using the probabilistic programming language Stan. A variety of\ndistributions and link functions are supported, allowing users to fit this\nextension to the generalized linear model with both independent and correlated\noutcomes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 02:59:38 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Peterson", "Adam", ""], ["Sanchez", "Brisa", ""]]}, {"id": "1812.10551", "submitter": "Shiqing Yu", "authors": "Shiqing Yu, Mathias Drton, Ali Shojaie", "title": "Generalized Score Matching for Non-Negative Data", "comments": "70 pages, 76 figures", "journal-ref": "Journal of Machine Learning Research, 20(76):1-70, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge in estimating parameters of probability density functions\nis the intractability of the normalizing constant. While in such cases maximum\nlikelihood estimation may be implemented using numerical integration, the\napproach becomes computationally intensive. The score matching method of\nHyv\\\"arinen [2005] avoids direct calculation of the normalizing constant and\nyields closed-form estimates for exponential families of continuous\ndistributions over $\\mathbb{R}^m$. Hyv\\\"arinen [2007] extended the approach to\ndistributions supported on the non-negative orthant, $\\mathbb{R}_+^m$. In this\npaper, we give a generalized form of score matching for non-negative data that\nimproves estimation efficiency. As an example, we consider a general class of\npairwise interaction models. Addressing an overlooked inexistence problem, we\ngeneralize the regularized score matching method of Lin et al. [2016] and\nimprove its theoretical guarantees for non-negative Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 21:50:40 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 00:28:04 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Yu", "Shiqing", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "1812.10556", "submitter": "Rodrigo S. Targino", "authors": "Milan Merkle and Yuri F. Saporito and Rodrigo S. Targino", "title": "Bayesian Approach for Parameter Estimation of Continuous-Time Stochastic\n  Volatility Models using Fourier Transform Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two stage procedure for the estimation of the parameters of a\nfairly general, continuous-time stochastic volatility. An important ingredient\nof the proposed method is the Cuchiero-Teichmann volatility estimator, which is\nbased on Fourier transforms and provides a continuous time estimate of the\nlatent process. This estimate is then used to construct an approximate\nlikelihood for the parameters of interest, whose restrictions are taken into\naccount through prior distributions. The procedure is shown to be highly\nsuccessful for constructing the posterior distribution of the parameters of a\nHeston model, while limited success is achieved when applied to the highly\nparametrized exponential-Ornstein-Uhlenbeck.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:10:45 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Merkle", "Milan", ""], ["Saporito", "Yuri F.", ""], ["Targino", "Rodrigo S.", ""]]}, {"id": "1812.10594", "submitter": "Qifan Song", "authors": "Qifan Song and Guang Cheng", "title": "Bayesian Fusion Estimation via t-Shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage prior has gained great successes in many data analysis, however,\nits applications mostly focus on the Bayesian modeling of sparse parameters. In\nthis work, we will apply Bayesian shrinkage to model high dimensional parameter\nthat possesses an unknown blocking structure. We propose to impose heavy-tail\nshrinkage prior, e.g., $t$ prior, on the differences of successive parameter\nentries, and such a fusion prior will shrink successive differences towards\nzero and hence induce posterior blocking. Comparing to conventional Bayesian\nfused lasso which implements Laplace fusion prior, $t$ fusion prior induces\nstronger shrinkage effect and enjoys a nice posterior consistency property.\nSimulation studies and real data analyses show that $t$ fusion has superior\nperformance to the frequentist fusion estimator and Bayesian Laplace-fusion\nprior. This $t$-fusion strategy is further developed to conduct a Bayesian\nclustering analysis, and simulation shows that the proposed algorithm obtains\nbetter posterior distributional convergence than the classical Dirichlet\nprocess modeling.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 02:24:08 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Song", "Qifan", ""], ["Cheng", "Guang", ""]]}, {"id": "1812.10625", "submitter": "Long Feng", "authors": "Long Feng", "title": "Power Comparison between High Dimensional t-Test, Sign, and Signed Rank\n  Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a power comparison between high dimensional t-test,\nsign and signed rank test for the one sample mean test. We show that the high\ndimensional signed rank test is superior to a high dimensional t test, but\ninferior to a high dimensional sign test.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 05:04:39 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Feng", "Long", ""]]}, {"id": "1812.10644", "submitter": "Yichong Zhang", "authors": "Yichong Zhang and Xin Zheng", "title": "Quantile Treatment Effects and Bootstrap Inference under\n  Covariate-Adaptive Randomization", "comments": "121 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the estimation and inference of the quantile\ntreatment effect under covariate-adaptive randomization. We propose two\nestimation methods: (1) the simple quantile regression and (2) the inverse\npropensity score weighted quantile regression. For the two estimators, we\nderive their asymptotic distributions uniformly over a compact set of quantile\nindexes, and show that, when the treatment assignment rule does not achieve\nstrong balance, the inverse propensity score weighted estimator has a smaller\nasymptotic variance than the simple quantile regression estimator. For the\ninference of method (1), we show that the Wald test using a weighted bootstrap\nstandard error under-rejects. But for method (2), its asymptotic size equals\nthe nominal level. We also show that, for both methods, the asymptotic size of\nthe Wald test using a covariate-adaptive bootstrap standard error equals the\nnominal level. We illustrate the finite sample performance of the new\nestimation and inference methods using both simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 06:42:24 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 08:02:30 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 07:45:25 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 23:23:13 GMT"}, {"version": "v5", "created": "Tue, 25 Feb 2020 02:33:13 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Zhang", "Yichong", ""], ["Zheng", "Xin", ""]]}, {"id": "1812.10694", "submitter": "Jae-Kwang Kim", "authors": "Jae Kwang Kim and Seho Park and Yilin Chen and Changbao Wu", "title": "Combining Non-probability and Probability Survey Samples Through Mass\n  Imputation", "comments": "Submitted to Journal of the Royal Statistical Society: Series A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents theoretical results on combining non-probability and\nprobability survey samples through mass imputation, an approach originally\nproposed by Rivers (2007) as sample matching without rigorous theoretical\njustification. Under suitable regularity conditions, we establish the\nconsistency of the mass imputation estimator and derive its asymptotic variance\nformula. Variance estimators are developed using either linearization or\nbootstrap. Finite sample performances of the mass imputation estimator are\ninvestigated through simulation studies and an application to analyzing a\nnon-probability sample collected by the Pew Research Centre.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 10:59:05 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 11:20:58 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 23:41:38 GMT"}, {"version": "v4", "created": "Sat, 21 Nov 2020 20:54:13 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kim", "Jae Kwang", ""], ["Park", "Seho", ""], ["Chen", "Yilin", ""], ["Wu", "Changbao", ""]]}, {"id": "1812.10752", "submitter": "David Preinerstorfer", "authors": "David Preinerstorfer", "title": "How to avoid the zero-power trap in testing for correlation", "comments": null, "journal-ref": null, "doi": "10.1017/S0266466621000062", "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In testing for correlation of the errors in regression models the power of\ntests can be very low for strongly correlated errors. This counterintuitive\nphenomenon has become known as the \"zero-power trap\". Despite a considerable\namount of literature devoted to this problem, mainly focusing on its detection,\na convincing solution has not yet been found. In this article we first discuss\ntheoretical results concerning the occurrence of the zero-power trap\nphenomenon. Then, we suggest and compare three ways to avoid it. Given an\ninitial test that suffers from the zero-power trap, the method we recommend for\npractice leads to a modified test whose power converges to one as the\ncorrelation gets very strong. Furthermore, the modified test has approximately\nthe same power function as the initial test, and thus approximately preserves\nall of its optimality properties. We also provide some numerical illustrations\nin the context of testing for network generated correlation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 16:10:19 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Preinerstorfer", "David", ""]]}, {"id": "1812.10911", "submitter": "Xinran Li", "authors": "Xinran Li, Peng Ding, Donald B. Rubin", "title": "Rerandomization in $2^K$ Factorial Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With many pretreatment covariates and treatment factors, the classical\nfactorial experiment often fails to balance covariates across multiple\nfactorial effects simultaneously. Therefore, it is intuitive to restrict the\nrandomization of the treatment factors to satisfy certain covariate balance\ncriteria, possibly conforming to the tiers of factorial effects and covariates\nbased on their relative importances. This is rerandomization in factorial\nexperiments. We study the asymptotic properties of this experimental design\nunder the randomization inference framework without imposing any distributional\nor modeling assumptions of the covariates and outcomes. We derive the joint\nasymptotic sampling distribution of the usual estimators of the factorial\neffects, and show that it is symmetric, unimodal, and more \"concentrated\" at\nthe true factorial effects under rerandomization than under the classical\nfactorial experiment. We quantify this advantage of rerandomization using the\nnotions of \"central convex unimodality\" and \"peakedness\" of the joint\nasymptotic sampling distribution. We also construct conservative large-sample\nconfidence sets for the factorial effects.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 06:55:06 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "Xinran", ""], ["Ding", "Peng", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1812.11026", "submitter": "Larry Wasserman", "authors": "Isabella Verdinelli and Larry Wasserman", "title": "Hybrid Wasserstein Distance and Fast Distribution Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a modified Wasserstein distance for distribution clustering which\ninherits many of the properties of the Wasserstein distance but which can be\nestimated easily and computed quickly. The modified distance is the sum of two\nterms. The first term --- which has a closed form --- measures the\nlocation-scale differences between the distributions. The second term is an\napproximation that measures the remaining distance after accounting for\nlocation-scale differences. We consider several forms of approximation with our\nmain emphasis being a tangent space approximation that can be estimated using\nnonparametric regression. We evaluate the strengths and weaknesses of this\napproach on simulated and real examples.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 15:11:10 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1812.11361", "submitter": "Michail Tsagris", "authors": "Michail Tsagris, Abdulaziz Alenazi, Kleio-Maria Verrou and Nikolaos\n  Pandis", "title": "Hypothesis testing for two population means: parametric or\n  non-parametric test?", "comments": "Accepted for publication in the Journal of Statistical Computation\n  and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parametric Welch $t$-test and the non-parametric Wilcoxon-Mann-Whitney\ntest are the most commonly used two independent sample means tests. More recent\ntesting approaches include the non-parametric, empirical likelihood and\nexponential empirical likelihood. However, the applicability of these\nnon-parametric likelihood testing procedures is limited partially because of\ntheir tendency to inflate the type I error in small sized samples. In order to\ncircumvent the type I error problem, we propose simple calibrations using the\n$t$ distribution and bootstrapping. The two non-parametric likelihood testing\nprocedures, with and without those calibrations, are then compared against the\nWilcoxon-Mann-Whitney test and the Welch $t$-test. The comparisons are\nimplemented via extensive Monte Carlo simulations on the grounds of type I\nerror and power in small/medium sized samples generated from various non-normal\npopulations. The simulation studies clearly demonstrate that a) the $t$\ncalibration improves the type I error of the empirical likelihood, b) bootstrap\ncalibration improves the type I error of both non-parametric likelihoods, c)\nthe Welch $t$-test with or without bootstrap calibration attains the type I\nerror and produces similar levels of power with the former testing procedures,\nand d) the Wilcoxon-Mann-Whitney test produces inflated type I error while the\ncomputation of an exact p-value is not feasible in the presence of ties with\ndiscrete data. Further, an application to real gene expression data illustrates\nthe computational high cost and thus the impracticality of the non parametric\nlikelihoods. Overall, the Welch t-test, which is highly computationally\nefficient and readily interpretable, is shown to be the best method when\ntesting equality of two population means.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 13:09:31 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 20:13:11 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Tsagris", "Michail", ""], ["Alenazi", "Abdulaziz", ""], ["Verrou", "Kleio-Maria", ""], ["Pandis", "Nikolaos", ""]]}, {"id": "1812.11433", "submitter": "Emmanuel Candes", "authors": "Rina Foygel Barber and Emmanuel Candes", "title": "On the Construction of Knockoffs in Case-Control Studies", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a case-control study in which we have a random sample, constructed\nin such a way that the proportion of cases in our sample is different from that\nin the general population---for instance, the sample is constructed to achieve\na fixed ratio of cases to controls. Imagine that we wish to determine which of\nthe potentially many covariates under study truly influence the response by\napplying the new model-X knockoffs approach. This paper demonstrates that it\nsuffices to design knockoff variables using data that may have a different\nratio of cases to controls. For example, the knockoff variables can be\nconstructed using the distribution of the original variables under any of the\nfollowing scenarios: (1) a population of controls only; (2) a population of\ncases only; (3) a population of cases and controls mixed in an arbitrary\nproportion (irrespective of the fraction of cases in the sample at hand). The\nconsequence is that knockoff variables may be constructed using unlabeled data,\nwhich is often available more easily than labeled data, while maintaining\nType-I error guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 20:25:03 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1812.11555", "submitter": "Yiyuan She", "authors": "Yiyuan She and Hoang Tran", "title": "On Cross-validation for Sparse Reduced Rank Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data analysis, regularization methods pursuing sparsity\nand/or low rank have received a lot of attention recently. To provide a proper\namount of shrinkage, it is typical to use a grid search and a model comparison\ncriterion to find the optimal regularization parameters. However, we show that\nfixing the parameters across all folds may result in an inconsistency issue,\nand it is more appropriate to cross-validate projection-selection patterns to\nobtain the best coefficient estimate. Our in-sample error studies in jointly\nsparse and rank-deficient models lead to a new class of information criteria\nwith four scale-free forms to bypass the estimation of the noise level. By use\nof an identity, we propose a novel scale-free calibration to help\ncross-validation achieve the minimax optimal error rate non-asymptotically.\nExperiments support the efficacy of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 15:24:40 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["She", "Yiyuan", ""], ["Tran", "Hoang", ""]]}, {"id": "1812.11598", "submitter": "Matthew Masten", "authors": "Matthew A. Masten, Alexandre Poirier", "title": "Salvaging Falsified Instrumental Variable Models", "comments": "66 pages with 61 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What should researchers do when their baseline model is refuted? We provide\nfour constructive answers. First, researchers can measure the extent of\nfalsification. To do this, we consider continuous relaxations of the baseline\nassumptions of concern. We then define the falsification frontier: The smallest\nrelaxations of the baseline model which are not refuted. This frontier provides\na quantitative measure of the extent of falsification. Second, researchers can\npresent the identified set for the parameter of interest under the assumption\nthat the true model lies somewhere on this frontier. We call this the\nfalsification adaptive set. This set generalizes the standard baseline estimand\nto account for possible falsification. Third, researchers can present the\nidentified set for a specific point on this frontier. Finally, as a sensitivity\nanalysis, researchers can present identified sets for points beyond the\nfrontier. To illustrate these four ways of salvaging falsified models, we study\noveridentifying restrictions in two instrumental variable models: a homogeneous\neffects linear model, and heterogeneous effect models with either binary or\ncontinuous outcomes. In the linear model, we consider the classical\noveridentifying restrictions implied when multiple instruments are observed. We\ngeneralize these conditions by considering continuous relaxations of the\nclassical exclusion restrictions. By sufficiently weakening the assumptions, a\nfalsified baseline model becomes non-falsified. We obtain analogous results in\nthe heterogeneous effect models, where we derive identified sets for marginal\ndistributions of potential outcomes, falsification frontiers, and falsification\nadaptive sets under continuous relaxations of the instrument exogeneity\nassumptions. We illustrate our results in four different empirical\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 20:04:17 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 20:28:40 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 18:49:53 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Masten", "Matthew A.", ""], ["Poirier", "Alexandre", ""]]}, {"id": "1812.11689", "submitter": "Donghui Yan", "authors": "Donghui Yan, Yingjie Wang, Jin Wang, Honggang Wang and Zhenpeng Li", "title": "K-nearest Neighbor Search by Random Projection Forests", "comments": "15 pages, 4 figures, 2018 IEEE Big Data Conference", "journal-ref": "IEEE International Conference on Big Data, 2018", "doi": "10.1109/BigData.2018.8622307", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-nearest neighbor (kNN) search has wide applications in many areas,\nincluding data mining, machine learning, statistics and many applied domains.\nInspired by the success of ensemble methods and the flexibility of tree-based\nmethodology, we propose random projection forests (rpForests), for kNN search.\nrpForests finds kNNs by aggregating results from an ensemble of random\nprojection trees with each constructed recursively through a series of\ncarefully chosen random projections. rpForests achieves a remarkable accuracy\nin terms of fast decay in the missing rate of kNNs and that of discrepancy in\nthe kNN distances. rpForests has a very low computational complexity. The\nensemble nature of rpForests makes it easily run in parallel on multicore or\nclustered computers; the running time is expected to be nearly inversely\nproportional to the number of cores or machines. We give theoretical insights\nby showing the exponential decay of the probability that neighboring points\nwould be separated by ensemble random projection trees when the ensemble size\nincreases. Our theory can be used to refine the choice of random projections in\nthe growth of trees, and experiments show that the effect is remarkable.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 03:54:27 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Yan", "Donghui", ""], ["Wang", "Yingjie", ""], ["Wang", "Jin", ""], ["Wang", "Honggang", ""], ["Li", "Zhenpeng", ""]]}, {"id": "1812.11699", "submitter": "Arnab Hazra", "authors": "Arnab Hazra, Brian J. Reich, Benjamin A. Shaby, Ana-Maria Staicu", "title": "A semiparametric spatiotemporal Bayesian model for the bulk and extremes\n  of the Fosberg Fire Weather Index", "comments": "69 pages, 16 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large wildfires pose a major environmental concern, and precise maps of fire\nrisk can improve disaster relief planning. Fosberg Fire Weather Index (FFWI) is\noften used to measure wildfire risk; FFWI exhibits non-Gaussian marginal\ndistributions as well as strong spatiotemporal extremal dependence and thus,\nmodeling FFWI using geostatistical models like Gaussian processes is\nquestionable. Extreme value theory (EVT)-driven models like max-stable\nprocesses are theoretically appealing but are computationally demanding and\napplicable only for threshold exceedances or block maxima. Disaster management\npolicies often consider moderate-to-extreme quantiles of climate parameters and\nhence, joint modeling of the bulk and the tail of the data is required. In this\npaper, we consider a Dirichlet process mixture of spatial skew-t processes that\ncan flexibly model the bulk as well as the tail. The proposed model has\nnonstationary mean and covariance structure, and also nonzero spatiotemporal\nextremal dependence. A simulation study demonstrates that the proposed model\nhas better spatial prediction performance compared to some competing models. We\ndevelop spatial maps of FFWI medians and extremes, and discuss the wildfire\nrisk throughout the Santa Ana region of California.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 05:57:32 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 16:35:46 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hazra", "Arnab", ""], ["Reich", "Brian J.", ""], ["Shaby", "Benjamin A.", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1812.11918", "submitter": "Joshua Brul\\'e", "authors": "Joshua Brul\\'e", "title": "Whittemore: An embedded domain specific language for causal programming", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Whittemore, a language for causal programming. Causal\nprogramming is based on the theory of structural causal models and consists of\ntwo primary operations: identification, which finds formulas that compute\ncausal queries, and estimation, which applies formulas to transform probability\ndistributions to other probability distribution. Causal programming provides\nabstractions to declare models, queries, and distributions with syntax similar\nto standard mathematical notation, and conducts rigorous causal inference,\nwithout requiring detailed knowledge of the underlying algorithms. Examples of\ncausal inference with real data are provided, along with discussion of the\nimplementation and possibilities for future extension.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 20:41:20 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Brul\u00e9", "Joshua", ""]]}]