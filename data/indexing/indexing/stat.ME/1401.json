[{"id": "1401.0100", "submitter": "Feng Li", "authors": "Feng Li and Yanfei Kang", "title": "Improving forecasting performance using covariate-dependent copula\n  models", "comments": null, "journal-ref": "International Journal of Forecasting (2018), 34(3), pp. 456-476", "doi": "10.1016/j.ijforecast.2018.01.007", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas provide an attractive approach for constructing multivariate\ndistributions with flexible marginal distributions and different forms of\ndependences. Of particular importance in many areas is the possibility of\nexplicitly forecasting the tail-dependences. Most of the available approaches\nare only able to estimate tail-dependences and correlations via nuisance\nparameters, but can neither be used for interpretation, nor for forecasting.\nAiming to improve copula forecasting performance, we propose a general Bayesian\napproach for modeling and forecasting tail-dependences and correlations as\nexplicit functions of covariates. The proposed covariate-dependent copula model\nalso allows for Bayesian variable selection among covariates from the marginal\nmodels as well as the copula density. The copulas we study include Joe-Clayton\ncopula, Clayton copula, Gumbel copula and Student's \\emph{t}-copula. Posterior\ninference is carried out using an efficient MCMC simulation method. Our\napproach is applied to both simulated data and the S\\&P 100 and S\\&P 600 stock\nindices. The forecasting performance of the proposed approach is compared with\nother modeling strategies based on log predictive scores. Value-at-Risk\nevaluation is also preformed for model comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 06:01:53 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 03:01:23 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 14:03:32 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Li", "Feng", ""], ["Kang", "Yanfei", ""]]}, {"id": "1401.0118", "submitter": "Rajesh Ranganath", "authors": "Rajesh Ranganath and Sean Gerrish and David M. Blei", "title": "Black Box Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference has become a widely used method to approximate\nposteriors in complex latent variables models. However, deriving a variational\ninference algorithm generally requires significant model-specific analysis, and\nthese efforts can hinder and deter us from quickly developing and exploring a\nvariety of models for a problem at hand. In this paper, we present a \"black\nbox\" variational inference algorithm, one that can be quickly applied to many\nmodels with little additional derivation. Our method is based on a stochastic\noptimization of the variational objective where the noisy gradient is computed\nfrom Monte Carlo samples from the variational distribution. We develop a number\nof methods to reduce the variance of the gradient, always maintaining the\ncriterion that we want to avoid difficult model-based derivations. We evaluate\nour method against the corresponding black box sampling based methods. We find\nthat our method reaches better predictive likelihoods much faster than sampling\nmethods. Finally, we demonstrate that Black Box Variational Inference lets us\neasily explore a wide space of models by quickly constructing and evaluating\nseveral models of longitudinal healthcare data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 09:32:43 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Gerrish", "Sean", ""], ["Blei", "David M.", ""]]}, {"id": "1401.0168", "submitter": "Emeric Thibaud", "authors": "Emeric Thibaud and Thomas Opitz", "title": "Efficient inference and simulation for elliptical Pareto processes", "comments": null, "journal-ref": "Biometrika (2015) 102 (4): 855-870", "doi": "10.1093/biomet/asv045", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in extreme value theory have established $\\ell$-Pareto\nprocesses as the natural limits for extreme events defined in terms of\nexceedances of a risk functional. Here we provide methods for the practical\nmodelling of data based on a tractable yet flexible dependence model. We\nintroduce the class of elliptical $\\ell$-Pareto processes, which arise as the\nlimit of threshold exceedances of certain elliptical processes characterized by\na correlation function and a shape parameter. An efficient inference method\nbased on maximizing a full likelihood with partial censoring is developed.\nNovel procedures for exact conditional and unconditional simulation are\nproposed. These ideas are illustrated using precipitation extremes in\nSwitzerland.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 16:17:10 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 06:25:05 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Thibaud", "Emeric", ""], ["Opitz", "Thomas", ""]]}, {"id": "1401.0201", "submitter": "Ping Li", "authors": "Ping Li, Cun-Hui Zhang, Tong Zhang", "title": "Sparse Recovery with Very Sparse Compressed Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (sparse signal recovery) often encounters nonnegative data\n(e.g., images). Recently we developed the methodology of using (dense)\nCompressed Counting for recovering nonnegative K-sparse signals. In this paper,\nwe adopt very sparse Compressed Counting for nonnegative signal recovery. Our\ndesign matrix is sampled from a maximally-skewed p-stable distribution (0<p<1),\nand we sparsify the design matrix so that on average (1-g)-fraction of the\nentries become zero. The idea is related to very sparse stable random\nprojections (Li et al 2006 and Li 2007), the prior work for estimating summary\nstatistics of the data.\n  In our theoretical analysis, we show that, when p->0, it suffices to use M=\nK/(1-exp(-gK) log N measurements, so that all coordinates can be recovered in\none scan of the coordinates. If g = 1 (i.e., dense design), then M = K log N.\nIf g= 1/K or 2/K (i.e., very sparse design), then M = 1.58K log N or M = 1.16K\nlog N. This means the design matrix can be indeed very sparse at only a minor\ninflation of the sample complexity.\n  Interestingly, as p->1, the required number of measurements is essentially M\n= 2.7K log N, provided g= 1/K. It turns out that this result is a general\nworst-case bound.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 18:17:09 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""], ["Zhang", "Tong", ""]]}, {"id": "1401.0211", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng, Jiancheng Jiang and Xin Tong", "title": "Feature Augmentation via Nonparametrics and Selection (FANS) in High\n  Dimensional Classification", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a high dimensional classification method that involves\nnonparametric feature augmentation. Knowing that marginal density ratios are\nthe most powerful univariate classifiers, we use the ratio estimates to\ntransform the original feature measurements. Subsequently, penalized logistic\nregression is invoked, taking as input the newly transformed or augmented\nfeatures. This procedure trains models equipped with local complexity and\nglobal simplicity, thereby avoiding the curse of dimensionality while creating\na flexible nonlinear decision boundary. The resulting method is called Feature\nAugmentation via Nonparametrics and Selection (FANS). We motivate FANS by\ngeneralizing the Naive Bayes model, writing the log ratio of joint densities as\na linear combination of those of marginal densities. It is related to\ngeneralized additive models, but has better interpretability and computability.\nRisk bounds are developed for FANS. In numerical analysis, FANS is compared\nwith competing methods, so as to provide a guideline on its best application\ndomain. Real data analysis demonstrates that FANS performs very competitively\non benchmark email spam and gene expression data sets. Moreover, FANS is\nimplemented by an extremely fast algorithm through parallel computing.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 19:53:11 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 17:27:38 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Jiang", "Jiancheng", ""], ["Tong", "Xin", ""]]}, {"id": "1401.0267", "submitter": "Lixing Zhu", "authors": "Tao Wang, Xu Guo, Peirong Xu, Lixing Zhu", "title": "Transformed sufficient dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel general framework is proposed in this paper for dimension reduction\nin regression to fill the gap between linear and fully nonlinear dimension\nreduction. The main idea is to transform first each of the raw predictors\nmonotonically, and then search for a low-dimensional projection in the space\ndefined by the transformed variables. Both user-specified and data-driven\ntransformations are suggested. In each case, the methodology is discussed first\nin a general manner, and a representative method, as an example, is then\nproposed and evaluated by simulation. The proposed methods are applied to a\nreal data set for illustration.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 09:47:36 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wang", "Tao", ""], ["Guo", "Xu", ""], ["Xu", "Peirong", ""], ["Zhu", "Lixing", ""]]}, {"id": "1401.0303", "submitter": "Bernardo Nipoti", "authors": "Stefano Favaro, Bernardo Nipoti and Yee Whye Teh", "title": "Rediscovery of Good-Turing estimators via Bayesian nonparametrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating discovery probabilities originated in the context\nof statistical ecology, and in recent years it has become popular due to its\nfrequent appearance in challenging applications arising in genetics,\nbioinformatics, linguistics, designs of experiments, machine learning, etc. A\nfull range of statistical approaches, parametric and nonparametric as well as\nfrequentist and Bayesian, has been proposed for estimating discovery\nprobabilities. In this paper we investigate the relationships between the\ncelebrated Good-Turing approach, which is a frequentist nonparametric approach\ndeveloped in the 1940s, and a Bayesian nonparametric approach recently\nintroduced in the literature. Specifically, under the assumption of a two\nparameter Poisson-Dirichlet prior, we show that Bayesian nonparametric\nestimators of discovery probabilities are asymptotically equivalent, for a\nlarge sample size, to suitably smoothed Good-Turing estimators. As a by-product\nof this result, we introduce and investigate a methodology for deriving exact\nand asymptotic credible intervals to be associated with the Bayesian\nnonparametric estimators of discovery probabilities. The proposed methodology\nis illustrated through a comprehensive simulation study and the analysis of\nExpressed Sequence Tags data generated by sequencing a benchmark complementary\nDNA library.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 16:22:10 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2014 16:12:10 GMT"}, {"version": "v3", "created": "Fri, 4 Apr 2014 14:26:34 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2015 10:19:18 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Favaro", "Stefano", ""], ["Nipoti", "Bernardo", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1401.0404", "submitter": "Luca Tardella", "authors": "Cristina Mollica and Luca Tardella", "title": "Epitope profiling via mixture modeling of ranked data", "comments": "(revised to properly include references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of probability models for ranked data as a useful\nalternative to a quantitative data analysis to investigate the outcome of\nbioassay experiments, when the preliminary choice of an appropriate\nnormalization method for the raw numerical responses is difficult or subject to\ncriticism. We review standard distance-based and multistage ranking models and\nin this last context we propose an original generalization of the Plackett-Luce\nmodel to account for the order of the ranking elicitation process. The\nusefulness of the novel model is illustrated with its maximum likelihood\nestimation for a real data set. Specifically, we address the heterogeneous\nnature of experimental units via model-based clustering and detail the\nnecessary steps for a successful likelihood maximization through a hybrid\nversion of the Expectation-Maximization algorithm. The performance of the\nmixture model using the new distribution as mixture components is compared with\nthose relative to alternative mixture models for random rankings. A discussion\non the interpretation of the identified clusters and a comparison with more\nstandard quantitative approaches are finally provided.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 10:04:12 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 13:58:32 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Mollica", "Cristina", ""], ["Tardella", "Luca", ""]]}, {"id": "1401.0688", "submitter": "Jungsik Noh", "authors": "Jungsik Noh and Sangyeol Lee", "title": "Quantile Regression for Location-Scale Time Series Models with\n  Conditional Heteroscedasticity", "comments": "37 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers quantile regression for a wide class of time series\nmodels including ARMA models with asymmetric GARCH (AGARCH) errors. The\nclassical mean-variance models are reinterpreted as conditional location-scale\nmodels so that the quantile regression method can be naturally geared into the\nconsidered models. The consistency and asymptotic normality of the quantile\nregression estimator is established in location-scale time series models under\nmild conditions. In the application of this result to ARMA-AGARCH models, more\nprimitive conditions are deduced to obtain the asymptotic properties. For\nillustration, a simulation study and a real data analysis are provided.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 17:44:06 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 20:56:05 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Noh", "Jungsik", ""], ["Lee", "Sangyeol", ""]]}, {"id": "1401.0844", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana, Fabio Rapallo and Maria Piera Rogantin", "title": "$D$-optimal saturated designs: a simulation study", "comments": "8 pages. Preliminary version submitted to the 7th IWS Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we focus on saturated $D$-optimal designs. Using recent results,\nwe identify $D$-optimal designs with the solutions of an optimization problem\nwith linear constraints. We introduce new objective functions based on the\ngeometric structure of the design and we compare them with the classical\n$D$-efficiency criterion. We perform a simulation study. In all the test cases\nwe observe that designs with high values of $D$-efficiency have also high\nvalues of the new objective functions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 21:19:02 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""], ["Rogantin", "Maria Piera", ""]]}, {"id": "1401.0852", "submitter": "Qing Zhou", "authors": "Bryon Aragam and Qing Zhou", "title": "Concave Penalized Estimation of Sparse Gaussian Bayesian Networks", "comments": "57 pages", "journal-ref": "Journal of Machine Learning Research 16(Nov):2273-2328, 2015", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a penalized likelihood estimation framework to estimate the\nstructure of Gaussian Bayesian networks from observational data. In contrast to\nrecent methods which accelerate the learning problem by restricting the search\nspace, our main contribution is a fast algorithm for score-based structure\nlearning which does not restrict the search space in any way and works on\nhigh-dimensional datasets with thousands of variables. Our use of concave\nregularization, as opposed to the more popular $\\ell_0$ (e.g. BIC) penalty, is\nnew. Moreover, we provide theoretical guarantees which generalize existing\nasymptotic results when the underlying distribution is Gaussian. Most notably,\nour framework does not require the existence of a so-called faithful DAG\nrepresentation, and as a result the theory must handle the inherent\nnonidentifiability of the estimation problem in a novel way. Finally, as a\nmatter of independent interest, we provide a comprehensive comparison of our\napproach to several standard structure learning methods using open-source\npackages developed for the R language. Based on these experiments, we show that\nour algorithm is significantly faster than other competing methods while\nobtaining higher sensitivity with comparable false discovery rates for\nhigh-dimensional data. In particular, the total runtime for our method to\ngenerate a solution path of 20 estimates for DAGs with 8000 nodes is around one\nhour.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 23:27:48 GMT"}, {"version": "v2", "created": "Sun, 4 Jan 2015 23:34:01 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Aragam", "Bryon", ""], ["Zhou", "Qing", ""]]}, {"id": "1401.0903", "submitter": "Emmanuel Bacry", "authors": "Emmanuel Bacry and Jean-Francois Muzy", "title": "Second order statistics characterization of Hawkes processes and\n  non-parametric estimation", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.geo-ph q-fin.ST q-fin.TR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the jumps correlation matrix of a multivariate Hawkes process is\nrelated to the Hawkes kernel matrix through a system of Wiener-Hopf integral\nequations. A Wiener-Hopf argument allows one to prove that this system (in\nwhich the kernel matrix is the unknown) possesses a unique causal solution and\nconsequently that the second-order properties fully characterize a Hawkes\nprocess. The numerical inversion of this system of integral equations allows us\nto propose a fast and efficient method, which main principles were initially\nsketched in [Bacry and Muzy, 2013], to perform a non-parametric estimation of\nthe Hawkes kernel matrix. In this paper, we perform a systematic study of this\nnon-parametric estimation procedure in the general framework of marked Hawkes\nprocesses. We describe precisely this procedure step by step. We discuss the\nestimation error and explain how the values for the main parameters should be\nchosen. Various numerical examples are given in order to illustrate the broad\npossibilities of this estimation procedure ranging from 1-dimensional\n(power-law or non positive kernels) up to 3-dimensional (circular dependence)\nprocesses. A comparison to other non-parametric estimation procedures is made.\nApplications to high frequency trading events in financial markets and to\nearthquakes occurrence dynamics are finally considered.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 15:28:07 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 08:33:23 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Bacry", "Emmanuel", ""], ["Muzy", "Jean-Francois", ""]]}, {"id": "1401.1052", "submitter": "Dalia Chakrabarty Dr.", "authors": "Dalia Chakrabarty and Prasenjit Saha", "title": "Inverse Bayesian Estimation of Gravitational Mass Density in Galaxies\n  from Missing Kinematic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.GA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on a type of inverse problem in which the data is\nexpressed as an unknown function of the sought and unknown model function (or\nits discretised representation as a model parameter vector). In particular, we\ndeal with situations in which training data is not available. Then we cannot\nmodel the unknown functional relationship between data and the unknown model\nfunction (or parameter vector) with a Gaussian Process of appropriate\ndimensionality. A Bayesian method based on state space modelling is advanced\ninstead. Within this framework, the likelihood is expressed in terms of the\nprobability density function ($pdf$) of the state space variable and the sought\nmodel parameter vector is embedded within the domain of this $pdf$. As the\nmeasurable vector lives only inside an identified sub-volume of the system\nstate space, the $pdf$ of the state space variable is projected onto the space\nof the measurables, and it is in terms of the projected state space density\nthat the likelihood is written; the final form of the likelihood is achieved\nafter convolution with the distribution of measurement errors. Application\nmotivated vague priors are invoked and the posterior probability density of the\nmodel parameter vectors, given the data is computed. Inference is performed by\ntaking posterior samples with adaptive MCMC. The method is illustrated on\nsynthetic as well as real galactic data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 11:58:05 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chakrabarty", "Dalia", ""], ["Saha", "Prasenjit", ""]]}, {"id": "1401.1073", "submitter": "Mate Manfay", "authors": "L. Gerencser, M. Manfay", "title": "Empirical characteristic function identification of linear stochastic\n  systems with possibly unstable zeros", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to adapt the empirical characteristic function\n(ECF) method to stable, but possibly not inverse stable linear stochastic\nsystem driven by the increments of a Levy-process. A remarkable property of the\nECF method for i.i.d. data is that, under an ideal setting, it gives an\nefficient estimate of the unknown parameters of a given parametric family of\ndistributions. Variants of the ECF method for special classes of dependent data\nhas been suggested in several papers using the joint characteristic function of\nblocks of unprocessed data. However, the latter may be unavailable for\nLevy-systems. We introduce a new, computable score that is essentially a kind\nof output error. The feasibility of the procedure is based on a result of\nDevroye on the generation of r.v.-s with given c.f. Two special cases are\nconsidered in detail, and the asymptotic covariance matrices of the estimators\nare given. The present work extends our previous work on the ECF identification\nof stable and inverse stable linear stochastic Levy-systems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 13:33:13 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Gerencser", "L.", ""], ["Manfay", "M.", ""]]}, {"id": "1401.1130", "submitter": "Pierre-Andr\\'e Maugis", "authors": "P-A. G. Maugis", "title": "Event Conditional Correlation: Or How Non-Linear Linear Dependence Can\n  Be", "comments": "20 pages, 6 figures. Correlation, Linear-Approximation, Measures of\n  association, Regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entries of datasets are often collected only if an event occurred: taking a\nsurvey, enrolling in an experiment and so forth. However, such partial samples\nbias classical correlation estimators. Here we show how to correct for such\nsampling effects through two complementary estimators of event conditional\ncorrelation: the correlation of two random variables conditional on a given\nevent. First, we provide under minimal assumptions proof of consistency and\nasymptotic normality for the proposed estimators. Then, through synthetic\nexamples, we show that these estimators behave well in small-sample and yield\npowerful methodologies for non-linear regression as well as dependence testing.\nFinally, by using the two estimators in tandem, we explore counterfactual\ndependence regimes in a financial dataset. By so doing we show that the\ncontagion which took place during the 2007--2011 financial crisis cannot be\nexplained solely by increased financial risk.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 16:08:43 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2014 16:25:15 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 18:05:07 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Maugis", "P-A. G.", ""]]}, {"id": "1401.1137", "submitter": "Fran\\c{c}ois Caron", "authors": "Fran\\c{c}ois Caron and Emily B. Fox", "title": "Sparse graphs using exchangeable random measures", "comments": "New title. Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical network modeling has focused on representing the graph as a\ndiscrete structure, namely the adjacency matrix, and considering the\nexchangeability of this array. In such cases, the Aldous-Hoover representation\ntheorem (Aldous, 1981;Hoover, 1979} applies and informs us that the graph is\nnecessarily either dense or empty. In this paper, we instead consider\nrepresenting the graph as a measure on $\\mathbb{R}_+^2$. For the associated\ndefinition of exchangeability in this continuous space, we rely on the\nKallenberg representation theorem (Kallenberg, 2005). We show that for certain\nchoices of such exchangeable random measures underlying our graph construction,\nour network process is sparse with power-law degree distribution. In\nparticular, we build on the framework of completely random measures (CRMs) and\nuse the theory associated with such processes to derive important network\nproperties, such as an urn representation for our analysis and network\nsimulation. Our theoretical results are explored empirically and compared to\ncommon network models. We then present a Hamiltonian Monte Carlo algorithm for\nefficient exploration of the posterior distribution and demonstrate that we are\nable to recover graphs ranging from dense to sparse--and perform associated\ntests--based on our flexible CRM-based formulation. We explore network\nproperties in a range of real datasets, including Facebook social circles, a\npolitical blogosphere, protein networks, citation networks, and world wide web\nnetworks, including networks with hundreds of thousands of nodes and millions\nof edges.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 16:57:16 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 15:13:26 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2015 12:40:04 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Caron", "Fran\u00e7ois", ""], ["Fox", "Emily B.", ""]]}, {"id": "1401.1301", "submitter": "Laura Anderlucci", "authors": "Laura Anderlucci, Cinzia Viroli", "title": "Covariance pattern mixture models for the analysis of multivariate\n  heterogeneous longitudinal data", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS816 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 777-800", "doi": "10.1214/15-AOAS816", "report-no": "IMS-AOAS-AOAS816", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for modeling multivariate longitudinal data in\nthe presence of unobserved heterogeneity for the analysis of the Health and\nRetirement Study (HRS) data. Our proposal can be cast within the framework of\nlinear mixed models with discrete individual random intercepts; however,\ndifferently from the standard formulation, the proposed Covariance Pattern\nMixture Model (CPMM) does not require the usual local independence assumption.\nThe model is thus able to simultaneously model the heterogeneity, the\nassociation among the responses and the temporal dependence structure. We focus\non the investigation of temporal patterns related to the cognitive functioning\nin retired American respondents. In particular, we aim to understand whether it\ncan be affected by some individual socio-economical characteristics and whether\nit is possible to identify some homogenous groups of respondents that share a\nsimilar cognitive profile. An accurate description of the detected groups\nallows government policy interventions to be opportunely addressed. Results\nidentify three homogenous clusters of individuals with specific cognitive\nfunctioning, consistent with the class conditional distribution of the\ncovariates. The flexibility of CPMM allows for a different contribution of each\nregressor on the responses according to group membership. In so doing, the\nidentified groups receive a global and accurate phenomenological\ncharacterization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 08:01:19 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 12:51:50 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Anderlucci", "Laura", ""], ["Viroli", "Cinzia", ""]]}, {"id": "1401.1403", "submitter": "Atul Mallik", "authors": "Atul Mallik, Moulinath Banerjee and George Michailidis", "title": "M-estimation in multistage sampling procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-stage (designed) procedures, obtained by splitting the sampling budget\nsuitably across stages, and designing the sampling at a particular stage based\non information about the parameter obtained from previous stages, are often\nadvantageous from the perspective of precise inference. We develop a generic\nframework for M-estimation in a multistage setting and apply empirical process\ntechniques to develop limit theorems that describe the large sample behavior of\nthe resulting M-estimates. Applications to change-point estimation, inverse\nisotonic regression, classification and mode estimation are provided: it is\ntypically seen that the multistage procedure accentuates the efficiency of the\nM-estimates by accelerating the rate of convergence, relative to one-stage\nprocedures. The step-by-step process induces dependence across stages and\ncomplicates the analysis in such problems, which we address through careful\nconditioning arguments.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 14:48:23 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Mallik", "Atul", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "1401.1457", "submitter": "Tomaso Aste", "authors": "Anna Zaremba and Tomaso Aste", "title": "Measures of Causality in Complex Datasets with application to financial\n  data", "comments": "40 pages; 13 figures", "journal-ref": "Entropy 16 (2014) 2309-2349", "doi": "10.3390/e16042309", "report-no": null, "categories": "q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates the causality structure of financial time series.\nWe concentrate on three main approaches to measuring causality: linear Granger\ncausality, kernel generalisations of Granger causality (based on ridge\nregression and the Hilbert--Schmidt norm of the cross-covariance operator) and\ntransfer entropy, examining each method and comparing their theoretical\nproperties, with special attention given to the ability to capture nonlinear\ncausality. We also present the theoretical benefits of applying non-symmetrical\nmeasures rather than symmetrical measures of dependence. We apply the measures\nto a range of simulated and real data. The simulated data sets were generated\nwith linear and several types of nonlinear dependence, using bivariate, as well\nas multivariate settings. An application to real-world financial data\nhighlights the practical difficulties, as well as the potential of the methods.\nWe use two real data sets: (1) U.S. inflation and one-month Libor; (2) S$\\&$P\ndata and exchange rates for the following currencies: AUDJPY, CADJPY, NZDJPY,\nAUDCHF, CADCHF, NZDCHF. Overall, we reach the conclusion that no single method\ncan be recognised as the best in all circumstances, and each of the methods has\nits domain of best applicability. We also highlight areas for improvement and\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 17:44:09 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 11:00:43 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Zaremba", "Anna", ""], ["Aste", "Tomaso", ""]]}, {"id": "1401.1831", "submitter": "Yan Sun", "authors": "Yan Sun, Chunyang Li", "title": "On Linear Regression for Interval-valued Data in\n  $\\mathcal{K}_{\\mathcal{C}}\\left(\\mathbb{R}\\right)$", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been some time since interval-valued linear regression was\ninvestigated. In this paper, we focus on linear regression for interval-valued\ndata within the framework of random sets. The model we propose generalizes a\nseries of existing models. We establish important properties of the model in\nthe space of compact convex subsets of $\\mathbb{R}$, analogous to those for the\nclassical linear regression. Furthermore, we carry out theoretical\ninvestigations into the least squares estimation that is widely used in the\nliterature. A simulation study is presented that supports our theorems.\nFinally, an application to a climate data set is provided to demonstrate the\napplicability of our model.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 21:23:15 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 18:46:36 GMT"}, {"version": "v3", "created": "Sun, 18 Jan 2015 19:32:14 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2015 21:18:27 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Sun", "Yan", ""], ["Li", "Chunyang", ""]]}, {"id": "1401.1999", "submitter": "Roel Braekers", "authors": "Leen Prenen, Roel Braekers and Luc Duchateau", "title": "Extending the Archimedean copula methodology to model multivariate\n  survival data grouped in clusters of variable size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the analysis of clustered survival data, two different types of models\nthat take the association into account, are commonly used: frailty models and\ncopula models. Frailty models assume that conditional on a frailty term for\neach cluster, the hazard functions of individuals within that cluster are\nindependent. These unknown frailty terms with their imposed distribution are\nused to express the association between the different individuals in a cluster.\nCopula models on the other hand assume that the joint survival function of the\nindividuals within a cluster is given by a copula function, evaluated in the\nmarginal survival function of each individual. It is the copula function which\ndescribes the association between the lifetimes within a cluster. A major\ndisadvantage of the present copula models over the frailty models is that the\nsize of the different clusters must be small and equal in order to set up\nmanageable estimation procedures for the different model parameters. We\ndescribe in this manuscript a copula model for clustered survival data where\nthe clusters are allowed to be moderate to large and varying in size by\nconsidering the class of Archimedean copulas with completely monotone\ngenerator. We develop both one- and two-stage estimators for the different\ncopula parameters. Furthermore we show the consistency and asymptotic normality\nof these estimators. Finally, we perform a simulation study to investigate the\nfinite sample properties of the estimators. We illustrate the method on a data\nset containing the time to first insemination in cows, with cows clustered in\nherds.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 13:43:27 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Prenen", "Leen", ""], ["Braekers", "Roel", ""], ["Duchateau", "Luc", ""]]}, {"id": "1401.2054", "submitter": "Zhiyong Zhang", "authors": "Zhiyong Zhang, Kaifeng Jiang, Haiyan Liu, In-Sue Oh", "title": "Bayesian meta-analysis of correlation coefficients through power prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  To answer the call of introducing more Bayesian techniques to organizational\nresearch (e.g., Kruschke, Aguinis, & Joo, 2012; Zyphur & Oswald, 2013), we\npropose a Bayesian approach for meta-analysis with power prior in this article.\nThe primary purpose of this method is to allow meta-analytic researchers to\ncontrol the contribution of each individual study to an estimated overall\neffect size though power prior. This is due to the consideration that not all\nstudies included in a meta-analysis should be viewed as equally reliable, and\nthat by assigning more weights to reliable studies with power prior,\nresearchers may obtain an overall effect size that reflects the population\neffect size more accurately. We use the relationship between high-performance\nwork systems and financial performance as an example to illustrate how to apply\nthis method to organizational research. We also provide free online software\nthat can be used to conduct Bayesian meta-analysis proposed in this study.\nResearch implications and future directions are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 16:15:41 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 15:03:48 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Zhang", "Zhiyong", ""], ["Jiang", "Kaifeng", ""], ["Liu", "Haiyan", ""], ["Oh", "In-Sue", ""]]}, {"id": "1401.2081", "submitter": "Zhiyong Zhang", "authors": "Lijuan Wang, Zhiyong Zhang, Xin Tong", "title": "Mediation analysis with missing data through multiple imputation and\n  bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A method using multiple imputation and bootstrap for dealing with missing\ndata in mediation analysis is introduced and implemented in SAS. Through\nsimulation studies, it is shown that the method performs well for both MCAR and\nMAR data without and with auxiliary variables. It is also shown that the method\nworks equally well for MNAR data if auxiliary variables related to missingness\nare included. The application of the method is demonstrated through the\nanalysis of a subset of data from the National Longitudinal Survey of Youth.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 17:11:44 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Wang", "Lijuan", ""], ["Zhang", "Zhiyong", ""], ["Tong", "Xin", ""]]}, {"id": "1401.2163", "submitter": "Ying Lu", "authors": "Xia Cui, Ying Lu, Heng Peng", "title": "Estimation of Partially Linear Regression Model under Partial\n  Consistency Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, utilizing recent theoretical results in high dimensional\nstatistical modeling, we propose a model-free yet computationally simple\napproach to estimate the partially linear model $Y=X\\beta+g(Z)+\\varepsilon$.\nMotivated by the partial consistency phenomena, we propose to model $g(Z)$ via\nincidental parameters. Based on partitioning the support of $Z$, a simple local\naverage is used to estimate the response surface. The proposed method seeks to\nstrike a balance between computation burden and efficiency of the estimators\nwhile minimizing model bias. Computationally this approach only involves least\nsquares. We show that given the inconsistent estimator of $g(Z)$, a root $n$\nconsistent estimator of parametric component $\\beta$ of the partially linear\nmodel can be obtained with little cost in efficiency. Moreover, conditional on\nthe $\\beta$ estimates, an optimal estimator of $g(Z)$ can then be obtained\nusing classic nonparametric methods. The statistical inference problem\nregarding $\\beta$ and a two-population nonparametric testing problem regarding\n$g(Z)$ are considered. Our results show that the behavior of test statistics\nare satisfactory. To assess the performance of our method in comparison with\nother methods, three simulation studies are conducted and a real dataset about\nrisk factors of birth weights is analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 21:05:41 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Cui", "Xia", ""], ["Lu", "Ying", ""], ["Peng", "Heng", ""]]}, {"id": "1401.2377", "submitter": "Christophe Ley", "authors": "Rainer Dyckerhoff, Christophe Ley and Davy Paindaveine", "title": "Depth-based Runs Tests for Bivariate Central Symmetry", "comments": "33 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  McWilliams (1990) introduced a nonparametric procedure based on runs for the\nproblem of testing univariate symmetry about the origin (equivalently, about an\narbitrary specified center). His procedure first reorders the observations\naccording to their absolute values, then rejects the null when the number of\nruns in the resulting series of signs is too small. This test is universally\nconsistent and enjoys nice robustness properties, but is unfortunately limited\nto the univariate setup. In this paper, we extend McWilliams' procedure into\ntests of bivariate central symmetry. The proposed tests first reorder the\nobservations according to their statistical depth in a symmetrized version of\nthe sample, then reject the null when an original concept of simplicial runs is\ntoo small. Our tests are affine-invariant and have good robustness properties.\nIn particular, they do not require any finite moment assumption. We derive\ntheir limiting null distribution, which establishes their asymptotic\ndistribution-freeness. We study their finite-sample properties through Monte\nCarlo experiments, and conclude with some final comments.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 15:44:41 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Dyckerhoff", "Rainer", ""], ["Ley", "Christophe", ""], ["Paindaveine", "Davy", ""]]}, {"id": "1401.2480", "submitter": "Steven Roberts", "authors": "Bala Rajaratnam and Steven Roberts and Doug Sparks and Onkar Dalal", "title": "Lasso Regression: Estimation and Shrinkage via Limit of Gibbs Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of the lasso is espoused in high-dimensional settings where\nonly a small number of the regression coefficients are believed to be nonzero.\nMoreover, statistical properties of high-dimensional lasso estimators are often\nproved under the assumption that the correlation between the predictors is\nbounded. In this vein, coordinatewise methods, the most common means of\ncomputing the lasso solution, work well in the presence of low to moderate\nmulticollinearity. The computational speed of coordinatewise algorithms\ndegrades however as sparsity decreases and multicollinearity increases.\nMotivated by these limitations, we propose the novel \"Deterministic Bayesian\nLasso\" algorithm for computing the lasso solution. This algorithm is developed\nby considering a limiting version of the Bayesian lasso. The performance of the\nDeterministic Bayesian Lasso improves as sparsity decreases and\nmulticollinearity increases, and can offer substantial increases in\ncomputational speed. A rigorous theoretical analysis demonstrates that (1) the\nDeterministic Bayesian Lasso algorithm converges to the lasso solution, and (2)\nit leads to a representation of the lasso estimator which shows how it achieves\nboth $\\ell_1$ and $\\ell_2$ types of shrinkage simultaneously. Connections to\nother algorithms are also provided. The benefits of the Deterministic Bayesian\nLasso algorithm are then illustrated on simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 23:16:41 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2014 11:30:55 GMT"}, {"version": "v3", "created": "Fri, 7 Nov 2014 10:45:03 GMT"}, {"version": "v4", "created": "Tue, 6 Jan 2015 02:10:00 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Rajaratnam", "Bala", ""], ["Roberts", "Steven", ""], ["Sparks", "Doug", ""], ["Dalal", "Onkar", ""]]}, {"id": "1401.2597", "submitter": "Linxi Liu", "authors": "Linxi Liu and Wing Hung Wong", "title": "Multivariate Density Estimation via Adaptive Partitioning (I): Sieve MLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a non-parametric approach to multivariate density estimation. The\nestimators are piecewise constant density functions supported by binary\npartitions. The partition of the sample space is learned by maximizing the\nlikelihood of the corresponding histogram on that partition. We analyze the\nconvergence rate of the sieve maximum likelihood estimator, and reach a\nconclusion that for a relatively rich class of density functions the rate does\nnot directly depend on the dimension. This suggests that, under certain\nconditions, this method is immune to the curse of dimensionality, in the sense\nthat it is possible to get close to the parametric rate even in high\ndimensions. We also apply this method to several special cases, and calculate\nthe explicit convergence rates respectively.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 07:45:51 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 21:53:55 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Liu", "Linxi", ""], ["Wong", "Wing Hung", ""]]}, {"id": "1401.2678", "submitter": "Arend Voorman", "authors": "Arend Voorman, Ali Shojaie, Daniela Witten", "title": "Inference in High Dimensions with the Penalized Score Test", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been considerable theoretical development\nregarding variable selection consistency of penalized regression techniques,\nsuch as the lasso. However, there has been relatively little work on\nquantifying the uncertainty in these selection procedures. In this paper, we\npropose a new method for inference in high dimensions using a score test based\non penalized regression. In this test, we perform penalized regression of an\noutcome on all but a single feature, and test for correlation of the residuals\nwith the held-out feature. This procedure is applied to each feature in turn.\nInterestingly, when an $\\ell_1$ penalty is used, the sparsity pattern of the\nlasso corresponds exactly to a decision based on the proposed test. Further,\nwhen an $\\ell_2$ penalty is used, the test corresponds precisely to a score\ntest in a mixed effects model, in which the effects of all but one feature are\nassumed to be random. We formulate the hypothesis being tested as a compromise\nbetween the null hypotheses tested in simple linear regression on each feature\nand in multiple linear regression on all features, and develop reference\ndistributions for some well-known penalties. We also examine the behavior of\nthe test on real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 22:28:04 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 19:24:35 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 21:47:32 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Voorman", "Arend", ""], ["Shojaie", "Ali", ""], ["Witten", "Daniela", ""]]}, {"id": "1401.3084", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Gayan Dharmarathne", "title": "A comparison of Bayesian and frequentist interval estimators in\n  regression that utilize uncertain prior information", "comments": null, "journal-ref": "A comparison of Bayesian and frequentist interval estimators in\n  regression that utilize uncertain prior information. Australian & New Zealand\n  Journal of Statistics, 57, 99-118 (2015)", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a linear regression model with regression parameter beta and\nnormally distributed errors. Suppose that the parameter of interest is theta =\na^T beta where a is a specified vector. Define the parameter tau = c^T beta - t\nwhere c and t are specified and a and c are linearly independent. Also suppose\nthat we have uncertain prior information that tau = 0. Kabaila and Giri, 2009,\nJSPI, describe a new frequentist 1-alpha confidence interval for theta that\nutilizes this uncertain prior information. We compare this confidence interval\nwith Bayesian 1-alpha equi-tailed and shortest credible intervals for theta\nthat result from a prior density for tau that is a mixture of a rectangular\n\"slab\" and a Dirac delta function \"spike\", combined with noninformative prior\ndensities for the other parameters of the model. We show that these frequentist\nand Bayesian interval estimators depend on the data in very different ways. We\nalso consider some close variants of this prior distribution that lead to\nBayesian and frequentist interval estimators with greater similarity.\nNonetheless, as we show, substantial differences between these interval\nestimators remain.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 06:47:59 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Dharmarathne", "Gayan", ""]]}, {"id": "1401.3229", "submitter": "Ngoc Mai Tran", "authors": "Ngoc Mai Tran, Maria Osipenko, and Wolfgang Karl Haerdle", "title": "Principal Component Analysis in an Asymmetric Norm", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a widely used dimension reduction tool\nin the analysis of many kind of high-dimensional data. It is used in signal\nprocessing, mechanical engineering, psychometrics, and other fields under\ndifferent names. It still bears the same mathematical idea: the decomposition\nof variation of a high dimensional object into uncorrelated factors or\ncomponents. However, in many of the above applications, one is interested in\ncapturing the tail variables of the data rather than variation around the mean.\nSuch applications include weather related event curves, expected shortfalls,\nand speeding analysis among others. These are all high dimensional tail objects\nwhich one would like to study in a PCA fashion. The tail character though\nrequires to do the dimension reduction in an asymmetric norm rather than the\nclassical $L_2$-type orthogonal projection. We develop an analogue of PCA in an\nasymmetric norm. These norms cover both quantiles and expectiles, another tail\nevent measure. The difficulty is that there is no natural basis, no `principal\ncomponents', to the $k$-dimensional subspace found. We propose two definitions\nof principal components and provide algorithms based on iterative least\nsquares. We prove upper bounds on their convergence times, and compare their\nperformances in a simulation study. We apply the algorithms to a Chinese\nweather dataset with a view to weather derivative pricing\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 15:45:45 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Tran", "Ngoc Mai", ""], ["Osipenko", "Maria", ""], ["Haerdle", "Wolfgang Karl", ""]]}, {"id": "1401.3290", "submitter": "Brett T. McClintock", "authors": "Brett T. McClintock, Larissa L. Bailey, Brian P. Dreher, William A.\n  Link", "title": "Probit models for capture-recapture data subject to imperfect detection,\n  individual heterogeneity and misidentification", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS783 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2461-2484", "doi": "10.1214/14-AOAS783", "report-no": "IMS-AOAS-AOAS783", "categories": "q-bio.QM q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As noninvasive sampling techniques for animal populations have become more\npopular, there has been increasing interest in the development of\ncapture-recapture models that can accommodate both imperfect detection and\nmisidentification of individuals (e.g., due to genotyping error). However,\ncurrent methods do not allow for individual variation in parameters, such as\ndetection or survival probability. Here we develop misidentification models for\ncapture-recapture data that can simultaneously account for temporal variation,\nbehavioral effects and individual heterogeneity in parameters. To facilitate\nBayesian inference using our approach, we extend standard probit regression\ntechniques to latent multinomial models where the dimension and zeros of the\nresponse cannot be observed. We also present a novel Metropolis-Hastings within\nGibbs algorithm for fitting these models using Markov chain Monte Carlo. Using\nclosed population abundance models for illustration, we re-visit a DNA\ncapture-recapture population study of black bears in Michigan, USA and find\nevidence of misidentification due to genotyping error, as well as temporal,\nbehavioral and individual variation in detection probability. We also estimate\na salamander population of known size from laboratory experiments evaluating\nthe effectiveness of a marking technique commonly used for amphibians and fish.\nOur model was able to reliably estimate the size of this population and\nprovided evidence of individual heterogeneity in misidentification probability\nthat is attributable to variable mark quality. Our approach is more\ncomputationally demanding than previously proposed methods, but it provides the\nflexibility necessary for a much broader suite of models to be explored while\nproperly accounting for uncertainty introduced by misidentification and\nimperfect detection. In the absence of misidentification, our probit\nformulation also provides a convenient and efficient Gibbs sampler for Bayesian\nanalysis of traditional closed population capture-recapture data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 19:14:31 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 18:44:53 GMT"}, {"version": "v3", "created": "Thu, 2 Oct 2014 20:59:51 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 12:47:54 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["McClintock", "Brett T.", ""], ["Bailey", "Larissa L.", ""], ["Dreher", "Brian P.", ""], ["Link", "William A.", ""]]}, {"id": "1401.3362", "submitter": "James Long", "authors": "James P. Long, Noureddine El Karoui, John A. Rice", "title": "Kernel Density Estimation with Berkson Error", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sample $\\{X_i\\}_{i=1}^n$ from $f_X$, we construct kernel density\nestimators for $f_Y$, the convolution of $f_X$ with a known error density\n$f_{\\epsilon}$. This problem is known as density estimation with Berkson error\nand has applications in epidemiology and astronomy. Little is understood about\nbandwidth selection for Berkson density estimation. We compare three approaches\nto selecting the bandwidth both asymptotically, using large sample\napproximations to the MISE, and at finite samples, using simulations. Our\nresults highlight the relationship between the structure of the error\n$f_{\\epsilon}$ and the optimal bandwidth. In particular, the results\ndemonstrate the importance of smoothing when the error term $f_{\\epsilon}$ is\nconcentrated near 0. We propose a data--driven bandwidth estimator and test its\nperformance on NO$_2$ exposure data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 21:29:17 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 16:19:42 GMT"}, {"version": "v3", "created": "Mon, 28 Apr 2014 23:21:20 GMT"}, {"version": "v4", "created": "Tue, 29 Jul 2014 15:35:44 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Long", "James P.", ""], ["Karoui", "Noureddine El", ""], ["Rice", "John A.", ""]]}, {"id": "1401.3383", "submitter": "Frederico Caeiro", "authors": "Frederico Caeiro, Dora Prata Gomes", "title": "A Log Probability Weighted Moment Estimator of Extreme Quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the semi-parametric estimation of extreme quantiles\nof a right heavy-tail model. We propose a new Log Probability Weighted Moment\nestimator for extreme quantiles, which is obtained from the estimators of the\nshape and scale parameters of the tail. Under a second-order regular variation\ncondition on the tail, of the underlying distribution function, we deduce the\nnon degenerate asymptotic behaviour of the estimators under study and present\nan asymptotic comparison at their optimal levels. In addition, the performance\nof the estimators is illustrated through an application to real data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 23:14:00 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Caeiro", "Frederico", ""], ["Gomes", "Dora Prata", ""]]}, {"id": "1401.3760", "submitter": "Xiao Yang", "authors": "Xiao Yang and Andrew R. Barron", "title": "Large Alphabet Compression and Predictive Distributions through\n  Poissonization and Tilting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a convenient strategy for coding and predicting\nsequences of independent, identically distributed random variables generated\nfrom a large alphabet of size $m$. In particular, the size of the sample is\nallowed to be variable. The employment of a Poisson model and tilting method\nsimplifies the implementation and analysis through independence. The resulting\nstrategy is optimal within the class of distributions satisfying a moment\ncondition, and is close to optimal for the class of all i.i.d distributions on\nstrings of a given length. Moreover, the method can be used to code and predict\nstrings with a condition on the tail of the ordered counts. It can also be\napplied to distributions in an envelope class.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 21:18:56 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Yang", "Xiao", ""], ["Barron", "Andrew R.", ""]]}, {"id": "1401.3813", "submitter": "Heather Patsolic", "authors": "Heather Patsolic, Sancar Adali, Joshua T. Vogelstein, Youngser Park,\n  Carey E. Friebe, Gongkai Li, Vince Lyzinski", "title": "Seeded Graph Matching Via Joint Optimization of Fidelity and\n  Commensurability", "comments": "26 pages, 7 figures. Updated content and added application of\n  simultaneous matching for several time-steps for zebrafish connectomes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approximate graph matching algorithm that incorporates\nseeded data into the graph matching paradigm. Our Joint Optimization of\nFidelity and Commensurability (JOFC) algorithm embeds two graphs into a common\nEuclidean space where the matching inference task can be performed. Through\nreal and simulated data examples, we demonstrate the versatility of our\nalgorithm in matching graphs with various characteristics--weightedness,\ndirectedness, loopiness, many-to-one and many-to-many matchings, and soft\nseedings.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 02:33:44 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 20:01:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Patsolic", "Heather", ""], ["Adali", "Sancar", ""], ["Vogelstein", "Joshua T.", ""], ["Park", "Youngser", ""], ["Friebe", "Carey E.", ""], ["Li", "Gongkai", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1401.3889", "submitter": "Rob Tibshirani", "authors": "Ryan J. Tibshirani, Jonathan Taylor, Richard Lockhart, and Robert\n  Tibshirani", "title": "Exact Post-Selection Inference for Sequential Regression Procedures", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new inference tools for forward stepwise regression, least angle\nregression, and the lasso. Assuming a Gaussian model for the observation vector\ny, we first describe a general scheme to perform valid inference after any\nselection event that can be characterized as y falling into a polyhedral set.\nThis framework allows us to derive conditional (post-selection) hypothesis\ntests at any step of forward stepwise or least angle regression, or any step\nalong the lasso regularization path, because, as it turns out, selection events\nfor these procedures can be expressed as polyhedral constraints on y. The\np-values associated with these tests are exactly uniform under the null\ndistribution, in finite samples, yielding exact type I error control. The tests\ncan also be inverted to produce confidence intervals for appropriate underlying\nregression parameters. The R package \"selectiveInference\", freely available on\nthe CRAN repository, implements the new inference tools described in this\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:15:57 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2014 22:40:33 GMT"}, {"version": "v3", "created": "Wed, 1 Oct 2014 18:09:56 GMT"}, {"version": "v4", "created": "Fri, 31 Oct 2014 11:40:39 GMT"}, {"version": "v5", "created": "Sun, 11 Oct 2015 17:02:23 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Tibshirani", "Ryan J.", ""], ["Taylor", "Jonathan", ""], ["Lockhart", "Richard", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1401.3988", "submitter": "Lotfi Chaari", "authors": "Lotfi Chaari, Jean-Yves Tourneret, Caroline Chaux and Hadj Batatia", "title": "A Hamiltonian Monte Carlo Method for Non-Smooth Energy Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient sampling from high-dimensional distributions is a challenging issue\nwhich is encountered in many large data recovery problems involving Markov\nchain Monte Carlo schemes. In this context, sampling using Hamiltonian dynamics\nis one of the recent techniques that have been proposed to exploit the target\ndistribution geometry. Such schemes have clearly been shown to be efficient for\nmulti-dimensional sampling, but are rather adapted to the exponential families\nof distributions with smooth energy function. In this paper, we address the\nproblem of using Hamiltonian dynamics to sample from probability distributions\nhaving non-differentiable energy functions such as $\\ell_1$. Such distributions\nare being more and more used in sparse signal and image recovery applications.\nThe proposed technique uses a modified leapfrog transform involving a proximal\nstep. The resulting non-smooth Hamiltonian Monte Carlo (ns-HMC) method is\ntested and validated on a number of experiments. Results show its ability to\naccurately sample according to various multivariate target distributions. The\nproposed technique is illustrated on synthetic examples and is applied to an\nimage denoising problem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 11:15:14 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 14:08:12 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Chaari", "Lotfi", ""], ["Tourneret", "Jean-Yves", ""], ["Chaux", "Caroline", ""], ["Batatia", "Hadj", ""]]}, {"id": "1401.4082", "submitter": "Shakir Mohamed", "authors": "Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative\n  Models", "comments": "Appears In Proceedings of the 31st International Conference on\n  Machine Learning (ICML), JMLR: W\\&CP volume 32, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We marry ideas from deep neural networks and approximate Bayesian inference\nto derive a generalised class of deep, directed generative models, endowed with\na new algorithm for scalable inference and learning. Our algorithm introduces a\nrecognition model to represent approximate posterior distributions, and that\nacts as a stochastic encoder of the data. We develop stochastic\nback-propagation -- rules for back-propagation through stochastic variables --\nand use this to develop an algorithm that allows for joint optimisation of the\nparameters of both the generative and recognition model. We demonstrate on\nseveral real-world data sets that the model generates realistic samples,\nprovides accurate imputations of missing data and is a useful tool for\nhigh-dimensional data visualisation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 16:33:23 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 12:53:17 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 10:00:36 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Wierstra", "Daan", ""]]}, {"id": "1401.4425", "submitter": "Qing Zhou", "authors": "Qing Zhou", "title": "Monte Carlo Simulation for Lasso-Type Problems by Estimator Augmentation", "comments": "43 pages, 4 figures", "journal-ref": "Journal of the American Statistical Association, 109: 1495-1516\n  (2014)", "doi": "10.1080/01621459.2014.946035", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized linear regression under the $\\ell_1$ penalty, such as the Lasso,\nhas been shown to be effective in variable selection and sparse modeling. The\nsampling distribution of an $\\ell_1$-penalized estimator $\\hat{\\beta}$ is hard\nto determine as the estimator is defined by an optimization problem that in\ngeneral can only be solved numerically and many of its components may be\nexactly zero. Let $S$ be the subgradient of the $\\ell_1$ norm of the\ncoefficient vector $\\beta$ evaluated at $\\hat{\\beta}$. We find that the joint\nsampling distribution of $\\hat{\\beta}$ and $S$, together called an augmented\nestimator, is much more tractable and has a closed-form density under a normal\nerror distribution in both low-dimensional ($p\\leq n$) and high-dimensional\n($p>n$) settings. Given $\\beta$ and the error variance $\\sigma^2$, one may\nemploy standard Monte Carlo methods, such as Markov chain Monte Carlo and\nimportance sampling, to draw samples from the distribution of the augmented\nestimator and calculate expectations with respect to the sampling distribution\nof $\\hat{\\beta}$. We develop a few concrete Monte Carlo algorithms and\ndemonstrate with numerical examples that our approach may offer huge advantages\nand great flexibility in studying sampling distributions in $\\ell_1$-penalized\nlinear regression. We also establish nonasymptotic bounds on the difference\nbetween the true sampling distribution of $\\hat{\\beta}$ and its estimator\nobtained by plugging in estimated parameters, which justifies the validity of\nMonte Carlo simulation from an estimated sampling distribution even when $p\\gg\nn\\to \\infty$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 18:16:49 GMT"}, {"version": "v2", "created": "Sun, 13 Jul 2014 23:13:50 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Zhou", "Qing", ""]]}, {"id": "1401.4718", "submitter": "Simon Lunagomez", "authors": "Simon Lunagomez and Edoardo Airoldi", "title": "Bayesian Inference from Non-Ignorable Network Sampling Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a population of individuals and a network that encodes social\nconnections among them. We are interested in making inference on finite\npopulation and super-population estimands that are a function of both\nindividuals' responses and of the network, from a sample. Neither the sampling\nframe nor the network are available. However, the sampling mechanism implicitly\nleverages the network to recruit individuals, thus partially revealing social\ninteractions among the individuals in the sample, as well as their responses.\nThis is a common setting that arises, for instance, in epidemiology and\nhealthcare, where samples from hard-to-reach populations are collected using\nlink-tracing mechanisms, including respondent-driven sampling. In this paper,\nwe study statistical properties of popular network sampling mechanisms. We\nformulate the estimation problem in terms of Rubin's inferential framework to\nexplicitly account for social network structure. We then identify key modeling\nelements that lead to inferences with good frequentist properties when dealing\nwith data collected through non-ignorable network sampling mechanisms. We\ndemonstrate these methods on a study of the incidence of HIV in Brazil\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2014 18:55:30 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 00:52:01 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Lunagomez", "Simon", ""], ["Airoldi", "Edoardo", ""]]}, {"id": "1401.4764", "submitter": "Can Yang Dr", "authors": "Dongjun Chung, Can Yang, Cong Li, Joel Gelernter and Hongyu Zhao", "title": "GPA: A statistical approach to prioritizing GWAS results by integrating\n  pleiotropy information and annotation data", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWAS) suggests that a complex disease is\ntypically affected by many genetic variants with small or moderate effects.\nIdentification of these risk variants remains to be a very challenging problem.\nTraditional approaches focusing on a single GWAS dataset alone ignore relevant\ninformation that could potentially improve our ability to detect these\nvariants. We proposed a novel statistical approach, named GPA, to performing\nintegrative analysis of multiple GWAS datasets and functional annotations.\nHypothesis testing procedures were developed to facilitate statistical\ninference of pleiotropy and enrichment of functional annotation. We applied our\napproach to perform systematic analysis of five psychiatric disorders. Not only\ndid GPA identify many weak signals missed by the original single phenotype\nanalysis, but also revealed interesting genetic architectures of these\ndisorders. We also applied GPA to the bladder cancer GWAS data with the ENCODE\nDNase-seq data from 125 cell lines and showed that GPA can detect cell lines\nthat are more biologically relevant to the phenotype of interest.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 01:05:43 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Chung", "Dongjun", ""], ["Yang", "Can", ""], ["Li", "Cong", ""], ["Gelernter", "Joel", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1401.4896", "submitter": "Sonja Petrovic", "authors": "Elizabeth Gross, Sonja Petrovi\\'c, Despina Stasi", "title": "Goodness-of-fit for log-linear network models: Dynamic Markov bases\n  using hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks and other large sparse data sets pose significant challenges\nfor statistical inference, as many standard statistical methods for testing\nmodel fit are not applicable in such settings. Algebraic statistics offers a\ntheoretically justified approach to goodness-of-fit testing that relies on the\ntheory of Markov bases and is intimately connected with the geometry of the\nmodel as described by its fibers.\n  Most current practices require the computation of the entire basis, which is\ninfeasible in many practical settings. We present a dynamic approach to explore\nthe fiber of a model, which bypasses this issue, and is based on the\ncombinatorics of hypergraphs arising from the toric algebra structure of\nlog-linear models.\n  We demonstrate the approach on the Holland-Leinhardt $p_1$ model for random\ndirected graphs that allows for reciprocated edges.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 13:38:10 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Gross", "Elizabeth", ""], ["Petrovi\u0107", "Sonja", ""], ["Stasi", "Despina", ""]]}, {"id": "1401.4980", "submitter": "Alexander Shlemov J", "authors": "Alex Shlemov and Nina Golyandina", "title": "Shaped extensions of singular spectrum analysis", "comments": null, "journal-ref": "21st International Symposium on Mathematical Theory of Networks\n  and Systems, July 7-11, 2014. Groningen, The Netherlands. p.1813-1820", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensions of singular spectrum analysis (SSA) for processing of\nnon-rectangular images and time series with gaps are considered. A circular\nversion is suggested, which allows application of the method to the data given\non a circle or on a cylinder, e.g. cylindrical projection of a 3D ellipsoid.\nThe constructed Shaped SSA method with planar or circular topology is able to\nproduce low-rank approximations for images of complex shapes. Together with\nShaped SSA, a shaped version of the subspace-based ESPRIT method for frequency\nestimation is developed. Examples of 2D circular SSA and 2D Shaped ESPRIT are\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 16:44:26 GMT"}, {"version": "v2", "created": "Sun, 11 May 2014 23:21:10 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Shlemov", "Alex", ""], ["Golyandina", "Nina", ""]]}, {"id": "1401.5002", "submitter": "Xianyang Zhang", "authors": "Xianyang Zhang and Xiaofeng Shao", "title": "On the Coverage Bound Problem of Empirical Likelihood Methods For Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The upper bounds on the coverage probabilities of the confidence regions\nbased on blockwise empirical likelihood [Kitamura (1997)] and nonstandard\nexpansive empirical likelihood [Nordman et al. (2013)] methods for time series\ndata are investigated via studying the probability for the violation of the\nconvex hull constraint. The large sample bounds are derived on the basis of the\npivotal limit of the blockwise empirical log-likelihood ratio obtained under\nthe fixed-b asymptotics, which has been recently shown to provide a more\naccurate approximation to the finite sample distribution than the conventional\nchi-square approximation. Our theoretical and numerical findings suggest that\nboth the finite sample and large sample upper bounds for coverage probabilities\nare strictly less than one and the blockwise empirical likelihood confidence\nregion can exhibit serious undercoverage when (i) the dimension of moment\nconditions is moderate or large; (ii) the time series dependence is positively\nstrong; or (iii) the block size is large relative to sample size. A similar\nfinite sample coverage problem occurs for the nonstandard expansive empirical\nlikelihood. To alleviate the coverage bound problem, we propose to penalize\nboth empirical likelihood methods by relaxing the convex hull constraint.\nNumerical simulations and data illustration demonstrate the effectiveness of\nour proposed remedies in terms of delivering confidence sets with more accurate\ncoverage.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 18:00:50 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 17:13:35 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Zhang", "Xianyang", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1401.5015", "submitter": "Ngom Papa", "authors": "Papa Ngom, Badiassiatta Don Bosco Diatta", "title": "Model selection of stochastic simulation algorithm based on generalized\n  divergence measures", "comments": "23 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  MCMC methods (Monte Carlo Markov Chain) are a class of methods used to\nperform simulations per a probability distribution $P$. These methods are often\nused when we have difficulties to directly sample per a given probability\ndistribution $P$ . This distribution is then considered as a target and\ngenerates a Markov chain $(X_n)_{n\\in\\mathbb{N}}$ that, when $n$ is large we\nhave $X_n\\sim P$. These MCMC methods consist of several simulation strategies\nincluding the \\emph{Independent Sampler (IS)}, the \\emph{Random Walk of\nMetropolis Hastings \\small{(RWMH)}}, the \\emph{Gibbs sampler}, the\n\\emph{Adaptive Metropolis (AM)} and \\emph{Metropolis Within Gibbs (MWG)}\nstrategy. Each of these strategies can generate a Markov chain and is\nassociated with a convergence speed. It is interesting, with a given target\nlaw, to compare several simulation strategies for determining the best.\nChauveau and Vandekerkhove \\cite{Chauv2007} have compared IS and RWMH\nstrategies using the Kullback-Leibler divergence measure. In our article we\nwill compare our five simulation methods already mentioned using generalized\ndivergence measures. These divergence measures are taken in family of\n$\\alpha$-divergence measures \\cite{Cichocki2010}, with a parameter $\\alpha$.\nThis is the R\\'enyi divergence, Tsallis divergence and $D_\\alpha$ divergence .\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 18:59:01 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Ngom", "Papa", ""], ["Diatta", "Badiassiatta Don Bosco", ""]]}, {"id": "1401.5031", "submitter": "Joseph Ramsey", "authors": "Joseph D. Ramsey", "title": "A Scalable Conditional Independence Test for Nonlinear, Non-Gaussian\n  Data", "comments": "4 Figures, 2 Boxes, 1 Table, 15 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many relations of scientific interest are nonlinear, and even in linear\nsystems distributions are often non-Gaussian, for example in fMRI BOLD data. A\nclass of search procedures for causal relations in high dimensional data relies\non sample derived conditional independence decisions. The most common\napplications rely on Gaussian tests that can be systematically erroneous in\nnonlinear non-Gaussian cases. Recent work (Gretton et al. (2009), Tillman et\nal. (2009), Zhang et al. (2011)) has proposed conditional independence tests\nusing Reproducing Kernel Hilbert Spaces (RKHS). Among these, perhaps the most\nefficient has been KCI (Kernel Conditional Independence, Zhang et al. (2011)),\nwith computational requirements that grow effectively at least as O(N3),\nplacing it out of range of large sample size analysis, and restricting its\napplicability to high dimensional data sets. We propose a class of O(N2) tests\nusing conditional correlation independence (CCI) that require a few seconds on\na standard workstation for tests that require tens of minutes to hours for the\nKCI method, depending on degree of parallelization, with similar accuracy. For\naccuracy on difficult nonlinear, non-Gaussian data sets, we also compare a\nrecent test due to Harris & Drton (2012), applicable to nonlinear, non-Gaussian\ndistributions in the Gaussian copula, as well as to partial correlation, a\nlinear Gaussian test.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 19:54:27 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2014 16:05:12 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Ramsey", "Joseph D.", ""]]}, {"id": "1401.5264", "submitter": "Fentaw Abegaz", "authors": "Fentaw Abegaz and Ernst Wit", "title": "Penalized EM algorithm and copula skeptic graphical models for inferring\n  networks for mixed variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the problem of reconstructing networks for\ncontinuous, binary, count and discrete ordinal variables by estimating sparse\nprecision matrix in Gaussian copula graphical models. We propose two\napproaches: $\\ell_1$ penalized extended rank likelihood with Monte Carlo\nExpectation-Maximization algorithm (copula EM glasso) and copula skeptic with\npair-wise copula estimation for copula Gaussian graphical models. The proposed\napproaches help to infer networks arising from nonnormal and mixed variables.\nWe demonstrate the performance of our methods through simulation studies and\nanalysis of breast cancer genomic and clinical data and maize genetics data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 11:05:51 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Abegaz", "Fentaw", ""], ["Wit", "Ernst", ""]]}, {"id": "1401.5285", "submitter": "Ngom Papa", "authors": "Hamza Dhaker, Papa Ngom and Pierre Mendy", "title": "Comparaison between the two models : new approach using the\n  $\\alpha$-divergence", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose new nonparametric accordance R\\'enyi-$\\alpha$ and $\\alpha$-Tsallis\ndivergence estimators for continuous distributions. We discuss this approach\nwith a view to the selection model (on al\\'etoire and autoregressive AR (1)).\nWe lestimateur used by kernel density esttimer underlying. Nevertheless, we are\nable to prove that the estimators are consistent under certain conditions. We\nalso describe how to apply these estimators and demonstrate their effectiveness\nthrough numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 12:10:50 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Dhaker", "Hamza", ""], ["Ngom", "Papa", ""], ["Mendy", "Pierre", ""]]}, {"id": "1401.5343", "submitter": "Damien McParland", "authors": "Damien McParland, Isobel Claire Gormley, Tyler H. McCormick, Samuel J.\n  Clark, Chodziwadziwa Whiteson Kabudula, Mark A. Collinson", "title": "Clustering South African households based on their asset status using\n  latent variable models", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS726 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 747-776", "doi": "10.1214/14-AOAS726", "report-no": "IMS-AOAS-AOAS726", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Agincourt Health and Demographic Surveillance System has since 2001\nconducted a biannual household asset survey in order to quantify household\nsocio-economic status (SES) in a rural population living in northeast South\nAfrica. The survey contains binary, ordinal and nominal items. In the absence\nof income or expenditure data, the SES landscape in the study population is\nexplored and described by clustering the households into homogeneous groups\nbased on their asset status. A model-based approach to clustering the Agincourt\nhouseholds, based on latent variable models, is proposed. In the case of\nmodeling binary or ordinal items, item response theory models are employed. For\nnominal survey items, a factor analysis model, similar in nature to a\nmultinomial probit model, is used. Both model types have an underlying latent\nvariable structure - this similarity is exploited and the models are combined\nto produce a hybrid model capable of handling mixed data types. Further, a\nmixture of the hybrid models is considered to provide clustering capabilities\nwithin the context of mixed binary, ordinal and nominal response data. The\nproposed model is termed a mixture of factor analyzers for mixed data (MFA-MD).\nThe MFA-MD model is applied to the survey data to cluster the Agincourt\nhouseholds into homogeneous groups. The model is estimated within the Bayesian\nparadigm, using a Markov chain Monte Carlo algorithm. Intuitive groupings\nresult, providing insight to the different socio-economic strata within the\nAgincourt region.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 15:23:37 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 12:20:19 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["McParland", "Damien", ""], ["Gormley", "Isobel Claire", ""], ["McCormick", "Tyler H.", ""], ["Clark", "Samuel J.", ""], ["Kabudula", "Chodziwadziwa Whiteson", ""], ["Collinson", "Mark A.", ""]]}, {"id": "1401.5506", "submitter": "Joshua Goldstein", "authors": "Joshua Goldstein, Murali Haran, Ivan Simeonov, John Fricks and\n  Francesca Chiaromonte", "title": "An attraction-repulsion point process model for respiratory syncytial\n  virus infections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How is the progression of a virus influenced by properties intrinsic to\nindividual cells? We address this question by studying the susceptibility of\ncells infected with two strains of the human respiratory syncytial virus (RSV-A\nand RSV-B) in an in vitro experiment. Spatial patterns of infected cells give\nus insight into how local conditions influence susceptibility to the virus. We\nobserve a complicated attraction and repulsion behavior, a tendency for\ninfected cells to lump together or remain apart. We develop a new spatial point\nprocess model to describe this behavior. Inference on spatial point processes\nis difficult because the likelihood functions of these models contain\nintractable normalizing constants; we adapt an MCMC algorithm called double\nMetropolis-Hastings to overcome this computational challenge. Our methods are\ncomputationally efficient even for large point patterns consisting of over\n10,000 points. We illustrate the application of our model and inferential\napproach to simulated data examples and fit our model to various RSV\nexperiments. Because our model parameters are easy to interpret, we are able to\ndraw meaningful scientific conclusions from the fitted models.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 22:18:53 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 02:12:52 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Goldstein", "Joshua", ""], ["Haran", "Murali", ""], ["Simeonov", "Ivan", ""], ["Fricks", "John", ""], ["Chiaromonte", "Francesca", ""]]}, {"id": "1401.5649", "submitter": "Cedric Fevotte", "authors": "C\\'edric F\\'evotte and Nicolas Dobigeon", "title": "Nonlinear hyperspectral unmixing with robust nonnegative matrix\n  factorization", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2468177", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a robust mixing model to describe hyperspectral data\nresulting from the mixture of several pure spectral signatures. This new model\nnot only generalizes the commonly used linear mixing model, but also allows for\npossible nonlinear effects to be easily handled, relying on mild assumptions\nregarding these nonlinearities. The standard nonnegativity and sum-to-one\nconstraints inherent to spectral unmixing are coupled with a group-sparse\nconstraint imposed on the nonlinearity component. This results in a new form of\nrobust nonnegative matrix factorization. The data fidelity term is expressed as\na beta-divergence, a continuous family of dissimilarity measures that takes the\nsquared Euclidean distance and the generalized Kullback-Leibler divergence as\nspecial cases. The penalized objective is minimized with a block-coordinate\ndescent that involves majorization-minimization updates. Simulation results\nobtained on synthetic and real data show that the proposed strategy competes\nwith state-of-the-art linear and nonlinear unmixing methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 13:04:16 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 16:48:45 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["F\u00e9votte", "C\u00e9dric", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1401.5747", "submitter": "Vincent Audigier", "authors": "Vincent Audigier and Fran\\c{c}ois Husson and Julie Josse", "title": "Multiple imputation for continuous variables using a Bayesian principal\n  component analysis", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiple imputation method based on principal component analysis\n(PCA) to deal with incomplete continuous data. To reflect the uncertainty of\nthe parameters from one imputation to the next, we use a Bayesian treatment of\nthe PCA model. Using a simulation study and real data sets, the method is\ncompared to two classical approaches: multiple imputation based on joint\nmodelling and on fully conditional modelling. Contrary to the others, the\nproposed method can be easily used on data sets where the number of individuals\nis less than the number of variables and when the variables are highly\ncorrelated. In addition, it provides unbiased point estimates of quantities of\ninterest, such as an expectation, a regression coefficient or a correlation\ncoefficient, with a smaller mean squared error. Furthermore, the widths of the\nconfidence intervals built for the quantities of interest are often smaller\nwhilst ensuring a valid coverage.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 18:17:04 GMT"}, {"version": "v2", "created": "Fri, 2 May 2014 12:49:19 GMT"}, {"version": "v3", "created": "Tue, 29 Jul 2014 08:54:55 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2015 08:45:09 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Audigier", "Vincent", ""], ["Husson", "Fran\u00e7ois", ""], ["Josse", "Julie", ""]]}, {"id": "1401.5755", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, Anru Zhang, T. Tony Cai, Dylan S. Small", "title": "Instrumental Variables Estimation with Some Invalid Instruments and its\n  Application to Mendelian Randomization", "comments": "99 pages, 29 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables have been widely used for estimating the causal effect\nbetween exposure and outcome. Conventional estimation methods require complete\nknowledge about all the instruments' validity; a valid instrument must not have\na direct effect on the outcome and not be related to unmeasured confounders.\nOften, this is impractical as highlighted by Mendelian randomization studies\nwhere genetic markers are used as instruments and complete knowledge about\ninstruments' validity is equivalent to complete knowledge about the involved\ngenes' functions.\n  In this paper, we propose a method for estimation of causal effects when this\ncomplete knowledge is absent. It is shown that causal effects are identified\nand can be estimated as long as less than $50$% of instruments are invalid,\nwithout knowing which of the instruments are invalid. We also introduce\nconditions for identification when the 50% threshold is violated. A fast\npenalized $\\ell_1$ estimation method, called sisVIVE, is introduced for\nestimating the causal effect without knowing which instruments are valid, with\ntheoretical guarantees on its performance. The proposed method is demonstrated\non simulated data and a real Mendelian randomization study concerning the\neffect of body mass index on health-related quality of life index. An R package\n\\emph{sisVIVE} is available online.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 19:03:49 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 17:13:00 GMT"}, {"version": "v3", "created": "Sun, 21 Sep 2014 17:16:03 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Kang", "Hyunseung", ""], ["Zhang", "Anru", ""], ["Cai", "T. Tony", ""], ["Small", "Dylan S.", ""]]}, {"id": "1401.6371", "submitter": "Frederic Lavancier", "authors": "Fr\\'ed\\'eric Lavancier (LMJL, INRIA), Paul Rochet (LMJL)", "title": "A general procedure to combine estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general method to combine several estimators of the same quantity is\ninvestigated. In the spirit of model and forecast averaging, the final\nestimator is computed as a weighted average of the initial ones, where the\nweights are constrained to sum to one. In this framework, the optimal weights,\nminimizing the quadratic loss, are entirely determined by the mean square error\nmatrix of the vector of initial estimators. The averaging estimator is built\nusing an estimation of this matrix, which can be computed from the same\ndataset. A non-asymptotic error bound on the averaging estimator is derived,\nleading to asymptotic optimality under mild conditions on the estimated mean\nsquare error matrix. This method is illustrated on standard statistical\nproblems in parametric and semi-parametric models where the averaging estimator\noutperforms the initial estimators in most cases.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 15:16:20 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 17:15:00 GMT"}, {"version": "v3", "created": "Sun, 5 Oct 2014 07:21:05 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2015 12:39:06 GMT"}, {"version": "v5", "created": "Sun, 24 May 2015 12:41:14 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Lavancier", "Fr\u00e9d\u00e9ric", "", "LMJL, INRIA"], ["Rochet", "Paul", "", "LMJL"]]}, {"id": "1401.6433", "submitter": "Luca Tardella", "authors": "Danilo Alunni Fegatelli and Luca Tardella", "title": "Flexible behavioral capture-recapture modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop some new strategies for building and fitting new flexible classes\nof parametric capture-recapture models for closed populations which can be used\nto address a better understanding of behavioural patterns. We first rely on a\nconditional probability parameterization and review how to regard a large\nsubset of standard capture-recapture models as a suitable partitioning in\nequivalence classes of the full set of conditional probability parameters. We\nthen propose the use of new suitable quantifications of the conditioning binary\npartial capture histories as a device for enlarging the scope of flexible\nbehavioural models and also exploring the range of all possible partitions. We\nshow how one can easily find unconditional MLE of such models within a\ngeneralized linear model framework. We illustrate the potential of our approach\nwith the analysis of some known datasets and a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 19:40:06 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Fegatelli", "Danilo Alunni", ""], ["Tardella", "Luca", ""]]}, {"id": "1401.6504", "submitter": "Y. X. Rachel Wang", "authors": "Y. X. Rachel Wang, Keni Jiang, Lewis J. Feldman, Peter J. Bickel,\n  Haiyan Huang", "title": "Inferring gene-gene interactions and functional modules using sparse\n  canonical correlation analysis", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS792 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 300-323", "doi": "10.1214/14-AOAS792", "report-no": "IMS-AOAS-AOAS792", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks pervade many disciplines of science for analyzing complex systems\nwith interacting components. In particular, this concept is commonly used to\nmodel interactions between genes and identify closely associated genes forming\nfunctional modules. In this paper, we focus on gene group interactions and\ninfer these interactions using appropriate partial correlations between genes,\nthat is, the conditional dependencies between genes after removing the\ninfluences of a set of other functionally related genes. We introduce a new\nmethod for estimating group interactions using sparse canonical correlation\nanalysis (SCCA) coupled with repeated random partition and subsampling of the\ngene expression data set. By considering different subsets of genes and ways of\ngrouping them, our interaction measure can be viewed as an aggregated estimate\nof partial correlations of different orders. Our approach is unique in\nevaluating conditional dependencies when the correct dependent sets are unknown\nor only partially known. As a result, a gene network can be constructed using\nthe interaction measures as edge weights and gene functional groups can be\ninferred as tightly connected communities from the network. Comparisons with\nseveral popular approaches using simulated and real data show our procedure\nimproves both the statistical significance and biological interpretability of\nthe results. In addition to achieving considerably lower false positive rates,\nour procedure shows better performance in detecting important biological\npathways.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 08:07:21 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 07:35:16 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Wang", "Y. X. Rachel", ""], ["Jiang", "Keni", ""], ["Feldman", "Lewis J.", ""], ["Bickel", "Peter J.", ""], ["Huang", "Haiyan", ""]]}, {"id": "1401.6849", "submitter": "Kyle Vincent Ph. D", "authors": "Kyle Vincent and Saman Muthukumarana", "title": "A New Approach to Inference in Multi-Survey Studies with Unknown\n  Population Size", "comments": "Paper has been updated and changed to Rao-Blackwellization to give\n  Improved Estimates in Multi-List Studies [arXiv:1709.09138]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a Poisson sampling design in the presence of unknown selection\nprobabilities when applied to a population of unknown size for multiple\nsampling occasions. The fixed-population model is adopted and extended upon for\ninference. The complete minimal sufficient statistic is derived for the\nsampling model parameters and fixed-population parameter vector. The\nRao-Blackwell version of population quantity estimators is detailed. An\napplication is applied to an emprical population. The extended inferential\nframework is found to have much potential and utility for empirical studies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 13:50:51 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 12:10:39 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 14:45:09 GMT"}, {"version": "v4", "created": "Tue, 26 Sep 2017 16:45:27 GMT"}, {"version": "v5", "created": "Wed, 28 Nov 2018 19:29:00 GMT"}, {"version": "v6", "created": "Wed, 29 Jan 2020 14:29:42 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Vincent", "Kyle", ""], ["Muthukumarana", "Saman", ""]]}, {"id": "1401.7214", "submitter": "Michail Papathomas Dr", "authors": "Michail Papathomas and Sylvia Richardson", "title": "Exploring dependence between categorical variables: benefits and\n  limitations of using variable selection within Bayesian clustering in\n  relation to log-linear modelling with interaction terms", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript is concerned with relating two approaches that can be used to\nexplore complex dependence structures between categorical variables, namely\nBayesian partitioning of the covariate space incorporating a variable selection\nprocedure that highlights the covariates that drive the clustering, and\nlog-linear modelling with interaction terms. We derive theoretical results on\nthis relation and discuss if they can be employed to assist log-linear model\ndetermination, demonstrating advantages and limitations with simulated and real\ndata sets. The main advantage concerns sparse contingency tables. Inferences\nfrom clustering can potentially reduce the number of covariates considered and,\nsubsequently, the number of competing log-linear models, making the exploration\nof the model space feasible. Variable selection within clustering can inform on\nmarginal independence in general, thus allowing for a more efficient\nexploration of the log-linear model space. However, we show that the clustering\nstructure is not informative on the existence of interactions in a consistent\nmanner. This work is of interest to those who utilize log-linear models, as\nwell as practitioners such as epidemiologists that use clustering models to\nreduce the dimensionality in the data and to reveal interesting patterns on how\ncovariates combine.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 15:19:09 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 15:30:32 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 11:06:52 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Papathomas", "Michail", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1401.7241", "submitter": "Li Ma", "authors": "Li Ma", "title": "Markov adaptive P\\'olya trees and multi-resolution adaptive shrinkage in\n  nonparametric modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a hierarchical nonparametric model for probability measures\nbased on a multi-resolution transformation of probability distributions. The\nmodel allows a varying amount of shrinkage to be applied to data features of\ndifferent scales and/or at different locations in the sample space, and the\nvarying shrinkage level is locally adaptive to the empirical behavior of the\ndata. Moreover, the model's hierarchical design---through a latent Markov tree\nstructure---allows borrowing of information across locations and scales in\nsetting the adaptive shrinkage level. Inference under the model proceeds\nefficiently using general recipes for conjugate hierarchical models. We\nillustrate the work of the model in density estimation and evaluate its\nperformance through simulation under several schematic scenarios carefully\ndesigned to be representative of a variety of applications. We compare its\nperformance to those of several state-of-the-art nonparametric models---the\nP\\'olya tree, the optional P\\'olya tree, and the Dirichlet process mixture of\nnormals. In addition, we establish several important theoretical properties for\nthe model including absolute continuity, full nonparametricity, and posterior\nconsistency.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 16:07:32 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2015 21:34:00 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ma", "Li", ""]]}, {"id": "1401.7645", "submitter": "Noah Simon", "authors": "Noah Simon, Robert Tibshirani", "title": "Comment on \"Detecting Novel Associations In Large Data Sets\" by Reshef\n  Et Al, Science Dec 16, 2011", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposal of Reshef et al. (2011) is an interesting new approach for\ndiscovering non-linear dependencies among pairs of measurements in exploratory\ndata mining. However, it has a potentially serious drawback. The authors laud\nthe fact that MIC has no preference for some alternatives over others, but as\nthe authors know, there is no free lunch in Statistics: tests which strive to\nhave high power against all alternatives can have low power in many important\nsituations. To investigate this, we ran simulations to compare the power of MIC\nto that of standard Pearson correlation and distance correlation (dcor). We\nsimulated pairs of variables with different relationships (most of which were\nconsidered by the Reshef et. al.), but with varying levels of noise added. To\ndetermine proper cutoffs for testing the independence hypothesis, we simulated\nindependent data with the appropriate marginals. As one can see from the\nFigure, MIC has lower power than dcor, in every case except the somewhat\npathological high-frequency sine wave. MIC is sometimes less powerful than\nPearson correlation as well, the linear case being particularly worrisome.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 20:06:45 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Simon", "Noah", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1401.7686", "submitter": "Wenjiang Fu", "authors": "Martina Fu (1), David Todem (2), Wenjiang J. Fu (2), Shuangge Ma (3)\n  ((1) Stanford University, (2) Michigan State University, (3) Yale University)", "title": "A Millennium Bug Still Bites Public Health - An Illustration Using\n  Cancer Mortality", "comments": "38 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of cancer mortality rates and the comparison across\ncancer sites, populations or time periods is crucial to public health, as\nidentification of vulnerable groups who suffer the most from these diseases may\nlead to efficient cancer care and control with timely treatment. Because cancer\nmortality rate varies with age, comparisons require age-standardization using a\nreference population. The current method of using the Year 2000 Population\nStandard is standard practice, but serious concerns have been raised about its\nlack of justification. We have found that using the US Year 2000 Population\nStandard as reference overestimates prostate cancer mortality rates by 12-91%\nduring the period 1970-2009 across all six sampled U.S. states, and also\nunderestimates case fatality rates by 9-78% across six cancer sites, including\nfemale breast, cervix, prostate, lung, leukemia and colon-rectum. We develop a\nmean reference population method to minimize the bias using mathematical\noptimization theory and statistical modeling. The method corrects the bias to\nthe largest extent in terms of squared loss and can be applied broadly to\nstudies of many diseases.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 21:52:41 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Fu", "Martina", "", "Stanford University"], ["Todem", "David", "", "Michigan State University"], ["Fu", "Wenjiang J.", "", "Michigan State University"], ["Ma", "Shuangge", "", "Yale University"]]}, {"id": "1401.7953", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Training population selection for (breeding value) prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training population selection for genomic selection has captured a great deal\nof interest in animal and plant breeding. In this article, we derive a\ncomputationally efficient statistic to measure the reliability of estimates of\ngenetic breeding values for a fixed set of genotypes based on a given training\nset of genotypes and phenotypes. We adopt a genetic algorithm scheme to find a\ntraining set of certain size from a larger set of candidate genotypes that\noptimizes this reliability measure. Our results show that, compared to a random\nsample of the same size, phenotyping individuals selected by our method results\nin models with better accuracies. We implement the proposed training selection\nmethodology on four data sets, namely, the arabidopsis, wheat, rice and the\nmaize data sets. Our results indicate that dynamic model building process which\nuses genotypes of the individuals in the test sample into account while\nselecting the training individuals improves the performance of GS models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 19:03:30 GMT"}, {"version": "v2", "created": "Wed, 21 May 2014 19:07:14 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1401.8097", "submitter": "Fr\\'ed\\'eric Ferraty", "authors": "Fr\\'ed\\'eric Ferraty, Peter Hall", "title": "An Algorithm for Nonlinear, Nonparametric Model Choice and Prediction", "comments": "26 pages, 3 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm which, in the context of nonlinear regression on\nvector-valued explanatory variables, chooses those combinations of vector\ncomponents that provide best prediction. The algorithm devotes particular\nattention to components that might be of relatively little predictive value by\nthemselves, and so might be ignored by more conventional methodology for model\nchoice, but which, in combination with other difficult-to-find components, can\nbe particularly beneficial for prediction. Additionally the algorithm avoids\nchoosing vector components that become redundant once appropriate combinations\nof other, more relevant components are selected. It is suitable for very high\ndimensional problems, where it keeps computational labour in check by using a\nnovel sequential argument, and also for more conventional prediction problems,\nwhere dimension is relatively low. We explore properties of the algorithm using\nboth theoretical and numerical arguments.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 09:37:01 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Ferraty", "Fr\u00e9d\u00e9ric", ""], ["Hall", "Peter", ""]]}, {"id": "1401.8182", "submitter": "Sharon Lee", "authors": "Sharon X. Lee, Geoffrey J. McLachlan", "title": "Maximum Likelihood Estimation for Finite Mixtures of Canonical\n  Fundamental Skew t-Distributions: the Unification of the Unrestricted and\n  Restricted Skew t-Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for the fitting of a location-scale\nvariant of the canonical fundamental skew t (CFUST) distribution, a superclass\nof the restricted and unrestricted skew t-distributions. In recent years, a few\nversions of the multivariate skew $t$ (MST) model have been put forward,\ntogether with various EM-type algorithms for parameter estimation. These\nformulations adopted either a restricted or unrestricted characterization for\ntheir MST densities.\n  In this paper, we examine a natural generalization of these developments,\nemploying the CFUST distribution as the parametric family for the component\ndistributions, and point out that the restricted and unrestricted\ncharacterizations can be unified under this general formulation. We show that\nan exact implementation of the EM algorithm can be achieved for the CFUST\ndistribution and mixtures of this distribution, and present some new analytical\nresults for a conditional expectation involved in the E-step.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 14:51:36 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1401.8189", "submitter": "Bo Wang", "authors": "Bo Wang and Jian Qing Shi", "title": "Generalized Gaussian Process Regression Model for Non-Gaussian\n  Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a generalized Gaussian process concurrent regression\nmodel for functional data where the functional response variable has a\nbinomial, Poisson or other non-Gaussian distribution from an exponential family\nwhile the covariates are mixed functional and scalar variables. The proposed\nmodel offers a nonparametric generalized concurrent regression method for\nfunctional data with multi-dimensional covariates, and provides a natural\nframework on modeling common mean structure and covariance structure\nsimultaneously for repeatedly observed functional data. The mean structure\nprovides an overall information about the observations, while the covariance\nstructure can be used to catch up the characteristic of each individual batch.\nThe prior specification of covariance kernel enables us to accommodate a wide\nclass of nonlinear models. The definition of the model, the inference and the\nimplementation as well as its asymptotic properties are discussed. Several\nnumerical examples with different non-Gaussian response variables are\npresented. Some technical details and more numerical examples as well as an\nextension of the model are provided as supplementary materials.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 15:12:33 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Wang", "Bo", ""], ["Shi", "Jian Qing", ""]]}, {"id": "1401.8205", "submitter": "Florian Hartig", "authors": "Florian Hartig, Claudia Dislich, Thorsten Wiegand and Andreas Huth", "title": "Technical Note: Approximate Bayesian parameterization of a process-based\n  tropical forest model", "comments": "Corresponds, apart from layout, to the final version published in\n  Biogeosciences", "journal-ref": "Biogeosciences (2014) 11, 1261-1272", "doi": "10.5194/bg-11-1261-2014", "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse parameter estimation of process-based models is a long-standing\nproblem in many scientific disciplines. A key question for inverse parameter\nestimation is how to define the metric that quantifies how well model\npredictions fit to the data. This metric can be expressed by general cost or\nobjective functions, but statistical inversion methods require a particular\nmetric, the probability of observing the data given the model parameters, known\nas the likelihood.\n  For technical and computational reasons, likelihoods for process-based\nstochastic models are usually based on general assumptions about variability in\nthe observed data, and not on the stochasticity generated by the model. Only in\nrecent years have new methods become available that allow the generation of\nlikelihoods directly from stochastic simulations. Previous applications of\nthese approximate Bayesian methods have concentrated on relatively simple\nmodels. Here, we report on the application of a simulation-based likelihood\napproximation for FORMIND, a parameter-rich individual-based model of tropical\nforest dynamics.\n  We show that approximate Bayesian inference, based on a parametric likelihood\napproximation placed in a conventional Markov chain Monte Carlo (MCMC) sampler,\nperforms well in retrieving known parameter values from virtual inventory data\ngenerated by the forest model. We analyze the results of the parameter\nestimation, examine its sensitivity to the choice and aggregation of model\noutputs and observed data (summary statistics), and demonstrate the application\nof this method by fitting the FORMIND model to field data from an Ecuadorian\ntropical forest. Finally, we discuss how this approach differs from approximate\nBayesian computation (ABC), another method commonly used to generate\nsimulation-based likelihood approximations.\n  Our results demonstrate that simulation-based inference, [...]\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 16:02:33 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 15:35:45 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Hartig", "Florian", ""], ["Dislich", "Claudia", ""], ["Wiegand", "Thorsten", ""], ["Huth", "Andreas", ""]]}, {"id": "1401.8236", "submitter": "Michael Braun", "authors": "Michael Braun and Paul Damien", "title": "Scalable Rejection Sampling for Bayesian Hierarchical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical modeling is a popular approach to capturing unobserved\nheterogeneity across individual units. However, standard estimation methods\nsuch as Markov chain Monte Carlo (MCMC) can be impracticable for modeling\noutcomes from a large number of units. We develop a new method to sample from\nposterior distributions of Bayesian models, without using MCMC. Samples are\nindependent, so they can be collected in parallel, and we do not need to be\nconcerned with issues like chain convergence and autocorrelation. The algorithm\nis scalable under the weak assumption that individual units are conditionally\nindependent, making it applicable for large datasets. It can also be used to\ncompute marginal likelihoods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 17:35:09 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 21:36:14 GMT"}, {"version": "v3", "created": "Sun, 2 Nov 2014 01:00:14 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Braun", "Michael", ""], ["Damien", "Paul", ""]]}, {"id": "1401.8274", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela and Victor M. Panaretos", "title": "Empirical Bayes unfolding of elementary particle spectra at the Large\n  Hadron Collider", "comments": "40 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP hep-ex physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the so-called unfolding problem in experimental high energy\nphysics, where the goal is to estimate the true spectrum of elementary\nparticles given observations distorted by measurement error due to the limited\nresolution of a particle detector. This an important statistical inverse\nproblem arising in the analysis of data at the Large Hadron Collider at CERN.\nMathematically, the problem is formalized as one of estimating the intensity\nfunction of an indirectly observed Poisson point process. Particle physicists\nare particularly keen on unfolding methods that feature a principled way of\nchoosing the regularization strength and allow for the quantification of the\nuncertainty inherent in the solution. Though there are many approaches that\nhave been considered by experimental physicists, it can be argued that few --\nif any -- of these deal with these two key issues in a satisfactory manner. In\nthis paper, we propose to attack the unfolding problem within the framework of\nempirical Bayes estimation: we consider Bayes estimators of the coefficients of\na basis expansion of the unknown intensity, using a regularizing prior; and\nemploy a Monte Carlo expectation-maximization algorithm to find the marginal\nmaximum likelihood estimate of the hyperparameter controlling the strength of\nthe regularization. Due to the data-driven choice of the hyperparameter,\ncredible intervals derived using the empirical Bayes posterior lose their\nsubjective Bayesian interpretation. Since the properties and meaning of such\nintervals are poorly understood, we explore instead the use of bootstrap\nresampling for constructing purely frequentist confidence bands for the true\nintensity. The performance of the proposed methodology is demonstrated using\nboth simulations and real data from the Large Hadron Collider.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 20:01:23 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Kuusela", "Mikael", ""], ["Panaretos", "Victor M.", ""]]}]