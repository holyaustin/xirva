[{"id": "1003.0182", "submitter": "Richard D. Gill", "authors": "Richard D. Gill, Niels Keiding", "title": "Product-limit estimators of the gap time distribution of a renewal\n  process under different sampling patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of the gap time distribution in a simple renewal\nprocess may be considered a problem in survival analysis under particular\nsampling frames corresponding to how the renewal process is observed. This note\ndescribes several such situations where simple product limit estimators, though\ninefficient, may still be useful.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2010 14:44:05 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Gill", "Richard D.", ""], ["Keiding", "Niels", ""]]}, {"id": "1003.0188", "submitter": "Richard D. Gill", "authors": "Odd O. Aalen, Per Kragh Andersen, {\\O}rnulf Borgan, Richard D. Gill,\n  and Niels Keiding", "title": "History of applications of martingales in survival analysis", "comments": null, "journal-ref": "Electronic Journal for History of Probability and Statistics, Vol.\n  5, Nr. 1, June 2009 (www.jehps.net), 28 pp", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper traces the development of the use of martingale methods in survival\nanalysis from the mid 1970's to the early 1990's. This development was\ninitiated by Aalen's Berkeley PhD-thesis in 1975, progressed through the work\non estimation of Markov transition probabilities, non-parametric tests and\nCox's regression model in the late 1970's and early 1980's, and it was\nconsolidated in the early 1990's with the publication of the monographs by\nFleming and Harrington (1991) and Andersen, Borgan, Gill and Keiding (1993).\nThe development was made possible by an unusually fast technology transfer of\npure mathematical concepts, primarily from French probability, into practical\nbiostatistical methodology, and we attempt to outline some of the personal\nrelationships that helped this happen. We also point out that survival analysis\nwas ready for this development since the martingale ideas inherent in the deep\nunderstanding of temporal development so intrinsic to the French theory of\nprocesses were already quite close to the surface in survival analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2010 16:11:25 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Aalen", "Odd O.", ""], ["Andersen", "Per Kragh", ""], ["Borgan", "\u00d8rnulf", ""], ["Gill", "Richard D.", ""], ["Keiding", "Niels", ""]]}, {"id": "1003.0243", "submitter": "Bernard Silverman", "authors": "Graeme K. Ambler and Bernard W. Silverman", "title": "Perfect simulation using dominated coupling from the past with\n  application to area-interaction point processes and wavelet thresholding", "comments": "27 pages, 8 figures. Chapter 3 of \"Probability and Mathematical\n  Genetics: Papers in Honour of Sir John Kingman\" (Editors N.H. Bingham and\n  C.M. Goldie), Cambridge University Press, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider perfect simulation algorithms for locally stable point processes\nbased on dominated coupling from the past, and apply these methods in two\ndifferent contexts. A new version of the algorithm is developed which is\nfeasible for processes which are neither purely attractive nor purely\nrepulsive. Such processes include multiscale area-interaction processes, which\nare capable of modelling point patterns whose clustering structure varies\nacross scales. The other topic considered is nonparametric regression using\nwavelets, where we use a suitable area-interaction process on the discrete\nspace of indices of wavelet coefficients to model the notion that if one\nwavelet coefficient is non-zero then it is more likely that neighbouring\ncoefficients will be also. A method based on perfect simulation within this\nmodel shows promising results compared to the standard methods which threshold\ncoefficients independently.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 03:35:56 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Ambler", "Graeme K.", ""], ["Silverman", "Bernard W.", ""]]}, {"id": "1003.0275", "submitter": "Denis Belomestny", "authors": "Denis Belomestny", "title": "Statistical inference for time-changed L\\'{e}vy processes via composite\n  characteristic function estimation", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS901 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 4, 2205-2242", "doi": "10.1214/11-AOS901", "report-no": "IMS-AOS-AOS901", "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the problem of semi-parametric inference on the parameters\nof a multidimensional L\\'{e}vy process $L_t$ with independent components based\non the low-frequency observations of the corresponding time-changed L\\'{e}vy\nprocess $L_{\\mathcal{T}(t)}$, where $\\mathcal{T}$ is a nonnegative,\nnondecreasing real-valued process independent of $L_t$, is studied. We show\nthat this problem is closely related to the problem of composite function\nestimation that has recently gotten much attention in statistical literature.\nUnder suitable identifiability conditions, we propose a consistent estimate for\nthe L\\'{e}vy density of $L_t$ and derive the uniform as well as the pointwise\nconvergence rates of the estimate proposed. Moreover, we prove that the rates\nobtained are optimal in a minimax sense over suitable classes of time-changed\nL\\'{e}vy models. Finally, we present a simulation study showing the performance\nof our estimation algorithm in the case of time-changed Normal Inverse Gaussian\n(NIG) L\\'{e}vy processes.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 08:45:30 GMT"}, {"version": "v2", "created": "Wed, 29 Dec 2010 14:14:12 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2012 14:32:55 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Belomestny", "Denis", ""]]}, {"id": "1003.0315", "submitter": "Aurore Delaigle", "authors": "Aurore Delaigle and Peter Hall", "title": "Kernel methods and minimum contrast estimators for empirical\n  deconvolution", "comments": "To appear in: Bingham, N. H., and Goldie, C. M. (eds), Probability\n  and Mathematical Genetics: Papers in Honour of Sir John Kingman. London Math.\n  Soc. Lecture Note Ser. Cambridge: Cambridge Univ. Press, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey classical kernel methods for providing nonparametric solutions to\nproblems involving measurement error. In particular we outline kernel-based\nmethodology in this setting, and discuss its basic properties. Then we point to\nclose connections that exist between kernel methods and much newer approaches\nbased on minimum contrast techniques. The connections are through use of the\nsinc kernel for kernel-based inference. This `infinite order' kernel is not\noften used explicitly for kernel-based deconvolution, although it has received\nattention in more conventional problems where measurement error is not an\nissue. We show that in a comparison between kernel methods for density\ndeconvolution, and their counterparts based on minimum contrast, the two\napproaches give identical results on a grid which becomes increasingly fine as\nthe bandwidth decreases. In consequence, the main numerical differences between\nthese two techniques are arguably the result of different approaches to\nchoosing smoothing parameters.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 11:33:37 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Delaigle", "Aurore", ""], ["Hall", "Peter", ""]]}, {"id": "1003.0747", "submitter": "Pierre Neuvial", "authors": "Pierre Neuvial (LPMA, SG)", "title": "Asymptotic Results on Adaptive False Discovery Rate Controlling\n  Procedures Based on Kernel Estimators", "comments": null, "journal-ref": "Journal of Machine Learning Research 14 (2013) 1423-1459", "doi": null, "report-no": null, "categories": "math.ST physics.data-an q-bio.QM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The False Discovery Rate (FDR) is a commonly used type I error rate in\nmultiple testing problems. It is defined as the expected False Discovery\nProportion (FDP), that is, the expected fraction of false positives among\nrejected hypotheses. When the hypotheses are independent, the\nBenjamini-Hochberg procedure achieves FDR control at any pre-specified level.\nBy construction, FDR control offers no guarantee in terms of power, or type II\nerror. A number of alternative procedures have been developed, including\nplug-in procedures that aim at gaining power by incorporating an estimate of\nthe proportion of true null hypotheses. In this paper, we study the asymptotic\nbehavior of a class of plug-in procedures based on kernel estimators of the\ndensity of the $p$-values, as the number $m$ of tested hypotheses grows to\ninfinity. In a setting where the hypotheses tested are independent, we prove\nthat these procedures are asymptotically more powerful in two respects: (i) a\ntighter asymptotic FDR control for any target FDR level and (ii) a broader\nrange of target levels yielding positive asymptotic power. We also show that\nthis increased asymptotic power comes at the price of slower, non-parametric\nconvergence rates for the FDP. These rates are of the form $m^{-k/(2k+1)}$,\nwhere $k$ is determined by the regularity of the density of the $p$-value\ndistribution, or, equivalently, of the test statistics distribution. These\nresults are applied to one- and two-sided tests statistics for Gaussian and\nLaplace location models, and for the Student model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 08:17:28 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2013 08:47:22 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Neuvial", "Pierre", "", "LPMA, SG"]]}, {"id": "1003.0804", "submitter": "Pritam Ranjan", "authors": "Mark Franey, Pritam Ranjan and Hugh Chipman", "title": "Branch and Bound Algorithms for Maximizing Expected Improvement\n  Functions", "comments": "26 pages, 14 figures, preprint submitted to the Journal of\n  Statistical Planning and Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic computer simulations are often used as a replacement for\ncomplex physical experiments. Although less expensive than physical\nexperimentation, computer codes can still be time-consuming to run. An\neffective strategy for exploring the response surface of the deterministic\nsimulator is the use of an approximation to the computer code, such as a\nGaussian process (GP) model, coupled with a sequential sampling strategy for\nchoosing design points that can be used to build the GP model. The ultimate\ngoal of such studies is often the estimation of specific features of interest\nof the simulator output, such as the maximum, minimum, or a level set\n(contour). Before approximating such features with the GP model, sufficient\nruns of the computer simulator must be completed.\n  Sequential designs with an expected improvement (EI) function can yield good\nestimates of the features with a minimal number of runs. The challenge is that\nthe expected improvement function itself is often multimodal and difficult to\nmaximize. We develop branch and bound algorithms for efficiently maximizing the\nEI function in specific problems, including the simultaneous estimation of a\nminimum and a maximum, and in the estimation of a contour. These branch and\nbound algorithms outperform other optimization strategies such as genetic\nalgorithms, and over a number of sequential design steps can lead to\ndramatically superior accuracy in estimation of features of interest.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 13:49:48 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Franey", "Mark", ""], ["Ranjan", "Pritam", ""], ["Chipman", "Hugh", ""]]}, {"id": "1003.0848", "submitter": "Niels Richard Hansen", "authors": "Niels Richard Hansen", "title": "Penalized maximum likelihood estimation for generalized linear point\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized linear point process is specified in terms of an intensity that\ndepends upon a linear predictor process through a fixed non-linear function. We\npresent a framework where the linear predictor is parametrized by a Banach\nspace and give results on Gateaux differentiability of the log-likelihood. Of\nparticular interest is when the intensity is expressed in terms of a linear\nfilter parametrized by a Sobolev space. Using that the Sobolev spaces are\nreproducing kernel Hilbert spaces we derive results on the representation of\nthe penalized maximum likelihood estimator in a special case and the gradient\nof the negative log-likelihood in general. The latter is used to develop a\ndescent algorithm in the Sobolev space. We conclude the paper by extensions to\nmultivariate and additive model specifications. The methods are implemented in\nthe R-package ppstat.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 16:30:56 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 15:39:36 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Hansen", "Niels Richard", ""]]}, {"id": "1003.0996", "submitter": "Ada Lau", "authors": "Ada Lau, Patrick McSharry", "title": "Approaches for multi-step density forecasts with application to\n  aggregated wind power", "comments": "Corrected version includes updated equation (18). Published in at\n  http://dx.doi.org/10.1214/09-AOAS320 the Annals of Applied Statistics\n  (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 3, 1311-1341", "doi": "10.1214/09-AOAS320", "report-no": "IMS-AOAS-AOAS320", "categories": "stat.AP physics.ao-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of multi-step density forecasts for non-Gaussian data mostly\nrelies on Monte Carlo simulations which are computationally intensive. Using\naggregated wind power in Ireland, we study two approaches of multi-step density\nforecasts which can be obtained from simple iterations so that intensive\ncomputations are avoided. In the first approach, we apply a logistic\ntransformation to normalize the data approximately and describe the transformed\ndata using ARIMA--GARCH models so that multi-step forecasts can be iterated\neasily. In the second approach, we describe the forecast densities by truncated\nnormal distributions which are governed by two parameters, namely, the\nconditional mean and conditional variance. We apply exponential smoothing\nmethods to forecast the two parameters simultaneously. Since the underlying\nmodel of exponential smoothing is Gaussian, we are able to obtain multi-step\nforecasts of the parameters by simple iterations and thus generate forecast\ndensities as truncated normal distributions. We generate forecasts for wind\npower from 15 minutes to 24 hours ahead. Results show that the first approach\ngenerates superior forecasts and slightly outperforms the second approach under\nvarious proper scores. Nevertheless, the second approach is computationally\nmore efficient and gives more robust results under different lengths of\ntraining data. It also provides an attractive alternative approach since one is\nallowed to choose a particular parametric density for the forecasts, and is\nvaluable when there are no obvious transformations to normalize the data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 09:37:37 GMT"}, {"version": "v2", "created": "Fri, 12 Nov 2010 14:07:13 GMT"}, {"version": "v3", "created": "Mon, 6 Dec 2010 12:27:38 GMT"}, {"version": "v4", "created": "Mon, 10 Jan 2011 08:04:51 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Lau", "Ada", ""], ["McSharry", "Patrick", ""]]}, {"id": "1003.1002", "submitter": "Daniel Commenges", "authors": "Daniel Commenges", "title": "Extending The Range of Application of Permutation Tests: the Expected\n  Permutation p-value Approach", "comments": "15 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limitation of permutation tests is that they assume exchangeability. It\nis shown that in generalized linear models one can construct permutation tests\nfrom score statistics in particular cases. When under the null hypothesis the\nobservations are not exchangeable, a representation in terms of Cox-Snell\nresiduals allows to develop an approach based on an expected permutation\np-value (Eppv); this is applied to the logistic regression model. A small\nsimulation study and an illustration with real data are given.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 10:06:09 GMT"}], "update_date": "2010-03-05", "authors_parsed": [["Commenges", "Daniel", ""]]}, {"id": "1003.1315", "submitter": "Pritam Ranjan", "authors": "Pritam Ranjan, Ronald Haynes and Richard Karsten", "title": "A Computationally Stable Approach to Gaussian Process Interpolation of\n  Deterministic Computer Simulation Data", "comments": "26 pages, 12 figures", "journal-ref": "Technometrics, 2011, 53(4), 366-378", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many expensive deterministic computer simulators, the outputs do not have\nreplication error and the desired metamodel (or statistical emulator) is an\ninterpolator of the observed data. Realizations of Gaussian spatial processes\n(GP) are commonly used to model such simulator outputs. Fitting a GP model to\n$n$ data points requires the computation of the inverse and determinant of $n\n\\times n$ correlation matrices, $R$, that are sometimes computationally\nunstable due to near-singularity of $R$. This happens if any pair of design\npoints are very close together in the input space. The popular approach to\novercome near-singularity is to introduce a small nugget (or jitter) parameter\nin the model that is estimated along with other model parameters. The inclusion\nof a nugget in the model often causes unnecessary over-smoothing of the data.\nIn this paper, we propose a lower bound on the nugget that minimizes the\nover-smoothing and an iterative regularization approach to construct a\npredictor that further improves the interpolation accuracy. We also show that\nthe proposed predictor converges to the GP interpolator.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 18:03:27 GMT"}, {"version": "v2", "created": "Wed, 13 Oct 2010 12:29:42 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2012 10:02:45 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Ranjan", "Pritam", ""], ["Haynes", "Ronald", ""], ["Karsten", "Richard", ""]]}, {"id": "1003.1325", "submitter": "Mayra Ivanoff Lora", "authors": "Mayra Ivanoff Lora, Julio M Singer", "title": "Beta-binomial/gamma-Poisson regression models for repeated counts with\n  random parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta-binomial/Poisson models have been used by many authors to model\nmultivariate count data. Lora and Singer (Statistics in Medicine, 2008)\nextended such models to accommodate repeated multivariate count data with\noverdipersion in the binomial component. To overcome some of the limitations of\nthat model, we consider a beta-binomial/gamma-Poisson alternative that also\nallows for both overdispersion and different covariances between the Poisson\ncounts. We obtain maximum likelihood estimates for the parameters using a\nNewton-Raphson algorithm and compare both models in a practical example.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 18:39:47 GMT"}], "update_date": "2010-03-08", "authors_parsed": [["Lora", "Mayra Ivanoff", ""], ["Singer", "Julio M", ""]]}, {"id": "1003.1557", "submitter": "Jie Yang", "authors": "Abhyuday Mandal and Jie Yang and Dibyen Majumdar", "title": "Optimal Designs for Two-Level Factorial Experiments with Binary Response", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of obtaining locally D-optimal designs for factorial\nexperiments with qualitative factors at two levels each with binary response.\nOur focus is primarily on the 2^2 experiment. In this paper, we derive analytic\nresults for some special cases and indicate how to handle the general case. The\nperformance of the uniform design in examined and we show that this design is\nhighly efficient in general. For the general 2^k case we show that the uniform\ndesign has a maximin property.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2010 05:41:36 GMT"}, {"version": "v2", "created": "Wed, 12 May 2010 04:25:13 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Mandal", "Abhyuday", ""], ["Yang", "Jie", ""], ["Majumdar", "Dibyen", ""]]}, {"id": "1003.1727", "submitter": "Alexandre B. Simas", "authors": "Wagner Barreto-Souza and Alexandre B. Simas", "title": "The exp-$G$ family of probability distributions", "comments": "A much improved version of the pioneering manuscript \"A new family of\n  distributions based on the trucanted exponential distribution\" presented at\n  the 18 SINAPE (Simp\\'osio Nacional de Probabilidade e Estat\\'istica), 2008,\n  S\\~ao Paulo, Brazil.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new method to add a parameter to a family of\ndistributions. The additional parameter is completely studied and a full\ndescription of its behaviour in the distribution is given. We obtain several\nmathematical properties of the new class of distributions such as\nKullback-Leibler divergence, Shannon entropy, moments, order statistics,\nestimation of the parameters and inference for large sample. Further, we showed\nthat the new distribution have the reference distribution as special case, and\nthat the usual inference procedures also hold in this case. Furthermore, we\napplied our method to yield three-parameter extensions of the Weibull and beta\ndistributions. To motivate the use of our class of distributions, we present a\nsuccessful application to fatigue life data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2010 21:09:18 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Simas", "Alexandre B.", ""]]}, {"id": "1003.2253", "submitter": "Anton Westveld", "authors": "Anton H. Westveld and Peter D. Hoff", "title": "A Statistical View of Learning in the Centipede Game", "comments": null, "journal-ref": "Stat 2 (1), 242-254. 2013", "doi": "10.1002/sta4.32", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we evaluate the statistical evidence that a population of\nstudents learn about the sub-game perfect Nash equilibrium of the centipede\ngame via repeated play of the game. This is done by formulating a model in\nwhich a player's error in assessing the utility of decisions changes as they\ngain experience with the game. We first estimate parameters in a statistical\nmodel where the probabilities of choices of the players are given by a Quantal\nResponse Equilibrium (QRE) (McKelvey and Palfrey, 1995, 1996, 1998), but are\nallowed to change with repeated play. This model gives a better fit to the data\nthan similar models previously considered. However, substantial correlation of\noutcomes of games having a common player suggests that a statistical model that\ncaptures within-subject correlation is more appropriate. Thus we then estimate\nparameters in a model which allows for within-player correlation of decisions\nand rates of learning. Through out the paper we also consider and compare the\nuse of randomization tests and posterior predictive tests in the context of\nexploratory and confirmatory data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2010 04:36:05 GMT"}, {"version": "v2", "created": "Wed, 8 Sep 2010 01:52:56 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Westveld", "Anton H.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1003.2294", "submitter": "Jean-Baptiste Aubin", "authors": "Jean-Baptiste Aubin and Samuela Leoni-Aubin", "title": "A Simple Lack-of-Fit Test for Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple test is proposed for examining the correctness of a given completely\nspecified response function against unspecified general alternatives in the\ncontext of univariate regression. The usual diagnostic tools based on residuals\nplots are useful but heuristic. We introduce a formal statistical test\nsupplementing the graphical analysis. Technically, the test statistic is the\nmaximum length of the sequences of ordered (with respect to the covariate)\nobservations that are consecutively overestimated or underestimated by the\ncandidate regression function. Note that the testing procedure can cope with\nheteroscedastic errors and no replicates. Recursive formulae allowing to\ncalculate the exact distribution of the test statistic under the null\nhypothesis and under a class of alternative hypotheses are given.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2010 09:59:27 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2010 11:14:38 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Aubin", "Jean-Baptiste", ""], ["Leoni-Aubin", "Samuela", ""]]}, {"id": "1003.2390", "submitter": "Babak Shahbaba", "authors": "Babak Shahbaba", "title": "Bayesian Nonparametric Variable Selection as an Exploratory Tool for\n  Finding Genes that Matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput scientific studies involving no clear a'priori hypothesis are\ncommon. For example, a large-scale genomic study of a disease may examine\nthousands of genes without hypothesizing that any specific gene is responsible\nfor the disease. In these studies, the objective is to explore a large number\nof possible factors (e.g. genes) in order to identify a small number that will\nbe considered in follow-up studies that tend to be more thorough and on smaller\nscales. For large-scale studies, we propose a nonparametric Bayesian approach\nbased on random partition models. Our model thus divides the set of candidate\nfactors into several subgroups according to their degrees of relevance, or\npotential effect, in relation to the outcome of interest. The model allows for\na latent rank to be assigned to each factor according to the overall potential\nimportance of its corresponding group. The posterior expectation or mode of\nthese ranks is used to set up a threshold for selecting potentially relevant\nfactors. Using simulated data, we demonstrate that our approach could be quite\neffective in finding relevant genes compared to several alternative methods. We\napply our model to two large-scale studies. The first study involves\ntranscriptome analysis of infection by human cytomegalovirus (HCMV). The\nobjective of the second study is to identify differentially expressed genes\nbetween two types of leukemia.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2010 19:11:09 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 23:19:38 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2012 03:18:03 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Shahbaba", "Babak", ""]]}, {"id": "1003.2619", "submitter": "Andrew Gelman", "authors": "Andrew Gelman", "title": "Causality and Statistical Learning", "comments": "A version of this article will appear in the American Journal of\n  Sociology.", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some approaches and philosophies of causal inference coming from\nsociology, economics, computer science, cognitive science, and statistics\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 20:50:33 GMT"}], "update_date": "2010-04-02", "authors_parsed": [["Gelman", "Andrew", ""]]}, {"id": "1003.2823", "submitter": "Donald Percival", "authors": "Werner Stuetzle, Donald B. Percival and Caren Marzban", "title": "Targeted Event Detection", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of event detection based upon a (typically\nmultivariate) data stream characterizing some system. Most of the time the\nsystem is quiescent - nothing of interest is happening - but occasionally\nevents of interest occur. The goal of event detection is to raise an alarm as\nsoon as possible after the onset of an event. A simple way of addressing the\nevent detection problem is to look for changes in the data stream and equate\n`change' with `onset of event'. However, there might be many kinds of changes\nin the stream that are uninteresting. We assume that we are given a segment of\nthe stream where interesting events have been marked. We propose a method for\nusing these training data to construct a `targeted' detector that is\nspecifically sensitive to changes signaling the onset of interesting events.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2010 21:23:26 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["Stuetzle", "Werner", ""], ["Percival", "Donald B.", ""], ["Marzban", "Caren", ""]]}, {"id": "1003.2831", "submitter": "Rolf Turner", "authors": "Rolf Turner and Patrick Chareka", "title": "A note on the Berman condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is established that if a time series satisfies the Berman condition, and\nanother related (summability) condition, the result of filtering that series\nthrough a certain type of filter also satisfies the two conditions. In\nparticular it follows that if $X_t$ satisfies the two conditions and if $X_t$\nand $a_t$ are related by an invertible ARMA model, then the $a_t$ satisfy the\ntwo conditions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2010 23:12:56 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["Turner", "Rolf", ""], ["Chareka", "Patrick", ""]]}, {"id": "1003.3259", "submitter": "Nanny Wermuth", "authors": "Nanny Wermuth", "title": "Probability distributions with summary graph structure", "comments": "Published in at http://dx.doi.org/10.3150/10-BEJ309 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2011, Vol. 17, No. 3, 845-879", "doi": "10.3150/10-BEJ309", "report-no": "IMS-BEJ-BEJ309", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of independence statements may define the independence structure of\ninterest in a family of joint probability distributions. This structure is\noften captured by a graph that consists of nodes representing the random\nvariables and of edges that couple node pairs. One important class contains\nregression graphs. Regression graphs are a type of so-called chain graph and\ndescribe stepwise processes, in which at each step single or joint responses\nare generated given the relevant explanatory variables in their past. For joint\ndensities that result after possible marginalising or conditioning, we\nintroduce summary graphs. These graphs reflect the independence structure\nimplied by the generating process for the reduced set of variables and they\npreserve the implied independences after additional marginalising and\nconditioning. They can identify generating dependences that remain unchanged\nand alert to possibly severe distortions due to direct and indirect\nconfounding. Operators for matrix representations of graphs are used to derive\nthese properties of summary graphs and to translate them into special types of\npaths in graphs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2010 21:46:39 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2011 11:29:28 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Wermuth", "Nanny", ""]]}, {"id": "1003.3700", "submitter": "David J. Aldous", "authors": "David J. Aldous, Julian Shun", "title": "Connected Spatial Networks over Random Points and a Route-Length\n  Statistic", "comments": "Published in at http://dx.doi.org/10.1214/10-STS335 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 3, 275-288", "doi": "10.1214/10-STS335", "report-no": "IMS-STS-STS335", "categories": "math.PR cond-mat.dis-nn stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review mathematically tractable models for connected networks on random\npoints in the plane, emphasizing the class of proximity graphs which deserves\nto be better known to applied probabilists and statisticians. We introduce and\nmotivate a particular statistic $R$ measuring shortness of routes in a network.\nWe illustrate, via Monte Carlo in part, the trade-off between normalized\nnetwork length and $R$ in a one-parameter family of proximity graphs. How close\nthis family comes to the optimal trade-off over all possible networks remains\nan intriguing open question. The paper is a write-up of a talk developed by the\nfirst author during 2007--2009.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 02:04:05 GMT"}, {"version": "v2", "created": "Wed, 5 Jan 2011 06:57:02 GMT"}], "update_date": "2011-01-06", "authors_parsed": [["Aldous", "David J.", ""], ["Shun", "Julian", ""]]}, {"id": "1003.3829", "submitter": "Emily Fox", "authors": "Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, Alan S. Willsky", "title": "Bayesian Nonparametric Inference of Switching Linear Dynamical Systems", "comments": "50 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TSP.2010.2102756", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex dynamical phenomena can be effectively modeled by a system that\nswitches among a set of conditionally linear dynamical modes. We consider two\nsuch models: the switching linear dynamical system (SLDS) and the switching\nvector autoregressive (VAR) process. Our Bayesian nonparametric approach\nutilizes a hierarchical Dirichlet process prior to learn an unknown number of\npersistent, smooth dynamical modes. We additionally employ automatic relevance\ndetermination to infer a sparse set of dynamic dependencies allowing us to\nlearn SLDS with varying state dimension or switching VAR processes with varying\nautoregressive order. We develop a sampling algorithm that combines a truncated\napproximation to the Dirichlet process with efficient joint sampling of the\nmode and state sequences. The utility and flexibility of our model are\ndemonstrated on synthetic data, sequences of dancing honey bees, the IBOVESPA\nstock index, and a maneuvering target tracking application.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 16:22:02 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Fox", "Emily B.", ""], ["Sudderth", "Erik B.", ""], ["Jordan", "Michael I.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1003.3988", "submitter": "Peter Green", "authors": "Peter J. Green", "title": "Colouring and breaking sticks: random distributions and heterogeneous\n  clustering", "comments": "26 pages, 3 figures. Chapter 13 of \"Probability and Mathematical\n  Genetics: Papers in Honour of Sir John Kingman\" (Editors N.H. Bingham and\n  C.M. Goldie), Cambridge University Press, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We begin by reviewing some probabilistic results about the Dirichlet Process\nand its close relatives, focussing on their implications for statistical\nmodelling and analysis. We then introduce a class of simple mixture models in\nwhich clusters are of different `colours', with statistical characteristics\nthat are constant within colours, but different between colours. Thus cluster\nidentities are exchangeable only within colours. The basic form of our model is\na variant on the familiar Dirichlet process, and we find that much of the\nstandard modelling and computational machinery associated with the Dirichlet\nprocess may be readily adapted to our generalisation. The methodology is\nillustrated with an application to the partially-parametric clustering of gene\nexpression profiles.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 09:48:06 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Green", "Peter J.", ""]]}, {"id": "1003.4156", "submitter": "Jean-Baptiste Aubin", "authors": "Jean-Baptiste Aubin, Samuela Leoni-Aubin", "title": "A longest run test for heteroscedasticity in univariate regression model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scope of this paper is the presentation of a test that enables to detect\nheteroscedasticity in univariate regression model. The test is simple to\ncompute and very general since no hypothesis is made on the regularity of the\nresponse function or on the normality of errors. Simulations show that our test\nfairs well with respect to other less general nonparametric tests.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 13:44:42 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Aubin", "Jean-Baptiste", ""], ["Leoni-Aubin", "Samuela", ""]]}, {"id": "1003.4466", "submitter": "Marie Kratz", "authors": "Armelle Guillou, Marie Kratz and Yann Le Strat", "title": "An Extreme Value Theory approach for the early detection of time\n  clusters with application to the surveillance of Salmonella", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to generate a warning system for the early detection of\ntime clusters applied to public health surveillance data. This new method\nrelies on the evaluation of a return period associated to any new count of a\nparticular infection reported to a surveillance system. The method is applied\nto Salmonella surveillance in France and compared to the model developed by\nFarrington et al.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2010 17:06:16 GMT"}], "update_date": "2010-03-24", "authors_parsed": [["Guillou", "Armelle", ""], ["Kratz", "Marie", ""], ["Strat", "Yann Le", ""]]}, {"id": "1003.4890", "submitter": "Jacques Poitevineau", "authors": "Jacques Poitevineau, Bruno Lecoutre (LMRS)", "title": "Implementing Bayesian predictive procedures: The K-prime and K-square\n  distributions", "comments": null, "journal-ref": "Computational Statistics & Data Analysis / Computational\n  Statistics and Data Analysis 54, 3 (2010) 724-731", "doi": "10.1016/j.csda.2008.11.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of Bayesian predictive procedures under standard normal\nmodels is considered. Two distributions are of particular interest, the K-prime\nand K-square distributions. They also give exact inferences for simple and\nmultiple correlation coefficients. Their cumulative distribution functions can\nbe expressed in terms of infinite series of multiples of incomplete beta\nfunction ratios, thus adequate for recursive calculations. Efficient algorithms\nare provided. To deal with special cases where possible underflows may prevent\na recurrence to work properly, a simple solution is proposed which results in a\nprocedure which is intermediate between two classes of algorithm. Some examples\nof applications are given.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 13:57:50 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Poitevineau", "Jacques", "", "LMRS"], ["Lecoutre", "Bruno", "", "LMRS"]]}, {"id": "1003.5165", "submitter": "Catherine Matias", "authors": "Christophe Ambroise and Catherine Matias", "title": "New consistent and asymptotically normal estimators for random graph\n  mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graph mixture models are now very popular for modeling real data\nnetworks. In these setups, parameter estimation procedures usually rely on\nvariational approximations, either combined with the expectation-maximisation\n(\\textsc{em}) algorithm or with Bayesian approaches. Despite good results on\nsynthetic data, the validity of the variational approximation is however not\nestablished. Moreover, the behavior of the maximum likelihood or of the maximum\na posteriori estimators approximated by these procedures is not known in these\nmodels, due to the dependency structure on the variables. In this work, we show\nthat in many different affiliation contexts (for binary or weighted graphs),\nestimators based either on moment equations or on the maximization of some\ncomposite likelihood are strongly consistent and $\\sqrt{n}$-convergent, where\n$n$ is the number of nodes. As a consequence, our result establishes that the\noverall structure of an affiliation model can be caught by the description of\nthe network in terms of its number of triads (order 3 structures) and edges\n(order 2 structures). We illustrate the efficiency of our method on simulated\ndata and compare its performances with other existing procedures. A data set of\ncross-citations among economics journals is also analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2010 15:36:55 GMT"}, {"version": "v2", "created": "Wed, 8 Dec 2010 17:23:18 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Ambroise", "Christophe", ""], ["Matias", "Catherine", ""]]}, {"id": "1003.5930", "submitter": "Mu Zhu", "authors": "Lu Xin, Mu Zhu", "title": "Stochastic Stepwise Ensembles for Variable Selection", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, June 2012, Vol.\n  21, No. 2, Pages 275 - 294", "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we advocate the ensemble approach for variable selection. We\npoint out that the stochastic mechanism used to generate the variable-selection\nensemble (VSE) must be picked with care. We construct a VSE using a stochastic\nstepwise algorithm, and compare its performance with numerous state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 21:35:26 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2011 21:12:32 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Xin", "Lu", ""], ["Zhu", "Mu", ""]]}]