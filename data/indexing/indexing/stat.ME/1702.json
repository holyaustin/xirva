[{"id": "1702.00046", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo Lewi Hammer, Anis Yazidi and H{\\aa}vard Rue", "title": "Estimation of Multiple Quantiles in Dynamically Varying Data Streams", "comments": "The manuscript under review to Scandinavian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of estimating quantiles when data are\nreceived sequentially (data stream). For real life data streams, the\ndistribution of the data typically varies with time making estimation of\nquantiles challenging. We present a method that simultaneously maintain\nestimates of multiple quantiles of the data stream distribution. The method is\nbased on making incremental updates of the quantile estimates every time a new\nsample from the data stream is received. The method is memory and\ncomputationally efficient since it only stores one value for each quantile\nestimate and only performs one operation per quantile estimate when a new\nsample is received from the data stream. The estimates are realistic in the\nsense that the monotone property of quantiles is satisfied in every iteration.\nExperiments show that the method efficiently tracks multiple quantiles and\noutperforms state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 20:28:17 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Hammer", "Hugo Lewi", ""], ["Yazidi", "Anis", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1702.00099", "submitter": "Ranjan Maitra", "authors": "Ye Tian, Ranjan Maitra, William Q. Meeker and Stephen D. Holland", "title": "A Statistical Framework for Improved Automatic Flaw Detection in\n  Nondestructive Evaluation Images", "comments": "To appear in Technometrics", "journal-ref": null, "doi": "10.1080/00401706.2016.1153000", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondestructive evaluation (NDE) techniques are widely used to detect flaws in\ncritical components of systems like aircraft engines, nuclear power plants and\noil pipelines in order to prevent catastrophic events. Many modern NDE systems\ngenerate image data. In some applications an experienced inspector performs the\ntedious task of visually examining every image to provide accurate conclusions\nabout the existence of flaws. This approach is labor-intensive and can cause\nmisses due to operator ennui. Automated evaluation methods seek to eliminate\nhuman-factors variability and improve throughput. Simple methods based on peak\namplitude in an image are sometimes employed and a trained-operator-controlled\nrefinement that uses a dynamic threshold based on signal-to-noise ratio (SNR)\nhas also been implemented. We develop an automated and optimized detection\nprocedure that mimics these operations. The primary goal of our methodology is\nto reduce the number of images requiring expert visual evaluation by filtering\nout images that are overwhelmingly definitive on the existence or absence of a\nflaw. We use an appropriate model for the observed values of the SNR-detection\ncriterion to estimate the probability of detection. Our methodology outperforms\ncurrent methods in terms of its ability to detect flaws.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 01:00:14 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Tian", "Ye", ""], ["Maitra", "Ranjan", ""], ["Meeker", "William Q.", ""], ["Holland", "Stephen D.", ""]]}, {"id": "1702.00111", "submitter": "Ranjan Maitra", "authors": "Israel Almod\\'ovar-Rivera and Ranjan Maitra", "title": "FAST Adaptive Smoothing and Thresholding for Improved Activation\n  Detection in Low-Signal fMRI", "comments": "26 pages, 2 tables, 19 figures. Accepted for publication in IEEE\n  Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2915052", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Imaging is a noninvasive tool for studying\ncerebral function. Many factors challenge activation detection, especially in\nlow-signal scenarios that arise in the performance of high-level cognitive\ntasks. We provide a fully automated fast adaptive smoothing and thresholding\n(FAST) algorithm that uses smoothing and extreme value theory on correlated\nstatistical parametric maps for thresholding. Performance on experiments\nspanning a range of low-signal settings is very encouraging. The methodology\nalso performs well in a study to identify the cerebral regions that perceive\nonly-auditory-reliable or only-visual-reliable speech stimuli.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 03:17:50 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 03:12:48 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 18:08:37 GMT"}, {"version": "v4", "created": "Sun, 5 May 2019 04:55:49 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Almod\u00f3var-Rivera", "Israel", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1702.00261", "submitter": "Leah R. Johnson", "authors": "Leah R. Johnson, Robert B. Gramacy, Jeremy Cohen, Erin Mordecai,\n  Courtney Murdock, Jason Rohr, Sadie J. Ryan, Anna M. Stewart-Ibarra, Daniel\n  Weikel", "title": "Phenomenological forecasting of disease incidence using heteroskedastic\n  Gaussian processes: a dengue case study", "comments": "39 pages, 13 figures, 4 tables, including appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2015 the US federal government sponsored a dengue forecasting competition\nusing historical case data from Iquitos, Peru and San Juan, Puerto Rico.\nCompetitors were evaluated on several aspects of out-of-sample forecasts\nincluding the targets of peak week, peak incidence during that week and total\nseason incidence across each of several seasons. Our team was one of the top\nperformers of that competition, outperforming all other teams in multiple\ntargets/locals. In this paper we report on our methodology, a large component\nof which, surprisingly, ignores the known biology of epidemics at large---in\nparticular relationships between dengue transmission and environmental\nfactors---and instead relies on flexible nonparametric nonlinear Gaussian\nprocess (GP) regression fits that \"memorize\" the trajectories of past seasons,\nand then \"match\" the dynamics of the unfolding season to past ones in\nreal-time. Our phenomenological approach has advantages in situations where\ndisease dynamics are less well understood, e.g., at sites with shorter\nhistories of disease (such as Iquitos), or where measurements and forecasts of\nancillary covariates like precipitation are unavailable and/or where the\nstrength of association with cases are as yet unknown. In particular, we show\nthat the GP approach generally outperforms a more classical generalized linear\n(autoregressive) model (GLM) that we developed to utilize abundant covariate\ninformation. We illustrate variations of our method(s) on the two benchmark\nlocales alongside a full summary of results submitted by other contest\ncompetitors.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 14:01:57 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 14:16:18 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 14:06:06 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Johnson", "Leah R.", ""], ["Gramacy", "Robert B.", ""], ["Cohen", "Jeremy", ""], ["Mordecai", "Erin", ""], ["Murdock", "Courtney", ""], ["Rohr", "Jason", ""], ["Ryan", "Sadie J.", ""], ["Stewart-Ibarra", "Anna M.", ""], ["Weikel", "Daniel", ""]]}, {"id": "1702.00308", "submitter": "Lingjiong Zhu", "authors": "Angelo Mele, Lingjiong Zhu", "title": "Approximate Variational Estimation for a Model of Network Formation", "comments": "52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop approximate estimation methods for exponential random graph models\n(ERGMs), whose likelihood is proportional to an intractable normalizing\nconstant. The usual approach approximates this constant with Monte Carlo\nsimulations, however convergence may be exponentially slow. We propose a\ndeterministic method, based on a variational mean-field approximation of the\nERGM's normalizing constant. We compute lower and upper bounds for the\napproximation error for any network size, adapting nonlinear large deviations\nresults. This translates into bounds on the distance between true likelihood\nand mean-field likelihood. Monte Carlo simulations suggest that in practice our\ndeterministic method performs better than our conservative theoretical\napproximation bounds imply, for a large class of models.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 15:21:42 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 18:30:58 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 19:19:16 GMT"}, {"version": "v4", "created": "Sat, 9 May 2020 22:30:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Mele", "Angelo", ""], ["Zhu", "Lingjiong", ""]]}, {"id": "1702.00327", "submitter": "Fabio M. Bayer Ph.D", "authors": "Diego Ramos Canterle and F\\'abio Mariano Bayer", "title": "Variable dispersion beta regressions with parametric link functions", "comments": "Accepted paper", "journal-ref": "Statistical Papers, 2017", "doi": "10.1007/s00362-017-0885-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new class of regression models for continuous data\nrestricted to the interval $(0,1)$, such as rates and proportions. The proposed\nclass of models assumes a beta distribution for the variable of interest with\nregression structures for the mean and dispersion parameters. These structures\nconsider covariates, unknown regression parameters, and parametric link\nfunctions. Link functions depend on parameters that model the relationship\nbetween the random component and the linear predictors. The symmetric and\nassymetric Aranda-Ordaz link functions are considered in details. Depending on\nthe parameter values, these link functions refer to particular cases of fixed\nlinks such as logit and complementary log-log functions. Joint estimation of\nthe regression and link function parameters is performed by maximum likelihood.\nClosed-form expressions for the score function and Fisher's information matrix\nare presented. Aspects of large sample inferences are discussed, and some\ndiagnostic measures are proposed. A Monte Carlo simulation study is used to\nevaluate the finite sample performance of point estimators. Finally, a\npractical application that employs real data is presented and discussed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 15:58:05 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Canterle", "Diego Ramos", ""], ["Bayer", "F\u00e1bio Mariano", ""]]}, {"id": "1702.00376", "submitter": "Michael Chiu", "authors": "Michael Chiu, Kenneth R. Jackson and Alexander Kreinin", "title": "Correlated Multivariate Poisson Processes and Extreme Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Poisson processes have many important applications in Insurance,\nFinance, and many other areas of Applied Probability. In this paper we study\nthe backward simulation approach to modelling multivariate Poisson processes\nand analyze the connection to the extreme measures describing the joint\ndistribution of the processes at the terminal simulation time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 18:06:53 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 17:54:35 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Chiu", "Michael", ""], ["Jackson", "Kenneth R.", ""], ["Kreinin", "Alexander", ""]]}, {"id": "1702.00444", "submitter": "Yeying Zhu", "authors": "Wei Luo and Yeying Zhu", "title": "Matching Using Sufficient Dimension Reduction for Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To estimate casual treatment effects, we propose a new matching approach\nbased on the reduced covariates obtained from sufficient dimension reduction.\nCompared to the original covariates and the propensity score, which are\ncommonly used for matching in the literature, the reduced covariates are\nestimable nonparametrically under a mild assumption on the original covariates,\nand are sufficient and effective in imputing the missing potential outcomes.\nUnder the ignorability assumption, the consistency of the proposed approach\nrequires a weaker common support condition. In addition, the researchers are\nallowed to use different reduced covariates to find matched subjects for\ndifferent treatment groups. We develop relative asymptotic results, and conduct\nsimulation studies as well as real data analysis to illustrate the usefulness\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 20:41:21 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Luo", "Wei", ""], ["Zhu", "Yeying", ""]]}, {"id": "1702.00501", "submitter": "Julia Fukuyama", "authors": "Julia Fukuyama", "title": "Adaptive gPCA: A method for structured dimensionality reduction", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with large biological data sets, exploratory analysis is an\nimportant first step for understanding the latent structure and for generating\nhypotheses to be tested in subsequent analyses. However, when the number of\nvariables is large compared to the number of samples, standard methods such as\nprincipal components analysis give results which are unstable and difficult to\ninterpret.\n  To mitigate these problems, we have developed a method which allows the\nanalyst to incorporate side information about the relationships between the\nvariables in a way that encourages similar variables to have similar loadings\non the principal axes. This leads to a low-dimensional representation of the\nsamples which both describes the latent structure and which has axes which are\ninterpretable in terms of groups of closely related variables.\n  The method is derived by putting a prior encoding the relationships between\nthe variables on the data and following through the analysis on the posterior\ndistributions of the samples. We show that our method does well at\nreconstructing true latent structure in simulated data and we also demonstrate\nthe method on a dataset investigating the effects of antibiotics on the\ncomposition of bacteria in the human gut.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 23:38:57 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Fukuyama", "Julia", ""]]}, {"id": "1702.00525", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee, Dylan S. Small and Paul R. Rosenbaum", "title": "A powerful approach to the study of moderate effect modification in\n  observational studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effect modification means the magnitude or stability of a treatment effect\nvaries as a function of an observed covariate. Generally, larger and more\nstable treatment effects are insensitive to larger biases from unmeasured\ncovariates, so a causal conclusion may be considerably firmer if this pattern\nis noted if it occurs. We propose a new strategy, called the submax-method,\nthat combines exploratory and confirmatory efforts to determine whether there\nis stronger evidence of causality - that is, greater insensitivity to\nunmeasured confounding - in some subgroups of individuals. It uses the joint\ndistribution of test statistics that split the data in various ways based on\ncertain observed covariates. For $L$ binary covariates, the method splits the\npopulation $L$ times into two subpopulations, perhaps first men and women,\nperhaps then smokers and nonsmokers, computing a test statistic from each\nsubpopulation, and appends the test statistic for the whole population, making\n$2L+1$ test statistics in total. Although $L$ binary covariates define $2^{L}$\ninteraction groups, only $2L+1$ tests are performed, and at least $L+1$ of\nthese tests use at least half of the data. The submax-method achieves the\nhighest design sensitivity and the highest Bahadur efficiency of its component\ntests. Moreover, the form of the test is sufficiently tractable that its large\nsample power may be studied analytically. The simulation suggests that the\nsubmax method exhibits superior performance, in comparison with an approach\nusing CART, when there is effect modification of moderate size. Using data from\nthe NHANES I Epidemiologic Follow-Up Survey, an observational study of the\neffects of physical activity on survival is used to illustrate the method. The\nmethod is implemented in the $\\texttt{R}$ package $\\texttt{submax}$ which\ncontains the NHANES example.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 02:02:13 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 01:58:46 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 07:23:25 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Lee", "Kwonsang", ""], ["Small", "Dylan S.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1702.00556", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth and Andrew Gelman", "title": "The statistical significance filter leads to overconfident expectations\n  of replicability", "comments": "6 pages, 3 figures. Submitted to the conference Cognitive Science\n  2017, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that publishing results using the statistical significance\nfilter---publishing only when the p-value is less than 0.05---leads to a\nvicious cycle of overoptimistic expectation of the replicability of results.\nFirst, we show analytically that when true statistical power is relatively low,\ncomputing power based on statistically significant results will lead to\noverestimates of power. Then, we present a case study using 10 experimental\ncomparisons drawn from a recently published meta-analysis in psycholinguistics\n(J\\\"ager et al., 2017). We show that the statistically significant results\nyield an illusion of replicability. This illusion holds even if the researcher\ndoesn't conduct any formal power analysis but just uses statistical\nsignificance to informally assess robustness (i.e., replicability) of results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 07:14:21 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 06:24:02 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Vasishth", "Shravan", ""], ["Gelman", "Andrew", ""]]}, {"id": "1702.00564", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth, Nicolas Chopin, Robin Ryder, Bruno Nicenboim", "title": "Modelling dependency completion in sentence comprehension as a Bayesian\n  hierarchical mixture process: A case study involving Chinese relative clauses", "comments": "6 pages, 2 figures. To appear in the Proceedings of the Cognitive\n  Science Conference 2017, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case-study demonstrating the usefulness of Bayesian hierarchical\nmixture modelling for investigating cognitive processes. In sentence\ncomprehension, it is widely assumed that the distance between linguistic\nco-dependents affects the latency of dependency resolution: the longer the\ndistance, the longer the retrieval time (the distance-based account). An\nalternative theory, direct-access, assumes that retrieval times are a mixture\nof two distributions: one distribution represents successful retrievals (these\nare independent of dependency distance) and the other represents an initial\nfailure to retrieve the correct dependent, followed by a reanalysis that leads\nto successful retrieval. We implement both models as Bayesian hierarchical\nmodels and show that the direct-access model explains Chinese relative clause\nreading time data better than the distance account.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 07:48:58 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 05:44:34 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Vasishth", "Shravan", ""], ["Chopin", "Nicolas", ""], ["Ryder", "Robin", ""], ["Nicenboim", "Bruno", ""]]}, {"id": "1702.00609", "submitter": "Raphael Bacher", "authors": "Raphael Bacher, Celine Meillier, Florent Chatelain, Olivier Michel", "title": "Robust control of varying weak hyperspectral target detection with\n  sparse non-negative representation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2688965", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a multiple-comparison approach is developed for detecting\nfaint hyperspectral sources. The detection method relies on a sparse and\nnon-negative representation on a highly coherent dictionary to track a\nspatially varying source. A robust control of the detection errors is ensured\nby learning the test statistic distributions on the data. The resulting control\nis based on the false discovery rate, to take into account the large number of\npixels to be tested. This method is applied to data recently recorded by the\nthree-dimensional spectrograph Multi-Unit Spectrograph Explorer.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 10:37:40 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 08:30:55 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Bacher", "Raphael", ""], ["Meillier", "Celine", ""], ["Chatelain", "Florent", ""], ["Michel", "Olivier", ""]]}, {"id": "1702.00728", "submitter": "Tobias Michael Erhardt", "authors": "T. M. Erhardt, C. Czado and T. L. Thorarinsdottir", "title": "Evaluation of time series models under non-stationarity with application\n  to the comparison of regional climate models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different disciplines pursue the aim to develop models which characterize\ncertain phenomena as accurately as possible. Climatology is a prime example,\nwhere the temporal evolution of the climate is modeled. In order to compare and\nimprove different models, methodology for a fair model evaluation is\nindispensable. As models and forecasts of a phenomenon are usually associated\nwith uncertainty, proper scoring rules, which are tools that account for this\nkind of uncertainty, are an adequate choice for model evaluation. However,\nunder the presence of non-stationarity, such a model evaluation becomes\nchallenging, as the characteristics of the phenomenon of interest change. We\nprovide methodology for model evaluation in the context of non-stationary time\nseries. Our methodology assumes stationarity of the time series in shorter\nmoving time windows. These moving windows, which are selected based on a\nchangepoint analysis, are used to characterize the uncertainty of the\nphenomenon/model for the corresponding time instances. This leads to the\nconcept of moving scores allowing for a temporal assessment of the model\nperformance. The merits of the proposed methodology are illustrated in a\nsimulation and a case study.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 16:05:01 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Erhardt", "T. M.", ""], ["Czado", "C.", ""], ["Thorarinsdottir", "T. L.", ""]]}, {"id": "1702.00836", "submitter": "Myung Hwan Seo", "authors": "Javier Hidalgo, Jungyoon Lee, Myung Hwan Seo", "title": "Robust inference for threshold regression models", "comments": "56 pages, 3 figures", "journal-ref": "Journal of Econometrics (2019), 210 (2), 291-309", "doi": "10.1016/j.jeconom.2019.01.008", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with inference in threshold regression models when\nthe practitioners do not know whether at the threshold point the true\nspecification has a kink or a jump. We nest previous works that assume either\ncontinuity or discontinuity at the threshold point and develop robust inference\nmethods on the parameters of the model, which are valid under both\nspecifications. In particular, we found that the parameter values under the\nkink restriction are irregular points of the Hessian matrix of the expected\nGaussian quasi-likelihood. This irregularity destroys the asymptotic normality\nand induces the non-standard cube root convergence rate for the threshold\nestimate. However, it also enables us to obtain the same asymptotic\ndistribution as in Hansen (2000) for the quasi-likelihood ratio statistic for\nthe unknown threshold up to an unknown scale parameter. We show that this scale\nparameter can be consistently estimated by a kernel method as long as no higher\norder kernel is used. Furthermore, we propose to construct confidence intervals\nfor the unknown threshold by bootstrap test inversion, also known as grid\nbootstrap. Finite sample performances of the grid bootstrap confidence\nintervals are examined through Monte Carlo simulations. We also implement our\nprocedure to an economic empirical application.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 21:38:47 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 15:18:43 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Hidalgo", "Javier", ""], ["Lee", "Jungyoon", ""], ["Seo", "Myung Hwan", ""]]}, {"id": "1702.00888", "submitter": "Jiannan Lu", "authors": "Jiannan Lu, Alex Deng", "title": "On randomization-based causal inference for matched-pair factorial\n  designs", "comments": "To appear in Statistics & Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the potential outcomes framework, we introduce matched-pair factorial\ndesigns, and propose the matched-pair estimator of the factorial effects. We\nalso calculate the randomization-based covariance matrix of the matched-pair\nestimator, and provide the \"Neymanian\" estimator of the covariance matrix.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 01:49:20 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Lu", "Jiannan", ""], ["Deng", "Alex", ""]]}, {"id": "1702.00971", "submitter": "Vincent Audigier", "authors": "Vincent Audigier, Ian R. White, Shahab Jolani, Thomas P. A. Debray,\n  Matteo Quartagno, James Carpenter, Stef van Buuren, Matthieu Resche-Rigon", "title": "Multiple imputation for multilevel data with continuous and binary\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and compare multiple imputation methods for multilevel continuous\nand binary data where variables are systematically and sporadically missing.\n  The methods are compared from a theoretical point of view and through an\nextensive simulation study motivated by a real dataset comprising multiple\nstudies. Simulations are reproducible. The comparisons show why these multiple\nimputation methods are the most appropriate to handle missing values in a\nmultilevel setting and why their relative performances can vary according to\nthe missing data pattern, the multilevel structure and the type of missing\nvariables.\n  This study shows that valid inferences can only be obtained if the dataset\ngathers a large number of clusters. In addition, it highlights that\nheteroscedastic MI methods provide more accurate inferences than homoscedastic\nmethods, which should be reserved for data with few individuals per cluster.\nFinally, the method of Quartagno and Carpenter (2016a) appears generally\naccurate for binary variables, the method of Resche-Rigon and White (2016) with\nlarge clusters, and the approach of Jolani et al. (2015) with small clusters.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 11:16:36 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 16:55:13 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Audigier", "Vincent", ""], ["White", "Ian R.", ""], ["Jolani", "Shahab", ""], ["Debray", "Thomas P. A.", ""], ["Quartagno", "Matteo", ""], ["Carpenter", "James", ""], ["van Buuren", "Stef", ""], ["Resche-Rigon", "Matthieu", ""]]}, {"id": "1702.00978", "submitter": "Jeremy Oakley", "authors": "Ziyad A. Alhussain and Jeremy E. Oakley", "title": "Assurance for clinical trial design with normally distributed outcomes:\n  eliciting uncertainty about variances", "comments": "The previous version of this paper had the title \"Eliciting\n  judgements about uncertain population means and variances\". The elicitation\n  methods in this paper are unchanged from the previous version. The motivation\n  for the methods has changed, and the scope of the paper has been widened, to\n  show the application of the elicitation methods within the assurance method\n  for clinical trial planning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assurance method is growing in popularity in clinical trial planning. The\nmethod involves eliciting a prior distribution for the treatment effect, and\nthen calculating the probability that a proposed trial will produce a\n`successful' outcome. For normally distributed observations, uncertainty about\nthe variance of the normal distribution also needs to be accounted for, but\nthere is little guidance in the literature on how to elicit a distribution for\na variance parameter. We present a simple elicitation method, and illustrate\nhow the elicited distribution is incorporated within an assurance calculation.\nWe also consider multi-stage trials, where a decision to proceed with a larger\ntrial will follow from the outcome of a smaller trial; we illustrate the role\nof the elicted distribution in assessing the information provided by a proposed\nsmaller trial. Free software is available for implementing our methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 11:48:27 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 15:51:19 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Alhussain", "Ziyad A.", ""], ["Oakley", "Jeremy E.", ""]]}, {"id": "1702.00988", "submitter": "Tristan Senga Kiess\\'e", "authors": "Tristan Senga Kiess\\'e", "title": "On finite sample properties of nonparametric discrete asymmetric kernel\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete kernel method was developed to estimate count data\ndistributions, distinguishing discrete associated kernels based on their\nasymptotic behaviour. This study investigates the class of discrete asymmetric\nkernels and their resulting non-consistent estimators, but this theoretical\ndrawback of the estimators is balanced by some interesting features in\nsmall/medium samples. The role of modal probability and variance of discrete\nasymmetric kernels is highlighted to help better understand the performance of\nthese kernels, in particular how the binomial kernel outperforms other\nasymmetric kernels. The performance of discrete asymmetric kernel estimators of\nprobability mass functions is illustrated using simulations, in addition to\napplications to real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 12:42:38 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 10:57:37 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Kiess\u00e9", "Tristan Senga", ""]]}, {"id": "1702.01166", "submitter": "HaiYing Wang", "authors": "HaiYing Wang, Rong Zhu, Ping Ma", "title": "Optimal Subsampling for Large Sample Logistic Regression", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2017.1292914", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For massive data, the family of subsampling algorithms is popular to downsize\nthe data volume and reduce computational burden. Existing studies focus on\napproximating the ordinary least squares estimate in linear regression, where\nstatistical leverage scores are often used to define subsampling probabilities.\nIn this paper, we propose fast subsampling algorithms to efficiently\napproximate the maximum likelihood estimate in logistic regression. We first\nestablish consistency and asymptotic normality of the estimator from a general\nsubsampling algorithm, and then derive optimal subsampling probabilities that\nminimize the asymptotic mean squared error of the resultant estimator. An\nalternative minimization criterion is also proposed to further reduce the\ncomputational cost. The optimal subsampling probabilities depend on the full\ndata estimate, so we develop a two-step algorithm to approximate the optimal\nsubsampling procedure. This algorithm is computationally efficient and has a\nsignificant reduction in computing time compared to the full data approach.\nConsistency and asymptotic normality of the estimator from a two-step algorithm\nare also established. Synthetic and real data sets are used to evaluate the\npractical performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:23:46 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 17:01:40 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "HaiYing", ""], ["Zhu", "Rong", ""], ["Ma", "Ping", ""]]}, {"id": "1702.01236", "submitter": "Indika Udagedara", "authors": "Indika Udagedara, Brian Helenbrook, Aaron Luttman and Jared Catenacci", "title": "Improved Probabilistic Principal Component Analysis for Application to\n  Reduced Order Modeling", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous work, a reduced order model (ROM) for a stochastic system was\nmade, where noisy data was projected onto principal component analysis\n(PCA)-derived basis vectors to obtain an accurate reconstruction of the\nnoise-free data. That work used techniques designed for deterministic data, PCA\nwas used for the basis function generation and $L_2$ projection was used to\ncreate the reconstructions. In this work, probabilistic approaches are used.\nThe probabilistic PCA (PPCA) is used to generate the basis, which then allows\nthe noise in the training data to be estimated. PPCA has also been improved so\nthat the derived basis vectors are orthonormal and the variance of the basis\nexpansion coefficients over the training data set can be estimated. The\nstandard approach assumes a unit variance for these coefficients. Based on the\nresults of the PPCA, model selection criteria are applied to automatically\nchoose the dimension of the ROM. In our previous work, a heuristic approach was\nused to pick the dimension. Lastly, a new statistical approach is used for the\nprojection step where the variance information obtained from the improved PPCA\nis used as a prior to improve the projection. This gives improved accuracy over\n$L_2$ projection when the projected data is noisy. In addition, the noise\nstatistics for the projected data are not assumed to be the same as that of the\ntraining data, but are estimated in the projection process. The entire approach\ngives a fully stochastic method for computing a ROM from noisy training data,\ndetermining ideal model selection, and projecting noisy test data, thus\nenabling accurate predictions of noise-free data from data that is dominated by\nnoise.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 04:19:01 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Udagedara", "Indika", ""], ["Helenbrook", "Brian", ""], ["Luttman", "Aaron", ""], ["Catenacci", "Jared", ""]]}, {"id": "1702.01250", "submitter": "Guido Imbens", "authors": "Susan Athey, Guido Imbens, Thai Pham, Stefan Wager", "title": "Estimating Average Treatment Effects: Supplementary Analyses and\n  Remaining Challenges", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large literature on semiparametric estimation of average treatment\neffects under unconfounded treatment assignment in settings with a fixed number\nof covariates. More recently attention has focused on settings with a large\nnumber of covariates. In this paper we extend lessons from the earlier\nliterature to this new setting. We propose that in addition to reporting point\nestimates and standard errors, researchers report results from a number of\nsupplementary analyses to assist in assessing the credibility of their\nestimates.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 07:43:07 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Athey", "Susan", ""], ["Imbens", "Guido", ""], ["Pham", "Thai", ""], ["Wager", "Stefan", ""]]}, {"id": "1702.01349", "submitter": "David Cheng", "authors": "David Cheng, Abhishek Chakrabortty, Ashwin N. Ananthakrishnan, Tianxi\n  Cai", "title": "Estimating Average Treatment Effects with a Double-Index Propensity\n  Score", "comments": "48 pages, 1 figure; Finalized revised version", "journal-ref": "Biometrics 76 (2020) 767-777", "doi": "10.1111/biom.13195", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating average treatment effects (ATE) of a binary treatment\nin observational data when data-driven variable selection is needed to select\nrelevant covariates from a moderately large number of available covariates\n$\\mathbf{X}$. To leverage covariates among $\\mathbf{X}$ predictive of the\noutcome for efficiency gain while using regularization to fit a parametric\npropensity score (PS) model, we consider a dimension reduction of $\\mathbf{X}$\nbased on fitting both working PS and outcome models using adaptive LASSO. A\nnovel PS estimator, the Double-index Propensity Score (DiPS), is proposed, in\nwhich the treatment status is smoothed over the linear predictors for\n$\\mathbf{X}$ from both the initial working models. The ATE is estimated by\nusing the DiPS in a normalized inverse probability weighting (IPW) estimator,\nwhich is found to maintain double-robustness and also local semiparametric\nefficiency with a fixed number of covariates $p$. Under misspecification of\nworking models, the smoothing step leads to gains in efficiency and robustness\nover traditional doubly-robust estimators. These results are extended to the\ncase where $p$ diverges with sample size and working models are sparse.\nSimulations show the benefits of the approach in finite samples. We illustrate\nthe method by estimating the ATE of statins on colorectal cancer risk in an\nelectronic medical record (EMR) study and the effect of smoking on C-reactive\nprotein (CRP) in the Framingham Offspring Study.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 21:39:47 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 00:06:33 GMT"}, {"version": "v3", "created": "Sun, 30 Sep 2018 23:54:50 GMT"}, {"version": "v4", "created": "Sun, 25 Oct 2020 04:10:43 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cheng", "David", ""], ["Chakrabortty", "Abhishek", ""], ["Ananthakrishnan", "Ashwin N.", ""], ["Cai", "Tianxi", ""]]}, {"id": "1702.01418", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli, Pierre Latouche, Nial Friel", "title": "Choosing the number of groups in a latent stochastic block model for\n  dynamic networks", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent stochastic block models are flexible statistical models that are\nwidely used in social network analysis. In recent years, efforts have been made\nto extend these models to temporal dynamic networks, whereby the connections\nbetween nodes are observed at a number of different times. In this paper we\nextend the original stochastic block model by using a Markovian property to\ndescribe the evolution of nodes' cluster memberships over time. We recast the\nproblem of clustering the nodes of the network into a model-based context, and\nshow that the integrated completed likelihood can be evaluated analytically for\na number of likelihood models. Then, we propose a scalable greedy algorithm to\nmaximise this quantity, thereby estimating both the optimal partition and the\nideal number of groups in a single inferential framework. Finally we propose\napplications of our methodology to both real and artificial datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 15:43:17 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 14:35:39 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Latouche", "Pierre", ""], ["Friel", "Nial", ""]]}, {"id": "1702.01591", "submitter": "Robin Ince", "authors": "Robin A. A. Ince", "title": "The Partial Entropy Decomposition: Decomposing multivariate entropy and\n  mutual information via pointwise common surprisal", "comments": "Added Section 3.7 (Quantifying source vs mechanistic redundancy) and\n  Section 3.8 (Shared entropy as a measure of dependence: pure mutual\n  information) and updated abstract, results, and discussion accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST q-bio.NC q-bio.QM stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Obtaining meaningful quantitative descriptions of the statistical dependence\nwithin multivariate systems is a difficult open problem. Recently, the Partial\nInformation Decomposition (PID) was proposed to decompose mutual information\n(MI) about a target variable into components which are redundant, unique and\nsynergistic within different subsets of predictor variables. Here, we propose\nto apply the elegant formalism of the PID to multivariate entropy, resulting in\na Partial Entropy Decomposition (PED). We implement the PED with an entropy\nredundancy measure based on pointwise common surprisal; a natural definition\nwhich is closely related to the definition of MI. We show how this approach can\nreveal the dyadic vs triadic generative structure of multivariate systems that\nare indistinguishable with classical Shannon measures. The entropy perspective\nalso shows that misinformation is synergistic entropy and hence that MI itself\nincludes both redundant and synergistic effects. We show the relationships\nbetween the PED and MI in two predictors, and derive two alternative\ninformation decompositions which we illustrate on several example systems. This\nreveals that in entropy terms, univariate predictor MI is not a proper subset\nof the joint MI, and we suggest this previously unrecognised fact explains in\npart why obtaining a consistent PID has proven difficult. The PED also allows\nseparate quantification of mechanistic redundancy (related to the function of\nthe system) versus source redundancy (arising from dependencies between\ninputs); an important distinction which no existing methods can address. The\nnew perspective provided by the PED helps to clarify some of the difficulties\nencountered with the PID approach and the resulting decompositions provide\nuseful tools for practical data analysis across a wide range of application\nareas.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 12:28:27 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 16:11:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Ince", "Robin A. A.", ""]]}, {"id": "1702.01777", "submitter": "Konstantinos Spiliopoulos", "authors": "Michela Ottobre, Natesh S. Pillai and Konstantinos Spiliopoulos", "title": "Optimal Scaling of the MALA algorithm with Irreversible Proposals for\n  Gaussian targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known in many settings that reversible Langevin diffusions in\nconfining potentials converge to equilibrium exponentially fast. Adding\nirreversible perturbations to the drift of a Langevin diffusion that maintain\nthe same invariant measure accelerates its convergence to stationarity. Many\nexisting works thus advocate the use of such non-reversible dynamics for\nsampling. When implementing Markov Chain Monte Carlo algorithms (MCMC) using\ntime discretisations of such Stochastic Differential Equations (SDEs), one can\nappend the discretization with the usual Metropolis-Hastings accept-reject step\nand this is often done in practice because the accept--reject step eliminates\nbias. On the other hand, such a step makes the resulting chain reversible. It\nis not known whether adding the accept-reject step preserves the faster mixing\nproperties of the non-reversible dynamics. In this paper, we address this gap\nbetween theory and practice by analyzing the optimal scaling of MCMC algorithms\nconstructed from proposal moves that are time-step Euler discretisations of an\nirreversible SDE, for high dimensional Gaussian target measures. We call the\nresulting algorithm the \\imala, in comparison to the classical MALA algorithm\n(here {\\em ip} is for irreversible proposal). In order to quantify how the cost\nof the algorithm scales with the dimension $N$, we prove invariance principles\nfor the appropriately rescaled chain. In contrast to the usual MALA algorithm,\nwe show that there could be two regimes asymptotically: (i) a diffusive regime,\nas in the MALA algorithm and (ii) a ``fluid\" regime where the limit is an\nordinary differential equation. We provide concrete examples where the limit is\na diffusion, as in the standard MALA, but with provably higher limiting\nacceptance probabilities. Numerical results are also given corroborating the\ntheory.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 19:59:33 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 14:43:57 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 15:06:32 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ottobre", "Michela", ""], ["Pillai", "Natesh S.", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1702.01805", "submitter": "Renato J Cintra", "authors": "F. M. Bayer, R. J. Cintra, A. Edirisuriya, A. Madanayake", "title": "A Digital Hardware Fast Algorithm and FPGA-based Prototype for a Novel\n  16-point Approximate DCT for Image Compression Applications", "comments": "17 pages, 6 figures, 6 tables", "journal-ref": "Measurement Science and Technology, Volume 23, Number 11, 2012", "doi": "10.1088/0957-0233/23/11/114010", "report-no": null, "categories": "cs.MM cs.AR cs.DS cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete cosine transform (DCT) is the key step in many image and video\ncoding standards. The 8-point DCT is an important special case, possessing\nseveral low-complexity approximations widely investigated. However, 16-point\nDCT transform has energy compaction advantages. In this sense, this paper\npresents a new 16-point DCT approximation with null multiplicative complexity.\nThe proposed transform matrix is orthogonal and contains only zeros and ones.\nThe proposed transform outperforms the well-know Walsh-Hadamard transform and\nthe current state-of-the-art 16-point approximation. A fast algorithm for the\nproposed transform is also introduced. This fast algorithm is experimentally\nvalidated using hardware implementations that are physically realized and\nverified on a 40 nm CMOS Xilinx Virtex-6 XC6VLX240T FPGA chip for a maximum\nclock rate of 342 MHz. Rapid prototypes on FPGA for 8-bit input word size shows\nsignificant improvement in compressed image quality by up to 1-2 dB at the cost\nof only eight adders compared to the state-of-art 16-point DCT approximation\nalgorithm in the literature [S. Bouguezel, M. O. Ahmad, and M. N. S. Swamy. A\nnovel transform for image compression. In {\\em Proceedings of the 53rd IEEE\nInternational Midwest Symposium on Circuits and Systems (MWSCAS)}, 2010].\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 22:00:34 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Edirisuriya", "A.", ""], ["Madanayake", "A.", ""]]}, {"id": "1702.01875", "submitter": "Nan Zhang", "authors": "Ping Ma, Nan Zhang, Jianhua Z. Huang, Wenxuan Zhong", "title": "Adaptive Basis Selection for Exponential Family Smoothing Splines with\n  Application in Joint Modeling of Multiple Sequencing Samples", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second-generation sequencing technologies have replaced array-based\ntechnologies and become the default method for genomics and epigenomics\nanalysis. Second-generation sequencing technologies sequence tens of millions\nof DNA/cDNA fragments in parallel. After the resulting sequences (short reads)\nare mapped to the genome, one gets a sequence of short read counts along the\ngenome. Effective extraction of signals in these short read counts is the key\nto the success of sequencing technologies. Nonparametric methods, in particular\nsmoothing splines, have been used extensively for modeling and processing\nsingle sequencing samples. However, nonparametric joint modeling of multiple\nsecond-generation sequencing samples is still lacking due to computational\ncost. In this article, we develop an adaptive basis selection method for\nefficient computation of exponential family smoothing splines for modeling\nmultiple second-generation sequencing samples. Our adaptive basis selection\ngives a sparse approximation of smoothing splines, yielding a lower-dimensional\neffective model space for a more scalable computation. The asymptotic analysis\nshows that the effective model space is rich enough to retain essential\nfeatures of the data. Moreover, exponential family smoothing spline models\ncomputed via adaptive basis selection are shown to have good statistical\nproperties, e.g., convergence at the same rate as that of full basis\nexponential family smoothing splines. The empirical performance is demonstrated\nthrough simulation studies and two second-generation sequencing data examples.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 04:32:23 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Ma", "Ping", ""], ["Zhang", "Nan", ""], ["Huang", "Jianhua Z.", ""], ["Zhong", "Wenxuan", ""]]}, {"id": "1702.01906", "submitter": "Yong Zhang", "authors": "Yong Zhang, Xiaodi Qian, Hong Qin, Ting Yan", "title": "Affiliation networks with an increasing degree sequence", "comments": "24 pages,2 figures,4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Affiliation network is one kind of two-mode social network with two different\nsets of nodes (namely, a set of actors and a set of social events) and edges\nrepresenting the affiliation of the actors with the social events. Although a\nnumber of statistical models are proposed to analyze affiliation networks, the\nasymptotic behaviors of the estimator are still unknown or have not been\nproperly explored. In this paper, we study an affiliation model with the degree\nsequence as the exclusively natural sufficient statistic in the exponential\nfamily distributions. We establish the uniform consistency and asymptotic\nnormality of the maximum likelihood estimator when the numbers of actors and\nevents both go to infinity. Simulation studies and a real data example\ndemonstrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 07:26:41 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Zhang", "Yong", ""], ["Qian", "Xiaodi", ""], ["Qin", "Hong", ""], ["Yan", "Ting", ""]]}, {"id": "1702.02009", "submitter": "Hidetoshi Matsui", "authors": "Hidetoshi Matsui", "title": "Quadratic regression for functional response models", "comments": "17 pages, 6 figures", "journal-ref": "Econometrics and Statistics 13 (2020) 125-136", "doi": "10.1016/j.ecosta.2018.12.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing a regression model with a functional\npredictor and a functional response. We extend the functional linear model to\nthe quadratic model, where the quadratic term also takes the interaction\nbetween the argument of the functional data into consideration. We assume that\nthe predictor and the coefficient functions are expressed by basis expansions,\nand then parameters included in the model are estimated by the penalized\nlikelihood method assuming that the error function follows a Gaussian process.\nMonte Carlo simulations are conducted to illustrate the efficacy of the\nproposed method. Finally, we apply the proposed method to the analysis of\nmeteorological data and explore the results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 13:45:35 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 06:15:10 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 07:57:37 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Matsui", "Hidetoshi", ""]]}, {"id": "1702.02010", "submitter": "Hidetoshi Matsui", "authors": "Hidetoshi Matsui", "title": "Selection of variables and decision boundaries for functional data via\n  bi-level selection", "comments": "12 pages, 1 figure", "journal-ref": "Communications in Statistics - Simulation and Computation 6 (2019)\n  1784-1797", "doi": "10.1080/03610918.2018.1423693", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-inducing penalties are useful tools for variable selection and they\nare also effective for regression settings where the data are functions. We\nconsider the problem of selecting not only variables but also decision\nboundaries in logistic regression models for functional data, using the sparse\nregularization. The functional logistic regression model is estimated by the\nframework of the penalized likelihood method with the sparse group lasso-type\npenalty, and then tuning parameters are selected using the model selection\ncriterion. The effectiveness of the proposed method is investigated through\nreal data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 13:46:08 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 06:12:49 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Matsui", "Hidetoshi", ""]]}, {"id": "1702.02048", "submitter": "Giona Casiraghi", "authors": "Giona Casiraghi", "title": "Multiplex Network Regression: How do relations drive interactions?", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a statistical regression model to investigate the impact of\ndyadic relations on complex networks generated from observed repeated\ninteractions. It is based on generalised hypergeometric ensembles (gHypEG), a\nclass of statistical network ensembles developed recently to deal with\nmulti-edge graph and count data. We represent different types of known\nrelations between system elements by weighted graphs, separated in the\ndifferent layers of a multiplex network. With our method, we can regress the\ninfluence of each relational layer, the explanatory variables, on the\ninteraction counts, the dependent variables. Moreover, we can quantify the\nstatistical significance of the relations as explanatory variables for the\nobserved interactions. To demonstrate the power of our approach, we investigate\nan example based on empirical data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 14:53:37 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 09:37:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Casiraghi", "Giona", ""]]}, {"id": "1702.02231", "submitter": "Marcelo Moreira", "authors": "Jose Diogo Barbosa, Marcelo J. Moreira", "title": "Likelihood Inference and The Role of Initial Conditions for the Dynamic\n  Panel Data Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lancaster (2002} proposes an estimator for the dynamic panel data model with\nhomoskedastic errors and zero initial conditions. In this paper, we show this\nestimator is invariant to orthogonal transformations, but is inefficient\nbecause it ignores additional information available in the data. The zero\ninitial condition is trivially satisfied by subtracting initial observations\nfrom the data. We show that differencing out the data further erodes efficiency\ncompared to drawing inference conditional on the first observations.\n  Finally, we compare the conditional method with standard random effects\napproaches for unobserved data. Standard approaches implicitly rely on normal\napproximations, which may not be reliable when unobserved data is very skewed\nwith some mass at zero values. For example, panel data on firms naturally\ndepend on the first period in which the firm enters on a new state. It seems\nunreasonable then to assume that the process determining unobserved data is\nknown or stationary. We can instead make inference on structural parameters by\nconditioning on the initial observations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 00:29:42 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Barbosa", "Jose Diogo", ""], ["Moreira", "Marcelo J.", ""]]}, {"id": "1702.02286", "submitter": "Hongmei Liu", "authors": "Hongmei Liu and J. Sunil Rao", "title": "Prediction Weighted Maximum Frequency Selection", "comments": "This manuscript contains 41 pages and 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimators that possess the ability to produce sparse solutions\nhave become increasingly important to the analysis of today's complex datasets.\nExamples include the LASSO, the Elastic-Net and their adaptive counterparts.\nEstimation of penalty parameters still presents difficulties however. While\nvariable selection consistent procedures have been developed, their finite\nsample performance can often be less than satisfactory. We develop a new\nstrategy for variable selection using the adaptive LASSO and adaptive\nElastic-Net estimators with $p_n$ diverging. The basic idea first involves\nusing the trace paths of their LARS solutions to bootstrap estimates of maximum\nfrequency (MF) models conditioned on dimension. Conditioning on dimension\neffectively mitigates overfitting, however to deal with underfitting, these MFs\nare then prediction-weighted, and it is shown that not only can consistent\nmodel selection be achieved, but that attractive convergence rates can as well,\nleading to excellent finite sample performance. Detailed numerical studies are\ncarried out on both simulated and real datasets. Extensions to the class of\ngeneralized linear models are also detailed.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 04:51:37 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Liu", "Hongmei", ""], ["Rao", "J. Sunil", ""]]}, {"id": "1702.02484", "submitter": "Daniel Paulin", "authors": "Daniel Paulin, Ajay Jasra, Dan Crisan, Alexandros Beskos", "title": "Optimization Based Methods for Partially Observed Chaotic Systems", "comments": "68 pages, 3 figures. Some minor corrections and references added in\n  this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider filtering and smoothing of partially observed\nchaotic dynamical systems that are discretely observed, with an additive\nGaussian noise in the observation. These models are found in a wide variety of\nreal applications and include the Lorenz 96' model. In the context of a fixed\nobservation interval $T$, observation time step $h$ and Gaussian observation\nvariance $\\sigma_Z^2$, we show under assumptions that the filter and smoother\nare well approximated by a Gaussian with high probability when $h$ and\n$\\sigma^2_Z h$ are sufficiently small. Based on this result we show that the\nMaximum-a-posteriori (MAP) estimators are asymptotically optimal in mean square\nerror as $\\sigma^2_Z h$ tends to $0$. Given these results, we provide a batch\nalgorithm for the smoother and filter, based on Newton's method, to obtain the\nMAP. In particular, we show that if the initial point is close enough to the\nMAP, then Newton's method converges to it at a fast rate. We also provide a\nmethod for computing such an initial point. These results contribute to the\ntheoretical understanding of widely used 4D-Var data assimilation method. Our\napproach is illustrated numerically on the Lorenz 96' model with state vector\nup to 1 million dimensions, with code running in the order of minutes. To our\nknowledge the results in this paper are the first of their type for this class\nof models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 15:56:20 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 15:09:46 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 00:56:11 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Paulin", "Daniel", ""], ["Jasra", "Ajay", ""], ["Crisan", "Dan", ""], ["Beskos", "Alexandros", ""]]}, {"id": "1702.02590", "submitter": "Vladimir Vovk", "authors": "Yuri Gurevich and Vladimir Vovk", "title": "Test statistics and p-values", "comments": "22 pages", "journal-ref": "Proceedings of Machine Learning Research 105:89-104 (2019)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We point out that the traditional notion of test statistic is too narrow, and\nwe propose a natural generalization that is arguably maximal. The study is\nrestricted to simple statistical hypotheses.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 19:27:44 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 13:15:56 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Gurevich", "Yuri", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1702.02658", "submitter": "Patrick Perry", "authors": "Wei Fu and Patrick O. Perry", "title": "Estimating the number of clusters using cross-validation", "comments": "44 pages; includes supplementary appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering methods, including k-means, require the user to specify the\nnumber of clusters as an input parameter. A variety of methods have been\ndevised to choose the number of clusters automatically, but they often rely on\nstrong modeling assumptions. This paper proposes a data-driven approach to\nestimate the number of clusters based on a novel form of cross-validation. The\nproposed method differs from ordinary cross-validation, because clustering is\nfundamentally an unsupervised learning problem. Simulation and real data\nanalysis results show that the proposed method outperforms existing methods,\nespecially in high-dimensional settings with heterogeneous or heavy-tailed\nnoise. In a yeast cell cycle dataset, the proposed method finds a parsimonious\nclustering with interpretable gene groupings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 00:11:27 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Fu", "Wei", ""], ["Perry", "Patrick O.", ""]]}, {"id": "1702.02686", "submitter": "Yining Wang", "authors": "Yining Wang, Jialei Wang, Sivaraman Balakrishnan, Aarti Singh", "title": "Rate Optimal Estimation and Confidence Intervals for High-dimensional\n  Regression with Missing Covariates", "comments": "41 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a majority of the theoretical literature in high-dimensional\nstatistics has focused on settings which involve fully-observed data, settings\nwith missing values and corruptions are common in practice. We consider the\nproblems of estimation and of constructing component-wise confidence intervals\nin a sparse high-dimensional linear regression model when some covariates of\nthe design matrix are missing completely at random. We analyze a variant of the\nDantzig selector [9] for estimating the regression model and we use a\nde-biasing argument to construct component-wise confidence intervals. Our first\nmain result is to establish upper bounds on the estimation error as a function\nof the model parameters (the sparsity level s, the expected fraction of\nobserved covariates $\\rho_*$, and a measure of the signal strength\n$\\|\\beta^*\\|_2$). We find that even in an idealized setting where the\ncovariates are assumed to be missing completely at random, somewhat\nsurprisingly and in contrast to the fully-observed setting, there is a\ndichotomy in the dependence on model parameters and much faster rates are\nobtained if the covariance matrix of the random design is known. To study this\nissue further, our second main contribution is to provide lower bounds on the\nestimation error showing that this discrepancy in rates is unavoidable in a\nminimax sense. We then consider the problem of high-dimensional inference in\nthe presence of missing data. We construct and analyze confidence intervals\nusing a de-biased estimator. In the presence of missing data, inference is\ncomplicated by the fact that the de-biasing matrix is correlated with the pilot\nestimator and this necessitates the design of a new estimator and a novel\nanalysis. We also complement our mathematical study with extensive simulations\non synthetic and semi-synthetic data that show the accuracy of our asymptotic\npredictions for finite sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 03:10:13 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 00:04:20 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Jialei", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""]]}, {"id": "1702.02708", "submitter": "Xiaodong Yan", "authors": "Xiaodong Yan, Niangsheng Tang and Xingqiu Zhao", "title": "The Spearman rank correlation screening for ultrahigh dimensional\n  censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Spearman rank correlation screening procedure for\nultrahigh dimensional data. Two adjusted versions are concerned for\nnon-censored and censored response, respectively. The proposed method, based on\nthe robust rank correlation coefficient between response and predictor\nvariables rather than the Pear- son correlation has the following\ndistingushiable merits: (i) It is robust and model-free without specifying any\nregression form of predictors and response variable; (ii) The sure screening\nand rank consistency properties can hold under some mild regularity condi-\ntions; (iii) It still works well when the covariates or error distribution is\nheavy-tailed or when the predictors are strongly dependent with each other;\n(iv) The use of indica- tor functions in rank correlation screening greatly\nsimplifies the theoretical derivation due to the boundedness and monotonic\ninvariance of the resulting statistics, compared with previous studies on\nvariable screening. Numerical comparison indicates that the proposed approach\nperforms much better than the most existing methods in various models,\nespecially for censored response with high-censoring ratio. We also illustrate\nour method using mantle cell lymphoma microarray dataset with censored\nresponse.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 05:29:11 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Yan", "Xiaodong", ""], ["Tang", "Niangsheng", ""], ["Zhao", "Xingqiu", ""]]}, {"id": "1702.02794", "submitter": "Vladimir Panov", "authors": "Denis Belomestny, Tatiana Orlova, and Vladimir Panov", "title": "Statistical inference for moving-average L\\'evy-driven processes:\n  Fourier-based approach", "comments": "23 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new method of the semiparametric statistical estimation for the\ncontinuous-time moving average L\\'evy processes. We derive the convergence\nrates of the proposed estimators, and show that these rates are optimal in the\nminimax sense.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 11:55:08 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Belomestny", "Denis", ""], ["Orlova", "Tatiana", ""], ["Panov", "Vladimir", ""]]}, {"id": "1702.02827", "submitter": "James Liley", "authors": "James Liley", "title": "Combining controls can improve power in two-stage association studies", "comments": "Four figures, includes two supplementary figures and appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional case control studies are ubiquitous in the biological\nsciences, particularly genomics. To maximise power while constraining cost and\nto minimise type-1 error rates, researchers typically seek to replicate\nfindings in a second experiment on independent cohorts before proceeding with\nfurther analyses.\n  This paper presents a method in which control (or case) samples from the\ndiscovery cohort are re-used in the replication study. The theoretical\nimplications of this method are discussed and simulations used to compare\nperformance against the standard method in a range of circumstances. In several\ncommon study designs, a shared-control method allows a substantial improvement\nin power while retaining type-1 error rate control.\n  An important area of potential application arises when control samples are\ndifficult to recruit or ascertain; for example in inter-disease comparisons, or\nstudies on degenerative diseases. Using similar methods, a procedure is\nproposed for `partial replication' using a new independent cohort consisting of\nonly controls. This methods can be used to provide some validation of findings\nwhen a full replication procedure is not possible.\n  The new method has differing sensitivity to confounding in study cohorts\ncompared to the standard procedure, which must be considered in its\napplication. Type-1 error rates in these scenarios are analytically and\nempirically derived, and an online tool for comparing power and error rates is\nprovided.\n  Although careful consideration must be made of all necessary assumptions,\nthis method can enable more efficient use of data in genome-wide association\nstudies (GWAS) and other applications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 13:29:48 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 14:13:01 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Liley", "James", ""]]}, {"id": "1702.03080", "submitter": "Wieslaw Szajnowski", "authors": "W. J. Szajnowski", "title": "Estimators of the correlation coefficient in the bivariate exponential\n  distribution", "comments": "4 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A finite-support constraint on the parameter space is used to derive a lower\nbound on the error of an estimator of the correlation coefficient in the\nbivariate exponential distribution. The bound is then exploited to examine\noptimality of three estimators, each being a nonlinear function of moments of\nexponential or Rayleigh observables. The estimator based on a measure of cosine\nsimilarity is shown to be highly efficient for values of the correlation\ncoefficient greater than 0.35; for smaller values, however, it is the\ntransformed Pearson correlation coefficient that exhibits errors closer to the\nderived bound.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 07:09:13 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Szajnowski", "W. J.", ""]]}, {"id": "1702.03129", "submitter": "Michael Wood", "authors": "Michael Wood", "title": "Simple Methods for Estimating Confidence Levels, or Tentative\n  Probabilities, for Hypotheses Instead of P Values", "comments": "Published in Methodological Innovations, 2019, 1-9. 2 figures, 2\n  tables, 3 linked spreadsheets which are also at\n  https://soths.wordpress.com/statistics-links/", "journal-ref": null, "doi": "10.1177/2059799119826518", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields of research null hypothesis significance tests and p values\nare the accepted way of assessing the degree of certainty with which research\nresults can be extrapolated beyond the sample studied. However, there are very\nserious concerns about the suitability of p values for this purpose. An\nalternative approach is to cite confidence intervals for a statistic of\ninterest, but this does not directly tell readers how certain a hypothesis is.\nHere, I suggest how the framework used for confidence intervals could easily be\nextended to derive confidence levels, or \"tentative probabilities\", for\nhypotheses. I also outline four quick methods for estimating these. This allows\nresearchers to state their confidence in a hypothesis as a direct probability,\ninstead of circuitously by p values referring to an unstated, hypothetical null\nhypothesis. The inevitable difficulties of statistical inference mean that\nthese probabilities can only be tentative, but probabilities are the natural\nway to express uncertainties, so, arguably, researchers using statistical\nmethods have an obligation to estimate how probable their hypotheses are by the\nbest available method. Otherwise misinterpretations will fill the void. Key\nwords: Confidence, Null hypothesis significance test, p value, Statistical\ninference\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 11:08:57 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 14:16:36 GMT"}, {"version": "v3", "created": "Sun, 12 Jan 2020 19:27:09 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Wood", "Michael", ""]]}, {"id": "1702.03244", "submitter": "Martin Spindler", "authors": "Ye Luo and Martin Spindler", "title": "$L_2$Boosting for Economic Applications", "comments": "Submitted to American Economic Review, Papers and Proceedings 2017.\n  arXiv admin note: text overlap with arXiv:1602.08927", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years more and more high-dimensional data sets, where the\nnumber of parameters $p$ is high compared to the number of observations $n$ or\neven larger, are available for applied researchers. Boosting algorithms\nrepresent one of the major advances in machine learning and statistics in\nrecent years and are suitable for the analysis of such data sets. While Lasso\nhas been applied very successfully for high-dimensional data sets in Economics,\nboosting has been underutilized in this field, although it has been proven very\npowerful in fields like Biostatistics and Pattern Recognition. We attribute\nthis to missing theoretical results for boosting. The goal of this paper is to\nfill this gap and show that boosting is a competitive method for inference of a\ntreatment effect or instrumental variable (IV) estimation in a high-dimensional\nsetting. First, we present the $L_2$Boosting with componentwise least squares\nalgorithm and variants which are tailored for regression problems which are the\nworkhorse for most Econometric problems. Then we show how $L_2$Boosting can be\nused for estimation of treatment effects and IV estimation. We highlight the\nmethods and illustrate them with simulations and empirical examples. For\nfurther results and technical details we refer to Luo and Spindler (2016, 2017)\nand to the online supplement of the paper.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 16:35:06 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Luo", "Ye", ""], ["Spindler", "Martin", ""]]}, {"id": "1702.03417", "submitter": "Keren Shen", "authors": "Keren Shen, Jianfeng Yao and Wai Keung Li", "title": "On a spiked model for large volatility matrix estimation from noisy\n  high-frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, inference about high-dimensional integrated covariance matrices\n(ICVs) based on noisy high-frequency data has emerged as a challenging problem.\nIn the literature, a pre-averaging estimator (PA-RCov) is proposed to deal with\nthe microstructure noise. Using the large-dimensional random matrix theory, it\nhas been established that the eigenvalue distribution of the PA-RCov matrix is\nintimately linked to that of the ICV through the Marcenko-Pastur equation.\nConsequently, the spectrum of the ICV can be inferred from that of the PA-RCov.\nHowever, extensive data analyses demonstrate that the spectrum of the PA-RCov\nis spiked, that is, a few large eigenvalues (spikes) stay away from the others\nwhich form a rather continuous distribution with a density function (bulk).\nTherefore, any inference on the ICVs must take into account this spiked\nstructure. As a methodological contribution, we propose a spiked model for the\nICVs where spikes can be inferred from those of the available PA-RCov matrices.\nThe consistency of the inference procedure is established and is checked by\nextensive simulation studies. In addition, we apply our method to the real data\nfrom the US and Hong Kong markets. It is found that our model clearly\noutperforms the existing one in predicting the existence of spikes and in\nmimicking the empirical PA-RCov matrices.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 12:17:50 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Shen", "Keren", ""], ["Yao", "Jianfeng", ""], ["Li", "Wai Keung", ""]]}, {"id": "1702.03442", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao", "title": "On sensitivity value of pair-matched observational studies", "comments": "17 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An observational study may be biased for estimating causal effects by failing\nto control for unmeasured confounders. This paper proposes a new quantity\ncalled the \"sensitivity value\", which is defined as the minimum strength of\nunmeasured confounders needed to change the qualitative conclusions of a naive\nanalysis assuming no unmeasured confounder. We establish the asymptotic\nnormality of the sensitivity value in pair-matched observational studies. The\ntheoretical results are then used to approximate the power of a sensitivity\nanalysis and select the design of a study. We explore the potential to use\nsensitivity values to screen multiple hypotheses in presence of unmeasured\nconfounding using a microarray dataset.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 17:17:32 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 21:58:04 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Zhao", "Qingyuan", ""]]}, {"id": "1702.03453", "submitter": "Hejian Sang", "authors": "Hejian Sang and Jae Kwang Kim", "title": "An approximate Bayesian inference on propensity score estimation under\n  unit nonresponse", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonresponse weighting adjustment using the response propensity score is a\npopular tool for handling unit nonresponse. Statistical inference after the\nnonresponse weighting adjustment is complicated because the effect of\nestimating the propensity model parameter needs to be incorporated. In this\npaper, we propose an approximate Bayesian approach to handle unit nonresponse\nwith parametric model assumptions on the response probability, but without\nmodel assumptions for the outcome variable. The proposed Bayesian method is\ncalibrated to the frequentist inference in that the credible region obtained\nfrom the posterior distribution asymptotically matches to the frequentist\nconfidence interval obtained from the Taylor linearization method. Unlike the\nfrequentist approach, however, the proposed method does not involve Taylor\nlinearization. The proposed method can be extended to handle over-identified\ncases in which there are more estimating equations than the parameters.\nBesides, the proposed method can also be modified to handle nonignorable\nnonresponse. Results from two simulation studies confirm the validity of the\nproposed methods, which are then applied to data from a Korean longitudinal\nsurvey.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 19:59:51 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Sang", "Hejian", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1702.03476", "submitter": "Stefan Haufe", "authors": "Irene Dowding and Stefan Haufe", "title": "Powerful statistical inference for nested data using sufficient summary\n  statistics", "comments": "17 pages, 5 figures", "journal-ref": "Dowding, I., & Haufe, S. (2018). Powerful Statistical Inference\n  for Nested Data Using Sufficient Summary Statistics. Frontiers in human\n  neuroscience, 12, 103", "doi": "10.3389/fnhum.2018.00103", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchically-organized data arise naturally in many psychology and\nneuroscience studies. As the standard assumption of independent and identically\ndistributed samples does not hold for such data, two important problems are to\naccurately estimate group-level effect sizes, and to obtain powerful\nstatistical tests against group-level null hypotheses. A common approach is to\nsummarize subject-level data by a single quantity per subject, which is often\nthe mean or the difference between class means, and treat these as samples in a\ngroup-level t-test. This 'naive' approach is, however, suboptimal in terms of\nstatistical power, as it ignores information about the intra-subject variance.\nTo address this issue, we review several approaches to deal with nested data,\nwith a focus on methods that are easy to implement. With what we call the\nsufficient-summary-statistic approach, we highlight a computationally efficient\ntechnique that can improve statistical power by taking into account\nwithin-subject variances, and we provide step-by-step instructions on how to\napply this approach to a number of frequently-used measures of effect size. The\nproperties of the reviewed approaches and the potential benefits over a\ngroup-level t-test are quantitatively assessed on simulated data and\ndemonstrated on EEG data from a simulated-driving experiment.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 01:25:56 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 20:15:36 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Dowding", "Irene", ""], ["Haufe", "Stefan", ""]]}, {"id": "1702.03556", "submitter": "Anirvan Chakraborty Dr.", "authors": "Anirvan Chakraborty and Victor M. Panaretos", "title": "Functional Registration and Local Variations: Identifiability, Rank, and\n  Tuning", "comments": "54 pages (Accepted for publication in Bernoulli)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop theory and methodology for the problem of nonparametric\nregistration of functional data that have been subjected to random deformation\n(warping) of their time scale. The separation of this phase variation\n(\"horizontal\" variation) from the amplitude variation (\"vertical\" variation) is\ncrucial in order to properly conduct further analyses, which otherwise can be\nseverely distorted. We determine precise nonparametric conditions under which\nthe two forms of variation are identifiable. These show that the\nidentifiability delicately depends on the underlying rank. By means of several\ncounterexamples, we demonstrate that our conditions are sharp if one wishes a\ngenuinely nonparametric setup; and in doing so we caution that popular remedies\nsuch as structural assumptions or roughness penalties can easily fail. We then\npropose a nonparametric registration method based on a \"local variation\nmeasure\", the main element in elucidating identifiability. A key advantage of\nthe method is that it is free of any tuning or penalisation parameters\nregulating the amount of alignment, thus circumventing the problem of\nover/under-registration often encountered in practice. We provide asymptotic\ntheory for the resulting estimators under the identifiable regime, but also\nunder mild departures from identifiability, quantifying the resulting bias in\nterms of the amplitude variation's spectral gap.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 18:43:29 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 10:17:39 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 17:07:24 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Chakraborty", "Anirvan", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1702.03557", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "Improvements in the Small Sample Efficiency of the Minimum\n  $S$-Divergence Estimators under Discrete Models", "comments": "25 pages, pre-print, submitted to Journal", "journal-ref": "Journal of Statistical Computation and Simulation (2018) Volume\n  88, Issue 3", "doi": "10.1080/00949655.2017.1397150", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of inliers and empty cells and the resulting\nissue of relative inefficiency in estimation under pure samples from a discrete\npopulation when the sample size is small. Many minimum divergence estimators in\nthe $S$-divergence family, although possessing very strong outlier stability\nproperties, often have very poor small sample efficiency in the presence of\ninliers and some are not even defined in the presence of a single empty cell;\nthis limits the practical applicability of these estimators, in spite of their\notherwise sound robustness properties and high asymptotic efficiency. Here, we\nwill study a penalized version of the $S$-divergences such that the resulting\nminimum divergence estimators are free from these issues without altering their\nrobustness properties and asymptotic efficiencies. We will give a general proof\nfor the asymptotic properties of these minimum penalized $S$-divergence\nestimators. This provides a significant addition to the literature as the\nasymptotics of penalized divergences which are not finitely defined are\ncurrently unavailable in the literature. The small sample advantages of the\nminimum penalized $S$-divergence estimators are examined through an extensive\nsimulation study and some empirical suggestions regarding the choice of the\nrelevant underlying tuning parameters are also provided.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 18:44:58 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1702.03578", "submitter": "Daniel Sussman", "authors": "Daniel L. Sussman, Edoardo M. Airoldi", "title": "Elements of estimation theory for causal effects in the presence of\n  network interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments in which the treatment of a unit can affect the\noutcomes of other units are becoming increasingly common in healthcare,\neconomics, and in the social and information sciences. From a causal inference\nperspective, the typical assumption of no interference becomes untenable in\nsuch experiments. In many problems, however, the patterns of interference may\nbe informed by the observation of network connections among the units of\nanalysis. Here, we develop elements of optimal estimation theory for causal\neffects leveraging an observed network, by assuming that the potential outcomes\nof an individual depend only on the individual's treatment and on the treatment\nof the neighbors. We propose a collection of exclusion restrictions on the\npotential outcomes, and show how subsets of these restrictions lead to various\nparameterizations. Considering the class of linear unbiased estimators of the\naverage direct treatment effect, we derive conditions on the design that lead\nto the existence of unbiased estimators, and offer analytical insights on the\nweights that lead to minimum integrated variance estimators. We illustrate the\nimproved performance of these estimators when compared to more standard biased\nand unbiased estimators, using simulations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 21:17:44 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Sussman", "Daniel L.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1702.03597", "submitter": "Vianey Leos Barajas", "authors": "Vianey Leos-Barajas, Eric Gangloff, Timo Adam, Roland Langrock, Floris\n  M. van Beest, Jacob Nabe-Nielsen and Juan M. Morales", "title": "Multi-scale modeling of animal movement and general behavior data using\n  hidden Markov models with hierarchical structures", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) are commonly used to model animal movement data\nand infer aspects of animal behavior. An HMM assumes that each data point from\na time series of observations stems from one of $N$ possible states. The states\nare loosely connected to behavioral modes that manifest themselves at the\ntemporal resolution at which observations are made. However, due to advances in\ntag technology, data can be collected at increasingly fine temporal\nresolutions. Yet, inferences at time scales cruder than those at which data are\ncollected, and which correspond to larger-scale behavioral processes, are not\nyet answered via HMMs. We include additional hierarchical structures to the\nbasic HMM framework in order to incorporate multiple Markov chains at various\ntime scales. The hierarchically structured HMMs allow for behavioral inferences\nat multiple time scales and can also serve as a means to avoid coarsening data.\nOur proposed framework is one of the first that models animal behavior\nsimultaneously at multiple time scales, opening new possibilities in the area\nof animal movement modeling. We illustrate the application of hierarchically\nstructured HMMs in two real-data examples: (i) vertical movements of harbor\nporpoises observed in the field, and (ii) garter snake movement data collected\nas part of an experimental design.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 00:15:02 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Leos-Barajas", "Vianey", ""], ["Gangloff", "Eric", ""], ["Adam", "Timo", ""], ["Langrock", "Roland", ""], ["van Beest", "Floris M.", ""], ["Nabe-Nielsen", "Jacob", ""], ["Morales", "Juan M.", ""]]}, {"id": "1702.03618", "submitter": "Tatsushi Oka", "authors": "Brantly Callaway, Tong Li, Tatsushi Oka", "title": "Quantile Treatment Effects in Difference in Differences Models under\n  Dependence Restrictions and with only Two Time Periods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that the Conditional Quantile Treatment Effect on the\nTreated can be identified using a combination of (i) a conditional\nDistributional Difference in Differences assumption and (ii) an assumption on\nthe conditional dependence between the change in untreated potential outcomes\nand the initial level of untreated potential outcomes for the treated group.\nThe second assumption recovers the unknown dependence from the observed\ndependence for the untreated group. We also consider estimation and inference\nin the case where all of the covariates are discrete. We propose a uniform\ninference procedure based on the exchangeable bootstrap and show its validity.\nWe conclude the paper by estimating the effect of state-level changes in the\nminimum wage on the distribution of earnings for subgroups defined by race,\ngender, and education.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 03:30:57 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Callaway", "Brantly", ""], ["Li", "Tong", ""], ["Oka", "Tatsushi", ""]]}, {"id": "1702.03628", "submitter": "Seongil Jo", "authors": "Ajay Jasra, Seongil Jo, David Nott, Christine Shoemaker, Raul Tempone", "title": "Multilevel Monte Carlo in Approximate Bayesian Computation", "comments": "This paper was submitted in Dec 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following article we consider approximate Bayesian computation (ABC)\ninference. We introduce a method for numerically approximating ABC posteriors\nusing the multilevel Monte Carlo (MLMC). A sequential Monte Carlo version of\nthe approach is developed and it is shown under some assumptions that for a\ngiven level of mean square error, this method for ABC has a lower cost than\ni.i.d. sampling from the most accurate ABC approximation. Several numerical\nexamples are given.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 04:36:14 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Jasra", "Ajay", ""], ["Jo", "Seongil", ""], ["Nott", "David", ""], ["Shoemaker", "Christine", ""], ["Tempone", "Raul", ""]]}, {"id": "1702.03632", "submitter": "James Wilson", "authors": "Jihui Lee, Gen Li and James D. Wilson", "title": "Varying-coefficient models for dynamic networks", "comments": "25 Pages. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks are commonly used in applications where relational data is\nobserved over time. Statistical models for such data should capture not only\nthe temporal dependencies between networks observed in time, but also the\nstructural dependencies among the nodes and edges in each network. As a\nconsequence, effectively making inference on dynamic networks is a\ncomputationally challenging task, and many models established for dynamic\nnetworks are intractable even for moderately sized networks. In this paper, we\npropose and investigate a family of dynamic network models, known as\nvarying-coefficient exponential random graph models (VCERGMs), that\ncharacterize the evolution of network topology through smoothly varying\nparameters in an exponential family of distributions. The VCERGM provides an\ninterpretable dynamic network model that enables the inference of temporal\nheterogeneity in a dynamic network. We establish how to fit the VCERGM through\nmaximum pseudo-likelihood techniques, and thus provide a computationally\ntractable method for statistical inference of complex dynamic networks. We\nfurthermore devise a bootstrap hypothesis testing framework for testing the\ntemporal heterogeneity of an observed dynamic network sequence. We apply the\nVCERGM to the US Congress co-voting network and a resting-state brain\nconnectivity case study and show that our method provides relevant and\ninterpretable patterns describing each data set. Comprehensive simulation\nstudies demonstrate the advantages of our proposed method over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 05:15:28 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 22:34:28 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Lee", "Jihui", ""], ["Li", "Gen", ""], ["Wilson", "James D.", ""]]}, {"id": "1702.03673", "submitter": "Jon Cockayne", "authors": "Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami", "title": "Bayesian Probabilistic Numerical Methods", "comments": null, "journal-ref": "SIAM Review 61(4):756--789, 2019", "doi": "10.1137/17M1139357", "report-no": null, "categories": "stat.ME cs.NA math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergent field of probabilistic numerics has thus far lacked clear\nstatistical principals. This paper establishes Bayesian probabilistic numerical\nmethods as those which can be cast as solutions to certain inverse problems\nwithin the Bayesian framework. This allows us to establish general conditions\nunder which Bayesian probabilistic numerical methods are well-defined,\nencompassing both non-linear and non-Gaussian models. For general computation,\na numerical approximation scheme is proposed and its asymptotic convergence\nestablished. The theoretical development is then extended to pipelines of\ncomputation, wherein probabilistic numerical methods are composed to solve more\nchallenging numerical tasks. The contribution highlights an important research\nfrontier at the interface of numerical analysis and uncertainty quantification,\nwith a challenging industrial application presented.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 08:52:58 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 13:58:53 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Cockayne", "Jon", ""], ["Oates", "Chris", ""], ["Sullivan", "Tim", ""], ["Girolami", "Mark", ""]]}, {"id": "1702.03696", "submitter": "Ben Swallow", "authors": "B. Swallow, M. Rigby, J.C. Rougier, A.J. Manning, M. Lunt and S.\n  O'Doherty", "title": "Parametric uncertainty in complex environmental models: a cheap\n  emulation approach for models with high-dimensional output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to understand underlying processes governing environmental and\nphysical processes, and predict future outcomes, a complex computer model is\nfrequently required to simulate these dynamics. However there is inevitably\nuncertainty related to the exact parametric form or the values of such\nparameters to be used when developing these simulators, with \\emph{ranges} of\nplausible values prevalent in the literature. Systematic errors introduced by\nfailing to account for these uncertainties have the potential to have a large\neffect on resulting estimates in unknown quantities of interest. Due to the\ncomplexity of these types of models, it is often unfeasible to run large\nnumbers of training runs that are usually required for full statistical\nemulators of the environmental processes. We therefore present a method for\naccounting for uncertainties in complex environmental simulators without the\nneed for very large numbers of training runs and illustrate the method through\nan application to the Met Office's atmospheric transport model NAME. We\nconclude that there are two principle parameters that are linked with\nvariability in NAME outputs, namely the free tropospheric turbulence parameter\nand particle release height. Our results suggest the former should be\nsignificantly larger than is currently implemented as a default in NAME, whilst\nchanges in the latter most likely stem from inconsistencies between the model\nspecified ground height at the observation locations and the true height at\nthis location. Estimated discrepancies from independent data are consistent\nwith the discrepancy between modelled and true ground height.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 10:06:07 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Swallow", "B.", ""], ["Rigby", "M.", ""], ["Rougier", "J. C.", ""], ["Manning", "A. J.", ""], ["Lunt", "M.", ""], ["O'Doherty", "S.", ""]]}, {"id": "1702.03877", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Kun Zhang, Shyam Visweswaran", "title": "Approximate Kernel-based Conditional Independence Tests for Fast\n  Non-Parametric Causal Discovery", "comments": "R package: github.com/ericstrobl/RCIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based causal discovery (CCD) algorithms require fast and accurate\nconditional independence (CI) testing. The Kernel Conditional Independence Test\n(KCIT) is currently one of the most popular CI tests in the non-parametric\nsetting, but many investigators cannot use KCIT with large datasets because the\ntest scales cubicly with sample size. We therefore devise two relaxations\ncalled the Randomized Conditional Independence Test (RCIT) and the Randomized\nconditional Correlation Test (RCoT) which both approximate KCIT by utilizing\nrandom Fourier features. In practice, both of the proposed tests scale linearly\nwith sample size and return accurate p-values much faster than KCIT in the\nlarge sample size context. CCD algorithms run with RCIT or RCoT also return\ngraphs at least as accurate as the same algorithms run with KCIT but with large\nreductions in run time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 17:07:29 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 02:40:31 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Strobl", "Eric V.", ""], ["Zhang", "Kun", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1702.03912", "submitter": "Lucas Gabriel Souza Fran\\c{c}a", "authors": "Lucas Gabriel Souza Fran\\c{c}a, Pedro Montoya, Jos\\'e Garcia Vivas\n  Miranda", "title": "On multifractals: a non-linear study of actigraphy data", "comments": "Preprint accepted for publication at Physica A: Statistical Mechanics\n  and its Applications", "journal-ref": null, "doi": "10.1016/j.physa.2018.09.122", "report-no": null, "categories": "nlin.CD nlin.AO physics.data-an q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aimed, to determine the characteristics of activity series from\nfractal geometry concepts application, in addition to evaluate the possibility\nof identifying individuals with fibromyalgia. Activity level data were\ncollected from 27 healthy subjects and 27 fibromyalgia patients, with the use\nof clock-like devices equipped with accelerometers, for about four weeks, all\nday long. The activity series were evaluated through fractal and multifractal\nmethods. Hurst exponent analysis exhibited values according to other studies\n($H>0.5$) for both groups ($H=0.98\\pm0.04$ for healthy subjects and\n$H=0.97\\pm0.03$ for fibromyalgia patients), however, it is not possible to\ndistinguish between the two groups by such analysis. Activity time series also\nexhibited a multifractal pattern. A paired analysis of the spectra indices for\nthe sleep and awake states revealed differences between healthy subjects and\nfibromyalgia patients. The individuals feature differences between awake and\nsleep states, having statistically significant differences for $\\alpha_{q-} -\n\\alpha_{0}$ in healthy subjects ($p = 0.014$) and $D_{0}$ for patients with\nfibromyalgia ($p = 0.013$). The approach has proven to be an option on the\ncharacterisation of such kind of signals and was able to differ between both\nhealthy and fibromyalgia groups. This outcome suggests changes in the\nphysiologic mechanisms of movement control.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 18:32:13 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 13:34:43 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Fran\u00e7a", "Lucas Gabriel Souza", ""], ["Montoya", "Pedro", ""], ["Miranda", "Jos\u00e9 Garcia Vivas", ""]]}, {"id": "1702.03951", "submitter": "Shu Yang", "authors": "Shu Yang, Linbo Wang, and Peng Ding", "title": "Causal inference with confounders missing not at random", "comments": "42 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to draw causal inference from observational studies, which,\nhowever, becomes challenging if the confounders have missing values. Generally,\ncausal effects are not identifiable if the confounders are missing not at\nrandom. We propose a novel framework to nonparametrically identify causal\neffects with confounders subject to an outcome-independent missingness, that\nis, the missing data mechanism is independent of the outcome, given the\ntreatment and possibly missing confounders. We then propose a nonparametric\ntwo-stage least squares estimator and a parametric estimator for causal\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 19:07:05 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 15:08:44 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 16:16:21 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Yang", "Shu", ""], ["Wang", "Linbo", ""], ["Ding", "Peng", ""]]}, {"id": "1702.03967", "submitter": "Franz Hamilton", "authors": "Joseph Arthur, Adam Attarian, Franz Hamilton, Hien Tran", "title": "Nonlinear Kalman Filtering for Censored Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Kalman filtering, as well as its nonlinear extensions, for the\nestimation of system variables and parameters has played a pivotal role in many\nfields of scientific inquiry where observations of the system are restricted to\na subset of variables. However in the case of censored observations, where\nmeasurements of the system beyond a certain detection point are impossible, the\nestimation problem is complicated. Without appropriate consideration, censored\nobservations can lead to inaccurate estimates. Motivated by the work of [1], we\ndevelop a modified version of the extended Kalman filter to handle the case of\ncensored observations in nonlinear systems. We validate this methodology in a\nsimple oscillator system first, showing its ability to accurately reconstruct\nstate variables and track system parameters when observations are censored.\nFinally, we utilize the nonlinear censored filter to analyze censored datasets\nfrom patients with hepatitis C and human immunodeficiency virus.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 20:04:11 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Arthur", "Joseph", ""], ["Attarian", "Adam", ""], ["Hamilton", "Franz", ""], ["Tran", "Hien", ""]]}, {"id": "1702.04025", "submitter": "Geoffrey Webb", "authors": "Geoffrey I. Webb and Mark van der Laan", "title": "Controlling Familywise Error When Rejecting at Most One Null Hypothesis\n  Each From a Sequence of Sub-Families of Null Hypotheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a procedure for controlling FWER when sequentially considering\nsuccessive subfamilies of null hypotheses and rejecting at most one from each\nsubfamily. Our procedure differs from previous procedures for controlling FWER\nby adjusting the critical values that are applied in subsequent rejection\ndecisions by subtracting from the global significance level $\\alpha$ quantities\nbased on the p-values of rejected null hypotheses and the numbers of null\nhypotheses considered.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 23:50:54 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Webb", "Geoffrey I.", ""], ["van der Laan", "Mark", ""]]}, {"id": "1702.04031", "submitter": "Caroline Uhler", "authors": "Steffen Lauritzen, Caroline Uhler, and Piotr Zwiernik", "title": "Maximum likelihood estimation in Gaussian models under total positivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of maximum likelihood estimation for Gaussian\ndistributions that are multivariate totally positive of order two (MTP2). By\nexploiting connections to phylogenetics and single-linkage clustering, we give\na simple proof that the maximum likelihood estimator (MLE) for such\ndistributions exists based on at least 2 observations, irrespective of the\nunderlying dimension. Slawski and Hein, who first proved this result, also\nprovided empirical evidence showing that the MTP2 constraint serves as an\nimplicit regularizer and leads to sparsity in the estimated inverse covariance\nmatrix, determining what we name the ML graph. We show that we can find an\nupper bound for the ML graph by adding edges corresponding to correlations in\nexcess of those explained by the maximum weight spanning forest of the\ncorrelation matrix. Moreover, we provide globally convergent coordinate descent\nalgorithms for calculating the MLE under the MTP2 constraint which are\nstructurally similar to iterative proportional scaling. We conclude the paper\nwith a discussion of signed MTP2 distributions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 00:34:06 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 07:31:07 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 12:26:42 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Lauritzen", "Steffen", ""], ["Uhler", "Caroline", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1702.04329", "submitter": "Ali Reza Fotouhi", "authors": "Ali Reza Fotouhi", "title": "Analysis of extreme values with random location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of the rare and extreme values through statistical modeling is an\nimportant issue in economical crises, climate forecasting, and risk management\nof financial portfolios. Extreme value theory provides the probability models\nneeded for statistical modeling of the extreme values. There are generally two\nways to identifying the extreme values in a data set, the block-maxima and the\npeak-over threshold method. The block-maxima method uses the Generalized\nExtreme Value distribution and the peak-over threshold method uses the\nGeneralized Pareto distribution. It is common that the location of these\ndistributions kept fixed. It is possible that some unobserved variables produce\nheterogeneity in the location of the assumed distribution. In this article we\nfocus on modeling this unobserved heterogeneity in block-maxima method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 18:32:50 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Fotouhi", "Ali Reza", ""]]}, {"id": "1702.04330", "submitter": "Yunbo Ouyang", "authors": "Yunbo Ouyang, Feng Liang", "title": "A Nonparametric Bayesian Approach for Sparse Sequence Estimation", "comments": "Revise technical conditions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric Bayes approach is proposed for the problem of estimating a\nsparse sequence based on Gaussian random variables. We adopt the popular\ntwo-group prior with one component being a point mass at zero, and the other\ncomponent being a mixture of Gaussian distributions. Although the Gaussian\nfamily has been shown to be suboptimal for this problem, we find that Gaussian\nmixtures, with a proper choice on the means and mixing weights, have the\ndesired asymptotic behavior, e.g., the corresponding posterior concentrates on\nballs with the desired minimax rate. To achieve computation efficiency, we\npropose to obtain the posterior distribution using a deterministic variational\nalgorithm. Empirical studies on several benchmark data sets demonstrate the\nsuperior performance of the proposed algorithm compared to other alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 18:34:10 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 01:25:45 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ouyang", "Yunbo", ""], ["Liang", "Feng", ""]]}, {"id": "1702.04430", "submitter": "Yuya Sasaki", "authors": "Harold D. Chiang, Yu-Chin Hsu, Yuya Sasaki", "title": "Robust Uniform Inference for Quantile Treatment Effects in Regression\n  Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical importance of inference with robustness against large\nbandwidths for causal effects in regression discontinuity and kink designs is\nwidely recognized. Existing robust methods cover many cases, but do not handle\nuniform inference for CDF and quantile processes in fuzzy designs, despite its\nuse in the recent literature in empirical microeconomics. In this light, this\npaper extends the literature by developing a unified framework of inference\nwith robustness against large bandwidths that applies to uniform inference for\nquantile treatment effects in fuzzy designs, as well as all the other cases of\nsharp/fuzzy mean/quantile regression discontinuity/kink designs. We present\nMonte Carlo simulation studies and an empirical application for evaluations of\nthe Oklahoma pre-K program.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 00:29:29 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 22:33:59 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 18:31:24 GMT"}, {"version": "v4", "created": "Sat, 14 Apr 2018 19:03:09 GMT"}, {"version": "v5", "created": "Sat, 23 Feb 2019 16:05:32 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Chiang", "Harold D.", ""], ["Hsu", "Yu-Chin", ""], ["Sasaki", "Yuya", ""]]}, {"id": "1702.04473", "submitter": "Thai Pham", "authors": "Thai Pham", "title": "Balancing Method for High Dimensional Causal Inference", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference has received great attention across different fields from\neconomics, statistics, education, medicine, to machine learning. Within this\narea, inferring causal effects at individual level in observational studies has\nbecome an important task, especially in high dimensional settings. In this\npaper, we propose a framework for estimating Individualized Treatment Effects\nin high-dimensional non-experimental data. We provide both theoretical and\nempirical justifications, the latter by comparing our framework with current\nbest-performing methods. Our proposed framework rivals the state-of-the-art\nmethods in most settings and even outperforms them while being much simpler and\neasier to implement.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 06:28:58 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Pham", "Thai", ""]]}, {"id": "1702.04552", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Nirian Martin, Ayanendranath Basu and Leandro Pardo", "title": "A new class of robust two-sample Wald-type tests", "comments": "32 pages, Submitted to journal", "journal-ref": "The International Journal of Biostatistics (2018)", "doi": "10.1515/ijb-2017-0023", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric hypothesis testing associated with two independent samples arises\nfrequently in several applications in biology, medical sciences, epidemiology,\nreliability and many more. In this paper, we propose robust Wald-type tests for\ntesting such two sample problems using the minimum density power divergence\nestimators of the underlying parameters. In particular, we consider the simple\ntwo-sample hypothesis concerning the full parametric homogeneity of the samples\nas well as the general two-sample (composite) hypotheses involving nuisance\nparameters also. The asymptotic and theoretical robustness properties of the\nproposed Wald-type tests have been developed for both the simple and general\ncomposite hypotheses. Some particular cases of testing against one-sided\nalternatives are discussed with specific attention to testing the effectiveness\nof a treatment in clinical trials. Performances of the proposed tests have also\nbeen illustrated numerically through appropriate real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 11:26:02 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Martin", "Nirian", ""], ["Basu", "Ayanendranath", ""], ["Pardo", "Leandro", ""]]}, {"id": "1702.04570", "submitter": "Jinzhu Jia", "authors": "Deqiang Zheng, Jinzhu Jia, Xiangzhong Fang and Xiuhua Guo", "title": "Main and Interaction Effects Selection for Quadratic Discriminant\n  Analysis via Penalized Linear Regression", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminant analysis is a useful classification method. Variable selection\nfor discriminant analysis is becoming more and more im- portant in a\nhigh-dimensional setting. This paper is concerned with the binary-class\nproblems of main and interaction effects selection for the quadratic\ndiscriminant analysis. We propose a new penalized quadratic discriminant\nanalysis (QDA) for variable selection in binary classification. Under sparsity\nassumption on the relevant variables, we conduct a penalized liner regression\nto derive sparse QDA by plug- ging the main and interaction effects in the\nmodel. Then the QDA problem is converted to a penalized sparse ordinary least\nsquares op- timization by using the composite absolute penalties (CAP). Coor-\ndinate descent algorithm is introduced to solve the convex penalized least\nsquares. The penalized linear regression can simultaneously se- lect the main\nand interaction effects, and also conduct classification. Compared with the\nexisting methods of variable selection in QDA, the extensive simulation studies\nand two real data analyses demon- strate that our proposed method works well\nand is robust in the performance of variable selection and classification.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 12:13:05 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Zheng", "Deqiang", ""], ["Jia", "Jinzhu", ""], ["Fang", "Xiangzhong", ""], ["Guo", "Xiuhua", ""]]}, {"id": "1702.04625", "submitter": "Takuya Ura", "authors": "Liangjun Su, Takuya Ura, Yichong Zhang", "title": "Non-separable Models with High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies non-separable models with a continuous treatment when the\ndimension of the control variables is high and potentially larger than the\neffective sample size. We propose a three-step estimation procedure to estimate\nthe average, quantile, and marginal treatment effects. In the first stage we\nestimate the conditional mean, distribution, and density objects by penalized\nlocal least squares, penalized local maximum likelihood estimation, and\nnumerical differentiation, respectively, where control variables are selected\nvia a localized method of L1-penalization at each value of the continuous\ntreatment. In the second stage we estimate the average and marginal\ndistribution of the potential outcome via the plug-in principle. In the third\nstage, we estimate the quantile and marginal treatment effects by inverting the\nestimated distribution function and using the local linear regression,\nrespectively. We study the asymptotic properties of these estimators and\npropose a weighted-bootstrap method for inference. Using simulated and real\ndatasets, we demonstrate that the proposed estimators perform well in finite\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 14:36:44 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 11:47:54 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 01:04:36 GMT"}, {"version": "v4", "created": "Wed, 6 Mar 2019 03:40:32 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Su", "Liangjun", ""], ["Ura", "Takuya", ""], ["Zhang", "Yichong", ""]]}, {"id": "1702.04682", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz, Oleksandr Savenkov, and Karla Ballman", "title": "Targeted Learning Ensembles for Optimal Individualized Treatment Rules\n  with Time-to-Event Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of an optimal individualized treatment rule from\nobservational and randomized studies when a high-dimensional vector of baseline\nvariables is available. Our optimality criterion is with respect to delaying\nexpected time to occurrence of an event of interest (e.g., death or relapse of\ncancer). We leverage semiparametric efficiency theory to construct estimators\nwith desirable properties such as double robustness. We propose two estimators\nof the optimal rule, which arise from considering two loss functions aimed at\n(i) directly estimating the conditional treatment effect (also know as the blip\nfunction), and (ii) recasting the problem as a weighted classification problem\nthat uses the 0-1 loss function. Our estimated rules are super learning\nensembles that minimize the cross-validated risk of a linear combination in a\nuser-supplied library of candidate estimators. We prove oracle inequalities\nbounding the finite sample excess risk of the estimator. The bounds depend on\nthe excess risk of the oracle selector and a doubly robust term related to\nestimation of the nuisance parameters. We discuss some important implications\nof these oracle inequalities such as the convergence rates of the value of our\nestimator to that of the oracle selector. We illustrate our methods in the\nanalysis of a phase III randomized study testing the efficacy of a new therapy\nfor the treatment of breast cancer.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 16:55:56 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 15:28:48 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Savenkov", "Oleksandr", ""], ["Ballman", "Karla", ""]]}, {"id": "1702.04755", "submitter": "Jingxiang Chen", "authors": "Jingxiang Chen, Haoda Fu, Xuanyao He, Michael R. Kosorok, and Yufeng\n  Liu", "title": "Estimating Individualized Treatment Rules for Ordinal Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision medicine is an emerging scientific topic for disease treatment and\nprevention that takes into account individual patient characteristics. It is an\nimportant direction for clinical research, and many statistical methods have\nbeen recently proposed. One of the primary goals of precision medicine is to\nobtain an optimal individual treatment rule (ITR), which can help make\ndecisions on treatment selection according to each patient's specific\ncharacteristics. Recently, outcome weighted learning (OWL) has been proposed to\nestimate such an optimal ITR in a binary treatment setting by maximizing the\nexpected clinical outcome. However, for ordinal treatment settings, such as\nindividualized dose finding, it is unclear how to use OWL. In this paper, we\npropose a new technique for estimating ITR with ordinal treatments. In\nparticular, we propose a data duplication technique with a piecewise convex\nloss function. We establish Fisher consistency for the resulting estimated ITR\nunder certain conditions, and obtain the convergence and risk bound properties.\nSimulated examples and two applications to datasets from an irritable bowel\nproblem and a type 2 diabetes mellitus observational study demonstrate the\nhighly competitive performance of the proposed method compared to existing\nalternatives.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 19:54:38 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Chen", "Jingxiang", ""], ["Fu", "Haoda", ""], ["He", "Xuanyao", ""], ["Kosorok", "Michael R.", ""], ["Liu", "Yufeng", ""]]}, {"id": "1702.05008", "submitter": "Malte Nalenz", "authors": "Malte Nalenz and Mattias Villani", "title": "Tree Ensembles with Rule Structured Horseshoe Regularization", "comments": "24 pages. R package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Bayesian model for flexible nonlinear regression and\nclassification using tree ensembles. The model is based on the RuleFit approach\nin Friedman and Popescu (2008) where rules from decision trees and linear terms\nare used in a L1-regularized regression. We modify RuleFit by replacing the\nL1-regularization by a horseshoe prior, which is well known to give aggressive\nshrinkage of noise predictor while leaving the important signal essentially\nuntouched. This is especially important when a large number of rules are used\nas predictors as many of them only contribute noise. Our horseshoe prior has an\nadditional hierarchical layer that applies more shrinkage a priori to rules\nwith a large number of splits, and to rules that are only satisfied by a few\nobservations. The aggressive noise shrinkage of our prior also makes it\npossible to complement the rules from boosting in Friedman and Popescu (2008)\nwith an additional set of trees from random forest, which brings a desirable\ndiversity to the ensemble. We sample from the posterior distribution using a\nvery efficient and easily implemented Gibbs sampler. The new model is shown to\noutperform state-of-the-art methods like RuleFit, BART and random forest on 16\ndatasets. The model and its interpretation is demonstrated on the well known\nBoston housing data, and on gene expression data for cancer classification. The\nposterior sampling, prediction and graphical tools for interpreting the model\nresults are implemented in a publicly available R package.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 15:16:27 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 14:47:21 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Nalenz", "Malte", ""], ["Villani", "Mattias", ""]]}, {"id": "1702.05056", "submitter": "Yunbo Ouyang", "authors": "Yunbo Ouyang, Feng Liang", "title": "An Empirical Bayes Approach for High Dimensional Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an empirical Bayes estimator based on Dirichlet process mixture\nmodel for estimating the sparse normalized mean difference, which could be\ndirectly applied to the high dimensional linear classification. In theory, we\nbuild a bridge to connect the estimation error of the mean difference and the\nmisclassification error, also provide sufficient conditions of sub-optimal\nclassifiers and optimal classifiers. In implementation, a variational Bayes\nalgorithm is developed to compute the posterior efficiently and could be\nparallelized to deal with the ultra-high dimensional case.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 17:11:01 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Ouyang", "Yunbo", ""], ["Liang", "Feng", ""]]}, {"id": "1702.05189", "submitter": "Paul Kabaila", "authors": "Paul Kabaila", "title": "Upper bounds on the minimum coverage probability of model averaged tail\n  area confidence intervals in regression", "comments": null, "journal-ref": "Canadian Journal of Statistics 2018", "doi": "10.1002/cjs.11349", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequentist model averaging has been proposed as a method for incorporating\n\"model uncertainty\" into confidence interval construction. Such proposals have\nbeen of particular interest in the environmental and ecological statistics\ncommunities. A promising method of this type is the model averaged tail area\n(MATA) confidence interval put forward by Turek and Fletcher, 2012. The\nperformance of this interval depends greatly on the data-based model weights on\nwhich it is based. A computationally convenient formula for the coverage\nprobability of this interval is provided by Kabaila, Welsh and Abeysekera,\n2016, in the simple scenario of two nested linear regression models. We\nconsider the more complicated scenario that there are many (32,768 in the\nexample considered) linear regression models obtained as follows. For each of a\nspecified set of components of the regression parameter vector, we either set\nthe component to zero or let it vary freely. We provide an easily-computed\nupper bound on the minimum coverage probability of the MATA confidence\ninterval. This upper bound provides evidence against the use of a model weight\nbased on the Bayesian Information Criterion (BIC).\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 23:56:19 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Kabaila", "Paul", ""]]}, {"id": "1702.05195", "submitter": "Xianyang Zhang", "authors": "Xianyang Zhang and Anirban Bhattacharya", "title": "Empirical Bayes, SURE and Sparse Normal Mean Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the sparse normal mean models under the empirical Bayes\nframework. We focus on the mixture priors with an atom at zero and a density\ncomponent centered at a data driven location determined by maximizing the\nmarginal likelihood or minimizing the Stein Unbiased Risk Estimate. We study\nthe properties of the corresponding posterior median and posterior mean. In\nparticular, the posterior median is a thresholding rule and enjoys the\nmulti-direction shrinkage property that shrinks the observation toward either\nthe origin or the data-driven location. The idea is extended by considering a\nfinite mixture prior, which is flexible to model the cluster structure of the\nunknown means. We further generalize the results to heteroscedastic normal mean\nmodels. Specifically, we propose a semiparametric estimator which can be\ncalculated efficiently by combining the familiar EM algorithm with the\nPool-Adjacent-Violators algorithm for isotonic regression. The effectiveness of\nour methods is demonstrated via extensive numerical studies.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 00:38:13 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Zhang", "Xianyang", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1702.05340", "submitter": "Praneeth Vepakomma Praneeth Vepakomma", "authors": "Praneeth Vepakomma, Yulia Kempner", "title": "Combinatorics of Distance Covariance: Inclusion-Minimal Maximizers of\n  Quasi-Concave Set Functions for Diverse Variable Selection", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the negative sample distance covariance function\nis a quasi-concave set function of samples of random variables that are not\nstatistically independent. We use these properties to propose greedy algorithms\nto combinatorially optimize some diversity (low statistical dependence)\npromoting functions of distance covariance. Our greedy algorithm obtains all\nthe inclusion-minimal maximizers of this diversity promoting objective.\nInclusion-minimal maximizers are multiple solution sets of globally optimal\nmaximizers that are not a proper subset of any other maximizing set in the\nsolution set. We present results upon applying this approach to obtain diverse\nfeatures (covariates/variables/predictors) in a feature selection setting for\nregression (or classification) problems. We also combine our diverse feature\nselection algorithm with a distance covariance based relevant feature selection\nalgorithm of [7] to produce subsets of covariates that are both relevant yet\nordered in non-increasing levels of diversity of these subsets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 13:53:15 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Kempner", "Yulia", ""]]}, {"id": "1702.05462", "submitter": "Fabrizio Leisen", "authors": "Laurentiu Hinoveanu, Fabrizio Leisen and Cristiano Villa", "title": "Objective Bayesian Analysis for Change Point Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a loss-based approach to change point analysis. In\nparticular, we look at the problem from two perspectives. The first focuses on\nthe definition of a prior when the number of change points is known a priori.\nThe second contribution aims to estimate the number of change points by using a\nloss-based approach recently introduced in the literature. The latter considers\nchange point estimation as a model selection exercise. We show the performance\nof the proposed approach on simulated data and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 18:06:27 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 13:58:48 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hinoveanu", "Laurentiu", ""], ["Leisen", "Fabrizio", ""], ["Villa", "Cristiano", ""]]}, {"id": "1702.05829", "submitter": "Arturo Erdely", "authors": "Arturo Erdely", "title": "Copula-based piecewise regression", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most common parametric families of copulas are totally ordered, and in many\ncases they are also positively or negatively regression dependent and therefore\nthey lead to monotone regression functions, which makes them not suitable for\ndependence relationships that imply or suggest a non-monotone regression\nfunction. A gluing copula approach is proposed to decompose the underlying\ncopula into totally ordered copulas that combined may lead to a non-monotone\nregression function.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 01:34:39 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 00:37:27 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Erdely", "Arturo", ""]]}, {"id": "1702.05832", "submitter": "Adrijo Chakraborty", "authors": "Adrijo Chakraborty, Gauri Sankar Datta, Abhyuday Mandal", "title": "Robust Hierarchical Bayes Small Area Estimation for Nested Error\n  Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National statistical institutes in many countries are now mandated to produce\nreliable statistics for important variables such as population, income,\nunemployment, health outcomes, etc. for small areas, defined by geography\nand/or demography. Due to small samples from these areas, direct sample-based\nestimates are often unreliable. Model-based small area estimation is now\nextensively used to generate reliable statistics by \"borrowing strength\" from\nother areas and related variables through suitable models. Outliers adversely\ninfluence standard model-based small area estimates. To deal with outliers,\nSinha and Rao (2009) proposed a robust frequentist approach. In this article,\nwe present a robust Bayesian alternative to the nested error regression model\nfor unit-level data to mitigate outliers. We consider a two-component scale\nmixture of normal distributions for the unit-level error to model outliers and\npresent a computational approach to produce Bayesian predictors of small area\nmeans under a noninformative prior for model parameters. A real example and\nextensive simulations convincingly show robustness of our Bayesian predictors\nto outliers. Simulations comparison of these two procedures with Bayesian\npredictors by Datta and Ghosh (1991) and M-quantile estimators by Chambers et\nal. (2014) shows that our proposed procedure is better than the others in terms\nof bias, variability, and coverage probability of prediction intervals, when\nthere are outliers. The superior frequentist performance of our procedure shows\nits dual (Bayes and frequentist) dominance, and makes it attractive to all\npractitioners, both Bayesian and frequentist, of small area estimation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 01:50:34 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 02:19:52 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Chakraborty", "Adrijo", ""], ["Datta", "Gauri Sankar", ""], ["Mandal", "Abhyuday", ""]]}, {"id": "1702.05879", "submitter": "Tania Roy", "authors": "Fushing Hsieh, Tania Roy", "title": "Complexity of Possibly-gapped Histogram and Analysis of Histogram\n  (ANOHT)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without unrealistic continuity and smoothness assumptions on a distributional\ndensity of one dimensional dataset, constructing an authentic possibly-gapped\nhistogram becomes rather complex. The candidate ensemble is described via a\ntwo-layer Ising model, and its size is shown to grow exponentially. This\nexponential complexity makes any exhaustive search in-feasible and all boundary\nparameters local. For data compression via Uniformity, the decoding error\ncriterion is nearly independent of sample size. These characteristics nullify\nstatistical model selection techniques, such as Minimum Description Length\n(MDL). Nonetheless practical and nearly optimal solutions are algorithmically\ncomputable. A data-driven algorithm is devised to construct such histograms\nalong the branching hierarchy of a Hierarchical Clustering tree. Such resultant\nhistograms naturally manifest data's physical information contents:\ndeterministic structures of bin-boundaries coupled with stochastic structures\nof Uniformity within each bin. Without enforcing unrealistic Normality and\nconstant variance assumptions, an application of possibly-gapped histogram is\ndevised, called analysis of Histogram (ANOHT), to replace Analysis of Variance\n(ANOVA). Its potential applications are foreseen in digital re-normalization\nschemes and associative pattern extraction among features of heterogeneous data\ntypes. Thus constructing possibly-gapped histograms becomes a prerequisite for\nknowledge discovery, via exploratory data analysis and unsupervised Machine\nLearning.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 07:07:05 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 06:35:05 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 02:07:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hsieh", "Fushing", ""], ["Roy", "Tania", ""]]}, {"id": "1702.05960", "submitter": "Jun Fan", "authors": "Yunlong Feng, Jun Fan, and Johan A.K. Suykens", "title": "A Statistical Learning Approach to Modal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the nonparametric modal regression problem systematically\nfrom a statistical learning view. Originally motivated by pursuing a\ntheoretical understanding of the maximum correntropy criterion based regression\n(MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is\nessentially modal regression. We show that nonparametric modal regression\nproblem can be approached via the classical empirical risk minimization. Some\nefforts are then made to develop a framework for analyzing and implementing\nmodal regression. For instance, the modal regression function is described, the\nmodal regression risk is defined explicitly and its \\textit{Bayes} rule is\ncharacterized; for the sake of computational tractability, the surrogate modal\nregression risk, which is termed as the generalization risk in our study, is\nintroduced. On the theoretical side, the excess modal regression risk, the\nexcess generalization risk, the function estimation error, and the relations\namong the above three quantities are studied rigorously. It turns out that\nunder mild conditions, function estimation consistency and convergence may be\npursued in modal regression as in vanilla regression protocols, such as mean\nregression, median regression, and quantile regression. However, it outperforms\nthese regression models in terms of robustness as shown in our study from a\nre-descending M-estimation view. This coincides with and in return explains the\nmerits of MCCR on robustness. On the practical side, the implementation issues\nof modal regression including the computational algorithm and the tuning\nparameters selection are discussed. Numerical assessments on modal regression\nare also conducted to verify our findings empirically.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 13:31:39 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 18:22:26 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 06:39:00 GMT"}, {"version": "v4", "created": "Fri, 17 Jan 2020 05:29:25 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Feng", "Yunlong", ""], ["Fan", "Jun", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1702.05972", "submitter": "Sarah Elizabeth Heaps", "authors": "Sarah E. Heaps, Tom M. W. Nye, Richard J. Boys, Tom A. Williams,\n  Svetlana Cherlin, T. Martin Embley", "title": "Generalising rate heterogeneity across sites in statistical\n  phylogenetics", "comments": "13 figures, 1 accompanying file of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetics uses alignments of molecular sequence data to learn about\nevolutionary trees relating species. Along branches, sequence evolution is\nmodelled using a continuous-time Markov process characterised by an\ninstantaneous rate matrix. Early models assumed the same rate matrix governed\nsubstitutions at all sites of the alignment, ignoring variation in evolutionary\npressures. Substantial improvements in phylogenetic inference and model fit\nwere achieved by augmenting these models with multiplicative random effects\nthat describe the result of variation in selective constraints and allow sites\nto evolve at different rates which linearly scale a baseline rate matrix.\nMotivated by this pioneering work, we consider an extension using a quadratic,\nrather than linear, transformation. The resulting models allow for variation in\nthe selective coefficients of different types of point mutation at a site in\naddition to variation in selective constraints.\n  We derive properties of the extended models. For certain non-stationary\nprocesses, the extension gives a model that allows variation in sequence\ncomposition both across sites and taxa. We adopt a Bayesian approach, describe\nan MCMC algorithm for posterior inference and provide software. Our quadratic\nmodels are applied to alignments spanning the tree of life and compared with\nsite-homogeneous and linear models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 13:56:48 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 12:10:42 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Heaps", "Sarah E.", ""], ["Nye", "Tom M. W.", ""], ["Boys", "Richard J.", ""], ["Williams", "Tom A.", ""], ["Cherlin", "Svetlana", ""], ["Embley", "T. Martin", ""]]}, {"id": "1702.06166", "submitter": "Tammo Rukat", "authors": "Tammo Rukat and Chris C. Holmes and Michalis K. Titsias and\n  Christopher Yau", "title": "Bayesian Boolean Matrix Factorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA q-bio.GN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean matrix factorisation aims to decompose a binary data matrix into an\napproximate Boolean product of two low rank, binary matrices: one containing\nmeaningful patterns, the other quantifying how the observations can be\nexpressed as a combination of these patterns. We introduce the OrMachine, a\nprobabilistic generative model for Boolean matrix factorisation and derive a\nMetropolised Gibbs sampler that facilitates efficient parallel posterior\ninference. On real world and simulated data, our method outperforms all\ncurrently existing approaches for Boolean matrix factorisation and completion.\nThis is the first method to provide full posterior inference for Boolean Matrix\nfactorisation which is relevant in applications, e.g. for controlling false\npositive rates in collaborative filtering and, crucially, improves the\ninterpretability of the inferred patterns. The proposed algorithm scales to\nlarge datasets as we demonstrate by analysing single cell gene expression data\nin 1.3 million mouse brain cells across 11 thousand genes on commodity\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 20:31:39 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 14:17:44 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Rukat", "Tammo", ""], ["Holmes", "Chris C.", ""], ["Titsias", "Michalis K.", ""], ["Yau", "Christopher", ""]]}, {"id": "1702.06220", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami and Daniel A. Griffith", "title": "Eigenvector spatial filtering for large data sets: fixed and random\n  effects approaches", "comments": null, "journal-ref": null, "doi": "10.1111/gean.12156", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigenvector spatial filtering (ESF) is a spatial modeling approach, which has\nbeen applied in urban and regional studies, ecological studies, and so on.\nHowever, it is computationally demanding, and may not be suitable for large\ndata modeling. The objective of this study is developing fast ESF and random\neffects ESF (RE-ESF), which are capable of handling very large samples. To\nachieve it, we accelerate eigen-decomposition and parameter estimation, which\nmake ESF and RE-ESF slow. The former is accelerated by utilizing the Nystrom\nextension, whereas the latter is by small matrix tricks. The resulting fast ESF\nand fast RE-ESF are compared with non-approximated ESF and RE-ESF in Monte\nCarlo simulation experiments. The result shows that, while ESF and RE-ESF are\nslow for several thousand samples, fast ESF and RE-ESF require only several\nseconds for the samples. They also suggest that the proposed approaches\neffectively remove positive spatial dependence in the residuals with very small\napproximation errors when the number of eigenvectors considered is 200 or more.\nNote that these approaches cannot deal with negative spatial dependence. The\nproposed approaches are implemented in an R package \"spmoran.\"\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 00:28:39 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 02:18:14 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Murakami", "Daisuke", ""], ["Griffith", "Daniel A.", ""]]}, {"id": "1702.06221", "submitter": "Joshua Chang", "authors": "Joshua C. Chang", "title": "Determination of hysteresis in finite-state random walks using Bayesian\n  cross validation", "comments": "Reworked as totally different paper in arXiv:1706.08881", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of modeling hysteresis for finite-state random walks\nusing higher-order Markov chains. This Letter introduces a Bayesian framework\nto determine, from data, the number of prior states of recent history upon\nwhich a trajectory is statistically dependent. The general recommendation is to\nuse leave-one-out cross validation, using an easily-computable formula that is\nprovided in closed form. Importantly, Bayes factors using flat model priors are\nbiased in favor of too-complex a model (more hysteresis) when a large amount of\ndata is present and the Akaike information criterion (AIC) is biased in favor\nof too-sparse a model (less hysteresis) when few data are present.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 00:28:39 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 04:47:05 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Chang", "Joshua C.", ""]]}, {"id": "1702.06240", "submitter": "Vira Semenova", "authors": "Vira Semenova, Victor Chernozhukov", "title": "Debiased Machine Learning of Conditional Average Treatment Effects and\n  Other Causal Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides estimation and inference methods for the best linear\npredictor (approximation) of a structural function, such as conditional average\nstructural and treatment effects, and structural derivatives, based on modern\nmachine learning (ML) tools. We represent this structural function as a\nconditional expectation of an unbiased signal that depends on a nuisance\nparameter, which we estimate by modern machine learning techniques. We first\nadjust the signal to make it insensitive (Neyman-orthogonal) with respect to\nthe first-stage regularization bias. We then project the signal onto a set of\nbasis functions, growing with sample size, which gives us the best linear\npredictor of the structural function. We derive a complete set of results for\nestimation and simultaneous inference on all parameters of the best linear\npredictor, conducting inference by Gaussian bootstrap. When the structural\nfunction is smooth and the basis is sufficiently rich, our estimation and\ninference result automatically targets this function. When basis functions are\ngroup indicators, the best linear predictor reduces to group average\ntreatment/structural effect, and our inference automatically targets these\nparameters. We demonstrate our method by estimating uniform confidence bands\nfor the average price elasticity of gasoline demand conditional on income.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 02:23:41 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 15:38:42 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 19:46:22 GMT"}, {"version": "v4", "created": "Sun, 3 May 2020 03:07:49 GMT"}, {"version": "v5", "created": "Thu, 13 Aug 2020 16:25:06 GMT"}, {"version": "v6", "created": "Fri, 14 Aug 2020 14:11:05 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Semenova", "Vira", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "1702.06570", "submitter": "Denise Duarte Scarpa Magalhaes Alves", "authors": "Denise Duarte, Sokol Ndreca and Wecsley O. Prates", "title": "Inference for Stochastically Contaminated Variable Length Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a methodology to estimate the parameters of\nstochastically contaminated models under two contamination regimes. In both\nregimes, we assume that the original process is a variable length Markov chain\nthat is contaminated by a random noise. In the first regime we consider that\nthe random noise is added to the original source and in the second regime, the\nrandom noise is multiplied by the original source. Given a contaminated sample\nof these models, the original process is hidden. Then we propose a two steps\nestimator for the parameters of these models, that is, the probability\ntransitions and the noise parameter, and prove its consistency. The first step\nis an adaptation of the Baum-Welch algorithm for Hidden Markov Models. This\nstep provides an estimate of a complete order $k$ Markov chain, where $k$ is\nbigger than the order of the variable length Markov chain if it has finite\norder and is a constant depending on the sample size if the hidden process has\ninfinite order. In the second estimation step, we propose a bootstrap Bayesian\nInformation Criterion, given a sample of the Markov chain estimated in the\nfirst step, to obtain the variable length time dependence structure associated\nwith the hidden process. We present a simulation study showing that our\nmethodology is able to accurately recover the parameters of the models for a\nreasonable interval of random noises.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 20:23:35 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Duarte", "Denise", ""], ["Ndreca", "Sokol", ""], ["Prates", "Wecsley O.", ""]]}, {"id": "1702.06635", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa", "title": "Robust Empirical Bayes Small Area Estimation with Density Power\n  Divergence", "comments": "29 pages; to appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-stage normal hierarchical model called the Fay--Herriot model and the\nempirical Bayes estimator are widely used to provide indirect and model-based\nestimates of means in small areas. However, the performance of the empirical\nBayes estimator might be poor when the assumed normal distribution is\nmisspecified. In this article, we propose a simple modification by using\ndensity power divergence and suggest a new robust empirical Bayes small area\nestimator. The mean squared error and estimated mean squared error of the\nproposed estimator are derived based on the asymptotic properties of the robust\nestimator of the model parameters. We investigate the numerical performance of\nthe proposed method through simulations and an application to survey data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 01:14:43 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 07:39:24 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 13:24:41 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Sugasawa", "Shonosuke", ""]]}, {"id": "1702.06790", "submitter": "Thomas Ortner", "authors": "Thomas Ortner, Peter Filzmoser, Maia Zaharieva, Christian Breiteneder,\n  Sarka Brodinova", "title": "Guided projections for analysing the structure of high-dimensional data", "comments": "submitted to Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A powerful data transformation method named guided projections is proposed\ncreating new possibilities to reveal the group structure of high-dimensional\ndata in the presence of noise variables. Utilising projections onto a space\nspanned by a selection of a small number of observations allows measuring the\nsimilarity of other observations to the selection based on orthogonal and score\ndistances. Observations are iteratively exchanged from the selection creating a\nnon-random sequence of projections which we call guided projections. In\ncontrast to conventional projection pursuit methods, which typically identify a\nlow-dimensional projection revealing some interesting features contained in the\ndata, guided projections generate a series of projections that serve as a basis\nnot just for diagnostic plots but to directly investigate the group structure\nin data. Based on simulated data we identify the strengths and limitations of\nguided projections in comparison to commonly employed data transformation\nmethods. We further show the relevance of the transformation by applying it to\nreal-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 13:27:02 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ortner", "Thomas", ""], ["Filzmoser", "Peter", ""], ["Zaharieva", "Maia", ""], ["Breiteneder", "Christian", ""], ["Brodinova", "Sarka", ""]]}, {"id": "1702.06986", "submitter": "Jean Morrison", "authors": "Jean Morrison and Noah Simon", "title": "Rank conditional coverage and confidence intervals in high dimensional\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence interval procedures used in low dimensional settings are often\ninappropriate for high dimensional applications. When a large number of\nparameters are estimated, marginal confidence intervals associated with the\nmost significant estimates have very low coverage rates: They are too small and\ncentered at biased estimates. The problem of forming confidence intervals in\nhigh dimensional settings has previously been studied through the lens of\nselection adjustment. In this framework, the goal is to control the proportion\nof non-covering intervals formed for selected parameters.\n  In this paper we approach the problem by considering the relationship between\nrank and coverage probability. Marginal confidence intervals have very low\ncoverage rates for significant parameters and high rates for parameters with\nmore boring estimates. Many selection adjusted intervals display the same\npattern. This connection motivates us to propose a new coverage criterion for\nconfidence intervals in multiple testing/covering problems --- the rank\nconditional coverage (RCC). This is the expected coverage rate of an interval\ngiven the significance ranking for the associated estimator. We propose\ninterval construction via bootstrapping which produces small intervals and have\na rank conditional coverage close to the nominal level. These methods are\nimplemented in the R package rcc.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 19:58:06 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Morrison", "Jean", ""], ["Simon", "Noah", ""]]}, {"id": "1702.07007", "submitter": "Jakob Runge", "authors": "Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino\n  Sejdinovic", "title": "Detecting causal associations in large nonlinear time series datasets", "comments": "46 pages, 19 figures", "journal-ref": "Science Advances Vol. 5, no. 11, eaau4996 (2019)", "doi": "10.1126/sciadv.aau4996", "report-no": null, "categories": "stat.ME physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying causal relationships from observational time series data is a key\nproblem in disciplines such as climate science or neuroscience, where\nexperiments are often not possible. Data-driven causal inference is challenging\nsince datasets are often high-dimensional and nonlinear with limited sample\nsizes. Here we introduce a novel method that flexibly combines linear or\nnonlinear conditional independence tests with a causal discovery algorithm that\nallows to reconstruct causal networks from large-scale time series datasets. We\nvalidate the method on a well-established climatic teleconnection connecting\nthe tropical Pacific with extra-tropical temperatures and using large-scale\nsynthetic datasets mimicking the typical properties of real data. The\nexperiments demonstrate that our method outperforms alternative techniques in\ndetection power from small to large-scale datasets and opens up entirely new\npossibilities to discover causal networks from time series across a range of\nresearch fields.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 21:09:19 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 12:27:48 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Runge", "Jakob", ""], ["Nowack", "Peer", ""], ["Kretschmer", "Marlene", ""], ["Flaxman", "Seth", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1702.07027", "submitter": "Yen-Chi Chen", "authors": "Gang Cheng, Yen-Chi Chen", "title": "Nonparametric Inference via Bootstrapping the Debiased Estimator", "comments": "Accepted to the Electronic Journal of Statistics. 64 pages, 6 tables,\n  11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to construct confidence bands by bootstrapping the\ndebiased kernel density estimator (for density estimation) and the debiased\nlocal polynomial regression estimator (for regression analysis). The idea of\nusing a debiased estimator was recently employed by Calonico et al. (2018b) to\nconstruct a confidence interval of the density function (and regression\nfunction) at a given point by explicitly estimating stochastic variations. We\nextend their ideas of using the debiased estimator and further propose a\nbootstrap approach for constructing simultaneous confidence bands. This\nmodified method has an advantage that we can easily choose the smoothing\nbandwidth from conventional bandwidth selectors and the confidence band will be\nasymptotically valid. We prove the validity of the bootstrap confidence band\nand generalize it to density level sets and inverse regression problems.\nSimulation studies confirm the validity of the proposed confidence bands/sets.\nWe apply our approach to an Astronomy dataset to show its applicability\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 22:21:05 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 20:06:00 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 22:30:55 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Cheng", "Gang", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "1702.07089", "submitter": "Shaoyang Ning", "authors": "Shaoyang Ning and Neil Shephard", "title": "A Nonparametric Bayesian Approach to Copula Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Dirichlet-based P\\'olya tree (D-P tree) prior on the\ncopula and based on the D-P tree prior, a nonparametric Bayesian inference\nprocedure. Through theoretical analysis and simulations, we are able to show\nthat the flexibility of the D-P tree prior ensures its consistency in copula\nestimation, thus able to detect more subtle and complex copula structures than\nearlier nonparametric Bayesian models, such as a Gaussian copula mixture.\nFurther, the continuity of the imposed D-P tree prior leads to a more favorable\nsmoothing effect in copula estimation over classic frequentist methods,\nespecially with small sets of observations. We also apply our method to the\ncopula prediction between the S\\&P 500 index and the IBM stock prices during\nthe 2007-08 financial crisis, finding that D-P tree-based methods enjoy strong\nrobustness and flexibility over classic methods under such irregular market\nbehaviors.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 04:13:42 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Ning", "Shaoyang", ""], ["Shephard", "Neil", ""]]}, {"id": "1702.07269", "submitter": "Mahdi Imani", "authors": "Mahdi Imani and Ulisses Braga-Neto", "title": "Particle Filters for Partially-Observed Boolean Dynamical Systems", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2614798", "report-no": null, "categories": "stat.ME math.DS q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially-observed Boolean dynamical systems (POBDS) are a general class of\nnonlinear models with application in estimation and control of Boolean\nprocesses based on noisy and incomplete measurements. The optimal minimum mean\nsquare error (MMSE) algorithms for POBDS state estimation, namely, the Boolean\nKalman filter (BKF) and Boolean Kalman smoother (BKS), are intractable in the\ncase of large systems, due to computational and memory requirements. To address\nthis, we propose approximate MMSE filtering and smoothing algorithms based on\nthe auxiliary particle filter (APF) method from sequential Monte-Carlo theory.\nThese algorithms are used jointly with maximum-likelihood (ML) methods for\nsimultaneous state and parameter estimation in POBDS models. In the presence of\ncontinuous parameters, ML estimation is performed using the\nexpectation-maximization (EM) algorithm; we develop for this purpose a special\nsmoother which reduces the computational complexity of the EM algorithm. The\nresulting particle-based adaptive filter is applied to a POBDS model of Boolean\ngene regulatory networks observed through noisy RNA-Seq time series data, and\nperformance is assessed through a series of numerical experiments using the\nwell-known cell cycle gene regulatory model.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 15:58:00 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Imani", "Mahdi", ""], ["Braga-Neto", "Ulisses", ""]]}, {"id": "1702.07283", "submitter": "Jonathan P Williams", "authors": "Jonathan P Williams and Jan Hannig", "title": "Non-penalized variable selection in high-dimensional linear model\n  settings via generalized fiducial inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard penalized methods of variable selection and parameter estimation\nrely on the magnitude of coefficient estimates to decide which variables to\ninclude in the final model. However, coefficient estimates are unreliable when\nthe design matrix is collinear. To overcome this challenge an entirely new\nperspective on variable selection is presented within a generalized fiducial\ninference framework. This new procedure is able to effectively account for\nlinear dependencies among subsets of covariates in a high-dimensional setting\nwhere $p$ can grow almost exponentially in $n$, as well as in the classical\nsetting where $p \\le n$. It is shown that the procedure very naturally assigns\nsmall probabilities to subsets of covariates which include redundancies by way\nof explicit $L_{0}$ minimization. Furthermore, with a typical sparsity\nassumption, it is shown that the proposed method is consistent in the sense\nthat the probability of the true sparse subset of covariates converges in\nprobability to 1 as $n \\to \\infty$, or as $n \\to \\infty$ and $p \\to \\infty$.\nVery reasonable conditions are needed, and little restriction is placed on the\nclass of possible subsets of covariates to achieve this consistency result.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 16:43:02 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 23:40:31 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Williams", "Jonathan P", ""], ["Hannig", "Jan", ""]]}, {"id": "1702.07304", "submitter": "Anne Presanis", "authors": "Anne M. Presanis, David Ohlssen, Kai Cui, Magdalena Rosinska and\n  Daniela De Angelis", "title": "Conflict diagnostics for evidence synthesis in a multiple testing\n  framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence synthesis models that combine multiple datasets of varying design,\nto estimate quantities that cannot be directly observed, require the\nformulation of complex probabilistic models that can be expressed as graphical\nmodels. An assessment of whether the different datasets synthesised contribute\ninformation that is consistent with each other, and in a Bayesian context, with\nthe prior distribution, is a crucial component of the model criticism process.\nHowever, a systematic assessment of conflict suffers from the multiple testing\nproblem, through testing for conflict at multiple locations in a model. We\ndemonstrate the systematic use of conflict diagnostics, while accounting for\nthe multiple hypothesis tests of no conflict at each location in the graphical\nmodel. The method is illustrated by a network meta-analysis to estimate\ntreatment effects in smoking cessation programs and an evidence synthesis to\nestimate HIV prevalence in Poland.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 17:44:10 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 16:53:26 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Presanis", "Anne M.", ""], ["Ohlssen", "David", ""], ["Cui", "Kai", ""], ["Rosinska", "Magdalena", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1702.07449", "submitter": "Tianqi Liu", "authors": "Tianqi Liu, Ming Yuan, and Hongyu Zhao", "title": "Characterizing Spatiotemporal Transcriptome of Human Brain via Low Rank\n  Tensor Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal gene expression data of the human brain offer insights on the\nspa- tial and temporal patterns of gene regulation during brain development.\nMost existing methods for analyzing these data consider spatial and temporal\nprofiles separately with the implicit assumption that different brain regions\ndevelop in similar trajectories, and that the spatial patterns of gene\nexpression remain similar at different time points. Al- though these analyses\nmay help delineate gene regulation either spatially or temporally, they are not\nable to characterize heterogeneity in temporal dynamics across different brain\nregions, or the evolution of spatial patterns of gene regulation over time. In\nthis article, we develop a statistical method based on low rank tensor\ndecomposition to more effectively analyze spatiotemporal gene expression data.\nWe generalize the clas- sical principal component analysis (PCA) which is\napplicable only to data matrices, to tensor PCA that can simultaneously capture\nspatial and temporal effects. We also propose an efficient algorithm that\ncombines tensor unfolding and power iteration to estimate the tensor principal\ncomponents, and provide guarantees on their statistical performances. Numerical\nexperiments are presented to further demonstrate the mer- its of the proposed\nmethod. An application of our method to a spatiotemporal brain expression data\nprovides insights on gene regulation patterns in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 02:28:26 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Liu", "Tianqi", ""], ["Yuan", "Ming", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1702.07630", "submitter": "Yannick Deville", "authors": "Charlotte Revel, Yannick Deville, V\\'eronique Achard, Xavier Briottet", "title": "Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a\n  Hyperspectral Unmixing Method Dealing with Intra-class Variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation is a common processing tool to analyse the\nconstitution of pixels of hyperspectral images. Such methods usually suppose\nthat pure pixel spectra (endmembers) are the same in all the image for each\nclass of materials. In the framework of remote sensing, such an assumption is\nno more valid in the presence of intra-class variabilities due to illumination\nconditions, weathering, slight variations of the pure materials, etc... In this\npaper, we first describe the results of investigations highlighting intra-class\nvariability measured in real images. Considering these results, a new\nformulation of the linear mixing model is presented leading to two new methods.\nUnconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation\nmethod based on the assumption of a linear mixing model, which can deal with\nintra-class variability. To overcome UP-NMF limitations an extended method is\nproposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each\nsensed spectrum, these extended versions of NMF extract a corresponding set of\nsource spectra. A constraint is set to limit the spreading of each source's\nestimates in IP-NMF. The methods are tested on a semi-synthetic data set built\nwith spectra extracted from a real hyperspectral image and then numerically\nmixed. We thus demonstrate the interest of our methods for realistic source\nvariabilities. Finally, IP-NMF is tested on a real data set and it is shown to\nyield better performance than state of the art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 15:43:10 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Revel", "Charlotte", ""], ["Deville", "Yannick", ""], ["Achard", "V\u00e9ronique", ""], ["Briottet", "Xavier", ""]]}, {"id": "1702.07662", "submitter": "Clement Lee", "authors": "Clement Lee and Andrew Garbett and Darren J. Wilkinson", "title": "A Network Epidemic Model for Online Community Commissioning Data", "comments": "28 pages, 9 figures, 2 tables", "journal-ref": null, "doi": "10.1007/s11222-017-9770-6", "report-no": null, "categories": "stat.CO cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical model assuming a preferential attachment network, which is\ngenerated by adding nodes sequentially according to a few simple rules, usually\ndescribes real-life networks better than a model assuming, for example, a\nBernoulli random graph, in which any two nodes have the same probability of\nbeing connected, does. Therefore, to study the propogation of \"infection\"\nacross a social network, we propose a network epidemic model by combining a\nstochastic epidemic model and a preferential attachment model. A simulation\nstudy based on the subsequent Markov Chain Monte Carlo algorithm reveals an\nidentifiability issue with the model parameters. Finally, the network epidemic\nmodel is applied to a set of online commissioning data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:01:59 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 11:03:01 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Lee", "Clement", ""], ["Garbett", "Andrew", ""], ["Wilkinson", "Darren J.", ""]]}, {"id": "1702.07763", "submitter": "Jeffrey Simonoff", "authors": "Wei Fu, Jeffrey S. Simonoff", "title": "Survival Trees for Interval-Censored Survival data", "comments": "20 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval-censored data, in which the event time is only known to lie in some\ntime interval, arise commonly in practice; for example, in a medical study in\nwhich patients visit clinics or hospitals at pre-scheduled times, and the\nevents of interest occur between visits. Such data are appropriately analyzed\nusing methods that account for this uncertainty in event time measurement. In\nthis paper we propose a survival tree method for interval-censored data based\non the conditional inference framework. Using Monte Carlo simulations we find\nthat the tree is effective in uncovering underlying tree structure, performs\nsimilarly to an interval-censored Cox proportional hazards model fit when the\ntrue relationship is linear, and performs at least as well as (and in the\npresence of right-censoring outperforms) the Cox model when the true\nrelationship is not linear. Further, the interval-censored tree outperforms\nsurvival trees based on imputing the event time as an endpoint or the midpoint\nof the censoring interval. We illustrate the application of the method on tooth\nemergence data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 21:03:27 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 19:21:02 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Fu", "Wei", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "1702.07778", "submitter": "Ho-Hsiang Wu", "authors": "Yuanyuan Bian and Ho-Hsiang Wu", "title": "A Note on Nonlocal Prior Method", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of nonlocal prior to improve the performance of\nvariable selection in high dimensional setting. We prove our new prior\npossesses the robustness to hyper parameter settings and is able to detect\nsmaller decreasing signals.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 21:44:47 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Bian", "Yuanyuan", ""], ["Wu", "Ho-Hsiang", ""]]}, {"id": "1702.07804", "submitter": "Vik Gopal", "authors": "Claudio Fuentes, Vik Gopal", "title": "A Constrained Conditional Likelihood Approach for Estimating the Means\n  of Selected Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given p independent normal populations, we consider the problem of estimating\nthe mean of those populations, that based on the observed data, give the\nstrongest signals. We explicitly condition on the ranking of the sample means,\nand consider a constrained conditional maximum likelihood (CCMLE) approach,\navoiding the use of any priors and of any sparsity requirement between the\npopulation means. Our results show that if the observed means are too close\ntogether, we should in fact use the grand mean to estimate the mean of the\npopulation with the larger sample mean. If they are separated by more than a\ncertain threshold, we should shrink the observed means towards each other. As\nintuition suggests, it is only if the observed means are far apart that we\nshould conclude that the magnitude of separation and consequent ranking are not\ndue to chance. Unlike other methods, our approach does not need to pre-specify\nthe number of selected populations and the proposed CCMLE is able to perform\nsimultaneous inference. Our method, which is conceptually straightforward, can\nbe easily adapted to incorporate other selection criteria.\n  Selected populations, Maximum likelihood, Constrained MLE, Post-selection\ninference\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 23:43:44 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Fuentes", "Claudio", ""], ["Gopal", "Vik", ""]]}, {"id": "1702.08061", "submitter": "Michael Roth", "authors": "Michael Roth, Gustaf Hendeby, Carsten Fritsche, Fredrik Gustafsson", "title": "The Ensemble Kalman Filter: A Signal Processing Perspective", "comments": null, "journal-ref": "EURASIP J. Adv. Signal Process. (2017) 2017: 56", "doi": "10.1186/s13634-017-0492-x", "report-no": null, "categories": "stat.ME cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ensemble Kalman filter (EnKF) is a Monte Carlo based implementation of\nthe Kalman filter (KF) for extremely high-dimensional, possibly nonlinear and\nnon-Gaussian state estimation problems. Its ability to handle state dimensions\nin the order of millions has made the EnKF a popular algorithm in different\ngeoscientific disciplines. Despite a similarly vital need for scalable\nalgorithms in signal processing, e.g., to make sense of the ever increasing\namount of sensor data, the EnKF is hardly discussed in our field.\n  This self-contained review paper is aimed at signal processing researchers\nand provides all the knowledge to get started with the EnKF. The algorithm is\nderived in a KF framework, without the often encountered geoscientific\nterminology. Algorithmic challenges and required extensions of the EnKF are\nprovided, as well as relations to sigma-point KF and particle filters. The\nrelevant EnKF literature is summarized in an extensive survey and unique\nsimulation examples, including popular benchmark problems, complement the\ntheory with practical insights. The signal processing perspective highlights\nnew directions of research and facilitates the exchange of potentially\nbeneficial ideas, both for the EnKF and high-dimensional nonlinear and\nnon-Gaussian filtering in general.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 17:50:28 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 09:59:57 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Roth", "Michael", ""], ["Hendeby", "Gustaf", ""], ["Fritsche", "Carsten", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1702.08088", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Selection of training populations (and other subset selection problems)\n  with an accelerated genetic algorithm (STPGA: An R-package for selection of\n  training populations with a genetic algorithm)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal subset selection is an important task that has numerous algorithms\ndesigned for it and has many application areas. STPGA contains a special\ngenetic algorithm supplemented with a tabu memory property (that keeps track of\npreviously tried solutions and their fitness for a number of iterations), and\nwith a regression of the fitness of the solutions on their coding that is used\nto form the ideal estimated solution (look ahead property) to search for\nsolutions of generic optimal subset selection problems. I have initially\ndeveloped the programs for the specific problem of selecting training\npopulations for genomic prediction or association problems, therefore I give\ndiscussion of the theory behind optimal design of experiments to explain the\ndefault optimization criteria in STPGA, and illustrate the use of the programs\nin this endeavor. Nevertheless, I have picked a few other areas of application:\nsupervised and unsupervised variable selection based on kernel alignment,\nsupervised variable selection with design criteria, influential observation\nidentification for regression, solving mixed integer quadratic optimization\nproblems, balancing gains and inbreeding in a breeding population. Some of\nthese illustrations pertain new statistical approaches.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 21:23:33 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1702.08142", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama and Hiroyuki Nakahara and Koji Tsuda", "title": "Tensor Balancing on Statistical Manifold", "comments": "19 pages, 5 figures, accepted to the 34th International Conference on\n  Machine Learning (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve tensor balancing, rescaling an Nth order nonnegative tensor by\nmultiplying N tensors of order N - 1 so that every fiber sums to one. This\ngeneralizes a fundamental process of matrix balancing used to compare matrices\nin a wide range of applications from biology to economics. We present an\nefficient balancing algorithm with quadratic convergence using Newton's method\nand show in numerical experiments that the proposed algorithm is several orders\nof magnitude faster than existing ones. To theoretically prove the correctness\nof the algorithm, we model tensors as probability distributions in a\nstatistical manifold and realize tensor balancing as projection onto a\nsubmanifold. The key to our algorithm is that the gradient of the manifold,\nused as a Jacobian matrix in Newton's method, can be analytically obtained\nusing the Moebius inversion formula, the essential of combinatorial\nmathematics. Our model is not limited to tensor balancing, but has a wide\napplicability as it includes various statistical and machine learning models\nsuch as weighted DAGs and Boltzmann machines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 04:30:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:43:01 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 15:18:28 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Sugiyama", "Mahito", ""], ["Nakahara", "Hiroyuki", ""], ["Tsuda", "Koji", ""]]}, {"id": "1702.08148", "submitter": "Jiali Wang", "authors": "Jiali Wang, Bronwyn Loong, Anton H. Westveld, Alan H. Welsh", "title": "A Copula-based Imputation Model for Missing Data of Mixed Type in\n  Multilevel Data Sets", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a copula based method to handle missing values in multivariate\ndata of mixed types in multilevel data sets. Building upon the extended rank\nlikelihood of \\cite{hoff2007extending} and the multinomial probit model, our\nmodel is a latent variable model which is able to capture the relationship\namong variables of different types as well as accounting for the clustering\nstructure. We fit the model by approximating the posterior distribution of the\nparameters and the missing values through a Gibbs sampling scheme. We use the\nmultiple imputation procedure to incorporate the uncertainty due to missing\nvalues in the analysis of the data. Our proposed method is evaluated through\nsimulations to compare it with several conventional methods of handling missing\ndata. We also apply our method to a data set from a cluster randomized\ncontrolled trial of a multidisciplinary intervention in acute stroke units. We\nconclude that our proposed copula based imputation model for mixed type\nvariables achieves reasonably good imputation accuracy and recovery of\nparameters in some models of interest, and that adding random effects enhances\nperformance when the clustering effect is strong.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 05:02:58 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Wang", "Jiali", ""], ["Loong", "Bronwyn", ""], ["Westveld", "Anton H.", ""], ["Welsh", "Alan H.", ""]]}, {"id": "1702.08496", "submitter": "Jason Roy", "authors": "Jason Roy and Kirsten J Lum and Michael J. Daniels and Bret Zeldow and\n  Jordan Dworkin and Vincent Lo Re III", "title": "Bayesian nonparametric generative models for causal inference with\n  missing at random covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general Bayesian nonparametric (BNP) approach to causal\ninference in the point treatment setting. The joint distribution of the\nobserved data (outcome, treatment, and confounders) is modeled using an\nenriched Dirichlet process. The combination of the observed data model and\ncausal assumptions allows us to identify any type of causal effect -\ndifferences, ratios, or quantile effects, either marginally or for\nsubpopulations of interest. The proposed BNP model is well-suited for causal\ninference problems, as it does not require parametric assumptions about the\ndistribution of confounders and naturally leads to a computationally efficient\nGibbs sampling algorithm. By flexibly modeling the joint distribution, we are\nalso able to impute (via data augmentation) values for missing covariates\nwithin the algorithm under an assumption of ignorable missingness, obviating\nthe need to create separate imputed data sets. This approach for imputing the\nmissing covariates has the additional advantage of guaranteeing congeniality\nbetween the imputation model and the analysis model, and because we use a BNP\napproach, parametric models are avoided for imputation. The performance of the\nmethod is assessed using simulation studies. The method is applied to data from\na cohort study of human immunodeficiency virus/hepatitis C virus co-infected\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 19:59:41 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Roy", "Jason", ""], ["Lum", "Kirsten J", ""], ["Daniels", "Michael J.", ""], ["Zeldow", "Bret", ""], ["Dworkin", "Jordan", ""], ["Re", "Vincent Lo", "III"]]}, {"id": "1702.08572", "submitter": "Richard Minkah", "authors": "Richard Minkah and Tertius de Wet", "title": "Comparison of Confidence Interval Estimators: an Index Approach", "comments": "26 pages, 1 figure and 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical problems, several estimators are usually available for\ninterval estimation of a parameter of interest, and hence, the selection of an\nappropriate estimator is important. The criterion for a good estimator is to\nhave a high coverage probability close to the nominal level and a shorter\ninterval length. However, these two concepts are in opposition to each other:\nhigh and low coverages are associated with longer and shorter interval lengths\nrespectively. Some methods, such as bootstrap calibration, modify the nominal\nlevel to improve the coverage and thereby allow the selection of intervals\nbased on interval lengths only. Nonetheless, these methods are computationally\nexpensive. In this paper, we propose an index which offers an easy to compute\napproach of comparing confidence interval estimators based on a compromise\nbetween the coverage probability and the confidence interval length. We\nillustrate that the confidence interval index has range of values within the\nneighborhood of the range of the coverage probability, [0,1]. In addition, a\ngood confidence interval estimator has an index value approaching 1; and a bad\nconfidence interval has an index value approaching 0. A simulation study was\nconducted to assess the finite sample performance of the index. The proposed\nindex is illustrated with a practical example from the literature\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:52:29 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 11:51:50 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 14:54:13 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Minkah", "Richard", ""], ["de Wet", "Tertius", ""]]}, {"id": "1702.08694", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama and Karsten Borgwardt", "title": "Finding Statistically Significant Interactions between Continuous\n  Features", "comments": "13 pages, 5 figures, 2 tables, accepted to the 28th International\n  Joint Conference on Artificial Intelligence (IJCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for higher-order feature interactions that are statistically\nsignificantly associated with a class variable is of high relevance in fields\nsuch as Genetics or Healthcare, but the combinatorial explosion of the\ncandidate space makes this problem extremely challenging in terms of\ncomputational efficiency and proper correction for multiple testing. While\nrecent progress has been made regarding this challenge for binary features, we\nhere present the first solution for continuous features. We propose an\nalgorithm which overcomes the combinatorial explosion of the search space of\nhigher-order interactions by deriving a lower bound on the p-value for each\ninteraction, which enables us to massively prune interactions that can never\nreach significance and to thereby gain more statistical power. In our\nexperiments, our approach efficiently detects all significant interactions in a\nvariety of synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 08:46:37 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 08:39:03 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 15:46:12 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Sugiyama", "Mahito", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "1702.08797", "submitter": "Pulong Ma", "authors": "Pulong Ma and Emily L. Kang", "title": "A Fused Gaussian Process Model for Very Large Spatial Data", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of new remote sensing technology, large or even massive\nspatial datasets covering the globe become available. Statistical analysis of\nsuch data is challenging. This article proposes a semiparametric approach to\nmodel large or massive spatial datasets. In particular, a Gaussian process with\nadditive components is proposed, with its covariance structure consisting of\ntwo components: one component is flexible without assuming a specific\nparametric covariance function but is able to achieve dimension reduction; the\nother is parametric and simultaneously induces sparsity. The inference\nalgorithm for parameter estimation and spatial prediction is devised. The\nresulting spatial prediction methodology that we call fused Gaussian process\n(FGP), is applied to simulated data and a massive satellite dataset. The\nresults demonstrate the computational and inferential benefits of FGP over\ncompeting methods and show that FGP is robust against model misspecification\nand captures spatial nonstationarity. The supplemental materials are available\nonline.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 14:22:03 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 16:59:39 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 20:45:08 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Ma", "Pulong", ""], ["Kang", "Emily L.", ""]]}, {"id": "1702.08846", "submitter": "Patrick Heas", "authors": "Patrick H\\'eas and C\\'edric Herzet", "title": "Reduced Modeling of Unknown Trajectories", "comments": null, "journal-ref": null, "doi": "10.1007/s11831-017-9229-0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with model order reduction of parametrical dynamical\nsystems. We consider the specific setup where the distribution of the system's\ntrajectories is unknown but the following two sources of information are\navailable: \\textit{(i)} some \"rough\" prior knowledge on the system's\nrealisations; \\textit{(ii)} a set of \"incomplete\" observations of the system's\ntrajectories. We propose a Bayesian methodological framework to build\nreduced-order models (ROMs) by exploiting these two sources of information. We\nemphasise that complementing the prior knowledge with the collected data\nprovably enhances the knowledge of the distribution of the system's\ntrajectories. We then propose an implementation of the proposed methodology\nbased on Monte-Carlo methods. In this context, we show that standard ROM\nlearning techniques, such e.g. Proper Orthogonal Decomposition or Dynamic Mode\nDecomposition, can be revisited and recast within the probabilistic framework\nconsidered in this paper.~We illustrate the performance of the proposed\napproach by numerical results obtained for a standard geophysical model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 16:28:17 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 08:38:01 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["H\u00e9as", "Patrick", ""], ["Herzet", "C\u00e9dric", ""]]}, {"id": "1702.08896", "submitter": "Dustin Tran", "authors": "Dustin Tran, Rajesh Ranganath, David M. Blei", "title": "Hierarchical Implicit Models and Likelihood-Free Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit probabilistic models are a flexible class of models defined by a\nsimulation process for data. They form the basis for theories which encompass\nour understanding of the physical world. Despite this fundamental nature, the\nuse of implicit models remains limited due to challenges in specifying complex\nlatent structure in them, and in performing inferences in such models with\nlarge data sets. In this paper, we first introduce hierarchical implicit models\n(HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian\nmodeling, thereby defining models via simulators of data with rich hidden\nstructure. Next, we develop likelihood-free variational inference (LFVI), a\nscalable variational inference algorithm for HIMs. Key to LFVI is specifying a\nvariational family that is also implicit. This matches the model's flexibility\nand allows for accurate approximation of the posterior. We demonstrate diverse\napplications: a large-scale physical simulator for predator-prey populations in\necology; a Bayesian generative adversarial network for discrete data; and a\ndeep implicit model for text generation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:33:32 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 17:28:04 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 01:52:45 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1702.08897", "submitter": "Xiao Pu", "authors": "Xiao Pu, Ery Arias-Castro", "title": "Semiparametric Estimation of Symmetric Mixture Models with Monotone and\n  Log-Concave Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we revisit the problem of fitting a mixture model under the\nassumption that the mixture components are symmetric and log-concave. To this\nend, we first study the nonparametric maximum likelihood estimation (NPMLE) of\na monotone log-concave probability density. By following the arguments of\nRufibach (2006), we show that the NPMLE is uniformly consistent with respect to\nthe supremum norm on compact subsets of the interior of the support. To fit the\nmixture model, we propose a semiparametric EM (SEM) algorithm, which can be\nadapted to other semiparametric mixture models. In our numerical experiments,\nwe compare our algorithm to that of Balabdaoui and Doss (2014) and other\nmixture models both on simulated and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:34:39 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 22:27:36 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 01:48:43 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Pu", "Xiao", ""], ["Arias-Castro", "Ery", ""]]}, {"id": "1702.08900", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko", "title": "Asymptotic Exponentiality of the First Exit Time of the Shiryaev-Roberts\n  Diffusion with Constant Positive Drift", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the first exit time of a Shiryaev-Roberts diffusion with constant\npositive drift from the interval $[0,A]$ where $A>0$. We show that the moment\ngenerating function (Laplace transform) of a suitably standardized version of\nthe first exit time converges to that of the unit-mean exponential distribution\nas $A\\to+\\infty$. The proof is explicit in that the moment generating function\nof the first exit time is first expressed analytically and in a closed form,\nand then the desired limit as $A\\to+\\infty$ is evaluated directly. The result\nis of importance in the area of quickest change-point detection, and its\ndiscrete-time counterpart has been previously established - although in a\ndifferent manner - by Pollak and Tartakovsky (2009).\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:39:32 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 15:19:31 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Polunchenko", "Aleksey S.", ""]]}]