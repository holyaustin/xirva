[{"id": "1901.00068", "submitter": "Yin Song", "authors": "Yin Song, Shufei Ge, Jiguo Cao, Liangliang Wang, and Farouk S. Nathoo", "title": "A Bayesian Spatial Model for Imaging Genetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a Bayesian bivariate spatial model for multivariate regression\nanalysis applicable to studies examining the influence of genetic variation on\nbrain structure. Our model is motivated by an imaging genetics study of the\nAlzheimer's Disease Neuroimaging Initiative (ADNI), where the objective is to\nexamine the association between images of volumetric and cortical thickness\nvalues summarizing the structure of the brain as measured by magnetic resonance\nimaging (MRI) and a set of 486 SNPs from 33 Alzheimer's Disease (AD) candidate\ngenes obtained from 632 subjects. A bivariate spatial process model is\ndeveloped to accommodate the correlation structures typically seen in\nstructural brain imaging data. First, we allow for spatial correlation on a\ngraph structure in the imaging phenotypes obtained from a neighbourhood matrix\nfor measures on the same hemisphere of the brain. Second, we allow for\ncorrelation in the same measures obtained from different hemispheres\n(left/right) of the brain. We develop a mean-field variational Bayes algorithm\nand a Gibbs sampling algorithm to fit the model. We also incorporate Bayesian\nfalse discovery rate (FDR) procedures to select SNPs. We implement the\nmethodology in a new release of the R package bgsmtr. We show that the new\nspatial model demonstrates superior performance over a standard model in our\napplication. Data used in the preparation of this article were obtained from\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) database\n(adni.loni.usc.edu).\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 00:32:45 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 03:57:32 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 10:49:02 GMT"}, {"version": "v4", "created": "Sun, 24 May 2020 01:38:40 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Song", "Yin", ""], ["Ge", "Shufei", ""], ["Cao", "Jiguo", ""], ["Wang", "Liangliang", ""], ["Nathoo", "Farouk S.", ""]]}, {"id": "1901.00139", "submitter": "Hongsheng Dai PhD", "authors": "Hongsheng Dai, Murray Pollock and Gareth Roberts", "title": "Monte Carlo Fusion", "comments": null, "journal-ref": "J. Appl. Probab. 56 (2019) 174-191", "doi": "10.1017/jpr.2019.12", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new theory and methodology to tackle the problem of\nunifying distributed analyses and inferences on shared parameters from multiple\nsources, into a single coherent inference. This surprisingly challenging\nproblem arises in many settings (for instance, expert elicitation, multi-view\nlearning, distributed 'big data' problems etc.), but to-date the framework and\nmethodology proposed in this paper (Monte Carlo Fusion) is the first general\napproach which avoids any form of approximation error in obtaining the unified\ninference. In this paper we focus on the key theoretical underpinnings of this\nnew methodology, and simple (direct) Monte Carlo interpretations of the theory.\nThere is considerable scope to tailor the theory introduced in this paper to\nparticular application settings (such as the big data setting), construct\nefficient parallelised schemes, understand the approximation and computational\nefficiencies of other such unification paradigms, and explore new theoretical\nand methodological directions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 11:49:43 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Dai", "Hongsheng", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth", ""]]}, {"id": "1901.00180", "submitter": "Pavlo Mozharovskyi", "authors": "Pierre Lafaye de Micheaux, Pavlo Mozharovskyi, Myriam Vimond", "title": "Depth for curve data and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  John W. Tukey (1975) defined statistical data depth as a function that\ndetermines centrality of an arbitrary point with respect to a data cloud or to\na probability measure. During the last decades, this seminal idea of data depth\nevolved into a powerful tool proving to be useful in various fields of science.\nRecently, extending the notion of data depth to the functional setting\nattracted a lot of attention among theoretical and applied statisticians. We go\nfurther and suggest a notion of data depth suitable for data represented as\ncurves, or trajectories, which is independent of the parametrization. We show\nthat our curve depth satisfies theoretical requirements of general depth\nfunctions that are meaningful for trajectories. We apply our methodology to\ndiffusion tensor brain images and also to pattern recognition of hand written\ndigits and letters. Supplementary Materials are available online.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 17:07:15 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 15:29:42 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["de Micheaux", "Pierre Lafaye", ""], ["Mozharovskyi", "Pavlo", ""], ["Vimond", "Myriam", ""]]}, {"id": "1901.00336", "submitter": "Ross Towe", "authors": "Ross Towe, Jonathan Tawn, Emma Eastoe, Rob Lamb", "title": "Modelling the clustering of extreme events for short-term risk\n  assessment", "comments": null, "journal-ref": null, "doi": "10.1007/s13253-019-00376-0", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having reliable estimates of the occurrence rates of extreme events is highly\nimportant for insurance companies, government agencies and the general public.\nThe rarity of an extreme event is typically expressed through its return\nperiod, i.e., the expected waiting time between events of the observed size if\nthe extreme events of the processes are independent and identically\ndistributed. A major limitation with this measure is when an unexpectedly high\nnumber of events occur within the next few months immediately after a\n\\textit{T} year event, with \\textit{T} large. Such events undermine the trust\nin the quality of these risk estimates. The clustering of apparently\nindependent extreme events can occur as a result of local non-stationarity of\nthe process, which can be explained by covariates or random effects. We show\nhow accounting for these covariates and random effects provides more accurate\nestimates of return levels and aids short-term risk assessment through the use\nof a new risk measure, which provides evidence of risk which is complementary\nto the return period.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 12:21:50 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 15:25:53 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Towe", "Ross", ""], ["Tawn", "Jonathan", ""], ["Eastoe", "Emma", ""], ["Lamb", "Rob", ""]]}, {"id": "1901.00403", "submitter": "Peter Schulam", "authors": "Peter Schulam and Suchi Saria", "title": "Can You Trust This Prediction? Auditing Pointwise Reliability After\n  Learning", "comments": "To appear in the proceedings of Artificial Intelligence and\n  Statistics (AISTATS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To use machine learning in high stakes applications (e.g. medicine), we need\ntools for building confidence in the system and evaluating whether it is\nreliable. Methods to improve model reliability often require new learning\nalgorithms (e.g. using Bayesian inference to obtain uncertainty estimates). An\nalternative is to audit a model after it is trained. In this paper, we describe\nresampling uncertainty estimation (RUE), an algorithm to audit the pointwise\nreliability of predictions. Intuitively, RUE estimates the amount that a\nprediction would change if the model had been fit on different training data.\nThe algorithm uses the gradient and Hessian of the model's loss function to\ncreate an ensemble of predictions. Experimentally, we show that RUE more\neffectively detects inaccurate predictions than existing tools for auditing\nreliability subsequent to training. We also show that RUE can create predictive\ndistributions that are competitive with state-of-the-art methods like Monte\nCarlo dropout, probabilistic backpropagation, and deep ensembles, but does not\ndepend on specific algorithms at train-time like these methods do.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 14:53:33 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 21:33:19 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Schulam", "Peter", ""], ["Saria", "Suchi", ""]]}, {"id": "1901.00405", "submitter": "Grzegorz A Rempala", "authors": "Wasiur R. KhudaBukhsh and Boseung Choi and Eben Kenah and Grzegorz A.\n  Rempala", "title": "Survival Dynamical Systems for the Population-level Analysis of\n  Epidemics", "comments": "27 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the classical Susceptible-Infected-Recovered (SIR) epidemic\nmodels proposed by Kermack and Mckendrick, we consider a class of stochastic\ncompartmental dynamical systems with a notion of partial ordering among the\ncompartments. We call such systems unidirectional Mass Transfer Models (MTMs).\nWe show that there is a natural way of interpreting a uni-directional MTM as a\nSurvival Dynamical System (SDS) that is described in terms of survival\nfunctions instead of population counts. This SDS interpretation allows us to\nemploy tools from survival analysis to address various issues with data\ncollection and statistical inference of unidirectional MTMs. In particular, we\npropose and numerically validate a statistical inference procedure based on\nSDS-likelihoods. We use the SIR model as a running example throughout the paper\nto illustrate the ideas.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 14:57:34 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["KhudaBukhsh", "Wasiur R.", ""], ["Choi", "Boseung", ""], ["Kenah", "Eben", ""], ["Rempala", "Grzegorz A.", ""]]}, {"id": "1901.00419", "submitter": "Ivan Fernandez-Val", "authors": "Iv\\'an Fern\\'andez-Val, Franco Peracchi, Aico van Vuuren and Francis\n  Vella", "title": "Decomposing Changes in the Distribution of Real Hourly Wages in the U.S", "comments": "52 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the sources of changes in the distribution of hourly wages in the\nUnited States using CPS data for the survey years 1976 to 2019. We account for\nthe selection bias from the employment decision by modeling the distribution of\nannual hours of work and estimating a nonseparable model of wages which uses a\ncontrol function to account for selection. This allows the inclusion of all\nindividuals working positive hours and provides a fuller description of the\nwage distribution. We decompose changes in the distribution of wages into\ncomposition, structural and selection effects. Composition effects have\nincreased wages at all quantiles but the patterns of change are generally\ndetermined by the structural effects. Evidence of changes in the selection\neffects only appear at the lower quantiles of the female wage distribution.\nThese various components combine to produce a substantial increase in wage\ninequality.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 16:26:50 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 17:32:37 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 15:23:43 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Peracchi", "Franco", ""], ["van Vuuren", "Aico", ""], ["Vella", "Francis", ""]]}, {"id": "1901.00433", "submitter": "Patrick Forr\\'e", "authors": "Patrick Forr\\'e, Joris M. Mooij", "title": "Causal Calculus in the Presence of Cycles, Latent Confounders and\n  Selection Bias", "comments": "Accepted for publication in Conference on Uncertainty in Artificial\n  Intelligence 2019 (UAI-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the main rules of causal calculus (also called do-calculus) for i/o\nstructural causal models (ioSCMs), a generalization of a recently proposed\ngeneral class of non-/linear structural causal models that allow for cycles,\nlatent confounders and arbitrary probability distributions. We also generalize\nadjustment criteria and formulas from the acyclic setting to the general one\n(i.e. ioSCMs). Such criteria then allow to estimate (conditional) causal\neffects from observational data that was (partially) gathered under selection\nbias and cycles. This generalizes the backdoor criterion, the\nselection-backdoor criterion and extensions of these to arbitrary ioSCMs.\nTogether, our results thus enable causal reasoning in the presence of cycles,\nlatent confounders and selection bias. Finally, we extend the ID algorithm for\nthe identification of causal effects to ioSCMs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 15:58:49 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 16:38:18 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Forr\u00e9", "Patrick", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1901.00456", "submitter": "Donghui Yan", "authors": "Donghui Yan, Zhiwei Qin, Songxiang Gu, Haiping Xu, Ming Shao", "title": "Cost-sensitive Selection of Variables by Ensemble of Model Sequences", "comments": "26 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many applications require the collection of data on different variables or\nmeasurements over many system performance metrics. We term those broadly as\nmeasures or variables. Often data collection along each measure incurs a cost,\nthus it is desirable to consider the cost of measures in modeling. This is a\nfairly new class of problems in the area of cost-sensitive learning. A few\nattempts have been made to incorporate costs in combining and selecting\nmeasures. However, existing studies either do not strictly enforce a budget\nconstraint, or are not the `most' cost effective. With a focus on\nclassification problem, we propose a computationally efficient approach that\ncould find a near optimal model under a given budget by exploring the most\n`promising' part of the solution space. Instead of outputting a single model,\nwe produce a model schedule -- a list of models, sorted by model costs and\nexpected predictive accuracy. This could be used to choose the model with the\nbest predictive accuracy under a given budget, or to trade off between the\nbudget and the predictive accuracy. Experiments on some benchmark datasets show\nthat our approach compares favorably to competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 17:35:30 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 14:13:19 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 00:21:30 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Yan", "Donghui", ""], ["Qin", "Zhiwei", ""], ["Gu", "Songxiang", ""], ["Xu", "Haiping", ""], ["Shao", "Ming", ""]]}, {"id": "1901.00663", "submitter": "Yingqi Zhao", "authors": "Ying-Qi Zhao and Eric B. Laber and Yang Ning and Sumona Saha and Bruce\n  Sands", "title": "Efficient augmentation and relaxation learning for individualized\n  treatment rules using observational data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individualized treatment rules aim to identify if, when, which, and to whom\ntreatment should be applied. A globally aging population, rising healthcare\ncosts, and increased access to patient-level data have created an urgent need\nfor high-quality estimators of individualized treatment rules that can be\napplied to observational data. A recent and promising line of research for\nestimating individualized treatment rules recasts the problem of estimating an\noptimal treatment rule as a weighted classification problem. We consider a\nclass of estimators for optimal treatment rules that are analogous to convex\nlarge-margin classifiers. The proposed class applies to observational data and\nis doubly-robust in the sense that correct specification of either a propensity\nor outcome model leads to consistent estimation of the optimal individualized\ntreatment rule. Using techniques from semiparametric efficiency theory, we\nderive rates of convergence for the proposed estimators and use these rates to\ncharacterize the bias-variance trade-off for estimating individualized\ntreatment rules with classification-based methods. Simulation experiments\ninformed by these results demonstrate that it is possible to construct new\nestimators within the proposed framework that significantly outperform existing\nones. We illustrate the proposed methods using data from a labor training\nprogram and a study of inflammatory bowel syndrome.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 09:47:35 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zhao", "Ying-Qi", ""], ["Laber", "Eric B.", ""], ["Ning", "Yang", ""], ["Saha", "Sumona", ""], ["Sands", "Bruce", ""]]}, {"id": "1901.00769", "submitter": "Elynn Chen", "authors": "Elynn Y. Chen and Rong Chen", "title": "Modeling Dynamic Transport Network with Matrix Factor Models: with an\n  Application to International Trade Flow", "comments": "arXiv admin note: text overlap with arXiv:1710.06325", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  International trade research plays an important role to inform trade policy\nand shed light on wider issues relating to poverty, development, migration,\nproductivity, and economy. With recent advances in information technology,\nglobal and regional agencies distribute an enormous amount of internationally\ncomparable trading data among a large number of countries over time, providing\na goldmine for empirical analysis of international trade. Meanwhile, an array\nof new statistical methods are recently developed for dynamic network analysis.\nHowever, these advanced methods have not been utilized for analyzing such\nmassive dynamic cross-country trading data. International trade data can be\nviewed as a dynamic transport network because it emphasizes the amount of goods\nmoving across a network. Most literature on dynamic network analysis\nconcentrates on the connectivity network that focuses on link formation or\ndeformation rather than the transport moving across the network. We take a\ndifferent perspective from the pervasive node-and-edge level modeling: the\ndynamic transport network is modeled as a time series of relational matrices.\nWe adopt a matrix factor model of \\cite{wang2018factor}, with a specific\ninterpretation for the dynamic transport network. Under the model, the observed\nsurface network is assumed to be driven by a latent dynamic transport network\nwith lower dimensions. The proposed method is able to unveil the latent dynamic\nstructure and achieve the objective of dimension reduction. We applied the\nproposed framework and methodology to a data set of monthly trading volumes\namong 24 countries and regions from 1982 to 2015. Our findings shed light on\ntrading hubs, centrality, trends and patterns of international trade and show\nmatching change points to trading policies. The dataset also provides a fertile\nground for future research on international trade.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 18:17:17 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Chen", "Elynn Y.", ""], ["Chen", "Rong", ""]]}, {"id": "1901.00772", "submitter": "Lola Etievant", "authors": "Lola Etievant and Vivian Viallon", "title": "Which practical interventions does the do-operator refer to in causal\n  inference? Illustration on the example of obesity and cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For exposures $X$ like obesity, no precise and unambiguous definition exists\nfor the hypothetical intervention $do(X = x_0)$. This has raised concerns about\nthe relevance of causal effects estimated from observational studies for such\nexposures. Under the framework of structural causal models, we study how the\neffect of $do(X = x_0)$ relates to the effect of interventions on causes of\n$X$. We show that for interventions focusing on causes of $X$ that affect the\noutcome through $X$ only, the effect of $do(X = x_0)$ equals the effect of the\nconsidered intervention. On the other hand, for interventions on causes $W$ of\n$X$ that affect the outcome not only through $X$, we show that the effect of\n$do(X = x_0)$ only partly captures the effect of the intervention. In\nparticular, under simple causal models (e.g., linear models with no\ninteraction), the effect of $do(X = x_0)$ can be seen as an indirect effect of\nthe intervention on $W$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 14:40:18 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Etievant", "Lola", ""], ["Viallon", "Vivian", ""]]}, {"id": "1901.00833", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena", "title": "Energy distance and kernel mean embedding for two sample survival test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article a new family of tests is proposed for the comparison problem\nof the equality of distribution of two-sample under right censoring scheme. The\ntests are based on energy distance and kernels mean embedding, are calibrated\nby permutations and are consistent against all alternatives. The good\nperformance of the new tests in real situations with finite samples is\nestablished with a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 18:05:57 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Matabuena", "Marcos", ""]]}, {"id": "1901.00886", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy and David B Dunson", "title": "Nonparametric graphical model for counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although multivariate count data are routinely collected in many application\nareas, there is surprisingly little work developing flexible models for\ncharacterizing their dependence structure. This is particularly true when\ninterest focuses on inferring the conditional independence graph. In this\narticle, we propose a new class of pairwise Markov random field-type models for\nthe joint distribution of a multivariate count vector. By employing a novel\ntype of transformation, we avoid restricting to non-negative dependence\nstructures or inducing other restrictions through truncations. Taking a\nBayesian approach to inference, we choose a Dirichlet process prior for the\ndistribution of a random effect to induce great flexibility in the\nspecification. An efficient Markov chain Monte Carlo (MCMC) algorithm is\ndeveloped for posterior computation. We prove various theoretical properties,\nincluding posterior consistency, and show that our COunt Nonparametric\nGraphical Analysis (CONGA) approach has good performance relative to\ncompetitors in simulation studies. The methods are motivated by an application\nto neuron spike count data in mice.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 19:25:15 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 21:55:34 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Roy", "Arkaprava", ""], ["Dunson", "David B", ""]]}, {"id": "1901.00908", "submitter": "Chanmin Kim", "authors": "Chanmin Kim, Corwin M Zigler, Michael J Daniels, Christine Choirat,\n  Jason A Roy", "title": "Bayesian Longitudinal Causal Inference in the Analysis of the Public\n  Health Impact of Pollutant Emissions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pollutant emissions from coal-burning power plants have been deemed to\nadversely impact ambient air quality and public health conditions. Despite the\nnoticeable reduction in emissions and the improvement of air quality since the\nClean Air Act (CAA) became the law, the public-health benefits from changes in\nemissions have not been widely evaluated yet. In terms of the chain of\naccountability (HEI Accountability Working Group, 2003), the link between\npollutant emissions from the power plants (SO2) and public health conditions\n(respiratory diseases) accounting for changes in ambient air quality (PM2.5) is\nunknown. We provide the first assessment of the longitudinal effect of specific\npollutant emission (SO2) on public health outcomes that is mediated through\nchanges in the ambient air quality. It is of particular interest to examine the\nextent to which the effect that is mediated through changes in local ambient\nair quality differs from year to year. In this paper, we propose a Bayesian\napproach to estimate novel causal estimands: time-varying mediation effects in\nthe presence of mediators and responses measured every year. We replace the\ncommonly invoked sequential ignorability assumption with a new set of\nassumptions which are sufficient to identify the distributions of the natural\nindirect and direct effects in this setting.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 20:26:36 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Kim", "Chanmin", ""], ["Zigler", "Corwin M", ""], ["Daniels", "Michael J", ""], ["Choirat", "Christine", ""], ["Roy", "Jason A", ""]]}, {"id": "1901.01163", "submitter": "Yanglei Song", "authors": "Yanglei Song, Xiaohui Chen, Kengo Kato", "title": "Approximating high-dimensional infinite-order $U$-statistics:\n  statistical and computational guarantees", "comments": null, "journal-ref": "Electronic Journal of Statistics 2019, Vol. 13, No. 2, 4794-4848", "doi": "10.1214/19-EJS1643", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributional approximations to high-dimensional\nnon-degenerate $U$-statistics with random kernels of diverging orders.\nInfinite-order $U$-statistics (IOUS) are a useful tool for constructing\nsimultaneous prediction intervals that quantify the uncertainty of ensemble\nmethods such as subbagging and random forests. A major obstacle in using the\nIOUS is their computational intractability when the sample size and/or order\nare large. In this article, we derive non-asymptotic Gaussian approximation\nerror bounds for an incomplete version of the IOUS with a random kernel. We\nalso study data-driven inferential methods for the incomplete IOUS via\nbootstraps and develop their statistical and computational guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 14:59:48 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 15:29:29 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 14:50:28 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Song", "Yanglei", ""], ["Chen", "Xiaohui", ""], ["Kato", "Kengo", ""]]}, {"id": "1901.01230", "submitter": "David Arbour", "authors": "David Arbour and Drew Dimmery and Arjun Sondhi", "title": "Permutation Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational causal inference, in order to emulate a randomized\nexperiment, weights are used to render treatments independent of observed\ncovariates. This property is known as balance; in its absence, estimated causal\neffects may be arbitrarily biased. In this work we introduce permutation\nweighting, a method for estimating balancing weights using a standard binary\nclassifier (regardless of cardinality of treatment). A large class of\nprobabilistic classifiers may be used in this method; the choice of loss for\nthe classifier implies the particular definition of balance. We bound bias and\nvariance in terms of the excess risk of the classifier, show that these\ndisappear asymptotically, and demonstrate that our classification problem\ndirectly minimizes imbalance. A wide variety of existing balancing weights may\nbe estimated through this regime, allowing for direct comparison between\nmethods based on classifier loss, as well as hyper-parameter tuning using\ncross-validation. Empirical evaluations indicate that permutation weighting\nprovides favorable performance in comparison to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 18:01:01 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 05:33:19 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 03:48:48 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 23:10:50 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Arbour", "David", ""], ["Dimmery", "Drew", ""], ["Sondhi", "Arjun", ""]]}, {"id": "1901.01303", "submitter": "Meizi Liu", "authors": "Meizi Liu, Sue-Jane Wang, Yuan Ji", "title": "The i3+3 Design for Phase I Clinical Trials", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The 3+3 design has been shown to be less likely to achieve the\nobjectives of phase I dose-finding trials when compared with more advanced\nmodel-based designs. One major criticism of the 3+3 design is that it is based\non simple rules, does not depend on statistical models for inference, and leads\nto unsafe and unreliable operating characteristics. On the other hand, being\nrule-based allows 3+3 to be easily understood and implemented in practice,\nmaking it the first choice among clinicians. Is it possible to have a\nrule-based design with great performance? Methods: We propose a new rule-based\ndesign called i3+3, where the letter \"i\" represents the word \"interval\". The\ni3+3 design is based on simple but more advanced rules that account for the\nvariabilities in the observed data. We compare the operating characteristics\nfor the proposed i3+3 design with other popular phase I designs by simulation.\nResults: The i3+3 design is far superior than the 3+3 design in trial safety\nand the ability to identify the true MTD. Compared with model-based phase I\ndesigns, i3+3 also demonstrates comparable performances. In other words, the\ni3+3 design possesses both the simplicity and transparency of the rule-based\napproaches, and the superior operating characteristics seen in model-based\napproaches. An online R Shiny tool (https://i3design.shinyapps.io/i3plus3/) is\nprovided to illustrate the i3+3 design, although in practice it requires no\nsoftware to design or conduct a dose-finding trial. Conclusion: The i3+3 design\ncould be a practice-altering method for the clinical community.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 20:34:49 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 02:37:58 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Liu", "Meizi", ""], ["Wang", "Sue-Jane", ""], ["Ji", "Yuan", ""]]}, {"id": "1901.01477", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and John Nagorski and Genevera I. Allen", "title": "Dynamic Visualization and Fast Computation for Convex Clustering via\n  Algorithmic Regularization", "comments": "To appear in the Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": "10.1080/10618600.2019.1629943", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convex clustering is a promising new approach to the classical problem of\nclustering, combining strong performance in empirical studies with rigorous\ntheoretical foundations. Despite these advantages, convex clustering has not\nbeen widely adopted, due to its computationally intensive nature and its lack\nof compelling visualizations. To address these impediments, we introduce\nAlgorithmic Regularization, an innovative technique for obtaining high-quality\nestimates of regularization paths using an iterative one-step approximation\nscheme. We justify our approach with a novel theoretical result, guaranteeing\nglobal convergence of the approximate path to the exact solution under\neasily-checked non-data-dependent assumptions. The application of algorithmic\nregularization to convex clustering yields the Convex Clustering via\nAlgorithmic Regularization Paths (CARP) algorithm for computing the clustering\nsolution path. On example data sets from genomics and text analysis, CARP\ndelivers over a 100-fold speed-up over existing methods, while attaining a\nfiner approximation grid than standard methods. Furthermore, CARP enables\nimproved visualization of clustering solutions: the fine solution grid returned\nby CARP can be used to construct a convex clustering-based dendrogram, as well\nas forming the basis of a dynamic path-wise visualization based on modern web\ntechnologies. Our methods are implemented in the open-source R package\nclustRviz, available at https://github.com/DataSlingers/clustRviz.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 00:22:35 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 22:40:47 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 21:36:39 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 18:32:32 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Weylandt", "Michael", ""], ["Nagorski", "John", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1901.01583", "submitter": "Vivian Viallon", "authors": "Nadim Ballout, Cedric Garcia and Vivian Viallon", "title": "Sparse estimation for case-control studies with multiple subtypes of\n  cases", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of case-control studies with several subtypes of cases is\nincreasingly common, e.g. in cancer epidemiology. For matched designs, we show\nthat a natural strategy is based on a stratified conditional logistic\nregression model. Then, to account for the potential homogeneity among the\nsubtypes of cases, we adapt the ideas of data shared lasso, which has been\nrecently proposed for the estimation of regression models in a stratified\nsetting. For unmatched designs, we compare two standard methods based on\nL1-norm penalized multinomial logistic regression. We describe formal\nconnections between these two approaches, from which practical guidance can be\nderived. We show that one of these approaches, which is based on a symmetric\nformulation of the multinomial logistic regression model, actually reduces to a\ndata shared lasso version of the other. Consequently, the relative performance\nof the two approaches critically depends on the level of homogeneity that\nexists among the subtypes of cases: more precisely, when homogeneity is\nmoderate to high, the non-symmetric formulation with controls as the reference\nis not recommended. Empirical results obtained from synthetic data are\npresented, which confirm the benefit of properly accounting for potential\nhomogeneity under both matched and unmatched designs. We also present\npreliminary results from the analysis a case-control study nested within the\nEPIC cohort, where the objective is to identify metabolites associated with the\noccurrence of subtypes of breast cancer.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 18:07:44 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 19:32:02 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ballout", "Nadim", ""], ["Garcia", "Cedric", ""], ["Viallon", "Vivian", ""]]}, {"id": "1901.01610", "submitter": "Li-Pang Chen", "authors": "Li-Pang Chen", "title": "Iterated Feature Screening based on Distance Correlation for\n  Ultrahigh-Dimensional Censored Data with Covariates Measurement Error", "comments": "26 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature screening is an important method to reduce the dimension and capture\ninformative variables in ultrahigh-dimensional data analysis. Many methods have\nbeen developed for feature screening. These methods, however, are challenged by\ncomplex features pertinent to the data collection as well as the nature of the\ndata themselves. Typically, incomplete response caused by right-censoring and\ncovariates measurement error are often accompanying with survival analysis.\nEven though there are many methods have been proposed for censored data, little\nwork has been available when both incomplete response and measurement error\noccur simultaneously. In addition, the conventional feature screening methods\nmay fail to detect the truly important covariates which are marginally\nindependent of the response variable due to correlations among covariates. In\nthis paper, we explore this important problem and propose the valid feature\nscreening method in the presence of survival data with measurement error. In\naddition, we also develop the iteration method to improve the accuracy of\nselecting all important covariates. Numerical studies are reported to assess\nthe performance of the proposed method. Finally, we implement the proposed\nmethod to two different real datasets.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 21:52:35 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Chen", "Li-Pang", ""]]}, {"id": "1901.01645", "submitter": "Zhonglei Wang", "authors": "Zhonglei Wang, Jae Kwang Kim, Liuhua Peng", "title": "Bootstrap inference for the finite population total under complex\n  sampling designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap is a useful tool for making statistical inference, but it may\nprovide erroneous results under complex survey sampling. Most studies about\nbootstrap-based inference are developed under simple random sampling and\nstratified random sampling. In this paper, we propose a unified bootstrap\nmethod applicable to some complex sampling designs, including Poisson sampling\nand probability-proportional-to-size sampling. Two main features of the\nproposed bootstrap method are that studentization is used to make inference,\nand the finite population is bootstrapped based on a multinomial distribution\nby incorporating the sampling information. We show that the proposed bootstrap\nmethod is second-order accurate using the Edgeworth expansion. Two simulation\nstudies are conducted to compare the proposed bootstrap method with the\nWald-type method, which is widely used in survey sampling. Results show that\nthe proposed bootstrap method is better in terms of coverage rate especially\nwhen sample size is limited.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 02:14:36 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Wang", "Zhonglei", ""], ["Kim", "Jae Kwang", ""], ["Peng", "Liuhua", ""]]}, {"id": "1901.01864", "submitter": "Giles Hooker", "authors": "Zi Ye, Giles Hooker, Stephen Ellner", "title": "The Jensen Effect and Functional Single Index Models: Estimating the\n  Ecological Implications of Nonlinear Reaction Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops tools to characterize how species are affected by\nenvironmental variability, based on a functional single index model relating a\nresponse such as growth rate or survival to environmental conditions. In\necology, the curvature of such responses are used, via Jensen's inequality, to\ndetermine whether environmental variability is harmful or beneficial, and\ndiffering nonlinear responses to environmental variability can contribute to\nthe coexistence of competing species.\n  Here, we address estimation and inference for these models with observational\ndata on individual responses to environmental conditions. Because nonparametric\nestimation of the curvature (second derivative) in a nonparametric functional\nsingle index model requires unrealistic sample sizes, we instead focus on\ndirectly estimating the effect of the nonlinearity, by comparing the average\nresponse to a variable environment with the response at the expected\nenvironment, which we call the Jensen Effect. We develop a test statistic to\nassess whether this effect is significantly different from zero. In doing so we\nre-interpret the SiZer method of Chaudhuri and Marron (1995) by maximizing a\ntest statistic over smoothing parameters. We show that our proposed method\nworks well both in simulations and on real ecological data from the long-term\ndata set described in Drake (2005).\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 15:05:00 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 18:27:25 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Ye", "Zi", ""], ["Hooker", "Giles", ""], ["Ellner", "Stephen", ""]]}, {"id": "1901.01918", "submitter": "Tao Sun", "authors": "Tao Sun and Ying Ding", "title": "Copula-based Semiparametric Regression Method for Bivariate Data under\n  General Interval Censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research is motivated by discovering and underpinning genetic causes for\nthe progression of a bilateral eye disease, Age-related Macular Degeneration\n(AMD), of which the primary outcomes, progression times to late-AMD, are\nbivariate and interval-censored due to intermittent assessment times. We\npropose a novel class of copula-based semiparametric transformation models for\nbivariate data under general interval censoring, which includes the case 1\ninterval censoring (current status data) and case 2 interval censoring.\nSpecifically, the joint likelihood is modeled through a two-parameter\nArchimedean copula, which can flexibly characterize the dependence between the\ntwo margins in both tails. The marginal distributions are modeled through\nsemiparametric transformation models using sieves, with the proportional\nhazards or odds model being a special case. We develop a computationally\nefficient sieve maximum likelihood estimation procedure for the unknown\nparameters, together with a generalized score test for the regression\nparameter(s). For the proposed sieve estimators of finite-dimensional\nparameters, we establish their asymptotic normality and efficiency. Extensive\nsimulations are conducted to evaluate the performance of the proposed method in\nfinite samples. Finally, we apply our method to a genome-wide analysis of AMD\nprogression using the Age-Related Eye Disease Study (AREDS) data, to\nsuccessfully identify novel risk variants associated with the disease\nprogression. We also produce predicted joint and conditional progression-free\nprobabilities, for patients with different genetic characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 16:57:57 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 01:45:27 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Sun", "Tao", ""], ["Ding", "Ying", ""]]}, {"id": "1901.02034", "submitter": "Farzana Nasrin", "authors": "Vasileios Maroulas, Farzana Nasrin and Christopher Oballe", "title": "A Bayesian Framework for Persistent Homology", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams offer a way to summarize topological and geometric\nproperties latent in datasets. While several methods have been developed that\nutilize persistence diagrams in statistical inference, a full Bayesian\ntreatment remains absent. This paper, relying on the theory of point processes,\npresents a Bayesian framework for inference with persistence diagrams relying\non a substitution likelihood argument. In essence, we model persistence\ndiagrams as Poisson point processes with prior intensities and compute\nposterior intensities by adopting techniques from the theory of marked point\nprocesses. We then propose a family of conjugate prior intensities via Gaussian\nmixtures to obtain a closed form of the posterior intensity. Finally we\ndemonstrate the utility of this Bayesian framework with a classification\nproblem in materials science using Bayes factors.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 19:42:56 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 00:13:02 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Maroulas", "Vasileios", ""], ["Nasrin", "Farzana", ""], ["Oballe", "Christopher", ""]]}, {"id": "1901.02058", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli and Eva Riccomagno", "title": "A geometric characterisation of sensitivity analysis in monomial models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity analysis in probabilistic discrete graphical models is usually\nconducted by varying one probability value at a time and observing how this\naffects output probabilities of interest. When one probability is varied then\nothers are proportionally covaried to respect the sum-to-one condition of\nprobability laws. The choice of proportional covariation is justified by a\nvariety of optimality conditions, under which the original and the varied\ndistributions are as close as possible under different measures of closeness.\nFor variations of more than one parameter at a time proportional covariation is\njustified in some special cases only. In this work, for the large class of\ndiscrete statistical models entertaining a regular monomial parametrisation, we\ndemonstrate the optimality of newly defined proportional multi-way schemes with\nrespect to an optimality criterion based on the notion of I-divergence. We\ndemonstrate that there are varying parameters choices for which proportional\ncovariation is not optimal and identify the sub-family of model distributions\nwhere the distance between the original distribution and the one where\nprobabilities are covaried proportionally is minimum. This is shown by adopting\na new formal, geometric characterization of sensitivity analysis in monomial\nmodels, which include a wide array of probabilistic graphical models. We also\ndemonstrate the optimality of proportional covariation for multi-way analyses\nin Naive Bayes classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 11:21:53 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 17:27:45 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Leonelli", "Manuele", ""], ["Riccomagno", "Eva", ""]]}, {"id": "1901.02062", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli", "title": "Sensitivity analysis beyond linearity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide array of graphical models can be parametrised to have atomic\nprobabilities represented by monomial functions. Such monomial structure has\nproven very useful when studying robustness under the assumption of a\nmultilinear model where all monomial have either zero or one exponents.\nRobustness in probabilistic graphical models is usually investigated by varying\nsome of the input probabilities and observing the effects of these on output\nprobabilities of interest. Here the assumption of multilinearity is relaxed and\na general approach for sensitivity analysis in non-multilinear models is\npresented. It is shown that in non-multilinear models sensitivity functions\nhave a polynomial form, conversely to multilinear models where these are simply\nlinear. The form of various divergences and distances under different\ncovariation schemes is also formally derived. Proportional covariation is\nproven to be optimal in non-multilinear models under some specific choices of\nvaried parameters. The methodology is illustrated throughout by an educational\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 16:32:34 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Leonelli", "Manuele", ""]]}, {"id": "1901.02117", "submitter": "Yajuan Si", "authors": "Yajuan Si and Peigen Zhou", "title": "Bayes-raking: Bayesian Finite Population Inference with Known Margins", "comments": null, "journal-ref": null, "doi": "10.1093/jssam/smaa008", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raking is widely used in categorical data modeling and survey practice but\nfaced with methodological and computational challenges. We develop a Bayesian\nparadigm for raking by incorporating the marginal constraints as a prior\ndistribution via two main strategies: 1) constructing the solution subspaces\nvia basis functions or projection matrix and 2) modeling soft constraints. The\nproposed Bayes-raking estimation integrates the models for the margins, the\nsample selection and response mechanism, and the outcome, with the capability\nto propagate all sources of uncertainty. Computation is done via Stan, and\ncodes are ready for public use. Simulation studies show that Bayes-raking can\nperform as well as raking with large samples and outperform in terms of\nvalidity and efficiency gains, especially with a sparse contingency table or\ndependent raking factors. We apply the new method to the Longitudinal Study of\nWellbeing study and demonstrate that model-based approaches significantly\nimprove inferential reliability and substantive findings as a unified survey\ninference framework.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 01:20:28 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 20:08:09 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 19:46:19 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 19:06:11 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Si", "Yajuan", ""], ["Zhou", "Peigen", ""]]}, {"id": "1901.02333", "submitter": "Anirvan Chakraborty Dr.", "authors": "Anirvan Chakraborty and Victor M. Panaretos", "title": "Testing for the Rank of a Covariance Operator", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we discern whether the covariance operator of a stochastic process is\nof reduced rank, and if so, what its precise rank is? And how can we do so at a\ngiven level of confidence? This question is central to a great deal of methods\nfor functional data, which require low-dimensional representations whether by\nfunctional PCA or other methods. The difficulty is that the determination is to\nbe made on the basis of i.i.d. replications of the process observed discretely\nand with measurement error contamination. This adds a ridge to the empirical\ncovariance, obfuscating the underlying dimension. We build a matrix-completion\ninspired test statistic that circumvents this issue by measuring the best\npossible least square fit of the empirical covariance's off-diagonal elements,\noptimised over covariances of given finite rank. For a fixed grid of\nsufficiently large size, we determine the statistic's asymptotic null\ndistribution as the number of replications grows. We then use it to construct a\nbootstrap implementation of a stepwise testing procedure controlling the\nfamily-wise error rate corresponding to the collection of hypotheses\nformalising the question at hand. Under minimal regularity assumptions we prove\nthat the procedure is consistent and that its bootstrap implementation is\nvalid. The procedure circumvents smoothing and associated smoothing parameters,\nis indifferent to measurement error heteroskedasticity, and does not assume a\nlow-noise regime. An extensive simulation study reveals an excellent practical\nperformance, stably across a wide range of settings, and the procedure is\nfurther illustrated by means of two data analyses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 14:52:07 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 14:31:55 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Chakraborty", "Anirvan", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1901.02398", "submitter": "Alexandre Moesching", "authors": "Alexandre M\\\"osching and Lutz Duembgen", "title": "Monotone Least Squares and Isotonic Quantiles", "comments": null, "journal-ref": null, "doi": "10.1214/19-EJS1659", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider bivariate observations $(X_1,Y_1), \\ldots, (X_n,Y_n)$ such that,\nconditional on the $X_i$, the $Y_i$ are independent random variables with\ndistribution functions $F_{X_i}$, where $(F_x)_x$ is an unknown family of\ndistribution functions. Under the sole assumption that $x \\mapsto F_x$ is\nisotonic with respect to stochastic order, one can estimate $(F_x)_x$ in two\nways:\n  (i) For any fixed $y$ one estimates the antitonic function $x \\mapsto F_x(y)$\nvia nonparametric monotone least squares, replacing the responses $Y_i$ with\nthe indicators $1_{[Y_i \\le y]}$.\n  (ii) For any fixed $\\beta \\in (0,1)$ one estimates the isotonic quantile\nfunction $x \\mapsto F_x^{-1}(\\beta)$ via a nonparametric version of regression\nquantiles.\n  We show that these two approaches are closely related, with (i) being more\nflexible than (ii). Then, under mild regularity conditions, we establish rates\nof convergence for the resulting estimators $\\hat{F}_x(y)$ and\n$\\hat{F}_x^{-1}(\\beta)$, uniformly over $(x,y)$ and $(x,\\beta)$ in certain\nrectangles as well as uniformly in $y$ or $\\beta$ for a fixed $x$.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 16:48:50 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 14:13:11 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["M\u00f6sching", "Alexandre", ""], ["Duembgen", "Lutz", ""]]}, {"id": "1901.02419", "submitter": "Gordon Chavez", "authors": "Gordon V. Chavez", "title": "Dynamic tail inference with log-Laplace volatility", "comments": "Preprint, 27 pages, 7 figures, 3 tables", "journal-ref": "Extremes 23, 287-315 (2020)", "doi": "10.1007/s10687-019-00368-w", "report-no": null, "categories": "stat.ME econ.EM math.ST q-fin.RM q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of models that enable predictive estimation of\ntime-varying extreme event probabilities in heavy-tailed and nonlinearly\ndependent time series. The models are a white noise process with conditionally\nlog-Laplace stochastic volatility. In contrast to other, similar stochastic\nvolatility formalisms, this process has analytic expressions for its\nconditional probabilistic structure that enable straightforward estimation of\ndynamically changing extreme event probabilities. The process and volatility\nare conditionally Pareto-tailed, with tail exponent given by the reciprocal of\nthe log-volatility's mean absolute innovation. This formalism can accommodate a\nwide variety of nonlinear dependence, as well as conditional power law-tail\nbehavior ranging from weakly non-Gaussian to Cauchy-like tails. We provide a\ncomputationally straightforward estimation procedure that uses an asymptotic\napproximation of the process' dynamic large deviation probabilities. We\ndemonstrate the estimator's utility with a simulation study. We then show the\nmethod's predictive capabilities on a simulated nonlinear time series where the\nvolatility is driven by the chaotic Lorenz system. Lastly we provide an\nempirical application, which shows that this simple modeling method can be\neffectively used for dynamic and predictive tail inference in financial time\nseries.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 17:43:38 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 18:23:26 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 18:17:24 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2019 17:20:59 GMT"}, {"version": "v5", "created": "Wed, 31 Jul 2019 16:46:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chavez", "Gordon V.", ""]]}, {"id": "1901.02614", "submitter": "Murray Aitkin", "authors": "Murray Aitkin", "title": "The Universal model and prior: multinomial GLMs", "comments": "27 page,s 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalises the exponential family GLM to allow arbitrary\ndistributions for the response variable. This is achieved by combining the\nmodel-assisted regression approach from survey sampling with the GLM scoring\nalgorithm, weighted by random draws from the posterior Dirichlet distribution\nof the support point probabilities of the multinomial distribution. The\ngeneralisation provides fully Bayesian analyses from the posterior sampling,\nwithout MCMC. Several examples are given, of published GLM data sets. The\napproach can be extended widely: an example of a GLMM extension is given.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 06:25:04 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Aitkin", "Murray", ""]]}, {"id": "1901.02776", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz and Nima Hejazi", "title": "Causal mediation analysis for stochastic interventions", "comments": null, "journal-ref": "Journal of the Royal Statistical Society, Series B (Statistical\n  Methodology), 2020", "doi": "10.1111/rssb.12362", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis in causal inference has traditionally focused on binary\nexposures and deterministic interventions, and a decomposition of the average\ntreatment effect in terms of direct and indirect effects. In this paper we\npresent an analogous decomposition of the \\textit{population intervention\neffect}, defined through stochastic interventions on the exposure. Population\nintervention effects provide a generalized framework in which a variety of\ninteresting causal contrasts can be defined, including effects for continuous\nand categorical exposures. We show that identification of direct and indirect\neffects for the population intervention effect requires weaker assumptions than\nits average treatment effect counterpart, under the assumption of no\nmediator-outcome confounders affected by exposure. In particular,\nidentification of direct effects is guaranteed in experiments that randomize\nthe exposure and the mediator. We discuss various estimators of the direct and\nindirect effects, including substitution, re-weighted, and efficient estimators\nbased on flexible regression techniques, allowing for multivariate mediators.\nOur efficient estimator is asymptotically linear under a condition requiring\n$n^{1/4}$-consistency of certain regression functions. We perform a simulation\nstudy in which we assess the finite-sample properties of our proposed\nestimators. We present the results of an illustrative study where we assess the\neffect of participation in a sports team on BMI among children, using mediators\nsuch as exercise habits, daily consumption of snacks, and overweight status.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 15:20:00 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 15:05:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Hejazi", "Nima", ""]]}, {"id": "1901.02945", "submitter": "Alan Lenarcic", "authors": "Alan Lenarcic and William Valdar", "title": "Algorithmic Bayesian Group Gibbs Selection", "comments": "Code for Group Bayes Estimator available to public in R package\n  format at https://github.com/lenarcica/BayesSpike.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection, with precedents in George and McCulloch (1993) and\nAbramovich et al. (1998), support credibility measures that relate model\nuncertainty, but computation can be costly when sparse priors are approximate.\nWe design an exact selection engine suitable for Gauss noise, t-distributed\nnoise, and logistic learning, benefiting from data-structures derived from\ncoordinate descent lasso. Gibbs sampler chains are stored in a compressed\nbinary format compatible with Equi-Energy (Kou et al., 2006) tempering. We\nachieve a grouped-effects selection model, similar to the setting for group\nlasso, to determine co-entry of coefficients into the model. We derive a\nfunctional integrand for group inclusion, and introduce a new MCMC switching\nstep to avoid numerical integration. Theorems show this step has exponential\nconvergence to target distribution. We demonstrate a role for group selection\nto inform on genetic decomposition in a diallel experiment, and identify\npotential quantitative trait loci in p > 40K Heterogenous Stock\nhaplotype/phenotype studies.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 22:02:46 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 15:18:53 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Lenarcic", "Alan", ""], ["Valdar", "William", ""]]}, {"id": "1901.02947", "submitter": "Yan Sun", "authors": "Yan Sun, Guanghua Lian, Zudi Lu, Jennifer Loveland, Isaac Blackhurst", "title": "An interval-valued GARCH model for range-measured return processes", "comments": "30 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1502.04740", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range-measured return contains more information than the traditional\nscalar-valued return. In this paper, we propose to model the [low, high] price\nrange as a random interval and suggest an interval-valued GARCH (Int-GARCH)\nmodel for the corresponding range-measured return process. Under the general\nframework of random sets, the model properties are investigated. Parameters are\nestimated by the maximum likelihood method, and the asymptotic properties are\nestablished. Empirical application to stocks and financial indices data sets\nsuggests that our Int-GARCH model overall outperforms the traditional GARCH for\nboth in-sample estimation and out-of-sample prediction of volatility.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 22:04:51 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Sun", "Yan", ""], ["Lian", "Guanghua", ""], ["Lu", "Zudi", ""], ["Loveland", "Jennifer", ""], ["Blackhurst", "Isaac", ""]]}, {"id": "1901.02991", "submitter": "Jason Poulos", "authors": "Kellie Ottoboni and Jason Poulos", "title": "Estimating population average treatment effects from experiments with\n  noncompliance", "comments": "Forthcoming, Journal of Causal Inference", "journal-ref": "Journal of Causal Inference, 8(1), 108-130 (2020)", "doi": "10.1515/jci-2018-0035", "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized control trials (RCTs) are the gold standard for estimating causal\neffects, but often use samples that are non-representative of the actual\npopulation of interest. We propose a reweighting method for estimating\npopulation average treatment effects in settings with noncompliance.\nSimulations show the proposed compliance-adjusted population estimator\noutperforms its unadjusted counterpart when compliance is relatively low and\ncan be predicted by observed covariates. We apply the method to evaluate the\neffect of Medicaid coverage on health care use for a target population of\nadults who may benefit from expansions to the Medicaid program. We draw RCT\ndata from the Oregon Health Insurance Experiment, where less than one-third of\nthose randomly selected to receive Medicaid benefits actually enrolled.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 01:35:38 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 20:22:25 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 14:42:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Ottoboni", "Kellie", ""], ["Poulos", "Jason", ""]]}, {"id": "1901.03090", "submitter": "Johannes Bracher", "authors": "Johannes Bracher and Leonhard Held", "title": "Endemic-epidemic models with discrete-time serial interval distributions\n  for infectious disease prediction", "comments": "A previous version of this paper had the title \"Multivariate\n  endemic-epidemic models with higher-order lags and an application to outbreak\n  detection\" (see v1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate count time series models are an important tool for the analysis\nand prediction of infectious disease spread. We consider the endemic-epidemic\nframework, an autoregressive model class for infectious disease surveillance\ncounts, and replace the default autoregression on counts from the previous time\nperiod with more flexible weighting schemes inspired by discrete-time serial\ninterval distributions. We employ three different parametric formulations, each\nwith an additional unknown weighting parameter estimated via a profile\nlikelihood approach, and compare them to an unrestricted nonparametric\napproach. The new methods are illustrated in a univariate analysis of dengue\nfever incidence in San Juan, Puerto Rico, and a spatio-temporal study of viral\ngastroenteritis in the twelve districts of Berlin. We assess the predictive\nperformance of the suggested models and several reference models at various\nforecast horizons. In both applications, the performance of the\nendemic-epidemic models is considerably improved by the proposed weighting\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 10:46:29 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 08:17:11 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 15:48:38 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Bracher", "Johannes", ""], ["Held", "Leonhard", ""]]}, {"id": "1901.03144", "submitter": "Svetlana Dubinkina", "authors": "Svetlana Dubinkina and Sangeetika Ruchi", "title": "Accounting for model error in Tempered Ensemble Transform Particle\n  Filter and its application to non-additive model error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we trivially extend Tempered (Localized) Ensemble Transform\nParticle Filter---T(L)ETPF---to account for model error. We examine T(L)ETPF\nperformance for non-additive model error in a low-dimensional and a\nhigh-dimensional test problem. The former one is a nonlinear toy model, where\nuncertain parameters are non-Gaussian distributed but model error is Gaussian\ndistributed. The latter one is a steady-state single-phase Darcy flow model,\nwhere uncertain parameters are Gaussian distributed but model error is\nnon-Gaussian distributed. The source of model error in the Darcy flow problem\nis uncertain boundary conditions. We comapare T(L)ETPF to a Regularized\n(Localized) Ensemble Kalman Filter---R(L)EnKF. We show that T(L)ETPF\noutperforms R(L)EnKF for both the low-dimensional and the high-dimensional\nproblem. This holds even when ensemble size of TLETPF is 100 while ensemble\nsize of R(L)EnKF is greater than 6000. As a side note, we show that TLETPF\ntakes less iterations than TETPF, which decreases computational costs; while\nRLEnKF takes more iterations than REnKF, which incerases computational costs.\nThis is due to an influence of localization on a tempering and a regularizing\nparameter.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 13:20:23 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Dubinkina", "Svetlana", ""], ["Ruchi", "Sangeetika", ""]]}, {"id": "1901.03182", "submitter": "Gautam Sabnis", "authors": "Gautam Sabnis, Yves Atchad\\'e, Prosper Dovonon", "title": "Bayesian variable selection in linear regression models with\n  instrumental variables", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many papers on high-dimensional statistics have proposed methods for variable\nselection and inference in linear regression models by relying explicitly or\nimplicitly on the assumption that all regressors are exogenous. However,\napplications abound where endogeneity arises from selection biases, omitted\nvariables, measurement errors, unmeasured confounding and many other challenges\ncommon to data collection Fan et al. (2014). The most common cure to\nendogeneity issues consists in resorting to instrumental variable (IV)\ninference. The objective of this paper is to present a Bayesian approach to\ntackling endogeneity in high-dimensional linear IV models. Using a working\nquasi-likelihood combined with an appropriate sparsity inducing spike-and-slab\nprior distribution, we develop a semi-parametric method for variable selection\nin high-dimensional linear models with endogeneous regressors within a\nquasi-Bayesian framework. We derive some conditions under which the\nquasi-posterior distribution is well defined and puts most of its probability\nmass around the true value of the parameter as $p \\rightarrow \\infty$. We\ndemonstrate through empirical work the fine performance of the proposed\napproach relative to some other alternatives. We also include include an\nempirical application that assesses the return on education by revisiting the\nwork of Angrist and Keueger (1991).\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 14:25:47 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Sabnis", "Gautam", ""], ["Atchad\u00e9", "Yves", ""], ["Dovonon", "Prosper", ""]]}, {"id": "1901.03210", "submitter": "Kevin Burke Dr", "authors": "M.C. Jones, Angela Noufaily, Kevin Burke", "title": "A Bivariate Power Generalized Weibull Distribution: a Flexible\n  Parametric Model for Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the flexible parametric analysis of bivariate survival\ndata. Elsewhere, we have extolled the virtues of the \"power generalized\nWeibull\" (PGW) distribution as an attractive vehicle for univariate parametric\nsurvival analysis: it is a tractable, parsimonious, model which interpretably\nallows for a wide variety of hazard shapes and, when adapted (to give an\nadapted PGW, or APGW, distribution), covers a wide variety of important\nspecial/limiting cases. Here, we additionally observe a frailty relationship\nbetween a PGW distribution with one value of the parameter which controls\ndistributional choice within the family and a PGW distribution with a smaller\nvalue of the same parameter. We exploit this frailty relationship to propose a\nbivariate shared frailty model with PGW marginal distributions: these marginals\nturn out to be linked by the so-called BB9 or \"power variance function\" copula.\nThis particular choice of copula is, therefore, a natural one in the current\ncontext. We then adapt the bivariate PGW distribution, in turn, to accommodate\nAPGW marginals. We provide a number of theoretical properties of the bivariate\nPGW and APGW models and show the potential of the latter for practical work via\nan illustrative example involving a well-known retinopathy dataset, for which\nthe analysis proves to be straightforward to implement and informative in its\noutcomes. The novelty in this article is in the appropriate combination of\nspecific ingredients into a coherent and successful whole.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 15:15:43 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Jones", "M. C.", ""], ["Noufaily", "Angela", ""], ["Burke", "Kevin", ""]]}, {"id": "1901.03212", "submitter": "Kevin Burke Dr", "authors": "Kevin Burke, M.C. Jones, Angela Noufaily", "title": "A Flexible Parametric Modelling Framework for Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general, flexible, parametric survival modelling framework\nwhich encompasses key shapes of hazard function (constant, increasing,\ndecreasing, up-then-down, down-then-up), various common survival distributions\n(log-logistic, Burr type XII, Weibull, Gompertz), and includes defective\ndistributions (i.e., cure models). This generality is achieved using four basic\ndistributional parameters: two scale-type parameters and two shape parameters.\nGeneralising to covariate dependence, the scale-type regression components\ncorrespond to accelerated failure time (AFT) and proportional hazards (PH)\nmodels. Therefore, this general formulation unifies the most popular survival\nmodels which allows us to consider the practical value of possible modelling\nchoices for survival data. Furthermore, in line with our proposed flexible\nbaseline distribution, we advocate the use of multi-parameter regression in\nwhich more than one distributional parameter depends on covariates - rather\nthan the usual convention of having a single covariate-dependent (scale)\nparameter. While many choices are available, we suggest introducing covariates\nthrough just one or other of the two scale parameters, which covers AFT and PH\nmodels, in combination with a `power' shape parameter, which allows for more\ncomplex non-AFT/non-PH effects, while the other shape parameter remains\ncovariate-independent, and handles automatic selection of the baseline\ndistribution. We explore inferential issues in simulations, both with and\nwithout a covariate, with particular focus on evidence concerning the need, or\notherwise, to include both AFT and PH parameters. We illustrate the efficacy of\nour modelling framework by investigating differences between treatment groups\nusing data from a lung cancer study and a melanoma study. Censoring is\naccommodated throughout.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 15:18:21 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Burke", "Kevin", ""], ["Jones", "M. C.", ""], ["Noufaily", "Angela", ""]]}, {"id": "1901.03227", "submitter": "Raif Rustamov", "authors": "Raif M. Rustamov", "title": "Closed-form Expressions for Maximum Mean Discrepancy with Applications\n  to Wasserstein Auto-Encoders", "comments": "Main paper is considerably shortened by moving some of the material\n  into Appendix", "journal-ref": null, "doi": null, "report-no": "TD:102449/2018-11-27", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Mean Discrepancy (MMD) has found numerous applications in\nstatistics and machine learning, most recently as a penalty in the Wasserstein\nAuto-Encoder (WAE). In this paper we compute closed-form expressions for\nestimating the Gaussian kernel based MMD between a given distribution and the\nstandard multivariate normal distribution. This formula reveals a connection to\nthe Baringhaus-Henze-Epps-Pulley (BHEP) statistic of the Henze-Zirkler test and\nprovides further insights about the MMD. We introduce the standardized version\nof MMD as a penalty for the WAE training objective, allowing for a better\ninterpretability of MMD values and more compatibility across different\nhyperparameter settings. Next, we propose using a version of batch\nnormalization at the code layer; this has the benefits of making the kernel\nwidth selection easier, reducing the training effort, and preventing outliers\nin the aggregate code distribution. Our experiments on synthetic and real data\nshow that the analytic formulation improves over the commonly used stochastic\napproximation of the MMD, and demonstrate that code normalization provides\nsignificant benefits when training WAEs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 15:43:58 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 17:53:22 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Rustamov", "Raif M.", ""]]}, {"id": "1901.03275", "submitter": "Timo Adam", "authors": "Timo Adam, Roland Langrock, Christian H. Wei{\\ss}", "title": "Penalized estimation of flexible hidden Markov models for time series of\n  counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models are versatile tools for modeling sequential\nobservations, where it is assumed that a hidden state process selects which of\nfinitely many distributions generates any given observation. Specifically for\ntime series of counts, the Poisson family often provides a natural choice for\nthe state-dependent distributions, though more flexible distributions such as\nthe negative binomial or distributions with a bounded range can also be used.\nHowever, in practice, choosing an adequate class of (parametric) distributions\nis often anything but straightforward, and an inadequate choice can have severe\nnegative consequences on the model's predictive performance, on state\nclassification, and generally on inference related to the system considered. To\naddress this issue, we propose an effectively nonparametric approach to fitting\nhidden Markov models to time series of counts, where the state-dependent\ndistributions are estimated in a completely data-driven way without the need to\nselect a distributional family. To avoid overfitting, we add a roughness\npenalty based on higher-order differences between adjacent count probabilities\nto the likelihood, which is demonstrated to produce smooth probability mass\nfunctions of the state-dependent distributions. The feasibility of the\nsuggested approach is assessed in a simulation experiment, and illustrated in\ntwo real-data applications, where we model the distribution of i) major\nearthquake counts and ii) acceleration counts of an oceanic whitetip shark\n(Carcharhinus longimanus) over time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:05:55 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Adam", "Timo", ""], ["Langrock", "Roland", ""], ["Wei\u00df", "Christian H.", ""]]}, {"id": "1901.03277", "submitter": "Kevin Burke Dr", "authors": "Kevin Burke, Gilbert MacKenzie", "title": "Multi-Parameter Regression Survival Modelling: An Alternative to\n  Proportional Hazards", "comments": null, "journal-ref": "Biometrics 73 (2017) 678-686", "doi": "10.1111/biom.12625", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is standard practice for covariates to enter a parametric model through a\nsingle distributional parameter of interest, for example, the scale parameter\nin many standard survival models. Indeed, the well-known proportional hazards\nmodel is of this kind. In this paper we discuss a more general approach whereby\ncovariates enter the model through more than one distributional parameter\nsimultaneously (e.g., scale and shape parameters). We refer to this practice as\n\"multi-parameter regression\" (MPR) modelling and explore its use in a survival\nanalysis context. We find that multi-parameter regression leads to more\nflexible models which can offer greater insight into the underlying data\ngenerating process. To illustrate the concept, we consider the two-parameter\nWeibull model which leads to time-dependent hazard ratios, thus relaxing the\ntypical proportional hazards assumption and motivating a new test of\nproportionality. A novel variable selection strategy is introduced for such\nmulti-parameter regression models. It accounts for the correlation arising\nbetween the estimated regression coefficients in two or more linear predictors\n-- a feature which has not been considered by other authors in similar\nsettings. The methods discussed have been implemented in the mpr package in R.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:13:05 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Burke", "Kevin", ""], ["MacKenzie", "Gilbert", ""]]}, {"id": "1901.03311", "submitter": "Bruno Sudret", "authors": "M. Moustapha and B. Sudret", "title": "Surrogate-assisted reliability-based design optimization: a survey and a\n  new general framework", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2019-001", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability-based design optimization (RBDO) is an active field of research\nwith an ever increasing number of contributions. Numerous methods have been\nproposed for the solution of RBDO, a complex problem that combines optimization\nand reliability analysis. Classical approaches are based on approximation\nmethods and have been classified in review papers. In this paper, we first\nreview classical approaches based on approximation methods such as FORM, and\nalso more recent methods that rely upon surrogate modelling and Monte Carlo\nsimulation. We then propose a general framework for the solution of RBDO\nproblems that includes three independent blocks, namely adaptive surrogate\nmodelling, reliability analysis and optimization. These blocks are\nnon-intrusive with respect to each other and can be plugged independently in\nthe framework. After a discussion on numerical considerations that require\nattention for the framework to yield robust solutions to various types of\nproblems, the latter is applied to three examples (using two analytical\nfunctions and a finite element model). Kriging and support vector machines\ntogether with their own active learning schemes are considered in the surrogate\nmodel block. In terms of reliability analysis, the proposed framework is\nillustrated using both crude Monte Carlo and subset simulation. Finally, the\ncovariance-matrix adaptation - evolution scheme (CMA-ES), a global search\nalgorithm, or sequential quadratic programming (SQP), a local gradient-based\nmethod, are used in the optimization block. The comparison of the results to\nbenchmark studies show the effectiveness and efficiency of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 18:30:13 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Moustapha", "M.", ""], ["Sudret", "B.", ""]]}, {"id": "1901.03423", "submitter": "Eric Jay Daza", "authors": "Eric Jay Daza", "title": "Person as Population: A Longitudinal View of Single-Subject Causal\n  Inference for Analyzing Self-Tracked Health Data", "comments": "18 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-subject health data are becoming increasingly available thanks to\nadvances in self-tracking technology (e.g., wearable devices, mobile apps,\nsensors, implants). Many users and health caregivers seek to use such\nobservational time series data to recommend changing health practices in order\nto achieve desired health outcomes. However, there are few available causal\ninference approaches that are flexible enough to analyze such idiographic data.\nWe develop a recently introduced causal-analysis framework based on n-of-1\nrandomized trials, and implement a flexible random-forests g-formula approach\nto estimating a recurring individualized effect called the \"average period\ntreatment effect\". In the process, we argue that our approach essentially\nresembles that of a longitudinal study by partitioning a single time series\ninto periods taking on binary treatment levels. We analyze six years of the\nauthor's own self-tracked physical activity and weight data to demonstrate our\napproach, and compare the results of our analysis to one that does not properly\naccount for confounding.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 22:39:21 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 06:30:26 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 18:35:31 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Daza", "Eric Jay", ""]]}, {"id": "1901.03466", "submitter": "Haidong Li", "authors": "Haidong Li, Xiaoyun Xu, Yijie Peng, and Chun-Hung Chen", "title": "Efficient Sampling for Selecting Important Nodes in Random Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting important nodes in a random network,\nwhere the nodes connect to each other randomly with certain transition\nprobabilities. The node importance is characterized by the stationary\nprobabilities of the corresponding nodes in a Markov chain defined over the\nnetwork, as in Google's PageRank. Unlike deterministic network, the transition\nprobabilities in random network are unknown but can be estimated by sampling.\nUnder a Bayesian learning framework, we apply the first-order Taylor expansion\nand normal approximation to provide a computationally efficient posterior\napproximation of the stationary probabilities. In order to maximize the\nprobability of correct selection, we propose a dynamic sampling procedure which\nuses not only posterior means and variances of certain interaction parameters\nbetween different nodes, but also the sensitivities of the stationary\nprobabilities with respect to each interaction parameter. Numerical experiment\nresults demonstrate the superiority of the proposed sampling procedure.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 03:16:30 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Li", "Haidong", ""], ["Xu", "Xiaoyun", ""], ["Peng", "Yijie", ""], ["Chen", "Chun-Hung", ""]]}, {"id": "1901.03531", "submitter": "Chieh-Hsi Wu", "authors": "Chieh-Hsi Wu and Chris C. Holmes", "title": "Supervised variable selection in randomised controlled trials prior to\n  exploration of treatment effect heterogeneity: an example from severe malaria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration of treatment effect heterogeneity (TEH) is an increasingly\nimportant aspect of modern statistical analysis for stratified medicine in\nrandomised controlled trials (RCTs) as we start to gather more information on\ntrial participants and wish to maximise the opportunities for learning from\ndata. However, the analyst should refrain from including a large number of\nvariables in a treatment interaction discovery stage. Because doing so can\nsignificantly dilute the power to detect any true outcome-predictive\ninteractions between treatments and covariates. Current guidance is limited and\nmainly relies on the use of unsupervised learning methods, such as hierarchical\nclustering or principal components analysis, to reduce the dimension of the\nvariable space prior to interaction tests. In this article we show that\noutcome-driven dimension reduction, i.e. supervised variable selection, can\nmaintain power without inflating the type-I error or false-positive rate. We\nprovide theoretical and applied results to support our approach. The applied\nresults are obtained from illustrating our framework on the dataset from an RCT\nin severe malaria. We also pay particular attention to the internal risk model\napproach for TEH discovery, which we show is a particular case of our method\nand we point to improvements over current implementation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 09:58:20 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Wu", "Chieh-Hsi", ""], ["Holmes", "Chris C.", ""]]}, {"id": "1901.03590", "submitter": "Elad Domanovitz", "authors": "Elad Domanovitz and Uri Erez", "title": "On the Importance of Asymmetry and Monotonicity Constraints in Maximal\n  Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximal correlation coefficient is a well-established generalization of\nthe Pearson correlation coefficient for measuring non-linear dependence between\nrandom variables. It is appealing from a theoretical standpoint, satisfying\nR\\'enyi's axioms for a measure of dependence. It is also attractive from a\ncomputational point of view due to the celebrated alternating conditional\nexpectation algorithm, allowing to compute its empirical version directly from\nobserved data. Nevertheless, from the outset, it was recognized that the\nmaximal correlation coefficient suffers from some fundamental deficiencies,\nlimiting its usefulness as an indicator of estimation quality. Another\nwell-known measure of dependence is the correlation ratio which also suffers\nfrom some drawbacks. Specifically, the maximal correlation coefficient equals\none too easily whereas the correlation ratio equals zero too easily. The\npresent work recounts some attempts that have been made in the past to alter\nthe definition of the maximal correlation coefficient in order to overcome its\nweaknesses and then proceeds to suggest a natural variant of the maximal\ncorrelation coefficient as well as a modified conditional expectation algorithm\nto compute it. The proposed dependence measure at the same time resolves the\nmajor weakness of the correlation ratio measure and may be viewed as a bridge\nbetween these two classical measures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 14:26:34 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 18:36:54 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Domanovitz", "Elad", ""], ["Erez", "Uri", ""]]}, {"id": "1901.03742", "submitter": "Masoud  Nasari", "authors": "Masoud M Nasari, Mohamedou Ould-Haye", "title": "Confidence intervals with higher accuracy for short and long memory\n  linear processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an easy to implement method of stochastically weighing short\nand long memory linear processes is introduced. The method renders\nasymptotically exact size confidence intervals for the population mean which\nare significantly more accurate than their classical counterparts for each\nfixed sample size $n$. It is illustrated both theoretically and numerically\nthat the randomization framework of this paper produces randomized (asymptotic)\npivotal quantities, for the mean, which admit central limit theorems with\nsmaller magnitudes of error as compared to those of their leading classical\ncounterparts. An Edgeworth expansion result for randomly weighted linear\nprocesses whose innovations do not necessarily satisfy the Cramer condition, is\nalso established.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 20:44:49 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Nasari", "Masoud M", ""], ["Ould-Haye", "Mohamedou", ""]]}, {"id": "1901.03791", "submitter": "Matthew Williams", "authors": "Matthew R. Williams and Terrance D. Savitsky", "title": "Optimization of Survey Weights under a Large Number of Conflicting\n  Constraints", "comments": "23 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of survey data, sampling weights are needed for consistent\nestimation of the population. However, the original inverse probability weights\nfrom the survey sample design are typically modified to account for\nnon-response, to increase efficiency by incorporating auxiliary population\ninformation, and to reduce the variability in estimates due to extreme weights.\nIt is often the case that no single set of weights can be found which\nsuccessfully incorporates all of these modifications because together they\ninduce a large number of constraints and restrictions on the feasible solution\nspace. For example, a unique combination of categorical variables may not be\npresent in the sample data, even if the corresponding population level\ninformation is available. Additional requirements for weights to fall within\nspecified ranges may also lead to fewer population level adjustments being\nincorporated. We present a framework and accompanying computational methods to\naddress this issue of constraint achievement or selection within a restricted\nspace that will produce revised weights with reasonable properties. By\ncombining concepts from generalized raking, ridge and lasso regression,\nbenchmarking of small area estimates, augmentation of state-space equations,\npath algorithms, and data-cloning, this framework simultaneously selects\nconstraints and provides diagnostics suggesting why a fully constrained\nsolution is not possible. Combinatoric operations such as brute force\nevaluations of all possible combinations of constraints and restrictions are\navoided. We demonstrate this framework by applying alternative methods to\npost-stratification for the National Survey on Drug Use and Health. We also\ndiscuss strategies for scaling up to even larger data sets. Computations were\nperformed in R and code is available from the authors.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 03:28:55 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Williams", "Matthew R.", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1901.03797", "submitter": "Fei Xue", "authors": "Fei Xue and Annie Qu", "title": "Integrating multi-source block-wise missing data in model selection", "comments": "35 pages, 2 figures, accepted for publication in Journal of the\n  American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multi-source data, blocks of variable information from certain sources\nare likely missing. Existing methods for handling missing data do not take\nstructures of block-wise missing data into consideration. In this paper, we\npropose a Multiple Block-wise Imputation (MBI) approach, which incorporates\nimputations based on both complete and incomplete observations. Specifically,\nfor a given missing pattern group, the imputations in MBI incorporate more\nsamples from groups with fewer observed variables in addition to the group with\ncomplete observations. We propose to construct estimating equations based on\nall available information, and optimally integrate informative estimating\nfunctions to achieve efficient estimators. We show that the proposed method has\nestimation and model selection consistency under both fixed-dimensional and\nhigh-dimensional settings. Moreover, the proposed estimator is asymptotically\nmore efficient than the estimator based on a single imputation from complete\nobservations only. In addition, the proposed method is not restricted to\nmissing completely at random. Numerical studies and ADNI data application\nconfirm that the proposed method outperforms existing variable selection\nmethods under various missing mechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 04:47:15 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 04:01:13 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Xue", "Fei", ""], ["Qu", "Annie", ""]]}, {"id": "1901.03821", "submitter": "Ivan Fernandez-Val", "authors": "Shuowen Chen, Victor Chernozhukov and Iv\\'an Fern\\'andez-Val", "title": "Mastering Panel 'Metrics: Causal Impact of Democracy on Growth", "comments": "8 pages, 2 tables, includes supplementary appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between democracy and economic growth is of long-standing\ninterest. We revisit the panel data analysis of this relationship by Acemoglu,\nNaidu, Restrepo and Robinson (forthcoming) using state of the art econometric\nmethods. We argue that this and lots of other panel data settings in economics\nare in fact high-dimensional, resulting in principal estimators -- the fixed\neffects (FE) and Arellano-Bond (AB) estimators -- to be biased to the degree\nthat invalidates statistical inference. We can however remove these biases by\nusing simple analytical and sample-splitting methods, and thereby restore valid\nstatistical inference. We find that the debiased FE and AB estimators produce\nsubstantially higher estimates of the long-run effect of democracy on growth,\nproviding even stronger support for the key hypothesis in Acemoglu, Naidu,\nRestrepo and Robinson (forthcoming). Given the ubiquitous nature of panel data,\nwe conclude that the use of debiased panel data estimators should substantially\nimprove the quality of empirical inference in economics.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 08:30:10 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Chen", "Shuowen", ""], ["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""]]}, {"id": "1901.03905", "submitter": "Lucy Gao", "authors": "Lucy L. Gao, Jacob Bien and Daniela Witten", "title": "Are Clusterings of Multiple Data Views Independent?", "comments": "20 pages, 4 figures, 1 table (main text); 15 pages, 9 figures\n  (supplement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Pioneer 100 (P100) Wellness Project (Price and others, 2017), multiple\ntypes of data are collected on a single set of healthy participants at multiple\ntimepoints in order to characterize and optimize wellness. One way to do this\nis to identify clusters, or subgroups, among the participants, and then to\ntailor personalized health recommendations to each subgroup. It is tempting to\ncluster the participants using all of the data types and timepoints, in order\nto fully exploit the available information. However, clustering the\nparticipants based on multiple data views implicitly assumes that a single\nunderlying clustering of the participants is shared across all data views. If\nthis assumption does not hold, then clustering the participants using multiple\ndata views may lead to spurious results. In this paper, we seek to evaluate the\nassumption that there is some underlying relationship among the clusterings\nfrom the different data views, by asking the question: are the clusters within\neach data view dependent or independent? We develop a new test for answering\nthis question, which we then apply to clinical, proteomic, and metabolomic\ndata, across two distinct timepoints, from the P100 study. We find that while\nthe subgroups of the participants defined with respect to any single data type\nseem to be dependent across time, the clustering among the participants based\non one data type (e.g. proteomic data) appears not to be associated with the\nclustering based on another data type (e.g. clinical data).\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 21:57:34 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Gao", "Lucy L.", ""], ["Bien", "Jacob", ""], ["Witten", "Daniela", ""]]}, {"id": "1901.03948", "submitter": "Claudia Kluppelberg", "authors": "Claudia Kl\\\"uppelberg and Steffen Lauritzen", "title": "Bayesian Networks for Max-linear Models", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Bayesian networks based on max-linear structural equations as\nintroduced in Gissibl and Kl\\\"uppelberg [16] and provide a summary of their\nindependence properties. In particular we emphasize that distributions for such\nnetworks are generally not faithful to the independence model determined by\ntheir associated directed acyclic graph. In addition, we consider some of the\nbasic issues of estimation and discuss generalized maximum likelihood\nestimation of the coefficients, using the concept of a generalized likelihood\nratio for non-dominated families as introduced by Kiefer and Wolfowitz [21].\nFinally we argue that the structure of a minimal network asymptotically can be\nidentified completely from observational data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 08:13:58 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kl\u00fcppelberg", "Claudia", ""], ["Lauritzen", "Steffen", ""]]}, {"id": "1901.03981", "submitter": "Helen Blake", "authors": "Helen A. Blake, Clemence Leyrat, Kathryn E. Mansfield, Shaun Seaman,\n  Laurie A. Tomlinson (LSHTM), James Carpenter, Elizabeth J. Williamson", "title": "Propensity scores using missingness pattern information: a practical\n  guide", "comments": "46 pages, 7 figures (2 in appendix), 3 tables (1 in appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records are a valuable data source for investigating\nhealth-related questions, and propensity score analysis has become an\nincreasingly popular approach to address confounding bias in such\ninvestigations. However, because electronic health records are typically\nroutinely recorded as part of standard clinical care, there are often missing\nvalues, particularly for potential confounders. In our motivating study --\nusing electronic health records to investigate the effect of renin-angiotensin\nsystem blockers on the risk of acute kidney injury -- two key confounders,\nethnicity and chronic kidney disease stage, have $59\\%$ and $53\\%$ missing\ndata, respectively.\n  The missingness pattern approach (MPA), a variant of the missing indicator\napproach, has been proposed as a method for handling partially observed\nconfounders in propensity score analysis. In the MPA, propensity scores are\nestimated separately for each missingness pattern present in the data. Although\nthe assumptions underlying the validity of the MPA are stated in the\nliterature, it can be difficult in practice to assess their plausibility.\n  In this paper, we explore the MPA's underlying assumptions by using causal\ndiagrams to assess their plausibility in a range of simple scenarios, drawing\ngeneral conclusions about situations in which they are likely to be violated.\nWe present a framework providing practical guidance for assessing whether the\nMPA's assumptions are plausible in a particular setting and thus deciding when\nthe MPA is appropriate. We apply our framework to our motivating study, showing\nthat the MPA's underlying assumptions appear reasonable, and we demonstrate the\napplication of MPA to this study.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 13:52:23 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Blake", "Helen A.", "", "LSHTM"], ["Leyrat", "Clemence", "", "LSHTM"], ["Mansfield", "Kathryn E.", "", "LSHTM"], ["Seaman", "Shaun", "", "LSHTM"], ["Tomlinson", "Laurie A.", "", "LSHTM"], ["Carpenter", "James", ""], ["Williamson", "Elizabeth J.", ""]]}, {"id": "1901.04030", "submitter": "Shiwei Lan", "authors": "Shiwei Lan", "title": "Learning Temporal Evolution of Spatial Dependence with Generalized\n  Spatiotemporal Gaussian Process Models", "comments": "Temporal Evolution of Spatial Dependence (TESD), Spatiotemporal\n  Gaussian process (STGP), Non-separable Non-stationary Kernel, Quasi Kronecker\n  Product/Sum Structure, Nonparametric Spatiotemporal Covariance Model; 53\n  pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of scientific studies and engineering problems involve\nhigh-dimensional spatiotemporal data with complicated relationships. In this\npaper, we focus on a type of space-time interaction named \\emph{temporal\nevolution of spatial dependence (TESD)}, which is a zero time-lag\nspatiotemporal covariance. For this purpose, we propose a novel Bayesian\nnonparametric method based on non-stationary spatiotemporal Gaussian process\n(STGP). The classic STGP has a covariance kernel separable in space and time,\nfailed to characterize TESD. More recent works on non-separable STGP treat\nlocation and time together as a joint variable, which is unnecessarily\ninefficient. We generalize STGP (gSTGP) to introduce the time-dependence to the\nspatial kernel by varying its eigenvalues over time in the Mercer's\nrepresentation. The resulting non-separable non-stationary covariance model\nbares a quasi Kronecker sum structure. Finally, a hierarchical Bayesian model\nfor the joint covariance is proposed to allow for full flexibility in learning\nTESD. A simulation study and a longitudinal neuroimaging analysis on\nAlzheimer's patients demonstrate that the proposed methodology is\n(statistically) effective and (computationally) efficient in characterizing\nTESD. Theoretic properties of gSTGP including posterior contraction (for\ncovariance) are also studied.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 18:07:05 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 04:30:02 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 21:46:34 GMT"}, {"version": "v4", "created": "Thu, 18 Feb 2021 09:08:15 GMT"}, {"version": "v5", "created": "Wed, 14 Apr 2021 09:31:02 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lan", "Shiwei", ""]]}, {"id": "1901.04312", "submitter": "Chenyang Gu", "authors": "Chenyang Gu, Michael J. Lopez, Liangyuan Hu", "title": "The Estimation of Causal Effects of Multiple Treatments in Observational\n  Studies Using Bayesian Additive Regression Trees", "comments": "This article has been replaced by \"Estimation of Causal Effects of\n  Multiple Treatments in Observational Studies with a Binary Outcome\"\n  (arXiv:2001.06483 [stat.ME])", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is currently a dearth of appropriate methods to estimate the causal\neffects of multiple treatments when the outcome is binary. For such settings,\nwe propose the use of nonparametric Bayesian modeling, Bayesian Additive\nRegression Trees (BART). We conduct an extensive simulation study to compare\nBART to several existing, propensity score-based methods and to identify its\noperating characteristics when estimating average treatment effects on the\ntreated. BART consistently demonstrates low bias and mean-squared errors. We\nillustrate the use of BART through a comparative effectiveness analysis of a\nlarge dataset, drawn from the latest SEER-Medicare linkage, on patients who\nwere operated via robotic-assisted surgery, video-assisted thoratic surgery or\nopen thoracotomy.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 06:07:43 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 21:19:29 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Gu", "Chenyang", ""], ["Lopez", "Michael J.", ""], ["Hu", "Liangyuan", ""]]}, {"id": "1901.04326", "submitter": "Chris Oates", "authors": "Chris. J. Oates, Jon Cockayne, Dennis Prangle, T.J. Sullivan, Mark\n  Girolami", "title": "Optimality Criteria for Probabilistic Numerical Methods", "comments": "Prepared for the proceedings of the RICAM workshop on Multivariate\n  Algorithms and Information-Based Complexity, November 2018", "journal-ref": "Multivariate Algorithms and Information-Based Complexity, Radon\n  Series on Computational and Applied Mathematics 27:65--88, 2020", "doi": "10.1515/9783110635461-005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well understood that Bayesian decision theory and average case analysis\nare essentially identical. However, if one is interested in performing\nuncertainty quantification for a numerical task, it can be argued that standard\napproaches from the decision-theoretic framework are neither appropriate nor\nsufficient. Instead, we consider a particular optimality criterion from\nBayesian experimental design and study its implied optimal information in the\nnumerical context. This information is demonstrated to differ, in general, from\nthe information that would be used in an average-case-optimal numerical method.\nThe explicit connection to Bayesian experimental design suggests several\ndistinct regimes in which optimal probabilistic numerical methods can be\ndeveloped.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 14:01:43 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 16:19:51 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Oates", "Chris. J.", ""], ["Cockayne", "Jon", ""], ["Prangle", "Dennis", ""], ["Sullivan", "T. J.", ""], ["Girolami", "Mark", ""]]}, {"id": "1901.04376", "submitter": "Lu Yang", "authors": "Lu Yang", "title": "Assessment of Regression Models with Discrete Outcomes Using\n  Quasi-Empirical Residual Distribution Functions", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2021.1910042", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making informed decisions about model adequacy has been an outstanding issue\nfor regression models with discrete outcomes. Standard assessment tools for\nsuch outcomes (e.g. deviance residuals) often show a large discrepancy from the\nhypothesized pattern even under the true model and are not informative\nespecially when data are highly discrete (e.g. binary). To fill this gap, we\npropose a quasi-empirical residual distribution function for general discrete\n(e.g. ordinal and count) outcomes that serves as an alternative to the\nempirical Cox-Snell residual distribution function. The assessment tool we\npropose is a principled approach and does not require injecting noise into the\ndata. When at least one continuous covariate is available, we show\nasymptotically that the proposed function converges uniformly to the identity\nfunction under the correctly specified model, even with highly discrete\noutcomes. Through simulation studies, we demonstrate empirically that the\nproposed quasi-empirical residual distribution function outperforms commonly\nused residuals for various model assessment tasks, since it is close to the\nhypothesized pattern under the true model and significantly departs from this\npattern under model misspecification, and is thus an effective assessment tool.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:11:35 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 17:34:03 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 21:46:00 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yang", "Lu", ""]]}, {"id": "1901.04443", "submitter": "Alvaro E. Cordero-Franco", "authors": "W. J. Conover, Victor G. Tercero, Alvaro E. Cordero-Franco", "title": "An Approach to Statistical Process Control that is New, Nonparametric,\n  Simple, and Powerful", "comments": "92 pages, 33 tables and 48 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To maintain the desired quality of a product or service it is necessary to\nmonitor the process that results in the product or service. This monitoring\nmethod is called Statistical Process Management, or Statistical Process\nControl. It is in widespread usage in industry. Extensive statistical\nmethodology has been developed to make it possible to detect when a process\ngoes out of control while allowing for natural variability that occurs when the\nprocess is in control. This paper introduces nonparametric methods for\nmonitoring data, whether it is univariate or multivariate, and whether the\ninterest is in detecting a change of location or scale or both. These methods,\nbased on sequential normal scores, are much simpler than the most popular\nnonparametric methods currently in use and have good power for detecting out of\ncontrol observations. Sixteen new statistical tests are introduced for the\nfirst time in this paper, with 17 examples, 33 tables, and 48 figures to\ncomplete the instructions for their application.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 18:19:11 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Conover", "W. J.", ""], ["Tercero", "Victor G.", ""], ["Cordero-Franco", "Alvaro E.", ""]]}, {"id": "1901.04597", "submitter": "Katie Mollan", "authors": "Katie R. Mollan, Ilana M. Trumble, Sarah A. Reifeis, Orlando Ferrer,\n  Camden P. Bay, Pedro L. Baldoni, and Michael G. Hudgens", "title": "Exact Power of the Rank-Sum Test for a Continuous Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate power calculations are essential in small studies containing\nexpensive experimental units or high-stakes exposures. Herein, exact power of\nthe Wilcoxon Mann-Whitney rank-sum test of a continuous variable is formulated\nusing a Monte Carlo approach and defining P(X < Y) = p as a measure of effect\nsize, where X and Y denote random observations from two distributions\nhypothesized to be equal under the null. Effect size p fosters productive\ncommunications because researchers understand p = 0.5 is analogous to a fair\ncoin toss, and p near 0 or 1 represents a large effect. This approach is\nfeasible even without background data. Simulations were conducted comparing the\nexact power approach to existing approaches by Rosner & Glynn (2009), Shieh et\nal. (2006), Noether (1987), and O'Brien-Castelloe (2006). Approximations by\nNoether and O'Brien-Castelloe are shown to be inaccurate for small sample\nsizes. The Rosner & Glynn and Shieh et al. approaches performed well in many\nsmall sample scenarios, though both are restricted to location-shift\nalternatives and neither approach is theoretically justified for small samples.\nThe exact method is recommended and available in the R package wmwpow.\n  KEYWORDS: Mann-Whitney test, Monte Carlo simulation, non-parametric, power\nanalysis, Wilcoxon rank-sum test\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 22:47:18 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Mollan", "Katie R.", ""], ["Trumble", "Ilana M.", ""], ["Reifeis", "Sarah A.", ""], ["Ferrer", "Orlando", ""], ["Bay", "Camden P.", ""], ["Baldoni", "Pedro L.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "1901.04599", "submitter": "Weichi Yao", "authors": "Weichi Yao, Halina Frydman and Jeffrey S. Simonoff", "title": "An Ensemble Method for Interval-Censored Time-to-Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval-censored data analysis is important in biomedical statistics for any\ntype of time-to-event response where the time of response is not known exactly,\nbut rather only known to occur between two assessment times. Many clinical\ntrials and longitudinal studies generate interval-censored data; one common\nexample occurs in medical studies that entail periodic follow-up. In this paper\nwe propose a survival forest method for interval-censored data based on the\nconditional inference framework. We describe how this framework can be adapted\nto the situation of interval-censored data. We show that the tuning parameters\nhave a non-negligible effect on the survival forest performance and guidance is\nprovided on how to tune the parameters in a data-dependent way to improve the\noverall performance of the method. Using Monte Carlo simulations we find that\nthe proposed survival forest is at least as effective as a survival tree method\nwhen the underlying model has a tree structure, performs similarly to an\ninterval-censored Cox proportional hazards model fit when the true relationship\nis linear, and outperforms the survival tree method and Cox model when the true\nrelationship is nonlinear. We illustrate the application of the method on a\ntooth emergence data set.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 22:52:55 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 21:28:15 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 15:51:18 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Yao", "Weichi", ""], ["Frydman", "Halina", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "1901.04678", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo Lewi Hammer and Anis Yazidi", "title": "Parameter Estimation in Abruptly Changing Dynamic Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-life dynamical systems change abruptly followed by almost\nstationary periods. In this paper, we consider streams of data with such abrupt\nbehavior and investigate the problem of tracking their statistical properties\nin an online manner.\n  We devise a tracking procedure where an estimator that is suitable for a\nstationary environment is combined together with an event detection method such\nthat the estimator rapidly can jump to a more suitable value if an event is\ndetected. Combining an estimation procedure with detection procedure is\ncommonly known idea in the literature. However, our contribution lies in\nbuilding the detection procedure based on the difference between the stationary\nestimator and a Stochastic Learning Weak Estimator (SLWE). The SLWE estimator\nis known to be the state-of-the art approach to tracking properties of\nnon-stationary environments and thus should be a better choice to detect\nchanges in abruptly changing environments than the far more common sliding\nwindow based approaches. To the best of our knowledge, the event detection\nprocedure suggested by Ross et al. (2012) is the only procedure in the\nliterature taking advantage of the powerful tracking properties of the SLWE\nestimator. The procedure in Ross et al. is however quite complex and not well\nfounded theoretically compared to the procedures in this paper. In this paper,\nwe focus on estimation procedure for the binomial and multinomial\ndistributions, but our approach can be easily generalized to cover other\ndistributions as well.\n  Extensive simulation results based on both synthetic and real-life data\nrelated to news classification demonstrate that our estimation procedure is\neasy to tune and performs well.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 06:44:44 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Hammer", "Hugo Lewi", ""], ["Yazidi", "Anis", ""]]}, {"id": "1901.04681", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo Lewi Hammer, Anis Yazidi and H{\\aa}vard Rue", "title": "Quantile Tracking in Dynamically Varying Data Streams Using a\n  Generalized Exponentially Weighted Average of Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Exponentially Weighted Average (EWA) of observations is known to be\nstate-of-art estimator for tracking expectations of dynamically varying data\nstream distributions. However, how to devise an EWA estimator to rather track\nquantiles of data stream distributions is not obvious. In this paper, we\npresent a lightweight quantile estimator using a generalized form of the EWA.\nTo the best of our knowledge, this work represents the first reported quantile\nestimator of this form in the literature. An appealing property of the\nestimator is that the update step size is adjusted online proportionally to the\ndifference between current observation and the current quantile estimate. Thus,\nif the estimator is off-track compared to the data stream, large steps will be\ntaken to promptly get the estimator back on-track. The convergence of the\nestimator to the true quantile is proven using the theory of stochastic\nlearning.\n  Extensive experimental results using both synthetic and real-life data show\nthat our estimator clearly outperforms legacy state-of-the-art quantile\ntracking estimators and achieves faster adaptivity in dynamic environments. The\nquantile estimator was further tested on real-life data where the objective is\nefficient online control of indoor climate. We show that the estimator can be\nincorporated into a concept drift detector for efficiently decide when a\nmachine learning model used to predict future indoor temperature should be\nretrained/updated.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 07:10:21 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Hammer", "Hugo Lewi", ""], ["Yazidi", "Anis", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1901.04691", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca and Mat\\'u\\v{s} Maciak", "title": "Change-point Detection by the Quantile LASSO Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simultaneous change-point detection and estimation in a piece-wise constant\nmodel is a common task in modern statistics. If, in addition, the whole\nestimation can be performed automatically, in just one single step without\ngoing through any hypothesis tests for non-identifiable models, or unwieldy\nclassical a-posterior methods, it becomes an interesting, but also challenging\nidea. In this paper we introduce the estimation method based on the quantile\nLASSO approach. Unlike standard LASSO approaches, our method does not rely on\ntypical assumptions usually required for the model errors, such as sub-Gaussian\nor Normal distribution. The proposed quantile LASSO method can effectively\nhandle heavy-tailed random error distributions, and, in general, it offers a\nmore complex view of the data as one can obtain any conditional quantile of the\ntarget distribution, not just the conditional mean. It is proved that under\nsome reasonable assumptions the number of change-points is not underestimated\nwith probability tenting to one, and, in addition, when the number of\nchange-points is estimated correctly, the change-point estimates provided by\nthe quantile LASSO are consistent. Numerical simulations are used to\ndemonstrate these results and to illustrate the empirical performance robust\nfavor of the proposed quantile LASSO method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 07:32:06 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Ciuperca", "Gabriela", ""], ["Maciak", "Mat\u00fa\u0161", ""]]}, {"id": "1901.04695", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo Lewi Hammer", "title": "Statistical models for short and long term forecasts of snow depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting of future snow depths is useful for many applications like road\nsafety, winter sport activities, avalanche risk assessment and hydrology.\nMotivated by the lack of statistical forecasts models for snow depth, in this\npaper we present a set of models to fill this gap. First, we present a model to\ndo short term forecasts when we assume that reliable weather forecasts of air\ntemperature and precipitation are available. The covariates are included\nnonlinearly into the model following basic physical principles of snowfall,\nsnow aging and melting. Due to the large set of observations with snow depth\nequal to zero, we use a zero-inflated gamma regression model, which is commonly\nused to similar applications like precipitation. We also do long term forecasts\nof snow depth and much further than traditional weather forecasts for\ntemperature and precipitation. The long-term forecasts are based on fitting\nmodels to historic time series of precipitation, temperature and snow depth. We\nfit the models to data from three locations in Norway with different climatic\nproperties. Forecasting five days into the future, the results showed that,\ngiven reliable weather forecasts of temperature and precipitation, the forecast\nerrors in absolute value was between 3 and 7 cm for different locations in\nNorway. Forecasting three weeks into the future, the forecast errors were\nbetween 7 and 16 cm.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 07:44:40 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Hammer", "Hugo Lewi", ""]]}, {"id": "1901.04708", "submitter": "Kevin Burke Dr", "authors": "Kevin Burke, Frank Eriksson, C. B. Pipper", "title": "Semiparametric multi-parameter regression survival modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a log-linear model for survival data, where both the location and\nscale parameters depend on covariates and the baseline hazard function is\ncompletely unspecified. This model provides the flexibility needed to capture\nmany interesting features of survival data at a relatively low cost in model\ncomplexity. Estimation procedures are developed and asymptotic properties of\nthe resulting estimators are derived using empirical process theory. Finally, a\nresampling procedure is developed to estimate the limiting variances of the\nestimators. The finite sample properties of the estimators are investigated by\nway of a simulation study, and a practical application to lung cancer data is\nillustrated.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 08:39:36 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Burke", "Kevin", ""], ["Eriksson", "Frank", ""], ["Pipper", "C. B.", ""]]}, {"id": "1901.04744", "submitter": "Francisco Cuevas-Pacheco Mr.", "authors": "Jean-Fran\\c{c}ois Coeurjolly and Francisco Cuevas-Pacheco and Rasmus\n  Waagepetersen", "title": "Second-order variational equations for spatial point processes with a\n  view to pair correlation function estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second-order variational type equations for spatial point processes are\nestablished. In case of log linear parametric models for pair correlation\nfunctions, it is demonstrated that the variational equations can be applied to\nconstruct estimating equations with closed form solutions for the parameter\nestimates. This result is used to fit orthogonal series expansions of log pair\ncorrelation functions of general form.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 10:18:35 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Cuevas-Pacheco", "Francisco", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "1901.04795", "submitter": "Maarten van Smeden", "authors": "Bas B.L. Penning de Vries, Maarten van Smeden, Rolf H.H. Groenwold", "title": "A weighting method for simultaneous adjustment for confounding and joint\n  exposure-outcome misclassifications", "comments": "36 pages, 7 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint misclassification of exposure and outcome variables can lead to\nconsiderable bias in epidemiological studies of causal exposure-outcome\neffects. In this paper, we present a new maximum likelihood based estimator for\nthe marginal causal odd-ratio that simultaneously adjusts for confounding and\nseveral forms of joint misclassification of the exposure and outcome variables.\nThe proposed method relies on validation data for the construction of weights\nthat account for both sources of bias. The weighting estimator, which is an\nextension of the exposure misclassification weighting estimator proposed by\nGravel and Platt (Statistics in Medicine, 2018), is applied to reinfarction\ndata. Simulation studies were carried out to study its finite sample properties\nand compare it with methods that do not account for confounding or\nmisclassification. The new estimator showed favourable large sample properties\nin the simulations. Further research is needed to study the sensitivity of the\nproposed method and that of alternatives to violations of their assumptions.\nThe implementation of the estimator is facilitated by a new R function in an\nexisting R package.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 12:42:22 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["de Vries", "Bas B. L. Penning", ""], ["van Smeden", "Maarten", ""], ["Groenwold", "Rolf H. H.", ""]]}, {"id": "1901.04885", "submitter": "Jelle Goeman", "authors": "Jelle Goeman, Jesse Hemerik, Aldo Solari", "title": "Only Closed Testing Procedures are Admissible for Controlling False\n  Discovery Proportions", "comments": null, "journal-ref": null, "doi": "10.1214/20-AOS1999", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the class of all multiple testing methods controlling tail\nprobabilities of the false discovery proportion, either for one random set or\nsimultaneously for many such sets. This class encompasses methods controlling\nfamilywise error rate, generalized familywise error rate, false discovery\nexceedance, joint error rate, simultaneous control of all false discovery\nproportions, and others, as well as seemingly unrelated methods such as gene\nset testing in genomics and cluster inference methods in neuroimaging. We show\nthat all such methods are either equivalent to a closed testing method, or are\nuniformly improved by one. Moreover, we show that a closed testing method is\nadmissible as a method controlling tail probabilities of false discovery\nproportions if and only if all its local tests are admissible. This implies\nthat, when designing such methods, it is sufficient to restrict attention to\nclosed testing methods only. We demonstrate the practical usefulness of this\ndesign principle by constructing a uniform improvement of a recently proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 15:41:38 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:51:14 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Goeman", "Jelle", ""], ["Hemerik", "Jesse", ""], ["Solari", "Aldo", ""]]}, {"id": "1901.04916", "submitter": "Eben Kenah", "authors": "Yushuf Sharker and Eben Kenah", "title": "Pairwise accelerated failure time models for infectious disease\n  transmission with external sources of infection", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise survival analysis handles dependent happenings in infectious disease\ntransmission data by analyzing failure times in ordered pairs of individuals.\nThe contact interval in the pair $ij$ is the time from the onset of\ninfectiousness in $i$ to infectious contact from $i$ to $j$, where an\ninfectious contact is sufficient to infect $j$ if he or she is susceptible. The\ncontact interval distribution determines transmission probabilities and the\ninfectiousness profile of infected individuals. Many important questions in\ninfectious disease epidemiology involve the effects of covariates (e.g., age or\nvaccination status) on transmission. Here, we generalize earlier pairwise\nmethods in two ways: First, we introduce an accelerated failure time model that\nallows the contact interval rate parameter to depend on infectiousness\ncovariates for $i$, susceptibility covariates for $j$, and pairwise covariates.\nSecond, we show how internal infections (caused by individuals under\nobservation) and external infections (caused environmental or community\nsources) can be handled simultaneously. In simulations, we show that these\nmethods produce valid point and interval estimates and that accounting for\nexternal infections is critical to consistent estimation. Finally, we use these\nmethods to analyze household surveillance data from Los Angeles County during\nthe 2009 influenza A(H1N1) pandemic.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 02:30:22 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 19:15:59 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Sharker", "Yushuf", ""], ["Kenah", "Eben", ""]]}, {"id": "1901.05056", "submitter": "David Benkeser", "authors": "David Benkeser, Weixin Cai, Mark J van der Laan", "title": "A nonparametric super-efficient estimator of the average treatment\n  effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly robust estimators of causal effects are a popular means of estimating\ncausal effects. Such estimators combine an estimate of the conditional mean of\nthe outcome given treatment and confounders (the so-called outcome regression)\nwith an estimate of the conditional probability of treatment given confounders\n(the propensity score) to generate an estimate of the effect of interest. In\naddition to enjoying the double-robustness property, these estimators have\nadditional benefits. First, flexible regression tools, such as those developed\nin the field of machine learning, can be utilized to estimate the relevant\nregressions, while the estimators of the treatment effects retain desirable\nstatistical properties. Furthermore, these estimators are often statistically\nefficient, achieving the lower bound on the variance of regular, asymptotically\nlinear estimators. However, in spite of their asymptotic optimality, in\nproblems where causal estimands are weakly identifiable, these estimators may\nbehave erratically. We propose two new estimation techniques for use in these\nchallenging settings. Our estimators build on two existing frameworks for\nefficient estimation: targeted minimum loss estimation and one-step estimation.\nHowever, rather than using an estimate of the propensity score in their\nconstruction, we instead opt for an alternative regression quantity when\nbuilding our estimators: the conditional probability of treatment given the\nconditional mean outcome. We discuss the theoretical implications and\ndemonstrate the estimators' performance in simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 21:43:45 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Benkeser", "David", ""], ["Cai", "Weixin", ""], ["van der Laan", "Mark J", ""]]}, {"id": "1901.05178", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Jayant Jha and Prajamitra Bhuyan", "title": "Two-stage Circular-circular Regression with Zero-inflation: Application\n  to Medical Sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the modeling of zero-inflated circular measurements\nconcerning real case studies from medical sciences. Circular-circular\nregression models have been discussed in the statistical literature and\nillustrated with various real-life applications. However, there are no models\nto deal with zero-inflated response as well as a covariate simultaneously. The\nMobius transformation based two-stage circular-circular regression model is\nproposed, and the Bayesian estimation of the model parameters is suggested\nusing the MCMC algorithm. Simulation results show the superiority of the\nperformance of the proposed method over the existing competitors. The method is\napplied to analyse real datasets on astigmatism due to cataract surgery and\nabnormal gait related to orthopaedic impairment. The methodology proposed can\nassist in efficient decision making during treatment or post-operative care.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 08:44:19 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 14:50:20 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 08:44:29 GMT"}, {"version": "v4", "created": "Fri, 9 Oct 2020 10:09:27 GMT"}, {"version": "v5", "created": "Thu, 17 Dec 2020 11:42:45 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Jha", "Jayant", ""], ["Bhuyan", "Prajamitra", ""]]}, {"id": "1901.05186", "submitter": "Shunsuke Horii", "authors": "Shunsuke Horii, Tota Suko", "title": "A Note on the Estimation Method of Intervention Effects based on\n  Statistical Decision Theory", "comments": "6 pages, accepted to CISS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the problem of estimating the intervention effect\nin the statistical causal analysis using the structural equation model and the\ncausal diagram. The intervention effect is defined as a causal effect on the\nresponse variable $Y$ when the causal variable $X$ is fixed to a certain value\nby an external operation and is defined based on the causal diagram. The\nintervention effect is defined as a function of the probability distributions\nin the causal diagram, however, generally these probability distributions are\nunknown, so it is required to estimate them from data. In other words, the\nsteps of the estimation of the intervention effect using the causal diagram are\nas follows: 1. Estimate the causal diagram from the data, 2. Estimate the\nprobability distributions in the causal diagram from the data, 3. Calculate the\nintervention effect. However, if the problem of estimating the intervention\neffect is formulated in the statistical decision theory framework, estimation\nwith this procedure is not necessarily optimal. In this study, we formulate the\nproblem of estimating the intervention effect for the two cases, the case where\nthe causal diagram is known and the case where it is unknown, in the framework\nof statistical decision theory and derive the optimal decision method under the\nBayesian criterion. We show the effectiveness of the proposed method through\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 09:01:25 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Horii", "Shunsuke", ""], ["Suko", "Tota", ""]]}, {"id": "1901.05194", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Kiranmoy Chatterjee and Prajamitra Bhuyan", "title": "On the Estimation of Population Size from a Dependent Triple Record\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population size estimation based on capture-recapture experiment under triple\nrecord system is an interesting problem in various fields including\nepidemiology, population studies, etc. In many real life scenarios, there\nexists inherent dependency between capture and recapture attempts. We propose a\nnovel model that successfully incorporates the possible dependency and the\nassociated parameters possess nice interpretations. We provide estimation\nmethodology for the population size and the associated model parameters based\non maximum likelihood method. The proposed model is applied to analyze real\ndata sets from public health and census coverage evaluation study. The\nperformance of the proposed estimate is evaluated through extensive simulation\nstudy and the results are compared with the existing competitors. The results\nexhibit superiority of the proposed model over the existing competitors both in\nreal data analysis and simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 09:40:40 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 13:34:02 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chatterjee", "Kiranmoy", ""], ["Bhuyan", "Prajamitra", ""]]}, {"id": "1901.05229", "submitter": "Yuehan Yang", "authors": "Yuehan Yang and Hu Yang", "title": "Smooth Adjustment for Correlated Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a high dimensional linear regression model with\ncorrected variables. A variety of methods have been developed in recent years,\nyet it is still challenging to keep accurate estimation when there are complex\ncorrelation structures among predictors and the response. We propose an\nadaptive and \"reversed\" penalty for regularization to solve this problem. This\npenalty doesn't shrink variables but focuses on removing the shrinkage bias and\nencouraging grouping effect. Combining the l_1 penalty and the Minimax Concave\nPenalty (MCP), we propose two methods called Smooth Adjustment for Correlated\nEffects (SACE) and Generalized Smooth Adjustment for Correlated Effects\n(GSACE). Compared with the traditional adaptive estimator, the proposed methods\nhave less influence from the initial estimator and can reduce the false\nnegatives of the initial estimation. The proposed methods can be seen as linear\nfunctions of the new penalty's tuning parameter, and are shown to estimate the\ncoefficients accurately in both extremely highly correlated variables situation\nand weakly correlated variables situation. Under mild regularity conditions we\nprove that the methods satisfy certain oracle property. We show by simulations\nand applications that the proposed methods often outperforms other methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 11:00:44 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Yang", "Yuehan", ""], ["Yang", "Hu", ""]]}, {"id": "1901.05315", "submitter": "Divine Wanduku", "authors": "Divine Wanduku", "title": "On a family of stochastic SVIR influenza epidemic models and maximum\n  likelihood estimation", "comments": "Math. Model. in Health, Social and Appl. Sci., Springer", "journal-ref": null, "doi": "10.1007/978-981-15-2286-4_2", "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study presents a family of stochastic models for the dynamics of\ninfluenza in a closed human population. We consider treatment for the disease\nin the form of vaccination, and incorporate the periods of effectiveness of the\nvaccine and infectiousness for the individuals in the population. Our model is\na SVIR model, with trinomial transition probabilities, where all individuals\nwho recover from the disease acquire permanent natural immunity against the\nstrain of the disease. Special SVIR models in the family are presented, based\non the structure of the probability of getting infection and vaccination at any\ninstant. The methods of maximum likelihood, and expectation maximization are\nderived for the parameters of the chain. Moreover, estimators for some\nepidemiological assessment parameters, such as the basic reproduction number\nare computed. Numerical simulation examples are presented for the model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 16:15:03 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wanduku", "Divine", ""]]}, {"id": "1901.05501", "submitter": "Holger Drees", "authors": "Holger Drees, Miran Knezevic", "title": "Peak-over-Threshold Estimators for Spectral Tail Processes: Random vs\n  Deterministic Thresholds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extreme value dependence of regularly varying stationary time series can\nbe described by the spectral tail process. Drees, Segers and Warchol [Extremes\n18(3): 369--402, 2015] proposed estimators of the marginal distributions of\nthis process based on exceedances over high deterministic thresholds and\nanalyzed their asymptotic behavior. In practice, however, versions of the\nestimators are applied which use exceedances over random thresholds like\nintermediate order statistics. We prove that these modified estimators have the\nsame limit distributions. This finding is corroborated in a simulation study,\nbut the version using order statistics performs a bit better for finite\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 19:21:47 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 09:11:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Drees", "Holger", ""], ["Knezevic", "Miran", ""]]}, {"id": "1901.05739", "submitter": "Malka Gorfine", "authors": "Malka Gorfine, Matan Schlesinger, and Li Hsu", "title": "$K$-sample omnibus non-proportional hazards tests based on\n  right-censored data", "comments": null, "journal-ref": null, "doi": "10.1177/0962280220907355", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents novel and powerful tests for comparing non-proportional\nhazard functions, based on sample-space partitions. Right censoring introduces\ntwo major difficulties which make the existing sample-space partition tests for\nuncensored data non-applicable: (i) the actual event times of censored\nobservations are unknown; and (ii) the standard permutation procedure is\ninvalid in case the censoring distributions of the groups are unequal. We\novercome these two obstacles, introduce invariant tests, and prove their\nconsistency. Extensive simulations reveal that under non-proportional\nalternatives, the proposed tests are often of higher power compared with\nexisting popular tests for non-proportional hazards. Efficient implementation\nof our tests is available in the R package KONPsurv, which can be freely\ndownloaded from {https://github.com/matan-schles/KONPsurv\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 11:44:55 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 13:05:45 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gorfine", "Malka", ""], ["Schlesinger", "Matan", ""], ["Hsu", "Li", ""]]}, {"id": "1901.05768", "submitter": "William Haskell", "authors": "Songhao Wang, Szu Hui Ng, William Benjamin Haskell", "title": "A Multi-Level Simulation Optimization Approach for Quantile Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile is a popular performance measure for a stochastic system to evaluate\nits variability and risk. To reduce the risk, selecting the actions that\nminimize the tail quantiles of some loss distributions is typically of interest\nfor decision makers. When the loss distribution is observed via simulations,\nevaluating and optimizing its quantile functions can be challenging, especially\nwhen the simulations are expensive, as it may cost a large number of simulation\nruns to obtain accurate quantile estimators. In this work, we propose a\nmulti-level metamodel (co-kriging) based algorithm to optimize quantile\nfunctions more efficiently. Utilizing non-decreasing properties of quantile\nfunctions, we first search on cheaper and informative lower quantiles which are\nmore accurate and easier to optimize. The quantile level iteratively increases\nto the objective level while the search has a focus on the possible promising\nregions identified by the previous levels. This enables us to leverage the\naccurate information from the lower quantiles to find the optimums faster and\nimprove algorithm efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 12:45:58 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wang", "Songhao", ""], ["Ng", "Szu Hui", ""], ["Haskell", "William Benjamin", ""]]}, {"id": "1901.05796", "submitter": "Ting Wang", "authors": "Ting Wang, Edgar C. Merkle, Joaquin A. Anguera, Brandon M. Turner", "title": "Score-based Tests for Explaining Upper-Level Heterogeneity in Linear\n  Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-level interactions among fixed effects in linear mixed models (also\nknown as multilevel models) are often complicated by the variances stemming\nfrom random effects and residuals. When these variances change across clusters,\ntests of fixed effects (including cross-level interaction terms) are subject to\ninflated Type I or Type II error. While the impact of variance\nchange/heterogeneity has been noticed in the literature, few methods have been\nproposed to detect this heterogeneity in a simple, systematic way. In addition,\nwhen heterogeneity among clusters is detected, researchers often wish to know\nwhich clusters' variances differed from the others. In this study, we utilize a\nrecently-proposed family of score-based tests to distinguish between\ncross-level interactions and heterogeneity in variance components, also\nproviding information about specific clusters that exhibit heterogeneity. These\nscore-based tests only require estimation of the null model (when variance\nhomogeneity is assumed to hold), and they have been previously applied to\npsychometric models. We extend the tests to linear mixed models here, detailing\ntheir implementation and performance when the data generating model is known.\nWe also include an empirical example illustrating the tests' use in practice.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 14:15:40 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 02:46:44 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Wang", "Ting", ""], ["Merkle", "Edgar C.", ""], ["Anguera", "Joaquin A.", ""], ["Turner", "Brandon M.", ""]]}, {"id": "1901.06030", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "A robust functional time series forecasting method", "comments": "24 pages, 6 figures, 4 tables, to appear in Journal of Statistical\n  Computation and Simulation", "journal-ref": "https://www.tandfonline.com/doi/full/10.1080/00949655.2019.1572146", "doi": "10.1080/00949655.2019.1572146", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Univariate time series often take the form of a collection of curves observed\nsequentially over time. Examples of these include hourly ground-level ozone\nconcentration curves. These curves can be viewed as a time series of functions\nobserved at equally spaced intervals over a dense grid. Since functional time\nseries may contain various types of outliers, we introduce a robust functional\ntime series forecasting method to down-weigh the influence of outliers in\nforecasting. Through a robust principal component analysis based on projection\npursuit, a time series of functions can be decomposed into a set of robust\ndynamic functional principal components and their associated scores.\nConditioning on the estimated functional principal components, the crux of the\ncurve-forecasting problem lies in modeling and forecasting principal component\nscores, through a robust vector autoregressive forecasting method. Via a\nsimulation study and an empirical study on forecasting ground-level ozone\nconcentration, the robust method demonstrates the superior forecast accuracy\nthat dynamic functional principal component regression entails. The robust\nmethod also shows the superior estimation accuracy of the parameters in the\nvector autoregressive models for modeling and forecasting principal component\nscores, and thus improves curve forecast accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 23:16:34 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1901.06211", "submitter": "Leonie Weinhold", "authors": "Leonie Weinhold and Matthias Schmid and Marvin N. Wright and Moritz\n  Berger", "title": "A Random Forest Approach for Modeling Bounded Outcomes", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests have become an established tool for classification and\nregression, in particular in high-dimensional settings and in the presence of\ncomplex predictor-response relationships. For bounded outcome variables\nrestricted to the unit interval, however, classical random forest approaches\nmay severely suffer as they do not account for the heteroscedasticity in the\ndata. A random forest approach is proposed for relating beta distributed\noutcomes to explanatory variables. The approach explicitly makes use of the\nlikelihood function of the beta distribution for the selection of splits during\nthe tree-building procedure. In each iteration of the tree-building algorithm\none chooses the combination of explanatory variable and splitting rule that\nmaximizes the log-likelihood function of the beta distribution with the\nparameter estimates derived from the nodes of the currently built tree. Several\nsimulation studies demonstrate the properties of the method and compare its\nperformance to classical random forest approaches as well as to parametric\nregression models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 13:08:37 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Weinhold", "Leonie", ""], ["Schmid", "Matthias", ""], ["Wright", "Marvin N.", ""], ["Berger", "Moritz", ""]]}, {"id": "1901.06386", "submitter": "Fabian Telschow J. E.", "authors": "Fabian J.E. Telschow and Armin Schwartzman", "title": "Simultaneous Confidence Bands for Functional Data Using the Gaussian\n  Kinematic Formula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article constructs simultaneous confidence bands (SCBs) for functional\nparameters using the Gaussian Kinematic formula of $t$-processes (tGKF).\nAlthough the tGKF relies on Gaussianity, we show that a central limit theorem\n(CLT) for the parameter of interest is enough to obtain asymptotically precise\ncovering rates even for non-Gaussian processes. As a proof of concept we study\nthe functional signal-plus-noise model and derive a CLT for an estimator of the\nLipschitz-Killing curvatures, the only data dependent quantities in the tGKF\nSCBs. Extensions to discrete sampling with additive observation noise are\ndiscussed using scale space ideas from regression analysis. Here we provide\nsufficient conditions on the processes and kernels to obtain convergence of the\nfunctional scale space surface.\n  The theoretical work is accompanied by a simulation study comparing different\nmethods to construct SCBs for the population mean. We show that the tGKF works\nwell even for small sample sizes and only a Rademacher multiplier-$t$ bootstrap\nperforms similarily well. For larger sample sizes the tGKF often outperforms\nthe bootstrap methods and is computational faster. We apply the method to\ndiffusion tensor imaging (DTI) fibers using a scale space approach for the\ndifference of population means. R code is available in our Rpackage SCBfda.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 16:29:10 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Telschow", "Fabian J. E.", ""], ["Schwartzman", "Armin", ""]]}, {"id": "1901.06462", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Terrance D. Savitsky", "title": "Bayesian Pseudo Posterior Synthesis for Data Privacy Protection", "comments": "This is to replace arXiv:1908.07639", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical agencies utilize models to synthesize respondent-level data for\nrelease to the general public as an alternative to the actual data records. A\nBayesian model synthesizer encodes privacy protection by employing a\nhierarchical prior construction that induces smoothing of the real data\ndistribution. Synthetic respondent-level data records are often preferred to\nsummary data tables due to the many possible uses by researchers and data\nanalysts. Agencies balance a trade-off between utility of the synthetic data\nversus disclosure risks and hold a specific target threshold for disclosure\nrisk before releasing synthetic datasets. We introduce a pseudo posterior\nlikelihood that exponentiates each contribution by an observation\nrecord-indexed weight in (0, 1), defined to be inversely proportional to the\ndisclosure risk for that record in the synthetic data. Our use of a vector of\nweights allows more precise downweighting of high risk records in a fashion\nthat better preserves utility as compared with using a scalar weight. We\nillustrate our method with a simulation study and an application to the\nConsumer Expenditure Survey of the U.S. Bureau of Labor Statistics. We\ndemonstrate how the frequentist consistency and uncertainty quantification are\naffected by the inverse risk-weighting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 03:29:28 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 16:02:25 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 21:03:42 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hu", "Jingchen", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1901.06708", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Aydin Ghojogh, Mark Crowley, Fakhri Karray", "title": "Fitting A Mixture Distribution to Data: Tutorial", "comments": "12 pages, 9 figures, 1 table. Some typos are corrected in this\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a step-by-step tutorial for fitting a mixture distribution to\ndata. It merely assumes the reader has the background of calculus and linear\nalgebra. Other required background is briefly reviewed before explaining the\nmain algorithm. In explaining the main algorithm, first, fitting a mixture of\ntwo distributions is detailed and examples of fitting two Gaussians and\nPoissons, respectively for continuous and discrete cases, are introduced.\nThereafter, fitting several distributions in general case is explained and\nexamples of several Gaussians (Gaussian Mixture Model) and Poissons are again\nprovided. Model-based clustering, as one of the applications of mixture\ndistributions, is also introduced. Numerical simulations are also provided for\nboth Gaussian and Poisson examples for the sake of better clarification.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 18:17:40 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 20:49:26 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghojogh", "Aydin", ""], ["Crowley", "Mark", ""], ["Karray", "Fakhri", ""]]}, {"id": "1901.06750", "submitter": "Samuel Orso", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "A simple recipe for making accurate parametric inference in finite\n  sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Constructing tests or confidence regions that control over the error rates in\nthe long-run is probably one of the most important problem in statistics. Yet,\nthe theoretical justification for most methods in statistics is asymptotic. The\nbootstrap for example, despite its simplicity and its widespread usage, is an\nasymptotic method. There are in general no claim about the exactness of\ninferential procedures in finite sample. In this paper, we propose an\nalternative to the parametric bootstrap. We setup general conditions to\ndemonstrate theoretically that accurate inference can be claimed in finite\nsample.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 23:37:11 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1901.07147", "submitter": "Cheng Wang", "authors": "Cheng Wang, Binyan Jiang and Liping Zhu", "title": "Penalized Interaction Estimation for Ultrahigh Dimensional Quadratic\n  Regression", "comments": "33 pages, 2 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic regression goes beyond the linear model by simultaneously including\nmain effects and interactions between the covariates. The problem of\ninteraction estimation in high dimensional quadratic regression has received\nextensive attention in the past decade. In this article we introduce a novel\nmethod which allows us to estimate the main effects and interactions\nseparately. Unlike existing methods for ultrahigh dimensional quadratic\nregressions, our proposal does not require the widely used heredity assumption.\nIn addition, our proposed estimates have explicit formulas and obey the\ninvariance principle at the population level. We estimate the interactions of\nmatrix form under penalized convex loss function. The resulting estimates are\nshown to be consistent even when the covariate dimension is an exponential\norder of the sample size. We develop an efficient ADMM algorithm to implement\nthe penalized estimation. This ADMM algorithm fully explores the cheap\ncomputational cost of matrix multiplication and is much more efficient than\nexisting penalized methods such as all pairs LASSO. We demonstrate the\npromising performance of our proposal through extensive numerical studies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 01:47:01 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Cheng", ""], ["Jiang", "Binyan", ""], ["Zhu", "Liping", ""]]}, {"id": "1901.07277", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (LMO, CELESTE)", "title": "Minimal penalties and the slope heuristics: a survey", "comments": null, "journal-ref": "Journal de la Societe Fran{\\c c}aise de Statistique, Societe\n  Fran{\\c c}aise de Statistique et Societe Mathematique de France, 2019,\n  Minimal penalties and the slope heuristics: a survey, 160 (3), pp.1-106", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Birg{\\'e} and Massart proposed in 2001 the slope heuristics as a way to\nchoose optimally from data an unknown multiplicative constant in front of a\npenalty. It is built upon the notion of minimal penalty, and it has been\ngeneralized since to some \"minimal-penalty algorithms\". This paper reviews the\ntheoretical results obtained for such algorithms, with a self-contained proof\nin the simplest framework, precise proof ideas for further generalizations, and\na few new results. Explicit connections are made with residual-variance\nestimators-with an original contribution on this topic, showing that for this\ntask the slope heuristics performs almost as well as a residual-based estimator\nwith the best model choice-and some classical algorithms such as L-curve or\nelbow heuristics, Mallows' C p , and Akaike's FPE. Practical issues are also\naddressed, including two new practical definitions of minimal-penalty\nalgorithms that are compared on synthetic data to previously-proposed\ndefinitions. Finally, several conjectures and open problems are suggested as\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 11:58:12 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 12:41:53 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Arlot", "Sylvain", "", "LMO, CELESTE"]]}, {"id": "1901.07300", "submitter": "Alessandro Casa", "authors": "Alessandro Casa, Jos\\'e E. Chac\\'on and Giovanna Menardi", "title": "Modal clustering asymptotics with applications to bandwidth selection", "comments": null, "journal-ref": "Electronic Journal of Statistics, 14(1) 835-856, 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density-based clustering relies on the idea of linking groups to some\nspecific features of the probability distribution underlying the data. The\nreference to a true, yet unknown, population structure allows to frame the\nclustering problem in a standard inferential setting, where the concept of\nideal population clustering is defined as the partition induced by the true\ndensity function. The nonparametric formulation of this approach, known as\nmodal clustering, draws a correspondence between the groups and the domains of\nattraction of the density modes. Operationally, a nonparametric density\nestimate is required and a proper selection of the amount of smoothing,\ngoverning the shape of the density and hence possibly the modal structure, is\ncrucial to identify the final partition. In this work, we address the issue of\ndensity estimation for modal clustering from an asymptotic perspective. A\nnatural and easy to interpret metric to measure the distance between\ndensity-based partitions is discussed, its asymptotic approximation explored,\nand employed to study the problem of bandwidth selection for nonparametric\nmodal clustering.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 14:06:58 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Casa", "Alessandro", ""], ["Chac\u00f3n", "Jos\u00e9 E.", ""], ["Menardi", "Giovanna", ""]]}, {"id": "1901.07413", "submitter": "Louise Kimpton", "authors": "Louise Kimpton, Peter Challenor, Daniel Williamson", "title": "Modelling Numerical Systems with Two Distinct Labelled Output Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method of modelling numerical systems where there are two\ndistinct output solution classes, for example tipping points or bifurcations.\nGaussian process emulation is a useful tool in understanding these complex\nsystems and provides estimates of uncertainty, but we aim to include systems\nwhere there are discontinuities between the two output solutions. Due to\ncontinuity assumptions, we consider current methods of classification to split\nour input space into two output regions. Classification and logistic regression\nmethods currently rely on drawing from an independent Bernoulli distribution,\nwhich neglects any information known in the neighbouring area. We build on this\nby including correlation between our input points. Gaussian processes are still\na vital element, but used in latent space to model the two regions. Using the\ninput values and an associated output class label, the latent variable is\nestimated using MCMC sampling and a unique likelihood. A threshold (usually at\nzero) defines the boundary. We apply our method to a motivating example\nprovided by the hormones associated with the reproductive system in mammals,\nwhere the two solutions are associated with high and low rates of reproduction.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 15:29:27 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kimpton", "Louise", ""], ["Challenor", "Peter", ""], ["Williamson", "Daniel", ""]]}, {"id": "1901.07515", "submitter": "Zhong Guan", "authors": "Tao Wang and Zhong Guan", "title": "Bernstein Polynomial Model for Nonparametric Multivariate Density", "comments": "30 pages, 2 figures, the final version will be published in\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Bernstein polynomial model for estimating the\nmultivariate distribution functions and densities with bounded support. As a\nmixture model of multivariate beta distributions, the maximum (approximate)\nlikelihood estimate can be obtained using EM algorithm. A change-point method\nof choosing optimal degrees of the proposed Bernstein polynomial model is\npresented. Under some conditions the optimal rate of convergence in the mean\n$\\chi^2$-divergence of new density estimator is shown to be nearly parametric.\nThe method is illustrated by an application to a real data set. Finite sample\nperformance of the proposed method is also investigated by simulation study and\nis shown to be much better than the kernel density estimate but close to the\nparametric ones.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 18:44:36 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Tao", ""], ["Guan", "Zhong", ""]]}, {"id": "1901.07599", "submitter": "Zhiyang Zhou", "authors": "Zhiyang Zhou", "title": "Functional continuum regression", "comments": null, "journal-ref": "Journal of Multivariate Analysis 173: 328-346 (2019)", "doi": "10.1016/j.jmva.2019.03.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional principal component regression (PCR) can fail to provide good\nprediction if the response is highly correlated with some excluded functional\nprincipal component(s). This situation is common since the construction of\nfunctional principal components never involves the response. Aiming at this\nshortcoming, we develop functional continuum regression (CR). The framework of\nfunctional CR includes, as special cases, both functional PCR and functional\npartial least squares (PLS). Functional CR is expected to own a better accuracy\nthan functional PCR and functional PLS both in estimation and prediction;\nevidence for this is provided through simulation and numerical case studies.\nAlso, we demonstrate the consistency of estimators given by functional CR.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 20:04:38 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 07:12:40 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Zhou", "Zhiyang", ""]]}, {"id": "1901.07903", "submitter": "Jerome Stenger", "authors": "Jerome Stenger (EDF R&D PRISME, GdR MASCOT-NUM, IMT), Fabrice Gamboa\n  (IMT), Merlin Keller (EDF R&D PRISME), Bertrand Iooss (EDF R&D PRISME, IMT,\n  GdR MASCOT-NUM)", "title": "Optimal Uncertainty Quantification of a risk measurement from a\n  thermal-hydraulic code using Canonical Moments", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.12788", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an industrial computer code related to nuclear safety. A major topic\nof interest is to assess the uncertainties tainting the results of a computer\nsimulation. In this work we gain robustness on the quantification of a risk\nmeasurement by accounting for all sources of uncertainties tainting the inputs\nof a computer code. To that extent, we evaluate the maximum quantile over a\nclass of distributions defined only by constraints on their moments. Two\noptions are available when dealing with such complex optimization problems: one\ncan either optimize under constraints; or preferably, one should reformulate\nthe objective function. We identify a well suited parameterization to compute\nthe optimal quantile based on the theory of canonical moments. It allows an\neffective, free of constraints, optimization.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 10:22:59 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 14:08:00 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Stenger", "Jerome", "", "EDF R&D PRISME, GdR MASCOT-NUM, IMT"], ["Gamboa", "Fabrice", "", "IMT"], ["Keller", "Merlin", "", "EDF R&D PRISME"], ["Iooss", "Bertrand", "", "EDF R&D PRISME, IMT,\n  GdR MASCOT-NUM"]]}, {"id": "1901.07970", "submitter": "Cheng Yong Tang", "authors": "Cheng Yong Tang and Ethan X. Fang and Yuexiao Dong", "title": "High-dimensional Interactions Detection with Sparse Principal Hessian\n  Matrix", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical learning framework with regressions, interactions are the\ncontributions to the response variable from the products of the explanatory\nvariables. In high-dimensional problems, detecting interactions is challenging\ndue to combinatorial complexity and limited data information. We consider\ndetecting interactions by exploring their connections with the principal\nHessian matrix. Specifically, we propose a one-step synthetic approach for\nestimating the principal Hessian matrix by a penalized M-estimator. An\nalternating direction method of multipliers (ADMM) is proposed to efficiently\nsolve the encountered regularized optimization problem. Based on the sparse\nestimator, we detect the interactions by identifying its nonzero components.\nOur method directly targets at the interactions, and it requires no structural\nassumption on the hierarchy of the interaction effects. We show that our\nestimator is theoretically valid, computationally efficient, and practically\nuseful for detecting the interactions in a broad spectrum of scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 15:59:52 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 23:47:32 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Tang", "Cheng Yong", ""], ["Fang", "Ethan X.", ""], ["Dong", "Yuexiao", ""]]}, {"id": "1901.07976", "submitter": "Mark Meyer", "authors": "Mark J. Meyer, Jeffrey S. Morris, Regina Paxton Gazes, and Brent A.\n  Coull", "title": "Ordinal Probit Functional Outcome Regression with Application to\n  Computer-Use Behavior in Rhesus Monkeys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in functional regression has made great strides in expanding to\nnon-Gaussian functional outcomes, but exploration of ordinal functional\noutcomes remains limited. Motivated by a study of computer-use behavior in\nrhesus macaques (Macaca mulatta), we introduce the Ordinal Probit Functional\nOutcome Regression model (OPFOR). OPFOR models can be fit using one of several\nbasis functions including penalized B-splines, wavelets, and O'Sullivan splines\n-- the last of which typically performs best. Simulation using a variety of\nunderlying covariance patterns shows that the model performs reasonably well in\nestimation under multiple basis functions with near nominal coverage for joint\ncredible intervals. Finally, in application, we use Bayesian model selection\ncriteria adapted to functional outcome regression to best characterize the\nrelation between several demographic factors of interest and the monkeys'\ncomputer use over the course of a year. In comparison with a standard ordinal\nlongitudinal analysis, OPFOR outperforms a cumulative-link mixed-effects model\nin simulation and provides additional and more nuanced information on the\nnature of the monkeys' computer-use behavior.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 16:17:20 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 15:18:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Meyer", "Mark J.", ""], ["Morris", "Jeffrey S.", ""], ["Gazes", "Regina Paxton", ""], ["Coull", "Brent A.", ""]]}, {"id": "1901.08107", "submitter": "Kiranmoy Chatterjee Dr.", "authors": "Kiranmoy Chatterjee and Diganta Mukherjee", "title": "A new integrated likelihood for estimating population size in dependent\n  dual-record system", "comments": "19 Pages", "journal-ref": "Canadian Journal of Statistics. 46, 577-592 (2018)", "doi": "10.1002/cjs.11477", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient estimation of population size from dependent dual-record system\n(DRS) remains a statistical challenge in capture-recapture type experiment.\nOwing to the nonidentifiability of the suitable Time-Behavioral Response\nVariation model (denoted as $M_{tb}$) under DRS, few methods are developed in\nBayesian paradigm based on informative priors. Our contribution in this article\nis in developing integrated likelihood function from model $M_{tb}$ based on a\nnovel approach developed by Severini (2007, Biometrika). Suitable weight\nfunction on nuisance parameter is derived under the assumption of availability\nof knowledge on the direction of behavioral dependency. Such pseudo-likelihood\nfunction is constructed so that the resulting estimator possess some desirable\nproperties including invariance and negligible prior (or weight) sensitiveness.\nExtensive simulations explore the better performance of our proposed method in\nmost of the situations than the existing Bayesian methods. Moreover, being a\nnon-Bayesian estimator, it simply avoids heavy computational effort and time.\nFinally, illustration based on two real life data sets on epidemiology and\neconomic census are presented.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 19:57:05 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Chatterjee", "Kiranmoy", ""], ["Mukherjee", "Diganta", ""]]}, {"id": "1901.08117", "submitter": "Cecilia Balocchi", "authors": "Cecilia Balocchi and Shane T. Jensen", "title": "Spatial Modeling of Trends in Crime over Time in Philadelphia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationship between change in crime over time and the\ngeography of urban areas is an important problem for urban planning. Accurate\nestimation of changing crime rates throughout a city would aid law enforcement\nas well as enable studies of the association between crime and the built\nenvironment. Bayesian modeling is a promising direction since areal data\nrequire principled sharing of information to address spatial autocorrelation\nbetween proximal neighborhoods. We develop several Bayesian approaches to\nspatial sharing of information between neighborhoods while modeling trends in\ncrime counts over time. We apply our methodology to estimate changes in crime\nthroughout Philadelphia over the 2006-15 period, while also incorporating\nspatially-varying economic and demographic predictors. We find that the local\nshrinkage imposed by a conditional autoregressive model has substantial\nbenefits in terms of out-of-sample predictive accuracy of crime. We also\nexplore the possibility of spatial discontinuities between neighborhoods that\ncould represent natural barriers or aspects of the built environment.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 20:10:59 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 19:41:41 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Balocchi", "Cecilia", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1901.08169", "submitter": "Whitney Huang", "authors": "Whitney K. Huang, Daniel S. Cooley, Imme Ebert-Uphoff, Chen Chen, and\n  Snigdhansu Chatterjee", "title": "New Exploratory Tools for Extremal Dependence: Chi Networks and Annual\n  Extremal Networks", "comments": "26 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding dependence structure among extreme values plays an important\nrole in risk assessment in environmental studies. In this work we propose the\n$\\chi$ network and the annual extremal network for exploring the extremal\ndependence structure of environmental processes. A $\\chi$ network is\nconstructed by connecting pairs whose estimated upper tail dependence\ncoefficient, $\\hat \\chi$, exceeds a prescribed threshold. We develop an initial\n$\\chi$ network estimator and we use a spatial block bootstrap to assess both\nthe bias and variance of our estimator. We then develop a method to correct the\nbias of the initial estimator by incorporating the spatial structure in $\\chi$.\nIn addition to the $\\chi$ network, which assesses spatial extremal dependence\nover an extended period of time, we further introduce an annual extremal\nnetwork to explore the year-to-year temporal variation of extremal connections.\nWe illustrate the $\\chi$ and the annual extremal networks by analyzing the\nhurricane season maximum precipitation at the US Gulf Coast and surrounding\narea. Analysis suggests there exists long distance extremal dependence for\nprecipitation extremes in the study region and the strength of the extremal\ndependence may depend on some regional scale meteorological conditions, for\nexample, sea surface temperature.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 23:38:27 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Huang", "Whitney K.", ""], ["Cooley", "Daniel S.", ""], ["Ebert-Uphoff", "Imme", ""], ["Chen", "Chen", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "1901.08245", "submitter": "Masayo Hirose", "authors": "Masayo Y. Hirose and Partha Lahiri", "title": "Multi-Goal Prior Selection: A Way to Reconcile Bayesian and Classical\n  Approaches for Random Effects Models", "comments": "21 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-level normal hierarchical model has played an important role in\nstatistical theory and applications. In this paper, we first introduce a\ngeneral adjusted maximum likelihood method for estimating the unknown variance\ncomponent of the model and the associated empirical best linear unbiased\npredictor of the random effects. We then discuss a new idea for selecting prior\nfor the hyperparameters. The prior, called a multi-goal prior, produces\nBayesian solutions for hyperparmeters and random effects that match (in the\nhigher order asymptotic sense) the corresponding classical solution in linear\nmixed model with respect to several properties. Moreover, we establish for the\nfirst time an analytical equivalence of the posterior variances under the\nproposed multi-goal prior and the corresponding parametric bootstrap\nsecond-order mean squared error estimates in the context of a random effects\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 06:18:02 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Hirose", "Masayo Y.", ""], ["Lahiri", "Partha", ""]]}, {"id": "1901.08365", "submitter": "Tim Friede", "authors": "Tim Friede, Nigel Stallard and Nicholas Parsons", "title": "Seamless phase II/III clinical trials using early outcomes for treatment\n  or subgroup selection: Methods and aspects of their implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive seamless designs combine confirmatory testing, a domain of phase III\ntrials, with features such as treatment or subgroup selection, typically\nassociated with phase II trials. They promise to increase the efficiency of\ndevelopment programmes of new drugs, e.g. in terms of sample size and / or\ndevelopment time. It is well acknowledged that adaptive designs are more\ninvolved from a logistical perspective and require more upfront planning, often\nin form of extensive simulation studies, than conventional approaches. Here we\npresent a framework for adaptive treatment and subgroup selection using the\nsame notation, which links the somewhat disparate literature on treatment\nselection on one side and on subgroup selection on the other. Furthermore, we\nintroduce a flexible and yet efficient simulation model that serves both\ndesigns. As primary endpoints often take a long time to observe, interim\nanalyses are frequently informed by early outcomes. Therefore, all methods\npresented accommodate both, interim analyses informed by the primary outcome or\nan early outcome. The R package asd, previously developed to simulate designs\nwith treatment selection, was extended to include subpopulation selection\n(so-called adaptive enrichment designs). Here we describe the functionality of\nthe R package asd and use it to present some worked-up examples motivated by\nclinical trials in chronic obstructive pulmonary disease and oncology. The\nexamples illustrate various features of the R package providing at the same\ntime insights into the operating characteristics of adaptive seamless studies.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 11:50:01 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Friede", "Tim", ""], ["Stallard", "Nigel", ""], ["Parsons", "Nicholas", ""]]}, {"id": "1901.08589", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "Organic fiducial inference", "comments": "Final version with corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substantial generalisation is put forward of the theory of subjective\nfiducial inference as it was outlined in earlier papers. In particular, this\ntheory is extended to deal with cases where the data are discrete or\ncategorical rather than continuous, and cases where there was important\npre-data knowledge about some or all of the model parameters. The system for\ndirectly expressing and then handling this pre-data knowledge, which is via\nwhat are referred to as global and local pre-data functions for the parameters\nconcerned, is distinct from that which involves attempting to directly\nrepresent this knowledge in the form of a prior distribution function over\nthese parameters, and then using Bayes' theorem. In this regard, the individual\nattributes of what are identified as three separate types of fiducial argument,\nnamely the strong, moderate and weak fiducial arguments, form an integral part\nof the theory that is developed. Various practical examples of the application\nof this theory are presented, including examples involving binomial, Poisson\nand multinomial data. The fiducial distribution functions for the parameters of\nthe models in these examples are interpreted in terms of a generalised\ndefinition of subjective probability that was set out previously.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 19:35:29 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 16:28:29 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 17:38:28 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 16:48:25 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "1901.08738", "submitter": "Min Qian", "authors": "Min Qian, Bibhas Chakraborty, Raju Maiti and Ying Kuen Cheung", "title": "A Sequential Significance Test for Treatment by Covariate Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to patient heterogeneity in response to various aspects of any treatment\nprogram, biomedical and clinical research is gradually shifting from the\ntraditional \"one-size-fits-all\" approach to the new paradigm of personalized\nmedicine. An important step in this direction is to identify the treatment by\ncovariate interactions. We consider the setting in which there are potentially\na large number of covariates of interest. Although a number of novel machine\nlearning methodologies have been developed in recent years to aid in treatment\nselection in this setting, few, if any, have adopted formal hypothesis testing\nprocedures. In this article, we present a novel testing procedure based on\nm-out-of-n bootstrap that can be used to sequentially identify variables that\ninteract with treatment. We study the theoretical properties of the method and\nshow that it is more effective in controlling the type I error rate and\nachieving a satisfactory power as compared to competing methods, via extensive\nsimulations. Furthermore, the usefulness of the proposed method is illustrated\nusing real data examples, both from a randomized trial and from an\nobservational study.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 04:53:08 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 20:12:52 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Qian", "Min", ""], ["Chakraborty", "Bibhas", ""], ["Maiti", "Raju", ""], ["Cheung", "Ying Kuen", ""]]}, {"id": "1901.08741", "submitter": "Gery Geenens", "authors": "Gery Geenens", "title": "An essay on copula modelling for discrete random vectors; or how to pour\n  new wine into old bottles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas have now become ubiquitous statistical tools for describing,\nanalysing and modelling dependence between random variables. Sklar's theorem,\n\"the fundamental theorem of copulas\", makes a clear distinction between the\ncontinuous case and the discrete case, though. In particular, the copula of a\ndiscrete random vector is not identifiable, which causes serious\ninconsistencies. In spite of this, downplaying statements are widespread in the\nrelated literature, and copula methods are used for modelling dependence\nbetween discrete variables. This paper calls to reconsidering the soundness of\ncopula modelling for discrete data. It suggests a more fundamental construction\nwhich allows copula ideas to smoothly carry over to the discrete case. Actually\nit is an attempt at rejuvenating some century-old ideas of Udny Yule, who\nmentioned a similar construction a long time before copulas got in fashion.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 04:56:04 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 05:57:00 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Geenens", "Gery", ""]]}, {"id": "1901.08855", "submitter": "Jukka Sir\\'en", "authors": "Jukka Sir\\'en, Samuel Kaski", "title": "Local dimension reduction of summary statistics for likelihood-free\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) and other likelihood-free inference\nmethods have gained popularity in the last decade, as they allow rigorous\nstatistical inference for complex models without analytically tractable\nlikelihood functions. A key component for accurate inference with ABC is the\nchoice of summary statistics, which summarize the information in the data, but\nat the same time should be low-dimensional for efficiency. Several dimension\nreduction techniques have been introduced to automatically construct\ninformative and low-dimensional summaries from a possibly large pool of\ncandidate summaries. Projection-based methods, which are based on learning\nsimple functional relationships from the summaries to parameters, are widely\nused and usually perform well, but might fail when the assumptions behind the\ntransformation are not satisfied. We introduce a localization strategy for any\nprojection-based dimension reduction method, in which the transformation is\nestimated in the neighborhood of the observed data instead of the whole space.\nLocalization strategies have been suggested before, but the performance of the\ntransformed summaries outside the local neighborhood has not been guaranteed.\nIn our localization approach the transformation is validated and optimized over\nvalidation datasets, ensuring reliable performance. We demonstrate the\nimprovement in the estimation accuracy for localized versions of linear\nregression and partial least squares, for three different models of varying\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 12:45:01 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 10:25:15 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Sir\u00e9n", "Jukka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1901.08974", "submitter": "Amit Moscovich", "authors": "Amit Moscovich, Saharon Rosset", "title": "On the cross-validation bias due to unsupervised pre-processing", "comments": "31 pages, 6 figures, 1 table. New sections: (4.2.) Experiments on a\n  real dataset; (6.) Potential impact on model selection; (7.1.) Upper bounds\n  based on stability arguments. Updated Fig. 1. with larger sample sizes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation is the de facto standard for predictive model evaluation and\nselection. In proper use, it provides an unbiased estimate of a model's\npredictive performance. However, data sets often undergo various forms of\ndata-dependent preprocessing, such as mean-centering, rescaling, dimensionality\nreduction, and outlier removal. It is often believed that such preprocessing\nstages, if done in an unsupervised manner (that does not incorporate the class\nlabels or response values) are generally safe to do prior to cross-validation.\n  In this paper, we study three commonly-practiced preprocessing procedures\nprior to a regression analysis: (i) variance-based feature selection; (ii)\ngrouping of rare categorical features; and (iii) feature rescaling. We\ndemonstrate that unsupervised preprocessing can, in fact, introduce a\nsubstantial bias into cross-validation estimates and potentially hurt model\nselection. This bias may be either positive or negative and its exact magnitude\ndepends on all the parameters of the problem in an intricate manner. Further\nresearch is needed to understand the real-world impact of this bias across\ndifferent application domains, particularly when dealing with small sample\nsizes and high-dimensional data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 16:43:04 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 20:20:43 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 04:19:30 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 18:42:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Moscovich", "Amit", ""], ["Rosset", "Saharon", ""]]}, {"id": "1901.08984", "submitter": "Lulu Kang", "authors": "Yiou Li and Xiao Huang and Lulu Kang", "title": "A Discrepancy-Based Design for A/B Testing Experiments", "comments": "42 Pages 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to introduce a new design of experiment method for\nA/B tests in order to balance the covariate information in all treatment\ngroups. A/B tests (or \"A/B/n tests\") refer to the experiments and the\ncorresponding inference on the treatment effect(s) of a two-level or\nmulti-level controllable experimental factor. The common practice is to use a\nrandomized design and perform hypothesis tests on the estimates. However, such\nestimation and inference are not always accurate when covariate imbalance\nexists among the treatment groups. To overcome this issue, we propose a\ndiscrepancy-based criterion and show that the design minimizing this criterion\nsignificantly improves the accuracy of the treatment effect(s) estimates. The\ndiscrepancy-based criterion is model-free and thus makes the estimation of the\ntreatment effect(s) robust to the model assumptions. More importantly, the\nproposed design is applicable to both continuous and categorical response\nmeasurements. We develop two efficient algorithms to construct the designs by\noptimizing the criterion for both offline and online A/B tests. Through\nsimulation study and a real example, we show that the proposed design approach\nachieves good covariate balance and accurate estimation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 17:02:27 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Li", "Yiou", ""], ["Huang", "Xiao", ""], ["Kang", "Lulu", ""]]}, {"id": "1901.09138", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan", "title": "On doubly robust estimation for logistic partially linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a logistic partially linear model, in which the logit of the mean of\na binary response is related to a linear function of some covariates and a\nnonparametric function of other covariates. We derive simple, doubly robust\nestimators of coefficient for the covariates in the linear component of the\npartially linear model. Such estimators remain consistent if either a nuisance\nmodel is correctly specified for the nonparametric component, or another\nnuisance model is correctly specified for the means of the covariates of\ninterest given other covariates and the response at a fixed value. In previous\nworks, conditional density models are needed for the latter purposes unless a\nscalar, binary covariate is handled. We also propose two specific doubly robust\nestimators: one is locally-efficient like in our class of doubly robust\nestimators and the other is numerically and statistically simpler and can\nachieve reasonable efficiency especially when the true coefficients are close\nto 0.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 01:54:16 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Tan", "Zhiqiang", ""]]}, {"id": "1901.09174", "submitter": "Chaouki Ben Issaid", "authors": "Chaouki Ben Issaid, Mohamed-Slim Alouini and Raul Tempone", "title": "Eficient Monte Carlo Simulation of the Left Tail of Positive Gaussian\n  Quadratic Forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the left tail of quadratic forms in Gaussian random vectors is of\nmajor practical importance in many applications. In this paper, we propose an\nefficient and robust importance sampling estimator that is endowed with the\nbounded relative error property. This property significantly reduces the number\nof simulation runs required by the proposed estimator compared to naive Monte\nCarlo. Thus, our importance sampling estimator is especially useful when the\nprobability of interest is very small. Selected simulation results are\npresented to illustrate the efficiency of our estimator compared to naive Monte\nCarlo in both central and non-central cases, as well as both real and complex\nsettings.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 08:11:34 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Issaid", "Chaouki Ben", ""], ["Alouini", "Mohamed-Slim", ""], ["Tempone", "Raul", ""]]}, {"id": "1901.09249", "submitter": "Paul McNicholas", "authors": "Tyler Roick, Dimitris Karlis and Paul D. McNicholas", "title": "Clustering Discrete-Valued Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for the development of models that are able to account for\ndiscreteness in data, along with its time series properties and correlation.\nOur focus falls on INteger-valued AutoRegressive (INAR) type models. The INAR\ntype models can be used in conjunction with existing model-based clustering\ntechniques to cluster discrete-valued time series data. With the use of a\nfinite mixture model, several existing techniques such as the selection of the\nnumber of clusters, estimation using expectation-maximization and model\nselection are applicable. The proposed model is then demonstrated on real data\nto illustrate its clustering applications.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 17:36:51 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 20:10:12 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Roick", "Tyler", ""], ["Karlis", "Dimitris", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1901.09365", "submitter": "Janet Van Niekerk Dr", "authors": "Janet Van Niekerk, Haakon Bakka and Haavard Rue", "title": "Joint models as latent Gaussian models - not reinventing the wheel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models have received increasing attention during recent years with\nextensions into various directions; numerous hazard functions, different\nassociation structures, linear and non-linear longitudinal trajectories amongst\nothers. Many of these resulted in new R packages and new formulations of the\njoint model. However, a joint model with a linear bivariate Gaussian\nassociation structure is still a latent Gaussian model (LGM) and thus can be\nimplemented using most existing packages for LGM's. In this paper, we will show\nthat these joint models can be implemented from a LGM viewpoint using the\nR-INLA package. As a particular example, we will focus on the joint model with\na non-linear longitudinal trajectory, recently developed and termed the\npartially linear joint model. Instead of the usual spline approach, we argue\nfor using a Bayesian smoothing spline framework for the joint model that is\nstable with respect to knot selection and hence less cumbersome for\npractitioners.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 12:33:12 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Van Niekerk", "Janet", ""], ["Bakka", "Haakon", ""], ["Rue", "Haavard", ""]]}, {"id": "1901.09419", "submitter": "Kara Martinez", "authors": "Kara Martinez, Arnab Maity, Robert Yolken, Patrick Sullivan, Jung-Ying\n  Tzeng", "title": "The Robust Kernel Association Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing the association between SNP effects and a response is a common task.\nSuch tests are often carried out through kernel machine methods based on least\nsquares, such as the Sequence Kernel Association Test (SKAT). However, these\nleast squares procedures assume a normally distributed response, which is often\nviolated. Other robust procedures such as the Quantile Regression Kernel\nMachine (QRKM) restrict choice of loss function and only allow inference on\nconditional quantiles. We propose a general and robust kernel association test\nwith flexible choice of loss function, no distributional assumptions, and has\nSKAT and QRKM as special cases. We evaluate our proposed robust association\ntest (RobKAT) across various data distributions through simulation study. When\nerrors are normally distributed, RobKAT controls type I error and shows\ncomparable power to SKAT. In all other distributional settings investigated,\nour robust test has similar or greater power than SKAT. Finally, we apply our\nrobust kernel association test on data from the CATIE clinical trial to detect\nassociations between selected genes on chromosome 6, including the Major\nHistocompatibility Complex (MHC) region, and neurotrophic herpesvirus antibody\nlevels in schizophrenia patients. RobKAT detected significant association with\nfour SNP-sets (HST1H2BJ, MHC, POM12L2, and SLC17A1), three of which were\nundetected by SKAT.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 18:29:41 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Martinez", "Kara", ""], ["Maity", "Arnab", ""], ["Yolken", "Robert", ""], ["Sullivan", "Patrick", ""], ["Tzeng", "Jung-Ying", ""]]}, {"id": "1901.09472", "submitter": "Mats Julius Stensrud", "authors": "Mats J. Stensrud, Jessica G. Young, Vanessa Didelez, James M. Robins,\n  Miguel A. Hern\\'an", "title": "Separable Effects for Causal Inference in the Presence of Competing\n  Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-to-event settings, the presence of competing events complicates the\ndefinition of causal effects. Here we propose the new separable effects to\nstudy the causal effect of a treatment on an event of interest. The separable\ndirect effect is the treatment effect on the event of interest not mediated by\nits effect on the competing event. The separable indirect effect is the\ntreatment effect on the event of interest only through its effect on the\ncompeting event. Similar to Robins and Richardson's extended graphical approach\nfor mediation analysis, the separable effects can only be identified under the\nassumption that the treatment can be decomposed into two distinct components\nthat exert their effects through distinct causal pathways. Unlike existing\ndefinitions of causal effects in the presence of competing events, our\nestimands do not require cross-world contrasts or hypothetical interventions to\nprevent death. As an illustration, we apply our approach to a randomized\nclinical trial on estrogen therapy in individuals with prostate cancer.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 00:37:27 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 18:16:49 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 15:11:13 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Stensrud", "Mats J.", ""], ["Young", "Jessica G.", ""], ["Didelez", "Vanessa", ""], ["Robins", "James M.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1901.09551", "submitter": "Olatunji Johnson", "authors": "Olatunji Johnson, Peter Diggle and Emanuele Giorgi", "title": "A Spatially Discrete Approximation to Log-Gaussian Cox Processes for\n  Modelling Aggregated Disease Count Data", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": "10.1002/sim.8339", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we develop a computationally efficient discrete approximation\nto log-Gaussian Cox process (LGCP) models for the analysis of spatially\naggregated disease count data. Our approach overcomes an inherent limitation of\nspatial models based on Markov structures, namely that each such model is tied\nto a specific partition of the study area, and allows for spatially continuous\nprediction. We compare the predictive performance of our modelling approach\nwith LGCP through a simulation study and an application to primary biliary\ncirrhosis incidence data in Newcastle-Upon-Tyne, UK. Our results suggest that\nwhen disease risk is assumed to be a spatially continuous process, the proposed\napproximation to LGCP provides reliable estimates of disease risk both on\nspatially continuous and aggregated scales. The proposed methodology is\nimplemented in the open-source R package SDALGCP.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 08:50:44 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 19:41:36 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Johnson", "Olatunji", ""], ["Diggle", "Peter", ""], ["Giorgi", "Emanuele", ""]]}, {"id": "1901.09581", "submitter": "Satoshi Aoki", "authors": "Satoshi Aoki, Motomi Ito and Masakazu Shimada", "title": "Effect sizes of the differences between means without assuming the\n  variance equality and between a mean and a constant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hedges' d, an existing unbiased effect size of the difference between means,\nassumes the variance equality. However, the assumption of the variance equality\nis fragile, and is often violated in practical applications. Here, we define e,\na new effect size of the difference between means, which does not assume the\nvariance equality. In addition, another novel statistic c is defined as an\neffect size of the difference between a mean and a known constant. Hedges' g,\nour c, and e correspond to Student's unpaired two-sample t test, Student's\none-sample t test, and Welch's t test, respectively. An R package is also\nprovided to compute these effect sizes with their variance and confidence\ninterval.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 10:15:22 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Aoki", "Satoshi", ""], ["Ito", "Motomi", ""], ["Shimada", "Masakazu", ""]]}, {"id": "1901.09600", "submitter": "Nadezda Spiridovska", "authors": "Alexander M. Andronov, Nadezda Spiridovska", "title": "Markov-Modulated Linear Regression", "comments": "In proceedings book: International conference on Statistical Models\n  and Methods for Reliability and Survival Analysis and Their Validation\n  (S2MRSA), 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical linear regression is considered for a case when regression\nparameters depend on the external random environment. The last is described as\na continuous time Markov chain with finite state space. Here the expected\nsojourn times in various states are additional regressors. Necessary formulas\nfor an estimation of regression parameters have been derived. The numerical\nexample illustrates the results obtained.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 11:06:54 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Andronov", "Alexander M.", ""], ["Spiridovska", "Nadezda", ""]]}, {"id": "1901.09634", "submitter": "Kevin Burke Dr", "authors": "Defen Peng, Gilbert MacKenzie, Kevin Burke", "title": "A Multi-parameter regression model for interval censored survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop flexible multi-parameter regression survival models for interval\ncensored survival data arising in longitudinal prospective studies and\nlongitudinal randomised controlled clinical trials. A multi-parameter Weibull\nregression survival model, which is wholly parametric, and has non-proportional\nhazards, is the main focus of the paper. We describe the basic model, develop\nthe interval-censored likelihood and extend the model to include gamma frailty\nand a dispersion model. We evaluate the models by means of a simulation study\nand a detailed re-analysis of data from the Signal Tandmobiel$^{\\circledR}$\nstudy. The results demonstrate that the multi-parameter regression model with\nfrailty is computationally efficient and provides an excellent fit to the data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 13:10:09 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Peng", "Defen", ""], ["MacKenzie", "Gilbert", ""], ["Burke", "Kevin", ""]]}, {"id": "1901.09661", "submitter": "Yali Wan", "authors": "Yali Wan, Marina Meila", "title": "Measuring the Robustness of Graph Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a perturbation framework to measure the robustness\nof graph properties. Although there are already perturbation methods proposed\nto tackle this problem, they are limited by the fact that the strength of the\nperturbation cannot be well controlled. We firstly provide a perturbation\nframework on graphs by introducing weights on the nodes, of which the magnitude\nof perturbation can be easily controlled through the variance of the weights.\nMeanwhile, the topology of the graphs are also preserved to avoid\nuncontrollable strength in the perturbation. We then extend the measure of\nrobustness in the robust statistics literature to the graph properties.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 07:43:32 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Wan", "Yali", ""], ["Meila", "Marina", ""]]}, {"id": "1901.09779", "submitter": "Shovan Chowdhury", "authors": "Asok K Nanda and Shovan Chowdhury", "title": "Shannon's entropy and its Generalizations towards Statistics,\n  Reliability and Information Science during 1948-2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the pioneering works of Shannon and Weiner in 1948, a plethora\nof works have been reported on entropy in different directions. Entropy-related\nreview work in the direction of statistics, reliability and information\nscience, to the best of our knowledge, has not been reported so far. Here we\nhave tried to collect all possible works in this direction during the period\n1948-2018 so that people interested in entropy, specially the new researchers,\nget benefited.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:27:20 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Nanda", "Asok K", ""], ["Chowdhury", "Shovan", ""]]}, {"id": "1901.09828", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli and Michael Fop", "title": "A dynamic stochastic blockmodel for interaction lengths", "comments": "23 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new dynamic stochastic blockmodel that focuses on the analysis\nof interaction lengths in networks. The model does not rely on a discretization\nof the time dimension and may be used to analyze networks that evolve\ncontinuously over time. The framework relies on a clustering structure on the\nnodes, whereby two nodes belonging to the same latent group tend to create\ninteractions and non-interactions of similar lengths. We introduce a fast\nvariational expectation-maximization algorithm to perform inference, and adapt\na widely used clustering criterion to perform model choice. Finally, we test\nour methodology on artificial data, and propose a demonstration on a dataset\nconcerning face-to-face interactions between students in a high-school.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 17:25:34 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Fop", "Michael", ""]]}, {"id": "1901.09883", "submitter": "Daniel Garc\\'ia Iglesias", "authors": "Daniel Garc\\'ia Iglesias", "title": "Propensity Score Matching underestimates Real Treatment Effect, in a\n  simulated theoretical multivariate model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Propensity Score Matching (PSM) is an useful method to reduce the impact\nofTreatment - Selection Bias in the estimation of causal effects in\nobservational studies. After matching, the PSM significantly reduces the sample\nunder investigation, which may lead to other possible biases. In this sense, we\nwant to analyse the behaviour of this PSM compared with other widely used\nmethod to deal with non-comparable groups, as is the Multivariate Regression\nModel (MRM). Monte Carlo Simulations are made to construct groups with\ndifferent effects in order to compare the behaviour of PSM and MRM estimating\nthis effects. Also the Treatment - Selection Bias reduction for the PSM is\ncalculated. With the PSM a reduction in the Treatment - Selection Bias is\nachieved, with a reduction in the Relative Real Treatment Effect Estimation\nError, but despite of this bias reduction and estimation error reduction, the\nMRM significantly reduces more this estimation error compared with the PSM.\nAlso the PSM leads to a not insignificant reduction of the sample. This loss of\ninformation derived from the matching process may lead to another not known\nbias, and thus, to the inaccurate of the effect estimation compared with the\nMRM.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 18:55:53 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 20:34:37 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Iglesias", "Daniel Garc\u00eda", ""]]}, {"id": "1901.09917", "submitter": "David Watson", "authors": "David S. Watson and Marvin N. Wright", "title": "Testing Conditional Independence in Supervised Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose the conditional predictive impact (CPI), a consistent and unbiased\nestimator of the association between one or several features and a given\noutcome, conditional on a reduced feature set. Building on the knockoff\nframework of Cand\\`es et al. (2018), we develop a novel testing procedure that\nworks in conjunction with any valid knockoff sampler, supervised learning\nalgorithm, and loss function. The CPI can be efficiently computed for\nhigh-dimensional data without any sparsity constraints. We demonstrate\nconvergence criteria for the CPI and develop statistical inference procedures\nfor evaluating its magnitude, significance, and precision. These tests aid in\nfeature and model selection, extending traditional frequentist and Bayesian\ntechniques to general supervised learning tasks. The CPI may also be applied in\ncausal discovery to identify underlying multivariate graph structures. We test\nour method using various algorithms, including linear regression, neural\nnetworks, random forests, and support vector machines. Empirical results show\nthat the CPI compares favorably to alternative variable importance measures and\nother nonparametric tests of conditional independence on a diverse array of\nreal and simulated datasets. Simulations confirm that our inference procedures\nsuccessfully control Type I error and achieve nominal coverage probability. Our\nmethod has been implemented in an R package, cpi, which can be downloaded from\nhttps://github.com/dswatson/cpi.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 14:12:00 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 21:23:25 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 12:24:04 GMT"}, {"version": "v4", "created": "Mon, 16 Dec 2019 14:42:46 GMT"}, {"version": "v5", "created": "Thu, 13 May 2021 16:09:59 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Watson", "David S.", ""], ["Wright", "Marvin N.", ""]]}, {"id": "1901.09919", "submitter": "Muhammad Osama", "authors": "Muhammad Osama, Dave Zachariah, Thomas B. Sch\\\"on", "title": "Inferring Heterogeneous Causal Effects in Presence of Spatial\n  Confounding", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of inferring the causal effect of an exposure on an\noutcome across space, using observational data. The data is possibly subject to\nunmeasured confounding variables which, in a standard approach, must be\nadjusted for by estimating a nuisance function. Here we develop a method that\neliminates the nuisance function, while mitigating the resulting\nerrors-in-variables. The result is a robust and accurate inference method for\nspatially varying heterogeneous causal effects. The properties of the method\nare demonstrated on synthetic as well as real data from Germany and the US.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 14:29:34 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 15:19:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Osama", "Muhammad", ""], ["Zachariah", "Dave", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1901.09973", "submitter": "Jelena Markovic", "authors": "Jelena Markovic, Jonathan Taylor and Jeremy Taylor", "title": "Inference after black box selection", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inference for parameters selected to report only\nafter some algorithm, the canonical example being inference for model\nparameters after a model selection procedure. The conditional correction for\nselection requires knowledge of how the selection is affected by changes in the\nunderlying data, and current research explicitly describes this selection. In\nthis work, we assume 1) we have in silico access to the selection algorithm and\n2) for parameters of interest, the data input into the algorithm satisfies\n(pre-selection) a central limit theorem jointly with an estimator of our\nparameter of interest. Under these assumptions, we recast the problem into a\nstatistical learning problem which can be fit with off-the-shelf models for\nbinary regression. The feature points in this problem are set by the user,\nopening up the possibility of active learning methods for computationally\nexpensive selection algorithms. We consider two examples previously out of\nreach of this conditional approach: stability selection and multiple\ncross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 19:55:51 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Markovic", "Jelena", ""], ["Taylor", "Jonathan", ""], ["Taylor", "Jeremy", ""]]}, {"id": "1901.09982", "submitter": "Walter Dempsey", "authors": "Walter Dempsey and Brandon Oselio and Alfred Hero", "title": "Hierarchical network models for structured exchangeable interaction\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data often arises via a series of structured interactions among a\npopulation of constituent elements. E-mail exchanges, for example, have a\nsingle sender followed by potentially multiple receivers. Scientific articles,\non the other hand, may have multiple subject areas and multiple authors. We\nintroduce hierarchical edge exchangeable models for the study of these\nstructured interaction networks. In particular, we introduce the hierarchical\nvertex components model as a canonical example, which partially pools\ninformation via a latent, shared population-level distribution. Theoretical\nanalysis and supporting simulations provide clear model interpretation, and\nestablish global sparsity and power-law degree distribution. A computationally\ntractable Gibbs algorithm is derived. We demonstrate the model on both the\nEnron e-mail dataset and an ArXiv dataset, showing goodness of fit of the model\nvia posterior predictive validation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 20:12:57 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Dempsey", "Walter", ""], ["Oselio", "Brandon", ""], ["Hero", "Alfred", ""]]}, {"id": "1901.10068", "submitter": "Wei Ma", "authors": "Wei Ma, Zhen Qian", "title": "Statistical inference of probabilistic origin-destination demand using\n  day-to-day traffic data", "comments": null, "journal-ref": "Transportation Research Part C: Emerging Technologies 88 (2018):\n  227-256", "doi": "10.1016/j.trc.2017.12.015", "report-no": null, "categories": "stat.ME cs.NA cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent transportation network studies on uncertainty and reliability call for\nmodeling the probabilistic O-D demand and probabilistic network flow. Making\nthe best use of day-to-day traffic data collected over many years, this paper\ndevelops a novel theoretical framework for estimating the mean and\nvariance/covariance matrix of O-D demand considering the day-to-day variation\ninduced by travelers' independent route choices. It also estimates the\nprobability distributions of link/path flow and their travel cost where the\nvariance stems from three sources, O-D demand, route choice and unknown errors.\nThe framework estimates O-D demand mean and variance/covariance matrix\niteratively, also known as iterative generalized least squares (IGLS) in\nstatistics. Lasso regularization is employed to obtain sparse covariance matrix\nfor better interpretation and computational efficiency. Though the\nprobabilistic O-D estimation (ODE) works with a much larger solution space than\nthe deterministic ODE, we show that its estimator for O-D demand mean is no\nworse than the best possible estimator by an error that reduces with the\nincrease in sample size. The probabilistic ODE is examined on two small\nnetworks and two real-world large-scale networks. The solution converges\nquickly under the IGLS framework. In all those experiments, the results of the\nprobabilistic ODE are compelling, satisfactory and computationally plausible.\nLasso regularization on the covariance matrix estimation leans to underestimate\nmost of variance/covariance entries. A proper Lasso penalty ensures a good\ntrade-off between bias and variance of the estimation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 02:07:30 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ma", "Wei", ""], ["Qian", "Zhen", ""]]}, {"id": "1901.10186", "submitter": "Antonio Canale", "authors": "Martina Bravo, Antonio Canale", "title": "Pairwise likelihood inference for the multivariate ordered probit model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a closed form expression for the pairwise score vector\nfor the multivariate ordered probit model. This result has several implications\nin likelihood-based inference. It is indeed used both to speed-up gradient\nbased optimization routines for point estimation, and to provide a building\nblock to compute standard errors and confidence intervals by means of the\nGodambe matrix.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 09:17:10 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Bravo", "Martina", ""], ["Canale", "Antonio", ""]]}, {"id": "1901.10217", "submitter": "Gino Bertrand Kpogbezan", "authors": "Gino B. Kpogbezan, Mark A. van de Wiel, Wessel N. van Wieringen and\n  Aad W. van der Vaart", "title": "Incorporating prior information and borrowing information in\n  high-dimensional sparse regression using the horseshoe and variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a sparse high-dimensional regression approach that can\nincorporate prior information on the regression parameters and can borrow\ninformation across a set of similar datasets. Prior information may for\ninstance come from previous studies or genomic databases, and information\nborrowed across a set of genes or genomic networks. The approach is based on\nprior modelling of the regression parameters using the horseshoe prior, with a\nprior on the sparsity index that depends on external information. Multiple\ndatasets are integrated by applying an empirical Bayes strategy on\nhyperparameters. For computational efficiency we approximate the posterior\ndistribution using a variational Bayes method. The proposed framework is useful\nfor analysing large-scale data sets with complex dependence structures. We\nillustrate this by applications to the reconstruction of gene regulatory\nnetworks and to eQTL mapping.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 10:58:42 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kpogbezan", "Gino B.", ""], ["van de Wiel", "Mark A.", ""], ["van Wieringen", "Wessel N.", ""], ["van der Vaart", "Aad W.", ""]]}, {"id": "1901.10225", "submitter": "Sally Paganin", "authors": "Sally Paganin, Amy H. Herring, Andrew F. Olshan, David B. Dunson", "title": "Centered Partition Process: Informative Priors for Clustering", "comments": null, "journal-ref": null, "doi": "10.1214/20-BA1197", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a very rich literature proposing Bayesian approaches for clustering\nstarting with a prior probability distribution on partitions. Most approaches\nassume exchangeability, leading to simple representations in terms of\nExchangeable Partition Probability Functions (EPPF). Gibbs-type priors\nencompass a broad class of such cases, including Dirichlet and Pitman-Yor\nprocesses. Even though there have been some proposals to relax the\nexchangeability assumption, allowing covariate-dependence and partial\nexchangeability, limited consideration has been given on how to include\nconcrete prior knowledge on the partition. For example, we are motivated by an\nepidemiological application, in which we wish to cluster birth defects into\ngroups and we have prior knowledge of an initial clustering provided by\nexperts. As a general approach for including such prior knowledge, we propose a\nCentered Partition (CP) process that modifies the EPPF to favor partitions\nclose to an initial one. Some properties of the CP prior are described, a\ngeneral algorithm for posterior computation is developed, and we illustrate the\nmethodology through simulation examples and an application to the motivating\nepidemiology study of birth defects.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 11:18:19 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Paganin", "Sally", ""], ["Herring", "Amy H.", ""], ["Olshan", "Andrew F.", ""], ["Dunson", "David B.", ""]]}, {"id": "1901.10275", "submitter": "Mikko Heikkil\\\"a", "authors": "Mikko A. Heikkil\\\"a and Joonas J\\\"alk\\\"o and Onur Dikmen and Antti\n  Honkela", "title": "Differentially Private Markov Chain Monte Carlo", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in differentially private (DP) machine learning and DP\nBayesian learning have enabled learning under strong privacy guarantees for the\ntraining data subjects. In this paper, we further extend the applicability of\nDP Bayesian learning by presenting the first general DP Markov chain Monte\nCarlo (MCMC) algorithm whose privacy-guarantees are not subject to unrealistic\nassumptions on Markov chain convergence and that is applicable to posterior\ninference in arbitrary models. Our algorithm is based on a decomposition of the\nBarker acceptance test that allows evaluating the R\\'enyi DP privacy cost of\nthe accept-reject choice. We further show how to improve the DP guarantee\nthrough data subsampling and approximate acceptance tests.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:34:43 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 13:37:26 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Heikkil\u00e4", "Mikko A.", ""], ["J\u00e4lk\u00f6", "Joonas", ""], ["Dikmen", "Onur", ""], ["Honkela", "Antti", ""]]}, {"id": "1901.10296", "submitter": "David Hirshberg", "authors": "David A. Hirshberg and Arian Maleki and Jose R. Zubizarreta", "title": "Minimax Linear Estimation of the Retargeted Mean", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating treatments received by one population for application to a\ndifferent target population of scientific interest is a central problem in\ncausal inference from observational studies. We study the minimax linear\nestimator of the treatment-specific mean outcome on a target population and\nprovide a theoretical basis for inference based on it. In particular, we\nprovide a justification for the common practice of ignoring bias when building\nconfidence intervals with these linear estimators. Focusing on the case that\nthe class of the unknown outcome function is the unit ball of a reproducing\nkernel Hilbert space, we show that the resulting linear estimator is\nasymptotically optimal under conditions only marginally stronger than those\nused with augmented estimators. We establish bounds attesting to the\nestimator's good finite sample properties. In an extensive simulation study, we\nobserve promising performance of the estimator throughout a wide range of\nsample sizes, noise levels, and levels of overlap between the covariate\ndistributions of the treated and target populations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 04:01:58 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 12:47:22 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Hirshberg", "David A.", ""], ["Maleki", "Arian", ""], ["Zubizarreta", "Jose R.", ""]]}, {"id": "1901.10334", "submitter": "Alper Atamturk", "authors": "Alper Atamturk and Andres Gomez", "title": "Rank-one Convexification for Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": "BCOL 19.01, IEOR, UC Berkeley", "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse regression models are increasingly prevalent due to their ease of\ninterpretability and superior out-of-sample performance. However, the exact\nmodel of sparse regression with an $\\ell_0$ constraint restricting the support\nof the estimators is a challenging (\\NP-hard) non-convex optimization problem.\nIn this paper, we derive new strong convex relaxations for sparse regression.\nThese relaxations are based on the ideal (convex-hull) formulations for\nrank-one quadratic terms with indicator variables. The new relaxations can be\nformulated as semidefinite optimization problems in an extended space and are\nstronger and more general than the state-of-the-art formulations, including the\nperspective reformulation and formulations with the reverse Huber penalty and\nthe minimax concave penalty functions. Furthermore, the proposed rank-one\nstrengthening can be interpreted as a \\textit{non-separable, non-convex,\nunbiased} sparsity-inducing regularizer, which dynamically adjusts its penalty\naccording to the shape of the error function without inducing bias for the\nsparse solutions. In our computational experiments with benchmark datasets, the\nproposed conic formulations are solved within seconds and result in\nnear-optimal solutions (with 0.4\\% optimality gap) for non-convex\n$\\ell_0$-problems. Moreover, the resulting estimators also outperform\nalternative convex approaches from a statistical perspective, achieving high\nprediction accuracy and good interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 15:12:59 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 18:50:17 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Atamturk", "Alper", ""], ["Gomez", "Andres", ""]]}, {"id": "1901.10359", "submitter": "Bin Huang", "authors": "Bin Huang, Chen Chen and Jinzhong Liu", "title": "GPMatch: A Bayesian Doubly Robust Approach to Causal Inference with\n  Gaussian Process Covariance Function As a Matching Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) covariance function is proposed as a matching tool in\nGPMatch within a full Bayesian framework under relatively weaker causal\nassumptions. The matching is accomplished by utilizing GP prior covariance\nfunction to define matching distance. We show that GPMatch provides a doubly\nrobust estimate of the averaged treatment effect (ATE) much like the\nG-estimation, the ATE is correctly estimated when either conditions are\nsatisfied: 1) the GP mean function correctly specifies potential outcome\n\\(Y^{(0)}\\); or 2) the GP covariance function correctly specifies matching\nstructure. Simulation studies were carried out without assuming any known\nmatching structure nor functional form of the outcomes. The results demonstrate\nthat GPMatch enjoys well calibrated frequentist properties, and outperforms\nmany widely used methods including Bayesian Additive Regression Trees. The case\nstudy compares effectiveness of early aggressive use of biological medication\nin treating children with newly diagnosed Juvenile Idiopathic Arthritis, using\ndata extracted from electronic medical records.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 16:14:53 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 18:55:01 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Huang", "Bin", ""], ["Chen", "Chen", ""], ["Liu", "Jinzhong", ""]]}, {"id": "1901.10399", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Phalguni Nanda, Prajamitra Bhuyan and Anup Dewanji", "title": "Optimal Replacement Policy under Cumulative Damage Model and Strength\n  Degradation with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-life scenarios, system failure depends on dynamic\nstress-strength interference, where strength degrades and stress accumulates\nconcurrently over time. In this paper, we consider the problem of finding an\noptimal replacement strategy that balances the cost of replacement with the\ncost of failure and results in a minimum expected cost per unit time under\ncumulative damage model with strength degradation. The existing recommendations\nare applicable only under restricted distributional assumptions and/or with\nfixed strength. As theoretical evaluation of the expected cost per unit time\nturns out to be very complicated, a simulation-based algorithm is proposed to\nevaluate the expected cost rate and find the optimal replacement strategy. The\nproposed method is easy to implement having wider domain of application. For\nillustration, the proposed method is applied to real case studies on mailbox\nand cell-phone battery experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 21:39:33 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 08:18:27 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 15:03:30 GMT"}, {"version": "v4", "created": "Fri, 27 Mar 2020 09:51:45 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Nanda", "Phalguni", ""], ["Bhuyan", "Prajamitra", ""], ["Dewanji", "Anup", ""]]}, {"id": "1901.10505", "submitter": "Preetam Nandy", "authors": "Preetam Nandy, Kinjal Basu, Shaunak Chatterjee, Ye Tu", "title": "A/B Testing in Dense Large-Scale Networks: Design and Inference", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of experiments and estimation of treatment effects in large-scale\nnetworks, in the presence of strong interference, is a challenging and\nimportant problem. Most existing methods' performance deteriorates as the\ndensity of the network increases. In this paper, we present a novel strategy\nfor accurately estimating the causal effects of a class of treatments in a\ndense large-scale network. First, we design an approximate randomized\ncontrolled experiment by solving an optimization problem to allocate treatments\nin the presence of competition among neighboring nodes. Then we apply an\nimportance sampling adjustment to correct for any leftover bias (from the\napproximation) in estimating average treatment effects. We provide theoretical\nguarantees, verify robustness in a simulation study, and validate the\nscalability and usefulness of our procedure in a real-world experiment on a\nlarge social network.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 19:28:12 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 17:56:10 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 23:05:23 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 20:10:32 GMT"}, {"version": "v5", "created": "Mon, 14 Dec 2020 02:48:13 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nandy", "Preetam", ""], ["Basu", "Kinjal", ""], ["Chatterjee", "Shaunak", ""], ["Tu", "Ye", ""]]}, {"id": "1901.10516", "submitter": "Guobin Fang", "authors": "Guobin Fang, Huimin Ma, Michelle Xia, Bo Zhang", "title": "The FFBS Estimation of High Dimensional Panel Data Factor Stochastic\n  Volatility Models", "comments": "42 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, We propose a new style panel data factor stochastic volatility\nmodel with observable factors and unobservable factors based on the\nmultivariate stochastic volatility model, which is mainly composed of three\nparts, such as the mean equation, volatility equation and factor volatility\nevolution. The stochastic volatility equation is a 1-step forward prediction\nprocess with high dimensional parameters to be estimated. Using the Markov\nChain Monte Carlo Simulation (MCMC) method, the Forward Filtering Backward\nSampling (FFBS) algorithm of the stochastic volatility equation is mainly used\nto estimate the new model by Kalman Filter Recursive Algorithm (KFRA). The\nresults of numeric simulation and latent factor estimation show that the\nalgorithm possesses robustness and consistency for parameter estimation. This\npaper makes a comparative analysis of the observable and unobservable factors\nof internet finance and traditional financial listed companies in the Chinese\nstock market using the new model and its estimation method. The results show\nthat the influence of observable factors is similar to the two types of listed\ncompanies, but the influence of unobservable factors is obviously different.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 20:06:17 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 16:51:51 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Fang", "Guobin", ""], ["Ma", "Huimin", ""], ["Xia", "Michelle", ""], ["Zhang", "Bo", ""]]}, {"id": "1901.10543", "submitter": "Jameson Quinn", "authors": "Jameson Quinn", "title": "A High-Dimensional Particle Filter Algorithm", "comments": "17 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Online data assimilation in time series models over a large spatial extent is\nan important problem in both geosciences and robotics. Such models are\nintrinsically high-dimensional, rendering traditional particle filter\nalgorithms ineffective. Though methods that begin to address this problem\nexist, they either rely on additional assumptions or lead to error that is\nspatially inhomogeneous. I present a novel particle-based algorithm for online\napproximation of the filtering problem on such models, using the fact that each\nlocus affects only nearby loci at the next time step. The algorithm is based on\na Metropolis-Hastings-like MCMC for creating hybrid particles at each step. I\nshow simulation results that suggest the error of this algorithm is uniform in\nboth space and time, with a lower bias, though higher variance, as compared to\na previously-proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 20:56:38 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Quinn", "Jameson", ""]]}, {"id": "1901.10550", "submitter": "Ye Tu", "authors": "Ye Tu, Kinjal Basu, Cyrus DiCiccio, Romil Bansal, Preetam Nandy,\n  Padmini Jaikumar, Shaunak Chatterjee", "title": "Personalized Treatment Selection using Causal Heterogeneity", "comments": "12 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experimentation (also known as A/B testing or bucket testing) is\nwidely used in the internet industry to measure the metric impact obtained by\ndifferent treatment variants. A/B tests identify the treatment variant showing\nthe best performance, which then becomes the chosen or selected treatment for\nthe entire population. However, the effect of a given treatment can differ\nacross experimental units and a personalized approach for treatment selection\ncan greatly improve upon the usual global selection strategy. In this work, we\ndevelop a framework for personalization through (i) estimation of heterogeneous\ntreatment effect at either a cohort or member-level, followed by (ii) selection\nof optimal treatment variants for cohorts (or members) obtained through\n(deterministic or stochastic) constrained optimization.\n  We perform a two-fold evaluation of our proposed methods. First, a simulation\nanalysis is conducted to study the effect of personalized treatment selection\nunder carefully controlled settings. This simulation illustrates the\ndifferences between the proposed methods and the suitability of each with\nincreasing uncertainty. We also demonstrate the effectiveness of the method\nthrough a real-life example related to serving notifications at Linkedin. The\nsolution significantly outperformed both heuristic solutions and the global\ntreatment selection baseline leading to a sizable win on top-line metrics like\nmember visits.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 21:07:45 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 17:55:57 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 05:21:27 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2020 21:47:22 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Tu", "Ye", ""], ["Basu", "Kinjal", ""], ["DiCiccio", "Cyrus", ""], ["Bansal", "Romil", ""], ["Nandy", "Preetam", ""], ["Jaikumar", "Padmini", ""], ["Chatterjee", "Shaunak", ""]]}, {"id": "1901.10566", "submitter": "Sherri Rose", "authors": "Anna Zink and Sherri Rose", "title": "Fair Regression for Health Care Spending", "comments": "30 pages, 3 figures", "journal-ref": "Biometrics (2020)", "doi": "10.1111/biom.13206", "report-no": null, "categories": "stat.AP cs.CY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of health care payments to insurance plans has substantial\nconsequences for social policy. Risk adjustment formulas predict spending in\nhealth insurance markets in order to provide fair benefits and health care\ncoverage for all enrollees, regardless of their health status. Unfortunately,\ncurrent risk adjustment formulas are known to underpredict spending for\nspecific groups of enrollees leading to undercompensated payments to health\ninsurers. This incentivizes insurers to design their plans such that\nindividuals in undercompensated groups will be less likely to enroll, impacting\naccess to health care for these groups. To improve risk adjustment formulas for\nundercompensated groups, we expand on concepts from the statistics, computer\nscience, and health economics literature to develop new fair regression methods\nfor continuous outcomes by building fairness considerations directly into the\nobjective function. We additionally propose a novel measure of fairness while\nasserting that a suite of metrics is necessary in order to evaluate risk\nadjustment formulas more fully. Our data application using the IBM MarketScan\nResearch Databases and simulation studies demonstrate that these new fair\nregression methods may lead to massive improvements in group fairness (e.g.,\n98%) with only small reductions in overall fit (e.g., 4%).\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 04:06:50 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 06:11:28 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zink", "Anna", ""], ["Rose", "Sherri", ""]]}, {"id": "1901.10679", "submitter": "Mengyin Lu", "authors": "Mengyin Lu and Matthew Stephens", "title": "Empirical Bayes estimation of normal means, accounting for uncertainty\n  in estimated standard errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Empirical Bayes (EB) estimation in the normal means problem, when\nthe standard deviations of the observations are not known precisely, but\nestimated with error -- which is almost always the case in practical\napplications. In classical statistics accounting for estimated standard errors\nusually involves replacing a normal distribution with a $t$ distribution. This\nsuggests approaching this problem by replacing the normal assumption with a $t$\nassumption, leading to an \"EB $t$-means problem\". Here we show that an approach\nalong these lines can indeed work, but only with some care. Indeed, a naive\napplication of this idea is flawed, and can perform poorly. We suggest how this\nflaw can be remedied by a two-stage procedure, which first performs EB\nshrinkage estimation of the standard errors and then solves an EB $t$-means\nproblem. We give numerical results illustrating the effectiveness of this\nremedy.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 05:41:48 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Lu", "Mengyin", ""], ["Stephens", "Matthew", ""]]}, {"id": "1901.10782", "submitter": "Michele Nguyen", "authors": "Michele Nguyen, Rosalind E. Howes, Tim C.D. Lucas, Katherine E.\n  Battle, Ewan Cameron, Harry S. Gibson, Jennifer Rozier, Suzanne Keddie, Emma\n  Collins, Rohan Arambepola, Su Yun Kang, Chantal Hendriks, Anita Nandi, Susan\n  F. Rumisha, Samir Bhatt, Sedera A. Mioramalala, Mauricette Andriamananjara\n  Nambinisoa, Fanjasoa Rakotomanana, Peter W. Gething, Daniel J. Weiss", "title": "Mapping malaria seasonality: a case study from Madagascar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many malaria-endemic areas experience seasonal fluctuations in case incidence\nas Anopheles mosquito and Plasmodium parasite life cycles respond to changing\nenvironmental conditions. While most existing maps of malaria seasonality use\nfixed thresholds of rainfall, temperature, and/or vegetation indices to\nidentify suitable transmission months, we develop a statistical modelling\nframework for characterising the seasonal patterns derived directly from case\ndata.\n  The procedure involves a spatiotemporal regression model for estimating the\nmonthly proportions of total annual cases and an algorithm to identify\noperationally relevant characteristics such as the transmission start and peak\nmonths. A seasonality index combines the monthly proportion estimates and\nexisting estimates of annual case incidence to provide a summary of \"how\nseasonal\" locations are relative to their surroundings. An advancement upon\npast seasonality mapping endeavours is the presentation of the uncertainty\nassociated with each map, which will enable policymakers to make more\nstatistically sound decisions. The methodology is illustrated using health\nfacility data from Madagascar.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 12:20:52 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 15:04:41 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Nguyen", "Michele", ""], ["Howes", "Rosalind E.", ""], ["Lucas", "Tim C. D.", ""], ["Battle", "Katherine E.", ""], ["Cameron", "Ewan", ""], ["Gibson", "Harry S.", ""], ["Rozier", "Jennifer", ""], ["Keddie", "Suzanne", ""], ["Collins", "Emma", ""], ["Arambepola", "Rohan", ""], ["Kang", "Su Yun", ""], ["Hendriks", "Chantal", ""], ["Nandi", "Anita", ""], ["Rumisha", "Susan F.", ""], ["Bhatt", "Samir", ""], ["Mioramalala", "Sedera A.", ""], ["Nambinisoa", "Mauricette Andriamananjara", ""], ["Rakotomanana", "Fanjasoa", ""], ["Gething", "Peter W.", ""], ["Weiss", "Daniel J.", ""]]}, {"id": "1901.10852", "submitter": "Andreas Anastasiou Dr", "authors": "Andreas Anastasiou and Piotr Fryzlewicz", "title": "Detecting multiple generalized change-points by isolating single ones", "comments": "30 pages for the main article and 31 pages for the supplementary\n  material. There are also 23 tables and 9 figures in total", "journal-ref": null, "doi": "10.1007/s00184-021-00821-6", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach, called Isolate-Detect (ID), for the consistent\nestimation of the number and location of multiple generalized change-points in\nnoisy data sequences. Examples of signal changes that ID can deal with are\nchanges in the mean of a piecewise-constant signal and changes, continuous or\nnot, in the linear trend. The number of change-points can increase with the\nsample size. Our method is based on an isolation technique, which prevents the\nconsideration of intervals that contain more than one change-point. This\nisolation enhances ID's accuracy as it allows for detection in the presence of\nfrequent changes of possibly small magnitudes. In ID, model selection is\ncarried out via thresholding, or an information criterion, or SDLL, or a hybrid\ninvolving the former two. The hybrid model selection leads to a general method\nwith very good practical performance and minimal parameter choice. In the\nscenarios tested, ID is at least as accurate as the state-of-the-art methods;\nmost of the times it outperforms them. ID is implemented in the R packages\nIDetect and breakfast, available from CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 14:36:45 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 10:06:56 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Anastasiou", "Andreas", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1901.10870", "submitter": "Marta Crispino", "authors": "Marta Crispino and Isadora Antoniano-Villalobos", "title": "Informative extended Mallows priors in the Bayesian Mallows model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to study the problem of prior elicitation for the\nMallows model with Spearman's distance, a popular distance-based model for\nrankings or permutation data. Previous Bayesian inference for such model has\nbeen limited to the use of the uniform prior over the space of permutations. We\npresent a novel strategy to elicit subjective prior beliefs on the location\nparameter of the model, discussing the interpretation of hyper-parameters and\nthe implication of prior choices for the posterior analysis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 14:54:15 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Crispino", "Marta", ""], ["Antoniano-Villalobos", "Isadora", ""]]}, {"id": "1901.10928", "submitter": "William N Anderson", "authors": "William N. Anderson, Johan Verbeeck", "title": "Exact Bootstrap and Permutation Distribution of Wins and Losses in a\n  Hierarchical Trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finkelstein-Schoenfeld, Buyse, Pocock, and other authors have developed\ngeneralizations of the Mann-Whitney test that allow for pairwise patient\ncomparisons to include a hierarchy of measurements. Various authors present\neither asymptotic or randomized methods for analyzing the wins. We use graph\ntheory concepts to derive exact means and variances for the number of wins, as\na replacement for approximate values obtained from bootstrap analysis or random\nsampling from the permutation distribution. The time complexity of our\nalgorithm is $O(N^2)$, where $N$ is the total number of patients. In any\nsituation where the mean and variance of a bootstrap sample are used to draw\nconclusions, our methodology will be faster and more accurate than the\nrandomized bootstrap or permutation test.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 16:21:27 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 19:43:12 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Anderson", "William N.", ""], ["Verbeeck", "Johan", ""]]}, {"id": "1901.11033", "submitter": "Jakob Knollm\\\"uller", "authors": "Jakob Knollm\\\"uller and Torsten A. En{\\ss}lin", "title": "Metric Gaussian Variational Inference", "comments": "Code is part of NIFTy5 release at\n  https://gitlab.mpcdf.mpg.de/ift/NIFTy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.IM cs.LG physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving Bayesian inference problems approximately with variational approaches\ncan provide fast and accurate results. Capturing correlation within the\napproximation requires an explicit parametrization. This intrinsically limits\nthis approach to either moderately dimensional problems, or requiring the\nstrongly simplifying mean-field approach. We propose Metric Gaussian\nVariational Inference (MGVI) as a method that goes beyond mean-field. Here\ncorrelations between all model parameters are taken into account, while still\nscaling linearly in computational time and memory. With this method we achieve\nhigher accuracy and in many cases a significant speedup compared to traditional\nmethods. MGVI is an iterative method that performs a series of Gaussian\napproximations to the posterior. We alternate between approximating the\ncovariance with the inverse Fisher information metric evaluated at an\nintermediate mean estimate and optimizing the KL-divergence for the given\ncovariance with respect to the mean. This procedure is iterated until the\nuncertainty estimate is self-consistent with the mean parameter. We achieve\nlinear scaling by avoiding to store the covariance explicitly at any time.\nInstead we draw samples from the approximating distribution relying on an\nimplicit representation and numerical schemes to approximately solve linear\nequations. Those samples are used to approximate the KL-divergence and its\ngradient. The usage of natural gradient descent allows for rapid convergence.\nFormulating the Bayesian model in standardized coordinates makes MGVI\napplicable to any inference problem with continuous parameters. We demonstrate\nthe high accuracy of MGVI by comparing it to HMC and its fast convergence\nrelative to other established methods in several examples. We investigate\nreal-data applications, as well as synthetic examples of varying size and\ncomplexity and up to a million model parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 19:00:02 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 12:15:19 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 15:02:03 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Knollm\u00fcller", "Jakob", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1901.11050", "submitter": "Behzad Kianian", "authors": "Behzad Kianian (1), Jung In Kim (2), Jason P. Fine (2), Limin Peng (1)\n  ((1) Department of Biostatistics and Bioinformatics, Emory University, (2)\n  Department of Biostatistics, University of North Carolina at Chapel Hill)", "title": "Causal Proportional Hazards Estimation with a Binary Instrumental\n  Variable", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables (IV) are a useful tool for estimating causal effects\nin the presence of unmeasured confounding. IV methods are well developed for\nuncensored outcomes, particularly for structural linear equation models, where\nsimple two-stage estimation schemes are available. The extension of these\nmethods to survival settings is challenging, partly because of the nonlinearity\nof the popular survival regression models and partly because of the\ncomplications associated with right censoring or other survival features. We\ndevelop a simple causal hazard ratio estimator in a proportional hazards model\nwith right censored data. The method exploits a special characterization of IV\nwhich enables the use of an intuitive inverse weighting scheme that is\ngenerally applicable to more complex survival settings with left truncation,\ncompeting risks, or recurrent events. We rigorously establish the asymptotic\nproperties of the estimators, and provide plug-in variance estimators. The\nproposed method can be implemented in standard software, and is evaluated\nthrough extensive simulation studies. We apply the proposed IV method to a data\nset from the Prostate, Lung, Colorectal and Ovarian cancer screening trial to\ndelineate the causal effect of flexible sigmoidoscopy screening on colorectal\ncancer survival which may be confounded by informative noncompliance with the\nassigned screening regimen.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 19:06:44 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Kianian", "Behzad", ""], ["Kim", "Jung In", ""], ["Fine", "Jason P.", ""], ["Peng", "Limin", ""]]}, {"id": "1901.11172", "submitter": "Eric Lock", "authors": "Eric F. Lock and Dipankar Bandyopadhyay", "title": "Bayesian nonparametric multiway regression for clustered binomial data", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian nonparametric regression model for data with multiway\n(tensor) structure, motivated by an application to periodontal disease (PD)\ndata. Our outcome is the number of diseased sites measured over four different\ntooth types for each subject, with subject-specific covariates available as\npredictors. The outcomes are not well-characterized by simple parametric\nmodels, so we use a nonparametric approach with a binomial likelihood wherein\nthe latent probabilities are drawn from a mixture with an arbitrary number of\ncomponents, analogous to a Dirichlet Process (DP). We use a flexible probit\nstick-breaking formulation for the component weights that allows for covariate\ndependence and clustering structure in the outcomes. The parameter space for\nthis model is large and multiway: patients $\\times$ tooth types $\\times$\ncovariates $\\times$ components. We reduce its effective dimensionality, and\naccount for the multiway structure, via low-rank assumptions. We illustrate how\nthis can improve performance, and simplify interpretation, while still\nproviding sufficient flexibility. We describe a general and efficient Gibbs\nsampling algorithm for posterior computation. The resulting fit to the PD data\noutperforms competitors, and is interpretable and well-calibrated. An\ninteractive visual of the predictive model is available at\nhttp://ericfrazerlock.com/toothdata/ToothDisplay.html , and the code is\navailable at https://github.com/lockEF/NonparametricMultiway .\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 02:05:07 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Lock", "Eric F.", ""], ["Bandyopadhyay", "Dipankar", ""]]}, {"id": "1901.11279", "submitter": "Robin Genuer", "authors": "Louis Capitaine, Robin Genuer and Rodolphe Thi\\'ebaut", "title": "Random forests for high-dimensional longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests is a state-of-the-art supervised machine learning method which\nbehaves well in high-dimensional settings although some limitations may happen\nwhen $p$, the number of predictors, is much larger than the number of\nobservations $n$. Repeated measurements can help by offering additional\ninformation but no approach has been proposed for high-dimensional longitudinal\ndata. Random forests have been adapted to standard (i.e., $n > p$) longitudinal\ndata by using a semi-parametric mixed-effects model, in which the\nnon-parametric part is estimated using random forests. We first propose a\nstochastic extension of the model which allows the covariance structure to vary\nover time. Furthermore, we develop a new method which takes intra-individual\ncovariance into consideration to build the forest. Simulations reveal the\nsuperiority of our approach compared to existing ones. The method has been\napplied to an HIV vaccine trial including 17 HIV infected patients with 10\nrepeated measurements of 20000 gene transcripts and the blood concentration of\nhuman immunodeficiency virus RNA at the time of antiretroviral interruption.\nThe approach selected 21 gene transcripts for which the association with HIV\nviral load was fully relevant and consistent with results observed during\nprimary infection.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 09:16:45 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Capitaine", "Louis", ""], ["Genuer", "Robin", ""], ["Thi\u00e9baut", "Rodolphe", ""]]}, {"id": "1901.11355", "submitter": "Caterina Schiavoni", "authors": "Caterina Schiavoni, Franz Palm, Stephan Smeekes, Jan van den Brakel", "title": "A dynamic factor model approach to incorporate Big Data in state space\n  models for official statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider estimation of unobserved components in state space\nmodels using a dynamic factor approach to incorporate auxiliary information\nfrom high-dimensional data sources. We apply the methodology to unemployment\nestimation as done by Statistics Netherlands, who uses a multivariate state\nspace model to produce monthly figures for the unemployment using series\nobserved with the labour force survey (LFS). We extend the model by including\nauxiliary series of Google Trends about job-search and economic uncertainty,\nand claimant counts, partially observed at higher frequencies. Our factor model\nallows for nowcasting the variable of interest, providing reliable unemployment\nestimates in real-time before LFS data become available.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 13:47:13 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 18:09:08 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Schiavoni", "Caterina", ""], ["Palm", "Franz", ""], ["Smeekes", "Stephan", ""], ["Brakel", "Jan van den", ""]]}, {"id": "1901.11362", "submitter": "Li Xing", "authors": "Li Xing, Xuekui Zhang, Igor Burstyn, Paul Gustafson", "title": "Logistic Box-Cox Regression to Assess the Shape and Median Effect under\n  Uncertainty about Model Specification", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shape of the relationship between a continuous exposure variable and a\nbinary disease variable is often central to epidemiologic investigations. This\npaper investigates a number of issues surrounding inference and the shape of\nthe relationship. Presuming that the relationship can be expressed in terms of\nregression coefficients and a shape parameter, we investigate how well the\nshape can be inferred in settings which might typify epidemiologic\ninvestigations and risk assessment. We also consider a suitable definition of\nthe median effect of exposure, and investigate how precisely this can be\ninferred. This is done both in the case of using a model acknowledging\nuncertainty about the shape parameter and in the case of ignoring this\nuncertainty and using a two-step method, where in step one we transform the\npredictor and in step two we fit a simple linear model with transformed\npredictor. All these investigations require a family of exposure-disease\nrelationships indexed by a shape parameter. For this purpose, we employ a\nfamily based on the Box-Cox transformation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 14:10:01 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Xing", "Li", ""], ["Zhang", "Xuekui", ""], ["Burstyn", "Igor", ""], ["Gustafson", "Paul", ""]]}, {"id": "1901.11366", "submitter": "Tanuj Hasija", "authors": "Tanuj Hasija, Christian Lameiro, Timothy Marrinan and Peter J.\n  Schreier", "title": "Determining the Dimension and Structure of the Subspace Correlated\n  Across Multiple Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the components common or correlated across multiple data sets is\nchallenging due to a large number of possible correlation structures among the\ncomponents. Even more challenging is to determine the precise structure of\nthese correlations. Traditional work has focused on determining only the model\norder, i.e., the dimension of the correlated subspace, a number that depends on\nhow the model-order problem is defined. Moreover, identifying the model order\nis often not enough to understand the relationship among the components in\ndifferent data sets. We aim at solving the complete modelselection problem,\ni.e., determining which components are correlated across which data sets. We\nprove that the eigenvalues and eigenvectors of the normalized covariance matrix\nof the composite data vector, under certain conditions, completely characterize\nthe underlying correlation structure. We use these results to solve the\nmodel-selection problem by employing bootstrap-based hypothesis testing.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 14:15:33 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Hasija", "Tanuj", ""], ["Lameiro", "Christian", ""], ["Marrinan", "Timothy", ""], ["Schreier", "Peter J.", ""]]}, {"id": "1901.11491", "submitter": "Gregor Kastner", "authors": "Darjus Hosszejni and Gregor Kastner", "title": "Approaches Toward the Bayesian Estimation of the Stochastic Volatility\n  Model with Leverage", "comments": null, "journal-ref": "In R. Argiento, D. Durante, and S. Wade, editors, Bayesian\n  Statistics and New Generations - Selected Contributions from BAYSM 2018,\n  volume 296 of Springer Proceedings in Mathematics & Statistics, pages 75-83,\n  Cham, 2019. Springer", "doi": "10.1007/978-3-030-30611-3_8", "report-no": null, "categories": "stat.CO econ.EM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sampling efficiency of MCMC methods in Bayesian inference for stochastic\nvolatility (SV) models is known to highly depend on the actual parameter\nvalues, and the effectiveness of samplers based on different parameterizations\nvaries significantly. We derive novel algorithms for the centered and the\nnon-centered parameterizations of the practically highly relevant SV model with\nleverage, where the return process and innovations of the volatility process\nare allowed to correlate. Moreover, based on the idea of\nancillarity-sufficiency interweaving (ASIS), we combine the resulting samplers\nin order to guarantee stable sampling efficiency irrespective of the baseline\nparameterization.We carry out an extensive comparison to already existing\nsampling methods for this model using simulated as well as real world data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 17:43:13 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 15:34:13 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Hosszejni", "Darjus", ""], ["Kastner", "Gregor", ""]]}]