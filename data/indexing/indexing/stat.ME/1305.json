[{"id": "1305.0182", "submitter": "Pritam Ranjan", "authors": "Pritam Ranjan and Neil Spencer", "title": "Space-filling Latin Hypercube Designs based on Randomization\n  Restrictions in Factorial Experiments", "comments": "16 pages (accepted in SPL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latin hypercube designs (LHDs) with space-filling properties are widely used\nfor emulating computer simulators. Over the last three decades, a wide spectrum\nof LHDs have been proposed with space-filling criteria like minimum correlation\namong factors, maximin interpoint distance, and orthogonality among the factors\nvia orthogonal arrays (OAs). Projective geometric structures like spreads,\ncovers and stars of PG(p-1,q) can be used to characterize the randomization\nrestriction of multistage factorial experiments. These geometric structures can\nalso be used for constructing OAs and nearly OAs (NOAs). In this paper, we\npresent a new class of space-filling LHDs based on NOAs derived from stars of\nPG(p-1, 2).\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 14:31:51 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 04:27:44 GMT"}, {"version": "v3", "created": "Mon, 21 Jul 2014 14:10:23 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Ranjan", "Pritam", ""], ["Spencer", "Neil", ""]]}, {"id": "1305.0220", "submitter": "X. Jessie Jeng", "authors": "X. Jessie Jeng", "title": "Identification of Signal, Noise, and Indistinguishable Subsets in\n  High-Dimensional Data Analysis", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in high-dimensional data analysis where strong\nsignals often stand out easily and weak ones may be indistinguishable from the\nnoise, we develop a statistical framework to provide a novel categorization of\nthe data into the signal, noise, and indistinguishable subsets. The\nthree-subset categorization is especially relevant under high-dimensionality as\na large proportion of signals can be obscured by the large amount of noise.\nUnderstanding the three-subset phenomenon is important for the researchers in\nreal applications to design efficient follow-up studies. %For example,\ncandidates belonging to the signal subset may have priority for more focused\nstudy, while those in the noise subset can be removed; and, for candidates in\nthe indistinguishable subset, additional data may be collected to further\nseparate weak signals from the noise. We develop an efficient data-driven\nprocedure to identify the three subsets. Theoretical study shows that, under\ncertain conditions, only signals are included in the identified signal subset\nwhile the remaining signals are included in the identified indistinguishable\nsubsets with high probability. Moreover, the proposed procedure adapts to the\nunknown signal intensity, so that the identified indistinguishable subset\nshrinks with the true indistinguishable subset when signals become stronger.\nThe procedure is examined and compared with methods based on FDR control using\nMonte Carlo simulation. Further, it is applied successfully in a real-data\napplication to identify genomic variants having different signal intensity.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 16:38:53 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Jeng", "X. Jessie", ""]]}, {"id": "1305.0328", "submitter": "Shohei Hidaka", "authors": "Shohei Hidaka", "title": "General Type Token Distribution", "comments": "This paper is accepted in Biometrika. 5 pages and no figure in the\n  main paper. 3 pages and 1 figure in the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the number of types in a corpus using\nthe number of types observed in a sample of tokens from that corpus. We derive\nexact and asymptotic distributions for the number of observed types,\nconditioned upon the number of tokens and the latent type distribution. We use\nthe asymptotic distributions to derive an estimator of the latent number of\ntypes and we validate this estimator numerically.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 02:41:04 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 00:34:43 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Hidaka", "Shohei", ""]]}, {"id": "1305.0355", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Model Selection for High-Dimensional Regression under the Generalized\n  Irrepresentability Condition", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the high-dimensional regression model a response variable is linearly\nrelated to $p$ covariates, but the sample size $n$ is smaller than $p$. We\nassume that only a small subset of covariates is `active' (i.e., the\ncorresponding coefficients are non-zero), and consider the model-selection\nproblem of identifying the active covariates. A popular approach is to estimate\nthe regression coefficients through the Lasso ($\\ell_1$-regularized least\nsquares). This is known to correctly identify the active set only if the\nirrelevant covariates are roughly orthogonal to the relevant ones, as\nquantified through the so called `irrepresentability' condition. In this paper\nwe study the `Gauss-Lasso' selector, a simple two-stage method that first\nsolves the Lasso, and then performs ordinary least squares restricted to the\nLasso active set. We formulate `generalized irrepresentability condition'\n(GIC), an assumption that is substantially weaker than irrepresentability. We\nprove that, under GIC, the Gauss-Lasso correctly recovers the active set.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 07:25:52 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1305.0889", "submitter": "Bjoern Bornkamp", "authors": "Jos\\'e Pinheiro, Bj\\\"orn Bornkamp, Ekkehard Glimm and Frank Bretz", "title": "Model-based dose finding under model uncertainty using general\n  parametric models", "comments": "Now references were correctly compiled", "journal-ref": "updated version published in Statistics in Medicine (2014), 33,\n  1646-1661", "doi": "10.1002/sim.6052", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methodology for the design and analysis of clinical Phase II dose\nresponse studies, with related software implementation, are well developed for\nthe case of a normally distributed, homoscedastic response considered for a\nsingle timepoint in parallel group study designs. In practice, however, binary,\ncount, or time-to-event endpoints are often used, typically measured repeatedly\nover time and sometimes in more complex settings like crossover study designs.\nIn this paper we develop an overarching methodology to perform efficient\nmultiple comparisons and modeling for dose finding, under uncertainty about the\ndose-response shape, using general parametric models. The framework described\nhere is quite general and covers dose finding using generalized non-linear\nmodels, linear and non-linear mixed effects models, Cox proportional hazards\n(PH) models, etc. In addition to the core framework, we also develop a general\npurpose methodology to fit dose response data in a computationally and\nstatistically efficient way. Several examples, using a variety of different\nstatistical models, illustrate the breadth of applicability of the results. For\nthe analyses we developed the R add-on package DoseFinding, which provides a\nconvenient interface to the general approach adopted here.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2013 07:42:07 GMT"}, {"version": "v2", "created": "Sat, 11 May 2013 05:58:22 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Pinheiro", "Jos\u00e9", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Glimm", "Ekkehard", ""], ["Bretz", "Frank", ""]]}, {"id": "1305.1036", "submitter": "Paul McNicholas", "authors": "Ryan P. Browne and Paul D. McNicholas", "title": "A Mixture of Generalized Hyperbolic Distributions", "comments": null, "journal-ref": null, "doi": "10.1002/cjs.11246", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a mixture of generalized hyperbolic distributions as an\nalternative to the ubiquitous mixture of Gaussian distributions as well as\ntheir near relatives of which the mixture of multivariate t and skew-t\ndistributions are predominant. The mathematical development of our mixture of\ngeneralized hyperbolic distributions model relies on its relationship with the\ngeneralized inverse Gaussian distribution. The latter is reviewed before our\nmixture models are presented along with details of the aforesaid reliance.\nParameter estimation is outlined within the expectation-maximization framework\nbefore the clustering performance of our mixture models is illustrated via\napplications on simulated and real data. In particular, the ability of our\nmodels to recover parameters for data from underlying Gaussian and skew-t\ndistributions is demonstrated. Finally, the role of Generalized hyperbolic\nmixtures within the wider model-based clustering, classification, and density\nestimation literature is discussed.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 18:37:24 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2013 15:56:01 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2013 00:57:59 GMT"}, {"version": "v4", "created": "Tue, 23 Dec 2014 19:53:56 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1305.1137", "submitter": "Robert Hable", "authors": "Robert Hable", "title": "Practical Tikhonov Regularized Estimators in Reproducing Kernel Hilbert\n  Spaces for Statistical Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized kernel methods such as support vector machines (SVM) and support\nvector regression (SVR) constitute a broad and flexible class of methods which\nare theoretically well investigated and commonly used in nonparametric\nclassification and regression problems. As these methods are based on a\nTikhonov regularization which is also common in inverse problems, this article\ninvestigates the use of regularized kernel methods for inverse problems in a\nunifying way. Regularized kernel methods are based on the use of reproducing\nkernel Hilbert spaces (RKHS) which lead to very good computational properties.\nIt is shown that similar properties remain true in solving statistical inverse\nproblems and that standard software implementations developed for ordinary\nregression problems can still be used for inverse regression problems.\nConsistency of these methods and a rate of convergence for the risk is shown\nunder quite weak assumptions and rates of convergence for the estimator are\nshown under somehow stronger assumptions. The applicability of these methods is\ndemonstrated in a simulation.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 10:17:24 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Hable", "Robert", ""]]}, {"id": "1305.1184", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran", "title": "Probabilistic wind speed forecasting using Bayesian model averaging with\n  truncated normal components", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.2133", "journal-ref": "Computational Statistics and Data Analysis 75 (2014), 227-238", "doi": "10.1016/j.csda.2014.02.013", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging (BMA) is a statistical method for post-processing\nforecast ensembles of atmospheric variables, obtained from multiple runs of\nnumerical weather prediction models, in order to create calibrated predictive\nprobability density functions (PDFs). The BMA predictive PDF of the future\nweather quantity is the mixture of the individual PDFs corresponding to the\nensemble members and the weights and model parameters are estimated using\nensemble members and validating observation from a given training period.\n  In the present paper we introduce a BMA model for calibrating wind speed\nforecasts, where the components PDFs follow truncated normal distribution with\ncut-off at zero, and apply it to the ALADIN-HUNEPS ensemble of the Hungarian\nMeteorological Service. Three parameter estimation methods are proposed and\neach of the corresponding models outperforms the traditional gamma BMA model\nboth in calibration and in accuracy of predictions. Moreover, since here the\nmaximum likelihood estimation of the parameters does not require numerical\noptimization, modelling can be performed much faster than in case of gamma\nmixtures.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 13:34:13 GMT"}, {"version": "v2", "created": "Tue, 7 May 2013 12:54:01 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Baran", "S\u00e1ndor", ""]]}, {"id": "1305.1318", "submitter": "Dajiang Liu", "authors": "Dajiang J. Liu, Gina M. Peloso, Xiaowei Zhan, Oddgeir Holmen, Matthew\n  Zawistowski, Shuang Feng, Majid Nikpay, Paul L. Auer, Anuj Goel, He Zhang,\n  Ulrike Peters, Martin Farrall, Marju Orho-Melander, Charles Kooperberg, Ruth\n  McPherson, Hugh Watkins, Cristen J. Willer, Kristian Hveem, Olle Melander,\n  Sekar Kathiresan and Gon\\c{c}alo R. Abecasis", "title": "Meta-Analysis of Gene Level Association Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The vast majority of connections between complex disease and common genetic\nvariants were identified through meta-analysis, a powerful approach that\nenables large samples sizes while protecting against common artifacts due to\npopulation structure, repeated small sample analyses, and/or limitations with\nsharing individual level data. As the focus of genetic association studies\nshifts to rare variants, genes and other functional units are becoming the unit\nof analysis. Here, we propose and evaluate new approaches for meta-analysis of\nrare variant association. We show that our approach retains useful features of\nsingle variant meta-analytic approaches and demonstrate its utility in a study\nof blood lipid levels in ~18,500 individuals genotyped with exome arrays.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 20:24:30 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Liu", "Dajiang J.", ""], ["Peloso", "Gina M.", ""], ["Zhan", "Xiaowei", ""], ["Holmen", "Oddgeir", ""], ["Zawistowski", "Matthew", ""], ["Feng", "Shuang", ""], ["Nikpay", "Majid", ""], ["Auer", "Paul L.", ""], ["Goel", "Anuj", ""], ["Zhang", "He", ""], ["Peters", "Ulrike", ""], ["Farrall", "Martin", ""], ["Orho-Melander", "Marju", ""], ["Kooperberg", "Charles", ""], ["McPherson", "Ruth", ""], ["Watkins", "Hugh", ""], ["Willer", "Cristen J.", ""], ["Hveem", "Kristian", ""], ["Melander", "Olle", ""], ["Kathiresan", "Sekar", ""], ["Abecasis", "Gon\u00e7alo R.", ""]]}, {"id": "1305.1385", "submitter": "Yang Feng", "authors": "Weiping Ma, Yang Feng, Kani Chen, Zhiliang Ying", "title": "Functional and Parametric Estimation in a Semi- and Nonparametric Model\n  with Application to Mass-Spectrometry Data", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by modeling and analysis of mass-spectrometry data, a semi- and\nnonparametric model is proposed that consists of a linear parametric component\nfor individual location and scale and a nonparametric regression function for\nthe common shape. A multi-step approach is developed that simultaneously\nestimates the parametric components and the nonparametric function. Under\ncertain regularity conditions, it is shown that the resulting estimators is\nconsistent and asymptotic normal for the parametric part and achieve the\noptimal rate of convergence for the nonparametric part when the bandwidth is\nsuitably chosen. Simulation results are presented to demonstrate the\neffectiveness and finite-sample performance of the method. The method is also\napplied to a SELDI-TOF mass spectrometry data set from a study of liver cancer\npatients.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 02:30:58 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Ma", "Weiping", ""], ["Feng", "Yang", ""], ["Chen", "Kani", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1305.1656", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford and Daniel Zelterman", "title": "Markov counting models for correlated binary responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of continuous-time Markov counting processes for analyzing\ncorrelated binary data and establish a correspondence between these models and\nsums of exchangeable Bernoulli random variables. Our approach generalizes many\nprevious models for correlated outcomes, admits easily interpretable\nparameterizations, allows different cluster sizes, and incorporates\nascertainment bias in a natural way. We demonstrate several new models for\ndependent outcomes and provide algorithms for computing maximum likelihood\nestimates. We show how to incorporate cluster-specific covariates in a\nregression setting and demonstrate improved fits to well-known datasets from\nfamilial disease epidemiology and developmental toxicology.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 21:42:43 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2014 01:20:53 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Zelterman", "Daniel", ""]]}, {"id": "1305.2227", "submitter": "Camila Pedroso Estevam de Souza", "authors": "Camila P. E. de Souza and Nancy E. Heckman", "title": "Switching Nonparametric Regression Models and the Motorcycle Data\n  revisited", "comments": "The article has one supplementary pdf file\n  (DeSouzaHeckman-supplementA.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology to analyze data arising from a curve that, over its\ndomain, switches among J states. We consider a sequence of response variables,\nwhere each response y depends on a covariate x according to an unobserved state\nz. The states form a stochastic process and their possible values are\nj=1,...,J. If z equals j the expected response of y is one of J unknown smooth\nfunctions evaluated at x. We call this model a switching nonparametric\nregression model. We develop an EM algorithm to estimate the parameters of the\nlatent state process and the functions corresponding to the J states. We also\nobtain standard errors for the parameter estimates of the state process. We\nconduct simulation studies to analyze the frequentist properties of our\nestimates. We also apply the proposed methodology to the well-known motorcycle\ndata set treating the data as coming from more than one simulated accident run\nwith unobserved run labels.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 22:30:44 GMT"}, {"version": "v2", "created": "Wed, 22 May 2013 04:10:30 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["de Souza", "Camila P. E.", ""], ["Heckman", "Nancy E.", ""]]}, {"id": "1305.2250", "submitter": "Lucia Tabacu", "authors": "Manfred Denker, Lucia Tabacu", "title": "Logarithmic Quantile Estimation for Rank Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an almost sure weak limit theorem for simple linear rank statistics\nfor samples with continuous distributions functions. As a corollary the result\nextends to samples with ties, and the vector version of an a.s. central limit\ntheorem for vectors of linear rank statistics. Moreover, we derive such a weak\nconvergence result for some quadratic forms. These results are then applied to\nquantile estimation, and to hypothesis testing for nonparametric statistical\ndesigns, here demonstrated by the c-sample problem, where the samples may be\ndependent. In general, the method is known to be comparable to the bootstrap\nand other nonparametric methods (\\cite{THA, FRI}) and we confirm this finding\nfor the c-sample problem.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 02:50:21 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Denker", "Manfred", ""], ["Tabacu", "Lucia", ""]]}, {"id": "1305.2329", "submitter": "Thierry Klein", "authors": "Jean-Claude Fort (MAP5), Thierry Klein (IMT), Nabil Rachdi", "title": "New sensitivity analysis subordinated to a contrast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a model of the form $Y=h(X_1,\\ldots,X_d)$ where the goal is to estimate a\nparameter of the probability distribution of $Y$, we define new sensitivity\nindices which quantify the importance of each variable $X_i$ with respect to\nthis parameter of interest. The aim of this paper is to define {\\it goal\noriented sensitivity indices} and we will show that Sobol indices are\nsensitivity indices associated to a particular characteristic of the\ndistribution $Y$. We name the framework we present as {\\it Goal Oriented\nSensitivity Analysis} (GOSA).\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 12:24:17 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Fort", "Jean-Claude", "", "MAP5"], ["Klein", "Thierry", "", "IMT"], ["Rachdi", "Nabil", ""]]}, {"id": "1305.2634", "submitter": "Robert Kohn", "authors": "Minh-Ngoc Tran and Michael K. Pitt and Robert Kohn", "title": "Adaptive Metropolis-Hastings Sampling using Reversible Dependent Mixture\n  Proposals", "comments": "39 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a general-purpose adaptive sampler that approximates\nthe target density by a mixture of multivariate t densities. The adaptive\nsampler is based on reversible proposal distributions each of which has the\nmixture of multivariate t densities as its invariant density. The reversible\nproposals consist of a combination of independent and correlated steps that\nallow the sampler to traverse the parameter space efficiently as well as\nallowing the sampler to keep moving and locally exploring the parameter space.\nWe employ a two-chain approach, in which a trial chain is used to adapt the\nproposal densities used in the main chain. Convergence of the main chain and a\nstrong law of large numbers are proved under reasonable conditions, and without\nimposing a Diminishing Adaptation condition. The mixtures of multivariate t\ndensities are fitted by an efficient Variational Approximation algorithm in\nwhich the number of components is determined automatically. The performance of\nthe sampler is evaluated using simulated and real examples. Our autocorrelated\nframework is quite general and can handle mixtures other than multivariate t.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 21:24:00 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2013 23:26:12 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Pitt", "Michael K.", ""], ["Kohn", "Robert", ""]]}, {"id": "1305.2667", "submitter": "Jan Luts", "authors": "Jan Luts, John T. Ormerod", "title": "Mean field variational Bayesian inference for support vector machine\n  classification", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mean field variational Bayes approach to support vector machines (SVMs)\nusing the latent variable representation on Polson & Scott (2012) is presented.\nThis representation allows circumvention of many of the shortcomings associated\nwith classical SVMs including automatic penalty parameter selection, the\nability to handle dependent samples, missing data and variable selection. We\ndemonstrate on simulated and real datasets that our approach is easily\nextendable to non-standard situations and outperforms the classical SVM\napproach whilst remaining computationally efficient.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 03:31:15 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Luts", "Jan", ""], ["Ormerod", "John T.", ""]]}, {"id": "1305.2702", "submitter": "Debasish Roy", "authors": "M Venugopal, D Roy and R M Vasu", "title": "A New Evolutionary Bayesian Approach Incorporating Additive Path\n  Correction for Nonlinear Inverse Problems", "comments": "42 pages, 3 figures (not yet published in a refereed journal or any\n  conference proceedings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An evolutionary form of a generalized Bayesian update method, which is\nstrictly derivative- free yet directed through an additive update term based\npurely on the statistical moments of the design variables, is proposed for\nnonlinear inverse problems in general and applied in particular to an optical\nimaging problem, the ultrasound modulated optical tomography (UMOT). The\nadditive update term, which bypasses most pitfalls of a conventional weight-\nbased Bayesian update, results from a change of measures aimed at driving\nappropriately derived observation-prediction error terms or increments of cost\nfunctionals to zero-mean Brownian martingales. This constitutes a novel\ncharacterization corresponding to the extremization of the cost functional(s),\nwhere the design unknowns are represented as diffusion processes evolving with\nrespect to a continuously parameterized iteration variable. This leads to a\nrecursive prediction-update algorithm to implement the search. The scheme\noffers freedom from sample degeneracy and the accompanying divergence of the\nconventional weight-based Bayesian update schemes. We obtain the order of\nconvergence of the conditioned process and also establish that the solutions\nare stable against tolerable variations in the regularizing noise terms, even\nas the original inverse problem remains severely ill-posed. Numerical evidence\non solutions to the UMOT problem also confirms substantive improvements in the\nreconstruction efficacy through the proposed method vis-\\`a- vis a Gauss-Newton\napproach, especially where the regularized quasi-Newton direction has low\nsensitivity to variations in the design unknowns.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 08:35:53 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 06:19:41 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Venugopal", "M", ""], ["Roy", "D", ""], ["Vasu", "R M", ""]]}, {"id": "1305.2957", "submitter": "Carlo Sguera", "authors": "Carlo Sguera, Pedro Galeano and Rosa Lillo", "title": "Spatial Depth-Based Classification for Functional Data", "comments": null, "journal-ref": "TEST, December 2014, Volume 23, Issue 4, pp 725-750", "doi": "10.1007/s11749-014-0379-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We enlarge the number of available functional depths by introducing the\nkernelized functional spatial depth (KFSD). KFSD is a local-oriented and\nkernel-based version of the recently proposed functional spatial depth (FSD)\nthat may be useful for studying functional samples that require an analysis at\na local level. In addition, we consider supervised functional classification\nproblems, focusing on cases in which the differences between groups are not\nextremely clear-cut or the data may contain outlying curves. We perform\nclassification by means of some available robust methods that involve the use\nof a given functional depth, including FSD and KFSD, among others. We use the\nfunctional \\textit{k}-nearest neighbor classifier as a benchmark procedure. The\nresults of a simulation study indicate that the KFSD-based classification\napproach leads to good results. Finally, we consider two real classification\nproblems, obtaining results that are consistent with the findings observed with\nsimulated curves.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 21:11:16 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 21:42:01 GMT"}, {"version": "v3", "created": "Thu, 20 Mar 2014 10:19:58 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Sguera", "Carlo", ""], ["Galeano", "Pedro", ""], ["Lillo", "Rosa", ""]]}, {"id": "1305.3080", "submitter": "Antonio Canale", "authors": "Antonio Canale and Bruno Scarpa", "title": "Informative Bayesian inference for the skew-normal distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the analysis of the distribution of university grades, which is\nusually asymmetric, we discuss two informative priors for the shape parameter\nof the skew-normal distribution, showing that they lead to closed-form\nfull-conditional posterior distributions, particularly useful in MCMC\ncomputation. Gibbs sampling algorithms are discussed for the joint vector of\nparameters, given independent prior distributions for the location and scale\nparameters. Simulation studies are performed to assess the performance of Gibbs\nsamplers and to compare the choice of informative priors against a\nnon-informative one. The method is used to analyze the grades of the basic\nstatistics examination of the first-year undergraduate students at the School\nof Economics, University of Padua, Italy.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 09:46:00 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Canale", "Antonio", ""], ["Scarpa", "Bruno", ""]]}, {"id": "1305.3104", "submitter": "Werner M\\\"uller", "authors": "Werner G. M\\\"uller, Luc Pronzato, Joao Rendas, Helmut Waldl", "title": "Efficient Prediction Designs for Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": "IFAS research report #2013-63", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For estimation and predictions of random fields it is increasingly\nacknowledged that the kriging variance may be a poor representative of true\nuncertainty. Experimental designs based on more elaborate criteria that are\nappropriate for empirical kriging are then often non-space-filling and very\ncostly to determine. In this paper, we investigate the possibility of using a\ncompound criterion inspired by an equivalence theorem type relation to build\ndesigns quasi-optimal for the empirical kriging variance, when space-filling\ndesigns become unsuitable. Two algorithms are proposed, one relying on\nstochastic optimization to explicitly identify the Pareto front, while the\nsecond uses the surrogate criteria as local heuristic to chose the points at\nwhich the (costly) true Empirical Kriging variance is effectively computed. We\nillustrate the performance of the algorithms presented on both a simple\nsimulated example and a real oceanographic dataset.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 10:47:24 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["M\u00fcller", "Werner G.", ""], ["Pronzato", "Luc", ""], ["Rendas", "Joao", ""], ["Waldl", "Helmut", ""]]}, {"id": "1305.3299", "submitter": "David Campbell", "authors": "David Campbell and Subhash Lele", "title": "An ANOVA Test for Parameter Estimability using Data Cloning with\n  Application to Statistical Inference for Dynamic Systems", "comments": "this version has been accepted in Computational Statistics and Data\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for complex systems are often built with more parameters than can be\nuniquely identified by available data. Because of the variety of causes,\nidentifying a lack of parameter identifiability typically requires mathematical\nmanipulation of models, monte carlo simulations, and examination of the Fisher\nInformation Matrix. A simple test for parameter estimability is introduced,\nusing Data Cloning, a Markov Chain Monte Carlo based algorithm. Together, Data\ncloning and the ANOVA based test determine if the model parameters are\nestimable and if so, determine their maximum likelihood estimates and provide\nasymptotic standard errors. When not all model parameters are estimable, the\nData Cloning results and the ANOVA test can be used to determine estimable\nparameter combinations or infer identifiability problems in the model\nstructure. The method is illustrated using three different real data systems\nthat are known to be difficult to analyze.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 21:06:56 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 20:33:22 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Campbell", "David", ""], ["Lele", "Subhash", ""]]}, {"id": "1305.3312", "submitter": "Eric Chi", "authors": "Eric C. Chi and Kenneth Lange", "title": "Stable Estimation of a Covariance Matrix Guided by Nuclear Norm\n  Penalties", "comments": "25 pages, 3 figures", "journal-ref": "Computational Statistics & Data Analysis 80:117-128, 2014", "doi": "10.1016/j.csda.2014.06.018", "report-no": null, "categories": "stat.ME cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of covariance matrices or their inverses plays a central role in\nmany statistical methods. For these methods to work reliably, estimated\nmatrices must not only be invertible but also well-conditioned. In this paper\nwe present an intuitive prior that shrinks the classic sample covariance\nestimator towards a stable target. We prove that our estimator is consistent\nand asymptotically efficient. Thus, it gracefully transitions towards the\nsample covariance matrix as the number of samples grows relative to the number\nof covariates. We also demonstrate the utility of our estimator in two standard\nsituations -- discriminant analysis and EM clustering -- when the number of\nsamples is dominated by or comparable to the number of covariates.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 22:03:00 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 22:45:01 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Chi", "Eric C.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1305.3409", "submitter": "Thordis Thorarinsdottir", "authors": "Thordis L. Thorarinsdottir", "title": "Calibration diagnostics for point process models via the probability\n  integral transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of the probability integral transform (PIT) for model\nvalidation in point process models. The simple PIT diagnostics assess the\ncalibration of the model and can detect inconsistencies in both the intensity\nand the interaction structure. For the Poisson model, the PIT diagnostics can\nbe calculated explicitly. Generally, the calibration may be assessed\nempirically based on random draws from the model and the method applies to\nprocesses of any dimension.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 10:00:46 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Thorarinsdottir", "Thordis L.", ""]]}, {"id": "1305.3445", "submitter": "Roman Schefzik", "authors": "Roman Schefzik", "title": "Ensemble Copula Coupling as a Multivariate Discrete Copula Approach", "comments": "references corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In probability and statistics, copulas play important roles theoretically as\nwell as to address a wide range of problems in various application areas. In\nthis paper, we introduce the concept of multivariate discrete copulas, discuss\ntheir equivalence to stochastic arrays, and provide a multivariate discrete\nversion of Sklar's theorem. These results provide the theoretical frame for the\nensemble copula coupling approach proposed by Schefzik et al. (2013) for the\nmultivariate statistical postprocessing of weather forecasts made by ensemble\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 12:37:22 GMT"}, {"version": "v2", "created": "Fri, 24 May 2013 15:50:57 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Schefzik", "Roman", ""]]}, {"id": "1305.3492", "submitter": "Romain Guy", "authors": "Romain Guy and Catherine Lar\\'edo and Elisabeta Vergu", "title": "Approximation of epidemic models by diffusion processes and their\n  statistical inference", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional continuous-time Markov jump processes $(Z(t))$ on\n$\\mathbb{Z}^p$ form a usual set-up for modeling $SIR$-like epidemics. However,\nwhen facing incomplete epidemic data, inference based on $(Z(t))$ is not easy\nto be achieved. Here, we start building a new framework for the estimation of\nkey parameters of epidemic models based on statistics of diffusion processes\napproximating $(Z(t))$. First, \\previous results on the approximation of\ndensity-dependent $SIR$-like models by diffusion processes with small diffusion\ncoefficient $\\frac{1}{\\sqrt{N}}$, where $N$ is the population size, are\ngeneralized to non-autonomous systems. Second, our previous inference results\non discretely observed diffusion processes with small diffusion coefficient are\nextended to time-dependent diffusions. Consistent and asymptotically Gaussian\nestimates are obtained for a fixed number $n$ of observations, which\ncorresponds to the epidemic context, and for $N\\rightarrow \\infty$. A\ncorrection term, which yields better estimates non asymptotically, is also\nincluded. Finally, performances and robustness of our estimators with respect\nto various parameters such as $R_0$ (the basic reproduction number), $N$, $n$\nare investigated on simulations. Two models, $SIR$ and $SIRS$, corresponding to\nsingle and recurrent outbreaks, respectively, are used to simulate data. The\nfindings indicate that our estimators have good asymptotic properties and\nbehave noticeably well for realistic numbers of observations and population\nsizes. This study lays the foundations of a generic inference method currently\nunder extension to incompletely observed epidemic data. Indeed, contrary to the\nmajority of current inference techniques for partially observed processes,\nwhich necessitates computer intensive simulations, our method being mostly an\nanalytical approach requires only the classical optimization steps.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 14:20:48 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 15:59:07 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Guy", "Romain", ""], ["Lar\u00e9do", "Catherine", ""], ["Vergu", "Elisabeta", ""]]}, {"id": "1305.3585", "submitter": "Mathew McLean", "authors": "Mathew W. McLean, Fabian Scheipl, Giles Hooker, Sonja Greven, and\n  David Ruppert", "title": "Bayesian Functional Generalized Additive Models with Sparsely Observed\n  Covariates", "comments": "substantial updates based on referee comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional generalized additive model (FGAM) was recently proposed in\nMcLean et al. (2013) as a more flexible alternative to the common functional\nlinear model (FLM) for regressing a scalar on functional covariates. In this\npaper, we develop a Bayesian version of FGAM for the case of Gaussian errors\nwith identity link function. Our approach allows the functional covariates to\nbe sparsely observed and measured with error, whereas the estimation procedure\nof McLean et al. (2013) required that they be noiselessly observed on a regular\ngrid. We consider both Monte Carlo and variational Bayes methods for fitting\nthe FGAM with sparsely observed covariates. Due to the complicated form of the\nmodel posterior distribution and full conditional distributions, standard Monte\nCarlo and variational Bayes algorithms cannot be used. The strategies we use to\nhandle the updating of parameters without closed-form full conditionals should\nbe of independent interest to applied Bayesian statisticians working with\nnonconjugate models. Our numerical studies demonstrate the benefits of our\nalgorithms over a two-step approach of first recovering the complete\ntrajectories using standard techniques and then fitting a functional regression\nmodel. In a real data analysis, our methods are applied to forecasting closing\nprice for items up for auction on the online auction website eBay.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 18:49:44 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 13:56:12 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["McLean", "Mathew W.", ""], ["Scheipl", "Fabian", ""], ["Hooker", "Giles", ""], ["Greven", "Sonja", ""], ["Ruppert", "David", ""]]}, {"id": "1305.4126", "submitter": "Itai Dattner", "authors": "Itai Dattner and Chris A.J. Klaassen", "title": "Optimal Rate of Direct Estimators in Systems of Ordinary Differential\n  Equations Linear in Functions of the Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many processes in biology, chemistry, physics, medicine, and engineering are\nmodeled by a system of differential equations. Such a system is usually\ncharacterized via unknown parameters and estimating their 'true' value is thus\nrequired. In this paper we focus on the quite common systems for which the\nderivatives of the states may be written as sums of products of a function of\nthe states and a function of the parameters.\n  For such a system linear in functions of the unknown parameters we present a\nnecessary and sufficient condition for identifiability of the parameters. We\ndevelop an estimation approach that bypasses the heavy computational burden of\nnumerical integration and avoids the estimation of system states derivatives,\ndrawbacks from which many classic estimation methods suffer. We also suggest an\nexperimental design for which smoothing can be circumvented. The optimal rate\nof the proposed estimators, i.e., their $\\sqrt n$-consistency, is proved and\nsimulation results illustrate their excellent finite sample performance and\ncompare it to other estimation approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 16:20:07 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 04:52:09 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 02:53:07 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Dattner", "Itai", ""], ["Klaassen", "Chris A. J.", ""]]}, {"id": "1305.4268", "submitter": "Jos\\'e Miguel Hern\\'andez-Lobato", "authors": "Yue Wu, Jos\\'e Miguel Hern\\'andez-Lobato and Zoubin Ghahramani", "title": "Dynamic Covariance Models for Multivariate Financial Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate prediction of time-changing covariances is an important problem\nin the modeling of multivariate financial data. However, some of the most\npopular models suffer from a) overfitting problems and multiple local optima,\nb) failure to capture shifts in market conditions and c) large computational\ncosts. To address these problems we introduce a novel dynamic model for\ntime-changing covariances. Over-fitting and local optima are avoided by\nfollowing a Bayesian approach instead of computing point estimates. Changes in\nmarket conditions are captured by assuming a diffusion process in parameter\nvalues, and finally computationally efficient and scalable inference is\nperformed using particle filters. Experiments with financial data show\nexcellent performance of the proposed method with respect to current standard\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 14:08:12 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2013 18:29:10 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Wu", "Yue", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1305.4273", "submitter": "Werner M\\\"uller", "authors": "Markus Hainy, Werner G. M\\\"uller, Helga Wagner", "title": "Likelihood-free Simulation-based Optimal Design", "comments": null, "journal-ref": null, "doi": null, "report-no": "IFAS research report 2013-64", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based optimal design techniques are a convenient tool for solving\na particular class of optimal design problems. The goal is to find the optimal\nconfiguration of factor settings with respect to an expected utility criterion.\nThis criterion depends on the specified probability model for the data and on\nthe assumed prior distribution for the model parameters. We develop new\nsimulation-based optimal design methods which incorporate likelihood-free\napproaches and utilize them in novel applications.\n  Most simulation-based design strategies solve the intractable expected\nutility integral at a specific design point by using Monte Carlo simulations\nfrom the probability model. Optimizing the criterion over the design points is\ncarried out in a separate step. M\\\"uller (1999) introduces an MCMC algorithm\nwhich simultaneously addresses the simulation as well as the optimization\nproblem. In principle, the optimal design can be found by detecting the utility\nmode of the sampled design points. Several improvements have been suggested to\nfacilitate this task for multidimensional design problems (see e.g. Amzal et\nal. 2006).\n  We aim to extend this simulation-based design methodology to design problems\nwhere the likelihood of the probability model is of an unknown analytical form\nbut it is possible to simulate from the probability model. We further assume\nthat prior observations are available. In such a setting it is seems natural to\nemploy approximate Bayesian computation (ABC) techniques in order to be able to\nsimulate from the conditional probability model. We provide a thorough review\nof adjacent literature and we investigate the benefits and the limitations of\nour design methodology for a particular paradigmatic example.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 15:04:27 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Hainy", "Markus", ""], ["M\u00fcller", "Werner G.", ""], ["Wagner", "Helga", ""]]}, {"id": "1305.4283", "submitter": "Oliver Ratmann", "authors": "Oliver Ratmann, Anton Camacho, Adam Meijer and G\\'e Donker", "title": "Statistical modelling of summary values leads to accurate Approximate\n  Bayesian Computations", "comments": "Videos can be played with Acrobat Reader. Manuscript under review and\n  not accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) methods rely on asymptotic arguments,\nimplying that parameter inference can be systematically biased even when\nsufficient statistics are available. We propose to construct the ABC\naccept/reject step from decision theoretic arguments on a suitable auxiliary\nspace. This framework, referred to as ABC*, fully specifies which test\nstatistics to use, how to combine them, how to set the tolerances and how long\nto simulate in order to obtain accuracy properties on the auxiliary space. Akin\nto maximum-likelihood indirect inference, regularity conditions establish when\nthe ABC* approximation to the posterior density is accurate on the original\nparameter space in terms of the Kullback-Leibler divergence and the maximum a\nposteriori point estimate. Fundamentally, escaping asymptotic arguments\nrequires knowledge of the distribution of test statistics, which we obtain\nthrough modelling the distribution of summary values, data points on a summary\nlevel. Synthetic examples and an application to time series data of influenza A\n(H3N2) infections in the Netherlands illustrate ABC* in action.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 18:07:07 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 21:24:05 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Ratmann", "Oliver", ""], ["Camacho", "Anton", ""], ["Meijer", "Adam", ""], ["Donker", "G\u00e9", ""]]}, {"id": "1305.4301", "submitter": "Paul McNicholas", "authors": "Paula M. Murray and Ryan P. Browne and Paul D. McNicholas", "title": "Mixtures of Skew-t Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2014.03.012", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a mixture of skew-t factor analyzers as well as a\nfamily of mixture models based thereon. The mixture of skew-t distributions\nmodel that we use arises as a limiting case of the mixture of generalized\nhyperbolic distributions. Like their Gaussian and t-distribution analogues, our\nmixture of skew-t factor analyzers are very well-suited to the model-based\nclustering of high-dimensional data. Imposing constraints on components of the\ndecomposed covariance parameter results in the development of eight flexible\nmodels. The alternating expectation-conditional maximization algorithm is used\nfor model parameter estimation and the Bayesian information criterion is used\nfor model selection. The models are applied to both real and simulated data,\ngiving superior clustering results compared to a well-established family of\nGaussian mixture models.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 21:36:07 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 17:38:10 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Murray", "Paula M.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1305.4318", "submitter": "Dabuxilatu Wang", "authors": "Dabuxilatu Wang and Qiang Xiong", "title": "A nonparametric CUSUM control chart based on the Mann-Whitney statistic", "comments": "13 pages, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This article aims to consider a new univariate nonparametric cumulative sum\n(CUSUM) control chart for small shift of location based on both change-point\nmodel and Mann-Whitney statistic. Some comparisons on the performances of the\nproposed chart with other charts as well as the properties of the test\nstatistic are presented. Simulations indicate that the proposed chart is\nsensitive in detection of the small mean shifts of the process by a high\nintensive accumulation of sample information when the underlying variable is\ncompletely distribution-free.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 02:45:20 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Wang", "Dabuxilatu", ""], ["Xiong", "Qiang", ""]]}, {"id": "1305.4390", "submitter": "Libo Sun", "authors": "Libo Sun, Chihoon Lee, and Jennifer A. Hoeting", "title": "A penalized simulated maximum likelihood approach in parameter\n  estimation for stochastic differential equations", "comments": "23 pages, 4 figures, 3 tables", "journal-ref": "Computational Statistics & Data Analysis 84 (2015): 54-67", "doi": "10.1016/j.csda.2014.11.007", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating parameters of stochastic differential\nequations (SDEs) with discrete-time observations that are either completely or\npartially observed. The transition density between two observations is\ngenerally unknown. We propose an importance sampling approach with an auxiliary\nparameter when the transition density is unknown. We embed the auxiliary\nimportance sampler in a penalized maximum likelihood framework which produces\nmore accurate and computationally efficient parameter estimates. Simulation\nstudies in three different models illustrate promising improvements of the new\npenalized simulated maximum likelihood method. The new procedure is designed\nfor the challenging case when some state variables are unobserved and moreover,\nobserved states are sparse over time, which commonly arises in ecological\nstudies. We apply this new approach to two epidemics of chronic wasting disease\nin mule deer.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 18:42:46 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 17:03:19 GMT"}, {"version": "v3", "created": "Wed, 21 May 2014 02:41:56 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 18:24:06 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Sun", "Libo", ""], ["Lee", "Chihoon", ""], ["Hoeting", "Jennifer A.", ""]]}, {"id": "1305.4413", "submitter": "Jin Liu Jin Liu", "authors": "Jin Liu and Can Yang and Xingjie Shi and Cong Li and Jian Huang and\n  Hongyu Zhao and Shuangge Ma", "title": "A Penalized Multi-trait Mixed Model for Association Mapping in\n  Pedigree-based GWAS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide association studies (GWAS), penalization is an important\napproach for identifying genetic markers associated with trait while mixed\nmodel is successful in accounting for a complicated dependence structure among\nsamples. Therefore, penalized linear mixed model is a tool that combines the\nadvantages of penalization approach and linear mixed model. In this study, a\nGWAS with multiple highly correlated traits is analyzed. For GWAS with multiple\nquantitative traits that are highly correlated, the analysis using traits\nmarginally inevitably lose some essential information among multiple traits. We\npropose a penalized-MTMM, a penalized multivariate linear mixed model that\nallows both the within-trait and between-trait variance components\nsimultaneously for multiple traits. The proposed penalized-MTMM estimates\nvariance components using an AI-REML method and conducts variable selection and\npoint estimation simultaneously using group MCP and sparse group MCP. Best\nlinear unbiased predictor (BLUP) is used to find predictive values and the\nPearson's correlations between predictive values and their corresponding\nobservations are used to evaluate prediction performance. Both prediction and\nselection performance of the proposed approach and its comparison with the\nuni-trait penalized-LMM are evaluated through simulation studies. We apply the\nproposed approach to a GWAS data from Genetic Analysis Workshop (GAW) 18.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 22:55:52 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Liu", "Jin", ""], ["Yang", "Can", ""], ["Shi", "Xingjie", ""], ["Li", "Cong", ""], ["Huang", "Jian", ""], ["Zhao", "Hongyu", ""], ["Ma", "Shuangge", ""]]}, {"id": "1305.4496", "submitter": "Xiaodong Luo", "authors": "Xiaodong Luo and Ibrahim Hoteit", "title": "Covariance inflation in the ensemble Kalman filter: a residual nudging\n  perspective and some implications", "comments": "Accepted by Monthly Weather Review", "journal-ref": null, "doi": "10.1175/MWR-D-13-00067.1", "report-no": null, "categories": "physics.ao-ph math.OC math.ST nlin.CD stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note examines the influence of covariance inflation on the distance\nbetween the measured observation and the simulated (or predicted) observation\nwith respect to the state estimate. In order for the aforementioned distance to\nbe bounded in a certain interval, some sufficient conditions are derived,\nindicating that the covariance inflation factor should be bounded in a certain\ninterval, and that the inflation bounds are related to the maximum and minimum\neigenvalues of certain matrices. Implications of these analytic results are\ndiscussed, and a numerical experiment is presented to verify the validity of\nour analysis.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 11:06:03 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Luo", "Xiaodong", ""], ["Hoteit", "Ibrahim", ""]]}, {"id": "1305.4669", "submitter": "Antonio Punzo", "authors": "Antonio Punzo and Paul D. McNicholas", "title": "Parsimonious mixtures of multivariate contaminated normal distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of multivariate contaminated normal distributions is developed for\nmodel-based clustering. In addition to the parameters of the classical normal\nmixture, our contaminated mixture has, for each cluster, a parameter\ncontrolling the proportion of mild outliers and one specifying the degree of\ncontamination. Crucially, these parameters do not have to be specified a\npriori, adding a flexibility to our approach. Parsimony is introduced via\neigen-decomposition of the component covariance matrices, and sufficient\nconditions for the identifiability of all the members of the resulting family\nare provided. An expectation-conditional maximization algorithm is outlined for\nparameter estimation and various implementation issues are discussed. Using a\nlarge scale simulation study, the behaviour of the proposed approach is\ninvestigated and comparison with well-established finite mixtures is provided.\nThe performance of this novel family of models is also illustrated on\nartificial and real data.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 22:29:30 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 21:46:40 GMT"}, {"version": "v3", "created": "Wed, 9 Apr 2014 10:03:32 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2015 10:35:17 GMT"}, {"version": "v5", "created": "Thu, 19 May 2016 09:58:08 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1305.4792", "submitter": "Christophe Ley", "authors": "Christophe Ley and Anouk Neven", "title": "Efficient inference about the tail weight in multivariate Student $t$\n  distributions", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new testing procedure about the tail weight parameter of\nmultivariate Student $t$ distributions by having recourse to the Le Cam\nmethodology. Our test is asymptotically as efficient as the classical\nlikelihood ratio test, but outperforms the latter by its flexibility and\nsimplicity: indeed, our approach allows to estimate the location and scatter\nnuisance parameters by any root-$n$ consistent estimators, hereby avoiding\nnumerically complex maximum likelihood estimation. The finite-sample properties\nof our test are analyzed in a Monte Carlo simulation study, and we apply our\nmethod on a financial data set. We conclude the paper by indicating how to use\nthis framework for efficient point estimation.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 12:01:16 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 22:53:10 GMT"}, {"version": "v3", "created": "Tue, 8 Apr 2014 22:45:18 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Ley", "Christophe", ""], ["Neven", "Anouk", ""]]}, {"id": "1305.4896", "submitter": "Anru Zhang", "authors": "Pixu Shi and Anru Zhang", "title": "Methods to Calculate the Upper Bound of Gini Coefficient Based on\n  Grouped Data and the Result for China", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  How to give an upper bound, especially the smallest upper bound of Gini\ncoefficient based on grouped data in the absence of income brackets is still a\nproblem not properly solved. This article provides an upper bound which is easy\nto compute, and provides an effective algorithm to calculate the exact value of\nthe smallest upper bound. As illustrations, the calculation results of bounds\nfor Gini coefficients of urban and rural China from 2003 to 2008 will be\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 17:33:11 GMT"}, {"version": "v2", "created": "Wed, 22 May 2013 00:34:20 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Shi", "Pixu", ""], ["Zhang", "Anru", ""]]}, {"id": "1305.4977", "submitter": "Yaonan Zhang", "authors": "Yaonan Zhang, Eric D. Kolaczyk, Bruce D. Spencer", "title": "Estimating network degree distributions under sampling: An inverse\n  problem, with applications to monitoring social media networks", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS800 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 166-199", "doi": "10.1214/14-AOAS800", "report-no": "IMS-AOAS-AOAS800", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a popular tool for representing elements in a system and their\ninterconnectedness. Many observed networks can be viewed as only samples of\nsome true underlying network. Such is frequently the case, for example, in the\nmonitoring and study of massive, online social networks. We study the problem\nof how to estimate the degree distribution - an object of fundamental interest\n- of a true underlying network from its sampled network. In particular, we show\nthat this problem can be formulated as an inverse problem. Playing a key role\nin this formulation is a matrix relating the expectation of our sampled degree\ndistribution to the true underlying degree distribution. Under many network\nsampling designs, this matrix can be defined entirely in terms of the design\nand is found to be ill-conditioned. As a result, our inverse problem frequently\nis ill-posed. Accordingly, we offer a constrained, penalized weighted\nleast-squares approach to solving this problem. A Monte Carlo variant of\nStein's unbiased risk estimation (SURE) is used to select the penalization\nparameter. We explore the behavior of our resulting estimator of network degree\ndistribution in simulation, using a variety of combinations of network models\nand sampling regimes. In addition, we demonstrate the ability of our method to\naccurately reconstruct the degree distributions of various sub-communities\nwithin online social networks corresponding to Friendster, Orkut and\nLiveJournal. Overall, our results show that the true degree distributions from\nboth homogeneous and inhomogeneous networks can be recovered with substantially\ngreater accuracy than reflected in the empirical degree distribution resulting\nfrom the original sampling.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 22:30:18 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 00:21:10 GMT"}, {"version": "v3", "created": "Fri, 5 Dec 2014 22:29:42 GMT"}, {"version": "v4", "created": "Thu, 28 May 2015 13:10:29 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Zhang", "Yaonan", ""], ["Kolaczyk", "Eric D.", ""], ["Spencer", "Bruce D.", ""]]}, {"id": "1305.4981", "submitter": "Adam Kapelner", "authors": "Adam Kapelner and Abba Krieger", "title": "Matching on-the-fly in Sequential Experiments for Higher Power and\n  Efficiency", "comments": "20 pages, 1 algorithm, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic allocation procedure that increases power and efficiency\nwhen measuring an average treatment effect in sequential randomized trials.\nSubjects arrive iteratively and are either randomized or paired via a matching\ncriterion to a previously randomized subject and administered the alternate\ntreatment. We develop estimators for the average treatment effect that combine\ninformation from both the matched pairs and unmatched subjects as well as an\nexact test. Simulations illustrate the method's higher efficiency and power\nover competing allocation procedures in both controlled scenarios and\nhistorical experimental data.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 23:02:35 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Kapelner", "Adam", ""], ["Krieger", "Abba", ""]]}, {"id": "1305.4982", "submitter": "Brandy Ringham", "authors": "Brandy M. Ringham, Todd A. Alonzo, John T. Brinton, Sarah M. Kreidler,\n  Aarti Munjal, Keith E. Muller, Deborah H. Glueck", "title": "Reducing decision errors in the paired comparison of the diagnostic\n  accuracy of screening tests with Gaussian outcomes", "comments": "32 pages, including 5 figures and 2 tables, submitted to BMC Medical\n  Research Methodology March, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists often use a paired comparison of the areas under the receiver\noperating characteristic curves to decide which continuous cancer screening\ntest has the best diagnostic accuracy. In the paired design, all participants\nare screened with both tests. Participants with unremarkable screening results\nenter a follow-up period. Participants with suspicious screening results and\nthose who show evidence of disease during follow-up receive the gold standard\ntest. The remaining participants are classified as non-cases, even though some\nmay have occult disease. The standard analysis includes all study participants\nin the analysis, which can create bias in the estimates of diagnostic accuracy.\nIf the bias affects the area under the curve for one screening test more than\nthe other screening test, scientists may make the wrong decision as to which\nscreening test has better diagnostic accuracy. We describe a weighted maximum\nlikelihood bias correction method to reduce decision errors. We assessed the\nability of the bias correction method to reduce decision errors via simulation\nstudies. The simulations compared the Type I error rate and power of the\nstandard analysis with that of the bias-corrected analysis. The performance of\nthe bias correction method depends on characteristics of the screening tests\nand the disease, and on the percentage of study participants who receive the\ngold standard test. In studies with a large amount of bias in the difference in\nthe full area under the curve, the bias correction method reduces the Type I\nerror rate and improves power for the correct decision. In order to determine\nif bias correction is needed for a specific screening trial, we recommend the\ninvestigator conduct a simulation study using our free software.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 23:11:57 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Ringham", "Brandy M.", ""], ["Alonzo", "Todd A.", ""], ["Brinton", "John T.", ""], ["Kreidler", "Sarah M.", ""], ["Munjal", "Aarti", ""], ["Muller", "Keith E.", ""], ["Glueck", "Deborah H.", ""]]}, {"id": "1305.5363", "submitter": "David Golan", "authors": "David Golan and Saharon Rosset", "title": "Narrowing the gap on heritability of common disease by direct estimation\n  in case-control GWAS", "comments": "main text: 30 pages, 4 figures. Supplementary text: 35 pages, 16\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major developments in recent years in the search for missing\nheritability of human phenotypes is the adoption of linear mixed-effects models\n(LMMs) to estimate heritability due to genetic variants which are not\nsignificantly associated with the phenotype. A variant of the LMM approach has\nbeen adapted to case-control studies and applied to many major diseases by Lee\net al. (2011), successfully accounting for a considerable portion of the\nmissing heritability. For example, for Crohn's disease their estimated\nheritability was 22% compared to 50-60% from family studies. In this letter we\npropose to estimate heritability of disease directly by regression of phenotype\nsimilarities on genotype correlations, corrected to account for ascertainment.\nWe refer to this method as genetic correlation regression (GCR). Using GCR we\nestimate the heritability of Crohn's disease at 34% using the same data. We\ndemonstrate through extensive simulation that our method yields unbiased\nheritability estimates, which are consistently higher than LMM estimates.\nMoreover, we develop a heuristic correction to LMM estimates, which can be\napplied to published LMM results. Applying our heuristic correction increases\nthe estimated heritability of multiple sclerosis from 30% to 52.6%.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 09:48:55 GMT"}, {"version": "v2", "created": "Mon, 27 May 2013 10:46:32 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Golan", "David", ""], ["Rosset", "Saharon", ""]]}, {"id": "1305.5445", "submitter": "Duncan Lee", "authors": "Duncan Lee, Alastair Rushworth and Sujit K. Sahu", "title": "A Bayesian localised conditional auto-regressive model for estimating\n  the health effects of air pollution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the long-term health effects of air pollution is a challenging\ntask, especially when modelling small-area disease incidence data in an\necological study design. The challenge comes from the unobserved underlying\nspatial correlation structure in these data, which is accounted for using\nrandom effects modelled by a globally smooth conditional autoregressive model.\nThese smooth random effects confound the effects of air pollution, which are\nalso globally smooth. To avoid this collinearity a Bayesian localised\nconditional autoregressive model is developed for the random effects. This\nlocalised model is flexible spatially, in the sense that it is not only able to\nmodel step changes in the random effects surface, but also is able to capture\nareas of spatial smoothness in the study region. This methodological\ndevelopment allows us to improve the estimation performance of the covariate\neffects, compared to using traditional conditional auto-regressive models.\nThese results are established using a simulation study, and are then\nillustrated with our motivating study on air pollution and respiratory ill\nhealth in Greater Glasgow, Scotland in 2010. The model shows substantial health\neffects of particulate matter air pollution and income deprivation, whose\neffects have been consistently attenuated by the currently available globally\nsmooth models.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 15:04:27 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Lee", "Duncan", ""], ["Rushworth", "Alastair", ""], ["Sahu", "Sujit K.", ""]]}, {"id": "1305.5493", "submitter": "Robert Maier", "authors": "Robert S. Maier", "title": "Information Criteria for Deciding between Normal Regression Models", "comments": "27 pages, margins fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models fitted to data can be assessed on their goodness of fit,\nthough models with many parameters should be disfavored to prevent\nover-fitting. Statisticians' tools for this are little known to physical\nscientists. These include the Akaike Information Criterion (AIC), a penalized\ngoodness-of-fit statistic, and the AICc, a variant including a small-sample\ncorrection. They entered the physical sciences through being used by\nastrophysicists to compare cosmological models; e.g., predictions of the\ndistance-redshift relation. The AICc is shown to have been misapplied, being\napplicable only if error variances are unknown. If error bars accompany the\ndata, the AIC should be used instead. Erroneous applications of the AICc are\nlisted in an appendix. It is also shown how the variability of the AIC\ndifference between models with a known error variance can be estimated. This\nyields a significance test that can potentially replace the use of `Akaike\nweights' for deciding between such models. Additionally, the effects of model\nmisspecification are examined. For regression models fitted to data sets\nwithout (rather than with) error bars, they are major: the AICc may be shifted\nby an unknown amount. The extent of this in the fitting of physical models\nremains to be studied.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 17:33:09 GMT"}, {"version": "v2", "created": "Fri, 24 May 2013 19:07:24 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Maier", "Robert S.", ""]]}, {"id": "1305.5594", "submitter": "Moreno Bevilacqua", "authors": "Moreno Bevilacqua and Carlo Gaetan", "title": "Comparing composite likelihood methods based on pairs for spatial\n  Gaussian random fieldsM", "comments": "27 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years there has been a growing interest in proposing methods for\nestimating covariance functions for geostatistical data. Among these, maximum\nlikelihood estimators have nice features when we deal with a Gaussian model.\nHowever maximum likelihood becomes impractical when the number of observations\nis very large. In this work we review some solutions and we contrast them in\nterms of loss of statistical efficiency and computational burden. Specifically\nwe focus on three types of weighted composite likelihood functions based on\npairs and we compare them with the method of covariance tapering. Asymptotics\nproperties of the three estimation methods are derived. We illustrate the\neffectiveness of the methods through theoretical examples, simulation\nexperiments and by analysing a data set on yearly total precipitation anomalies\nat weather stations in the United States.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 01:20:07 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Bevilacqua", "Moreno", ""], ["Gaetan", "Carlo", ""]]}, {"id": "1305.5712", "submitter": "Ari Pakman", "authors": "Alexandro D. Ramirez and Liam Paninski", "title": "Fast inference in generalized linear models via expected log-likelihoods", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models play an essential role in a wide variety of\nstatistical applications. This paper discusses an approximation of the\nlikelihood in these models that can greatly facilitate computation. The basic\nidea is to replace a sum that appears in the exact log-likelihood by an\nexpectation over the model covariates; the resulting \"expected log-likelihood\"\ncan in many cases be computed significantly faster than the exact\nlog-likelihood. In many neuroscience experiments the distribution over model\ncovariates is controlled by the experimenter and the expected log-likelihood\napproximation becomes particularly useful; for example, estimators based on\nmaximizing this expected log-likelihood (or a penalized version thereof) can\noften be obtained with orders of magnitude computational savings compared to\nthe exact maximum likelihood estimators. A risk analysis establishes that these\nmaximum EL estimators often come with little cost in accuracy (and in some\ncases even improved accuracy) compared to standard maximum likelihood\nestimates. Finally, we find that these methods can significantly decrease the\ncomputation time of marginal likelihood calculations for model selection and of\nMarkov chain Monte Carlo methods for sampling from the posterior parameter\ndistribution. We illustrate our results by applying these methods to a\ncomputationally-challenging dataset of neural spike trains obtained via\nlarge-scale multi-electrode recordings in the primate retina.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 12:34:50 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Ramirez", "Alexandro D.", ""], ["Paninski", "Liam", ""]]}, {"id": "1305.5870", "submitter": "Matan Gavish", "authors": "Matan Gavish and David L. Donoho", "title": "The Optimal Hard Threshold for Singular Values is 4/sqrt(3)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider recovery of low-rank matrices from noisy data by hard\nthresholding of singular values, where singular values below a prescribed\nthreshold $\\lambda$ are set to 0. We study the asymptotic MSE in a framework\nwhere the matrix size is large compared to the rank of the matrix to be\nrecovered, and the signal-to-noise ratio of the low-rank piece stays constant.\nThe AMSE-optimal choice of hard threshold, in the case of n-by-n matrix in\nnoise level \\sigma, is simply $(4/\\sqrt{3}) \\sqrt{n}\\sigma \\approx 2.309\n\\sqrt{n}\\sigma$ when $\\sigma$ is known, or simply $2.858\\cdot y_{med}$ when\n$\\sigma$ is unknown, where $y_{med}$ is the median empirical singular value.\nFor nonsquare $m$ by $n$ matrices with $m \\neq n$, these thresholding\ncoefficients are replaced with different provided constants. In our asymptotic\nframework, this thresholding rule adapts to unknown rank and to unknown noise\nlevel in an optimal manner: it is always better than hard thresholding at any\nother value, no matter what the matrix is that we are trying to recover, and is\nalways better than ideal Truncated SVD (TSVD), which truncates at the true rank\nof the low-rank matrix we are trying to recover. Hard thresholding at the\nrecommended value to recover an n-by-n matrix of rank r guarantees an AMSE at\nmost $3nr\\sigma^2$. In comparison, the guarantee provided by TSVD is\n$5nr\\sigma^2$, the guarantee provided by optimally tuned singular value soft\nthresholding is $6nr\\sigma^2$, and the best guarantee achievable by any\nshrinkage of the data singular values is $2nr\\sigma^2$. Empirical evidence\nshows that these AMSE properties of the $4/\\sqrt{3}$ thresholding rule remain\nvalid even for relatively small n, and that performance improvement over TSVD\nand other shrinkage rules is substantial, turning it into the practical hard\nthreshold of choice.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 23:54:18 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 04:20:50 GMT"}, {"version": "v3", "created": "Wed, 4 Jun 2014 07:02:01 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Gavish", "Matan", ""], ["Donoho", "David L.", ""]]}, {"id": "1305.5879", "submitter": "Hanwen Huang", "authors": "Hanwen Huang, Yufeng Liu, Ming Yuan, J. S. Marron", "title": "Statistical Significance of Clustering using Soft Thresholding", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Clustering methods have led to a number of important discoveries in\nbioinformatics and beyond. A major challenge in their use is determining which\nclusters represent important underlying structure, as opposed to spurious\nsampling artifacts. This challenge is especially serious, and very few methods\nare available when the data are very high in dimension. Statistical\nSignificance of Clustering (SigClust) is a recently developed cluster\nevaluation tool for high dimensional low sample size data. An important\ncomponent of the SigClust approach is the very definition of a single cluster\nas a subset of data sampled from a multivariate Gaussian distribution. The\nimplementation of SigClust requires the estimation of the eigenvalues of the\ncovariance matrix for the null multivariate Gaussian distribution. We show that\nthe original eigenvalue estimation can lead to a test that suffers from severe\ninflation of type-I error, in the important case where there are huge single\nspikes in the eigenvalues. This paper addresses this critical challenge using a\nnovel likelihood based soft thresholding approach to estimate these eigenvalues\nwhich leads to a much improved SigClust. These major improvements in SigClust\nperformance are shown by both theoretical work and an extensive simulation\nstudy. Applications to some cancer genomic data further demonstrate the\nusefulness of these improvements.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2013 02:08:03 GMT"}, {"version": "v2", "created": "Wed, 29 May 2013 01:08:38 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Huang", "Hanwen", ""], ["Liu", "Yufeng", ""], ["Yuan", "Ming", ""], ["Marron", "J. S.", ""]]}, {"id": "1305.5894", "submitter": "Aida Toma", "authors": "Aida Toma and Samuela Leoni-Aubin", "title": "Robust portfolio optimization using pseudodistances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of outliers in financial asset returns is a frequently occuring\nphenomenon and may lead to unreliable mean-variance optimized portfolios. This\nfact is due to the unbounded influence that outliers can have on the mean\nreturns and covariance estimators that are inputs in the optimization\nprocedure. In the present paper we consider new robust estimators of location\nand covariance obtained by minimizing an empirical version of a pseudodistance\nbetween the assumed model and the true model underlying the data. We prove\nstatistical properties of the new mean and covariance matrix estimators, such\nas affine equivariance, B-robustness and efficiency. These estimators can be\neasily used in place of the classical estimators, thereby providing robust\noptimized portfolios. A Monte Carlo simulation study and an application to real\ndata show the advantages of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2013 07:21:49 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Toma", "Aida", ""], ["Leoni-Aubin", "Samuela", ""]]}, {"id": "1305.6156", "submitter": "Cyrus Samii", "authors": "Peter M. Aronow and Cyrus Samii", "title": "Estimating Average Causal Effects Under General Interference, with\n  Application to a Social Network Experiment", "comments": null, "journal-ref": "Aronow, Peter M.; Samii, Cyrus. Estimating average causal effects\n  under general interference, with application to a social network experiment.\n  Ann. Appl. Stat. 11 (2017), no. 4, 1912--1947", "doi": "10.1214/16-AOAS1005", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a randomization-based framework for estimating causal\neffects under interference between units, motivated by challenges that arise in\nanalyzing experiments on social networks. The framework integrates three\ncomponents: (i) an experimental design that defines the probability\ndistribution of treatment assignments, (ii) a mapping that relates experimental\ntreatment assignments to exposures received by units in the experiment, and\n(iii) estimands that make use of the experiment to answer questions of\nsubstantive interest. We develop the case of estimating average unit-level\ncausal effects from a randomized experiment with interference of arbitrary but\nknown form. The resulting estimators are based on inverse probability\nweighting. We provide randomization-based variance estimators that account for\nthe complex clustering that can occur when interference is present. We also\nestablish consistency and asymptotic normality under local dependence\nassumptions. We discuss refinements including covariate-adjusted effect\nestimators and ratio estimation. We evaluate empirical performance in realistic\nsettings with a naturalistic simulation using social network data from American\nschools. We then present results from a field experiment on the spread of\nanti-conflict norms and behavior among school students.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 09:35:07 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 15:52:35 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 16:07:35 GMT"}, {"version": "v4", "created": "Wed, 20 Jun 2018 15:16:51 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Aronow", "Peter M.", ""], ["Samii", "Cyrus", ""]]}, {"id": "1305.6204", "submitter": "Dimitris Kugiumtzis", "authors": "Dimitris Kugiumtzis", "title": "Direct coupling information measure from non-uniform embedding", "comments": "28 pages, 13 figures, 5 tables, accepted in Physical Review E", "journal-ref": null, "doi": "10.1103/PhysRevE.87.062918", "report-no": null, "categories": "physics.data-an cs.IT math.IT nlin.CD stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A measure to estimate the direct and directional coupling in multivariate\ntime series is proposed. The measure is an extension of a recently published\nmeasure of conditional Mutual Information from Mixed Embedding (MIME) for\nbivariate time series. In the proposed measure of Partial MIME (PMIME), the\nembedding is on all observed variables, and it is optimized in explaining the\nresponse variable. It is shown that PMIME detects correctly direct coupling,\nand outperforms the (linear) conditional Granger causality and the partial\ntransfer entropy. We demonstrate that PMIME does not rely on significance test\nand embedding parameters, and the number of observed variables has no effect on\nits statistical accuracy, it may only slow the computations. The importance of\nthese points is shown in simulations and in an application to epileptic\nmulti-channel scalp EEG.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 13:08:40 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Kugiumtzis", "Dimitris", ""]]}, {"id": "1305.6324", "submitter": "Benjamin Lenoir", "authors": "Benjamin Lenoir", "title": "A general approach of least squares estimation and optimal filtering", "comments": "7 pages", "journal-ref": "Optimization and Engineering 15:3 (2014) 609-617", "doi": "10.1007/s11081-013-9217-7", "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least squares method allows fitting parameters of a mathematical model\nfrom experimental data. This article proposes a general approach of this\nmethod. After introducing the method and giving a formal definition, the\ntransitivity of the method as well as numerical considerations are discussed.\nThen two particular cases are considered: the usual least squares method and\nthe Generalized Least Squares method. In both cases, the estimator and its\nvariance are characterized in the time domain and in the Fourier domain.\nFinally, the equivalence of the Generalized Least Squares method and the\noptimal filtering technique using a matched filter is established.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 20:53:44 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Lenoir", "Benjamin", ""]]}, {"id": "1305.6340", "submitter": "Joong-Ho Won", "authors": "Joong-Ho Won, Johan Lim, Donghyeon Yu, Byung Soo Kim, Kyunga Kim", "title": "Monotone false discovery rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a procedure to obtain monotone estimates of both the\nlocal and the tail false discovery rates that arise in large-scale multiple\ntesting. The proposed monotonization is asymptotically optimal for controlling\nthe false discovery rate and also has many attractive finite-sample properties.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 22:49:29 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2013 12:31:16 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2013 07:02:17 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Won", "Joong-Ho", ""], ["Lim", "Johan", ""], ["Yu", "Donghyeon", ""], ["Kim", "Byung Soo", ""], ["Kim", "Kyunga", ""]]}, {"id": "1305.6634", "submitter": "Jonathan Friedemann Donges", "authors": "Jonathan F. Donges, Irina Petrova, Alexander Loew, Norbert Marwan,\n  J\\\"urgen Kurths", "title": "How complex climate networks complement eigen techniques for the\n  statistical analysis of climatological data", "comments": "18 pages, 11 figures", "journal-ref": "Climate Dynamics 45(9), 2407-2424 (2015)", "doi": "10.1007/s00382-015-2479-3", "report-no": null, "categories": "physics.data-an physics.ao-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigen techniques such as empirical orthogonal function (EOF) or coupled\npattern (CP) / maximum covariance analysis have been frequently used for\ndetecting patterns in multivariate climatological data sets. Recently,\nstatistical methods originating from the theory of complex networks have been\nemployed for the very same purpose of spatio-temporal analysis. This climate\nnetwork (CN) analysis is usually based on the same set of similarity matrices\nas is used in classical EOF or CP analysis, e.g., the correlation matrix of a\nsingle climatological field or the cross-correlation matrix between two\ndistinct climatological fields. In this study, formal relationships as well as\nconceptual differences between both eigen and network approaches are derived\nand illustrated using exemplary global precipitation, evaporation and surface\nair temperature data sets. These results allow to pinpoint that CN analysis can\ncomplement classical eigen techniques and provides additional information on\nthe higher-order structure of statistical interrelationships in climatological\ndata. Hence, CNs are a valuable supplement to the statistical toolbox of the\nclimatologist, particularly for making sense out of very large data sets such\nas those generated by satellite observations and climate model intercomparison\nexercises.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 21:11:59 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 21:10:08 GMT"}, {"version": "v3", "created": "Sat, 9 Apr 2016 15:13:45 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Donges", "Jonathan F.", ""], ["Petrova", "Irina", ""], ["Loew", "Alexander", ""], ["Marwan", "Norbert", ""], ["Kurths", "J\u00fcrgen", ""]]}, {"id": "1305.6657", "submitter": "Rebecca Steorts", "authors": "Malay Ghosh and Rebecca C. Steorts", "title": "Two-stage Benchmarking as Applied to Small Area Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been recent growth in small area estimation due to the need for\nmore precise estimation of small geographic areas, which has led to groups such\nas the U.S. Census Bureau, Google, and the RAND corporation utilizing small\narea estimation procedures. We develop novel two-stage benchmarking methodology\nusing a single weighted squared error loss function that combines the loss at\nthe unit level and the area level without any specific distributional\nassumptions. We consider this loss while benchmarking the weighted means at\neach level or both the weighted means and weighted variability at the unit\nlevel. Multivariate extensions are immediate. We analyze the behavior of our\nmethods using a complex study from the National Health Interview Survey (NHIS)\nfrom 2000, which estimates the proportion of people that do not have health\ninsurance for many domains of an Asian subpopulation. Finally, the methodology\nis explored via simulated data under the proposed model. We ultimately conclude\nthat three proposed benchmarked Bayes estimators do not dominate each other,\nleaving much exploration for future research.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 23:33:47 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 22:21:09 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Ghosh", "Malay", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1305.6909", "submitter": "Pasquale Erto", "authors": "Pasquale Erto", "title": "The Inverse Weibull Survival Distribution and its Proper Application", "comments": "31 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The peculiar properties of the Inverse Weibull (IW) distribution are shown.\nIt is proven that the IW distribution is one of the few models having upside-\ndown bathtub (UBT) shaped hazard function. Three real and typical de generative\nmechanisms, which lead exactly to the IW random variable, are formulated. So a\nnew approach to proper application of this relatively unknown survival model is\nsupported. However, we consider also the case in which any knowledge about\ngenerative mechanism is unavailable. In this hypothesis, we study a procedure\nbased on the Anderson-Darling statistic and log-likelihood function to\ndiscriminate between the IW model and others alternative UBT distributions. The\ninvariant properties of the proposed discriminating criteria have been proven.\nBased on Monte Carlo simulations, the probability of the correct selection has\nbeen computed. A real applicative example closes the paper.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2013 19:13:36 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Erto", "Pasquale", ""]]}, {"id": "1305.6979", "submitter": "Johan Ugander", "authors": "Johan Ugander, Brian Karrer, Lars Backstrom, Jon Kleinberg", "title": "Graph cluster randomization: network exposure to multiple universes", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing is a standard approach for evaluating the effect of online\nexperiments; the goal is to estimate the `average treatment effect' of a new\nfeature or condition by exposing a sample of the overall population to it. A\ndrawback with A/B testing is that it is poorly suited for experiments involving\nsocial interference, when the treatment of individuals spills over to\nneighboring individuals along an underlying social network. In this work, we\npropose a novel methodology using graph clustering to analyze average treatment\neffects under social interference. To begin, we characterize graph-theoretic\nconditions under which individuals can be considered to be `network exposed' to\nan experiment. We then show how graph cluster randomization admits an efficient\nexact algorithm to compute the probabilities for each vertex being network\nexposed under several of these exposure conditions. Using these probabilities\nas inverse weights, a Horvitz-Thompson estimator can then provide an effect\nestimate that is unbiased, provided that the exposure model has been properly\nspecified.\n  Given an estimator that is unbiased, we focus on minimizing the variance.\nFirst, we develop simple sufficient conditions for the variance of the\nestimator to be asymptotically small in n, the size of the graph. However, for\ngeneral randomization schemes, this variance can be lower bounded by an\nexponential function of the degrees of a graph. In contrast, we show that if a\ngraph satisfies a restricted-growth condition on the growth rate of\nneighborhoods, then there exists a natural clustering algorithm, based on\nvertex neighborhoods, for which the variance of the estimator can be upper\nbounded by a linear function of the degrees. Thus we show that proper cluster\nrandomization can lead to exponentially lower estimator variance when\nexperimentally measuring average treatment effects under interference.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 00:04:42 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Ugander", "Johan", ""], ["Karrer", "Brian", ""], ["Backstrom", "Lars", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1305.7007", "submitter": "Xu Han", "authors": "Jianqing Fan, Xu Han", "title": "Estimation of False Discovery Proportion with Unknown Dependence", "comments": "39 pages, 7 figures", "journal-ref": "Published in Journal of Royal Statistical Society-Methodology, Vol\n  79, 1143-1164, 2017", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Large-scale multiple testing with highly correlated test statistics arises\nfrequently in many scientific research. Incorporating correlation information\nin estimating false discovery proportion has attracted increasing attention in\nrecent years. When the covariance matrix of test statistics is known, Fan, Han\n& Gu (2012) provided a consistent estimate of False Discovery Proportion (FDP)\nunder arbitrary dependence structure. However, the covariance matrix is often\nunknown in many applications and such dependence information has to be\nestimated before estimating FDP (Efron, 2010). The estimation accuracy can\ngreatly affect the convergence result of FDP or even violate its consistency.\nIn the current paper, we provide methodological modification and theoretical\ninvestigations for estimation of FDP with unknown covariance. First we develop\nrequirements for estimates of eigenvalues and eigenvectors such that we can\nobtain a consistent estimate of FDP. Secondly we give conditions on the\ndependence structures such that the estimate of FDP is consistent. Such\ndependence structures include sparse covariance matrices, which have been\npopularly considered in the contemporary random matrix theory. When data are\nsampled from an approximate factor model, which encompasses most practical\nsituations, we provide a consistent estimate of FDP via exploiting this\nspecific dependence structure. The results are further demonstrated by\nsimulation studies and some real data applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 05:30:25 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 21:48:47 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Fan", "Jianqing", ""], ["Han", "Xu", ""]]}, {"id": "1305.7477", "submitter": "Jason Lee", "authors": "Jason D. Lee, Yuekai Sun, Jonathan E. Taylor", "title": "On model selection consistency of regularized M-estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized M-estimators are used in diverse areas of science and engineering\nto fit high-dimensional models with some low-dimensional structure. Usually the\nlow-dimensional structure is encoded by the presence of the (unknown)\nparameters in some low-dimensional model subspace. In such settings, it is\ndesirable for estimates of the model parameters to be \\emph{model selection\nconsistent}: the estimates also fall in the model subspace. We develop a\ngeneral framework for establishing consistency and model selection consistency\nof regularized M-estimators and show how it applies to some special cases of\ninterest in statistical learning. Our analysis identifies two key properties of\nregularized M-estimators, referred to as geometric decomposability and\nirrepresentability, that ensure the estimators are consistent and model\nselection consistent.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 16:24:17 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 16:59:21 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2013 23:47:12 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2013 19:44:45 GMT"}, {"version": "v5", "created": "Wed, 23 Oct 2013 19:34:33 GMT"}, {"version": "v6", "created": "Tue, 10 Dec 2013 17:52:09 GMT"}, {"version": "v7", "created": "Wed, 12 Feb 2014 08:52:10 GMT"}, {"version": "v8", "created": "Sat, 11 Oct 2014 05:54:58 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""], ["Taylor", "Jonathan E.", ""]]}]