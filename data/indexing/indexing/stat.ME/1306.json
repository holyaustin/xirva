[{"id": "1306.0100", "submitter": "Pasquale Cirillo", "authors": "Pasquale Cirillo", "title": "Are your data really Pareto distributed?", "comments": "8 figures; presented at the \"Econophysics and Networks Across Scales\"\n  workshop at Lorentz Center Leiden in May 2013", "journal-ref": null, "doi": "10.1016/j.physa.2013.07.061", "report-no": null, "categories": "stat.ME physics.data-an q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pareto distributions, and power laws in general, have demonstrated to be very\nuseful models to describe very different phenomena, from physics to finance. In\nrecent years, the econophysical literature has proposed a large amount of\npapers and models justifying the presence of power laws in economic data. Most\nof the times, this Paretianity is inferred from the observation of some plots,\nsuch as the Zipf plot and the mean excess plot. If the Zipf plot looks almost\nlinear, then everything is ok and the parameters of the Pareto distribution are\nestimated. Often with OLS. Unfortunately, as we show in this paper, these\nheuristic graphical tools are not reliable. To be more exact, we show that only\na combination of plots can give some degree of confidence about the real\npresence of Paretianity in the data. We start by reviewing some of the most\nimportant plots, discussing their points of strength and weakness, and then we\npropose some additional tools that can be used to refine the analysis.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 12:45:54 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Cirillo", "Pasquale", ""]]}, {"id": "1306.0113", "submitter": "Johannes Lederer", "authors": "Johannes Lederer", "title": "Trust, but verify: benefits and pitfalls of least-squares refitting in\n  high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least-squares refitting is widely used in high dimensional regression to\nreduce the prediction bias of l1-penalized estimators (e.g., Lasso and\nSquare-Root Lasso). We present theoretical and numerical results that provide\nnew insights into the benefits and pitfalls of least-squares refitting. In\nparticular, we consider both prediction and estimation, and we pay close\nattention to the effects of correlations in the design matrices of linear\nregression models, since these correlations - although often neglected - are\ncrucial in the context of linear regression, especially in high dimensional\ncontexts. First, we demonstrate that the benefit of least-squares refitting\nstrongly depends on the setting and task under consideration: least-squares\nrefitting can be beneficial even for settings with highly correlated design\nmatrices but is not advisable for all settings, and least-squares refitting can\nbe beneficial for estimation but performs better for prediction. Finally, we\nintroduce a criterion that indicates whether least-squares refitting is\nadvisable for a specific setting and task under consideration, and we conduct a\nthorough simulation study involving the Lasso to show the usefulness of this\ncriterion.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 14:41:52 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Lederer", "Johannes", ""]]}, {"id": "1306.0156", "submitter": "Wagner Barreto-Souza", "authors": "Wagner Barreto-Souza, Marcelo Bourguignon", "title": "A skew true INAR(1) process with application", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer-valued time series models have been a recurrent theme considered in\nmany papers in the last three decades, but only a few of them have dealt with\nmodels on $\\mathbb Z$ (that is, including both negative and positive integers).\nOur aim in this paper is to introduce a first-order integer-valued\nautoregressive process on $\\mathbb Z$ with skew discrete Laplace marginals\n(Kozubowski and Inusah, 2006). For this, we define a new operator that acts on\ntwo independent latent processes, similarly as made by Freeland (2010). We\nderive some joint and conditional basic properties of the proposed process such\nas characteristic function, moments, higher-order moments and jumps. Estimators\nfor the parameters of our model are proposed and their asymptotic normality are\nestablished. We run a Monte Carlo simulation to evaluate the finite-sample\nperformance of these estimators. In order to illustrate the potentiality of our\nprocess, we apply it to a real data set about population increase rates.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 22:42:20 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Bourguignon", "Marcelo", ""]]}, {"id": "1306.0187", "submitter": "Marcelo Pereyra", "authors": "Marcelo Pereyra", "title": "Proximal Markov chain Monte Carlo algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Metropolis-adjusted Langevin algorithm (MALA) that\nuses convex analysis to simulate efficiently from high-dimensional densities\nthat are log-concave, a class of probability distributions that is widely used\nin modern high-dimensional statistics and data analysis. The method is based on\na new first-order approximation for Langevin diffusions that exploits\nlog-concavity to construct Markov chains with favourable convergence\nproperties. This approximation is closely related to Moreau-Yoshida\nregularisations for convex functions and uses proximity mappings instead of\ngradient mappings to approximate the continuous-time process. The proposed\nmethod complements existing MALA methods in two ways. First, the method is\nshown to have very robust stability properties and to converge geometrically\nfor many target densities for which other MALA are not geometric, or only if\nthe step size is sufficiently small. Second, the method can be applied to\nhigh-dimensional target densities that are not continuously differentiable, a\nclass of distributions that is increasingly used in image processing and\nmachine learning and that is beyond the scope of existing MALA and HMC\nalgorithms. To use this method it is necessary to compute or to approximate\nefficiently the proximity mappings of the logarithm of the target density. For\nseveral popular models, including many Bayesian models used in modern signal\nand image processing and machine learning, this can be achieved with convex\noptimisation algorithms and with approximations based on proximal splitting\ntechniques, which can be implemented in parallel. The proposed method is\ndemonstrated on two challenging high-dimensional and non-differentiable models\nrelated to image resolution enhancement and low-rank matrix estimation that are\nnot well addressed by existing MCMC methodology.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 09:41:33 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 12:56:44 GMT"}, {"version": "v3", "created": "Thu, 3 Jul 2014 15:00:44 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2015 11:50:36 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Pereyra", "Marcelo", ""]]}, {"id": "1306.0408", "submitter": "Geir-Arne Fuglstad", "authors": "Geir-Arne Fuglstad, Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Non-stationary Spatial Modelling with Applications to Spatial Prediction\n  of Precipitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-stationary spatial Gaussian random field (GRF) is described as the\nsolution of an inhomogeneous stochastic partial differential equation (SPDE),\nwhere the covariance structure of the GRF is controlled by the coefficients in\nthe SPDE. This allows for a flexible way to vary the covariance structure,\nwhere intuition about the resulting structure can be gained from the local\nbehaviour of the differential equation. Additionally, computations can be done\nwith computationally convenient Gaussian Markov random fields which approximate\nthe true GRFs. The model is applied to a dataset of annual precipitation in the\nconterminous US. The non-stationary model performs better than a stationary\nmodel measured with both CRPS and the logarithmic scoring rule.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 13:59:31 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Fuglstad", "Geir-Arne", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1306.0517", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es", "title": "Exact risk improvement of bandwidth selectors for kernel density\n  estimation with directional data", "comments": "26 pages, 4 figures, 8 tables", "journal-ref": "Electronic Journal of Statistics, 7(2013):1655-1685, 2013", "doi": "10.1214/13-EJS821", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  New bandwidth selectors for kernel density estimation with directional data\nare presented in this work. These selectors are based on asymptotic and exact\nerror expressions for the kernel density estimator combined with mixtures of\nvon Mises distributions. The performance of the proposed selectors is\ninvestigated in a simulation study and compared with other existing rules for a\nlarge variety of directional scenarios, sample sizes and dimensions. The\nselector based on the exact error expression turns out to have the best\nbehaviour of the studied selectors for almost all the situations. This selector\nis illustrated with real data for the circular and spherical cases.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 17:40:19 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2014 10:32:29 GMT"}, {"version": "v3", "created": "Sat, 27 Sep 2014 19:40:18 GMT"}, {"version": "v4", "created": "Wed, 17 Dec 2014 17:37:25 GMT"}, {"version": "v5", "created": "Sun, 20 Sep 2020 23:19:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""]]}, {"id": "1306.0657", "submitter": "Rachael Maltiel", "authors": "Rachael Maltiel, Adrian E. Raftery, Tyler H. McCormick, Aaron J.\n  Baraff", "title": "Estimating population size using the network scale up method", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS827 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1247-1277", "doi": "10.1214/15-AOAS827", "report-no": "IMS-AOAS-AOAS827", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for estimating the size of hard-to-reach populations from\ndata collected using network-based questions on standard surveys. Such data\narise by asking respondents how many people they know in a specific group\n(e.g., people named Michael, intravenous drug users). The Network Scale up\nMethod (NSUM) is a tool for producing population size estimates using these\nindirect measures of respondents' networks. Killworth et al. [Soc. Netw. 20\n(1998a) 23-50, Evaluation Review 22 (1998b) 289-308] proposed maximum\nlikelihood estimators of population size for a fixed effects model in which\nrespondents' degrees or personal network sizes are treated as fixed. We extend\nthis by treating personal network sizes as random effects, yielding principled\nstatements of uncertainty. This allows us to generalize the model to account\nfor variation in people's propensity to know people in particular subgroups\n(barrier effects), such as their tendency to know people like themselves, as\nwell as their lack of awareness of or reluctance to acknowledge their contacts'\ngroup memberships (transmission bias). NSUM estimates also suffer from recall\nbias, in which respondents tend to underestimate the number of members of\nlarger groups that they know, and conversely for smaller groups. We propose a\ndata-driven adjustment method to deal with this. Our methods perform well in\nsimulation studies, generating improved estimates and calibrated uncertainty\nintervals, as well as in back estimates of real sample data. We apply them to\ndata from a study of HIV/AIDS prevalence in Curitiba, Brazil. Our results show\nthat when transmission bias is present, external information about its likely\nextent can greatly improve the estimates. The methods are implemented in the\nNSUM R package.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 05:46:32 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 12:02:08 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Maltiel", "Rachael", ""], ["Raftery", "Adrian E.", ""], ["McCormick", "Tyler H.", ""], ["Baraff", "Aaron J.", ""]]}, {"id": "1306.0817", "submitter": "Steven Thompson", "authors": "Steven K. Thompson", "title": "Dynamic spatial and network sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers some designs for sampling and interventions in dynamic\nnetworks and spatial temporal settings. The sample spreads through the\npopulation largely by tracing network links, although random sampling or\nspatial designs may be used in addition. To investigate the effectiveness of\ndifferent designs for finding units on which to make observations and introduce\ninterventions is investigated through simulations. For this purpose a dynamic\nspatial network model is developed based on simple stochastic processes. The\nsampling processes considered have both acquisition processes and attrition\nprocess by which units are added and removed from the sample. The effect of an\nintervention introduced with a given sampling design is assessed by the change\nin the resulting equilibrium distribution or, in more detail, by the\ndistribution of sample paths resulting from the intervention strategy.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 20:01:18 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Thompson", "Steven K.", ""]]}, {"id": "1306.0959", "submitter": "Mark Tygert", "authors": "Mark Tygert and Rachel Ward", "title": "Testing goodness-of-fit for logistic regression", "comments": "13 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicitly accounting for all applicable independent variables, even when the\nmodel being tested does not, is critical in testing goodness-of-fit for\nlogistic regression. This can increase statistical power by orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 01:34:54 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2013 18:32:47 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1306.0964", "submitter": "Han Liu", "authors": "Michael Rosenblum and Han Liu and and En-Hsu Yen", "title": "Optimal Tests of Treatment Effects for the Overall Population and Two\n  Subpopulations in Randomized Trials, using Sparse Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new, optimal methods for analyzing randomized trials, when it is\nsuspected that treatment effects may differ in two predefined subpopulations.\nSuch sub-populations could be defined by a biomarker or risk factor measured at\nbaseline. The goal is to simultaneously learn which subpopulations benefit from\nan experimental treatment, while providing strong control of the familywise\nType I error rate. We formalize this as a multiple testing problem and show it\nis computationally infeasible to solve using existing techniques. Our solution\ninvolves a novel approach, in which we first transform the original multiple\ntesting problem into a large, sparse linear program. We then solve this problem\nusing advanced optimization techniques. This general method can solve a variety\nof multiple testing problems and decision theory problems related to optimal\ntrial design, for which no solution was previously available. In particular, we\nconstruct new multiple testing procedures that satisfy minimax and Bayes\noptimality criteria. For a given optimality criterion, our new approach yields\nthe optimal tradeoff? between power to detect an effect in the overall\npopulation versus power to detect effects in subpopulations. We demonstrate our\napproach in examples motivated by two randomized trials of new treatments for\nHIV.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 02:49:13 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Rosenblum", "Michael", ""], ["Liu", "Han", ""], ["Yen", "and En-Hsu", ""]]}, {"id": "1306.0976", "submitter": "Weidong Liu", "authors": "Weidong Liu", "title": "Gaussian Graphical Model Estimation with False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of high dimensional Gaussian graphical\nmodel (GGM). Typically, the existing methods depend on regularization\ntechniques. As a result, it is necessary to choose the regularized parameter.\nHowever, the precise relationship between the regularized parameter and the\nnumber of false edges in GGM estimation is unclear. Hence, it is impossible to\nevaluate their performance rigorously. In this paper, we propose an alternative\nmethod by a multiple testing procedure. Based on our new test statistics for\nconditional dependence, we propose a simultaneous testing procedure for\nconditional dependence in GGM. Our method can control the false discovery rate\n(FDR) asymptotically. The numerical performance of the proposed method shows\nthat our method works quite well.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 03:56:54 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Liu", "Weidong", ""]]}, {"id": "1306.1028", "submitter": "Mari Myllym\\\"aki", "authors": "Mari Myllym\\\"aki, Pavel Grabarnik, Henri Seijo, Dietrich Stoyan", "title": "Deviation test construction and power comparison for marked spatial\n  point patterns", "comments": null, "journal-ref": "Spatial Statistics 11 (2015), 19-34", "doi": "10.1016/j.spasta.2014.11.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deviation test belong to core tools in point process statistics, where\nhypotheses are typically tested considering differences between an empirical\nsummary function and its expectation under the null hypothesis, which depend on\na distance variable r. This test is a classical device to overcome the multiple\ncomparison problem which appears since the functional differences have to be\nconsidered for a range of distances r simultaneously. The test has three basic\ningredients: (i) choice of a suitable summary function, (ii) transformation of\nthe summary function or scaling of the differences, and (iii) calculation of a\nglobal deviation measure. We consider in detail the construction of such tests\nboth for stationary and finite point processes and show by two toy examples and\na simulation study for the case of the random labelling hypothesis that the\npoints (i) and (ii) have great influence on the power of the tests.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 09:30:42 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Myllym\u00e4ki", "Mari", ""], ["Grabarnik", "Pavel", ""], ["Seijo", "Henri", ""], ["Stoyan", "Dietrich", ""]]}, {"id": "1306.1294", "submitter": "Tianxiao Pang", "authors": "Pang Tianxiao, Zhang Danna and Chong Terence Tai-Leung", "title": "Asymptotic inferences for an AR(1) model with a change point: stationary\n  and nearly non-stationary cases", "comments": "35 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the asymptotic inference for AR(1) models with a possible\nstructural break in the AR parameter $\\beta $ near the unity at an unknown time\n$k_{0}$. Consider the model $y_{t}=\\beta_{1}y_{t-1}I\\{t\\leq k_{0}\\}+\\beta\n_{2}y_{t-1}I\\{t>k_{0}\\}+\\varepsilon_{t},~t=1,2,\\cdots ,T,$ where $I\\{\\cdot \\}$\ndenotes the indicator function. We examine two cases: Case (I)\n$|\\beta_{1}|<1,\\beta_{2}=\\beta_{2T}=1-c/T$; and case\n(II)~$\\beta_{1}=\\beta_{1T}=1-c/T,|\\beta _{2}|<1$, where $c$\\ is a fixed\nconstant, and $\\{\\varepsilon_{t},t\\geq 1\\}$\\ is a sequence of i.i.d. random\nvariables which are in the domain of attraction of the normal law with zero\nmeans and possibly infinite variances. We derive the limiting distributions of\nthe least squares estimators of $\\beta_{1}$ and $\\beta_{2} $, and that of the\nbreak-point estimator for shrinking break for the aforementioned cases. Monte\nCarlo simulations are conducted to demonstrate the finite sample properties of\nthe estimators. Our theoretical results are supported by Monte Carlo\nsimulations.\\newline\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 04:58:21 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Tianxiao", "Pang", ""], ["Danna", "Zhang", ""], ["Tai-Leung", "Chong Terence", ""]]}, {"id": "1306.1587", "submitter": "Hau-tieng Wu", "authors": "Amit Singer and Hau-tieng Wu", "title": "Spectral Convergence of the connection Laplacian from random samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral methods that are based on eigenvectors and eigenvalues of discrete\ngraph Laplacians, such as Diffusion Maps and Laplacian Eigenmaps are often used\nfor manifold learning and non-linear dimensionality reduction. It was\npreviously shown by Belkin and Niyogi \\cite{belkin_niyogi:2007} that the\neigenvectors and eigenvalues of the graph Laplacian converge to the\neigenfunctions and eigenvalues of the Laplace-Beltrami operator of the manifold\nin the limit of infinitely many data points sampled independently from the\nuniform distribution over the manifold. Recently, we introduced Vector\nDiffusion Maps and showed that the connection Laplacian of the tangent bundle\nof the manifold can be approximated from random samples. In this paper, we\npresent a unified framework for approximating other connection Laplacians over\nthe manifold by considering its principle bundle structure. We prove that the\neigenvectors and eigenvalues of these Laplacians converge in the limit of\ninfinitely many independent random samples. We generalize the spectral\nconvergence results to the case where the data points are sampled from a\nnon-uniform distribution, and for manifolds with and without boundary.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 02:06:31 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 22:38:55 GMT"}, {"version": "v3", "created": "Sun, 31 May 2015 17:58:26 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Singer", "Amit", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1306.1598", "submitter": "Anirban Bhattacharya", "authors": "Jing Zhou, Anirban Bhattacharya, Amy Herring, David Dunson", "title": "Bayesian factorizations of big sparse tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become routine to collect data that are structured as multiway arrays\n(tensors). There is an enormous literature on low rank and sparse matrix\nfactorizations, but limited consideration of extensions to the tensor case in\nstatistics. The most common low rank tensor factorization relies on parallel\nfactor analysis (PARAFAC), which expresses a rank $k$ tensor as a sum of rank\none tensors. When observations are only available for a tiny subset of the\ncells of a big tensor, the low rank assumption is not sufficient and PARAFAC\nhas poor performance. We induce an additional layer of dimension reduction by\nallowing the effective rank to vary across dimensions of the table. For\nconcreteness, we focus on a contingency table application. Taking a Bayesian\napproach, we place priors on terms in the factorization and develop an\nefficient Gibbs sampler for posterior computation. Theory is provided showing\nposterior concentration rates in high-dimensional settings, and the methods are\nshown to have excellent performance in simulations and several real data\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 03:24:56 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Zhou", "Jing", ""], ["Bhattacharya", "Anirban", ""], ["Herring", "Amy", ""], ["Dunson", "David", ""]]}, {"id": "1306.1725", "submitter": "Claudia Kluppelberg", "authors": "Habib Esmaeili and Claudia Kl\\\"uppelberg", "title": "Two-step estimation of a multivariate L\\'evy process", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the concept of a L\\'evy copula to describe the dependence structure\nof a multivariate L\\'evy process we present a new estimation procedure. We\nconsider a parametric model for the marginal L\\'evy processes as well as for\nthe L\\'evy copula and estimate the parameters by a two-step procedure. We first\nestimate the parameters of the marginal processes, and then estimate in a\nsecond step only the dependence structure parameter. For infinite L\\'evy\nmeasures we truncate the small jumps and base our statistical analysis on the\nlarge jumps of the model. Prominent example will be a bivariate stable \\lp,\nwhich allows for analytic calculations and, hence, for a comparison of\ndifferent methods. We prove asymptotic normality of the parameter estimates\nfrom the two-step procedure and, in particular, we derive the Godambe\ninformation matrix, whose inverse is the covariance matrix of the normal limit\nlaw. A simulation study investigates the loss of efficiency because of the\ntwo-step procedure and the truncation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 13:47:58 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Esmaeili", "Habib", ""], ["Kl\u00fcppelberg", "Claudia", ""]]}, {"id": "1306.1868", "submitter": "Xiao Wang", "authors": "Xiao Wang, Pang Du, Jinglai Shen", "title": "Smoothing splines with varying smoothing parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the development of spatially adaptive smoothing splines\nfor the estimation of a regression function with non-homogeneous smoothness\nacross the domain. Two challenging issues that arise in this context are the\nevaluation of the equivalent kernel and the determination of a local penalty.\nThe roughness penalty is a function of the design points in order to\naccommodate local behavior of the regression function. It is shown that the\nspatially adaptive smoothing spline estimator is approximately a kernel\nestimator. The resulting equivalent kernel is spatially dependent. The\nequivalent kernels for traditional smoothing splines are a special case of this\ngeneral solution. With the aid of the Green's function for a two-point boundary\nvalue problem, the explicit forms of the asymptotic mean and variance are\nobtained for any interior point. Thus, the optimal roughness penalty function\nis obtained by approximately minimizing the asymptotic integrated mean square\nerror. Simulation results and an application illustrate the performance of the\nproposed estimator.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 01:52:31 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Wang", "Xiao", ""], ["Du", "Pang", ""], ["Shen", "Jinglai", ""]]}, {"id": "1306.1970", "submitter": "Donghyeon Yu", "authors": "Donghyeon Yu, Joong-Ho Won, Taehoon Lee, Johan Lim, Sungroh Yoon", "title": "High-dimensional Fused Lasso Regression using Majorization-Minimization\n  and Parallel Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a majorization-minimization (MM) algorithm for\nhigh-dimensional fused lasso regression (FLR) suitable for parallelization\nusing graphics processing units (GPUs). The MM algorithm is stable and flexible\nas it can solve the FLR problems with various types of design matrices and\npenalty structures within a few tens of iterations. We also show that the\nconvergence of the proposed algorithm is guaranteed. We conduct numerical\nstudies to compare our algorithm with other existing algorithms, demonstrating\nthat the proposed MM algorithm is competitive in many settings including the\ntwo-dimensional FLR with arbitrary design matrices. The merit of GPU\nparallelization is also exhibited.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 00:56:41 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 08:05:03 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2013 13:05:38 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Yu", "Donghyeon", ""], ["Won", "Joong-Ho", ""], ["Lee", "Taehoon", ""], ["Lim", "Johan", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1306.1977", "submitter": "Sancar Adali", "authors": "Sancar Adali and Carey E. Priebe", "title": "Fidelity-Commensurability Tradeoff in Joint Embedding of Disparate\n  Dissimilarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various data settings, it is necessary to compare observations from\ndisparate data sources. We assume the data is in the dissimilarity\nrepresentation and investigate a joint embedding method that results in a\ncommensurate representation of disparate dissimilarities. We further assume\nthat there are \"matched\" observations from different conditions which can be\nconsidered to be highly similar, for the sake of inference. The joint embedding\nresults in the joint optimization of fidelity (preservation of within-condition\ndissimilarities) and commensurability (preservation of between-condition\ndissimilarities between matched observations). We show that the tradeoff\nbetween these two criteria can be made explicit using weighted raw stress as\nthe objective function for multidimensional scaling. In our investigations, we\nuse a weight parameter, $w$, to control the tradeoff, and choose match\ndetection as the inference task. Our results show weights that are optimal\n(with respect to the inference task) are different than equal weights for\ncommensurability and fidelity and the proposed weighted embedding scheme\nprovides significant improvements in statistical power.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 03:59:05 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 01:40:45 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 06:00:26 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Adali", "Sancar", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1306.2004", "submitter": "Przemys{\\l}aw Spurek", "authors": "Przemys{\\l}aw Spurek and Jacek Tabor", "title": "Optimal Rescaling and the Mahalanobis Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the basic problems in data analysis lies in choosing the optimal\nrescaling (change of coordinate system) to study properties of a given data-set\n$Y$. The classical Mahalanobis approach has its basis in the classical\nnormalization/rescaling formula $Y \\ni y \\to \\Sigma_Y^{-1/2} \\cdot\n(y-\\mathrm{m}_Y)$, where $\\mathrm{m}_Y$ denotes the mean of $Y$ and $\\Sigma_Y$\nthe covariance matrix .\n  Based on the cross-entropy we generalize this approach and define the\nparameter which measures the fit of a given affine rescaling of $Y$ compared to\nthe Mahalanobis one. This allows in particular to find an optimal change of\ncoordinate system which satisfies some additional conditions. In particular we\nshow that in the case when we put origin of coordinate system in $ \\mathrm{m} $\nthe optimal choice is given by the transformation $Y \\ni y \\to \\Sigma_Y^{-1/2}\n\\cdot (y-\\mathrm{m}_Y)$, where $$\n\\Sigma=\\Sigma_Y(\\Sigma_Y-\\frac{(\\mathrm{m}-\\mathrm{m}_Y)(\\mathrm{m}-\\mathrm{m}_Y)^T}{1+\\|\\mathrm{m}-\\mathrm{m}_Y\\|_{\\Sigma_Y}^2})^{-1}\\Sigma_Y.\n$$\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 10:41:44 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Spurek", "Przemys\u0142aw", ""], ["Tabor", "Jacek", ""]]}, {"id": "1306.2281", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic, Arthur Gretton and Wicher Bergsma", "title": "A Kernel Test for Three-Variable Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce kernel nonparametric tests for Lancaster three-variable\ninteraction and for total independence, using embeddings of signed measures\ninto a reproducing kernel Hilbert space. The resulting test statistics are\nstraightforward to compute, and are used in powerful interaction tests, which\nare consistent against all alternatives for a large family of reproducing\nkernels. We show the Lancaster test to be sensitive to cases where two\nindependent causes individually have weak influence on a third dependent\nvariable, but their combined effect has a strong influence. This makes the\nLancaster test especially suited to finding structure in directed graphical\nmodels, where it outperforms competing nonparametric tests in detecting such\nV-structures.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 18:54:47 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Sejdinovic", "Dino", ""], ["Gretton", "Arthur", ""], ["Bergsma", "Wicher", ""]]}, {"id": "1306.2365", "submitter": "Oksana Chkrebtii", "authors": "Oksana A. Chkrebtii, David A. Campbell, Ben Calderhead, Mark A.\n  Girolami", "title": "Bayesian Solution Uncertainty Quantification for Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore probability modelling of discretization uncertainty for system\nstates defined implicitly by ordinary or partial differential equations.\nAccounting for this uncertainty can avoid posterior under-coverage when\nlikelihoods are constructed from a coarsely discretized approximation to system\nequations. A formalism is proposed for inferring a fixed but a priori unknown\nmodel trajectory through Bayesian updating of a prior process conditional on\nmodel information. A one-step-ahead sampling scheme for interrogating the model\nis described, its consistency and first order convergence properties are\nproved, and its computational complexity is shown to be proportional to that of\nnumerical explicit one-step solvers. Examples illustrate the flexibility of\nthis framework to deal with a wide variety of complex and large-scale systems.\nWithin the calibration problem, discretization uncertainty defines a layer in\nthe Bayesian hierarchy, and a Markov chain Monte Carlo algorithm that targets\nthis posterior distribution is presented. This formalism is used for inference\non the JAK-STAT delay differential equation model of protein dynamics from\nindirectly observed measurements. The discussion outlines implications for the\nnew field of probabilistic numerics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 22:08:14 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 18:42:47 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 16:07:34 GMT"}, {"version": "v4", "created": "Sat, 26 Dec 2015 17:52:42 GMT"}, {"version": "v5", "created": "Sun, 23 Oct 2016 21:47:37 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Chkrebtii", "Oksana A.", ""], ["Campbell", "David A.", ""], ["Calderhead", "Ben", ""], ["Girolami", "Mark A.", ""]]}, {"id": "1306.2419", "submitter": "Paul Kabaila", "authors": "Waruni Abeysekera and Paul Kabaila", "title": "A new recentered confidence sphere for the multivariate normal mean", "comments": "A small error has been corrected", "journal-ref": "Optimized recentered confidence spheres for the multivariate\n  normal mean. Electronic Journal of Statistics, 11, 1798-1826 (2017)", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new recentered confidence sphere for the mean, theta, of a\nmultivariate normal distribution. This sphere is centred on the positive-part\nJames-Stein estimator, with radius that is a piecewise cubic Hermite\ninterpolating polynomial function of the norm of the data vector. This radius\nfunction is determined by numerically minimizing the scaled expected volume, at\ntheta = 0, of this confidence sphere, subject to the coverage constraint. We\nuse the computationally-convenient formula, derived by Casella and Hwang [3],\nfor the coverage probability of a recentered confidence sphere. Casella and\nHwang, op. cit., describe a recentered confidence sphere that is also centred\non the positive-part James-Stein estimator, but with radius function determined\nby empirical Bayes considerations. Our new recentered confidence sphere\ncompares favourably with this confidence sphere, in terms of both the minimum\ncoverage probability and the scaled expected volume at theta = 0.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 05:01:05 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 01:51:20 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Abeysekera", "Waruni", ""], ["Kabaila", "Paul", ""]]}, {"id": "1306.2427", "submitter": "Samuel M\\\"{u}ller", "authors": "Samuel M\\\"uller, J. L. Scealy, A. H. Welsh", "title": "Model Selection in Linear Mixed Models", "comments": "Published in at http://dx.doi.org/10.1214/12-STS410 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 135-167", "doi": "10.1214/12-STS410", "report-no": "IMS-STS-STS410", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed effects models are highly flexible in handling a broad range of\ndata types and are therefore widely used in applications. A key part in the\nanalysis of data is model selection, which often aims to choose a parsimonious\nmodel with other desirable properties from a possibly very large set of\ncandidate statistical models. Over the last 5-10 years the literature on model\nselection in linear mixed models has grown extremely rapidly. The problem is\nmuch more complicated than in linear regression because selection on the\ncovariance structure is not straightforward due to computational issues and\nboundary problems arising from positive semidefinite constraints on covariance\nmatrices. To obtain a better understanding of the available methods, their\nproperties and the relationships between them, we review a large body of\nliterature on linear mixed model selection. We arrange, implement, discuss and\ncompare model selection methods based on four major approaches: information\ncriteria such as AIC or BIC, shrinkage methods based on penalized loss\nfunctions such as LASSO, the Fence procedure and Bayesian techniques.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 06:08:29 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["M\u00fcller", "Samuel", ""], ["Scealy", "J. L.", ""], ["Welsh", "A. H.", ""]]}, {"id": "1306.2503", "submitter": "Jaeyong Lee", "authors": "Jaeyong Lee, Fernando A. Quintana, Peter M\\\"uller, Lorenzo Trippa", "title": "Defining Predictive Probability Functions for Species Sampling Models", "comments": "Published in at http://dx.doi.org/10.1214/12-STS407 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 209-222", "doi": "10.1214/12-STS407", "report-no": "IMS-STS-STS407", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the class of species sampling models (SSM). In particular, we\ninvestigate the relation between the exchangeable partition probability\nfunction (EPPF) and the predictive probability function (PPF). It is\nstraightforward to define a PPF from an EPPF, but the converse is not\nnecessarily true. In this paper we introduce the notion of putative PPFs and\nshow novel conditions for a putative PPF to define an EPPF. We show that all\npossible PPFs in a certain class have to define (unnormalized) probabilities\nfor cluster membership that are linear in cluster size. We give a new necessary\nand sufficient condition for arbitrary putative PPFs to define an EPPF.\nFinally, we show posterior inference for a large class of SSMs with a PPF that\nis not linear in cluster size and discuss a numerical method to derive its PPF.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 12:35:01 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Lee", "Jaeyong", ""], ["Quintana", "Fernando A.", ""], ["M\u00fcller", "Peter", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "1306.2728", "submitter": "David Johnstone", "authors": "David Johnstone, Dennis Lindley", "title": "Mean-Variance and Expected Utility: The Borch Paradox", "comments": "Published in at http://dx.doi.org/10.1214/12-STS408 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 223-237", "doi": "10.1214/12-STS408", "report-no": "IMS-STS-STS408", "categories": "stat.ME q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model of rational decision-making in most of economics and statistics is\nexpected utility theory (EU) axiomatised by von Neumann and Morgenstern, Savage\nand others. This is less the case, however, in financial economics and\nmathematical finance, where investment decisions are commonly based on the\nmethods of mean-variance (MV) introduced in the 1950s by Markowitz. Under the\nMV framework, each available investment opportunity (\"asset\") or portfolio is\nrepresented in just two dimensions by the ex ante mean and standard deviation\n$(\\mu,\\sigma)$ of the financial return anticipated from that investment.\nUtility adherents consider that in general MV methods are logically incoherent.\nMost famously, Norwegian insurance theorist Borch presented a proof suggesting\nthat two-dimensional MV indifference curves cannot represent the preferences of\na rational investor (he claimed that MV indifference curves \"do not exist\").\nThis is known as Borch's paradox and gave rise to an important but generally\nlittle-known philosophical literature relating MV to EU. We examine the main\nearly contributions to this literature, focussing on Borch's logic and the\narguments by which it has been set aside.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 07:23:24 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Johnstone", "David", ""], ["Lindley", "Dennis", ""]]}, {"id": "1306.2776", "submitter": "Christophe Ley", "authors": "Angelo Efoevi Koudou and Christophe Ley", "title": "Efficiency combined with simplicity: new testing procedures for\n  Generalized Inverse Gaussian models", "comments": "19 pages", "journal-ref": null, "doi": "10.1007/s11749-014-0378-2", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard efficient testing procedures in the Generalized Inverse Gaussian\n(GIG) family (also known as Halphen Type A family) are likelihood ratio tests,\nhence rely on Maximum Likelihood (ML) estimation of the three parameters of the\nGIG. The particular form of GIG densities, involving modified Bessel functions,\nprevents in general from a closed-form expression for ML estimators, which are\nobtained at the expense of complex numerical approximation methods. On the\ncontrary, Method of Moments (MM) estimators allow for concise expressions, but\ntests based on these estimators suffer from a lack of efficiency compared to\nlikelihood ratio tests. This is why, in recent years, trade-offs between ML and\nMM estimators have been proposed, resulting in simpler yet not completely\nefficient estimators and tests. In the present paper, we do not propose such a\ntrade-off but rather an optimal combination of both methods, our tests\ninheriting efficiency from an ML-like construction and simplicity from the MM\nestimators of the nuisance parameters. This goal shall be reached by attacking\nthe problem from a new angle, namely via the Le Cam methodology. Besides\nproviding simple efficient testing methods, the theoretical background of this\nmethodology further allows us to write out explicitly power expressions for our\ntests. A Monte Carlo simulation study shows that, also at small sample sizes,\nour simpler procedures do at least as good as the complex likelihood ratio\ntests. We conclude the paper by applying our findings on two real-data sets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 10:15:14 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 11:44:43 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Koudou", "Angelo Efoevi", ""], ["Ley", "Christophe", ""]]}, {"id": "1306.2791", "submitter": "Yiting Deng", "authors": "Yiting Deng, D. Sunshine Hillygus, Jerome P. Reiter, Yajuan Si, Siyu\n  Zheng", "title": "Handling Attrition in Longitudinal Studies: The Case for Refreshment\n  Samples", "comments": "Published in at http://dx.doi.org/10.1214/13-STS414 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 238-256", "doi": "10.1214/13-STS414", "report-no": "IMS-STS-STS414", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panel studies typically suffer from attrition, which reduces sample size and\ncan result in biased inferences. It is impossible to know whether or not the\nattrition causes bias from the observed panel data alone. Refreshment samples -\nnew, randomly sampled respondents given the questionnaire at the same time as a\nsubsequent wave of the panel - offer information that can be used to diagnose\nand adjust for bias due to attrition. We review and bolster the case for the\nuse of refreshment samples in panel studies. We include examples of both a\nfully Bayesian approach for analyzing the concatenated panel and refreshment\ndata, and a multiple imputation approach for analyzing only the original panel.\nFor the latter, we document a positive bias in the usual multiple imputation\nvariance estimator. We present models appropriate for three waves and two\nrefreshment samples, including nonterminal attrition. We illustrate the\nthree-wave analysis using the 2007-2008 Associated Press-Yahoo! News Election\nPoll.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 11:40:37 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Deng", "Yiting", ""], ["Hillygus", "D. Sunshine", ""], ["Reiter", "Jerome P.", ""], ["Si", "Yajuan", ""], ["Zheng", "Siyu", ""]]}, {"id": "1306.2812", "submitter": "Shaun Seaman", "authors": "Shaun Seaman, John Galati, Dan Jackson, John Carlin", "title": "What Is Meant by \"Missing at Random\"?", "comments": "Published in at http://dx.doi.org/10.1214/13-STS415 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 257-268", "doi": "10.1214/13-STS415", "report-no": "IMS-STS-STS415", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of missing at random is central in the literature on statistical\nanalysis with missing data. In general, inference using incomplete data should\nbe based not only on observed data values but should also take account of the\npattern of missing values. However, it is often said that if data are missing\nat random, valid inference using likelihood approaches (including Bayesian) can\nbe obtained ignoring the missingness mechanism. Unfortunately, the term\n\"missing at random\" has been used inconsistently and not always clearly; there\nhas also been a lack of clarity around the meaning of \"valid inference using\nlikelihood\". These issues have created potential for confusion about the exact\nconditions under which the missingness mechanism can be ignored, and perhaps\nfed confusion around the meaning of \"analysis ignoring the missingness\nmechanism\". Here we provide standardised precise definitions of \"missing at\nrandom\" and \"missing completely at random\", in order to promote unification of\nthe theory. Using these definitions we clarify the conditions that suffice for\n\"valid inference\" to be obtained under a variety of inferential paradigms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 13:00:47 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Seaman", "Shaun", ""], ["Galati", "John", ""], ["Jackson", "Dan", ""], ["Carlin", "John", ""]]}, {"id": "1306.2834", "submitter": "Ghislaine Gayraud", "authors": "Mauro Bernardi, Ghislaine Gayraud, Lea Petrella", "title": "Bayesian inference for CoVaR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent financial disasters emphasised the need to investigate the consequence\nassociated with the tail co-movements among institutions; episodes of contagion\nare frequently observed and increase the probability of large losses affecting\nmarket participants' risk capital. Commonly used risk management tools fail to\naccount for potential spillover effects among institutions because they provide\nindividual risk assessment. We contribute to analyse the interdependence\neffects of extreme events providing an estimation tool for evaluating the\nConditional Value-at-Risk (CoVaR) defined as the Value-at-Risk of an\ninstitution conditioned on another institution being under distress. In\nparticular, our approach relies on Bayesian quantile regression framework. We\npropose a Markov chain Monte Carlo algorithm exploiting the Asymmetric Laplace\ndistribution and its representation as a location-scale mixture of Normals.\nMoreover, since risk measures are usually evaluated on time series data and\nreturns typically change over time, we extend the CoVaR model to account for\nthe dynamics of the tail behaviour. Application on U.S. companies belonging to\ndifferent sectors of the Standard and Poor's Composite Index (S&P500) is\nconsidered to evaluate the marginal contribution to the overall systemic risk\nof each individual institution\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 14:26:07 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 15:56:32 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2013 16:46:12 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Bernardi", "Mauro", ""], ["Gayraud", "Ghislaine", ""], ["Petrella", "Lea", ""]]}, {"id": "1306.3014", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen, Geoffrey J. McLachlan, and Ian A. Wood", "title": "Mixtures of Spatial Spline Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension of the functional data analysis framework for\nunivariate functions to the analysis of surfaces: functions of two variables.\nThe spatial spline regression (SSR) approach developed can be used to model\nsurfaces that are sampled over a rectangular domain. Furthermore, combining SSR\nwith linear mixed effects models (LMM) allows for the analysis of populations\nof surfaces, and combining the joint SSR-LMM method with finite mixture models\nallows for the analysis of populations of surfaces with sub-family structures.\nThrough the mixtures of spatial splines regressions (MSSR) approach developed,\nwe present methodologies for clustering surfaces into sub-families, and for\nperforming surface-based discriminant analysis. The effectiveness of our\nmethodologies, as well as the modeling capabilities of the SSR model are\nassessed through an application to handwritten character recognition.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 03:45:47 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 02:17:23 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""], ["Wood", "Ian A.", ""]]}, {"id": "1306.3033", "submitter": "Minh-Ngoc Tran", "authors": "Minh-Ngoc Tran, Paolo Giordani, Xiuyan Mun, Robert Kohn, Mike Pitt", "title": "Copula-type Estimators for Flexible Multivariate Density Modeling using\n  Mixtures", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas are popular as models for multivariate dependence because they allow\nthe marginal densities and the joint dependence to be modeled separately.\nHowever, they usually require that the transformation from uniform marginals to\nthe marginals of the joint dependence structure is known. This can only be done\nfor a restricted set of copulas, e.g. a normal copula. Our article introduces\ncopula-type estimators for flexible multivariate density estimation which also\nallow the marginal densities to be modeled separately from the joint\ndependence, as in copula modeling, but overcomes the lack of flexibility of\nmost popular copula estimators. An iterative scheme is proposed for estimating\ncopula-type estimators and its usefulness is demonstrated through simulation\nand real examples. The joint dependence is is modeled by mixture of normals and\nmixture of normals factor analyzers models, and mixture of t and mixture of t\nfactor analyzers models. We develop efficient Variational Bayes algorithms for\nfitting these in which model selection is performed automatically. Based on\nthese mixture models, we construct four classes of copula-type densities which\nare far more flexible than current popular copula densities, and outperform\nthem in simulation and several real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 06:31:20 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Giordani", "Paolo", ""], ["Mun", "Xiuyan", ""], ["Kohn", "Robert", ""], ["Pitt", "Mike", ""]]}, {"id": "1306.3039", "submitter": "David Aldous", "authors": "David Aldous", "title": "Another Conversation with Persi Diaconis", "comments": "Published in at http://dx.doi.org/10.1214/12-STS404 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 269-281", "doi": "10.1214/12-STS404", "report-no": "IMS-STS-STS404", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persi Diaconis was born in New York on January 31, 1945. Upon receiving a\nPh.D. from Harvard in 1974 he was appointed Assistant Professor at Stanford.\nFollowing periods as Professor at Harvard (1987-1997) and Cornell (1996-1998),\nhe has been Professor in the Departments of Mathematics and Statistics at\nStanford since 1998. He is a member of the National Academy of Sciences, a past\nPresident of the IMS and has received honorary doctorates from Chicago and four\nother universities. The following conversation took place at his office and at\nAldous's home in early 2012.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 07:01:48 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Aldous", "David", ""]]}, {"id": "1306.3092", "submitter": "Ryan Martin", "authors": "Ryan Martin and Chuanhai Liu", "title": "Marginal inferential models: prior-free probabilistic inference on\n  interest parameters", "comments": "23 pages, 2 figures", "journal-ref": "Journal of the American Statistical Association, volume 110, pages\n  1621--1631, 2015", "doi": "10.1080/01621459.2014.985827", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inferential models (IM) framework provides prior-free,\nfrequency-calibrated, posterior probabilistic inference. The key is the use of\nrandom sets to predict unobservable auxiliary variables connected to the\nobservable data and unknown parameters. When nuisance parameters are present, a\nmarginalization step can reduce the dimension of the auxiliary variable which,\nin turn, leads to more efficient inference. For regular problems, exact\nmarginalization can be achieved, and we give conditions for marginal IM\nvalidity. We show that our approach provides exact and efficient marginal\ninference in several challenging problems, including a many-normal-means\nproblem. In non-regular problems, we propose a generalized marginalization\ntechnique and prove its validity. Details are given for two benchmark examples,\nnamely, the Behrens--Fisher and gamma mean problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 12:08:17 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2013 16:05:34 GMT"}, {"version": "v3", "created": "Thu, 10 Jul 2014 18:20:59 GMT"}, {"version": "v4", "created": "Fri, 24 Oct 2014 21:32:53 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Martin", "Ryan", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1306.3171", "submitter": "Andrea Montanari", "authors": "Adel Javanmard and Andrea Montanari", "title": "Confidence Intervals and Hypothesis Testing for High-Dimensional\n  Regression", "comments": "40 pages, 4 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting high-dimensional statistical models often requires the use of\nnon-linear parameter estimation procedures. As a consequence, it is generally\nimpossible to obtain an exact characterization of the probability distribution\nof the parameter estimates. This in turn implies that it is extremely\nchallenging to quantify the \\emph{uncertainty} associated with a certain\nparameter estimate. Concretely, no commonly accepted procedure exists for\ncomputing classical measures of uncertainty and statistical significance as\nconfidence intervals or $p$-values for these models.\n  We consider here high-dimensional linear regression problem, and propose an\nefficient algorithm for constructing confidence intervals and $p$-values. The\nresulting confidence intervals have nearly optimal size. When testing for the\nnull hypothesis that a certain parameter is vanishing, our method has nearly\noptimal power.\n  Our approach is based on constructing a `de-biased' version of regularized\nM-estimators. The new construction improves over recent work in the field in\nthat it does not assume a special structure on the design matrix. We test our\nmethod on synthetic data and a high-throughput genomic data set about\nriboflavin production rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 17:19:39 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 00:29:37 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1306.3185", "submitter": "Ryan Martin", "authors": "Ryan Martin and Zhen Han", "title": "A semiparametric scale-mixture regression model and predictive recursion\n  maximum likelihood", "comments": "17 pages, 4 figures, 2 tables", "journal-ref": "Computational Statistics and Data Analysis, volume 94, pages\n  75--85, 2016", "doi": "10.1016/j.csda.2015.08.005", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To avoid specification of the error distribution in a regression model, we\npropose a general nonparametric scale mixture model for the error distribution.\nFor fitting such mixtures, the predictive recursion method is a simple and\ncomputationally efficient alternative to existing methods. We define a\npredictive recursion-based marginal likelihood function, and estimation of the\nregression parameters proceeds by maximizing this function. A hybrid predictive\nrecursion--EM algorithm is proposed for this purpose. The method's performance\nis compared with that of existing methods in simulations and real data\nanalyses.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 18:28:46 GMT"}, {"version": "v2", "created": "Sat, 24 May 2014 11:59:54 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2015 21:22:51 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Martin", "Ryan", ""], ["Han", "Zhen", ""]]}, {"id": "1306.3494", "submitter": "Jelena Bradic", "authors": "Jelena Bradic", "title": "Randomized maximum-contrast selection: subagging for large-scale\n  regression", "comments": null, "journal-ref": "Electron. J. Statist. Volume 10, Number 1 (2016), 121-170", "doi": "10.1214/15-EJS1085", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a very general method for sparse and large-scale variable\nselection. The large-scale regression settings is such that both the number of\nparameters and the number of samples are extremely large. The proposed method\nis based on careful combination of penalized estimators, each applied to a\nrandom projection of the sample space into a low-dimensional space. In one\nspecial case that we study in detail, the random projections are divided into\nnon-overlapping blocks; each consisting of only a small portion of the original\ndata. Within each block we select the projection yielding the smallest\nout-of-sample error. Our random ensemble estimator then aggregates the results\naccording to new maximal-contrast voting scheme to determine the final selected\nset. Our theoretical results illuminate the effect on performance of increasing\nthe number of non-overlapping blocks. Moreover, we demonstrate that statistical\noptimality is retained along with the computational speedup. The proposed\nmethod achieves minimax rates for approximate recovery over all estimators\nusing the full set of samples. Furthermore, our theoretical results allow the\nnumber of subsamples to grow with the subsample size and do not require\nirrepresentable condition. The estimator is also compared empirically with\nseveral other popular high-dimensional estimators via an extensive simulation\nstudy, which reveals its excellent finite-sample performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 19:39:54 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2013 07:01:09 GMT"}, {"version": "v3", "created": "Fri, 19 Sep 2014 00:43:48 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2015 15:25:51 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bradic", "Jelena", ""]]}, {"id": "1306.3541", "submitter": "Hailin Sang", "authors": "Hailin Sang, Kenneth K. Lopiano, Denise A. Abreu, Andrea C. Lamas, Pam\n  Arroway and Linda J. Young", "title": "Adjusting for Misclassification: A Three-Phase Sampling Approach", "comments": "30 pages, to appear at Journal of Official Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United States Department of Agriculture's National Agricultural\nStatistics Service (NASS) conducts the June Agricultural Survey (JAS) annually.\nSubstantial misclassification occurs during the pre-screening process and from\nfield-estimating farm status for non-response and inaccessible records,\nresulting in a biased estimate of the number of US farms from the JAS. Here the\nAnnual Land Utilization Survey (ALUS) is proposed as a follow-on survey to the\nJAS to adjust the estimates of the number of US farms and other important\nvariables. A three-phase survey design-based estimator is developed for the\nJAS-ALUS with non-response adjustment for the second phase (ALUS). A\ndesign-unbiased estimator of the variance is provided in explicit form.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 02:59:05 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 18:42:23 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2013 02:21:36 GMT"}, {"version": "v4", "created": "Mon, 9 Jun 2014 20:15:24 GMT"}, {"version": "v5", "created": "Sat, 15 Oct 2016 02:53:36 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Sang", "Hailin", ""], ["Lopiano", "Kenneth K.", ""], ["Abreu", "Denise A.", ""], ["Lamas", "Andrea C.", ""], ["Arroway", "Pam", ""], ["Young", "Linda J.", ""]]}, {"id": "1306.3636", "submitter": "Guillemette Marot", "authors": "Andrea Rau (GABI), Guillemette Marot (INRIA Lille - Nord Europe,\n  CERIM), Florence Jaffr\\'ezic (GABI)", "title": "Differential meta-analysis of RNA-seq data from multiple studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput sequencing is now regularly used for studies of the\ntranscriptome (RNA-seq), particularly for comparisons among experimental\nconditions. For the time being, a limited number of biological replicates are\ntypically considered in such experiments, leading to low detection power for\ndifferential expression. As their cost continues to decrease, it is likely that\nadditional follow-up studies will be conducted to re-address the same\nbiological question. We demonstrate how p-value combination techniques\npreviously used for microarray meta-analyses can be used for the differential\nanalysis of RNA-seq data from multiple related studies. These techniques are\ncompared to a negative binomial generalized linear model (GLM) including a\nfixed study effect on simulated data and real data on human melanoma cell\nlines. The GLM with fixed study effect performed well for low inter-study\nvariation and small numbers of studies, but was outperformed by the\nmeta-analysis methods for moderate to large inter-study variability and larger\nnumbers of studies. To conclude, the p-value combination techniques illustrated\nhere are a valuable tool to perform differential meta-analyses of RNA-seq data\nby appropriately accounting for biological and technical variability within\nstudies as well as additional study-specific effects. An R package metaRNASeq\nis available on the R Forge.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 07:45:21 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Rau", "Andrea", "", "GABI"], ["Marot", "Guillemette", "", "INRIA Lille - Nord Europe,\n  CERIM"], ["Jaffr\u00e9zic", "Florence", "", "GABI"]]}, {"id": "1306.3750", "submitter": "Alexei Stepanov", "authors": "Alexei Stepanov", "title": "The Borel-Cantelli Lemma for Markov Sequences of Events", "comments": "no comment", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we propose a new generalization of the Borel-Cantelli\nlemma. This generalization can be further used to derive strong limit results\nfor Markov chains. Illustrative applications are provided.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 06:48:36 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2014 09:18:07 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Stepanov", "Alexei", ""]]}, {"id": "1306.3768", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "\\v{S}\\'arka Hudecov\\'a and Michal Pe\\v{s}ta", "title": "Modeling Dependencies in Claims Reserving with GEE", "comments": "Submitted on April 9, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to the claims reserving problem is based on generalized\nlinear models (GLM). Within this framework, the claims in different origin and\ndevelopment years are assumed to be independent variables. If this assumption\nis violated, the classical techniques may provide incorrect predictions of the\nclaims reserves or even misleading estimates of the prediction error.\n  In this article, the application of generalized estimating equations (GEE)\nfor estimation of the claims reserves is shown. Claim triangles are handled as\npanel data, where claim amounts within the same accident year are dependent.\nSince the GEE allow to incorporate dependencies, various correlation structures\nare introduced and some practical recommendations are given.\n  Model selection criteria within the GEE reserving method are proposed.\nMoreover, an estimate for the mean square error of prediction for the claims\nreserves is derived in a nonstandard way and its advantages are discussed. Real\ndata examples are provided as an illustration of the potential benefits of the\npresented approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 08:40:34 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Hudecov\u00e1", "\u0160\u00e1rka", ""], ["Pe\u0161ta", "Michal", ""]]}, {"id": "1306.3930", "submitter": "Axel B\\\"{u}cher", "authors": "Axel B\\\"ucher, Ivan Kojadinovic", "title": "A dependent multiplier bootstrap for the sequential empirical copula\n  process under strong mixing", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ682 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 927-968", "doi": "10.3150/14-BEJ682", "report-no": "IMS-BEJ-BEJ682", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two key ingredients to carry out inference on the copula of multivariate\nobservations are the empirical copula process and an appropriate resampling\nscheme for the latter. Among the existing techniques used for i.i.d.\nobservations, the multiplier bootstrap of R\\'{e}millard and Scaillet (J.\nMultivariate Anal. 100 (2009) 377-386) frequently appears to lead to inference\nprocedures with the best finite-sample properties. B\\\"{u}cher and Ruppert (J.\nMultivariate Anal. 116 (2013) 208-229) recently proposed an extension of this\ntechnique to strictly stationary strongly mixing observations by adapting the\ndependent multiplier bootstrap of B\\\"{u}hlmann (The blockwise bootstrap in time\nseries and empirical processes (1993) ETH Z\\\"{u}rich, Section 3.3) to the\nempirical copula process. The main contribution of this work is a\ngeneralization of the multiplier resampling scheme proposed by B\\\"{u}cher and\nRuppert along two directions. First, the resampling scheme is now genuinely\nsequential, thereby allowing to transpose to the strongly mixing setting many\nof the existing multiplier tests on the unknown copula, including nonparametric\ntests for change-point detection. Second, the resampling scheme is now fully\nautomatic as a data-adaptive procedure is proposed which can be used to\nestimate the bandwidth parameter. A simulation study is used to investigate the\nfinite-sample performance of the resampling scheme and provides suggestions on\nhow to choose several additional parameters. As by-products of this work, the\nvalidity of a sequential version of the dependent multiplier bootstrap for\nempirical processes of B\\\"{u}hlmann is obtained under weaker conditions on the\nstrong mixing coefficients and the multipliers, and the weak convergence of the\nsequential empirical copula process is established under many serial dependence\nconditions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 17:05:57 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 09:36:21 GMT"}, {"version": "v3", "created": "Wed, 8 Oct 2014 18:41:48 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2016 11:13:10 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Kojadinovic", "Ivan", ""]]}, {"id": "1306.4032", "submitter": "Anne-Marie Lyne", "authors": "Anne-Marie Lyne, Mark Girolami, Yves Atchad\\'e, Heiko Strathmann,\n  Daniel Simpson", "title": "On Russian Roulette Estimates for Bayesian Inference with\n  Doubly-Intractable Likelihoods", "comments": "Published at http://dx.doi.org/10.1214/15-STS523 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 443-467", "doi": "10.1214/15-STS523", "report-no": "IMS-STS-STS523", "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of statistical models are \"doubly-intractable\": the likelihood\nnormalising term, which is a function of the model parameters, is intractable,\nas well as the marginal likelihood (model evidence). This means that standard\ninference techniques to sample from the posterior, such as Markov chain Monte\nCarlo (MCMC), cannot be used. Examples include, but are not confined to,\nmassive Gaussian Markov random fields, autologistic models and Exponential\nrandom graph models. A number of approximate schemes based on MCMC techniques,\nApproximate Bayesian computation (ABC) or analytic approximations to the\nposterior have been suggested, and these are reviewed here. Exact MCMC schemes,\nwhich can be applied to a subset of doubly-intractable distributions, have also\nbeen developed and are described in this paper. As yet, no general method\nexists which can be applied to all classes of models with doubly-intractable\nposteriors. In addition, taking inspiration from the Physics literature, we\nstudy an alternative method based on representing the intractable likelihood as\nan infinite series. Unbiased estimates of the likelihood can then be obtained\nby finite time stochastic truncation of the series via Russian Roulette\nsampling, although the estimates are not necessarily positive. Results from the\nQuantum Chromodynamics literature are exploited to allow the use of possibly\nnegative estimates in a pseudo-marginal MCMC scheme such that expectations with\nrespect to the posterior distribution are preserved. The methodology is\nreviewed on well-known examples such as the parameters in Ising models, the\nposterior for Fisher-Bingham distributions on the $d$-Sphere and a large-scale\nGaussian Markov Random Field model describing the Ozone Column data. This leads\nto a critical assessment of the strengths and weaknesses of the methodology\nwith pointers to ongoing research.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 22:04:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 11:07:23 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 18:29:05 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2015 10:47:34 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Lyne", "Anne-Marie", ""], ["Girolami", "Mark", ""], ["Atchad\u00e9", "Yves", ""], ["Strathmann", "Heiko", ""], ["Simpson", "Daniel", ""]]}, {"id": "1306.4041", "submitter": "Lizhen Lin", "authors": "Lizhen Lin and David B. Dunson", "title": "Bayesian Monotone Regression using Gaussian Process Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape constrained regression analysis has applications in dose-response\nmodeling, environmental risk assessment, disease screening and many other\nareas. Incorporating the shape constraints can improve estimation efficiency\nand avoid implausible results. We propose two novel methods focusing on\nBayesian monotone curve and surface estimation using Gaussian process\nprojections. The first projects samples from an unconstrained prior, while the\nsecond projects samples from the Gaussian process posterior. Theory is\ndeveloped on continuity of the projection, posterior consistency and rates of\ncontraction. The second approach is shown to have an empirical Bayes\njustification and to lead to simple computation with good performance in finite\nsamples. Our projection approach can be applied in other constrained function\nestimation problems including in multivariate settings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 23:31:56 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Lin", "Lizhen", ""], ["Dunson", "David B.", ""]]}, {"id": "1306.4128", "submitter": "Abdelouahab Boudjellal boudjwahab", "authors": "Aissa Ikhlef, Redha Iferroujene, Abdelouahab Boudjellal, Karim\n  Abed-Meraim, Adel Belouchrani", "title": "Constant Modulus Algorithms Using Hyperbolic Givens Rotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose two new algorithms to minimize the constant modulus (CM) criterion\nin the context of blind source separation. The first algorithm, referred to as\nGivens CMA (G-CMA) uses unitary Givens rotations and proceeds in two stages:\nprewhitening step, which reduces the channel matrix to a unitary one followed\nby a separation step where the resulting unitary matrix is computed using\nGivens rotations by minimizing the CM criterion. However, for small sample\nsizes, the prewhitening does not make the channel matrix close enough to\nunitary and hence applying Givens rotations alone does not provide satisfactory\nperformance. To remediate to this problem, we propose to use non-unitary Shear\n(Hyperbolic) rotations in conjunction with Givens rotations. This second\nalgorithm referred to as Hyperbolic G-CMA (HG-CMA) is shown to outperform the\nG-CMA as well as the Analytical CMA (ACMA) in terms of separation quality. The\nlast part of this paper is dedicated to an efficient adaptive implementation of\nthe HG-CMA and to performance assessment through numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 10:17:56 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Ikhlef", "Aissa", ""], ["Iferroujene", "Redha", ""], ["Boudjellal", "Abdelouahab", ""], ["Abed-Meraim", "Karim", ""], ["Belouchrani", "Adel", ""]]}, {"id": "1306.4231", "submitter": "Ozgur Asar", "authors": "Ozgur Asar, Ozlem Ilk", "title": "Flexible multivariate marginal models for analyzing multivariate\n  longitudinal data, with applications in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Most of the available multivariate statistical models dictate on fitting\ndifferent parameters for the covariate effects on each multiple responses. This\nmight be unnecessary and inefficient for some cases. In this article, we\npropose a modeling framework for multivariate marginal models to analyze\nmultivariate longitudinal data which provides flexible model building\nstrategies. We show that the model handles several response families such as\nbinomial, count and continuous. We illustrate the model on the Mother's Stress\nand Children's Morbidity data set. A simulation study is conducted to examine\nthe parameter estimates. An R package mmm2 is proposed to fit the model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 15:05:52 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 09:02:29 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Asar", "Ozgur", ""], ["Ilk", "Ozlem", ""]]}, {"id": "1306.4397", "submitter": "Zhigeng Geng", "authors": "Zhigeng Geng, Sijian Wang, Menggang Yu, Patrick O. Monahan, Victoria\n  Champion and Grace Wahba", "title": "Group variable selection via convex Log-Exp-Sum penalty with application\n  to a breast cancer survivor study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific and engineering applications, covariates are naturally\ngrouped. When the group structures are available among covariates, people are\nusually interested in identifying both important groups and important variables\nwithin the selected groups. Among existing successful group variable selection\nmethods, some methods fail to conduct the within group selection. Some methods\nare able to conduct both group and within group selection, but the\ncorresponding objective functions are non-convex. Such a non-convexity may\nrequire extra numerical effort. In this paper, we propose a novel\nLog-Exp-Sum(LES) penalty for group variable selection. The LES penalty is\nstrictly convex. It can identify important groups as well as select important\nvariables within the group. We develop an efficient group-level coordinate\ndescent algorithm to fit the model. We also derive non-asymptotic error bounds\nand asymptotic group selection consistency for our method in the\nhigh-dimensional setting where the number of covariates can be much larger than\nthe sample size. Numerical results demonstrate the good performance of our\nmethod in both variable selection and prediction. We applied the proposed\nmethod to an American Cancer Society breast cancer survivor dataset. The\nfindings are clinically meaningful and lead immediately to testable clinical\nhypotheses.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 00:25:02 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Geng", "Zhigeng", ""], ["Wang", "Sijian", ""], ["Yu", "Menggang", ""], ["Monahan", "Patrick O.", ""], ["Champion", "Victoria", ""], ["Wahba", "Grace", ""]]}, {"id": "1306.4438", "submitter": "Veronica Vinciotti Dr", "authors": "Yanchun Bao, Veronica Vinciotti, Ernst Wit and Peter 't Hoen", "title": "Joint modelling of ChIP-seq data via a Markov random field model", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxt047", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromatin ImmunoPrecipitation-sequencing (ChIP-seq) experiments have now\nbecome routine in biology for the detection of protein binding sites. In this\npaper, we present a Markov random field model for the joint analysis of\nmultiple ChIP-seq experiments. The proposed model naturally accounts for\nspatial dependencies in the data, by assuming first order Markov properties,\nand for the large proportion of zero counts, by using zero-inflated mixture\ndistributions. In contrast to all other available implementations, the model\nallows for the joint modelling of multiple experiments, by incorporating key\naspects of the experimental design. In particular, the model uses the\ninformation about replicates and about the different antibodies used in the\nexperiments. An extensive simulation study shows a lower false non-discovery\nrate for the proposed method, compared to existing methods, at the same false\ndiscovery rate. Finally, we present an analysis on real data for the detection\nof histone modifications of two chromatin modifiers from eight ChIP-seq\nexperiments, including technical replicates with different IP efficiencies.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 07:02:26 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Bao", "Yanchun", ""], ["Vinciotti", "Veronica", ""], ["Wit", "Ernst", ""], ["Hoen", "Peter 't", ""]]}, {"id": "1306.4509", "submitter": "Jenny H\\\"aggstr\\\"om", "authors": "Jenny H\\\"aggstr\\\"om and Xavier de Luna", "title": "Targeted smoothing parameter selection for estimating average causal\n  effects", "comments": null, "journal-ref": "Computational Statistics 29 (2014) 1727-1748", "doi": "10.1007/s00180-014-0515-0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-parametric estimation of average causal effects in observational\nstudies often relies on controlling for confounding covariates through\nsmoothing regression methods such as kernel, splines or local polynomial\nregression. Such regression methods are tuned via smoothing parameters which\nregulates the amount of degrees of freedom used in the fit. In this paper we\npropose data-driven methods for selecting smoothing parameters when the\ntargeted parameter is an average causal effect. For this purpose, we propose to\nestimate the exact expression of the mean squared error of the estimators.\nAsymptotic approximations indicate that the smoothing parameters minimizing\nthis mean squared error converges to zero faster than the optimal smoothing\nparameter for the estimation of the regression functions. In a simulation study\nwe show that the proposed data-driven methods for selecting the smoothing\nparameters yield lower empirical mean squared error than other methods\navailable such as, e.g., cross-validation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 12:04:47 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["H\u00e4ggstr\u00f6m", "Jenny", ""], ["de Luna", "Xavier", ""]]}, {"id": "1306.4529", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "Michal Pe\\v{s}ta and Ostap Okhrin", "title": "Conditional Least Squares and Copulae in Claims Reserving for a Single\n  Line of Business", "comments": "Submitted on June 19, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main goals in non-life insurance is to estimate the claims reserve\ndistribution. A generalized time series model, that allows for modeling the\nconditional mean and variance of the claim amounts, is proposed for the claims\ndevelopment. On contrary to the classical stochastic reserving techniques, the\nnumber of model parameters does not depend on the number of development\nperiods, which leads to a more precise forecasting.\n  Moreover, the time series innovations for the consecutive claims are not\nconsidered to be independent anymore. Conditional least squares are used for\nmodel parameter estimation and consistency of such estimate is proved. Copula\napproach is used for modeling the dependence structure, which improves the\nprecision of the reserve distribution estimate as well.\n  Real data examples are provided as an illustration of the potential benefits\nof the presented approach.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 13:05:45 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Pe\u0161ta", "Michal", ""], ["Okhrin", "Ostap", ""]]}, {"id": "1306.4657", "submitter": "Alice Cleynen", "authors": "Elisabeth Gassiat and Alice Cleynen and St\\'ephane Robin", "title": "Finite state space non parametric Hidden Markov Models are in general\n  identifiable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that finite state space non parametric hidden Markov\nmodels are identifiable as soon as the transition matrix of the latent Markov\nchain has full rank and the emission probability distributions are linearly\nindependent. We then propose several non parametric likelihood based estimation\nmethods, which we apply to models used in applications. We finally show on\nexamples that the use of non parametric modeling and estimation may improve the\nclassification performances.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 19:37:53 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Gassiat", "Elisabeth", ""], ["Cleynen", "Alice", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1306.4708", "submitter": "Bailey Fosdick", "authors": "Bailey K. Fosdick, Peter D. Hoff", "title": "Testing and Modeling Dependencies Between a Network and Nodal Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network analysis is often focused on characterizing the dependencies between\nnetwork relations and node-level attributes. Potential relationships are\ntypically explored by modeling the network as a function of the nodal\nattributes or by modeling the attributes as a function of the network. These\nmethods require specification of the exact nature of the association between\nthe network and attributes, reduce the network data to a small number of\nsummary statistics, and are unable provide predictions simultaneously for\nmissing attribute and network information. Existing methods that model the\nattributes and network jointly also assume the data are fully observed. In this\narticle we introduce a unified approach to analysis that addresses these\nshortcomings. We use a latent variable model to obtain a low dimensional\nrepresentation of the network in terms of node-specific network factors and use\na test of dependence between the network factors and attributes as a surrogate\nfor a test of dependence between the network and attributes. We propose a\nformal testing procedure to determine if dependencies exists between the\nnetwork factors and attributes. We also introduce a joint model for the network\nand attributes, for use if the test rejects, that can capture a variety of\ndependence patterns and be used to make inference and predictions for missing\nobservations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 22:04:50 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Fosdick", "Bailey K.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1306.4723", "submitter": "Chris Strickland Ph.D", "authors": "Chris Strickland and Robert Burdett and Robert Denham and Robert Kohn\n  and Kerrie Mengersen", "title": "A Bayesian changepoint methodology for high dimensional multivariate\n  time series and space-time data: A study of structural change using remotely\n  sensed data", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian approach is developed to analyze change points in multivariate\ntime series and space-time data. The methodology is used to assess the impact\nof extended inundation on the ecosystem of the Gulf Plains bioregion in\nnorthern Australia. The proposed approach can be implemented for dynamic\nmixture models that have a conditionally Gaussian state space representation.\nDetails are given on how to efficiently implement the algorithm for a general\nclass of multivariate time series and space-time models. This efficient\nimplementation makes it feasible to analyze high dimensional, but of realistic\nsize, space-time data sets because our approach can be appreciably faster,\npossibly millions of times, than a standard implementation in such cases.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 00:39:14 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Strickland", "Chris", ""], ["Burdett", "Robert", ""], ["Denham", "Robert", ""], ["Kohn", "Robert", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1306.4734", "submitter": "Jan Luts", "authors": "Jan Luts", "title": "Real-time semiparametric regression for distributed data sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for semiparametric regression analysis of\nlarge-scale data which are distributed over multiple hosts. This enables\nmodeling of nonlinear relationships and both the batch approach, where analysis\nstarts after all data have been collected, and the real-time setting are\naddressed. The methodology is extended to operate in evolving environments,\nwhere it can no longer be assumed that model parameters remain constant over\ntime. Two areas of application for the methodology are presented: regression\nmodeling when there are multiple data owners and regression modeling within the\nMapReduce framework. A website, realtime-semiparametric-regression.net,\nillustrates the use of the proposed method on United States domestic airline\ndata in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 21:56:45 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Luts", "Jan", ""]]}, {"id": "1306.4911", "submitter": "David Matteson", "authors": "David S. Matteson and Ruey S. Tsay", "title": "Independent Component Analysis via Distance Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel statistical framework for independent component\nanalysis (ICA) of multivariate data. We propose methodology for estimating and\ntesting the existence of mutually independent components for a given dataset,\nand a versatile resampling-based procedure for inference. Independent\ncomponents are estimated by combining a nonparametric probability integral\ntransformation with a generalized nonparametric whitening method that\nsimultaneously minimizes all forms of dependence among the components.\nU-statistics of certain Euclidean distances between sample elements are\ncombined in succession to construct a statistic for testing the existence of\nmutually independent components. The proposed measures and tests are based on\nboth necessary and sufficient conditions for mutual independence. When\nindependent components exist, one may apply univariate analysis to study or\nmodel each component separately. Univariate models may then be combined to\nobtain a multivariate model for the original observations. We prove the\nconsistency of our estimator under minimal regularity conditions without\nassuming the existence of independent components a priori, and all assumptions\nare placed on the observations directly, not on the latent components. We\ndemonstrate the improvements of the proposed method over competing methods in\nsimulation studies. We apply the proposed ICA approach to two real examples and\ncontrast it with principal component analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 15:30:58 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Matteson", "David S.", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "1306.4933", "submitter": "Nicholas James", "authors": "David S. Matteson and Nicholas A. James", "title": "A Nonparametric Approach for Multiple Change Point Analysis of\n  Multivariate Data", "comments": "The appendix has been removed. Comments: A corollary to theorem 8 has\n  been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point analysis has applications in a wide variety of fields. The\ngeneral problem concerns the inference of a change in distribution for a set of\ntime-ordered observations. Sequential detection is an online version in which\nnew data is continually arriving and is analyzed adaptively. We are concerned\nwith the related, but distinct, offline version, in which retrospective\nanalysis of an entire sequence is performed. For a set of multivariate\nobservations of arbitrary dimension, we consider nonparametric estimation of\nboth the number of change points and the positions at which they occur. We do\nnot make any assumptions regarding the nature of the change in distribution or\nany distribution assumptions beyond the existence of the alpha-th absolute\nmoment, for some alpha in (0,2). Estimation is based on hierarchical clustering\nand we propose both divisive and agglomerative algorithms. The divisive method\nis shown to provide consistent estimates of both the number and location of\nchange points under standard regularity assumptions. We compare the proposed\napproach with competing methods in a simulation study. Methods from cluster\nanalysis are applied to assess performance and to allow simple comparisons of\nlocation estimates, even when the estimated number differs. We conclude with\napplications in genetics, finance and spatio-temporal analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 16:39:40 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2013 17:27:02 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Matteson", "David S.", ""], ["James", "Nicholas A.", ""]]}, {"id": "1306.5006", "submitter": "Antonio Punzo", "authors": "Luca Bagnato and Lucio De Capitani and Antonio Punzo", "title": "Improving the autodependogram using the Kulback-Leibler divergence", "comments": "We have decided to withdraw the paper due to a crucial error in\n  equation (9), that is in the definition of the p-value. This invalidates the\n  results reported into the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autodependogram is a graphical device recently proposed in the literature\nto analyze autodependencies. It is defined computing the classical Pearson\nchi-square statistics of independence at various lags in order to point out the\npresence lag-depedencies. This paper proposes an improvement of this diagram\nobtained by substituting the chi-square statistics with an estimator of the\nKulback-Leibler divergence between the bivariate density of two delayed\nvariables and the product of their marginal distributions. A simulation study,\non well-established time series models, shows that this new autodependogram is\nmore powerful than the previous one. An application to financial data is also\nshown.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 21:43:18 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 13:48:13 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 10:33:14 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Bagnato", "Luca", ""], ["De Capitani", "Lucio", ""], ["Punzo", "Antonio", ""]]}, {"id": "1306.5202", "submitter": "Edward Baskerville", "authors": "Edward B. Baskerville, Trevor Bedford, Robert C. Reiner, and Mercedes\n  Pascual", "title": "Nonparametric Bayesian grouping methods for spatial time-series data", "comments": "11 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe an approach for identifying groups of dynamically similar\nlocations in spatial time-series data based on a simple Markov transition\nmodel. We give maximum-likelihood, empirical Bayes, and fully Bayesian\nformulations of the model, and describe exhaustive, greedy, and MCMC-based\ninference methods. The approach has been employed successfully in several\nstudies to reveal meaningful relationships between environmental patterns and\ndisease dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 17:49:27 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Baskerville", "Edward B.", ""], ["Bedford", "Trevor", ""], ["Reiner", "Robert C.", ""], ["Pascual", "Mercedes", ""]]}, {"id": "1306.5253", "submitter": "Igor' Nikiforov Ivanovich", "authors": "I.I. Nikiforov", "title": "Exclusion of measurements with excessive residuals (blunders) in\n  estimating model parameters", "comments": "2 pages, 0 figures", "journal-ref": "Astronomical & Astrophysical Transactions, 2012, Volume 27, Issue\n  3, p. 537-538", "doi": null, "report-no": null, "categories": "stat.ME astro-ph.GA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adjustable algorithm of exclusion of conditional equations with excessive\nresiduals is proposed. The criteria applied in the algorithm use variable\nexclusion limits which decrease as the number of equations goes down. The\nalgorithm is easy to use, it possesses rapid convergence, minimal subjectivity,\nand high degree of generality.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 14:23:03 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Nikiforov", "I. I.", ""]]}, {"id": "1306.5289", "submitter": "Jie Yang", "authors": "Liping Tong, Hans W. Volkmer and Jie Yang", "title": "Analytic Solutions for D-optimal Factorial Designs under Generalized\n  Linear Models", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two analytic approaches to solve D-optimal approximate designs\nunder generalized linear models. The first approach provides analytic D-optimal\nallocations for generalized linear models with two factors, which include as a\nspecial case the $2^2$ main-effects model considered by Yang, Mandal and\nMajumdar (2012). The second approach leads to explicit solutions for a class of\ngeneralized linear models with more than two factors. With the aid of the\nanalytic solutions, we provide a necessary and sufficient condition under which\na D-optimal design with two quantitative factors could be constructed on the\nboundary points only. It bridges the gap between D-optimal factorial designs\nand D-optimal designs with continuous factors.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 05:06:53 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2013 05:19:06 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 05:26:10 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Tong", "Liping", ""], ["Volkmer", "Hans W.", ""], ["Yang", "Jie", ""]]}, {"id": "1306.5362", "submitter": "Michael Mahoney", "authors": "Ping Ma and Michael W. Mahoney and Bin Yu", "title": "A Statistical Perspective on Algorithmic Leveraging", "comments": "44 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular method for dealing with large-scale data sets is sampling. For\nexample, by using the empirical statistical leverage scores as an importance\nsampling distribution, the method of algorithmic leveraging samples and\nrescales rows/columns of data matrices to reduce the data size before\nperforming computations on the subproblem. This method has been successful in\nimproving computational efficiency of algorithms for matrix problems such as\nleast-squares approximation, least absolute deviations approximation, and\nlow-rank matrix approximation. Existing work has focused on algorithmic issues\nsuch as worst-case running times and numerical issues associated with providing\nhigh-quality implementations, but none of it addresses statistical aspects of\nthis method.\n  In this paper, we provide a simple yet effective framework to evaluate the\nstatistical properties of algorithmic leveraging in the context of estimating\nparameters in a linear regression model with a fixed number of predictors. We\nshow that from the statistical perspective of bias and variance, neither\nleverage-based sampling nor uniform sampling dominates the other. This result\nis particularly striking, given the well-known result that, from the\nalgorithmic perspective of worst-case analysis, leverage-based sampling\nprovides uniformly superior worst-case algorithmic results, when compared with\nuniform sampling. Based on these theoretical results, we propose and analyze\ntwo new leveraging algorithms. A detailed empirical evaluation of existing\nleverage-based methods as well as these two new methods is carried out on both\nsynthetic and real data sets. The empirical results indicate that our theory is\na good predictor of practical performance of existing and new leverage-based\nalgorithms and that the new algorithms achieve improved performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 00:31:15 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Ma", "Ping", ""], ["Mahoney", "Michael W.", ""], ["Yu", "Bin", ""]]}, {"id": "1306.5368", "submitter": "Sanjeena Subedi", "authors": "Sanjeena Subedi and Paul D. McNicholas", "title": "A Variational Approximations-DIC Rubric for Parameter Estimation and\n  Mixture Model Selection Within a Family Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture model-based clustering has become an increasingly popular data\nanalysis technique since its introduction over fifty years ago, and is now\ncommonly utilized within a family setting. Families of mixture models arise\nwhen the component parameters, usually the component covariance (or scale)\nmatrices, are decomposed and a number of constraints are imposed. Within the\nfamily setting, model selection involves choosing the member of the family,\ni.e., the appropriate covariance structure, in addition to the number of\nmixture components. To date, the Bayesian information criterion (BIC) has\nproved most effective for model selection, and the expectation-maximization\n(EM) algorithm is usually used for parameter estimation. In fact, this EM-BIC\nrubric has virtually monopolized the literature on families of mixture models.\nDeviating from this rubric, variational Bayes approximations are developed for\nparameter estimation and the deviance information criterion for model\nselection. The variational Bayes approach provides an alternate framework for\nparameter estimation by constructing a tight lower bound on the complex\nmarginal likelihood and maximizing this lower bound by minimizing the\nassociated Kullback-Leibler divergence. This approach is taken on the most\ncommonly used family of Gaussian mixture models, and real and simulated data\nare used to compare the new approach to the EM-BIC rubric.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 02:30:16 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 21:56:32 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 20:16:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Subedi", "Sanjeena", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1306.5393", "submitter": "Nicola Lunardon", "authors": "Nicola Lunardon and Elvezio Ronchetti", "title": "Composite Likelihood Inference by Nonparametric Saddlepoint Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of composite likelihood functions provides a flexible and powerful\ntoolkit to carry out approximate inference for complex statistical models when\nthe full likelihood is either impossible to specify or unfeasible to compute.\nHowever, the strenght of the composite likelihood approach is dimmed when\nconsidering hypothesis testing about a multidimensional parameter because the\nfinite sample behavior of likelihood ratio, Wald, and score-type test\nstatistics is tied to the Godambe information matrix. Consequently inaccurate\nestimates of the Godambe information translate in inaccurate p-values. In this\npaper it is shown how accurate inference can be obtained by using a fully\nnonparametric saddlepoint test statistic derived from the composite score\nfunctions. The proposed statistic is asymptotically chi-square distributed up\nto a relative error of second order and does not depend on the Godambe\ninformation. The validity of the method is demonstrated through simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 11:31:13 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Lunardon", "Nicola", ""], ["Ronchetti", "Elvezio", ""]]}, {"id": "1306.5431", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Serigne Touba Sall and Pape Djiby Mergane", "title": "Functional weak laws for the weighted mean losses or gains and\n  applications", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show in this paper that many risk measures arising in Actuarial Sciences,\nFinance, Medicine, Welfare analysis, etc. are garthered in classes of Weighted\nMean Loss or Gain (WMLG) statistics. Some of them are Upper Threshold Based\n(UTH) or Lower Threshold Based (LTH). These statistics may be time-dependent\nwhen the scene is monitored in the time and depend on specific functions $w$\nand $d$. This paper provides time-dependent and uniformly functional weak\nasymptotic laws that allow temporal and spatial studies of the risk as well as\ncomparison between statistics in terms of dependence and mutual influence. The\nresults are particularised for usual statistics of that kind such that the\nKakwani and Shorrocks ones. Datadriven applications based on pseudo-panel data\nare provided.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 16:13:39 GMT"}, {"version": "v2", "created": "Wed, 21 May 2014 20:24:37 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Lo", "Gane Samb", ""], ["Sall", "Serigne Touba", ""], ["Mergane", "Pape Djiby", ""]]}, {"id": "1306.5462", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Adja Mbarka Fall, Cheikhna Hamallah Ndiaye and Akym\n  Adekpejou", "title": "A supermartingale argument for characterizing the Functional Hill\n  process weak law for small parameters", "comments": "25 pages. arXiv admin note: text overlap with arXiv:1208.1487,\n  Submitted under the title : On The Weak Convergence Of The Functional Hill\n  Process For Small Parameters, Ti be published in Mathematical Methods of\n  Statistics 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the asymptotic laws of functional of standard random\nvariables. These classes of statistics are closely related to estimators of the\nextreme value index when the underlying distribution function is in the Weibull\ndomain of attraction. We use techniques based on martingales theory to describe\nthe non Gaussian asymptotic distribution of the aforementioned statistics. We\nprovide results of a simulation study as well as statistical tests that may be\nof interest with the proposed results.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 19:34:42 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 20:32:04 GMT"}, {"version": "v3", "created": "Sat, 29 Oct 2016 18:24:35 GMT"}, {"version": "v4", "created": "Sat, 19 Nov 2016 13:02:31 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Lo", "Gane Samb", ""], ["Fall", "Adja Mbarka", ""], ["Ndiaye", "Cheikhna Hamallah", ""], ["Adekpejou", "Akym", ""]]}, {"id": "1306.5718", "submitter": "Luo Xiao", "authors": "Luo Xiao, David Ruppert, Vadim Zipunnikov, and Ciprian Crainiceanu", "title": "Fast Covariance Estimation for High-dimensional Functional Data", "comments": "35 pages, 4 figures", "journal-ref": "Statistics and Computing, 2016, Vol. 26, No. 1, 409-421", "doi": "10.1007/s11222-014-9485-x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For smoothing covariance functions, we propose two fast algorithms that scale\nlinearly with the number of observations per function. Most available methods\nand software cannot smooth covariance matrices of dimension $J \\times J$ with\n$J>500$; the recently introduced sandwich smoother is an exception, but it is\nnot adapted to smooth covariance matrices of large dimensions such as $J \\ge\n10,000$. Covariance matrices of order $J=10,000$, and even $J=100,000$, are\nbecoming increasingly common, e.g., in 2- and 3-dimensional medical imaging and\nhigh-density wearable sensor data. We introduce two new algorithms that can\nhandle very large covariance matrices: 1) FACE: a fast implementation of the\nsandwich smoother and 2) SVDS: a two-step procedure that first applies singular\nvalue decomposition to the data matrix and then smoothes the eigenvectors.\nCompared to existing techniques, these new algorithms are at least an order of\nmagnitude faster in high dimensions and drastically reduce memory requirements.\nThe new algorithms provide instantaneous (few seconds) smoothing for matrices\nof dimension $J=10,000$ and very fast ($<$ 10 minutes) smoothing for\n$J=100,000$. Although SVDS is simpler than FACE, we provide ready to use,\nscalable R software for FACE. When incorporated into R package {\\it refund},\nFACE improves the speed of penalized functional regression by an order of\nmagnitude, even for data of normal size ($J <500$). We recommend that FACE be\nused in practice for the analysis of noisy and high-dimensional functional\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 19:30:46 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2014 20:16:11 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Xiao", "Luo", ""], ["Ruppert", "David", ""], ["Zipunnikov", "Vadim", ""], ["Crainiceanu", "Ciprian", ""]]}, {"id": "1306.5786", "submitter": "Alexander Volfovsky", "authors": "Alexander Volfovsky and Peter D. Hoff", "title": "Testing for nodal dependence in relational data matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data are often represented as a square matrix, the entries of\nwhich record the relationships between pairs of objects. Many statistical\nmethods for the analysis of such data assume some degree of similarity or\ndependence between objects in terms of the way they relate to each other.\nHowever, formal tests for such dependence have not been developed. We provide a\ntest for such dependence using the framework of the matrix normal model, a type\nof multivariate normal distribution parameterized in terms of row- and\ncolumn-specific covariance matrices. We develop a likelihood ratio test (LRT)\nfor row and column dependence based on the observation of a single relational\ndata matrix. We obtain a reference distribution for the LRT statistic, thereby\nproviding an exact test for the presence of row or column correlations in a\nsquare relational data matrix. Additionally, we provide extensions of the test\nto accommodate common features of such data, such as undefined diagonal\nentries, a non-zero mean, multiple observations, and deviations from normality.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 21:15:39 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Volfovsky", "Alexander", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1306.5993", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Jeffrey J. Early", "title": "Frequency-Domain Stochastic Modeling of Stationary Bivariate or\n  Complex-Valued Signals", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, 2017", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three equivalent ways of representing two jointly observed\nreal-valued signals: as a bivariate vector signal, as a single complex-valued\nsignal, or as two analytic signals known as the rotary components. Each\nrepresentation has unique advantages depending on the system of interest and\nthe application goals. In this paper we provide a joint framework for all three\nrepresentations in the context of frequency-domain stochastic modeling. This\nframework allows us to extend many established statistical procedures for\nbivariate vector time series to complex-valued and rotary representations.\nThese include procedures for parametrically modeling signal coherence,\nestimating model parameters using the Whittle likelihood, performing\nsemi-parametric modeling, and choosing between classes of nested models using\nmodel choice. We also provide a new method of testing for impropriety in\ncomplex-valued signals, which tests for noncircular or anisotropic second-order\nstatistical structure when the signal is represented in the complex plane.\nFinally, we demonstrate the usefulness of our methodology in capturing the\nanisotropic structure of signals observed from fluid dynamic simulations of\nturbulence.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 15:15:06 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 06:59:25 GMT"}, {"version": "v3", "created": "Sun, 22 May 2016 00:48:38 GMT"}, {"version": "v4", "created": "Wed, 15 Mar 2017 13:04:31 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "1306.6157", "submitter": "Rajesh  Singh", "authors": "Hemant Verma and R. D. Singh and Rajesh Singh", "title": "A general class of regression type estimators in systematic sampling\n  under non-response", "comments": "9 pages, 1 table", "journal-ref": "Octogon Mat. Mag. 20(2), 542-550 (2012)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have proposed a general class of modified regression type\nestimator in systematic sampling under non-response to estimate the population\nmean using auxiliary information. The expressions of bias and mean square error\n(MSE) up to the first order approximations are derived. A numerical study is\nincluded to support the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 08:12:37 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Verma", "Hemant", ""], ["Singh", "R. D.", ""], ["Singh", "Rajesh", ""]]}, {"id": "1306.6415", "submitter": "Olivier Besson", "authors": "Olivier Besson and Yuri I. Abramovich", "title": "On the Fisher information matrix for multivariate elliptically contoured\n  distributions", "comments": "submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2013.2281914", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Slepian-Bangs formula provides a very convenient way to compute the\nFisher information matrix (FIM) for Gaussian distributed data. The aim of this\nletter is to extend it to a larger family of distributions, namely elliptically\ncontoured (EC) distributions. More precisely, we derive a closed-form\nexpression of the FIM in this case. This new expression involves the usual term\nof the Gaussian FIM plus some corrective factors that depend only on the\nexpectations of some functions of the so-called modular variate. Hence, for\nmost distributions in the EC family, derivation of the FIM from its Gaussian\ncounterpart involves slight additional derivations. We show that the new\nformula reduces to the Slepian-Bangs formula in the Gaussian case and we\nprovide an illustrative example with Student distributions on how it can be\nused.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 06:35:32 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Besson", "Olivier", ""], ["Abramovich", "Yuri I.", ""]]}, {"id": "1306.6430", "submitter": "Pier Giovanni Bissiri Dott.", "authors": "Pier Giovanni Bissiri, Chris Holmes, Stephen Walker", "title": "A General Framework for Updating Belief Distributions", "comments": "This is the pre-peer reviewed version of the article \"A General\n  Framework for Updating Belief Distributions\", which has been accepted for\n  publication in the Journal of Statistical Society - Series B. This article\n  may be used for non-commercial purposes in accordance with Wiley Terms and\n  Conditions for Self-Archiving", "journal-ref": null, "doi": "10.1111/rssb.12158", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for general Bayesian inference. We argue that a valid\nupdate of a prior belief distribution to a posterior can be made for parameters\nwhich are connected to observations through a loss function rather than the\ntraditional likelihood function, which is recovered under the special case of\nusing self information loss. Modern application areas make it is increasingly\nchallenging for Bayesians to attempt to model the true data generating\nmechanism. Moreover, when the object of interest is low dimensional, such as a\nmean or median, it is cumbersome to have to achieve this via a complete model\nfor the whole data distribution. More importantly, there are settings where the\nparameter of interest does not directly index a family of density functions and\nthus the Bayesian approach to learning about such parameters is currently\nregarded as problematic. Our proposed framework uses loss-functions to connect\ninformation in the data to functionals of interest. The updating of beliefs\nthen follows from a decision theoretic approach involving cumulative loss\nfunctions. Importantly, the procedure coincides with Bayesian updating when a\ntrue likelihood is known, yet provides coherent subjective inference in much\nmore general settings. Connections to other inference frameworks are\nhighlighted.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 08:16:41 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 14:11:48 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Bissiri", "Pier Giovanni", ""], ["Holmes", "Chris", ""], ["Walker", "Stephen", ""]]}, {"id": "1306.6658", "submitter": "Johan Segers", "authors": "Johan Segers, Ramon van den Akker, Bas J. M. Werker", "title": "Semiparametric Gaussian copula models: Geometry and efficient rank-based\n  estimation", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1244 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 5, 1911-1940", "doi": "10.1214/14-AOS1244", "report-no": "IMS-AOS-AOS1244", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose, for multivariate Gaussian copula models with unknown margins and\nstructured correlation matrices, a rank-based, semiparametrically efficient\nestimator for the Euclidean copula parameter. This estimator is defined as a\none-step update of a rank-based pilot estimator in the direction of the\nefficient influence function, which is calculated explicitly. Moreover,\nfinite-dimensional algebraic conditions are given that completely characterize\nefficiency of the pseudo-likelihood estimator and adaptivity of the model with\nrespect to the unknown marginal distributions. For correlation matrices\nstructured according to a factor model, the pseudo-likelihood estimator turns\nout to be semiparametrically efficient. On the other hand, for Toeplitz\ncorrelation matrices, the asymptotic relative efficiency of the\npseudo-likelihood estimator can be as low as 20%. These findings are confirmed\nby Monte Carlo simulations. We indicate how our results can be extended to\njoint regression models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 20:42:46 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 08:10:25 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Segers", "Johan", ""], ["Akker", "Ramon van den", ""], ["Werker", "Bas J. M.", ""]]}, {"id": "1306.6928", "submitter": "Christian P. Robert", "authors": "Diego Salmeron (CIBERESP), Juan Antonio Cano (Universidad de Murcia),\n  and C.P. Robert (Universite Paris-Dauphine)", "title": "Objective Bayesian hypothesis testing in binomial regression models with\n  integral prior distributions", "comments": "17 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we apply the methodology of integral priors to handle Bayesian\nmodel selection in binomial regression models with a general link function.\nThese models are very often used to investigate associations and risks in\nepidemiological studies where one goal is to exhibit whether or not an exposure\nis a risk factor for developing a certain disease; the purpose of the current\npaper is to test the effect of specific exposure factors. We formulate the\nproblem as a Bayesian model selection case and solve it using objective Bayes\nfactors. To construct the reference prior distributions on the regression\ncoefficients of the binomial regression models, we rely on the methodology of\nintegral priors that is nearly automatic as it only requires the specification\nof estimation reference priors and it does not depend on tuning parameters or\non hyperparameters within these priors.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 18:58:35 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Salmeron", "Diego", "", "CIBERESP"], ["Cano", "Juan Antonio", "", "Universidad de Murcia"], ["Robert", "C. P.", "", "Universite Paris-Dauphine"]]}]