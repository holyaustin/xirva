[{"id": "1611.00030", "submitter": "Chiwoo Park", "authors": "Ali Esmaieeli Sikaroudi and Chiwoo Park", "title": "A Mixture of Linear-Linear Regression Models for Linear-Circular\n  Regression", "comments": "Paper Accepted in Statistical Modelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to a linear-circular regression problem that\nrelates multiple linear predictors to a circular response. We follow a modeling\napproach of a wrapped normal distribution that describes angular variables and\nangular distributions and advances it for a linear-circular regression\nanalysis. Some previous works model a circular variable as projection of a\nbivariate Gaussian random vector on the unit square, and the statistical\ninference of the resulting model involves complicated sampling steps. The\nproposed model treats circular responses as the result of the modulo operation\non unobserved linear responses. The resulting model is a mixture of multiple\nlinear-linear regression models. We present two EM algorithms for maximum\nlikelihood estimation of the mixture model, one for a parametric model and\nanother for a non-parametric model. The estimation algorithms provide a great\ntrade-off between computation and estimation accuracy, which was numerically\nshown using five numerical examples. The proposed approach was applied to a\nproblem of estimating wind directions that typically exhibit complex patterns\nwith large variation and circularity.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 20:24:08 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 20:30:47 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Sikaroudi", "Ali Esmaieeli", ""], ["Park", "Chiwoo", ""]]}, {"id": "1611.00087", "submitter": "Yongqiang Tang", "authors": "Yongqiang Tang", "title": "Closed-form REML estimators and sample size determination for mixed\n  effects models for repeated measures under monotone missingness", "comments": "14 pages, 3 tables", "journal-ref": "Statistics in Medicine 7270, 2017", "doi": "10.1002/sim.7270", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the closed-form restricted maximum likelihood (REML) estimator and\nKenward-Roger's variance estimator for fixed effects in the mixed effects model\nfor repeated measures (MMRM) when the missing data pattern is monotone. As an\nimportant application of the analytic result, we present the formula for\ncalculating the power of treatment comparison using the Wald t test with the\nKenward-Roger adjusted variance estimate in MMRM. It allows adjustment for\nbaseline covariates without the need to specify the covariate distribution in\nrandomized trials. A simple two-step procedure is proposed to determine the\nsample size needed to achieve the targeted power. The proposed method performs\nwell for both normal and moderately nonnormal data even in small samples (n =\n20) in simulations. An anti-depressant trial is analyzed for illustrative\npurposes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 00:21:13 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 03:15:00 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Tang", "Yongqiang", ""]]}, {"id": "1611.00113", "submitter": "Xueou Wang", "authors": "David J. Nott, Xueou Wang, Michael Evans, and Berthold-Georg Englert", "title": "Checking for prior-data conflict using prior to posterior divergences", "comments": "Corrected typos in Example 3.2 and Section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using complex Bayesian models to combine information, the checking for\nconsistency of the information being combined is good statistical practice.\nHere a new method is developed for detecting prior-data conflicts in Bayesian\nmodels based on comparing the observed value of a prior to posterior divergence\nto its distribution under the prior predictive distribution for the data. The\ndivergence measure used in our model check is a measure of how much beliefs\nhave changed from prior to posterior, and can be thought of as a measure of the\noverall size of a relative belief function. It is shown that the proposed\nmethod is intuitive, has desirable properties, can be extended to hierarchical\nsettings, and is related asymptotically to Jeffreys' and reference prior\ndistributions. In the case where calculations are difficult, the use of\nvariational approximations as a way of relieving the computational burden is\nsuggested. The methods are compared in a number of examples with an alternative\nbut closely related approach in the literature based on the prior predictive\ndistribution of a minimal sufficient statistic.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 03:00:56 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 01:31:29 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 08:09:04 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Nott", "David J.", ""], ["Wang", "Xueou", ""], ["Evans", "Michael", ""], ["Englert", "Berthold-Georg", ""]]}, {"id": "1611.00203", "submitter": "Matthew A  Plumlee", "authors": "Matthew Plumlee, V. Roshan Joseph", "title": "Orthogonal Gaussian process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes models are widely adopted for\nnonparameteric/semi-parametric modeling. Identifiability issues occur when the\nmean model contains polynomials with unknown coefficients. Though resulting\nprediction is unaffected, this leads to poor estimation of the coefficients in\nthe mean model, and thus the estimated mean model loses interpretability. This\npaper introduces a new Gaussian process model whose stochastic part is\northogonal to the mean part to address this issue. This paper also discusses\napplications to multi-fidelity simulations using data examples.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 12:52:29 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Plumlee", "Matthew", ""], ["Joseph", "V. Roshan", ""]]}, {"id": "1611.00328", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, David M.\n  Blei", "title": "Variational Inference via $\\chi$-Upper Bound Minimization", "comments": "Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) is widely used as an efficient alternative to\nMarkov chain Monte Carlo. It posits a family of approximating distributions $q$\nand finds the closest member to the exact posterior $p$. Closeness is usually\nmeasured via a divergence $D(q || p)$ from $q$ to $p$. While successful, this\napproach also has problems. Notably, it typically leads to underestimation of\nthe posterior variance. In this paper we propose CHIVI, a black-box variational\ninference algorithm that minimizes $D_{\\chi}(p || q)$, the $\\chi$-divergence\nfrom $p$ to $q$. CHIVI minimizes an upper bound of the model evidence, which we\nterm the $\\chi$ upper bound (CUBO). Minimizing the CUBO leads to improved\nposterior uncertainty, and it can also be used with the classical VI lower\nbound (ELBO) to provide a sandwich estimate of the model evidence. We study\nCHIVI on three models: probit regression, Gaussian process classification, and\na Cox process model of basketball plays. When compared to expectation\npropagation and classical VI, CHIVI produces better error rates and more\naccurate estimates of posterior variance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 18:40:23 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:00:03 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 00:29:21 GMT"}, {"version": "v4", "created": "Sun, 12 Nov 2017 19:00:57 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Dieng", "Adji B.", ""], ["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Paisley", "John", ""], ["Blei", "David M.", ""]]}, {"id": "1611.00336", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing", "title": "Stochastic Variational Deep Kernel Learning", "comments": "13 pages, 6 tables, 3 figures. Appearing in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep kernel learning combines the non-parametric flexibility of kernel\nmethods with the inductive biases of deep learning architectures. We propose a\nnovel deep kernel learning model and stochastic variational inference procedure\nwhich generalizes deep kernel learning approaches to enable classification,\nmulti-task learning, additive covariance structures, and stochastic gradient\ntraining. Specifically, we apply additive base kernels to subsets of output\nfeatures from deep neural architectures, and jointly learn the parameters of\nthe base kernels and deep network through a Gaussian process marginal\nlikelihood objective. Within this framework, we derive an efficient form of\nstochastic variational inference which leverages local kernel interpolation,\ninducing points, and structure exploiting algebra. We show improved performance\nover stand alone deep networks, SVMs, and state of the art scalable Gaussian\nprocesses on several classification benchmarks, including an airline delay\ndataset containing 6 million training points, CIFAR, and ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 19:04:47 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 18:06:16 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Hu", "Zhiting", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1611.00398", "submitter": "Suhasini Subba Rao", "authors": "Suhasini Subba Rao", "title": "Orthogonal samples for estimators in time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for statistics of a stationary time series often involve nuisance\nparameters and sampling distributions that are difficult to estimate. In this\npaper, we propose the method of orthogonal samples, which can be used to\naddress some of these issues. For a broad class of statistics, an orthogonal\nsample is constructed through a slight modification of the original statistic,\nsuch that it shares similar distributional properties as the centralised\nstatistic of interest. We use the orthogonal sample to estimate nuisance\nparameters of weighted average periodogram estimators and $L_{2}$-type spectral\nstatistics. Further, the orthogonal sample is utilized to estimate the finite\nsampling distribution of various test statistics under the null hypothesis. The\nproposed method is simple and computationally fast to implement. The viability\nof the method is illustrated with various simulations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 21:22:26 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Rao", "Suhasini Subba", ""]]}, {"id": "1611.00400", "submitter": "Andrew Raim", "authors": "Kofi P. Adragni, Andrew M. Raim, Elias Al-Najjar", "title": "Minimum Average Deviance Estimation for Sufficient Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction reduces the dimensionality of data while\npreserving relevant regression information. In this article, we develop Minimum\nAverage Deviance Estimation (MADE) methodology for sufficient dimension\nreduction. It extends the Minimum Average Variance Estimation (MAVE) approach\nof Xia et al. (2002) from continuous responses to exponential family\ndistributions to include Binomial and Poisson responses. Local likelihood\nregression is used to learn the form of the regression function from the data.\nThe main parameter of interest is a dimension reduction subspace which projects\nthe covariates to a lower dimension while preserving their relationship with\nthe outcome. To estimate this parameter within its natural space, we consider\nan iterative algorithm where one step utilizes a Stiefel manifold optimizer. We\nempirically evaluate the performance of three prediction methods, two that are\nintrinsic to local likelihood estimation and one that is based on the\nNadaraya-Watson estimator. Initial results show that, as expected, MADE can\noutperform MAVE when there is a departure from the assumption of additive\nerrors.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 21:26:12 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Adragni", "Kofi P.", ""], ["Raim", "Andrew M.", ""], ["Al-Najjar", "Elias", ""]]}, {"id": "1611.00717", "submitter": "David Scollnik", "authors": "David P.M. Scollnik", "title": "A Pareto scale-inflated outlier model and its Bayesian analysis", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a Pareto scale-inflated outlier model. This model is\nintended for use when data from some standard Pareto distribution of interest\nis suspected to have been contaminated with a relatively small number of\noutliers from a Pareto distribution with the same shape parameter but with an\ninflated scale parameter. The Bayesian analysis of this Pareto scale-inflated\noutlier model is considered and its implementation using the Gibbs sampler is\ndiscussed. The paper contains three worked illustrative examples, two of which\nfeature actual insurance claims data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 18:30:49 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Scollnik", "David P. M.", ""]]}, {"id": "1611.00965", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod and Ying Zhang", "title": "Faster ARMA maximum likelihood estimation", "comments": "27 pages, 3 Figures, 3 Tables", "journal-ref": "Computational Statistics & Data Analysis, 52-4, 2166-2176 (2008)", "doi": "10.1016/j.csda.2007.07.020", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new likelihood based AR approximation is given for ARMA models. The usual\nalgorithms for the computation of the likelihood of an ARMA model require\n$O(n)$ flops per function evaluation. Using our new approximation, an algorithm\nis developed which requires only $O(1)$ flops in repeated likelihood\nevaluations. In most cases, the new algorithm gives results identical to or\nvery close to the exact maximum likelihood estimate (MLE). This algorithm is\neasily implemented in high level Quantitative Programming Environments (QPEs)\nsuch as {\\it Mathematica\\/}, MatLab and R. In order to obtain reasonable speed,\nprevious ARMA maximum likelihood algorithms are usually implemented in C or\nsome other machine efficient language. With our algorithm it is easy to do\nmaximum likelihood estimation for long time series directly in the QPE of your\nchoice. The new algorithm is extended to obtain the MLE for the mean parameter.\nSimulation experiments which illustrate the effectiveness of the new algorithm\nare discussed. {\\it Mathematica\\/} and R packages which implement the algorithm\ndiscussed in this paper are available (McLeod and Zhang, 2007). Based on these\npackage implementations, it is expected that the interested researcher would be\nable to implement this algorithm in other QPE's.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 12:06:23 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["McLeod", "A. Ian", ""], ["Zhang", "Ying", ""]]}, {"id": "1611.01046", "submitter": "Gilles Louppe", "authors": "Gilles Louppe, Michael Kagan, Kyle Cranmer", "title": "Learning to Pivot with Adversarial Networks", "comments": "v1: Original submission. v2: Fixed references. v3: version submitted\n  to NIPS'2017. Code available at\n  https://github.com/glouppe/paper-learning-to-pivot", "journal-ref": "Advances in Neural Information Processing Systems 30, pages\n  981-990, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several techniques for domain adaptation have been proposed to account for\ndifferences in the distribution of the data used for training and testing. The\nmajority of this work focuses on a binary domain label. Similar problems occur\nin a scientific context where there may be a continuous family of plausible\ndata generation processes associated to the presence of systematic\nuncertainties. Robust inference is possible if it is based on a pivot -- a\nquantity whose distribution does not depend on the unknown values of the\nnuisance parameters that parametrize this family of data generation processes.\nIn this work, we introduce and derive theoretical results for a training\nprocedure based on adversarial networks for enforcing the pivotal property (or,\nequivalently, fairness with respect to continuous attributes) on a predictive\nmodel. The method includes a hyperparameter to control the trade-off between\naccuracy and robustness. We demonstrate the effectiveness of this approach with\na toy example and examples from particle physics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 14:41:40 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 12:31:03 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 19:04:01 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Louppe", "Gilles", ""], ["Kagan", "Michael", ""], ["Cranmer", "Kyle", ""]]}, {"id": "1611.01066", "submitter": "Sandra E. Safo", "authors": "Sandra E. Safo, Jeongyoun Ahn, Yongho Jeon, and Sungkyu Jung", "title": "Sparse Generalized Eigenvalue Problem with Application to Canonical\n  Correlation Analysis for Integrative Analysis of Methylation and Gene\n  Expression Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for individual and integrative analysis of high\ndimension, low sample size data that capitalizes on the recurring theme in\nmultivariate analysis of projecting higher dimensional data onto a few\nmeaningful directions that are solutions to a generalized eigenvalue problem.\nWe propose a general framework, called SELP (Sparse Estimation with Linear\nProgramming), with which one can obtain a sparse estimate for a solution vector\nof a generalized eigenvalue problem. We demonstrate the utility of SELP on\ncanonical correlation analysis for an integrative analysis of methylation and\ngene expression profiles from a breast cancer study, and we identify some genes\nknown to be associated with breast carcinogenesis, which indicates that the\nproposed method is capable of generating biologically meaningful insights.\nSimulation studies suggest that the proposed method performs competitive in\ncomparison with some existing methods in identifying true signals in various\nunderlying covariance structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 15:34:16 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Safo", "Sandra E.", ""], ["Ahn", "Jeongyoun", ""], ["Jeon", "Yongho", ""], ["Jung", "Sungkyu", ""]]}, {"id": "1611.01129", "submitter": "Anru Zhang", "authors": "Anru Zhang", "title": "Cross: Efficient Low-rank Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The completion of tensors, or high-order arrays, attracts significant\nattention in recent research. Current literature on tensor completion primarily\nfocuses on recovery from a set of uniformly randomly measured entries, and the\nrequired number of measurements to achieve recovery is not guaranteed to be\noptimal. In addition, the implementation of some previous methods is NP-hard.\nIn this article, we propose a framework for low-rank tensor completion via a\nnovel tensor measurement scheme we name Cross. The proposed procedure is\nefficient and easy to implement. In particular, we show that a third order\ntensor of Tucker rank-$(r_1, r_2, r_3)$ in $p_1$-by-$p_2$-by-$p_3$ dimensional\nspace can be recovered from as few as $r_1r_2r_3 + r_1(p_1-r_1) + r_2(p_2-r_2)\n+ r_3(p_3-r_3)$ noiseless measurements, which matches the sample complexity\nlower-bound. In the case of noisy measurements, we also develop a theoretical\nupper bound and the matching minimax lower bound for recovery error over\ncertain classes of low-rank tensors for the proposed procedure. The results can\nbe further extended to fourth or higher-order tensors. Simulation studies show\nthat the method performs well under a variety of settings. Finally, the\nprocedure is illustrated through a real dataset in neuroimaging.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:02:02 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 18:08:38 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhang", "Anru", ""]]}, {"id": "1611.01205", "submitter": "Xuan Cao", "authors": "Xuan Cao, Kshitij Khare and Malay Ghosh", "title": "Posterior Graph Selection and Estimation Consistency for\n  High-dimensional Bayesian DAG Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance estimation and selection for high-dimensional multivariate\ndatasets is a fundamental problem in modern statistics. Gaussian directed\nacyclic graph (DAG) models are a popular class of models used for this purpose.\nGaussian DAG models introduce sparsity in the Cholesky factor of the inverse\ncovariance matrix, and the sparsity pattern in turn corresponds to specific\nconditional independence assumptions on the underlying variables. A variety of\npriors have been developed in recent years for Bayesian inference in DAG\nmodels, yet crucial convergence and sparsity selection properties for these\nmodels have not been thoroughly investigated. Most of these priors are\nadaptations or generalizations of the Wishart distribution in the DAG context.\nIn this paper, we consider a flexible and general class of these 'DAG-Wishart'\npriors with multiple shape parameters. Under mild regularity assumptions, we\nestablish strong graph selection consistency and establish posterior\nconvergence rates for estimation when the number of variables p is allowed to\ngrow at an appropriate sub-exponential rate with the sample size n.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 21:58:30 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 16:14:18 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Cao", "Xuan", ""], ["Khare", "Kshitij", ""], ["Ghosh", "Malay", ""]]}, {"id": "1611.01238", "submitter": "Ting Yan", "authors": "Jianwei Hu, Hong Qin, Ting Yan, Yunpeng Zhao", "title": "Corrected Bayesian information criterion for stochastic block models", "comments": "42 pages, major revision. Change the title to \"Corrected Bayesian\n  information criterion for stochastic block models\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of communities is one of the fundamental problems in\ncommunity detection. We re-examine the Bayesian paradigm for stochastic block\nmodels and propose a \"corrected Bayesian information criterion\",to determine\nthe number of communities and show that the proposed estimator is consistent\nunder mild conditions. The proposed criterion improves those used in Wang and\nBickel (2016) and Saldana et al. (2017) which tend to underestimate and\noverestimate the number of communities, respectively. Along the way, we\nestablish the Wilks theorem for stochastic block models. Moreover, we show\nthat, to obtain the consistency of model selection for stochastic block models,\nwe need a so-called \"consistency condition\". We also provide sufficient\nconditions for both homogenous networks and non-homogenous networks. The\nresults are further extended to degree corrected stochastic block models.\nNumerical studies demonstrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 01:40:17 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 02:40:56 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 10:04:51 GMT"}, {"version": "v4", "created": "Fri, 19 May 2017 08:34:11 GMT"}, {"version": "v5", "created": "Tue, 17 Sep 2019 07:18:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Hu", "Jianwei", ""], ["Qin", "Hong", ""], ["Yan", "Ting", ""], ["Zhao", "Yunpeng", ""]]}, {"id": "1611.01241", "submitter": "Meng Li", "authors": "Meng Li and David B. Dunson", "title": "Comparing and weighting imperfect models using D-probabilities", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2019.1611140", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for assigning weights to models using a\ndivergence-based method ({\\em D-probabilities}), relying on evaluating\nparametric models relative to a nonparametric Bayesian reference using\nKullback-Leibler divergence. D-probabilities are useful in goodness-of-fit\nassessments, in comparing imperfect models, and in providing model weights to\nbe used in model aggregation. D-probabilities avoid some of the disadvantages\nof Bayesian model probabilities, such as large sensitivity to prior choice, and\ntend to place higher weight on a greater diversity of models. In an application\nto linear model selection against a Gaussian process reference, we provide\nsimple analytic forms for routine implementation and show that D-probabilities\nautomatically penalize model complexity. Some asymptotic properties are\ndescribed, and we provide interesting probabilistic interpretations of the\nproposed model weights. The framework is illustrated through simulation\nexamples and an ozone data application.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 01:50:57 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 18:32:35 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 14:54:45 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Meng", ""], ["Dunson", "David B.", ""]]}, {"id": "1611.01310", "submitter": "Angela Bitto", "authors": "Angela Bitto, Sylvia Fr\\\"uhwirth-Schnatter", "title": "Achieving Shrinkage in a Time-Varying Parameter Model Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage for time-varying parameter (TVP) models is investigated within a\nBayesian framework, with the aim to automatically reduce time-varying\nparameters to static ones, if the model is overfitting. This is achieved\nthrough placing the double gamma shrinkage prior on the process variances. An\nefficient Markov chain Monte Carlo scheme is developed, exploiting boosting\nbased on the ancillarity-sufficiency interweaving strategy. The method is\napplicable both to TVP models for univariate as well as multivariate time\nseries. Applications include a TVP generalized Phillips curve for EU area\ninflation modelling and a multivariate TVP Cholesky stochastic volatility model\nfor joint modelling of the returns from the DAX-30 index.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 10:21:55 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 13:48:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Bitto", "Angela", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "1611.01480", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic", "title": "Why comparing survival curves between two subgroups may be misleading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse an issue when comparing survival curves between two subgroups. We\nshow that there is a direct relationship between estimates of subgroups'\nsurvival at a time point and positive and negative predictive values in the\nbinary classification settings. Our findings present a case where current\nmethods of comparing survival curves between subgroups may be misleading. We\nthink that this ought to be taken into account during the validation of\nprognostic diagnostic tests that predict two prognostic subgroups for a given\ndisease or treatment, when the validation data set consists of censored data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:26:00 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 14:56:08 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Krstajic", "Damjan", ""]]}, {"id": "1611.01485", "submitter": "Meike K\\\"ohler", "authors": "Meike K\\\"ohler, Nikolaus Umlauf, Andreas Beyerlein, Christiane\n  Winkler, Anette-Gabriele Ziegler and Sonja Greven", "title": "Flexible Bayesian additive joint models with an application to type 1\n  diabetes research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint modeling of longitudinal and time-to-event data is an important\ntool of growing popularity to gain insights into the association between a\nbiomarker and an event process. We develop a general framework of flexible\nadditive joint models that allows the specification of a variety of effects,\nsuch as smooth nonlinear, time-varying and random effects, in the longitudinal\nand survival parts of the models. Our extensions are motivated by the\ninvestigation of the relationship between fluctuating disease-specific markers,\nin this case autoantibodies, and the progression to the autoimmune disease type\n1 diabetes. By making use of Bayesian P-splines we are in particular able to\ncapture highly nonlinear subject-specific marker trajectories as well as a\ntime-varying association between the marker and the event process allowing new\ninsights into disease progression. The model is estimated within a Bayesian\nframework and implemented in the R-package bamlss.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:40:01 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 11:11:53 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["K\u00f6hler", "Meike", ""], ["Umlauf", "Nikolaus", ""], ["Beyerlein", "Andreas", ""], ["Winkler", "Christiane", ""], ["Ziegler", "Anette-Gabriele", ""], ["Greven", "Sonja", ""]]}, {"id": "1611.01564", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod", "title": "Improved spread-location visualization", "comments": "19 pages, 2 figures, 1 table", "journal-ref": "Journal of Graphical and Computational Statistics 8/1, 135-141\n  (1999)", "doi": "10.1080/10618600.1999.10474806", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread-location plot has often been used as a diagnostic plot suitable\nfor many types of fitted statistical models. The spread-location plot which\nplots the absolute residual or square-root absolute residual versus fitted\nvalue along with a robust loess smooth is a useful replacement for the\ncustomary practice of plotting residuals versus fitted values. In this note, we\nshow that neither absolute residual or square-root absolute residual is always\nappropriate for error distributions likely to be encountered in actual\napplications. Hence we recommend a multipanel display showing a suitable\ntransformation of the absolute residual versus fitted value along with a\nboxplot to judge the symmetry achieved by the transformation. We conclude with\nan illustrative example.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 23:04:52 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["McLeod", "A. Ian", ""]]}, {"id": "1611.01602", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen, Jeremy F. P. Ullmann, Geoffrey J. McLachlan,\n  Venkatakaushik Voleti, Wenze Li, Elizabeth M. C. Hillman, David C. Reutens,\n  Andrew L. Janke", "title": "Whole-Volume Clustering of Time Series Data from Zebrafish Brain Calcium\n  Images via Mixture Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium is a ubiquitous messenger in neural signaling events. An increasing\nnumber of techniques are enabling visualization of neurological activity in\nanimal models via luminescent proteins that bind to calcium ions. These\ntechniques generate large volumes of spatially correlated time series. A\nmodel-based functional data analysis methodology via Gaussian mixtures is\nsuggested for the clustering of data from such visualizations is proposed. The\nmethodology is theoretically justified and a computationally efficient approach\nto estimation is suggested. An example analysis of a zebrafish imaging\nexperiment is presented.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 04:44:58 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 11:40:00 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Ullmann", "Jeremy F. P.", ""], ["McLachlan", "Geoffrey J.", ""], ["Voleti", "Venkatakaushik", ""], ["Li", "Wenze", ""], ["Hillman", "Elizabeth M. C.", ""], ["Reutens", "David C.", ""], ["Janke", "Andrew L.", ""]]}, {"id": "1611.01675", "submitter": "Georg Hahn", "authors": "Dong Ding, Axel Gandy, Georg Hahn", "title": "A simple method for implementing Monte Carlo tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a statistical test whose p-value can only be approximated using\nMonte Carlo simulations. We are interested in deciding whether the p-value for\nan observed data set lies above or below a given threshold such as 5%. We want\nto ensure that the resampling risk, the probability of the (Monte Carlo)\ndecision being different from the true decision, is uniformly bounded. This\narticle introduces a simple open-ended method with this property, the\nconfidence sequence method (CSM). We compare our approach to another algorithm,\nSIMCTEST, which also guarantees an (asymptotic) uniform bound on the resampling\nrisk, as well as to other Monte Carlo procedures without a uniform bound. CSM\nis free of tuning parameters and conservative. It has the same theoretical\nguarantee as SIMCTEST and, in many settings, similar stopping boundaries. As it\nis much simpler than other methods, CSM is a useful method for practical\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 17:04:56 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 21:53:37 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 01:17:44 GMT"}, {"version": "v4", "created": "Fri, 20 Oct 2017 00:29:21 GMT"}, {"version": "v5", "created": "Fri, 18 May 2018 23:36:28 GMT"}, {"version": "v6", "created": "Wed, 9 Oct 2019 15:33:40 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Ding", "Dong", ""], ["Gandy", "Axel", ""], ["Hahn", "Georg", ""]]}, {"id": "1611.01743", "submitter": "Dreamlee Sharma", "authors": "Dreamlee Sharma", "title": "The Quantile Based Flattened Logistic Distribution: Some properties and\n  Application", "comments": "This paper has been withdrawn by the author due to a crucial error in\n  section 7.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the quantile based flattened logistic distribution introduced\nby Gilchrist has been studied. Some classical and quantile based properties of\nthe distribution have been obtained. Closed form expression of L-moments and\nL-ratios of the distribution have been obtained. A quantile based analysis\nbased on the methods of matching L-moments estimation is employed to estimate\nthe parameters of the proposed model. We further derive the asymptotic\nvariance-covariance matrix of the L-Moments estimator of the proposed model.\nFinally, we apply the proposed model to a real data set and perform some\ngoodness of fit tests.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 08:26:56 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 09:09:04 GMT"}, {"version": "v3", "created": "Fri, 16 Dec 2016 07:29:41 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Sharma", "Dreamlee", ""]]}, {"id": "1611.02199", "submitter": "Alessio Sancetta", "authors": "Alessio Sancetta", "title": "Inference for Additive Models in the Presence of Possibly Infinite\n  Dimensional Nuisance Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for estimation and hypothesis testing of functional restrictions\nagainst general alternatives is proposed. The parameter space is a reproducing\nkernel Hilbert space (RKHS). The null hypothesis does not necessarily define a\nparametric model. The test allows us to deal with infinite dimensional nuisance\nparameters. The methodology is based on a moment equation similar in spirit to\nthe construction of the efficient score in semiparametric statistics. The\nfeasible version of such moment equation requires to consistently estimate\nprojections in the space of RKHS and it is shown that this is possible using\nthe proposed approach. This allows us to derive some tractable asymptotic\ntheory and critical values by fast simulation. Simulation results show that the\nfinite sample performance of the test is consistent with the asymptotics and\nthat ignoring the effect of nuisance parameters highly distorts the size of the\ntests.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 18:14:52 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 09:30:48 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 10:25:21 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Sancetta", "Alessio", ""]]}, {"id": "1611.02241", "submitter": "Patricia Alonso Ruiz", "authors": "Patricia Alonso Ruiz and Evgeny Spodarev", "title": "Entropy-based inhomogeneity detection in porous media", "comments": "18 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a change-point problem for random fields based on a univariate\ndetection of outliers via the $3\\sigma$-rule in order to recognize\ninhomogeneities in porous media. In particular, we focus on fibre reinforced\npolymers modeled by stochastic fibre processes with high fibre intensity and\nsearch for abrupt changes in the direction of the fibres. As a measure of\nchange, the entropy of the directional distribution is locally estimated within\na window that scans the region to be analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:47:25 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Ruiz", "Patricia Alonso", ""], ["Spodarev", "Evgeny", ""]]}, {"id": "1611.02314", "submitter": "Ying Liu", "authors": "Ying Liu, Yuanjia Wang, Michael R. Kosorok, Yingqi Zhao, Donglin Zeng", "title": "Robust Hybrid Learning for Estimating Personalized Dynamic Treatment\n  Regimens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic treatment regimens (DTRs) are sequential decision rules tailored at\neach stage by potentially time-varying patient features and intermediate\noutcomes observed in previous stages. The complexity, patient heterogeneity and\nchronicity of many diseases and disorders call for learning optimal DTRs which\nbest dynamically tailor treatment to each individual's response over time.\nProliferation of personalized data (e.g., genetic and imaging data) provides\nopportunities for deep tailoring as well as new challenges for statistical\nmethodology. In this work, we propose a robust hybrid approach referred as\nAugmented Multistage Outcome-Weighted Learning (AMOL) to integrate\noutcome-weighted learning and Q-learning to identify optimal DTRs from the\nSequential Multiple Assignment Randomization Trials (SMARTs). We generalize\noutcome weighted learning (O-learning; Zhao et al.~2012) to allow for negative\noutcomes; we propose methods to reduce variability of weights in O-learning to\nachieve numeric stability and higher efficiency; finally, for multiple-stage\nSMART studies, we introduce doubly robust augmentation to machine learning\nbased O-learning to improve efficiency by drawing information from regression\nmodel-based Q-learning at each stage. The proposed AMOL remains valid even if\nthe Q-learning model is misspecified. We establish the theoretical properties\nof AMOL, including the consistency of the estimated rules and the rates of\nconvergence to the optimal value function. The comparative advantage of AMOL\nover existing methods is demonstrated in extensive simulation studies and\napplications to two SMART data sets: a two-stage trial for attention deficit\nand hyperactive disorder (ADHD) and the STAR*D trial for major depressive\ndisorder (MDD).\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 21:43:30 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Liu", "Ying", ""], ["Wang", "Yuanjia", ""], ["Kosorok", "Michael R.", ""], ["Zhao", "Yingqi", ""], ["Zeng", "Donglin", ""]]}, {"id": "1611.02367", "submitter": "Chris Oates", "authors": "N. Friel, J. P. McKeone, C. J. Oates, A. N. Pettitt", "title": "Discussion of: \"A Bayesian information criterion for singular models\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contributed discussion to the paper of Drton and Plummer (2017), presented\nbefore the Royal Statistical Society on 5th October 2016.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 02:27:46 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Friel", "N.", ""], ["McKeone", "J. P.", ""], ["Oates", "C. J.", ""], ["Pettitt", "A. N.", ""]]}, {"id": "1611.02433", "submitter": "Daniel Graham", "authors": "Cian Naik and Emma J. McCoy and Daniel J. Graham", "title": "Multiply robust dose-response estimation for multivalued causal\n  inference problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a multiply robust (MR) dose-response estimator for causal\ninference problems involving multivalued treatments. We combine a family of\ngeneralised propensity score (GPS) models and a family of outcome regression\n(OR) models to achieve an average potential outcomes estimator that is\nconsistent if just one of the GPS or OR models in each family is correctly\nspecified. We provide proofs and simulations that demonstrate multiple\nrobustness in the context of multivalued causal inference problems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 08:51:52 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 12:59:27 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Naik", "Cian", ""], ["McCoy", "Emma J.", ""], ["Graham", "Daniel J.", ""]]}, {"id": "1611.02480", "submitter": "Nilabja Guha", "authors": "Nilabja Guha, Veera Baladandayuthapani and Bani K. Mallick", "title": "Quantile Graphical Models: Bayesian Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are ubiquitous tools to describe the interdependence between\nvariables measured simultaneously such as large-scale gene or protein\nexpression data. Gaussian graphical models (GGMs) are well-established tools\nfor probabilistic exploration of dependence structures using precision matrices\nand they are generated under a multivariate normal joint distribution. However,\nthey suffer from several shortcomings since they are based on Gaussian\ndistribution assumptions. In this article, we propose a Bayesian quantile based\napproach for sparse estimation of graphs. We demonstrate that the resulting\ngraph estimation is robust to outliers and applicable under general\ndistributional assumptions. Furthermore, we develop efficient variational Bayes\napproximations to scale the methods for large data sets. Our methods are\napplied to a novel cancer proteomics data dataset wherein multiple proteomic\nantibodies are simultaneously assessed on tumor samples using reverse-phase\nprotein arrays (RPPA) technology.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 11:22:06 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 23:22:56 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 14:49:13 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Guha", "Nilabja", ""], ["Baladandayuthapani", "Veera", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1611.02583", "submitter": "Nirian Mart\\'in", "authors": "Elena Castilla, Nirian Martin, Leandro Pardo", "title": "A logistic regression analysis approach for sample survey data based on\n  phi-divergence measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new family of minimum distance estimators for binary logistic regression\nmodels based on $\\phi$-divergence measures is introduced. The so called \"pseudo\nminimum phi-divergence estimator\"(PM$\\phi$E) family is presented as an\nextension of \"minimum phi-divergence estimator\" (M$\\phi$E) for general sample\nsurvey designs and contains, as a particular case, the pseudo maximum\nlikelihood estimator (PMLE) considered in Roberts et al. \\cite{r}. Through a\nsimulation study it is shown that some PM$\\phi$Es have a better behaviour, in\nterms of efficiency, than the PMLE.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 16:09:57 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Castilla", "Elena", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1611.02609", "submitter": "Feipeng Zhang", "authors": "Feipeng Zhang and Qunhua Li", "title": "A Continuous Threshold Expectile Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectile regression is a useful tool for exploring the relation between the\nresponse and the explanatory variables beyond the conditional mean. This\narticle develops a continuous threshold expectile regression for modeling data\nin which the effect of a covariate on the response variable is linear but\nvaries below and above an unknown threshold in a continuous way. Based on a\ngrid search approach, we obtain estimators for the threshold and the regression\ncoefficients via an asymmetric least squares regression method. We derive the\nasymptotic properties for all the estimators and show that the estimator for\nthe threshold achieves root-n consistency. We also develop a weighted CUSUM\ntype test statistic for the existence of a threshold in a given expectile, and\nderive its asymptotic properties under both the null and the local alternative\nmodels. This test only requires fitting the model under the null hypothesis in\nthe absence of a threshold, thus it is computationally more efficient than the\nlikelihood-ratio type tests. Simulation studies show desirable finite sample\nperformance in both homoscedastic and heteroscedastic cases. The application of\nour methods on a Dutch growth data and a baseball pitcher salary data reveals\ninteresting insights.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 17:05:28 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Zhang", "Feipeng", ""], ["Li", "Qunhua", ""]]}, {"id": "1611.02690", "submitter": "Aur\\'elien Nicosia", "authors": "Aur\\'elien Nicosia, Thierry Duchesne, Louis-Paul Rivest and Daniel\n  Fortin", "title": "A Multi-State Conditional Logistic Regression Model for the Analysis of\n  Animal Movement", "comments": null, "journal-ref": "The Annals of Applied Statistics, 2017, 11 (3), 1537-1560", "doi": "10.1214/17-AOAS1045", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-state version of an animal movement analysis method based on\nconditional logistic regression, called Step Selection Function (SSF), is\nproposed. In ecology SSF is developed from a comparison between the observed\nlocation of an animal and randomly sampled locations at each time step.\nInterpretation of the parameters in the multi-state model and the impact of\ndifferent sampling schemes for the random locations are discussed. We prove the\nequivalence between the new model and a random walk model on the plane. This\nequivalence allows one to use both pure movement and local discrete choice\nbehaviors in identifying the model's hidden states. The new method is used to\nmodel the movement behavior of GPS-collared bison in Prince Albert National\nPark, Canada. The multi-state SSF successfully teases apart areas used to\nforage and to travel. The analysis thus provides valuable insights into how\nbison adjust their movement to habitat features, thereby revealing spatial\ndeterminants of functional connectivity in heterogeneous landscapes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 20:59:29 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Nicosia", "Aur\u00e9lien", ""], ["Duchesne", "Thierry", ""], ["Rivest", "Louis-Paul", ""], ["Fortin", "Daniel", ""]]}, {"id": "1611.02762", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Generalized Cluster Trees and Singular Measures", "comments": "51 pages, 6 figures; accepted to the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the $\\alpha$-cluster tree ($\\alpha$-tree) under both\nsingular and nonsingular measures. The $\\alpha$-tree uses probability contents\nwithin a level set to construct a cluster tree so that it is well-defined for\nsingular measures. We first derive the convergence rate for a density level set\naround critical points, which leads to the convergence rate for estimating an\n$\\alpha$-tree under nonsingular measures. For singular measures, we study how\nthe kernel density estimator (KDE) behaves and prove that the KDE is not\nuniformly consistent but pointwisely consistent after rescaling. We further\nprove that the estimated $\\alpha$-tree fails to converge in the $L_\\infty$\nmetric but is still consistent under the integrated distance. We also observe a\nnew type of critical points--the dimensional critical points (DCPs)--of a\nsingular measure. DCPs occur only at singular measures, and similar to the\nusual critical points, DCPs contribute to cluster tree topology as well.\nBuilding on the analysis of the KDE and DCPs, we prove the topological\nconsistency of an estimated $\\alpha$-tree.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 23:08:02 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 18:55:50 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 18:11:18 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "1611.02802", "submitter": "Yichen Qin", "authors": "Yichen Qin, Yang Li, Wei Ma, Feifang Hu", "title": "Pairwise Sequential Randomization and Its Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In comparative studies, such as in causal inference and clinical trials,\nbalancing important covariates is often one of the most important concerns for\nboth efficient and credible comparison. However, chance imbalance still exists\nin many randomized experiments. This phenomenon of covariate imbalance becomes\nmuch more serious as the number of covariates $p$ increases. To address this\nissue, we introduce a new randomization procedure, called pairwise sequential\nrandomization (PSR). The proposed method allocates the units sequentially and\nadaptively, using information on the current level of imbalance and the\nincoming unit's covariate. With a large number of covariates or a large number\nof units, the proposed method shows substantial advantages over the traditional\nmethods in terms of the covariate balance, estimation accuracy, and\ncomputational time, making it an ideal technique in the era of big data. The\nproposed method attains the optimal covariate balance, in the sense that the\nestimated treatment effect under the proposed method attains its minimum\nvariance asymptotically. Also the proposed method is widely applicable in both\ncausal inference and clinical trials. Numerical studies and real data analysis\nprovide further evidence of the advantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 02:34:24 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 03:55:21 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Qin", "Yichen", ""], ["Li", "Yang", ""], ["Ma", "Wei", ""], ["Hu", "Feifang", ""]]}, {"id": "1611.02845", "submitter": "Serena Arima", "authors": "Serena Arima, Silvia Polettini", "title": "A unit-level small area model with misclassified covariates", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area models are mixed effects regression models that link the small\nareas and borrow strength from similar domains. When the auxiliary variables\nused in the models are measured with error, small area estimators that ignore\nthe measurement error may be worse than direct estimators. Alternative small\narea estimators accounting for measurement error have been proposed in the\nliterature but only for continuous auxiliary variables. Adopting a Bayesian\napproach, we extend the unit-level model in order to account for measurement\nerror in both continuous and categorical covariates. For the discrete variables\nwe model the misclassification probabilities and estimate them jointly with all\nthe unknown model parameters. We test our model through a simulation study\nexploring different scenarios. The impact of the proposed model is emphasized\nthrough application to data from the Ethiopia Demographic and Health Survey\nwhere we focus on the women's malnutrition issue, a dramatic problem in\ndeveloping countries and an important indicator of the socio-economic progress\nof a country.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 08:28:13 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 08:59:33 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Arima", "Serena", ""], ["Polettini", "Silvia", ""]]}, {"id": "1611.02978", "submitter": "Mehmet A. S\\\"uzen PhD", "authors": "Mehmet S\\\"uzen and Abed Ajraou", "title": "Evaluating Gaussian processes for sparse irregular spatio-temporal data", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical approach to evaluate performance of a Gaussian process regression\nmodels (GPR) for irregularly sampled sparse time-series is introduced. The\napproach entails construction of a secondary autoregressive model using the\nfine scale predictions to forecast a future observation used in GPR. We build\ndifferent GPR models for Ornstein-Uhlenbeck and Fractional processes for\nsimulated toy data with different sparsity levels to assess the utility of the\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 12:48:34 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["S\u00fczen", "Mehmet", ""], ["Ajraou", "Abed", ""]]}, {"id": "1611.03015", "submitter": "Andrii Babii", "authors": "Andrii Babii", "title": "Honest Confidence Sets in Nonparametric IV Regression and Other\n  Ill-Posed Models", "comments": null, "journal-ref": "Econometric Theory , 36(4), 2020, pp. 658-706", "doi": "10.1017/S0266466619000380", "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops inferential methods for a very general class of ill-posed\nmodels in econometrics encompassing the nonparametric instrumental variable\nregression, various functional regressions, and the density deconvolution. We\nfocus on uniform confidence sets for the parameter of interest estimated with\nTikhonov regularization, as in Darolles, Fan, Florens, and Renault (2011).\nSince it is impossible to have inferential methods based on the central limit\ntheorem, we develop two alternative approaches relying on the concentration\ninequality and bootstrap approximations. We show that expected diameters and\ncoverage properties of resulting sets have uniform validity over a large class\nof models, i.e., constructed confidence sets are honest. Monte Carlo\nexperiments illustrate that introduced confidence sets have reasonable width\nand coverage properties. Using U.S. data, we provide uniform confidence sets\nfor Engel curves for various commodities.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 17:10:31 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 22:50:56 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2019 03:28:41 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2020 19:59:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Babii", "Andrii", ""]]}, {"id": "1611.03146", "submitter": "Wenge Guo", "authors": "Gavin Lynch, Wenge Guo, Sanat K. Sarkar, Helmut Finner", "title": "The Control of the False Discovery Rate in Fixed Sequence Multiple\n  Testing", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling the false discovery rate (FDR) is a powerful approach to multiple\ntesting. In many applications, the tested hypotheses have an inherent\nhierarchical structure. In this paper, we focus on the fixed sequence structure\nwhere the testing order of the hypotheses has been strictly specified in\nadvance. We are motivated to study such a structure, since it is the most basic\nof hierarchical structures, yet it is often seen in real applications such as\nstatistical process control and streaming data analysis. We first consider a\nconventional fixed sequence method that stops testing once an acceptance\noccurs, and develop such a method controlling the FDR under both arbitrary and\nnegative dependencies. The method under arbitrary dependency is shown to be\nunimprovable without losing control of the FDR and unlike existing FDR methods;\nit cannot be improved even by restricting to the usual positive regression\ndependence on subset (PRDS) condition. To account for any potential mistakes in\nthe ordering of the tests, we extend the conventional fixed sequence method to\none that allows more but a given number of acceptances. Simulation studies show\nthat the proposed procedures can be powerful alternatives to existing FDR\ncontrolling procedures. The proposed procedures are illustrated through a real\ndata set from a microarray experiment.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 01:17:23 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Lynch", "Gavin", ""], ["Guo", "Wenge", ""], ["Sarkar", "Sanat K.", ""], ["Finner", "Helmut", ""]]}, {"id": "1611.03155", "submitter": "Wenge Guo", "authors": "Wenge Guo, Sanat Sarkar", "title": "Adaptive Controls of FWER and FDR Under Block Dependence", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in multiple testing, the hypotheses appear in non-overlapping blocks\nwith the associated $p$-values exhibiting dependence within but not between\nblocks. We consider adapting the Benjamini-Hochberg method for controlling the\nfalse discovery rate (FDR) and the Bonferroni method for controlling the\nfamilywise error rate (FWER) to such dependence structure without losing their\nultimate controls over the FDR and FWER, respectively, in a non-asymptotic\nsetting. We present variants of conventional adaptive Benjamini-Hochberg and\nBonferroni methods with proofs of their respective controls over the FDR and\nFWER. Numerical evidence is presented to show that these new adaptive methods\ncan capture the present dependence structure more effectively than the\ncorresponding conventional adaptive methods. This paper offers a solution to\nthe open problem of constructing adaptive FDR and FWER controlling methods\nunder dependence in a non-asymptotic setting and providing real improvements\nover the corresponding non-adaptive ones.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 01:38:39 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Guo", "Wenge", ""], ["Sarkar", "Sanat", ""]]}, {"id": "1611.03309", "submitter": "Roberto Di Mari Roberto Di Mari", "authors": "Roberto Di Mari, Roberto Rocci, Stefano Antonio Gattone", "title": "Estimation of clusterwise linear regression models with a shrinkage-like\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained approaches to maximum likelihood estimation in the context of\nfinite mixtures of normals have been presented in the literature. A fully\ndata-dependent constrained method for maximum likelihood estimation of\nclusterwise linear regression is proposed, which extends previous work in\nequivariant data-driven estimation of finite mixtures of Gaussians for\nclassification. The method imposes plausible bounds on the component variances,\nbased on a target value estimated from the data, which we take to be the\nhomoscedastic variance. Nevertheless, the present work does not only focus on\nclassification recovery, but also on how well model parameters are estimated.\nIn particular, the paper sheds light on the shrinkage-like interpretation of\nthe procedure, where the target is the homoscedastic model: this is not only\nrelated to how close to the target the estimated scales are, but extends to the\nestimated clusterwise linear regressions and classification. We show, based on\nsimulation and real-data based results, that our approach yields a final model\nbeing the most appropriate-to-the-data compromise between the heteroscedastic\nmodel and the homoscedastic model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 14:20:52 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Di Mari", "Roberto", ""], ["Rocci", "Roberto", ""], ["Gattone", "Stefano Antonio", ""]]}, {"id": "1611.03361", "submitter": "Carlos Martinez Mr.", "authors": "Carlos Alberto Mart\\'inez, Kshitij Khare, Syed Rahman, Mauricio A.\n  Elzo", "title": "Modelling correlated marker effects in genome-wide prediction via\n  Gaussian concentration graph models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide prediction, independence of marker allele substitution effects\nis typically assumed; however, since early stages of this technology it has\nbeen known that nature points to correlated effects. In statistics, graphical\nmodels have been identified as a useful and powerful tool for covariance\nestimation in high dimensional problems and it is an area that has recently\nexperienced a great expansion. In particular, Gaussian concentration graph\nmodels (GCGM) have been widely studied. These are models in which the\ndistribution of a set of random variables, the marker effects in this case, is\nassumed to be Markov with respect to an undirected graph G. In this paper,\nBayesian (Bayes G and Bayes G-D) and frequentist (GML-BLUP) methods adapting\nthe theory of GCGM to genome-wide prediction were developed. Different\napproaches to define the graph G based on domain-specific knowledge were\nproposed, and two propositions and a corollary establishing conditions to find\ndecomposable graphs were proven. These methods were implemented in small\nsimulated and real datasets. In our simulations, scenarios where correlations\namong allelic substitution effects were expected to arise due to various causes\nwere considered, and graphs were defined on the basis of physical marker\npositions. Results showed improvements in correlation between phenotypes and\npredicted breeding values and accuracies of predicted breeding values when\naccounting for partially correlated allele substitution effects. Extensions to\nthe multiallelic loci case were described and some possible refinements\nincorporating more flexible priors in the Bayesian setting were discussed. Our\nmodels are promising because they allow incorporation of biological information\nin the prediction process, and because they are more flexible and general than\nother models accounting for correlated marker effects that have been proposed\npreviously.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 15:50:53 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 21:13:07 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 15:35:09 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Mart\u00ednez", "Carlos Alberto", ""], ["Khare", "Kshitij", ""], ["Rahman", "Syed", ""], ["Elzo", "Mauricio A.", ""]]}, {"id": "1611.03439", "submitter": "Wenge Guo", "authors": "Zhiying Qiu, Wenge Guo, Sanat Sarkar", "title": "Bonferroni-based gatekeeping procedure with retesting option", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex clinical trials, multiple research objectives are often grouped\ninto sets of objectives based on their inherent hierarchical relationships.\nConsequently, the hypotheses formulated to address these objectives are grouped\ninto ordered families of hypotheses and thus to be tested in a pre-defined\nsequence. In this paper, we introduce a novel Bonferroni based multiple testing\nprocedure for testing hierarchically ordered families of hypotheses. The\nproposed procedure allows the families to be sequentially tested more than once\nwith updated local critical values. It is proved to control the global\nfamilywise error rate strongly under arbitrary dependence. Implementation of\nthe procedure is illustrated using two examples. Finally, the procedure is\nextended to testing multiple families of hypotheses with a complex two-layer\nhierarchical structure.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 18:31:03 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Qiu", "Zhiying", ""], ["Guo", "Wenge", ""], ["Sarkar", "Sanat", ""]]}, {"id": "1611.03456", "submitter": "Amy Willis", "authors": "Amy D. Willis and Rayna C. Bell", "title": "Uncertainty in phylogenetic tree estimates", "comments": "Final version accepted to Journal of Computational and Graphical\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating phylogenetic trees is an important problem in evolutionary\nbiology, environmental policy and medicine. Although trees are estimated, their\nuncertainties are discarded by mathematicians working in tree space. Here we\nexplicitly model the multivariate uncertainty of tree estimates. We consider\nboth the cases where uncertainty information arises extrinsically (through\ncovariate information) and intrinsically (through the tree estimates\nthemselves). The importance of accounting for tree uncertainty in tree space is\ndemonstrated in two case studies. In the first instance, differences between\ngene trees are small relative to their uncertainties, while in the second, the\ndifferences are relatively large. Our main goal is visualization of tree\nuncertainty, and we demonstrate advantages of our method with respect to\nreproducibility, speed and preservation of topological differences compared to\nvisualization based on multidimensional scaling. The proposal highlights that\nphylogenetic trees are estimated in an extremely high-dimensional space,\nresulting in uncertainty information that cannot be discarded. Most\nimportantly, it is a method that allows biologists to diagnose whether\ndifferences between gene trees are biologically meaningful, or due to\nuncertainty in estimation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 19:33:45 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 01:13:22 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Willis", "Amy D.", ""], ["Bell", "Rayna C.", ""]]}, {"id": "1611.03545", "submitter": "Thai Pham", "authors": "Thai Pham and Weixin Chen", "title": "The Instrumental Variable Method for Estimating Local Average Treatment\n  Regime Effects", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the instrumental variable regime (IVR) method to estimate the\ncausal effects of multiple sequential treatments. This method serves to address\nthe problem of endogenous selections of sequential treatments. An IVR is a\nsequence of instrumental variables in which each IV instruments for an\nendogenous treatment variable. Our proposed method generalizes the LATE model\nin Imbens and Angrist (1994) from a single treatment to many treatments applied\nsequentially. More precisely, with the IVR this model allows for estimating the\nlocal average treatment regime effects (LATRE), possibly conditional on a set\nof initial covariates. Though there exist studies in this area that use IVR,\nall of them require a structural functional form assumption. Our method is\nnovel in that we do not require any such assumption. Thus unlike previous\napproaches, ours is robust to model misspecifications, which usually occur in\ntreatment regime settings. The ideas and estimators in this paper are motivated\nand illustrated through a contextual example showing the use of IVR in\nestimating the treatment regime effect of advertisements on purchasing\nbehaviors when advertisements are displayed in multiple periods. We demonstrate\nthe performance of the proposed method with simulations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 23:59:14 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 22:39:14 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 05:12:07 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Pham", "Thai", ""], ["Chen", "Weixin", ""]]}, {"id": "1611.03665", "submitter": "Fabian Telschow J. E.", "authors": "Fabian J.E. Telschow and Stephan F. Huckemann and Michael R.\n  Pierrynowski", "title": "Functional Inference on Rotational Curves and Identification of Human\n  Gait at the Knee Joint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Gaussian perturbation models in classical functional data analysis\nto the three-dimensional rotational group where a zero-mean Gaussian process in\nthe Lie algebra under the Lie exponential spreads multiplicatively around a\ncentral curve. As an estimator, we introduce point-wise extrinsic mean curves\nwhich feature strong perturbation consistency, and which are asymptotically\na.s. unique and differentiable, if the model is so. Further, we consider the\ngroup action of time warping and that of spatial isometries that are connected\nto the identity. The latter can be asymptotically consistently estimated if\nlifted to the unit quaternions. Introducing a generic loss for Lie groups, the\nformer can be estimated, and based on curve length, due to asymptotic\ndifferentiability, we propose two-sample permutation tests involving various\ncombinations of the group actions. This methodology allows inference on gait\npatterns due to the rotational motion of the lower leg with respect to the\nupper leg. This was previously not possible because, among others, the usual\nanalysis of separate Euler angles is not independent of marker placement, even\nif performed by trained specialists.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 11:43:48 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Telschow", "Fabian J. E.", ""], ["Huckemann", "Stephan F.", ""], ["Pierrynowski", "Michael R.", ""]]}, {"id": "1611.03832", "submitter": "Budhi Arta Surya", "authors": "B.A. Surya", "title": "Generalized Phase-Type Distribution and Competing Risks for Markov\n  Mixtures Process", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase-type distribution has been an important probabilistic tool in the\nanalysis of complex stochastic system evolution. It was introduced by Neuts\n\\cite{Neuts1975} in 1975. The model describes the lifetime distribution of a\nfinite-state absorbing Markov chains, and has found many applications in wide\nrange of areas. It was brought to survival analysis by Aalen \\cite{Aalen1995}\nin 1995. However, the model has lacks of ability in modeling heterogeneity and\ninclusion of past information which is due to the Markov property of the\nunderlying process that forms the model. We attempt to generalize the\ndistribution by replacing the underlying by Markov mixtures process. Markov\nmixtures process was used to model jobs mobility by Blumen \\cite{Blumen} et al.\nin 1955. It was known as the mover-stayer model describing low-productivity\nworkers tendency to move out of their jobs by a Markov chains, while those with\nhigh-productivity tend to stay in the job. Frydman \\cite{Frydman2005} later\nextended the model to a mixtures of finite-state Markov chains moving at\ndifferent speeds on the same state space. In general the mixtures process does\nnot have Markov property. We revisit the mixtures model \\cite{Frydman2005} for\nmixtures of multi absorbing states Markov chains, and propose generalization of\nthe phase-type distribution under competing risks. The new distribution has two\nmain appealing features: it has the ability to model heterogeneity and to\ninclude past information of the underlying process, and it comes in a closed\nform. Built upon the new distribution, we propose conditional forward intensity\nwhich can be used to determine rate of occurrence of future events (caused by\ncertain type) based on available information. Numerical study suggests that the\nnew distribution and its forward intensity offer significant improvements over\nthe existing model.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 19:47:58 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Surya", "B. A.", ""]]}, {"id": "1611.03835", "submitter": "Matthias Katzfuss", "authors": "Jonathan R. Stroud, Matthias Katzfuss, and Christopher K. Wikle", "title": "A Bayesian adaptive ensemble Kalman filter for sequential state and\n  parameter estimation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new methodology for sequential state and parameter\nestimation within the ensemble Kalman filter. The method is fully Bayesian and\npropagates the joint posterior density of states and parameters over time. In\norder to implement the method we consider two representations of the marginal\nposterior distribution of the parameters: a grid-based approach and a Gaussian\napproximation. Contrary to existing algorithms, the new method explicitly\naccounts for parameter uncertainty and provides a formal way to combine\ninformation about the parameters from data at different time periods. The\nmethod is illustrated and compared to existing approaches using simulated and\nreal data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 19:59:26 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Stroud", "Jonathan R.", ""], ["Katzfuss", "Matthias", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1611.04158", "submitter": "Daniel Vogel", "authors": "Carina Gerstenberger, Daniel Vogel, Martin Wendler", "title": "Tests for scale changes based on pairwise differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications it is important to know whether the amount of\nfluctuation in a series of observations changes over time. In this article, we\ninvestigate different tests for detecting change in the scale of\nmean-stationary time series. The classical approach based on the CUSUM test\napplied to the squared centered, is very vulnerable to outliers and impractical\nfor heavy-tailed data, which leads us to contemplate test statistics based on\nalternative, less outlier-sensitive scale estimators.\n  It turns out that the tests based on Gini's mean difference (the average of\nall pairwise distances) or generalized Qn estimators (sample quantiles of all\npairwise distances) are very suitable candidates. They improve upon the\nclassical test not only under heavy tails or in the presence of outliers, but\nalso under normality. An explanation for this at first counterintuitive result\nis that the corresponding long-run variance estimates are less affected by a\nscale change than in the case of the sample-variance-based test.\n  We use recent results on the process convergence of U-statistics and\nU-quantiles for dependent sequences to derive the limiting distribution of the\ntest statistics and propose estimators for the long-run variance. We perform a\nsimulations study to investigate the finite sample behavior of the test and\ntheir power. Furthermore, we demonstrate the applicability of the new\nchange-point detection methods at two real-life data examples from hydrology\nand finance.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 17:08:26 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Gerstenberger", "Carina", ""], ["Vogel", "Daniel", ""], ["Wendler", "Martin", ""]]}, {"id": "1611.04333", "submitter": "Ravi Venkatesan", "authors": "R. C. Venkatesan and A. Plastino", "title": "Fisher Information Framework for Time Series Modeling", "comments": "30 pages, 8 figures. Minor typographical and syntactical changes\n  made. Eqs. (57)-(61) in Version 1 specialized to the case of \\alpha=1 to be\n  consistent with Eq. (56). Fig.'s 3, 5, 7, and 8 enlarged to enable better\n  visual coherence. Missing labels added to Fig.'s 3 and 4", "journal-ref": null, "doi": "10.1016/j.physa.2017.02.076", "report-no": null, "categories": "stat.ME cs.IT math.IT nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust prediction model invoking the Takens embedding theorem, whose\n\\textit{working hypothesis} is obtained via an inference procedure based on the\nminimum Fisher information principle, is presented. The coefficients of the\nansatz, central to the \\textit{working hypothesis} satisfy a time independent\nSchr\\\"{o}dinger-like equation in a vector setting. The inference of i) the\nprobability density function of the coefficients of the \\textit{working\nhypothesis} and ii) the establishing of constraint driven pseudo-inverse\ncondition for the modeling phase of the prediction scheme, is made, for the\ncase of normal distributions, with the aid of the quantum mechanical virial\ntheorem. The well-known reciprocity relations and the associated Legendre\ntransform structure for the Fisher information measure (FIM, hereafter)-based\nmodel in a vector setting (with least square constraints) are self-consistently\nderived. These relations are demonstrated to yield an intriguing form of the\nFIM for the modeling phase, which defines the \\textit{working hypothesis},\nsolely in terms of the observed data. Cases for prediction employing time\nseries' obtained from the: $(i)$ the Mackey-Glass delay-differential equation,\n$(ii)$ one ECG sample from the MIT-Beth Israel Deaconess Hospital (MIT-BIH)\ncardiac arrhythmia database, and $(iii)$ one ECG from the Creighton University\nventricular tachyarrhythmia database. The ECG samples were obtained from the\nPhysionet online repository. These examples demonstrate the efficiency of the\nprediction model. Numerical examples for exemplary cases are provided.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 10:55:24 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 20:35:47 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Venkatesan", "R. C.", ""], ["Plastino", "A.", ""]]}, {"id": "1611.04460", "submitter": "Tobias Kley", "authors": "Tobias Kley, Philip Preu{\\ss}, Piotr Fryzlewicz", "title": "Predictive, finite-sample model choice for time series under\n  stationarity and non-stationarity", "comments": "paper (42 pages, 9 figures, 7 tables), appendix (22 pages, 4\n  figures), and supplementary material (82 pages, 15 figures, 61 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical research there usually exists a choice between structurally\nsimpler or more complex models. We argue that, even if a more complex, locally\nstationary time series model were true, then a simple, stationary time series\nmodel may be advantageous to work with under parameter uncertainty. We present\na new model choice methodology, where one of two competing approaches is chosen\nbased on its empirical, finite-sample performance with respect to prediction,\nin a manner that ensures interpretability. A rigorous, theoretical analysis of\nthe procedure is provided. As an important side result we prove, for possibly\ndiverging model order, that the localised Yule-Walker estimator is strongly,\nuniformly consistent under local stationarity. An R package, forecastSNSTS, is\nprovided and used to apply the methodology to financial and meteorological data\nin empirical examples. We further provide an extensive simulation study and\ndiscuss when it is preferable to base forecasts on the more volatile\ntime-varying estimates and when it is advantageous to forecast as if the data\nwere from a stationary process, even though they might not be.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 16:54:27 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 19:46:42 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 11:11:18 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Kley", "Tobias", ""], ["Preu\u00df", "Philip", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1611.04473", "submitter": "Jean Morrison", "authors": "Jean Morrison, Noah Simon, and Daniela Witten", "title": "Simultaneous detection and estimation of trait associations with genomic\n  phenotypes", "comments": "In press in Biostatistics (2016)", "journal-ref": null, "doi": "10.1093/biostatistics/kxw033", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic phenotypes, such as DNA methylation and chromatin accessibility, can\nbe used to characterize the transcriptional and regulatory activity of DNA\nwithin a cell. Recent technological advances have made it possible to measure\nsuch phenotypes very densely. This density often results in spatial structure,\nin the sense that measurements at nearby sites are very similar.\n  In this paper, we consider the task of comparing genomic phenotypes across\nexperimental conditions, cell types, or disease subgroups. We propose a new\nmethod, Joint Adaptive Differential Estimation (JADE), which leverages the\nspatial structure inherent to genomic phenotypes. JADE simultaneously estimates\nsmooth underlying group average genomic phenotype profiles, and detects regions\nin which the average profile differs between groups. We evaluate JADE's\nperformance in several biologically plausible simulation settings. We also\nconsider an application to the detection of regions with differential\nmethylation between mature skeletal muscle cells, myotubes and myoblasts.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 17:03:45 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Morrison", "Jean", ""], ["Simon", "Noah", ""], ["Witten", "Daniela", ""]]}, {"id": "1611.04488", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De,\n  Aaditya Ramdas, Alex Smola, Arthur Gretton", "title": "Generative Models and Model Criticism via Optimized Maximum Mean\n  Discrepancy", "comments": "Published at ICLR 2017 (public comments:\n  http://openreview.net/forum?id=HJWHIKqgl )", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to optimize the representation and distinguishability of\nsamples from two probability distributions, by maximizing the estimated power\nof a statistical test based on the maximum mean discrepancy (MMD). This\noptimized MMD is applied to the setting of unsupervised learning by generative\nadversarial networks (GAN), in which a model attempts to generate realistic\nsamples, and a discriminator attempts to tell these apart from data samples. In\nthis context, the MMD may be used in two roles: first, as a discriminator,\neither directly on the samples, or on features of the samples. Second, the MMD\ncan be used to evaluate the performance of a generative model, by testing the\nmodel's samples against a reference data set. In the latter role, the optimized\nMMD is particularly helpful, as it gives an interpretable indication of how the\nmodel and data distributions differ, even in cases where individual model\nsamples are not easily distinguished either by eye or by classifier.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 17:28:27 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 13:07:50 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 20:30:37 GMT"}, {"version": "v4", "created": "Fri, 10 Feb 2017 18:28:49 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 19:54:37 GMT"}, {"version": "v6", "created": "Thu, 14 Jan 2021 06:14:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Tung", "Hsiao-Yu", ""], ["Strathmann", "Heiko", ""], ["De", "Soumyajit", ""], ["Ramdas", "Aaditya", ""], ["Smola", "Alex", ""], ["Gretton", "Arthur", ""]]}, {"id": "1611.04537", "submitter": "Frank Werner", "authors": "Katharina Proksch, Frank Werner, Axel Munk", "title": "Multiscale scanning in inverse problems", "comments": "55 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a multiscale scanning method to determine active\ncomponents of a quantity $f$ w.r.t. a dictionary $\\mathcal{U}$ from\nobservations $Y$ in an inverse regression model $Y=Tf+\\xi$ with linear operator\n$T$ and general random error $\\xi$. To this end, we provide uniform confidence\nstatements for the coefficients $\\langle \\varphi, f\\rangle$, $\\varphi \\in\n\\mathcal U$, under the assumption that $(T^*)^{-1} \\left(\\mathcal U\\right)$ is\nof wavelet-type. Based on this we obtain a multiple test that allows to\nidentify the active components of $\\mathcal{U}$, i.e. $\\left\\langle f,\n\\varphi\\right\\rangle \\neq 0$, $\\varphi \\in \\mathcal U$, at controlled,\nfamily-wise error rate. Our results rely on a Gaussian approximation of the\nunderlying multiscale statistic with a novel scale penalty adapted to the\nill-posedness of the problem. The scale penalty furthermore ensures weak\nconvergence of the statistic's distribution towards a Gumbel limit under\nreasonable assumptions. The important special cases of tomography and\ndeconvolution are discussed in detail. Further, the regression case, when $T =\n\\text{id}$ and the dictionary consists of moving windows of various sizes\n(scales), is included, generalizing previous results for this setting. We show\nthat our method obeys an oracle optimality, i.e. it attains the same asymptotic\npower as a single-scale testing procedure at the correct scale. Simulations\nsupport our theory and we illustrate the potential of the method as an\ninferential tool for imaging. As a particular application we discuss\nsuper-resolution microscopy and analyze experimental STED data to locate single\nDNA origami.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:26:22 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 15:03:32 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Proksch", "Katharina", ""], ["Werner", "Frank", ""], ["Munk", "Axel", ""]]}, {"id": "1611.04538", "submitter": "Li Ma", "authors": "Li Ma", "title": "Recursive partitioning and multi-scale modeling on conditional densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonparametric prior on the conditional distribution of a\n(univariate or multivariate) response given a set of predictors. The prior is\nconstructed in the form of a two-stage generative procedure, which in the first\nstage recursively partitions the predictor space, and then in the second stage\ngenerates the conditional distribution by a multi-scale nonparametric density\nmodel on each predictor partition block generated in the first stage. This\ndesign allows adaptive smoothing on both the predictor space and the response\nspace, and it results in the full posterior conjugacy of the model, allowing\nexact Bayesian inference to be completed analytically through a\nforward-backward recursive algorithm without the need of MCMC, and thus\nenjoying high computational efficiency (scaling linearly with the sample size).\nWe show that this prior enjoys desirable theoretical properties such as full\n$L_1$ support and posterior consistency. We illustrate how to apply the model\nto a variety of inference problems such as conditional density estimation as\nwell as hypothesis testing and model selection in a manner similar to applying\na parametric conjugate prior, while attaining full nonparametricity. Also\nprovided is a comparison to two other state-of-the-art Bayesian nonparametric\nmodels for conditional densities in both model fit and computational time. A\nreal data example from flow cytometry containing 455,472 observations is given\nto illustrate the substantial computational efficiency of our method and its\napplication to multivariate problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:28:25 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 23:08:54 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 18:38:41 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Ma", "Li", ""]]}, {"id": "1611.04619", "submitter": "Yishi Wang Yishi Wang", "authors": "Yishi Wang, Ann E. Stapleton, Cuixian Chen", "title": "Two Sample Order Free Trend Inference with an Application in Plant\n  Physiology", "comments": "13 pages, i figrue", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by a biological experiment with a split-plot design,\nfor the purpose of comparison of the changing patterns in seed weight from two\ntreatment groups as subgroups in each of the two groups subject to increasing\nlevels of stress. We formalize the question into a nonparametric two sample\ncomparison problem for changes among the sub samples, which was analyzed using\nU-statistics. Zero inflated value were also considered in the construction of\nthe U-statistics. The U-statistics were then used in a Chi-square type test\nstatistics framework for hypothesis testing. Bootstrapped p-values were\nobtained through simulated samples. It was proven that the distribution of the\nsimulated sample can be independent provided the observed samples have certain\nsummary statistics. Simulation results suggest that the test is consistent.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 21:06:42 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Wang", "Yishi", ""], ["Stapleton", "Ann E.", ""], ["Chen", "Cuixian", ""]]}, {"id": "1611.04638", "submitter": "Peibei Shi", "authors": "Peibei Shi and Annie Qu", "title": "Weak Signal Identification and Inference in Penalized Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak signal identification and inference are very important in the area of\npenalized model selection, yet they are under-developed and not well-studied.\nExisting inference procedures for penalized estimators are mainly focused on\nstrong signals. In this paper, we propose an identification procedure for weak\nsignals in finite samples, and pro- vide a transition phase in-between noise\nand strong signal strengths. We also introduce a new two-step inferential\nmethod to construct better confidence intervals for the identified weak\nsignals. Our theory development assumes that variables are orthogonally\ndesigned. Both theory and numerical studies indicate that the proposed method\nleads to better confidence coverage for weak signals, compared with those using\nasymptotic inference. In addition, the proposed method out- performs the\nperturbation and bootstrap resampling approaches. We illustrate our method for\nHIV antiretroviral drug susceptibility data to identify genetic mutations\nassociated with HIV drug resistance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 22:41:39 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Shi", "Peibei", ""], ["Qu", "Annie", ""]]}, {"id": "1611.04702", "submitter": "Jiangjiang Zhang", "authors": "Jiangjiang Zhang, Guang Lin, Weixuan Li, Laosheng Wu, Lingzao Zeng", "title": "An iterative local updating ensemble smoother for estimation and\n  uncertainty assessment of hydrologic model parameters with multimodal\n  distributions", "comments": null, "journal-ref": null, "doi": "10.1002/2017WR020906", "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble smoother (ES) has been widely used in inverse modeling of hydrologic\nsystems. However, for problems where the distribution of model parameters is\nmultimodal, using ES directly would be problematic. One popular solution is to\nuse a clustering algorithm to identify each mode and update the clusters with\nES separately. However, this strategy may not be very efficient when the\ndimension of parameter space is high or the number of modes is large.\nAlternatively, we propose in this paper a very simple and efficient algorithm,\ni.e., the iterative local updating ensemble smoother (ILUES), to explore\nmultimodal distributions of model parameters in nonlinear hydrologic systems.\nThe ILUES algorithm works by updating local ensembles of each sample with ES to\nexplore possible multimodal distributions. To achieve satisfactory data matches\nin nonlinear problems, we adopt an iterative form of ES to assimilate the\nmeasurements multiple times. Numerical cases involving nonlinearity and\nmultimodality are tested to illustrate the performance of the proposed method.\nIt is shown that overall the ILUES algorithm can well quantify the parametric\nuncertainties of complex hydrologic models, no matter whether the multimodal\ndistribution exists.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 04:20:19 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 04:32:18 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 21:26:05 GMT"}, {"version": "v4", "created": "Thu, 15 Dec 2016 02:14:58 GMT"}, {"version": "v5", "created": "Tue, 31 Jan 2017 08:01:14 GMT"}, {"version": "v6", "created": "Sun, 25 Feb 2018 08:33:23 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhang", "Jiangjiang", ""], ["Lin", "Guang", ""], ["Li", "Weixuan", ""], ["Wu", "Laosheng", ""], ["Zeng", "Lingzao", ""]]}, {"id": "1611.04808", "submitter": "Ottmar Cronie", "authors": "Adina Iftimi, Ottmar Cronie, Francisco Montes", "title": "The second-order analysis of marked spatio-temporal point processes,\n  with an application to earthquake data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyse interaction in marked spatio-temporal point processes (MSTPPs), we\nintroduce marked (cross) second-order reduced moment measures and K-functions\nfor general inhomogeneous second-order intensity reweighted stationary MSTPPs.\nThese summary statistics, which allow us to quantify dependence between\ndifferent mark categories of the points, are depending on the specific mark\nspace and mark reference measure chosen. We also look closer at how the summary\nstatistics reduce under assumptions such as the MSTPP being multivariate and/or\nstationary. A new test for independent marking is devised and unbiased\nminus-sampling estimators are derived for all statistics considered. In\naddition, we treat Voronoi intensity estimators for MSTPPs and indicate their\nunbiasedness. These new statistics are finally employed to analyse the\nwell-known Andaman sea earthquake dataset. We find that clustering takes place\nbetween main and fore-/aftershocks at virtually all space and time scales. In\naddition, we find evidence that, conditionally on the space-time locations of\nthe earthquakes, the magnitudes do not behave like an iid sequence.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 12:49:44 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Iftimi", "Adina", ""], ["Cronie", "Ottmar", ""], ["Montes", "Francisco", ""]]}, {"id": "1611.04908", "submitter": "Klaus Nordhausen", "authors": "Klaus Nordhausen, Hannu Oja and David E. Tyler", "title": "Asymptotic and bootstrap tests for subspace dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most linear dimension reduction methods proposed in the literature can be\nformulated using an appropriate pair of scatter matrices, see e.g. Ye and Weiss\n(2003), Tyler et al. (2009), Bura and Yang (2011), Liski et al. (2014) and Luo\nand Li (2016). The eigen-decomposition of one scatter matrix with respect to\nanother is then often used to determine the dimension of the signal subspace\nand to separate signal and noise parts of the data. Three popular dimension\nreduction methods, namely principal component analysis (PCA), fourth order\nblind identification (FOBI) and sliced inverse regression (SIR) are considered\nin detail and the first two moments of subsets of the eigenvalues are used to\ntest for the dimension of the signal space. The limiting null distributions of\nthe test statistics are discussed and novel bootstrap strategies are suggested\nfor the small sample cases. In all three cases, consistent test-based estimates\nof the signal subspace dimension are introduced as well. The asymptotic and\nbootstrap tests are compared in simulations and illustrated in real data\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 16:06:08 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 20:12:49 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""], ["Tyler", "David E.", ""]]}, {"id": "1611.04913", "submitter": "Huang Huang", "authors": "Huang Huang and Ying Sun", "title": "Total Variation Depth for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been extensive work on data depth-based methods for robust\nmultivariate data analysis. Recent developments have moved to\ninfinite-dimensional objects such as functional data. In this work, we propose\na new notion of depth, the total variation depth, for functional data. As a\nmeasure of depth, its properties are studied theoretically, and the associated\noutlier detection performance is investigated through simulations. Compared to\nmagnitude outliers, shape outliers are often masked among the rest of samples\nand harder to identify. We show that the proposed total variation depth has\nmany desirable features and is well suited for outlier detection. In\nparticular, we propose to decompose the total variation depth into two\ncomponents that are associated with shape and magnitude outlyingness,\nrespectively. This decomposition allows us to develop an effective procedure\nfor outlier detection and useful visualization tools, while naturally\naccounting for the correlation in functional data. Finally, the proposed\nmethodology is demonstrated using real datasets of curves, images, and video\nframes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 16:12:13 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Huang", "Huang", ""], ["Sun", "Ying", ""]]}, {"id": "1611.05201", "submitter": "Konstantin Eckle", "authors": "Konstantin Eckle, Nicolai Bissantz, Holger Dette", "title": "Multiscale inference for multivariate deconvolution", "comments": "Keywords and Phrases: deconvolution, modes, multivariate density,\n  multiple tests, Gaussian approximation AMS Subject Classification: 62G07,\n  62G10, 62G20", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide new methodology for inference of the geometric\nfeatures of a multivariate density in deconvolution. Our approach is based on\nmultiscale tests to detect significant directional derivatives of the unknown\ndensity at arbitrary points in arbitrary directions. The multiscale method is\nused to identify regions of monotonicity and to construct a general procedure\nfor the detection of modes of the multivariate density. Moreover, as an\nimportant application a significance test for the presence of a local maximum\nat a pre-specified point is proposed. The performance of the new methods is\ninvestigated from a theoretical point of view and the finite sample properties\nare illustrated by means of a small simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 10:10:47 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Eckle", "Konstantin", ""], ["Bissantz", "Nicolai", ""], ["Dette", "Holger", ""]]}, {"id": "1611.05224", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "A New Family of Divergences Originating from Model Adequacy Tests and\n  Application to Robust Statistical Inference", "comments": "17 pages, ato appear in IEEE transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2018.2794537", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum divergence methods are popular tools in a variety of statistical\napplications. We consider tubular model adequacy tests, and demonstrate that\nthe new divergences that are generated in the process are very useful in robust\nstatistical inference. In particular we show that the family of $S$-divergences\ncan be alternatively developed using the tubular model adequacy tests; a\nfurther application of the paradigm generates a larger superfamily of\ndivergences. We describe the properties of this larger class and its potential\napplications in robust inference. Along the way, the failure of the first order\ninfluence function analysis in capturing the robustness of these procedures is\nalso established.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 11:19:39 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2017 08:58:34 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 14:14:05 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1611.05344", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "On composite likelihood in bivariate meta-analysis of diagnostic test\n  accuracy studies", "comments": "arXiv admin note: text overlap with arXiv:1502.07505", "journal-ref": "AStA Advances in Statistical Analysis, 2018, 102 (2), 211--227", "doi": "10.1007/s10182-017-0299-y", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The composite likelihood (CL) is amongst the computational methods used for\nestimation of the generalized linear mixed model (GLMM) in the context of\nbivariate meta-analysis of diagnostic test accuracy studies. Its advantage is\nthat the likelihood can be derived conveniently under the assumption of\nindependence between the random effects, but there has not been a clear\nanalysis of the merit or necessity of this method. For synthesis of diagnostic\ntest accuracy studies, a copula mixed model has been proposed in the\nbiostatistics literature. This general model includes the GLMM as a special\ncase and can also allow for flexible dependence modelling, different from\nassuming simple linear correlation structures, normality and tail independence\nin the joint tails. A maximum likelihood (ML) method, which is based on\nevaluating the bi-dimensional integrals of the likelihood with quadrature\nmethods has been proposed, and in fact it eases any computational difficulty\nthat might be caused by the double integral in the likelihood function. Both\nmethods are thoroughly examined with extensive simulations and illustrated with\ndata of a published meta-analysis. It is shown that the ML method has no\nnon-convergence issues or computational difficulties and at the same time\nallows estimation of the dependence between study-specific sensitivity and\nspecificity and thus prediction via summary receiver operating curves.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 16:22:48 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 09:54:12 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1611.05405", "submitter": "Tyrus Berry", "authors": "John Harlim and Tyrus Berry", "title": "Correcting biased observation model error in data assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the formulation of most data assimilation schemes assumes an unbiased\nobservation model error, in real applications, model error with nontrivial\nbiases is unavoidable. A practical example is the error in the radiative\ntransfer model (which is used to assimilate satellite measurements) in the\npresence of clouds. As a consequence, many (in fact 99\\%) of the cloudy\nobserved measurements are not being used although they may contain useful\ninformation. This paper presents a novel nonparametric Bayesian scheme which is\nable to learn the observation model error distribution and correct the bias in\nincoming observations. This scheme can be used in tandem with any data\nassimilation forecasting system. The proposed model error estimator uses\nnonparametric likelihood functions constructed with data-driven basis functions\nbased on the theory of kernel embeddings of conditional distributions developed\nin the machine learning community. Numerically, we show positive results with\ntwo examples. The first example is designed to produce a bimodality in the\nobservation model error (typical of \"cloudy\" observations) by introducing\nobstructions to the observations which occur randomly in space and time. The\nsecond example, which is physically more realistic, is to assimilate cloudy\nsatellite brightness temperature-like quantities, generated from a stochastic\ncloud model for tropical convection and a simple radiative transfer model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 18:48:18 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Harlim", "John", ""], ["Berry", "Tyrus", ""]]}, {"id": "1611.05414", "submitter": "Tyrus Berry", "authors": "Franz Hamilton and Tyrus Berry and Timothy Sauer", "title": "Kalman-Takens filtering in the presence of dynamical noise", "comments": null, "journal-ref": null, "doi": "10.1140/epjst/e2016-60363-2", "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of data assimilation for the merging of observed data with dynamical\nmodels is becoming standard in modern physics. If a parametric model is known,\nmethods such as Kalman filtering have been developed for this purpose. If no\nmodel is known, a hybrid Kalman-Takens method has been recently introduced, in\norder to exploit the advantages of optimal filtering in a nonparametric\nsetting. This procedure replaces the parametric model with dynamics\nreconstructed from delay coordinates, while using the Kalman update formulation\nto assimilate new observations. We find that this hybrid approach results in\ncomparable efficiency to parametric methods in identifying underlying dynamics,\neven in the presence of dynamical noise. By combining the Kalman-Takens method\nwith an adaptive filtering procedure we are able to estimate the statistics of\nthe observational and dynamical noise. This solves a long standing problem of\nseparating dynamical and observational noise in time series data, which is\nespecially challenging when no dynamical model is specified.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 19:34:49 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Hamilton", "Franz", ""], ["Berry", "Tyrus", ""], ["Sauer", "Timothy", ""]]}, {"id": "1611.05420", "submitter": "Gane Samb Lo", "authors": "Diam Ba, Seck Cheikh Tidiane, Lo Gane Samb", "title": "Strong approximation for the deviation of kernel copula estimators", "comments": "15", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a uniform in bandwidth law of the iterated logarithm for the maximal\ndeviation of kernel copula estimators from their expectations. We deal\nespecially with the \\textit{local linear}, the \\textit{mirror-reflection} and\nthe \\textit{transformation} estimators. These results are useful for\nestablishing the strong uniform in bandwidth consistency of these kernel\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 19:51:37 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ba", "Diam", ""], ["Tidiane", "Seck Cheikh", ""], ["Samb", "Lo Gane", ""]]}, {"id": "1611.05509", "submitter": "Abhra Sarkar", "authors": "Abhra Sarkar, Jonathan Chabout, Joshua Jones Macopson, Erich D.\n  Jarvis, David B. Dunson", "title": "Bayesian Semiparametric Mixed Effects Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the neurological, genetic and evolutionary basis of human vocal\ncommunication mechanisms using animal vocalization models is an important field\nof neuroscience. The data sets typically comprise structured sequences of\nsyllables or `songs' produced by animals from different genotypes under\ndifferent social contexts. We develop a novel Bayesian semiparametric framework\nfor inference in such data sets. Our approach is built on a novel class of\nmixed effects Markov transition models for the songs that accommodates\nexogenous influences of genotype and context as well as animal-specific\nheterogeneity. We design efficient Markov chain Monte Carlo algorithms for\nposterior computation. Crucial advantages of the proposed approach include its\nability to provide insights into key scientific queries related to global and\nlocal influences of the exogenous predictors on the transition dynamics via\nautomated tests of hypotheses. The methodology is illustrated using simulation\nexperiments and the aforementioned motivating application in neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 00:03:21 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 18:20:19 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Sarkar", "Abhra", ""], ["Chabout", "Jonathan", ""], ["Macopson", "Joshua Jones", ""], ["Jarvis", "Erich D.", ""], ["Dunson", "David B.", ""]]}, {"id": "1611.05550", "submitter": "Lydia T. Liu", "authors": "Lydia T. Liu, Edgar Dobriban, Amit Singer", "title": "$e$PCA: High Dimensional Exponential Family PCA", "comments": "24 pages, 9 figures. An open-source implementation can be found at\n  http://github.com/lydiatliu/epca/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications, such as photon-limited imaging and genomics, involve large\ndatasets with noisy entries from exponential family distributions. It is of\ninterest to estimate the covariance structure and principal components of the\nnoiseless distribution. Principal Component Analysis (PCA), the standard method\nfor this setting, can be inefficient when the noise is non-Gaussian.\n  We develop $e$PCA (exponential family PCA), a new methodology for PCA on\nexponential family distributions. $e$PCA can be used for dimensionality\nreduction and denoising of large data matrices. $e$PCA involves the\neigendecomposition of a new covariance matrix estimator, constructed in a\nsimple and deterministic way using moment calculations, shrinkage, and random\nmatrix theory.\n  We provide several theoretical justifications for our estimator, including\nthe finite-sample convergence rate, and the Marchenko-Pastur law in high\ndimensions. $e$PCA compares favorably to PCA and various PCA alternatives for\nexponential families, in simulations as well as in XFEL and SNP data analysis.\nAn open-source implementation is available.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 03:34:24 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 22:58:55 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Liu", "Lydia T.", ""], ["Dobriban", "Edgar", ""], ["Singer", "Amit", ""]]}, {"id": "1611.05602", "submitter": "Clement Dombry", "authors": "Clement Dombry, Sebastian Engelke, Marco Oesting", "title": "Bayesian inference for multivariate extreme value distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of multivariate and spatial extreme events has attracted\nbroad attention in various areas of science. Max-stable distributions and\nprocesses are the natural class of models for this purpose, and many parametric\nfamilies have been developed and successfully applied. Due to complicated\nlikelihoods, the efficient statistical inference is still an active area of\nresearch, and usually composite likelihood methods based on bivariate densities\nonly are used. Thibaud et al. (2016, Ann. Appl. Stat., to appear) use a\nBayesian approach to fit a Brown--Resnick process to extreme temperatures. In\nthis paper, we extend this idea to a methodology that is applicable to general\nmax-stable distributions and that uses full likelihoods. We further provide\nsimple conditions for the asymptotic normality of the median of the posterior\ndistribution and verify them for the commonly used models in multivariate and\nspatial extreme value statistics. A simulation study shows that this point\nestimator is considerably more efficient than the composite likelihood\nestimator in a frequentist framework. From a Bayesian perspective, our approach\nopens the way for new techniques such as Bayesian model comparison in\nmultivariate and spatial extremes.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 08:19:25 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 11:45:49 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Dombry", "Clement", ""], ["Engelke", "Sebastian", ""], ["Oesting", "Marco", ""]]}, {"id": "1611.05668", "submitter": "Subhajit Dutta Dr.", "authors": "Subhajit Dutta and Anil K. Ghosh", "title": "On Affine Invariant $L_p$ Depth Classifiers based on an Adaptive Choice\n  of $p$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we use L$_p$ depth for classification of multivariate data,\nwhere the value of $p$ is chosen adaptively using observations from the\ntraining sample. While many depth based classifiers are constructed assuming\nelliptic symmetry of the underlying distributions, our proposed L$_p$ depth\nclassifiers cater to a larger class of distributions. We establish Bayes risk\nconsistency of these proposed classifiers under appropriate regularity\nconditions. Several simulated and benchmark data sets are analyzed to compare\ntheir finite sample performance with some existing parametric and nonparametric\nclassifiers including those based on other notions of data depth.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 13:04:51 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dutta", "Subhajit", ""], ["Ghosh", "Anil K.", ""]]}, {"id": "1611.05699", "submitter": "Isaac Skog", "authors": "Johan Wahlstr\\\"om, Isaac Skog, Patricio S. La Rosa, Peter H\\\"andel,\n  Arye Nehorai", "title": "The $\\beta$-model for Random Graphs --- Regression, Cram\\'er-Rao Bounds,\n  and Hypothesis Testing", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2691667", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a maximum-likelihood based method for regression in a setting\nwhere the dependent variable is a random graph and covariates are available on\na graph-level. The model generalizes the well-known $\\beta$-model for random\ngraphs by replacing the constant model parameters with regression functions.\nCram\\'er-Rao bounds are derived for the undirected $\\beta$-model, the directed\n$\\beta$-model, and the generalized $\\beta$-model. The corresponding maximum\nlikelihood estimators are compared to the bounds by means of simulations.\nMoreover, examples are given on how to use the presented maximum likelihood\nestimators to test for directionality and significance. Last, the applicability\nof the model is demonstrated using dynamic social network data describing\ncommunication among healthcare workers.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 08:39:44 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Wahlstr\u00f6m", "Johan", ""], ["Skog", "Isaac", ""], ["La Rosa", "Patricio S.", ""], ["H\u00e4ndel", "Peter", ""], ["Nehorai", "Arye", ""]]}, {"id": "1611.05806", "submitter": "Peter Melchior", "authors": "Peter Melchior and Andy D. Goulding", "title": "Filling the gaps: Gaussian mixture models from noisy, truncated or\n  incomplete samples", "comments": "13 pages, 5 figures, post-publication extension of section 2.3", "journal-ref": null, "doi": "10.1016/j.ascom.2018.09.013", "report-no": null, "categories": "astro-ph.IM astro-ph.HE physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomical data often suffer from noise and incompleteness. We extend the\ncommon mixtures-of-Gaussians density estimation approach to account for\nsituations with a known sample incompleteness by simultaneous imputation from\nthe current model. The method, called GMMis, generalizes existing\nExpectation-Maximization techniques for truncated data to arbitrary truncation\ngeometries and probabilistic rejection processes, as long as they can be\nspecified and do not depend on the density itself. The method accounts for\nindependent multivariate normal measurement errors for each of the observed\nsamples and recovers an estimate of the error-free distribution from which both\nobserved and unobserved samples are drawn. It can perform a separation of a\nmixtures-of-Gaussian signal from a specified background distribution whose\namplitude may be unknown. We compare GMMis to the standard Gaussian mixture\nmodel for simple test cases with different types of incompleteness, and apply\nit to observational data from the NASA Chandra X-ray telescope. The python code\nis released as an open-source package at https://github.com/pmelchior/pyGMMis\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 18:11:15 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 19:49:23 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 22:22:45 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Melchior", "Peter", ""], ["Goulding", "Andy D.", ""]]}, {"id": "1611.05902", "submitter": "Mickael Binois", "authors": "Mickael Binois, Robert B. Gramacy, Michael Ludkovski", "title": "Practical heteroskedastic Gaussian process modeling for large simulation\n  experiments", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": "10.1080/10618600.2018.1458625", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified view of likelihood based Gaussian progress regression\nfor simulation experiments exhibiting input-dependent noise. Replication plays\nan important role in that context, however previous methods leveraging\nreplicates have either ignored the computational savings that come from such\ndesign, or have short-cut full likelihood-based inference to remain tractable.\nStarting with homoskedastic processes, we show how multiple applications of a\nwell-known Woodbury identity facilitate inference for all parameters under the\nlikelihood (without approximation), bypassing the typical full-data sized\ncalculations. We then borrow a latent-variable idea from machine learning to\naddress heteroskedasticity, adapting it to work within the same thrifty\ninferential framework, thereby simultaneously leveraging the computational and\nstatistical efficiency of designs with replication. The result is an\ninferential scheme that can be characterized as single objective function,\ncomplete with closed form derivatives, for rapid library-based optimization.\nIllustrations are provided, including real-world simulation experiments from\nmanufacturing and the management of epidemics.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 21:21:52 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 22:28:26 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Binois", "Mickael", ""], ["Gramacy", "Robert B.", ""], ["Ludkovski", "Michael", ""]]}, {"id": "1611.06208", "submitter": "Lu Tang", "authors": "Lu Tang, Ling Zhou, Peter X.-K. Song", "title": "Distributed Simultaneous Inference in Generalized Linear Models via\n  Confidence Distribution", "comments": "This paper is published in Journal of Multivariate Analysis", "journal-ref": "Journal of Multivariate Analysis, 176, 104567 (2020)", "doi": "10.1016/j.jmva.2019.104567", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed method for simultaneous inference for datasets with\nsample size much larger than the number of covariates, i.e., N >> p, in the\ngeneralized linear models framework. When such datasets are too big to be\nanalyzed entirely by a single centralized computer, or when datasets are\nalready stored in distributed database systems, the strategy of\ndivide-and-combine has been the method of choice for scalability. Due to\npartition, the sub-dataset sample sizes may be uneven and some possibly close\nto p, which calls for regularization techniques to improve numerical stability.\nHowever, there is a lack of clear theoretical justification and practical\nguidelines to combine results obtained from separate regularized estimators,\nespecially when the final objective is simultaneous inference for a group of\nregression parameters. In this paper, we develop a strategy to combine\nbias-corrected lasso-type estimates by using confidence distributions. We show\nthat the resulting combined estimator achieves the same estimation efficiency\nas that of the maximum likelihood estimator using the centralized data. As\ndemonstrated by simulated and real data examples, our divide-and-combine method\nyields nearly identical inference as the centralized benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 19:51:07 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 03:17:48 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 21:05:02 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Tang", "Lu", ""], ["Zhou", "Ling", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "1611.06217", "submitter": "Pedro H. C. Sant'Anna", "authors": "Pedro H. C. Sant'Anna, Xiaojun Song", "title": "Specification Tests for the Propensity Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new nonparametric diagnostic tools to assess the\nasymptotic validity of different treatment effects estimators that rely on the\ncorrect specification of the propensity score. We derive a particular\nrestriction relating the propensity score distribution of treated and control\ngroups, and develop specification tests based upon it. The resulting tests do\nnot suffer from the \"curse of dimensionality\" when the vector of covariates is\nhigh-dimensional, are fully data-driven, do not require tuning parameters such\nas bandwidths, and are able to detect a broad class of local alternatives\nconverging to the null at the parametric rate $n^{-1/2}$, with $n$ the sample\nsize. We show that the use of an orthogonal projection on the tangent space of\nnuisance parameters facilitates the simulation of critical values by means of a\nmultiplier bootstrap procedure, and can lead to power gains. The finite sample\nperformance of the tests is examined by means of a Monte Carlo experiment and\nan empirical application. Open-source software is available for implementing\nthe proposed tests.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:21:16 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 19:41:46 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Sant'Anna", "Pedro H. C.", ""], ["Song", "Xiaojun", ""]]}, {"id": "1611.06221", "submitter": "Stephan Bongers", "authors": "Stephan Bongers, Patrick Forr\\'e, Jonas Peters, Joris M. Mooij", "title": "Foundations of Structural Causal Models with Cycles and Latent Variables", "comments": "75 pages (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural causal models (SCMs), also known as (nonparametric) structural\nequation models (SEMs), are widely used for causal modeling purposes. In\nparticular, acyclic SCMs, also known as recursive SEMs, form a well-studied\nsubclass of SCMs that generalize causal Bayesian networks to allow for latent\nconfounders. In this paper, we investigate SCMs in a more general setting,\nallowing for the presence of both latent confounders and cycles. We show that\nin the presence of cycles, many of the convenient properties of acyclic SCMs do\nnot hold in general: they do not always have a solution; they do not always\ninduce unique observational, interventional and counterfactual distributions; a\nmarginalization does not always exist, and if it exists the marginal model does\nnot always respect the latent projection; they do not always satisfy a Markov\nproperty; and their graphs are not always consistent with their causal\nsemantics. We prove that for SCMs in general each of these properties does hold\nunder certain solvability conditions. Our work generalizes results for SCMs\nwith cycles that were only known for certain special cases so far. We introduce\nthe class of simple SCMs that extends the class of acyclic SCMs to the cyclic\nsetting, while preserving many of the convenient properties of acyclic SCMs.\nWith this paper we aim to provide the foundations for a general theory of\nstatistical causal modeling with SCMs.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:54:03 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 02:15:32 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 16:19:44 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 13:27:13 GMT"}, {"version": "v5", "created": "Mon, 10 May 2021 13:58:29 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bongers", "Stephan", ""], ["Forr\u00e9", "Patrick", ""], ["Peters", "Jonas", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1611.06304", "submitter": "Haiqing Xu", "authors": "YuChin Hsu and TaCheng Huang and Haiqing Xu", "title": "Testing for unobserved heterogeneous treatment effects", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unobserved heterogeneous treatment effects have been emphasized in recent\npolicy evaluation literature. In this paper, we extend Lu and White (2014)'s\ntesting method for unobserved heterogeneous treatment effects by developing\nnonparametric tests under the standard exogenous instrumental variable\nassumption and allowing for endogenous treatment. Specifically, we propose\nKolmogorov--Smirnov--type statistics that are consistent and simple to\nimplement. To illustrate, we apply the proposed test method with two empirical\napplications: treatment effects of job training program on earnings as well as\nthe impact of fertility on family income. The null hypotheses, i.e., lack of\nunobserved heterogeneous treatment effects, cannot be rejected at a 10%\nsignificance level in the former case, but should be rejected at all usual\nsignificance levels in the latter.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 04:43:55 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Hsu", "YuChin", ""], ["Huang", "TaCheng", ""], ["Xu", "Haiqing", ""]]}, {"id": "1611.06381", "submitter": "Gyorgy Terdik DR", "authors": "Gyorgy H. Terdik, Stergios B. Fotopoulos, Venkata K. Jandhyala", "title": "Change-point analysis in frequency domain for chronological data", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to provide a new methodology of how one can\nconsistently estimate a change-point in time series data. In contrast with\nprevious studies, the suggested methodology employs only the empirical spectral\ndensity and its first moment. This is accomplished when both the means and\nvariances before and after the unidentified time point are unknown. Then, the\nwell-known Gauss-Newton algorithm is applied to estimate and provide asymptotic\nresults for the parameters involved. Simulations carried out under different\ndistributions, sizes and unknown time points confirm the validity and accuracy\nof the methodology. The real-world example considered in the paper illustrates\nthe robustness of the methodology in the presence of even extreme outliers.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 15:26:03 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Terdik", "Gyorgy H.", ""], ["Fotopoulos", "Stergios B.", ""], ["Jandhyala", "Venkata K.", ""]]}, {"id": "1611.06399", "submitter": "Sigrunn Holbek Sorbye", "authors": "Sigrunn Holbek S{\\o}rbye and H{\\aa}vard Rue", "title": "Fractional Gaussian noise: Prior specification and model comparison", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional Gaussian noise (fGn) is a self-similar stochastic process used to\nmodel anti-persistent or persistent dependency structures in observed time\nseries. Properties of the autocovariance function of fGn are characterised by\nthe Hurst exponent (H), which in Bayesian contexts typically has been assigned\na uniform prior on the unit interval. This paper argues why a uniform prior is\nunreasonable and introduces the use of a penalised complexity (PC) prior for H.\nThe PC prior is computed to penalise divergence from the special case of white\nnoise, and is invariant to reparameterisations. An immediate advantage is that\nthe exact same prior can be used for the autocorrelation coefficient of a\nfirst-order autoregressive process AR(1), as this model also reflects a\nflexible version of white noise. Within the general setting of latent Gaussian\nmodels, this allows us to compare an fGn model component with AR(1) using Bayes\nfactors, avoiding confounding effects of prior choices for the hyperparameters.\nAmong others, this is useful in climate regression models where inference for\nunderlying linear or smooth trends depends heavily on the assumed noise model.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 17:11:58 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["S\u00f8rbye", "Sigrunn Holbek", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1611.06585", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Nicholas Foti, Ryan P. Adams", "title": "Variational Boosting: Iteratively Refining Posterior Approximations", "comments": "25 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a black-box variational inference method to approximate\nintractable distributions with an increasingly rich approximating class. Our\nmethod, termed variational boosting, iteratively refines an existing\nvariational approximation by solving a sequence of optimization problems,\nallowing the practitioner to trade computation time for accuracy. We show how\nto expand the variational approximating class by incorporating additional\ncovariance structure and by introducing new components to form a mixture. We\napply variational boosting to synthetic and real statistical models, and show\nthat resulting posterior inferences compare favorably to existing posterior\napproximation algorithms in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 20:25:39 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 17:30:28 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Miller", "Andrew C.", ""], ["Foti", "Nicholas", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1611.06653", "submitter": "Gaorong Li", "authors": "Yiping Yang, Tiejun Tong and Gaorong Li", "title": "SIMEX estimation for single-index model with covariate measurement error", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the single-index measurement error model with\nmismeasured covariates in the nonparametric part. To solve the problem, we\ndevelop a simulation-extrapolation (SIMEX) algorithm based on the local linear\nsmoother and the estimating equation. For the proposed SIMEX estimation, it is\nnot needed to assume the distribution of the unobserved covariate. We transform\nthe boundary of a unit ball in $\\mathbb{R}^p$ to the interior of a unit ball in\n$\\mathbb{R}^{p-1}$ by using the constraint $\\|\\beta\\|=1$. The proposed SIMEX\nestimator of the index parameter is shown to be asymptotically normal under\nsome regularity conditions. We also derive the asymptotic bias and variance of\nthe estimator of the unknown link function. Finally, the performance of the\nproposed method is examined by simulation studies and is illustrated by a real\ndata example.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 05:22:43 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Yang", "Yiping", ""], ["Tong", "Tiejun", ""], ["Li", "Gaorong", ""]]}, {"id": "1611.06655", "submitter": "Zhigen Zhao", "authors": "Qian Lin, Zhigen Zhao and Jun S. Liu", "title": "Sparse Sliced Inverse Regression Via Lasso", "comments": "41 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multiple index models, it has recently been shown that the sliced inverse\nregression (SIR) is consistent for estimating the sufficient dimension\nreduction (SDR) space if and only if $\\rho=\\lim\\frac{p}{n}=0$, where $p$ is the\ndimension and $n$ is the sample size. Thus, when $p$ is of the same or a higher\norder of $n$, additional assumptions such as sparsity must be imposed in order\nto ensure consistency for SIR. By constructing artificial response variables\nmade up from top eigenvectors of the estimated conditional covariance matrix,\nwe introduce a simple Lasso regression method to obtain an estimate of the SDR\nspace. The resulting algorithm, Lasso-SIR, is shown to be consistent and\nachieve the optimal convergence rate under certain sparsity conditions when $p$\nis of order $o(n^2\\lambda^2)$, where $\\lambda$ is the generalized\nsignal-to-noise ratio. We also demonstrate the superior performance of\nLasso-SIR compared with existing approaches via extensive numerical studies and\nseveral real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 05:41:18 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 13:34:44 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lin", "Qian", ""], ["Zhao", "Zhigen", ""], ["Liu", "Jun S.", ""]]}, {"id": "1611.06739", "submitter": "Jelle Goeman", "authors": "Jelle Goeman, Rosa Meijer, Thijmen Krebs, Aldo Solari", "title": "Simultaneous Control of All False Discovery Proportions in Large-Scale\n  Multiple Hypothesis Testing", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asz041", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closed testing procedures are classically used for familywise error rate\n(FWER) control, but they can also be used to obtain simultaneous confidence\nbounds for the false discovery proportion (FDP) in all subsets of the\nhypotheses. In this paper we investigate the special case of closed testing\nwith Simes local tests. We construct a novel fast and exact shortcut which we\nuse to investigate the power of this method when the number of hypotheses goes\nto infinity. We show that, if a minimal amount of signal is present, the\naverage power to detect false hypotheses at any desired FDP level does not\nvanish. Additionally, we show that the confidence bounds for FDP are consistent\nestimators for the true FDP for every non-vanishing subset. For the case of a\nfinite number of hypotheses, we show connections between Simes-based closed\ntesting and the procedure of Benjamini and Hochberg.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 11:50:23 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 13:48:33 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Goeman", "Jelle", ""], ["Meijer", "Rosa", ""], ["Krebs", "Thijmen", ""], ["Solari", "Aldo", ""]]}, {"id": "1611.06874", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Fl\\'avio Eler de Melo, Simon Maskell", "title": "Langevin Incremental Mixture Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel method through which local information about the\ntarget density can be used to construct an efficient importance sampler. The\nbackbone of the proposed method is the Incremental Mixture Importance Sampling\n(IMIS) algorithm of Raftery and Bao (2010), which builds a mixture importance\ndistribution incrementally, by positioning new mixture components where the\nimportance density lacks mass, relative to the target. The key innovation\nproposed here is that the mixture components used by IMIS are local\napproximations to the target density. In particular, their mean vectors and\ncovariance matrices are constructed by numerically solving certain differential\nequations, whose solution depends on the gradient field of the target\nlog-density. The new sampler has a number of advantages: a) it provides an\nextremely parsimonious parametrization of the mixture importance density, whose\nconfiguration effectively depends only on the shape of the target and on a\nsingle free parameter representing pseudo-time; b) it scales well with the\ndimensionality of the target; c) it can deal with targets that are not log-\nconcave. The performance of the proposed approach is demonstrated on a\nsynthetic non-Gaussian multimodal density, defined on up to eighty dimensions,\nand on a Bayesian logistic regression model, using the Sonar data-set. The\nJulia code implementing the importance sampler proposed here can be found at\nhttps:/github.com/mfasiolo/LIMIS.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:14:30 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Fasiolo", "Matteo", ""], ["de Melo", "Fl\u00e1vio Eler", ""], ["Maskell", "Simon", ""]]}, {"id": "1611.07093", "submitter": "Ashkan Esmaeili", "authors": "Ahmadreza Moradipari, Sina Shahsavari, Ashkan Esmaeili, and Farokh\n  Marvasti", "title": "Using Empirical Covariance Matrix in Enhancing Prediction Accuracy of\n  Linear Models with Missing Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference and Estimation in Missing Information (MI) scenarios are important\ntopics in Statistical Learning Theory and Machine Learning (ML). In ML\nliterature, attempts have been made to enhance prediction through precise\nfeature selection methods. In sparse linear models, LASSO is well-known in\nextracting the desired support of the signal and resisting against noisy\nsystems. When sparse models are also suffering from MI, the sparse recovery and\ninference of the missing models are taken into account simultaneously. In this\npaper, we will introduce an approach which enjoys sparse regression and\ncovariance matrix estimation to improve matrix completion accuracy, and as a\nresult enhancing feature selection preciseness which leads to reduction in\nprediction Mean Squared Error (MSE). We will compare the effect of employing\ncovariance matrix in enhancing estimation accuracy to the case it is not used\nin feature selection. Simulations show the improvement in the performance as\ncompared to the case where the covariance matrix estimation is not used.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 23:07:51 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 23:26:32 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 11:29:34 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Moradipari", "Ahmadreza", ""], ["Shahsavari", "Sina", ""], ["Esmaeili", "Ashkan", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1611.07103", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Generation of discrete random variables in scalable frameworks", "comments": "The first sections of the paper have been almost completely\n  rewritten. A deep revision of the English has been made", "journal-ref": "Statistics & Probability Letters, Volume 132, January 2018, Pages\n  99-106", "doi": "10.1016/j.spl.2017.09.004", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we face the problem of simulating discrete random variables\nwith general and varying distributions in a scalable framework, where fully\nparallelizable operations should be preferred. The new paradigm is inspired by\nthe context of discrete choice models. Compared to classical algorithms, we add\nparallelized randomness, and we leave the final simulation of the random\nvariable to a single associative operation. We characterize the set of\nalgorithms that work in this way, and those algorithms that may have an\nadditive or multiplicative local noise. As a consequence, we could define a\nnatural way to solve some popular simulation problems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 23:37:31 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2017 15:29:43 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 02:47:02 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1611.07197", "submitter": "Tomoyuki Obuchi", "authors": "Tomoyuki Obuchi, Shiro Ikeda, Kazunori Akiyama, Yoshiyuki Kabashima", "title": "Accelerating cross-validation with total variation and its application\n  to super-resolution imaging", "comments": "14 pages, 4 figures. A Matlab package implementing the approximation\n  formula is available from https://github.com/T-Obuchi/AcceleratedCVon2DTVLR", "journal-ref": "PLoS ONE 12(12): e0188012 (2017)", "doi": "10.1371/journal.pone.0188012", "report-no": null, "categories": "stat.ME astro-ph.GA cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approximation formula for the cross-validation error (CVE) of a\nsparse linear regression penalized by $\\ell_1$-norm and total variation terms,\nwhich is based on a perturbative expansion utilizing the largeness of both the\ndata dimensionality and the model. The developed formula allows us to reduce\nthe necessary computational cost of the CVE evaluation significantly. The\npracticality of the formula is tested through application to simulated\nblack-hole image reconstruction on the event-horizon scale with super\nresolution. The results demonstrate that our approximation reproduces the CVE\nvalues obtained via literally conducted cross-validation with reasonably good\nprecision.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 08:47:31 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:35:27 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Obuchi", "Tomoyuki", ""], ["Ikeda", "Shiro", ""], ["Akiyama", "Kazunori", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1611.07247", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad", "title": "Estimation of parametric and semiparametric mixture models using\n  phi-divergences", "comments": "PhD thesis. The main text is in English. Only the introduction part\n  is in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of mixture models constitutes a large domain of research in\nstatistics. In the first part of this work, we present phi-divergences and the\nexisting methods which produce robust estimators. We are more particularly\ninterested in the so-called dual formula of phi-divergences. We build a new\nrobust estimator based on this formula. We study its asymptotic properties and\ngive a numerical comparison with existing methods on simulated data. We also\nintroduce a proximal-point algorithm whose aim is to calculate divergence-based\nestimators. We give some of the convergence properties of this algorithm and\nillustrate them on theoretical and simulated examples. In the second part of\nthis thesis, we build a new structure for two-component mixture models where\none component is unknown. The new approach permits to incorporate a prior\nlinear information about the unknown component such as moment-type and\nL-moments constraints. We study the asymptotic properties of the proposed\nestimators. Several experimental results on simulated data are illustrated\nshowing the advantage of the novel approach and the gain from using the prior\ninformation in comparison to existing methods which do not incorporate any\nprior information except for a symmetry assumption over the unknown component.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 11:10:36 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 10:23:51 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 11:12:32 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Mohamad", "Diaa Al", ""]]}, {"id": "1611.07256", "submitter": "Azzimonti Dario", "authors": "Dario Azzimonti (IDSIA), David Ginsbourger (Idiap, IMSV), Cl\\'ement\n  Chevalier (UNINE), Julien Bect (L2S, GdR MASCOT-NUM), Yann Richet (IRSN, GdR\n  MASCOT-NUM)", "title": "Adaptive Design of Experiments for Conservative Estimation of Excursion\n  Sets", "comments": null, "journal-ref": "Technometrics, 63(1):13-26, 2021", "doi": "10.1080/00401706.2019.1693427", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the set of all inputs that leads a\nsystem to some particular behavior. The system is modeled by an\nexpensive-to-evaluate function, such as a computer experiment, and we are\ninterested in its excursion set, i.e. the set of points where the function\ntakes values above or below some prescribed threshold. The objective function\nis emulated with a Gaussian Process (GP) model based on an initial design of\nexperiments enriched with evaluation results at (batch-)sequentially determined\ninput points. The GP model provides conservative estimates for the excursion\nset, which control false positives while minimizing false negatives. We\nintroduce adaptive strategies that sequentially select new evaluations of the\nfunction by reducing the uncertainty on conservative estimates. Following the\nStepwise Uncertainty Reduction approach we obtain new evaluations by minimizing\nadapted criteria. Tractable formulae for the conservative criteria are derived,\nwhich allow more convenient optimization. The method is benchmarked on random\nfunctions generated under the model assumptions in different scenarios of noise\nand batch size. We then apply it to a reliability engineering test case.\nOverall, the proposed strategy of minimizing false negatives in conservative\nestimation achieves competitive performance both in terms of model-based and\nmodel-free indicators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 11:40:25 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 09:36:00 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 08:01:43 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 12:48:22 GMT"}, {"version": "v5", "created": "Fri, 25 Oct 2019 13:14:35 GMT"}, {"version": "v6", "created": "Tue, 4 Feb 2020 07:22:36 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Azzimonti", "Dario", "", "IDSIA"], ["Ginsbourger", "David", "", "Idiap, IMSV"], ["Chevalier", "Cl\u00e9ment", "", "UNINE"], ["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Richet", "Yann", "", "IRSN, GdR\n  MASCOT-NUM"]]}, {"id": "1611.07345", "submitter": "Bernhard Klar", "authors": "Hajo Holzmann and Bernhard Klar", "title": "Weighted scoring rules and hypothesis testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss weighted scoring rules for forecast evaluation and their\nconnection to hypothesis testing. First, a general construction principle for\nstrictly locally proper weighted scoring rules based on conditional densities\nand scoring rules for probability forecasts is proposed. We show how\nlikelihood-based weighted scoring rules from the literature fit into this\nframework, and also introduce a weighted version of the Hyv\\\"arinen score,\nwhich is a local scoring rule in the sense that it only depends on the forecast\ndensity and its derivatives at the observation, and does not require evaluation\nof integrals. Further, we discuss the relation to hypothesis testing. Using a\nweighted scoring rule introduces a censoring mechanism, in which the form of\nthe density is irrelevant outside the region of interest. For the resulting\ntesting problem with composite null - and alternative hypotheses, we construct\noptimal tests, and identify the associated weighted scoring rule. As a\npractical consequence, using a weighted scoring rule allows to decide in favor\nof a forecast which is superior to a competing forecast on a region of\ninterest, even though it may be inferior outside this region. A simulation\nstudy and an application to financial time-series data illustrate these\nfindings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 15:07:18 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 14:45:35 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Holzmann", "Hajo", ""], ["Klar", "Bernhard", ""]]}, {"id": "1611.07469", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick, Michael Jordan", "title": "Fast Measurements of Robustness to Changing Priors in Variational Bayes", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian analysis, the posterior follows from the data and a choice of a\nprior and a likelihood. One hopes that the posterior is robust to reasonable\nvariation in the choice of prior, since this choice is made by the modeler and\nis often somewhat subjective. A different, equally subjectively plausible\nchoice of prior may result in a substantially different posterior, and so\ndifferent conclusions drawn from the data. Were this to be the case, our\nconclusions would not be robust to the choice of prior. To determine whether\nour model is robust, we must quantify how sensitive our posterior is to\nperturbations of our prior. Despite the importance of the problem and a\nconsiderable body of literature, generic, easy-to-use methods to quantify\nBayesian robustness are still lacking.\n  Abstract In this paper, we demonstrate that powerful measures of robustness\ncan be easily calculated from Variational Bayes (VB) approximate posteriors. We\nbegin with local robustness, which measures the effect of infinitesimal changes\nto the prior on a posterior mean of interest. In particular, we show that the\ninfluence function of Gustafson (2012) has a simple, easy-to-calculate closed\nform expression for VB approximations. We then demonstrate how local robustness\nmeasures can be inadequate for non-local prior changes, such as replacing one\nprior entirely with another. We propose a simple approximate non-local\nrobustness measure and demonstrate its effectiveness on a simulated data set.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 19:13:04 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 08:39:40 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""], ["Jordan", "Michael", ""]]}, {"id": "1611.07873", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead, Joris Bierkens, Murray Pollock and Gareth O Roberts", "title": "Piecewise Deterministic Markov Processes for Continuous-Time Monte Carlo", "comments": null, "journal-ref": "Statist. Sci., Volume 33, Number 3 (2018), 386-412", "doi": "10.1214/18-STS648", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there have been exciting developments in Monte Carlo methods, with\nthe development of new MCMC and sequential Monte Carlo (SMC) algorithms which\nare based on continuous-time, rather than discrete-time, Markov processes. This\nhas led to some fundamentally new Monte Carlo algorithms which can be used to\nsample from, say, a posterior distribution. Interestingly, continuous-time\nalgorithms seem particularly well suited to Bayesian analysis in big-data\nsettings as they need only access a small sub-set of data points at each\niteration, and yet are still guaranteed to target the true posterior\ndistribution. Whilst continuous-time MCMC and SMC methods have been developed\nindependently we show here that they are related by the fact that both involve\nsimulating a piecewise deterministic Markov process. Furthermore we show that\nthe methods developed to date are just specific cases of a potentially much\nwider class of continuous-time Monte Carlo algorithms. We give an informal\nintroduction to piecewise deterministic Markov processes, covering the aspects\nrelevant to these new Monte Carlo algorithms, with a view to making the\ndevelopment of new continuous-time Monte Carlo more accessible. We focus on how\nand why sub-sampling ideas can be used with these algorithms, and aim to give\ninsight into how these new algorithms can be implemented, and what are some of\nthe issues that affect their efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 16:42:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Fearnhead", "Paul", ""], ["Bierkens", "Joris", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O", ""]]}, {"id": "1611.08145", "submitter": "Seyed Mahdi Mahmoudi", "authors": "Seyed Mahdi Mahmoudi and Ernst Wit", "title": "Estimating Causal Effects From Nonparanormal Observational Data", "comments": "19 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the basic aims in science is to unravel the chain of cause and effect\nof particular systems. Especially for large systems this can be a daunting\ntask. Detailed interventional and randomized data sampling approaches can be\nused to resolve the causality question, but for many systems such interventions\nare impossible or too costly to obtain. Recently, Maathuis et al. (2010),\nfollowing ideas from Spirtes et al. (2000), introduced a framework to estimate\ncausal effects in large scale Gaussian systems. By describing the causal\nnetwork as a directed acyclic graph it is a possible to estimate a class of\nMarkov equivalent systems that describe the underlying causal interactions\nconsistently, even for non-Gaussian systems. In these systems, causal effects\nstop being linear and cannot be described any more by a single coefficient. In\nthis paper, we derive the general functional form of causal effect in a large\nsubclass of non-Gaussian distributions, called the non- paranormal. We also\nderive a convenient approximation, which can be used effectively in estimation.\nWe apply the method to an observational gene expression dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 11:26:04 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 15:05:50 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Mahmoudi", "Seyed Mahdi", ""], ["Wit", "Ernst", ""]]}, {"id": "1611.08261", "submitter": "Brian Bader", "authors": "Brian Bader", "title": "Automated, Efficient, and Practical Extreme Value Analysis with\n  Environmental Applications", "comments": "author's dissertation, 6 total chapters, 3 major chapters, Doctoral\n  Dissertations. Paper 1261. http://digitalcommons.uconn.edu/dissertations/1261", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the fundamental probabilistic theory of extremes has been well\ndeveloped, there are many practical considerations that must be addressed in\napplication. The contribution of this thesis is four-fold. The first concerns\nthe choice of r in the r largest order statistics modeling of extremes. The\nsecond contribution pertains to threshold selection in the peaks-over-threshold\napproach. The third combines a theoretical and methodological approach to\nimprove estimation within non-stationary regional frequency models of extremal\ndata\n  The methodology developed is demonstrated with climate based applications.\nLast, an overview of computational issues for extremes is provided, along with\na brief tutorial of the R package eva, which improves the functionality of\nexisting extreme value software, as well as providing new implementations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 17:15:08 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Bader", "Brian", ""]]}, {"id": "1611.08631", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho", "title": "Change-point detection in panel data via double CUSUM statistic", "comments": null, "journal-ref": "Electronic Journal of Statistics, Volume 10, Number 2 (2016),\n  2000-2038", "doi": "10.1214/16-EJS1155", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of (multiple) change-point detection\nin panel data. We propose the double CUSUM statistic which utilises the\ncross-sectional change-point structure by examining the cumulative sums of\nordered CUSUMs at each point. The efficiency of the proposed change-point test\nis studied, which is reflected on the rate at which the cross-sectional size of\na change is permitted to converge to zero while it is still detectable. Also,\nthe consistency of the proposed change-point detection procedure based on the\nbinary segmentation algorithm, is established in terms of both the total number\nand locations (in time) of the estimated change-points. Motivated by the\nrepresentation properties of the Generalised Dynamic Factor Model, we propose a\nbootstrap procedure for test criterion selection, which accounts for both\ncross-sectional and within-series correlations in high-dimensional data. The\nempirical performance of the double CUSUM statistics, equipped with the\nproposed bootstrap scheme, is investigated in a comparative simulation study\nwith the state-of-the-art. As an application, we analyse the log returns of S&P\n100 component stock prices over a period of one year.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 23:12:05 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cho", "Haeran", ""]]}, {"id": "1611.08632", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho, Yannig Goude, Xavier Brossat, Qiwei Yao", "title": "Modelling and forecasting daily electricity load curves: a hybrid\n  approach", "comments": null, "journal-ref": "Journal Of The American Statistical Association Vol. 108 , Iss.\n  501, 2013", "doi": "10.1080/01621459.2012.722900", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid approach for the modelling and the short-term forecasting\nof electricity loads. Two building blocks of our approach are (i) modelling the\noverall trend and seasonality by fitting a generalised additive model to the\nweekly averages of the load, and (ii) modelling the dependence structure across\nconsecutive daily loads via curve linear regression. For the latter, a new\nmethodology is proposed for linear regression with both curve response and\ncurve regressors. The key idea behind the proposed methodology is the dimension\nreduction based on a singular value decomposition in a Hilbert space, which\nreduces the curve regression problem to several ordinary (i.e. scalar) linear\nregression problems. We illustrate the hybrid method using the French\nelectricity loads between 1996 and 2009, on which we also compare our method\nwith other available models including the EDF operational model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 23:19:44 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cho", "Haeran", ""], ["Goude", "Yannig", ""], ["Brossat", "Xavier", ""], ["Yao", "Qiwei", ""]]}, {"id": "1611.08634", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho, Piotr Fryzlewicz", "title": "Multiscale interpretation of taut string estimation and its connection\n  to Unbalanced Haar wavelets", "comments": null, "journal-ref": "Statistics and Computing (2011) Volume 21, Issue 4, pp 671-681", "doi": "10.1007/s11222-010-9200-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two state-of-the-art non-linear techniques for nonparametric\nfunction estimation via piecewise constant approximation: the taut string and\nthe Unbalanced Haar methods. While it is well-known that the latter is\nmultiscale, it is not obvious that the former can also be interpreted as\nmultiscale. We provide a unified multiscale representation for both methods,\nwhich offers an insight into the relationship between them as well as\nsuggesting lessons both methods can learn from each other.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 23:27:26 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cho", "Haeran", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1611.08636", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho", "title": "A test for second-order stationarity of time series based on\n  unsystematic sub-samples", "comments": null, "journal-ref": "Stat (2016) Volume 5, Issue 1, Pages 262-277", "doi": "10.1002/sta4.126", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new method for testing the stationarity of time\nseries, where the test statistic is obtained from measuring and maximising the\ndifference in the second-order structure over pairs of randomly drawn\nintervals. The asymptotic normality of the test statistic is established for\nboth Gaussian and a range of non-Gaussian time series, and a bootstrap\nprocedure is proposed for estimating the variance of the main statistics.\nFurther, we show the consistency of our test under local alternatives. Due to\nthe flexibility inherent in the random, unsystematic sub-samples used for test\nstatistic construction, the proposed method is able to identify the intervals\nof significant departure from the stationarity without any dyadic constraints,\nwhich is an advantage over other tests employing systematic designs. We\ndemonstrate its good finite sample performance on both simulated and real data,\nparticularly in detecting localised departure from the stationarity.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 23:33:11 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cho", "Haeran", ""]]}, {"id": "1611.08639", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho, Piotr Fryzlewicz", "title": "Multiple-change-point detection for high dimensional time series via\n  sparsified binary segmentation", "comments": null, "journal-ref": "Journal of the Royal Statistical Society Series B (2015) Volume\n  77, Issue 2, Pages 475-507", "doi": "10.1111/rssb.12079", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series segmentation, a.k.a. multiple change-point detection, is a\nwell-established problem. However, few solutions are designed specifically for\nhigh-dimensional situations. In this paper, our interest is in segmenting the\nsecond-order structure of a high-dimensional time series. In a generic step of\na binary segmentation algorithm for multivariate time series, one natural\nsolution is to combine CUSUM statistics obtained from local periodograms and\ncross-periodograms of the components of the input time series. However, the\nstandard \"maximum\" and \"average\" methods for doing so often fail in high\ndimensions when, for example, the change-points are sparse across the panel or\nthe CUSUM statistics are spuriously large. In this paper, we propose the\nSparsified Binary Segmentation (SBS) algorithm which aggregates the CUSUM\nstatistics by adding only those that pass a certain threshold. This\n\"sparsifying\" step reduces the impact of irrelevant, noisy contributions, which\nis particularly beneficial in high dimensions. In order to show the consistency\nof SBS, we introduce the multivariate Locally Stationary Wavelet model for time\nseries, which is a separate contribution of this work.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 23:47:30 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cho", "Haeran", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1611.08640", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho, Piotr Fryzlewicz", "title": "High-dimensional variable selection via tilting", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series B (2012), 74:\n  593-622", "doi": "10.1111/j.1467-9868.2011.01023.x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers variable selection in linear regression models where the\nnumber of covariates is possibly much larger than the number of observations.\nHigh dimensionality of the data brings in many complications, such as (possibly\nspurious) high correlations between the variables, which result in marginal\ncorrelation being unreliable as a measure of association between the variables\nand the response. We propose a new way of measuring the contribution of each\nvariable to the response which takes into account high correlations between the\nvariables in a data-driven way. The proposed tilting procedure provides an\nadaptive choice between the use of marginal correlation and tilted correlation\nfor each variable, where the choice is made depending on the values of the hard\nthresholded sample correlation of the design matrix. We study the conditions\nunder which this measure can successfully discriminate between the relevant and\nthe irrelevant variables and thus be used as a tool for variable selection.\nFinally, an iterative variable screening algorithm is constructed to exploit\nthe theoretical properties of tilted correlation, and its good practical\nperformance is demonstrated in a comparative simulation study.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 00:03:46 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cho", "Haeran", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1611.08649", "submitter": "Yang Li", "authors": "Yang Li and Jun S. Liu", "title": "Robust Variable and Interaction Selection for Logistic Regression and\n  Multiple Index Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Stepwise cOnditional likelihood variable selection for\nDiscriminant Analysis (SODA) to detect both main and quadratic interaction\neffects in logistic regression and quadratic discriminant analysis (QDA)\nmodels. In the forward stage, SODA adds in important predictors evaluated based\non their overall contributions, whereas in the backward stage SODA removes\nunimportant terms so as to optimize the extended Bayesian Information Criterion\n(EBIC). Compared with existing methods on QDA variable selections, SODA can\ndeal with high-dimensional data with the number of predictors much larger than\nthe sample size and does not require the joint normality assumption on\npredictors, leading to much enhanced robustness. We further extend SODA to\nconduct variable selection and model fitting for multiple index models.\nCompared with existing variable selection methods based on the Sliced Inverse\nRegression (SIR) (Li 1991), SODA requires neither the linearity nor the\nconstant variance condition and is much more robust. Our theoretical analyses\nestablish the variable-selection consistency of SODA under high-dimensional\nsettings, and our simulation studies as well as real-data applications\ndemonstrate superior performances of SODA in dealing with non-Gaussian design\nmatrices in both classification problems and multiple index models.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 02:13:31 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 19:31:47 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Li", "Yang", ""], ["Liu", "Jun S.", ""]]}, {"id": "1611.08747", "submitter": "Hossein Masoumi Karakani", "authors": "Hossein Masoumi Karakani, Janet van Niekerk and Paul van Staden", "title": "Bayesian Analysis of AR(1) Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first-order autoregressive process, AR (1), has been widely used and\nimplemented in time series analysis. Different estimation methods have been\nemployed in order to estimate the autoregressive parameter. This article\nfocuses on subjective Bayesian estimation as opposed to objective Bayesian\nestimation and frequentist procedures. The truncated normal distribution is\nconsidered as a prior, to impose stationarity. The posterior distribution as\nwell as the Bayes estimator are derived. A comparative study between the newly\nderived estimator and other existing estimation methods (frequentist) is\nemployed in terms of simulation and real data. Furthermore, a posterior\nsensitivity analysis is performed based on four different priors; g prior,\nnatural conjugate prior, Jeffreys' prior and truncated normal prior and the\nperformance is compared in terms of Highest Posterior Density Region criterion.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 22:00:13 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Karakani", "Hossein Masoumi", ""], ["van Niekerk", "Janet", ""], ["van Staden", "Paul", ""]]}, {"id": "1611.08862", "submitter": "Adriano Polpo", "authors": "Natalia L. Oliveira, Carlos A. de B. Pereira, Marcio A. Diniz and\n  Adriano Polpo", "title": "The Likelihood Ratio Test and Full Bayesian Significance Test under\n  small sample sizes for contingency tables", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0199102", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing in contingency tables is usually based on asymptotic\nresults, thereby restricting its proper use to large samples. To study these\ntests in small samples, we consider the likelihood ratio test and define an\naccurate index, the P-value, for the celebrated hypotheses of homogeneity,\nindependence, and Hardy-Weinberg equilibrium. The aim is to understand the use\nof the asymptotic results of the frequentist Likelihood Ratio Test and the\nBayesian FBST -- Full Bayesian Significance Test -- under small-sample\nscenarios. The proposed exact P-value is used as a benchmark to understand the\nother indices. We perform analysis in different scenarios, considering\ndifferent sample sizes and different table dimensions. The exact Fisher test\nfor $2 \\times 2$ tables that drastically reduces the sample space is also\ndiscussed. The main message of this paper is that all indices have very similar\nbehavior, so the tests based on asymptotic results are very good to be used in\nany circumstance, even with small sample sizes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 15:21:04 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 13:25:11 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 15:28:44 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Oliveira", "Natalia L.", ""], ["Pereira", "Carlos A. de B.", ""], ["Diniz", "Marcio A.", ""], ["Polpo", "Adriano", ""]]}, {"id": "1611.09063", "submitter": "Colette Mair", "authors": "Colette Mair, Sema Nickbakhsh, Richard Reeve, Jim McMenamin, Arlene\n  Reynolds, Rory Gunson, Pablo R Murcia and Louise Matthews", "title": "Estimation of temporal covariances in pathogen dynamics using Bayesian\n  multivariate autoregressive models", "comments": "22 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well recognised that animal and plant pathogens form complex ecological\ncommunities of interacting organisms within their hosts. Although community\necology approaches have been applied to determine pathogen interactions at the\nwithin-host scale, methodologies enabling robust inference of the\nepidemiological impact of pathogen interactions are lacking. Here we developed\na novel statistical framework to identify statistical covariances from the\ninfection time-series of multiple pathogens simultaneously. Our framework\nextends Bayesian multivariate disease mapping models to analyse multivariate\ntime series data by accounting for within- and between-year dependencies in\ninfection risk and incorporating a between-pathogen covariance matrix which we\nestimate. Importantly, our approach accounts for possible confounding drivers\nof temporal patterns in pathogen infection frequencies, enabling robust\ninference of pathogen-pathogen interactions. We illustrate the validity of our\nstatistical framework using simulated data and applied it to diagnostic data\navailable for five respiratory viruses co-circulating in a major urban\npopulation between 2005 and 2013: adenovirus, human coronavirus, human\nmetapneumovirus, influenza B virus and respiratory syncytial virus. We found\npositive and negative covariances indicative of epidemiological interactions\namong specific virus pairs. This statistical framework enables a community\necology perspective to be applied to infectious disease epidemiology with\nimportant utility for public health planning and preparedness.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 10:57:59 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 14:40:06 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 12:22:05 GMT"}, {"version": "v4", "created": "Sat, 13 Apr 2019 22:02:39 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Mair", "Colette", ""], ["Nickbakhsh", "Sema", ""], ["Reeve", "Richard", ""], ["McMenamin", "Jim", ""], ["Reynolds", "Arlene", ""], ["Gunson", "Rory", ""], ["Murcia", "Pablo R", ""], ["Matthews", "Louise", ""]]}, {"id": "1611.09225", "submitter": "Jan G\\'orecki", "authors": "Jan G\\'orecki, Marius Hofert and Martin Hole\\v{n}a", "title": "On structure, family and parameter estimation of hierarchical\n  Archimedean copulas", "comments": "63 pages, one attachment in attachment.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on structure determination and parameter estimation of hierarchical\nArchimedean copulas (HACs) has so far mostly focused on the case in which all\nappearing Archimedean copulas belong to the same Archimedean family. The\npresent work addresses this issue and proposes a new approach for estimating\nHACs that involve different Archimedean families. It is based on employing\ngoodness-of-fit test statistics directly into HAC estimation. The approach is\nsummarized in a simple algorithm, its theoretical justification is given and\nits applicability is illustrated by several experiments, which include\nestimation of HACs involving up to five different Archimedean families.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:12:45 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["G\u00f3recki", "Jan", ""], ["Hofert", "Marius", ""], ["Hole\u0148a", "Martin", ""]]}, {"id": "1611.09414", "submitter": "Amit Sharma", "authors": "Amit Sharma, Jake M. Hofman, Duncan J. Watts", "title": "Split-door criterion: Identification of causal effects through auxiliary\n  outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating causal effects in time series data when\nfine-grained information about the outcome of interest is available.\nSpecifically, we examine what we call the split-door setting, where the outcome\nvariable can be split into two parts: one that is potentially affected by the\ncause being studied and another that is independent of it, with both parts\nsharing the same (unobserved) confounders. We show that under these conditions,\nthe problem of identification reduces to that of testing for independence among\nobserved variables, and present a method that uses this approach to\nautomatically find subsets of the data that are causally identified. We\ndemonstrate the method by estimating the causal impact of Amazon's recommender\nsystem on traffic to product pages, finding thousands of examples within the\ndataset that satisfy the split-door criterion. Unlike past studies based on\nnatural experiments that were limited to a single product category, our method\napplies to a large and representative sample of products viewed on the site. In\nline with previous work, we find that the widely-used click-through rate (CTR)\nmetric overestimates the causal impact of recommender systems; depending on the\nproduct category, we estimate that 50-80\\% of the traffic attributed to\nrecommender systems would have happened even without any recommendations. We\nconclude with guidelines for using the split-door criterion as well as a\ndiscussion of other contexts where the method can be applied.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 22:32:16 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 13:09:13 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Sharma", "Amit", ""], ["Hofman", "Jake M.", ""], ["Watts", "Duncan J.", ""]]}, {"id": "1611.09420", "submitter": "Yuan Liao", "authors": "Christian Hansen and Yuan Liao", "title": "The Factor-Lasso and K-Step Bootstrap Approach for Inference in\n  High-Dimensional Economic Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference about coefficients on a small number of variables of\ninterest in a linear panel data model with additive unobserved individual and\ntime specific effects and a large number of additional time-varying confounding\nvariables. We allow the number of these additional confounding variables to be\nlarger than the sample size, and suppose that, in addition to unrestricted time\nand individual specific effects, these confounding variables are generated by a\nsmall number of common factors and high-dimensional weakly-dependent\ndisturbances. We allow that both the factors and the disturbances are related\nto the outcome variable and other variables of interest. To make informative\ninference feasible, we impose that the contribution of the part of the\nconfounding variables not captured by time specific effects, individual\nspecific effects, or the common factors can be captured by a relatively small\nnumber of terms whose identities are unknown. Within this framework, we provide\na convenient computational algorithm based on factor extraction followed by\nlasso regression for inference about parameters of interest and show that the\nresulting procedure has good asymptotic properties. We also provide a simple\nk-step bootstrap procedure that may be used to construct inferential statements\nabout parameters of interest and prove its asymptotic validity. The proposed\nbootstrap may be of substantive independent interest outside of the present\ncontext as the proposed bootstrap may readily be adapted to other contexts\ninvolving inference after lasso variable selection and the proof of its\nvalidity requires some new technical arguments. We also provide simulation\nevidence about performance of our procedure and illustrate its use in two\nempirical applications.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 23:04:36 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 18:50:04 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Hansen", "Christian", ""], ["Liao", "Yuan", ""]]}, {"id": "1611.09421", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti, Andrea Ghiglietti and William F. Rosenberger", "title": "Nonparametric covariate-adjusted response-adaptive design based on a\n  functional urn model", "comments": "to Appear in The Annals of Statistics\n  http://www.imstat.org/aos/future_papers.html, 27 pages + 18 Supplementary\n  materials", "journal-ref": "Ann. Statist., Volume 46, Number 6B (2018), 3838-3866", "doi": "10.1214/17-AOS1677", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a general class of covariate-adjusted\nresponse-adaptive (CARA) designs based on a new functional urn model. We prove\nstrong consistency concerning the functional urn proportion and the proportion\nof subjects assigned to the treatment groups, in the whole study and for each\ncovariate profile, allowing the distribution of the responses conditioned on\ncovariates to be estimated nonparametrically. In addition, we establish joint\ncentral limit theorems for the above quantities and the sufficient statistics\nof features of interest, which allow to construct procedures to make inference\non the conditional response distributions. These results are then applied to\ntypical situations concerning Gaussian and binary responses.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 23:05:59 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 18:38:34 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Aletti", "Giacomo", ""], ["Ghiglietti", "Andrea", ""], ["Rosenberger", "William F.", ""]]}, {"id": "1611.09488", "submitter": "Pritam Ranjan", "authors": "Ru Zhang, Chunfang Devon Lin, Pritam Ranjan", "title": "Local Gaussian Process Model for Large-scale Dynamic Computer\n  Experiments", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": "10.1080/10618600.2018.1473778", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent accelerated growth in the computing power has generated\npopularization of experimentation with dynamic computer models in various\nphysical and engineering applications. Despite the extensive statistical\nresearch in computer experiments, most of the focus had been on the theoretical\nand algorithmic innovations for the design and analysis of computer models with\nscalar responses.\n  In this paper, we propose a computationally efficient statistical emulator\nfor a large-scale dynamic computer simulator (i.e., simulator which gives time\nseries outputs). The main idea is to first find a good local neighbourhood for\nevery input location, and then emulate the simulator output via a singular\nvalue decomposition (SVD) based Gaussian process (GP) model. We develop a new\ndesign criterion for sequentially finding this local neighbourhood set of\ntraining points. Several test functions and a real-life application have been\nused to demonstrate the performance of the proposed approach over a naive\nmethod of choosing local neighbourhood set using the Euclidean distance among\ndesign points.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 05:19:13 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 08:34:59 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Zhang", "Ru", ""], ["Lin", "Chunfang Devon", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1611.09509", "submitter": "Zhibing He", "authors": "Yang Li, Yuetian Luo, Davide Ferrari, Xiaonan Hu and Yichen Qin", "title": "Model Confidence Bounds for Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce the concept of model confidence bounds (MCB)\nfor variable selection in the context of nested models. Similarly to the\nendpoints in the familiar confidence interval for parameter estimation, the MCB\nidentifies two nested models (upper and lower confidence bound models)\ncontaining the true model at a given level of confidence. Instead of trusting a\nsingle selected model obtained from a given model selection method, the MCB\nproposes a group of nested models as candidates and the MCB's width and\ncomposition enable the practitioner to assess the overall model selection\nuncertainty. A new graphical tool --- the model uncertainty curve (MUC) --- is\nintroduced to visualize the variability of model selection and to compare\ndifferent model selection procedures. The MCB methodology is implemented by a\nfast bootstrap algorithm that is shown to yield the correct asymptotic coverage\nunder rather general conditions. Our Monte Carlo simulations and real data\nexamples confirm the validity and illustrate the advantages of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 06:50:12 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 10:10:48 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 11:27:08 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Li", "Yang", ""], ["Luo", "Yuetian", ""], ["Ferrari", "Davide", ""], ["Hu", "Xiaonan", ""], ["Qin", "Yichen", ""]]}, {"id": "1611.09727", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho and Piotr Fryzlewicz", "title": "Multiscale and multilevel technique for consistent segmentation of\n  nonstationary time series", "comments": null, "journal-ref": "Statistica Sinica (2012), 22, 207-229", "doi": "10.5705/ss.2009.280", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fast, well-performing, and consistent method for\nsegmenting a piecewise-stationary, linear time series with an unknown number of\nbreakpoints. The time series model we use is the nonparametric Locally\nStationary Wavelet model, in which a complete description of the\npiecewise-stationary second-order structure is provided by wavelet periodograms\ncomputed at multiple scales and locations. The initial stage of our method is a\nnew binary segmentation procedure, with a theoretically justified and rapidly\ncomputable test criterion that detects breakpoints in wavelet periodograms\nseparately at each scale. This is followed by within-scale and across-scales\npost-processing steps, leading to consistent estimation of the number and\nlocations of breakpoints in the second-order structure of the original process.\nAn extensive simulation study demonstrates good performance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 17:05:00 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Cho", "Haeran", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1611.09790", "submitter": "Xu Chen", "authors": "Xu Chen, Shaan Qamar and Surya T. Tokdar", "title": "Paired-move multiple-try stochastic search for Bayesian variable\n  selection", "comments": "28 pages; 5 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is a key issue when analyzing high-dimensional data. The\nexplosion of data with large sample sizes and dimensionality brings new\nchallenges to this problem in both inference accuracy and computational\ncomplexity. To alleviate these problems, we propose a new scalable Markov chain\nMonte Carlo (MCMC) sampling algorithm for \"large $p$ small $n$\" scenarios by\ngeneralizing multiple-try Metropolis to discrete model spaces and further\nincorporating neighborhood-based stochastic search. The proof of reversibility\nof the proposed MCMC algorithm is provided. Extensive simulation studies are\nperformed to examine the efficiency of the new algorithm compared with existing\nmethods. A real data example is provided to illustrate the prediction\nperformances of the new algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:01:06 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Chen", "Xu", ""], ["Qamar", "Shaan", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1611.09804", "submitter": "Holger Dette", "authors": "Holger Dette, Andrey Pepelyshev, Anatoly Zhigljavsky", "title": "Best linear unbiased estimators in continuous time regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the problem of best linear unbiased estimation is investigated\nfor continuous-time regression models. We prove several general statements\nconcerning the explicit form of the best linear unbiased estimator (BLUE), in\nparticular when the error process is a smooth process with one or several\nderivatives of the response process available for construction of the\nestimators. We derive the explicit form of the BLUE for many specific models\nincluding the cases of continuous autoregressive errors of order two and\nintegrated error processes (such as integrated Brownian motion). The results\nare illustrated by several examples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:40:09 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Dette", "Holger", ""], ["Pepelyshev", "Andrey", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1611.09890", "submitter": "Hsin-Hsiung Huang", "authors": "Hsin-Hsiung Huang and Jie Yang", "title": "An Affine-Invariant Bayesian Cluster Process", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify clusters of objects with features transformed by unknown\naffine transformations, we develop a Bayesian cluster process which is\ninvariant with respect to certain linear transformations of the feature space\nand able to cluster data without knowing the number of clusters in advance.\nSpecifically, our proposed method can identify clusters invariant to orthogonal\ntransformations under model I, invariant to scaling-coordinate orthogonal\ntransformations under model II, or invariant to arbitrary non-singular linear\ntransformations under model III. The proposed split-merge algorithm leads to an\nirreducible and aperiodic Markov chain, which is also efficient at identifying\nclusters reasonably well for various applications. We illustrate the\napplications of our approach to both synthetic and real data such as leukemia\ngene expression data for model I; wine data and two half-moons benchmark data\nfor model II; three-dimensional Denmark road geographic coordinate system data\nand an arbitrary non-singular transformed two half-moons data for model III.\nThese examples show that the proposed method could be widely applied in many\nfields, especially for finding the number of clusters and identifying clusters\nof samples of interest in aerial photography and genomic data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 21:25:54 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Huang", "Hsin-Hsiung", ""], ["Yang", "Jie", ""]]}, {"id": "1611.09925", "submitter": "Linbo Wang", "authors": "Linbo Wang and Eric Tchetgen Tchetgen", "title": "Bounded, efficient and multiply robust estimation of average treatment\n  effects using instrumental variables", "comments": "To appear in Journal of the Royal Statistical Society: Series B\n  (Statistical Methodology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables (IVs) are widely used for estimating causal effects in\nthe presence of unmeasured confounding. Under the standard IV model, however,\nthe average treatment effect (ATE) is only partially identifiable. To address\nthis, we propose novel assumptions that allow for identification of the ATE.\nOur identification assumptions are clearly separated from model assumptions\nneeded for estimation, so that researchers are not required to commit to a\nspecific observed data model in establishing identification. We then construct\nmultiple estimators that are consistent under three different observed data\nmodels, and multiply robust estimators that are consistent in the union of\nthese observed data models. We pay special attention to the case of binary\noutcomes, for which we obtain bounded estimators of the ATE that are guaranteed\nto lie between -1 and 1. Our approaches are illustrated with simulations and a\ndata analysis evaluating the causal effect of education on earnings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 22:35:01 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 19:29:58 GMT"}, {"version": "v3", "created": "Sun, 3 Dec 2017 23:24:59 GMT"}, {"version": "v4", "created": "Thu, 4 Jan 2018 09:29:20 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Wang", "Linbo", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1611.09972", "submitter": "Asad Haris", "authors": "Asad Haris, Ali Shojaie, Noah Simon", "title": "Nonparametric Regression with Adaptive Truncation via a Convex\n  Hierarchical Penalty", "comments": null, "journal-ref": "Biometrika 2018, Vol. 106, No. 1, 87-107", "doi": "10.1093/biomet/asy056", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of non-parametric regression with a potentially large\nnumber of covariates. We propose a convex, penalized estimation framework that\nis particularly well-suited for high-dimensional sparse additive models. The\nproposed approach combines appealing features of finite basis representation\nand smoothing penalties for non-parametric estimation. In particular, in the\ncase of additive models, a finite basis representation provides a parsimonious\nrepresentation for fitted functions but is not adaptive when component\nfunctions posses different levels of complexity. On the other hand, a smoothing\nspline type penalty on the component functions is adaptive but does not offer a\nparsimonious representation of the estimated function. The proposed approach\nsimultaneously achieves parsimony and adaptivity in a computationally efficient\nframework. We demonstrate these properties through empirical studies on both\nreal and simulated datasets. We show that our estimator converges at the\nminimax rate for functions within a hierarchical class. We further establish\nminimax rates for a large class of sparse additive models. The proposed method\nis implemented using an efficient algorithm that scales similarly to the Lasso\nwith the number of covariates and samples size.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 02:22:31 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 22:28:45 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 15:43:02 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 12:22:21 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Haris", "Asad", ""], ["Shojaie", "Ali", ""], ["Simon", "Noah", ""]]}, {"id": "1611.10221", "submitter": "Ottmar Cronie", "authors": "O. Cronie, M.N.M. van Lieshout", "title": "Bandwidth selection for kernel estimators of the spatial intensity\n  function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss and compare various approaches to the problem of bandwidth\nselection for kernel estimators of intensity functions of spatial point\nprocesses. We also propose a new method based on the Campbell formula applied\nto the reciprocal intensity function. The new method is fully non-parametric,\ndoes not require knowledge of the product densities, and is not restricted to a\nspecific class of point process models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 15:32:26 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Cronie", "O.", ""], ["van Lieshout", "M. N. M.", ""]]}, {"id": "1611.10242", "submitter": "Owen Thomas", "authors": "Owen Thomas, Ritabrata Dutta, Jukka Corander, Samuel Kaski, Michael U.\n  Gutmann", "title": "Likelihood-free inference by ratio estimation", "comments": "Accepted to Bayesian Analysis (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of parametric statistical inference when likelihood\ncomputations are prohibitively expensive but sampling from the model is\npossible. Several so-called likelihood-free methods have been developed to\nperform inference in the absence of a likelihood function. The popular\nsynthetic likelihood approach infers the parameters by modelling summary\nstatistics of the data by a Gaussian probability distribution. In another\npopular approach called approximate Bayesian computation, the inference is\nperformed by identifying parameter values for which the summary statistics of\nthe simulated data are close to those of the observed data. Synthetic\nlikelihood is easier to use as no measure of `closeness' is required but the\nGaussianity assumption is often limiting. Moreover, both approaches require\njudiciously chosen summary statistics. We here present an alternative inference\napproach that is as easy to use as synthetic likelihood but not as restricted\nin its assumptions, and that, in a natural way, enables automatic selection of\nrelevant summary statistic from a large set of candidates. The basic idea is to\nframe the problem of estimating the posterior as a problem of estimating the\nratio between the data generating distribution and the marginal distribution.\nThis problem can be solved by logistic regression, and including regularising\npenalty terms enables automatic selection of the summary statistics relevant to\nthe inference task. We illustrate the general theory on canonical examples and\nemploy it to perform inference for challenging stochastic nonlinear dynamical\nsystems and high-dimensional summary statistics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 16:03:07 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 19:38:59 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 15:44:21 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 10:14:43 GMT"}, {"version": "v5", "created": "Wed, 16 Oct 2019 15:09:31 GMT"}, {"version": "v6", "created": "Fri, 11 Sep 2020 09:28:01 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Thomas", "Owen", ""], ["Dutta", "Ritabrata", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1611.10359", "submitter": "Shinichiro Shirota Mr", "authors": "Shinichiro Shirota and Alan E. Gelfand", "title": "Inference for log Gaussian Cox processes using an approximate marginal\n  posterior", "comments": "previous version is \"Approximate Marginal Posterior for Log Gaussian\n  Cox Processes\". arXiv admin note: text overlap with arXiv:1606.07984", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log Gaussian Cox process is a flexible class of point pattern models for\ncapturing spatial and spatio-temporal dependence for point patterns. Model\nfitting requires approximation of stochastic integrals which is implemented\nthrough discretization of the domain of interest. With fine scale\ndiscretization, inference based on Markov chain Monte Carlo is computationally\nheavy because of the cost of repeated iteration or inversion or Cholesky\ndecomposition (cubic order) of high dimensional covariance matrices associated\nwith latent Gaussian variables. Furthermore, hyperparameters for latent\nGaussian variables have strong dependence with sampled latent Gaussian\nvariables. Altogether, standard Markov chain Monte Carlo strategies are\ninefficient and not well behaved.\n  In this paper, we propose an efficient computational strategy for fitting and\ninferring with spatial log Gaussian Cox processes. The proposed algorithm is\nbased on a pseudo-marginal Markov chain Monte Carlo approach. We estimate an\napproximate marginal posterior for parameters of log Gaussian Cox processes and\npropose comprehensive model inference strategy. We provide details for all of\nthe above along with some simulation investigation for the univariate and\nmultivariate settings. As an example, we present an analysis of a point pattern\nof locations of three tree species, exhibiting positive and negative\ninteraction between different species.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:59:10 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Gelfand", "Alan E.", ""]]}]