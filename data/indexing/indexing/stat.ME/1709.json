[{"id": "1709.00092", "submitter": "Emre Demirkaya", "authors": "Yingying Fan, Emre Demirkaya, Gaorong Li and Jinchi Lv", "title": "RANK: Large-Scale Inference with Graphical Nonlinear Knockoffs", "comments": "37 pages, 6 tables, 9 pages supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power and reproducibility are key to enabling refined scientific discoveries\nin contemporary big data applications with general high-dimensional nonlinear\nmodels. In this paper, we provide theoretical foundations on the power and\nrobustness for the model-free knockoffs procedure introduced recently in\nCand\\`{e}s, Fan, Janson and Lv (2016) in high-dimensional setting when the\ncovariate distribution is characterized by Gaussian graphical model. We\nestablish that under mild regularity conditions, the power of the oracle\nknockoffs procedure with known covariate distribution in high-dimensional\nlinear models is asymptotically one as sample size goes to infinity. When\nmoving away from the ideal case, we suggest the modified model-free knockoffs\nmethod called graphical nonlinear knockoffs (RANK) to accommodate the unknown\ncovariate distribution. We provide theoretical justifications on the robustness\nof our modified procedure by showing that the false discovery rate (FDR) is\nasymptotically controlled at the target level and the power is asymptotically\none with the estimated covariate distribution. To the best of our knowledge,\nthis is the first formal theoretical result on the power for the knockoffs\nprocedure. Simulation results demonstrate that compared to existing approaches,\nour method performs competitively in both FDR control and power. A real data\nset is analyzed to further assess the performance of the suggested knockoffs\nprocedure.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 21:50:52 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Fan", "Yingying", ""], ["Demirkaya", "Emre", ""], ["Li", "Gaorong", ""], ["Lv", "Jinchi", ""]]}, {"id": "1709.00232", "submitter": "Nina Munkholt Jakobsen", "authors": "Nina Munkholt Jakobsen and Michael S{\\o}rensen", "title": "Estimating functions for jump-diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic theory for approximate martingale estimating functions is\ngeneralised to diffusions with finite-activity jumps, when the sampling\nfrequency and terminal sampling time go to infinity. Rate optimality and\nefficiency are of particular concern. Under mild assumptions, it is shown that\nestimators of drift, diffusion, and jump parameters are consistent and\nasymptotically normal, as well as rate-optimal for the drift and jump\nparameters. Additional conditions are derived, which ensure rate-optimality for\nthe diffusion parameter as well as efficiency for all parameters. The findings\nindicate a potentially fruitful direction for the further development of\nestimation for jump-diffusions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 10:18:17 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 20:09:20 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jakobsen", "Nina Munkholt", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1709.00453", "submitter": "Dewei Zhong", "authors": "Dewei Zhong and John Kolassa", "title": "Moments and Cumulants of The Two-Stage Mann-Whitney Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper illustrates how to calculate the moments and cumulants of the\ntwo-stage Mann-Whitney statistic. These results may be used to calculate the\nasymptotic critical values of the two-stage Mann-Whitney test. In this paper, a\nlarge amount of deductions will be showed.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 19:32:28 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhong", "Dewei", ""], ["Kolassa", "John", ""]]}, {"id": "1709.00623", "submitter": "John Aston", "authors": "D. Pigoli, J.A.D. Aston, F. Ferraty, A. Mazumder, C. Richards and\n  M.J.R. Hall", "title": "Estimation of temperature-dependent growth profiles for the assessment\n  of time of hatching in forensic entomology", "comments": "33 pages; 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic entomology contributes important information to crime scene\ninvestigations. In this paper, we propose a method to estimate the hatching\ntime of larvae (or maggots) based on their lengths, the temperature profile at\nthe crime scene and experimental data on larval development. This requires the\nestimation of a time-dependent growth curve from experiments where larvae have\nbeen exposed to a relatively small number of constant temperature profiles.\nSince the temperature influences the developmental speed, a crucial step is the\ntime alignment of the curves at different temperatures. We propose a model for\ntime varying temperature profiles based on the local growth rate estimated from\nthe experimental data. This allows us to estimate the most likely hatching time\nfor a sample of larvae from the crime scene. Asymptotic properties are provided\nfor the estimators of the growth curves and the hatching time. We explore via\nsimulations the robustness of the method to errors in the estimated temperature\nprofile. We also apply the methodology to data from two criminal cases from the\nUnited Kingdom.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 20:07:45 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Pigoli", "D.", ""], ["Aston", "J. A. D.", ""], ["Ferraty", "F.", ""], ["Mazumder", "A.", ""], ["Richards", "C.", ""], ["Hall", "M. J. R.", ""]]}, {"id": "1709.00629", "submitter": "Denis Belomestny", "authors": "Denis Belomestny, Alexander Goldenshluger", "title": "Nonparametric density estimation from observations with multiplicative\n  measurement errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of pointwise density estimation from\nobservations with multiplicative measurement errors. We elucidate the main\nfeature of this problem: the influence of the estimation point on the\nestimation accuracy. In particular, we show that, depending on whether this\npoint is separated away from zero or not, there are two different regimes in\nterms of the rates of convergence of the minimax risk. In both regimes we\ndevelop kernel--type density estimators and prove upper bounds on their maximal\nrisk over suitable nonparametric classes of densities. We show that the\nproposed estimators are rate--optimal by establishing matching lower bounds on\nthe minimax risk. Finally we test our estimation procedures on simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 20:40:43 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 09:21:34 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Belomestny", "Denis", ""], ["Goldenshluger", "Alexander", ""]]}, {"id": "1709.00640", "submitter": "Hao Zhou", "authors": "Hao Henry Zhou, Yilin Zhang, Vamsi K. Ithapu, Sterling C. Johnson,\n  Grace Wahba, Vikas Singh", "title": "When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests,\n  $\\ell_2$-consistency and Neuroscience Applications", "comments": "34th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies in biomedical and health sciences involve small sample sizes due\nto logistic or financial constraints. Often, identifying weak (but\nscientifically interesting) associations between a set of predictors and a\nresponse necessitates pooling datasets from multiple diverse labs or groups.\nWhile there is a rich literature in statistical machine learning to address\ndistributional shifts and inference in multi-site datasets, it is less clear\n${\\it when}$ such pooling is guaranteed to help (and when it does not) --\nindependent of the inference algorithms we use. In this paper, we present a\nhypothesis test to answer this question, both for classical and high\ndimensional linear regression. We precisely identify regimes where pooling\ndatasets across multiple sites is sensible, and how such policy decisions can\nbe made via simple checks executable on each site before any data transfer ever\nhappens. With a focus on Alzheimer's disease studies, we present empirical\nresults showing that in regimes suggested by our analysis, pooling a local\ndataset with data from an international study improves power.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 22:17:31 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhou", "Hao Henry", ""], ["Zhang", "Yilin", ""], ["Ithapu", "Vamsi K.", ""], ["Johnson", "Sterling C.", ""], ["Wahba", "Grace", ""], ["Singh", "Vikas", ""]]}, {"id": "1709.00673", "submitter": "Saeid Rezakhah", "authors": "N. Modarresi and S. Rezakhah", "title": "Hurst estimation of scale invariant processes with drift and stationary\n  increments", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characteristic feature of the discrete scale invariant (DSI) processes is\nthe invariance of their finite dimensional distributions by dilation for\ncertain scaling factor. DSI process with piecewise linear drift and stationary\nincrements inside prescribed scale intervals is introduced and studied. To\nidentify the structure of the process, first we determine the scale intervals,\ntheir linear drifts and eliminate them. Then a new method for the estimation of\nthe Hurst parameter of such DSI processes is presented and applied to some\nperiod of the Dow Jones indices. This method is based on fixed number equally\nspaced samples inside successive scale intervals. We also present some\nefficient method for estimating Hurst parameter of self-similar processes with\nstationary increments. We compare the performance of this method with the\ncelebrated FA, DFA and DMA on the simulated data of fractional Brownian motion.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 06:55:15 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Modarresi", "N.", ""], ["Rezakhah", "S.", ""]]}, {"id": "1709.00771", "submitter": "Aurya Javeed", "authors": "Aurya Javeed and Giles Hooker", "title": "Timing Observations of Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a problem in experimental design: We consider It\\^o\ndiffusions specified by some $\\theta \\in \\mathbb{R}$ and assume that we are\nallowed to observe their sample paths only $n$ times before a terminal time\n$\\tau < \\infty$. We propose a policy for timing these observations to optimally\nestimate $\\theta$. Our policy is adaptive (meaning it leverages earlier\nobservations), and it maximizes the expected Fisher information for $\\theta$\ncarried by the observations. In numerical studies, this design reduces the\nvariation of estimated parameters by as much as 75% relative to observations\nspaced uniformly in time. The policy depends on the value of the parameter\nbeing estimated, so we also discuss strategies for incorporating Bayesian\npriors over $\\theta$.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 21:42:11 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Javeed", "Aurya", ""], ["Hooker", "Giles", ""]]}, {"id": "1709.00872", "submitter": "Michail Papathomas Dr", "authors": "Michail Papathomas", "title": "On synthetic data with predetermined subject partitioning and cluster\n  profiling, and pre-specified categorical variable marginal dependence\n  structure", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard approach for assessing the performance of partition or mixture\nmodels is to create synthetic data sets with a pre-specified clustering\nstructure, and assess how well the model reveals this structure. A common\nformat is that subjects are assigned to different clusters, with variable\nobservations simulated so that subjects within the same cluster have similar\nprofiles, allowing for some variability. In this manuscript, we consider\nobservations from nominal, ordinal and interval categorical variables.\nTheoretical and empirical results are utilized to explore the dependence\nstructure between the variables, in relation to the clustering structure for\nthe subjects. A novel approach is proposed that allows to control the marginal\nassociation or correlation structure of the variables, and to specify exact\ncorrelation values. Practical examples are shown and additional theoretical\nresults are derived for interval data, commonly observed in cohort studies,\nincluding observations that emulate Single Nucleotide Polymorphisms. We compare\na synthetic dataset to a real one, to demonstrate similarities and differences.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 09:14:09 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 11:41:06 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Papathomas", "Michail", ""]]}, {"id": "1709.00918", "submitter": "Jose Jimenez", "authors": "Jose L. Jimenez, Mourad Tighiouart and Mauro Gasparini", "title": "Cancer phase I trial design using drug combinations when a fraction of\n  dose limiting toxicities is attributable to one or more agents", "comments": null, "journal-ref": "Biometrical Journal (2018)", "doi": "10.1002/bimj.201700166", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug combination trials are increasingly common nowadays in clinical\nresearch. However, very few methods have been developed to consider toxicity\nattributions in the dose escalation process. We are motivated by a trial in\nwhich the clinician is able to identify certain toxicities that can be\nattributed to one of the agents. We present a Bayesian adaptive design in which\ntoxicity attributions are modeled via Copula regression and the maximum\ntolerated dose (MTD) curve is estimated as a function of model parameters. The\ndose escalation algorithm uses cohorts of two patients, following the continual\nreassessment method (CRM) scheme, where at each stage of the trial, we search\nfor the dose of one agent given the current dose of the other agent. The\nperformance of the design is studied by evaluating its operating\ncharacteristics when the underlying model is either correctly specified or\nmisspecified. We show that this method can be extended to accommodate discrete\ndose combinations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 12:28:39 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 09:49:53 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2018 09:03:36 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Jimenez", "Jose L.", ""], ["Tighiouart", "Mourad", ""], ["Gasparini", "Mauro", ""]]}, {"id": "1709.00960", "submitter": "Andreas Futschik", "authors": "Andreas Futschik, Thomas Taus and Sonja Zehetmayer", "title": "An omnibus test for the global null hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global hypothesis tests are a useful tool in the context of, e.g, clinical\ntrials, genetic studies or meta analyses, when researchers are not interested\nin testing individual hypotheses, but in testing whether none of the hypotheses\nis false. There are several possibilities how to test the global null\nhypothesis when the individual null hypotheses are independent. If it is\nassumed that many of the individual null hypotheses are false, combinations\ntests have been recommended to maximise power. If, however, it is assumed that\nonly one or a few null hypotheses are false, global tests based on individual\ntest statistics are more powerful (e.g., Bonferroni or Simes test). However,\nusually there is no a-priori knowledge on the number of false individual null\nhypotheses. We therefore propose an omnibus test based on the combination of\np-values. We show that this test yields an impressive overall performance. The\nproposed method is implemented in the R-package omnibus.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 13:53:02 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Futschik", "Andreas", ""], ["Taus", "Thomas", ""], ["Zehetmayer", "Sonja", ""]]}, {"id": "1709.00981", "submitter": "Takuya Ura", "authors": "Yuya Sasaki, Takuya Ura", "title": "Estimation and Inference for Moments of Ratios with Robustness against\n  Large Trimming Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical researchers often trim observations with small denominator A when\nthey estimate moments of the form E[B/A]. Large trimming is a common practice\nto mitigate variance, but it incurs large trimming bias. This paper provides a\nnovel method of correcting large trimming bias. If a researcher is willing to\nassume that the joint distribution between A and B is smooth, then a large\ntrimming bias may be estimated well. With the bias correction, we also develop\na valid and robust inference result for E[B/A].\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 14:40:09 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 19:46:49 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 03:31:11 GMT"}, {"version": "v4", "created": "Sat, 30 Dec 2017 19:34:53 GMT"}, {"version": "v5", "created": "Fri, 13 Apr 2018 16:09:20 GMT"}, {"version": "v6", "created": "Thu, 18 Oct 2018 16:29:35 GMT"}, {"version": "v7", "created": "Mon, 11 Jan 2021 16:01:28 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sasaki", "Yuya", ""], ["Ura", "Takuya", ""]]}, {"id": "1709.01050", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Eric Tchetgen Tchetgen, and Ryan Andrews", "title": "Modeling Interference Via Symmetric Treatment Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical causal inference assumes treatments meant for a given unit do not\nhave an effect on other units. This assumption is violated in interference\nproblems, where new types of spillover causal effects arise, and causal\ninference becomes much more difficult. In addition, interference introduces a\nunique complication where variables may transmit treatment influences to each\nother, which is a relationship that has some features of a causal one, but is\nsymmetric. In settings where a natural causal ordering on variables is not\navailable, addressing this complication using statistical inference methods\nbased on Directed Acyclic Graphs (DAGs) leads to conceptual difficulties. In\nthis paper, we develop a new approach to decomposing the spillover effect into\nunit-specific components that extends the DAG based treatment decomposition\napproach to mediation of Robins and Richardson. We give conditions for these\ncomponents of the spillover effect to be identified in a natural type of causal\nmodel that permits stable symmetric relations among outcomes induced by a\nprocess in equilibrium. We discuss statistical inference for identified\ncomponents of the spillover effect, including a maximum likelihood estimator,\nand a doubly robust estimator for the special case of two interacting outcomes.\nWe verify consistency and robustness of our estimators via a simulation study,\nand illustrate our method by assessing the causal effect of education\nattainment on depressive symptoms using the data on households from the\nWisconsin Longitudinal Study.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 17:25:51 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 22:16:14 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Shpitser", "Ilya", ""], ["Tchetgen", "Eric Tchetgen", ""], ["Andrews", "Ryan", ""]]}, {"id": "1709.01139", "submitter": "Stephen Bates", "authors": "Stephen Bates and Robert Tibshirani", "title": "Log-ratio Lasso: Scalable, Sparse Estimation for Log-ratio Models", "comments": null, "journal-ref": "Biometrics 109 (2019) 613-624", "doi": "10.1111/biom.12995", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive-valued signal data is common in many biological and medical\napplications, where the data are often generated from imaging techniques such\nas mass spectrometry. In such a setting, the relative intensities of the raw\nfeatures are often the scientifically meaningful quantities, so it is of\ninterest to identify relevant features that take the form of log-ratios of the\nraw inputs. When including the log-ratios of all pairs of predictors, the\ndimensionality of this predictor space becomes large, so computationally\nefficient statistical procedures are required. We introduce an embedding of the\nlog-ratio parameter space into a space of much lower dimension and develop\nefficient penalized fitting procedure using this more tractable representation.\nThis procedure serves as the foundation for a two-step fitting procedure that\ncombines a convex filtering step with a second non-convex pruning step to yield\nhighly sparse solutions. On a cancer proteomics data set we find that these\nmethods fit highly sparse models with log-ratio features of known biological\nrelevance while greatly improving upon the predictive accuracy of less\ninterpretable methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 20:04:39 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bates", "Stephen", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1709.01445", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi and Matteo Luciani", "title": "Common factors, trends, and cycles in large datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a non-stationary dynamic factor model for large datasets\nto disentangle long-run from short-run co-movements. We first propose a new\nQuasi Maximum Likelihood estimator of the model based on the Kalman Smoother\nand the Expectation Maximisation algorithm. The asymptotic properties of the\nestimator are discussed. Then, we show how to separate trends and cycles in the\nfactors by mean of eigenanalysis of the estimated non-stationary factors.\nFinally, we employ our methodology on a panel of US quarterly macroeconomic\nindicators to estimate aggregate real output, or Gross Domestic Output, and the\noutput gap.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:16:53 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 12:24:37 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Luciani", "Matteo", ""]]}, {"id": "1709.01447", "submitter": "Jakob Runge", "authors": "Jakob Runge", "title": "Conditional independence testing based on a nearest-neighbor estimator\n  of conditional mutual information", "comments": "17 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional independence testing is a fundamental problem underlying causal\ndiscovery and a particularly challenging task in the presence of nonlinear and\nhigh-dimensional dependencies. Here a fully non-parametric test for continuous\ndata based on conditional mutual information combined with a local permutation\nscheme is presented. Through a nearest neighbor approach, the test efficiently\nadapts also to non-smooth distributions due to strongly nonlinear dependencies.\nNumerical experiments demonstrate that the test reliably simulates the null\ndistribution even for small sample sizes and with high-dimensional conditioning\nsets. The test is better calibrated than kernel-based tests utilizing an\nanalytical approximation of the null distribution, especially for non-smooth\ndensities, and reaches the same or higher power levels. Combining the local\npermutation scheme with the kernel tests leads to better calibration, but\nsuffers in power. For smaller sample sizes and lower dimensions, the test is\nfaster than random fourier feature-based kernel tests if the permutation scheme\nis (embarrassingly) parallelized, but the runtime increases more sharply with\nsample size and dimensionality. Thus, more theoretical research to analytically\napproximate the null distribution and speed up the estimation for larger sample\nsizes is desirable.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:21:25 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Runge", "Jakob", ""]]}, {"id": "1709.01449", "submitter": "Jonah Gabry", "authors": "Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, Andrew\n  Gelman", "title": "Visualization in Bayesian workflow", "comments": "17 pages, 11 Figures. Includes supplementary material", "journal-ref": "J. R. Stat. Soc. A (2019) 182: 389-402", "doi": "10.1111/rssa.12378", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian data analysis is about more than just computing a posterior\ndistribution, and Bayesian visualization is about more than trace plots of\nMarkov chains. Practical Bayesian data analysis, like all data analysis, is an\niterative process of model building, inference, model checking and evaluation,\nand model expansion. Visualization is helpful in each of these stages of the\nBayesian workflow and it is indispensable when drawing inferences from the\ntypes of modern, high-dimensional models that are used by applied researchers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:26:13 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 19:12:48 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 18:46:49 GMT"}, {"version": "v4", "created": "Thu, 22 Feb 2018 18:10:26 GMT"}, {"version": "v5", "created": "Sat, 9 Jun 2018 00:37:21 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Gabry", "Jonah", ""], ["Simpson", "Daniel", ""], ["Vehtari", "Aki", ""], ["Betancourt", "Michael", ""], ["Gelman", "Andrew", ""]]}, {"id": "1709.01577", "submitter": "Isabel Fulcher", "authors": "Eric J. Tchetgen Tchetgen, Isabel Fulcher, Ilya Shpitser", "title": "Auto-G-Computation of Causal Effects on a Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for inferring average causal effects have traditionally relied on two\nkey assumptions: (i) the intervention received by one unit cannot causally\ninfluence the outcome of another; and (ii) units can be organized into\nnon-overlapping groups such that outcomes of units in separate groups are\nindependent. In this paper, we develop new statistical methods for causal\ninference based on a single realization of a network of connected units for\nwhich neither assumption (i) nor (ii) holds. The proposed approach allows both\nfor arbitrary forms of interference, whereby the outcome of a unit may depend\non interventions received by other units with whom a network path through\nconnected units exists; and long range dependence, whereby outcomes for any two\nunits likewise connected by a path in the network may be dependent. Under\nnetwork versions of consistency and no unobserved confounding, inference is\nmade tractable by an assumption that the network's outcome, treatment and\ncovariate vectors are a single realization of a certain chain graph model. This\nassumption allows inferences about various network causal effects via the\nauto-g-computation algorithm, a network generalization of Robins' well-known\ng-computation algorithm previously described for causal inference under\nassumptions (i) and (ii).\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 19:59:41 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 20:11:54 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 12:38:20 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Tchetgen", "Eric J. Tchetgen", ""], ["Fulcher", "Isabel", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1709.01589", "submitter": "Bruno Sudret", "authors": "S. Marelli and B. Sudret", "title": "An active-learning algorithm that combines sparse polynomial chaos\n  expansions and bootstrap for structural reliability analysis", "comments": null, "journal-ref": "Structural Safety, 75, pp. 67-74 (2018)", "doi": "10.1016/j.strusafe.2018.06.003", "report-no": "RSUQ-2017-009", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos expansions (PCE) have seen widespread use in the context of\nuncertainty quantification. However, their application to structural\nreliability problems has been hindered by the limited performance of PCE in the\ntails of the model response and due to the lack of local metamodel error\nestimates. We propose a new method to provide local metamodel error estimates\nbased on bootstrap resampling and sparse PCE. An initial experimental design is\niteratively updated based on the current estimation of the limit-state surface\nin an active learning algorithm. The greedy algorithm uses the bootstrap-based\nlocal error estimates for the polynomial chaos predictor to identify the best\ncandidate set of points to enrich the experimental design. We demonstrate the\neffectiveness of this approach on a well-known analytical benchmark\nrepresenting a series system, on a truss structure and on a complex realistic\nframe structure problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 20:44:32 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 16:52:07 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "1709.01781", "submitter": "Neil Chada", "authors": "Neil K. Chada, Marco A. Iglesias, Lassi Roininen and Andrew M. Stuart", "title": "Parameterizations for Ensemble Kalman Inversion", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aab6d9", "report-no": null, "categories": "math.NA math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of ensemble methods to solve inverse problems is attractive because\nit is a derivative-free methodology which is also well-adapted to\nparallelization. In its basic iterative form the method produces an ensemble of\nsolutions which lie in the linear span of the initial ensemble. Choice of the\nparameterization of the unknown field is thus a key component of the success of\nthe method. We demonstrate how both geometric ideas and hierarchical ideas can\nbe used to design effective parameterizations for a number of applied inverse\nproblems arising in electrical impedance tomography, groundwater flow and\nsource inversion. In particular we show how geometric ideas, including the\nlevel set method, can be used to reconstruct piecewise continuous fields, and\nwe show how hierarchical methods can be used to learn key parameters in\ncontinuous fields, such as length-scales, resulting in improved\nreconstructions. Geometric and hierarchical ideas are combined in the level set\nmethod to find piecewise constant reconstructions with interfaces of unknown\ntopology.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:44:34 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 07:35:07 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Chada", "Neil K.", ""], ["Iglesias", "Marco A.", ""], ["Roininen", "Lassi", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1709.02069", "submitter": "Dengdeng Yu", "authors": "Dengdeng Yu, Linglong Kong and Ivan Mizera", "title": "An Alternative Approach to Functional Linear Partial Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have previously proposed the partial quantile regression (PQR) prediction\nprocedure for functional linear model by using partial quantile covariance\ntechniques and developed the simple partial quantile regression (SIMPQR)\nalgorithm to efficiently extract PQR basis for estimating functional\ncoefficients. However, although the PQR approach is considered as an attractive\nalternative to projections onto the principal component basis, there are\ncertain limitations to uncovering the corresponding asymptotic properties\nmainly because of its iterative nature and the non-differentiability of the\nquantile loss function. In this article, we propose and implement an\nalternative formulation of partial quantile regression (APQR) for functional\nlinear model by using block relaxation method and finite smoothing techniques.\nThe proposed reformulation leads to insightful results and motivates new\ntheory, demonstrating consistency and establishing convergence rates by\napplying advanced techniques from empirical process theory. Two simulations and\ntwo real data from ADHD-200 sample and ADNI are investigated to show the\nsuperiority of our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 05:03:24 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Yu", "Dengdeng", ""], ["Kong", "Linglong", ""], ["Mizera", "Ivan", ""]]}, {"id": "1709.02223", "submitter": "Konstantinos Spiliopoulos", "authors": "Siragan Gailus and Konstantinos Spiliopoulos", "title": "Discrete-Time Statistical Inference for Multiscale Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical inference for small-noise-perturbed multiscale dynamical\nsystems under the assumption that we observe a single time series from the slow\nprocess only. We construct estimators for both averaging and homogenization\nregimes, based on an appropriate misspecified model motivated by a second-order\nstochastic Taylor expansion of the slow process with respect to a function of\nthe time-scale separation parameter. In the case of a fixed number of\nobservations, we establish consistency, asymptotic normality, and asymptotic\nstatistical efficiency of a minimum contrast estimator (MCE), the limiting\nvariance having been identified explicitly; we furthermore establish\nconsistency and asymptotic normality of a simplified minimum constrast\nestimator (SMCE), which is however not in general efficient. These results are\nthen extended to the case of high-frequency observations under a condition\nrestricting the rate at which the number of observations may grow vis-\\`a-vis\nthe separation of scales. Numerical simulations illustrate the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 13:12:20 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 01:18:22 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Gailus", "Siragan", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1709.02319", "submitter": "Anna Heath", "authors": "Anna Heath, Gianluca Baio", "title": "Calculating the Expected Value of Sample Information using Efficient\n  Nested Monte Carlo: A Tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The Expected Value of Sample Information (EVSI) quantifies the\neconomic benefit of reducing uncertainty in a health economic model by\ncollecting additional information. This has the potential to improve the\nallocation of research budgets. Despite this, practical EVSI evaluations are\nlimited, partly due to the computational cost of estimating this value using\nthe \"gold-standard\" nested simulation methods. Recently, however, Heath et al\ndeveloped an estimation procedure that reduces the number of simulations\nrequired for this \"gold-standard\" calculation. Up to this point, this new\nmethod has been presented in purely technical terms. Study Design: This study\npresents the practical application of this new method to aid its\nimplementation. We use a worked example to illustrate the key steps of the EVSI\nestimation procedure before discussing its optimal implementation using a\npractical health economic model. Methods: The worked example is based on a\nthree parameter linear health economic model. The more realistic model\nevaluates the cost-effectiveness of a new chemotherapy treatment which aims to\nreduce the number of side effects experienced by patients. We use a Markov\nModel structure to evaluate the health economic profile of experiencing side\neffects. Results: This EVSI estimation method offers accurate estimation within\na feasible computation time, seconds compared to days, even for more complex\nmodel structures. The EVSI estimation is more accurate if a greater number of\nnested samples are used, even for a fixed computational cost. Conclusions: This\nnew method reduces the computational cost of estimating the EVSI by nested\nsimulation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 15:45:31 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 13:36:25 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Heath", "Anna", ""], ["Baio", "Gianluca", ""]]}, {"id": "1709.02370", "submitter": "Diego Marcondes", "authors": "Diego Marcondes and Nilton Rogerio Marcondes", "title": "A Nonparametric Statistical Approach to Content Analysis of Items", "comments": null, "journal-ref": null, "doi": "10.3390/stats1010001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to use psychometric instruments to assess a multidimensional\nconstruct, we may decompose it in dimensions and, in order to assess each\ndimension, develop a set of items, so one may assess the construct as a whole,\nby assessing its dimensions. In this scenario, the content analysis of items\naims to verify if the developed items are assessing the dimension they are\nsupposed to. In the content analysis process, it is customary to request the\njudgement of specialists in the studied construct about the dimension that the\ndeveloped items assess, what makes it a subjective process as it relies upon\nthe personal opinion of the specialists. This paper aims to develop a\nnonparametric statistical approach to the content analysis of items in order to\npresent a practical method to assess the consistency of the content analysis\nprocess, by the development of a statistical test that seeks to determine if\nall the specialists have the same capability to judge the items. A simulation\nstudy is conducted to assess the consistency of the test and it is applied to a\nreal validation process.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 17:41:57 GMT"}, {"version": "v2", "created": "Sun, 24 Sep 2017 17:50:56 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 19:52:44 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Marcondes", "Diego", ""], ["Marcondes", "Nilton Rogerio", ""]]}, {"id": "1709.02532", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Generalizing Distance Covariance to Measure and Test Multivariate Mutual\n  Dependence", "comments": "34 pages, 10 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three measures of mutual dependence between multiple random\nvectors. All the measures are zero if and only if the random vectors are\nmutually independent. The first measure generalizes distance covariance from\npairwise dependence to mutual dependence, while the other two measures are sums\nof squared distance covariance. All the measures share similar properties and\nasymptotic distributions to distance covariance, and capture non-linear and\nnon-monotone mutual dependence between the random vectors. Inspired by complete\nand incomplete V-statistics, we define the empirical measures and simplified\nempirical measures as a trade-off between the complexity and power when testing\nmutual independence. Implementation of the tests is demonstrated by both\nsimulation results and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 04:36:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 05:59:44 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 00:56:40 GMT"}, {"version": "v4", "created": "Sun, 24 Dec 2017 01:21:46 GMT"}, {"version": "v5", "created": "Sun, 25 Feb 2018 22:58:23 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1709.02536", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick, Michael I. Jordan", "title": "Covariances, Robustness, and Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior\ninference technique that is increasingly popular due to its fast runtimes on\nlarge-scale datasets. However, even when MFVB provides accurate posterior means\nfor certain parameters, it often mis-estimates variances and covariances.\nFurthermore, prior robustness measures have remained undeveloped for MFVB. By\nderiving a simple formula for the effect of infinitesimal model perturbations\non MFVB posterior means, we provide both improved covariance estimates and\nlocal robustness measures for MFVB, thus greatly expanding the practical\nusefulness of MFVB posterior approximations. The estimates for MFVB posterior\ncovariances rely on a result from the classical Bayesian robustness literature\nrelating derivatives of posterior expectations to posterior covariances and\ninclude the Laplace approximation as a special case. Our key condition is that\nthe MFVB approximation provides good estimates of a select subset of posterior\nmeans---an assumption that has been shown to hold in many practical settings.\nIn our experiments, we demonstrate that our methods are simple, general, and\nfast, providing accurate posterior uncertainty estimates and robustness\nmeasures with runtimes that can be an order of magnitude faster than MCMC.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 04:45:30 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 01:15:32 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 04:03:54 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1709.02673", "submitter": "Ivan Kojadinovic", "authors": "Axel B\\\"ucher, Jean-David Fermanian and Ivan Kojadinovic", "title": "Combining cumulative sum change-point detection tests for assessing the\n  stationarity of univariate time series", "comments": "45 pages, 2 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive tests of stationarity for univariate time series by combining\nchange-point tests sensitive to changes in the contemporary distribution with\ntests sensitive to changes in the serial dependence. The proposed approach\nrelies on a general procedure for combining dependent tests based on\nresampling. After proving the asymptotic validity of the combining procedure\nunder the conjunction of null hypotheses and investigating its consistency, we\nstudy rank-based tests of stationarity by combining cumulative sum change-point\ntests based on the contemporary empirical distribution function and on the\nempirical autocopula at a given lag. Extensions based on tests solely focusing\non second-order characteristics are proposed next. The finite-sample behaviors\nof all the derived statistical procedures for assessing stationarity are\ninvestigated in large-scale Monte Carlo experiments and illustrations on two\nreal data sets are provided. Extensions to multivariate time series are briefly\ndiscussed as well.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 12:24:57 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 15:07:34 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 14:16:10 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Fermanian", "Jean-David", ""], ["Kojadinovic", "Ivan", ""]]}, {"id": "1709.02675", "submitter": "Molei Liu", "authors": "Molei Liu, Ming Hu, Xiaohua Zhou", "title": "Modeling Coefficient Alpha for Measurement of Individualized Test Score\n  Internal Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for measuring individualized reliability of several tests on\nsubjects with heterogenecity is proposed. A regression model is developed based\non three sets of generalized estimating equations (GEE). The first set of GEE\nmodels the expectation of the responses, the second set of GEE models the\nresponse's variance, and the third set is proposed to estimate the\nindividualized coefficient alpha, defined and used to measure individualized\ninternal consistency of the responses. We also extend our method to handle\nmissing data in the covariates. Asymptotic property of the estimators is\ndiscussed, based on which interval estimation of the coefficient alpha and\nsignificance detection are derived. Performance of our method is evaluated\nthrough simulation study and real data analysis. The real data application is\nfrom a health literacy study in Hunan province of China.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 12:45:32 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Liu", "Molei", ""], ["Hu", "Ming", ""], ["Zhou", "Xiaohua", ""]]}, {"id": "1709.02859", "submitter": "Reimar Heinrich Leike", "authors": "Reimar H. Leike, Torsten A. En{\\ss}lin", "title": "Towards information optimal simulation of partial differential equations", "comments": null, "journal-ref": "Phys. Rev. E 97, 033314 (2018)", "doi": "10.1103/PhysRevE.97.033314", "report-no": null, "categories": "stat.ME astro-ph.IM physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most simulation schemes for partial differential equations (PDEs) focus on\nminimizing a simple error norm of a discretized version of a field. This paper\ntakes a fundamentally different approach; the discretized field is interpreted\nas data providing information about a real physical field that is unknown. This\ninformation is sought to be conserved by the scheme as the field evolves in\ntime. Such an information theoretic approach to simulation was pursued before\nby information field dynamics (IFD). In this paper we work out the theory of\nIFD for nonlinear PDEs in a noiseless Gaussian approximation. The result is an\naction that can be minimized to obtain an informationally optimal simulation\nscheme. It can be brought into a closed form using field operators to calculate\nthe appearing Gaussian integrals. The resulting simulation schemes are tested\nnumerically in two instances for the Burgers equation. Their accuracy surpasses\nfinite-difference schemes on the same resolution. The IFD scheme, however, has\nto be correctly informed on the subgrid correlation structure. In certain\nlimiting cases we recover well-known simulation schemes like spectral Fourier\nGalerkin methods. We discuss implications of the approximations made.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 21:00:51 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 15:34:52 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Leike", "Reimar H.", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1709.02899", "submitter": "Tian Zheng", "authors": "Herman Chernoff, Shaw-Hwa Lo, Tian Zheng, Adeline Lo", "title": "Estimating the theoretical error rate for prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction for very large data sets is typically carried out in two stages,\nvariable selection and pattern recognition. Ordinarily variable selection\ninvolves seeing how well individual explanatory variables are correlated with\nthe dependent variable. This practice neglects the possible interactions among\nthe variables. Simulations have shown that a statistic I, that we used for\nvariable selection is much better correlated with predictivity than\nsignificance levels. We explain this by defining theoretical predictivity and\nshow how I is related to predictivity. We calculate the biases of the\noveroptimistic training estimate of predictivity and of the pessimistic out of\nsample estimate. Corrections for the bias lead to improved estimates of the\npotential predictivity using small groups of possibly interacting variables.\nThese results support the use of I in the variable selection phase of\nprediction for data sets such as in GWAS (Genome wide association studies)\nwhere there are very many explanatory variables and modest sample sizes.\nReference is made to another publication using I, which led to a reduction in\nthe error rate of prediction from 30% to 8%, for a data set with, 4,918\nvariables and 97 subjects. This data set had been previously studied by\nscientists for over 10 years.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 03:28:47 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Chernoff", "Herman", ""], ["Lo", "Shaw-Hwa", ""], ["Zheng", "Tian", ""], ["Lo", "Adeline", ""]]}, {"id": "1709.02942", "submitter": "Thomas Ortner", "authors": "Thomas Ortner, Irene Hoffmann, Peter Filzmoser, Maia Rohm, Christian\n  Breiteneder, Sarka Brodinova", "title": "Multigroup discrimination based on weighted local projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for supervised classification analysis for high dimensional\nand flat data (more variables than observations) is proposed. We use the\ninformation of class-membership of observations to determine groups of\nobservations locally describing the group structure. By projecting the data on\nthe subspace spanned by those groups, local projections are defined based on\nthe projection concepts from Ortner et al. (2017a) and Ortner et al. (2017b).\nFor each local projection a local discriminant analysis (LDA) model is computed\nusing the information within the projection space as well as the distance to\nthe projection space. The models provide information about the quality of\nseparation for each class combination. Based on this information, weights are\ndefined for aggregating the LDA-based posterior probabilities of each subspace\nto a new overall probability. The same weights are used for classifying new\nobservations.\n  In addition to the provided methodology, implemented in the R-package lop, a\nmethod of visualizing the connectivity of groups in high-dimensional spaces is\nproposed on the basis of the posterior probabilities. A thorough evaluation is\nperformed using three different real-world datasets, underlining the strengths\nof local projection based classification and the provided visualization\nmethodology.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 11:09:03 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Ortner", "Thomas", ""], ["Hoffmann", "Irene", ""], ["Filzmoser", "Peter", ""], ["Rohm", "Maia", ""], ["Breiteneder", "Christian", ""], ["Brodinova", "Sarka", ""]]}, {"id": "1709.03016", "submitter": "Sean McGrath", "authors": "Sean McGrath, XiaoFei Zhao, Zhi Zhen Qin, Russell Steele, Andrea\n  Benedetti", "title": "One-sample aggregate data meta-analysis of medians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An aggregate data meta-analysis is a statistical method that pools the\nsummary statistics of several selected studies to estimate the outcome of\ninterest. When considering a continuous outcome, typically each study must\nreport the same measure of the outcome variable and its spread (e.g., the\nsample mean and its standard error). However, some studies may instead report\nthe median along with various measures of spread. Recently, the task of\nincorporating medians in meta-analysis has been achieved by estimating the\nsample mean and its standard error from each study that reports a median in\norder to meta-analyze the means. In this paper, we propose two alternative\napproaches to meta-analyze data that instead rely on medians. We systematically\ncompare these approaches via simulation study to each other and to methods that\ntransform the study-specific medians and spread into sample means and their\nstandard errors. We demonstrate that the proposed median-based approaches\nperform better than the transformation-based approaches, especially when\napplied to skewed data and data with high inter-study variance. In addition,\nwhen meta-analyzing data that consists of medians, we show that the\nmedian-based approaches perform considerably better than or comparably to the\nbest-case scenario for a transformation approach: conducting a meta-analysis\nusing the actual sample mean and standard error of the mean of each study.\nFinally, we illustrate these approaches in a meta-analysis of patient delay in\ntuberculosis diagnosis.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 22:54:43 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 20:22:37 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["McGrath", "Sean", ""], ["Zhao", "XiaoFei", ""], ["Qin", "Zhi Zhen", ""], ["Steele", "Russell", ""], ["Benedetti", "Andrea", ""]]}, {"id": "1709.03076", "submitter": "Mervyn O Luing Mr", "authors": "Mervyn O'Luing, Steven Prestwich, S. Armagan Tarim", "title": "A Grouping Genetic Algorithm for Joint Stratification and Sample\n  Allocation Designs", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the cheapest sample size for the optimal stratification in\nmultivariate survey design is a problem in cases where the population frame is\nlarge. A solution exists that iteratively searches for the minimum sample size\nnecessary to meet accuracy constraints in partitions of atomic strata created\nby the Cartesian product of auxiliary variables into larger strata. The optimal\nstratification can be found by testing all possible partitions. However the\nnumber of possible partitions grows exponentially with the number of initial\nstrata. There are alternative ways of modelling this problem, one of the most\nnatural is using Genetic Algorithms (GA). These evolutionary algorithms use\nrecombination, mutation and selection to search for optimal solutions. They\noften converge on optimal or near-optimal solution more quickly than exact\nmethods. We propose a new GA approach to this problem using grouping genetic\noperators instead of traditional operators. The results show a significant\nimprovement in solution quality for similar computational effort, corresponding\nto large monetary savings.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 09:47:56 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 14:16:46 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["O'Luing", "Mervyn", ""], ["Prestwich", "Steven", ""], ["Tarim", "S. Armagan", ""]]}, {"id": "1709.03154", "submitter": "Richard Samworth", "authors": "Richard J. Samworth", "title": "Recent progress in log-concave density estimation", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, log-concave density estimation via maximum likelihood\nestimation has emerged as a fascinating alternative to traditional\nnonparametric smoothing techniques, such as kernel density estimation, which\nrequire the choice of one or more bandwidths. The purpose of this article is to\ndescribe some of the properties of the class of log-concave densities on\n$\\mathbb{R}^d$ which make it so attractive from a statistical perspective, and\nto outline the latest methodological, theoretical and computational advances in\nthe area.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 18:59:00 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Samworth", "Richard J.", ""]]}, {"id": "1709.03431", "submitter": "Peida Zhan", "authors": "Peida Zhan, Hong Jiao, Dandan Liao", "title": "A Longitudinal Higher-Order Diagnostic Classification Model", "comments": "35 pages, 12 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing diagnostic feedback about growth is crucial to formative decisions\nsuch as targeted remedial instructions or interventions. This paper proposed a\nlongitudinal higher-order diagnostic classification modeling approach for\nmeasuring growth. The new modeling approach is able to provide quantitative\nvalues of overall and individual growth by constructing a multidimensional\nhigher-order latent structure to take into account the correlations among\nmultiple latent attributes that are examined across different occasions. In\naddition, potential local item dependence among anchor (or repeated) items can\nalso be taken into account. Model parameter estimation is explored in a\nsimulation study. An empirical example is analyzed to illustrate the\napplications and advantages of the proposed modeling approach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 15:16:31 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 13:24:39 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Zhan", "Peida", ""], ["Jiao", "Hong", ""], ["Liao", "Dandan", ""]]}, {"id": "1709.03452", "submitter": "Alan Heavens", "authors": "Elena Sellentin, Andrew H. Jaffe, Alan F. Heavens", "title": "On the use of the Edgeworth expansion in cosmology I: how to foresee and\n  evade its pitfalls", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear gravitational collapse introduces non-Gaussian statistics into the\nmatter fields of the late Universe. As the large-scale structure is the target\nof current and future observational campaigns, one would ideally like to have\nthe full probability density function of these non-Gaussian fields. The only\nviable way we see to achieve this analytically, at least approximately and in\nthe near future, is via the Edgeworth expansion. We hence rederive this\nexpansion for Fourier modes of non-Gaussian fields and then continue by putting\nit into a wider statistical context than previously done. We show that in its\noriginal form, the Edgeworth expansion only works if the non-Gaussian signal is\naveraged away. This is counterproductive, since we target the\nparameter-dependent non-Gaussianities as a signal of interest. We hence alter\nthe analysis at the decisive step and now provide a roadmap towards a\ncontrolled and unadulterated analysis of non-Gaussianities in structure\nformation (with the Edgeworth expansion). Our central result is that, although\nthe Edgeworth expansion has pathological properties, these can be predicted and\navoided in a careful manner. We also show that, despite the non-Gaussianity\ncoupling all modes, the Edgeworth series may be applied to any desired subset\nof modes, since this is equivalent (to the level of the approximation) to\nmarginalising over the exlcuded modes. In this first paper of a series, we\nrestrict ourselves to the sampling properties of the Edgeworth expansion,\ni.e.~how faithfully it reproduces the distribution of non-Gaussian data. A\nfollow-up paper will detail its Bayesian use, when parameters are to be\ninferred.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 15:51:59 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Sellentin", "Elena", ""], ["Jaffe", "Andrew H.", ""], ["Heavens", "Alan F.", ""]]}, {"id": "1709.03555", "submitter": "Young-Geun Choi", "authors": "Young-Geun Choi, Wei-Yann Tsai and Myunghee Cho Paik", "title": "A general class of quasi-independence tests for left-truncated\n  right-censored data", "comments": "Accepted in Statistica Sinica", "journal-ref": null, "doi": "10.5705/ss.202017.0010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survival studies, classical inferences for left-truncated data require\nquasi-independence, a property that the joint density of truncation time and\nfailure time is factorizable into their marginal densities in the observable\nregion. The quasi-independence hypothesis is testable; many authors have\ndeveloped tests for left-truncated data with or without right-censoring. In\nthis paper, we propose a class of test statistics for testing the\nquasi-independence which unifies the existing methods and generates new useful\nstatistics such as conditional Spearman's rank correlation coefficient.\nAsymptotic normality of the proposed class of statistics is given. We show that\na new set of tests can be powerful under certain alternatives by theoretical\nand empirical power comparison.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 19:22:11 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Choi", "Young-Geun", ""], ["Tsai", "Wei-Yann", ""], ["Paik", "Myunghee Cho", ""]]}, {"id": "1709.03631", "submitter": "Joan Heck Wortman", "authors": "Joan Heck Wortman and Jerome P. Reiter", "title": "Simultaneous Record Linkage and Causal Inference with Propensity Score\n  Subclassification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methodology for causal inference in observational studies when\nusing propensity score subclassification on data constructed with probabilistic\nrecord linkage techniques. We focus on scenarios where covariates and binary\ntreatment assignments are in one file and outcomes are in another file, and the\ngoal is to estimate an additive treatment effect by merging the files. We\nassume that the files can be linked using variables common to both files, e.g.,\nnames or birth dates, but that links are subject to errors, e.g., due to\nreporting errors in the linking variables. We develop methodology for cases\nwhere such reporting errors are independent of the other variables on the\nfiles. We describe conceptually how linkage errors can affect causal estimates\nin subclassification contexts. We also present and evaluate several algorithms\nfor deciding which record pairs to use in estimation of causal effects. Using\nsimulation studies, we demonstrate that some of the procedures can result in\nimproved accuracy in estimates of treatment effects from linked data compared\nto using only cases known to be true links.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 00:27:08 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 15:16:21 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Wortman", "Joan Heck", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1709.03736", "submitter": "Duco Veen", "authors": "Duco Veen, Diederick Stoel, Naomi Schalken, Rens van de Schoot", "title": "Using the Data Agreement Criterion to Rank Experts' Beliefs", "comments": null, "journal-ref": "Entropy 2018, 20", "doi": "10.3390/e20080592", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experts' beliefs embody a present state of knowledge. It is desirable to take\nthis knowledge into account when doing analyses or making decisions. Yet\nranking experts based on the merit of their beliefs is a difficult task. In\nthis paper we show how experts can be ranked based on their knowledge and their\nlevel of (un)certainty. By letting experts specify their knowledge in the form\nof a probability distribution we can assess how accurately they can predict new\ndata, and how appropriate their level of (un)certainty is. The expert's\nspecified probability distribution can be seen as a prior in a Bayesian\nstatistical setting. By extending an existing prior-data conflict measure to\nevaluate multiple priors, i.e. experts' beliefs, we can compare experts with\neach other and the data to evaluate their appropriateness. Using this method\nnew research questions can be asked and answered, for instance: Which expert\npredicts the new data best? Is there agreement between my experts and the data?\nWhich experts' representation is more valid or useful? Can we reach convergence\nbetween expert judgement and data? We provided an empirical example ranking\n(regional) directors of a large financial institution based on their\npredictions of turnover.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 08:28:00 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Veen", "Duco", ""], ["Stoel", "Diederick", ""], ["Schalken", "Naomi", ""], ["van de Schoot", "Rens", ""]]}, {"id": "1709.03794", "submitter": "Johan Segers", "authors": "Anna Kiriliouk, Johan Segers, Laleh Tafakori", "title": "An estimator of the stable tail dependence function based on the\n  empirical beta copula", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The replacement of indicator functions by integrated beta kernels in the\ndefinition of the empirical stable tail dependence function is shown to produce\na smoothed version of the latter estimator with the same asymptotic\ndistribution but superior finite-sample performance. The link of the new\nestimator with the empirical beta copula enables a simple but effective\nresampling scheme.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 11:49:04 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Kiriliouk", "Anna", ""], ["Segers", "Johan", ""], ["Tafakori", "Laleh", ""]]}, {"id": "1709.03862", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser and Sourjya Sarkar", "title": "Personalizing Path-Specific Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike classical causal inference, which often has an average causal effect\nof a treatment within a population as a target, in settings such as\npersonalized medicine, the goal is to map a given unit's characteristics to a\ntreatment tailored to maximize the expected outcome for that unit. Obtaining\nhigh-quality mappings of this type is the goal of the dynamic regime literature\n(Chakraborty and Moodie 2013), with connections to reinforcement learning and\nexperimental design. Aside from the average treatment effects, mechanisms\nbehind causal relationships are also of interest. A well-studied approach to\nmechanism analysis is establishing average effects along with a particular set\nof causal pathways, in the simplest case the direct and indirect effects.\nEstimating such effects is the subject of the mediation analysis literature\n(Robins and Greenland 1992; Pearl 2001).\n  In this paper, we consider how unit characteristics may be used to tailor a\ntreatment assignment strategy that maximizes a particular path-specific effect.\nIn healthcare applications, finding such a policy is of interest if, for\ninstance, we are interested in maximizing the chemical effect of a drug on an\noutcome (corresponding to the direct effect), while assuming drug adherence\n(corresponding to the indirect effect) is set to some reference level. To solve\nour problem, we define counterfactuals associated with path-specific effects of\na policy, give a general identification algorithm for these counterfactuals,\ngive a proof of completeness, and show how classification algorithms in machine\nlearning (Chen, Zeng, and Kosorok 2016) may be used to find a high-quality\npolicy. We validate our approach via a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 14:30:39 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Shpitser", "Ilya", ""], ["Sarkar", "Sourjya", ""]]}, {"id": "1709.03904", "submitter": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "authors": "Wilhelmiina H\\\"am\\\"al\\\"ainen and Geoffrey I. Webb", "title": "A Tutorial on Statistically Sound Pattern Discovery", "comments": "51 pages. This is a prepublication version of an open-access journal\n  paper. This version presents the original math notations that were\n  compromised in the published version", "journal-ref": "Data Mining and Knowledge Discovery, First Online 20 December 2018", "doi": "10.1007/s10618-018-0590-x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistically sound pattern discovery harnesses the rigour of statistical\nhypothesis testing to overcome many of the issues that have hampered standard\ndata mining approaches to pattern discovery. Most importantly, application of\nappropriate statistical tests allows precise control over the risk of false\ndiscoveries -- patterns that are found in the sample data but do not hold in\nthe wider population from which the sample was drawn. Statistical tests can\nalso be applied to filter out patterns that are unlikely to be useful, removing\nuninformative variations of the key patterns in the data. This tutorial\nintroduces the key statistical and data mining theory and techniques that\nunderpin this fast developing field.\n  We concentrate on two general classes of patterns: dependency rules that\nexpress statistical dependencies between condition and consequent parts and\ndependency sets that express mutual dependence between set elements. We clarify\nalternative interpretations of statistical dependence and introduce appropriate\ntests for evaluating statistical significance of patterns in different\nsituations. We also introduce special techniques for controlling the likelihood\nof spurious discoveries when multitudes of patterns are evaluated.\n  The paper is aimed at a wide variety of audiences. It provides the necessary\nstatistical background and summary of the state-of-the-art for any data mining\nresearcher or practitioner wishing to enter or understand statistically sound\npattern discovery research or practice. It can serve as a general introduction\nto the field of statistically sound pattern discovery for any reader with a\ngeneral background in data sciences.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 15:16:23 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 19:20:04 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 16:04:19 GMT"}, {"version": "v4", "created": "Fri, 4 Jan 2019 18:23:01 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Wilhelmiina", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1709.03945", "submitter": "Xin Zhang", "authors": "Xin Zhang, Qing Mai", "title": "Model-free Envelope Dimension Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An envelope is a targeted dimension reduction subspace for simultaneously\nachieving dimension reduction and improving parameter estimation efficiency.\nWhile many envelope methods have been proposed in recent years, all envelope\nmethods hinge on the knowledge of a key hyperparameter, the structural\ndimension of the envelope. How to estimate the envelope dimension consistently\nis of substantial interest from both theoretical and practical aspects.\nMoreover, very recent advances in the literature have generalized envelope as a\nmodel-free method, which makes selecting the envelope dimension even more\nchallenging. Likelihood-based approaches such as information criteria and\nlikelihood-ratio tests either cannot be directly applied or have no theoretical\njustification. To address this critical issue of dimension selection, we\npropose two unified approaches -- called FG and 1D selections -- for\ndetermining the envelope dimension that can be applied to any envelope models\nand methods. The two model-free selection approaches are based on the two\ndifferent envelope optimization procedures: the full Grassmannian (FG)\noptimization and the 1D algorithm (Cook and Zhang, 2016), and are shown to be\ncapable of correctly identifying the structural dimension with a probability\ntending to 1 under mild moment conditions as the sample size increases. While\nthe FG selection unifies and generalizes the BIC and modified BIC approaches\nthat existing in the literature, and hence provides the theoretical\njustification of them under weak moment condition and model-free context, the\n1D selection is computationally more stable and efficient in finite sample.\nExtensive simulations and a real data analysis demonstrate the superb\nperformance of our proposals.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 16:47:08 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 04:43:52 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Zhang", "Xin", ""], ["Mai", "Qing", ""]]}, {"id": "1709.04192", "submitter": "Mark van de Wiel", "authors": "Mark A. van de Wiel, Dennis E. te Beest, Magnus M\\\"unch", "title": "Learning from a lot: Empirical Bayes in high-dimensional prediction\n  settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Bayes is a versatile approach to `learn from a lot' in two ways:\nfirst, from a large number of variables and second, from a potentially large\namount of prior information, e.g. stored in public repositories. We review\napplications of a variety of empirical Bayes methods to several well-known\nmodel-based prediction methods including penalized regression, linear\ndiscriminant analysis, and Bayesian models with sparse or dense priors. We\ndiscuss `formal' empirical Bayes methods which maximize the marginal\nlikelihood, but also more informal approaches based on other data summaries. We\ncontrast empirical Bayes to cross-validation and full Bayes, and discuss hybrid\napproaches. To study the relation between the quality of an empirical Bayes\nestimator and $p$, the number of variables, we consider a simple empirical\nBayes estimator in a linear model setting.\n  We argue that empirical Bayes is particularly useful when the prior contains\nmultiple parameters which model a priori information on variables, termed\n`co-data'. In particular, we present two novel examples that allow for co-data.\nFirst, a Bayesian spike-and-slab setting that facilitates inclusion of multiple\nco-data sources and types; second, a hybrid empirical Bayes-full Bayes ridge\nregression approach for estimation of the posterior predictive interval.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 08:50:27 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 09:34:38 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["van de Wiel", "Mark A.", ""], ["Beest", "Dennis E. te", ""], ["M\u00fcnch", "Magnus", ""]]}, {"id": "1709.04333", "submitter": "Zemei Xu", "authors": "Zemei Xu, Daniel F. Schmidt, Enes Makalic, Guoqi Qian and John L.\n  Hopper", "title": "Bayesian Sparse Global-Local Shrinkage Regression for Selection of\n  Grouped Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most estimates for penalised linear regression can be viewed as posterior\nmodes for an appropriate choice of prior distribution. Bayesian shrinkage\nmethods, particularly the horseshoe estimator, have recently attracted a great\ndeal of attention in the problem of estimating sparse, high-dimensional linear\nmodels. This paper extends these ideas, and presents a Bayesian grouped model\nwith continuous global-local shrinkage priors to handle complex group\nhierarchies that include overlapping and multilevel group structures. As the\nposterior mean is never a sparse estimate of the linear model coefficients, we\nextend the recently proposed decoupled shrinkage and selection (DSS) technique\nto the problem of selecting groups of variables from posterior samples. To\nchoose a final, sparse model, we also adapt generalised information criteria\napproaches to the DSS framework. To ensure that sparse groups, in which only a\nfew predictors are active, can be effectively identified, we provide an\nalternative degrees of freedom estimator for sparse Bayesian linear models that\ntakes into account the effects of shrinkage on the model coefficients.\nSimulations and real data analysis using our proposed method show promising\nperformance in terms of correct identification of active and inactive groups,\nand prediction, in comparison with a Bayesian grouped slab-and-spike approach.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 13:58:26 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 08:49:49 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 09:15:17 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Xu", "Zemei", ""], ["Schmidt", "Daniel F.", ""], ["Makalic", "Enes", ""], ["Qian", "Guoqi", ""], ["Hopper", "John L.", ""]]}, {"id": "1709.04342", "submitter": "Chao Zheng", "authors": "Chao Zheng, Davide Ferrari and Yuhong Yang", "title": "Model Selection Confidence Sets by Likelihood Ratio Testing", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional activity of model selection aims at discovering a single\nmodel superior to other candidate models. In the presence of pronounced noise,\nhowever, multiple models are often found to explain the same data equally well.\nTo resolve this model selection ambiguity, we introduce the general approach of\nmodel selection confidence sets (MSCSs) based on likelihood ratio testing. A\nMSCS is defined as a list of models statistically indistinguishable from the\ntrue model at a user-specified level of confidence, which extends the familiar\nnotion of confidence intervals to the model-selection framework. Our approach\nguarantees asymptotically correct coverage probability of the true model when\nboth sample size and model dimension increase. We derive conditions under which\nthe MSCS contains all the relevant information about the true model structure.\nIn addition, we propose natural statistics based on the MSCS to measure\nimportance of variables in a principled way that accounts for the overall model\nuncertainty. When the space of feasible models is large, MSCS is implemented by\nan adaptive stochastic search algorithm which samples MSCS models with high\nprobability. The MSCS methodology is illustrated through numerical experiments\non synthetic data and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 14:11:41 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Zheng", "Chao", ""], ["Ferrari", "Davide", ""], ["Yang", "Yuhong", ""]]}, {"id": "1709.04389", "submitter": "Ling Zhou", "authors": "Ling Zhou and Peter X.-K. Song", "title": "Scalable and Efficient Statistical Inference with Estimating Functions\n  in the MapReduce Paradigm for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The theory of statistical inference along with the strategy of\ndivide-and-conquer for large- scale data analysis has recently attracted\nconsiderable interest due to great popularity of the MapReduce programming\nparadigm in the Apache Hadoop software framework. The central analytic task in\nthe development of statistical inference in the MapReduce paradigm pertains to\nthe method of combining results yielded from separately mapped data batches.\nOne seminal solution based on the confidence distribution has recently been\nestablished in the setting of maximum likelihood estimation in the literature.\nThis paper concerns a more general inferential methodology based on estimating\nfunctions, termed as the Rao-type confidence distribution, of which the maximum\nlikelihood is a special case. This generalization provides a unified framework\nof statistical inference that allows regression analyses of massive data sets\nof important types in a parallel and scalable fashion via a distributed file\nsystem, including longitudinal data analysis, survival data analysis, and\nquantile regression, which cannot be handled using the maximum likelihood\nmethod. This paper investigates four important properties of the proposed\nmethod: computational scalability, statistical optimality, methodological\ngenerality, and operational robustness. In particular, the proposed method is\nshown to be closely connected to Hansen's generalized method of moments (GMM)\nand Crowder's optimality. An interesting theoretical finding is that the\nasymptotic efficiency of the proposed Rao-type confidence distribution\nestimator is always greater or equal to the estimator obtained by processing\nthe full data once. All these properties of the proposed method are illustrated\nvia numerical examples in both simulation studies and real-world data analyses.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 15:45:31 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Zhou", "Ling", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "1709.04412", "submitter": "Przemyslaw Biecek", "authors": "Agnieszka Sitko, Przemyslaw Biecek", "title": "The Merging Path Plot: adaptive fusing of k-groups with likelihood-based\n  model selection", "comments": "Submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many statistical tests that verify the null hypothesis: the\nvariable of interest has the same distribution among k-groups. But once the\nnull hypothesis is rejected, how to present the structure of dissimilarity\nbetween groups? In this article, we introduce The Merging Path Plot - a\nmethodology, and factorMerger - an R package, for exploration and visualization\nof k-group dissimilarities. Comparison of k-groups is one of the most important\nissues in exploratory analyses and it has zillions of applications. The\nclassical solution is to test a~null hypothesis that observations from all\ngroups come from the same distribution. If the global null hypothesis is\nrejected, a~more detailed analysis of differences among pairs of groups is\nperformed. The traditional approach is to use pairwise post hoc tests in order\nto verify which groups differ significantly. However, this approach fails with\na large number of groups in both interpretation and visualization layer.\nThe~Merging Path Plot methodology solves this problem by using an\neasy-to-understand description of dissimilarity among groups based on\nLikelihood Ratio Test (LRT) statistic.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 16:46:11 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 09:12:21 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Sitko", "Agnieszka", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1709.04589", "submitter": "Issa Dahabreh", "authors": "Issa Dahabreh, Sarah Robertson, Eric Tchetgen Tchetgen, Elizabeth\n  Stuart, Miguel Hernan", "title": "Generalizing causal inferences from individuals in randomized trials to\n  all trial-eligible individuals", "comments": null, "journal-ref": "Biometrics. 2019 Jun;75(2):685-694", "doi": "10.1111/biom.13009", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider methods for causal inference in randomized trials nested within\ncohorts of trial-eligible individuals, including those who are not randomized.\nWe show how baseline covariate data from the entire cohort, and treatment and\noutcome data only from randomized individuals, can be used to identify\npotential (counterfactual) outcome means and average treatment effects in the\ntarget population of all eligible individuals. We review identifiability\nconditions, propose estimators, and assess the estimators' finite-sample\nperformance in simulation studies. As an illustration, we apply the estimators\nin a trial nested within a cohort of trial-eligible individuals to compare\ncoronary artery bypass grafting surgery plus medical therapy vs. medical\ntherapy alone for chronic coronary artery disease.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 02:02:31 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 14:28:50 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Dahabreh", "Issa", ""], ["Robertson", "Sarah", ""], ["Tchetgen", "Eric Tchetgen", ""], ["Stuart", "Elizabeth", ""], ["Hernan", "Miguel", ""]]}, {"id": "1709.04606", "submitter": "Chao Gao", "authors": "Chao Gao", "title": "Goodness-of-Fit Tests for Random Partitions via Symmetric Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider goodness-of-fit tests with i.i.d. samples generated from a\ncategorical distribution $(p_1,...,p_k)$. For a given $(q_1,...,q_k)$, we test\nthe null hypothesis whether $p_j=q_{\\pi(j)}$ for some label permutation $\\pi$.\nThe uncertainty of label permutation implies that the null hypothesis is\ncomposite instead of being singular. In this paper, we construct a testing\nprocedure using statistics that are defined as indefinite integrals of some\nsymmetric polynomials. This method is aimed directly at the invariance of the\nproblem, and avoids the need of matching the unknown labels. The asymptotic\ndistribution of the testing statistic is shown to be chi-squared, and its power\nis proved to be nearly optimal under a local alternative hypothesis. Various\ndegenerate structures of the null hypothesis are carefully analyzed in the\npaper. A two-sample version of the test is also studied.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 03:50:26 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 02:38:39 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Gao", "Chao", ""]]}, {"id": "1709.04835", "submitter": "James Fry", "authors": "J.T. Fry, Matt Slifko, Scotland Leman", "title": "Generalized Biplots for Multidimensional Scaled Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction and visualization is a staple of data analytics. Methods\nsuch as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS)\nprovide low dimensional (LD) projections of high dimensional (HD) data while\npreserving an HD relationship between observations. Traditional biplots assign\nmeaning to the LD space of a PCA projection by displaying LD axes for the\nattributes. These axes, however, are specific to the linear projection used in\nPCA. MDS projections, which allow for arbitrary stress and dissimilarity\nfunctions, require special care when labeling the LD space. We propose an\niterative scheme to plot an LD axis for each attribute based on the\nuser-specified stress and dissimilarity metrics. We discuss the details of our\ngeneral biplot methodology, its relationship with PCA-derived biplots, and\nprovide examples using real data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 15:03:33 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 16:30:36 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Fry", "J. T.", ""], ["Slifko", "Matt", ""], ["Leman", "Scotland", ""]]}, {"id": "1709.04840", "submitter": "Fei Xue", "authors": "Fei Xue and Annie Qu", "title": "Variable Selection for Highly Correlated Predictors", "comments": "44 pages (including 14 pages of supplementary materials), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalty-based variable selection methods are powerful in selecting relevant\ncovariates and estimating coefficients simultaneously. However, variable\nselection could fail to be consistent when covariates are highly correlated.\nThe partial correlation approach has been adopted to solve the problem with\ncorrelated covariates. Nevertheless, the restrictive range of partial\ncorrelation is not effective for capturing signal strength for relevant\ncovariates. In this paper, we propose a new Semi-standard PArtial Covariance\n(SPAC) which is able to reduce correlation effects from other predictors while\nincorporating the magnitude of coefficients. The proposed SPAC variable\nselection facilitates choosing covariates which have direct association with\nthe response variable, via utilizing dependency among covariates. We show that\nthe proposed method with the Lasso penalty (SPAC-Lasso) enjoys strong sign\nconsistency in both finite-dimensional and high-dimensional settings under\nregularity conditions. Simulation studies and the `HapMap' gene data\napplication show that the proposed method outperforms the traditional Lasso,\nadaptive Lasso, SCAD, and Peter-Clark-simple (PC-simple) methods for highly\ncorrelated predictors.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 15:28:04 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Xue", "Fei", ""], ["Qu", "Annie", ""]]}, {"id": "1709.04851", "submitter": "Paula Cheira", "authors": "Paula Cheira, Paula Brito and A. Pedro Duarte Silva", "title": "Factor Analysis of Interval Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a factor analysis model for symbolic data, focusing on\nthe particular case of interval-valued variables. The proposed method describes\nthe correlation structure among the measured interval-valued variables in terms\nof a few underlying, but unobservable, uncorrelated interval-valued variables,\ncalled \\textit{common factors}. Uniform and Triangular distributions are\nconsidered within each observed interval. We obtain the corresponding sample\nmean, variance and covariance assuming a general Triangular distribution.\n  In our proposal, factors are extracted either by Principal Component or by\nPrincipal Axis Factoring, performed on the interval-valued variables\ncorrelation matrix. To estimate the values of the common factors, usually\ncalled \\textit{factor scores}, two approaches are considered, which are\ninspired in methods for real-valued data: the Bartlett and the Anderson-Rubin\nmethods. In both cases, the estimated values are obtained solving an\noptimization problem that minimizes a function of the weighted squared Mallows\ndistance between quantile functions. Explicit expressions for the quantile\nfunction and the squared Mallows distance are derived assuming a general\nTriangular distribution.\n  The applicability of the method is illustrated using two sets of data:\ntemperature and precipitation in cities of the United States of America between\nthe years 1971 and 2000 and measures of car characteristics of different makes\nand models. Moreover, the method is evaluated on synthetic data with predefined\ncorrelation structures.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 16:08:22 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Cheira", "Paula", ""], ["Brito", "Paula", ""], ["Silva", "A. Pedro Duarte", ""]]}, {"id": "1709.04938", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "M. D. Ruiz-Medina and J. \\'Alvarez-Li\\'ebana", "title": "A note on strong-consistency of componentwise ARH(1) predictors", "comments": "12 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a new result on strong-consistency, in the trace norm, of\na diagonal componentwise parameter estimator of the autocorrelation operator of\nan autoregressive process of order one (ARH(1) process), allowing\nstrong-consistency of the associated plug-in predictor. These results are\nderived, when the eigenvectors of the autocovariance operator are unknown, and\nthe autocorrelation operator does not admit a diagonal spectral representation\nwith respect to the eigenvectors of the autocovariance operator.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 18:19:33 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Ruiz-Medina", "M. D.", ""], ["\u00c1lvarez-Li\u00e9bana", "J.", ""]]}, {"id": "1709.04952", "submitter": "Holger Dette", "authors": "Kirsten Schorning, Holger Dette, Katrin Kettelhake, Tilman M\\\"oller", "title": "Optimal designs for enzyme inhibition kinetic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new method for determining optimal designs for\nenzyme inhibition kinetic models, which are used to model the influence of the\nconcentration of a substrate and an inhibition on the velocity of a reaction.\nThe approach uses a nonlinear transformation of the vector of predictors such\nthat the model in the new coordinates is given by an incomplete response\nsurface model. Although there exist no explicit solutions of the optimal design\nproblem for incomplete response surface models so far, the corresponding design\nproblem in the new coordinates is substantially more transparent, such that\nexplicit or numerical solutions can be determined more easily. The designs for\nthe original problem can finally be found by an inverse transformation of the\noptimal designs determined for the response surface model. We illustrate the\nmethod determining explicit solutions for the $D$-optimal design and for the\noptimal design problem for estimating the individual coefficients in a\nnon-competitive enzyme inhibition kinetic model.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 19:22:37 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Schorning", "Kirsten", ""], ["Dette", "Holger", ""], ["Kettelhake", "Katrin", ""], ["M\u00f6ller", "Tilman", ""]]}, {"id": "1709.04979", "submitter": "Guilherme Silva Mr", "authors": "G.P. Silva, C. A. Taconeli, W.M. Zeviani, I. S. Guimaraes", "title": "An improved quality control chart to monitor the mean based on ranked\n  sets", "comments": "27 pages, 9 figures, to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we considered the design and performance of control charts\nusing neoteric ranked set sampling (NRSS) in monitoring normal distributed\nprocesses. NRSS is a recently proposed sampling design, based on the\ntraditional ranked set sampling (RSS). We evaluated NRSS control charts by\naverage run length (ARL), based on Monte Carlo simulation results. NRSS control\ncharts performed the best, compared to RSS and some of its extensions, in most\nsimulated scenarios. The impact of imperfect ranking was also evaluated. An\napplication on strength concrete data serves as an illustration of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 21:11:53 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Silva", "G. P.", ""], ["Taconeli", "C. A.", ""], ["Zeviani", "W. M.", ""], ["Guimaraes", "I. S.", ""]]}, {"id": "1709.05062", "submitter": "Xiwei Tang", "authors": "Xiwei Tang, Fei Xue and Annie Qu", "title": "Individualized Multi-directional Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a heterogeneous modeling framework which achieves\nindividual-wise feature selection and individualized covariates' effects\nsubgrouping simultaneously. In contrast to conventional model selection\napproaches, the new approach constructs a separation penalty with\nmulti-directional shrinkages, which facilitates individualized modeling to\ndistinguish strong signals from noisy ones and selects different relevant\nvariables for different individuals. Meanwhile, the proposed model identifies\nsubgroups among which individuals share similar covariates' effects, and thus\nimproves individualized estimation efficiency and feature selection accuracy.\nMoreover, the proposed model also incorporates within-individual correlation\nfor longitudinal data to gain extra efficiency. We provide a general\ntheoretical foundation under a double-divergence modeling framework where the\nnumber of individuals and the number of individual-wise measurements can both\ndiverge, which enables inference on both an individual level and a population\nlevel. In particular, we establish strong oracle property for the\nindividualized estimator to ensure its optimal large sample property under\nvarious conditions. An efficient ADMM algorithm is developed for computational\nscalability. Simulation studies and applications to post-trauma mental disorder\nanalysis with genetic variation and an HIV longitudinal treatment study are\nillustrated to compare the new approach to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 05:15:05 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 04:19:50 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tang", "Xiwei", ""], ["Xue", "Fei", ""], ["Qu", "Annie", ""]]}, {"id": "1709.05269", "submitter": "Ye Liang", "authors": "Ye Liang, Joshua D. Habiger and Xiaoyi Min", "title": "The Inuence of Misspecified Covariance on False Discovery Control when\n  Using Posterior Probabilities", "comments": "22 pages, 5 figures", "journal-ref": "Statistical Theory and Related Fields, Vol 1 (2017) 205-215", "doi": "10.1080/24754269.2017.1387445", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the influence of a misspecified covariance structure on\nfalse discovery rate for the large scale multiple testing problem.\nSpecifically, we evaluate the influence on the marginal distribution of local\nfdr statistics, which are used in many multiple testing procedures and related\nto Bayesian posterior probabilities. Explicit forms of the marginal\ndistributions under both correctly specified and incorrectly specified models\nare derived. The Kullback-Leibler divergence is used to quantify the influence\ncaused by a misspecification. Several numerical examples are provided to\nillustrate the influence. A real spatio-temporal data on soil humidity is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 15:31:21 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Liang", "Ye", ""], ["Habiger", "Joshua D.", ""], ["Min", "Xiaoyi", ""]]}, {"id": "1709.05275", "submitter": "Ye Liang", "authors": "Ye Liang, Yang Li, Bin Zhang", "title": "Bayesian Nonparametric Inference for Panel Count Data with an\n  Informative Observation Process", "comments": "25 pages, 7 figures", "journal-ref": "Biometrical Journal (2018), 60(3), 583-596", "doi": "10.1002/bimj.201700176", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the panel count data analysis for recurrent events is\nconsidered. Such analysis is useful for studying tumor or infection recurrences\nin both clinical trial and observational studies. A bivariate Gaussian Cox\nprocess model is proposed to jointly model the observation process and the\nrecurrent event process. Bayesian nonparametric inference is proposed for\nsimultaneously estimating regression parameters, bivariate frailty effects and\nbaseline intensity functions. Inference is done through Markov chain Monte\nCarlo, with fully developed computational techniques. Predictive inference is\nalso discussed under the Bayesian setting. The proposed method is shown to be\nefficient via simulation studies. A clinical trial dataset on skin cancer\npatients is analyzed to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 15:40:14 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 21:37:15 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Liang", "Ye", ""], ["Li", "Yang", ""], ["Zhang", "Bin", ""]]}, {"id": "1709.05328", "submitter": "Xi Luo", "authors": "Yi Zhao and Xi Luo", "title": "Granger Mediation Analysis of Multiple Time Series with an Application\n  to fMRI", "comments": "59 pages. Presented at the 2017 ENAR, JSM, and other meetings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It becomes increasingly popular to perform mediation analysis for complex\ndata from sophisticated experimental studies. In this paper, we present Granger\nMediation Analysis (GMA), a new framework for causal mediation analysis of\nmultiple time series. This framework is motivated by a functional magnetic\nresonance imaging (fMRI) experiment where we are interested in estimating the\nmediation effects between a randomized stimulus time series and brain activity\ntime series from two brain regions. The stable unit treatment assumption for\ncausal mediation analysis is thus unrealistic for this type of time series\ndata. To address this challenge, our framework integrates two types of models:\ncausal mediation analysis across the variables and vector autoregressive models\nacross the temporal observations. We further extend this framework to handle\nmultilevel data to address individual variability and correlated errors between\nthe mediator and the outcome variables. These models not only provide valid\ncausal mediation for time series data but also model the causal dynamics across\ntime. We show that the modeling parameters in our models are identifiable, and\nwe develop computationally efficient methods to maximize the likelihood-based\noptimization criteria. Simulation studies show that our method reduces the\nestimation bias and improve statistical power, compared to existing approaches.\nOn a real fMRI data set, our approach not only infers the causal effects of\nbrain pathways but accurately captures the feedback effect of the outcome\nregion on the mediator region.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 17:47:51 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Zhao", "Yi", ""], ["Luo", "Xi", ""]]}, {"id": "1709.05409", "submitter": "Simo S\\\"arkk\\\"a", "authors": "Simo S\\\"arkk\\\"a and Mauricio A. \\'Alvarez and Neil D. Lawrence", "title": "Gaussian Process Latent Force Models for Learning and Stochastic Control\n  of Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with learning and stochastic control in physical\nsystems which contain unknown input signals. These unknown signals are modeled\nas Gaussian processes (GP) with certain parametrized covariance structures. The\nresulting latent force models (LFMs) can be seen as hybrid models that contain\na first-principles physical model part and a non-parametric GP model part. We\nbriefly review the statistical inference and learning methods for this kind of\nmodels, introduce stochastic control methodology for the models, and provide\nnew theoretical observability and controllability results for them.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 21:07:46 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 20:02:42 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["S\u00e4rkk\u00e4", "Simo", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1709.05422", "submitter": "Graciela Boente Prof.", "authors": "Claudio Agostinelli, Ana M. Bianco and Graciela Boente", "title": "Robust estimation in single index models when the errors have a unimodal\n  density with unknown nuisance parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust profile estimation method for the\nparametric and nonparametric components of a single index model when the errors\nhave a strongly unimodal density with unknown nuisance parameter. Under\nregularity conditions, we derive consistency results for the link function\nestimators as well as consistency and asymptotic distribution results for the\nsingle index parameter estimators. Under a log--Gamma model, the sensitivity to\nanomalous observations is studied by means of the empirical influence curve. We\nalso discuss a robust $K-$fold procedure to select the smoothing parameters\ninvolved. A numerical study is conducted to evaluate the small sample\nperformance of the robust proposal with that of their classical relatives, both\nfor errors following a log--Gamma model and for contaminated schemes. The\nnumerical experiment shows the good robustness properties of the proposed\nestimators and the advantages of considering a robust approach instead of the\nclassical one.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 21:57:43 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 18:46:50 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Agostinelli", "Claudio", ""], ["Bianco", "Ana M.", ""], ["Boente", "Graciela", ""]]}, {"id": "1709.05454", "submitter": "Avanti Athreya", "authors": "Avanti Athreya, Donniell E. Fishkind, Keith Levin, Vince Lyzinski,\n  Youngser Park, Yichen Qin, Daniel L. Sussman, Minh Tang, Joshua T.\n  Vogelstein, and Carey E. Priebe", "title": "Statistical inference on random dot product graphs: a survey", "comments": "An expository survey paper on a comprehensive paradigm for inference\n  for random dot product graphs, centered on graph adjacency and Laplacian\n  spectral embeddings. Paper outlines requisite background; summarizes theory,\n  methodology, and applications from previous and ongoing work; and closes with\n  a discussion of several open problems", "journal-ref": "Journal of Machine Learning Research, 2018", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random dot product graph (RDPG) is an independent-edge random graph that\nis analytically tractable and, simultaneously, either encompasses or can\nsuccessfully approximate a wide range of random graphs, from relatively simple\nstochastic block models to complex latent position graphs. In this survey\npaper, we describe a comprehensive paradigm for statistical inference on random\ndot product graphs, a paradigm centered on spectral embeddings of adjacency and\nLaplacian matrices. We examine the analogues, in graph inference, of several\ncanonical tenets of classical Euclidean inference: in particular, we summarize\na body of existing results on the consistency and asymptotic normality of the\nadjacency and Laplacian spectral embeddings, and the role these spectral\nembeddings can play in the construction of single- and multi-sample hypothesis\ntests for graph data. We investigate several real-world applications, including\ncommunity detection and classification in large social networks and the\ndetermination of functional and biologically relevant network properties from\nan exploratory data analysis of the Drosophila connectome. We outline requisite\nbackground and current open problems in spectral graph inference.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 04:22:57 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Athreya", "Avanti", ""], ["Fishkind", "Donniell E.", ""], ["Levin", "Keith", ""], ["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Qin", "Yichen", ""], ["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1709.05514", "submitter": "Sourish Das", "authors": "Rahul Sharma and Sourish Das", "title": "Regularization and Variable Selection with Copula Prior", "comments": "16 pages, 4 tables, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show that under specific choices of the copula, the lasso,\nelastic net, and $g$-prior are particular cases of `copula prior,' for\nregularization and variable selection method. We present `lasso with Gauss\ncopula prior' and `lasso with t-copula prior.' The simulation study and\nreal-world data for regression, classification, and large time-series data show\nthat the `copula prior' often outperforms the lasso and elastic net while\nhaving a comparable sparsity of representation. Also, the copula prior\nencourages a grouping effect. The strongly correlated predictors tend to be in\nor out of the model collectively under the copula prior. The `copula prior' is\na generic method, which can be used to define the new prior distribution. The\napplication of copulas in modeling prior distribution for Bayesian methodology\nhas not been explored much. We present the resampling-based optimization\nprocedure to handle big data with copula prior.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 14:07:28 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 00:10:34 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Sharma", "Rahul", ""], ["Das", "Sourish", ""]]}, {"id": "1709.05515", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Suhas N., Talasila Sai Teja and Anshul Juneja", "title": "Some variations on Ensembled Random Survival Forest with application to\n  Cancer Research", "comments": "16 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel implementation of adaboost for prediction\nof survival function. We take different variations of the algorithm and compare\nthe algorithms based on system run time and root mean square error. Our\nconstruction includes right censoring data and competing risk data too. We take\ndifferent data set to illustrate the performance of the algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 14:12:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 19:24:01 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["N.", "Suhas", ""], ["Teja", "Talasila Sai", ""], ["Juneja", "Anshul", ""]]}, {"id": "1709.05547", "submitter": "Olav Bjarte Fosso PhD", "authors": "Olav B. Fosso, Marta Molinas", "title": "Method for Mode Mixing Separation in Empirical Mode Decomposition", "comments": "7 pages, 10 figures and two images", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Empirical Mode Decomposition (EMD) is a signal analysis method that\nseparates multi-component signals into single oscillatory modes called\nintrinsic mode functions (IMFs), each of which can generally be associated to a\nphysical meaning of the process from which the signal is obtained. When the\nphenomena of mode mixing occur, as a result of the EMD sifting process, the\nIMFs can lose their physical meaning hindering the interpretation of the\nresults of the analysis. In the paper, \"One or Two frequencies? The Empirical\nMode Decomposition Answers\", Gabriel Rilling and Patrick Flandrin [3] presented\na rigorous mathematical analysis that explains how EMD behaves in the case of a\ncomposite two-tones signal and the amplitude and frequency ratios by which EMD\nwill perform a good separation of tones. However, the authors did not propose a\nsolution for separating the neighboring tones that will naturally remain mixed\nafter an EMD. In this paper, based on the findings by Rilling and Flandrin, a\nmethod that can separate neighbouring spectral components, that will naturally\nremain within a single IMF, is presented. This method is based on reversing the\nconditions by which mode mixing occurs and that were presented in the map by\nRilling and Flandrin in the above mentioned paper. Numerical experiments with\nsignals containing closely spaced spectral components shows the effective\nseparation of modes that EMD can perform after this principle is applied. The\nresults verify also the regimes presented in the theoretical analysis by\nRilling and Flandrin.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 18:34:12 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 12:40:59 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Fosso", "Olav B.", ""], ["Molinas", "Marta", ""]]}, {"id": "1709.05562", "submitter": "Nan Chen", "authors": "Nan Chen and Andrew J. Majda", "title": "Efficient Statistically Accurate Algorithms for the Fokker-Planck\n  Equation in Large Dimensions", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.10.022", "report-no": null, "categories": "stat.ME math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving the Fokker-Planck equation for high-dimensional complex turbulent\ndynamical systems is an important and practical issue. However, most\ntraditional methods suffer from the curse of dimensionality and have\ndifficulties in capturing the fat tailed highly intermittent probability\ndensity functions (PDFs) of complex systems in turbulence, neuroscience and\nexcitable media. In this article, efficient statistically accurate algorithms\nare developed for solving both the transient and the equilibrium solutions of\nFokker-Planck equations associated with high-dimensional nonlinear turbulent\ndynamical systems with conditional Gaussian structures. The algorithms involve\na hybrid strategy that requires only a small number of ensembles. Here, a\nconditional Gaussian mixture in a high-dimensional subspace via an extremely\nefficient parametric method is combined with a judicious non-parametric\nGaussian kernel density estimation in the remaining low-dimensional subspace.\nParticularly, the parametric method provides closed analytical formulae for\ndetermining the conditional Gaussian distributions in the high-dimensional\nsubspace and is therefore computationally efficient and accurate. The full\nnon-Gaussian PDF of the system is then given by a Gaussian mixture. Different\nfrom the traditional particle methods, each conditional Gaussian distribution\nhere covers a significant portion of the high-dimensional PDF. Therefore a\nsmall number of ensembles is sufficient to recover the full PDF, which\novercomes the curse of dimensionality. Notably, the mixture distribution has a\nsignificant skill in capturing the transient behavior with fat tails of the\nhigh-dimensional non-Gaussian PDFs, and this facilitates the algorithms in\naccurately describing the intermittency and extreme events in complex turbulent\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 20:13:38 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Chen", "Nan", ""], ["Majda", "Andrew J.", ""]]}, {"id": "1709.05786", "submitter": "Dominik Liebl", "authors": "Dominik Liebl and Fabian Walders", "title": "Parameter Regimes in Partial Functional Panel Regression", "comments": null, "journal-ref": null, "doi": "10.1016/j.ecosta.2018.05.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new partial functional linear regression model for panel data with time\nvarying parameters is introduced. The parameter vector of the multivariate\nmodel component is allowed to be completely time varying while the\nfunction-valued parameter of the functional model component is assumed to\nchange over K unknown parameter regimes. Consistency is derived for the\nsuggested estimators and for the classification procedure used to detect the K\nunknown parameter regimes. Additionally, the convergence rates of the\nestimators are derived under a double asymptotic differentiating between\nasymptotic scenarios depending on the relative order of the panel dimensions n\nand T. The statistical model is motivated by a real data application\nconsidering the so-called idiosyncratic volatility puzzle using high frequency\ndata from the S&P500.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 06:34:03 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 20:30:04 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Liebl", "Dominik", ""], ["Walders", "Fabian", ""]]}, {"id": "1709.05863", "submitter": "Kota Mori", "authors": "Kota Mori", "title": "Estimating the Variance of Measurement Errors in Running Variables of\n  Sharp Regression Discontinuity Designs", "comments": "9 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of a treatment effect by a regression discontinuity design faces a\nsevere challenge when the running variable contains measurement errors since\nthe errors smoothen the discontinuity on which the identification depends. The\nexisting studies show that the variance of the measurement errors plays a vital\nrole in both bias correction and identification under such situations. However,\nthe methodologies to estimate the variance from data are relatively\nundeveloped. This paper proposes two estimators for the variance of measurement\nerrors of running variables of sharp regression continuity designs. The\nproposed estimators can be constructed merely from data of observed running\nvariable and treatment assignment, and do not require any other external source\nof information.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:17:04 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 13:46:21 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 10:54:26 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Mori", "Kota", ""]]}, {"id": "1709.05906", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey, Sanku Dey and Debasis Kundu", "title": "Bayesian analysis of three parameter singular Marshall-Olkin bivariate\n  Pareto distribution", "comments": "23 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides bayesian analysis of singular Marshall-Olkin bivariate\nPareto distribution. We consider three parameter singular Marshall-Olkin\nbivariate Pareto distribution. We consider two types of prior - reference prior\nand gamma prior. Bayes estimate of the parameters are calculated based on slice\ncum gibbs sampler and Lindley approximation. Credible interval is also provided\nfor all methods and all prior distributions. A data analysis is kept for\nillustrative purpose.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:13:12 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 02:06:48 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Dey", "Sanku", ""], ["Kundu", "Debasis", ""]]}, {"id": "1709.06115", "submitter": "Sigrunn Holbek S{\\o}rbye", "authors": "Sigrunn H. S{\\o}rbye, Eirik Myrvoll-Nilsen and H{\\aa}vard Rue", "title": "An approximate fractional Gaussian noise model with ${\\mathcal O}(n)$\n  computational cost", "comments": "16 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional Gaussian noise (fGn) is a stationary time series model with long\nmemory properties applied in various fields like econometrics, hydrology and\nclimatology. The computational cost in fitting an fGn model of length $n$ using\na likelihood-based approach is ${\\mathcal O}(n^{2})$, exploiting the Toeplitz\nstructure of the covariance matrix. In most realistic cases, we do not observe\nthe fGn process directly but only through indirect Gaussian observations, so\nthe Toeplitz structure is easily lost and the computational cost increases to\n${\\mathcal O}(n^{3})$. This paper presents an approximate fGn model of\n${\\mathcal O}(n)$ computational cost, both with direct or indirect Gaussian\nobservations, with or without conditioning. This is achieved by approximating\nfGn with a weighted sum of independent first-order autoregressive processes,\nfitting the parameters of the approximation to match the autocorrelation\nfunction of the fGn model. The resulting approximation is stationary despite\nbeing Markov and gives a remarkably accurate fit using only four components.\nThe performance of the approximate fGn model is demonstrated in simulations and\ntwo real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 18:27:15 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["S\u00f8rbye", "Sigrunn H.", ""], ["Myrvoll-Nilsen", "Eirik", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1709.06181", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, Frank\n  Wood", "title": "On Nesting Monte Carlo Estimators", "comments": "To appear at International Conference on Machine Learning 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning and statistics involve nested expectations\nand thus do not permit conventional Monte Carlo (MC) estimation. For such\nproblems, one must nest estimators, such that terms in an outer estimator\nthemselves involve calculation of a separate, nested, estimation. We\ninvestigate the statistical implications of nesting MC estimators, including\ncases of multiple levels of nesting, and establish the conditions under which\nthey converge. We derive corresponding rates of convergence and provide\nempirical evidence that these rates are observed in practice. We further\nestablish a number of pitfalls that can arise from naive nesting of MC\nestimators, provide guidelines about how these can be avoided, and lay out\nnovel methods for reformulating certain classes of nested expectation problems\ninto single expectations, leading to improved convergence rates. We demonstrate\nthe applicability of our work by using our results to develop a new estimator\nfor discrete Bayesian experimental design problems and derive error bounds for\na class of variational objectives.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:01:05 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 20:36:06 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 16:04:11 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 17:11:26 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Rainforth", "Tom", ""], ["Cornish", "Robert", ""], ["Yang", "Hongseok", ""], ["Warrington", "Andrew", ""], ["Wood", "Frank", ""]]}, {"id": "1709.06233", "submitter": "Rina Foygel Barber", "authors": "Wenyu Chen, Kelli-Jean Chun, Rina Foygel Barber", "title": "Discretized conformal prediction for efficient distribution-free\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression problems where there is no known true underlying model,\nconformal prediction methods enable prediction intervals to be constructed\nwithout any assumptions on the distribution of the underlying data, except that\nthe training and test data are assumed to be exchangeable. However, these\nmethods bear a heavy computational cost-and, to be carried out exactly, the\nregression algorithm would need to be fitted infinitely many times. In\npractice, the conformal prediction method is run by simply considering only a\nfinite grid of finely spaced values for the response variable. This paper\ndevelops discretized conformal prediction algorithms that are guaranteed to\ncover the target value with the desired probability, and that offer a tradeoff\nbetween computational cost and prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 03:08:21 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Chen", "Wenyu", ""], ["Chun", "Kelli-Jean", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1709.06288", "submitter": "Jarod Yan Liang Lee", "authors": "Jarod Y.L. Lee and Peter J. Green and Louise M. Ryan", "title": "Conjugate generalized linear mixed models for clustered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article concerns a class of generalized linear mixed models for\nclustered data, where the random effects are mapped uniquely onto the grouping\nstructure and are independent between groups. We derive necessary and\nsufficient conditions that enable the marginal likelihood of such class of\nmodels to be expressed in closed-form. Illustrations are provided using the\nGaussian, Poisson, binomial and gamma distributions. These models are unified\nunder a single umbrella of conjugate generalized linear mixed models, where\n\"conjugate\" refers to the fact that the marginal likelihood can be expressed in\nclosed-form, rather than implying inference via the Bayesian paradigm. Having\nan explicit marginal likelihood means that these models are more\ncomputationally convenient, which can be important in big data contexts. Except\nfor the binomial distribution, these models are able to achieve simultaneous\nconjugacy, and thus able to accommodate both unit and group level covariates.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 08:16:47 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Lee", "Jarod Y. L.", ""], ["Green", "Peter J.", ""], ["Ryan", "Louise M.", ""]]}, {"id": "1709.06313", "submitter": "Silvano Fiorin", "authors": "Silvano Fiorin", "title": "Asymptotics for relative frequency when population is driven by\n  arbitrary evolution", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strongly consistent estimates are shown, via relative frequency, for the\nprobability of \"white balls\" inside a dichotomous urn when such a probability\nis an arbitrary continuous time dependent function over a bounded time\ninterval. The asymptotic behaviour of relative frequency is studied in a\nnonstationary context using a Riemann-Dini type theorem for SLLN of random\nvariables with arbitrarily different expectations; furthermore the theoretical\nresults concerning the SLLN can be applied for estimating the mean function of\nunknown form of a general nonstationary process.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:34:21 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Fiorin", "Silvano", ""]]}, {"id": "1709.06418", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy", "title": "Semiparametric theory", "comments": "arXiv admin note: text overlap with arXiv:1510.04740", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a brief review of semiparametric theory, using as a\nrunning example the common problem of estimating an average causal effect.\nSemiparametric models allow at least part of the data-generating process to be\nunspecified and unrestricted, and can often yield robust estimators that\nnonetheless behave similarly to those based on parametric likelihood\nassumptions, e.g., fast rates of convergence to normal limiting distributions.\nWe discuss the basics of semiparametric theory, focusing on influence\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 20:18:03 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kennedy", "Edward H.", ""]]}, {"id": "1709.06421", "submitter": "Wenyu Zhang", "authors": "Wenyu Zhang, Nicholas James, David Matteson", "title": "Pruning and Nonparametric Multiple Change Point Detection", "comments": "9 pages. arXiv admin note: text overlap with arXiv:1505.04302", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point analysis is a statistical tool to identify homogeneity within\ntime series data. We propose a pruning approach for approximate nonparametric\nestimation of multiple change points. This general purpose change point\ndetection procedure `cp3o' applies a pruning routine within a dynamic program\nto greatly reduce the search space and computational costs. Existing\ngoodness-of-fit change point objectives can immediately be utilized within the\nframework. We further propose novel change point algorithms by applying cp3o to\ntwo popular nonparametric goodness of fit measures: `e-cp3o' uses E-statistics,\nand `ks-cp3o' uses Kolmogorov-Smirnov statistics. Simulation studies highlight\nthe performance of these algorithms in comparison with parametric and other\nnonparametric change point methods. Finally, we illustrate these approaches\nwith climatological and financial applications.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 04:40:23 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Zhang", "Wenyu", ""], ["James", "Nicholas", ""], ["Matteson", "David", ""]]}, {"id": "1709.06588", "submitter": "Ye Liang", "authors": "Shangyuan Ye, Ye Liang and Ibrahim A. Ahmad", "title": "Orthogonal Series Density Estimation for Complex Surveys", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": "10.1080/10485252.2019.1585539", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an orthogonal series density estimator for complex surveys, where\nsamples are neither independent nor identically distributed. The proposed\nestimator is proved to be design-unbiased and asymptotically design-consistent.\nThe asymptotic normality is proved under both design and combined spaces. Two\ndata driven estimators are proposed based on the proposed oracle estimator. We\nshow the efficiency of the proposed estimators in simulation studies. A real\nsurvey data example is provided for an illustration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 18:10:18 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 16:38:37 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 03:55:37 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Ye", "Shangyuan", ""], ["Liang", "Ye", ""], ["Ahmad", "Ibrahim A.", ""]]}, {"id": "1709.06633", "submitter": "Michael Crowther", "authors": "Michael J. Crowther", "title": "Multilevel mixed effects parametric survival analysis: Estimation,\n  simulation and application", "comments": null, "journal-ref": null, "doi": "10.1177/1536867X19893639", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, I present the user written stmixed command for the fitting\nof multilevel survival models, which serves as both an alternative to Stata's\nofficial mestreg, and a complimentary program with substantial extensions.\nstmixed can fit multilevel survival models with any number of levels and random\neffects at each level, including flexible spline-based approaches (such as\nRoyston-Parmar and the log hazard equivalent) or user-defined hazard models.\nSimple or complex time-dependent effects can be included, as well as the\naddition of expected mortality for a relative survival model.\nLeft-truncation/delayed entry can be used and t-distributed random effects are\nprovided as an alternative to Gaussian random effects. The methods are\nillustrated with a commonly used dataset of patients with kidney disease\nsuffering recurrent infections, and a simulated example, illustrating a simple\napproach to simulating clustered survival data using survsim (Crowther and\nLambert 2012, 2013). stmixed is part of the merlin family (Crowther 2017,\n2018).\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 20:25:00 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 08:46:29 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Crowther", "Michael J.", ""]]}, {"id": "1709.06635", "submitter": "Alberto Carrassi", "authors": "Sammy Metref, Alexis Hannart, Juan Ruiz, Marc Bocquet, Alberto\n  Carrassi and Michael Ghil", "title": "Estimating model evidence using ensemble-based data assimilation with\n  localization - The model selection problem", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IIn recent years, there has been a growing interest in applying data\nassimilation (DA) methods, originally designed for state estimation, to the\nmodel selection problem. In this setting, Carrassi et al. (2017) introduced the\ncontextual formulation of model evidence (CME) and showed that CME can be\nefficiently computed using a hierarchy of ensemble-based DA procedures.\nAlthough Carrassi et al. (2017) analyzed the DA methods most commonly used for\noperational atmospheric and oceanic prediction worldwide, they did not study\nthese methods in conjunction with localization to a specific domain. Yet any\napplication of ensemble DA methods to realistic geophysical models requires the\nimplementation of some form of localization. The present study extends the\ntheory for estimating CME to ensemble DA methods with domain localization. The\ndomain-localized CME (DL-CME) developed herein is tested for model selection\nwith two models: (i) the Lorenz 40-variable mid-latitude atmospheric dynamics\nmodel (L95); and (ii) the simplified global atmospheric SPEEDY model. The CME\nis compared to the root-mean-square-error (RMSE) as a metric for model\nselection. The experiments show that CME improves systematically over the RMSE,\nand that this skill improvement is further enhanced by applying localization in\nthe estimate of the CME, using the DL-CME. The potential use and range of\napplications of the CME and DL-CME as a model selection metric are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 20:29:56 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 12:59:14 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Metref", "Sammy", ""], ["Hannart", "Alexis", ""], ["Ruiz", "Juan", ""], ["Bocquet", "Marc", ""], ["Carrassi", "Alberto", ""], ["Ghil", "Michael", ""]]}, {"id": "1709.06702", "submitter": "Andriy Derkach", "authors": "Andriy Derkach and Ruth M. Pfeiffer", "title": "Subset Testing and Analysis of Multiple Phenotypes (STAMP)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis of multiple genome-wide association studies (GWAS) is effective\nfor detecting single or multi marker associations with complex traits. We\ndevelop a flexible procedure (\"STAMP\") based on mixture models to perform\nregion based meta-analysis of different phenotypes using data from different\nGWAS and identify subsets of associated phenotypes. Our model framework helps\ndistinguish true associations from between-study heterogeneity. As a measure of\nassociation we compute for each phenotype the posterior probability that the\ngenetic region under investigation is truly associated. Extensive simulations\nshow that STAMP is more powerful than standard approaches for meta analyses\nwhen the proportion of truly associated outcomes is $\\leq$ 50\\%. For other\nsettings, the power of STAMP is similar to that of existing methods. We\nillustrate our method on two examples, the association of a region on\nchromosome 9p21 with risk of fourteen cancers, and the associations of\nexpression of quantitative traits loci (eQTLs) from two genetic regions with\ntheir cis-SNPs measured in seventeen tissue types using data from The Cancer\nGenome Atlas (TCGA).\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 02:14:22 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 20:12:22 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Derkach", "Andriy", ""], ["Pfeiffer", "Ruth M.", ""]]}, {"id": "1709.06781", "submitter": "Sigrunn Holbek S{\\o}rbye", "authors": "Sigrunn H. S{\\o}rbye, Janine B. Illian, Daniel P. Simpson and David\n  Burslem", "title": "Careful prior specification avoids incautious inference for log-Gaussian\n  Cox point processes", "comments": "21 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior specifications for hyperparameters of random fields in Bayesian spatial\npoint process modelling can have a major impact on the statistical inference\nand the conclusions made. We consider fitting of log-Gaussian Cox processes to\nspatial point patterns relative to spatial covariate data. From an ecological\npoint of view, an important aim of the analysis is to assess significant\nassociations between the covariates and the point pattern intensity of a given\nspecies. This paper introduces the use of a reparameterised model to facilitate\nmeaningful interpretations of the results and how these depend on hyperprior\nspecifications. The model combines a scaled spatially structured field with an\nunstructured random field, having a common precision parameter. An additional\nhyperparameter identifies the fraction of variance explained by the spatially\nstructured term and proper scaling makes the analysis invariant to grid\nresolution. The hyperparameters are assigned penalised complexity priors, which\ncan be tuned intuitively by user-defined scaling parameters. We illustrate the\napproach analysing covariate effects on point patterns formed by two rainforest\ntree species in a study plot on Barro Colorado Island, Panama.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 09:23:20 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["S\u00f8rbye", "Sigrunn H.", ""], ["Illian", "Janine B.", ""], ["Simpson", "Daniel P.", ""], ["Burslem", "David", ""]]}, {"id": "1709.06855", "submitter": "Natalie Neumeyer", "authors": "Nick Kloodt and Natalie Neumeyer", "title": "Specification tests in semiparametric transformation models - a\n  multiplier bootstrap approach", "comments": "Comparison to the first version: new title; new content: multiplier\n  bootstrap", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider semiparametric transformation models, where after pre-estimation\nof a parametric transformation of the response the data are modeled by means of\nnonparametric regression. We suggest subsequent procedures for testing\nlack-of-fit of the regression function and for significance of covariables,\nwhich - in contrast to procedures from the literature - are asymptotically not\ninfluenced by the pre-estimation of the transformation. The test statistics are\nasymptotically pivotal and have the same asymptotic distribution as in\nregression models without transformation. We show validity of a multiplier\nbootstrap procedure which is easier to implement and much less computationally\ndemanding than bootstrap procedures based on the transformation model. In a\nsimulation study we demonstrate the superior performance of the procedure in\ncomparison with the competitors from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 13:43:02 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 10:11:41 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Kloodt", "Nick", ""], ["Neumeyer", "Natalie", ""]]}, {"id": "1709.06859", "submitter": "Matthew Sperrin", "authors": "Matthew Sperrin, Glen Martin, Tjeerd Van Staa, Niels Peek, Iain Buchan", "title": "Using marginal structural models to adjust for treatment drop-in when\n  developing clinical prediction models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Clinical prediction models (CPMs) can inform decision-making\nconcerning treatment initiation. Here, one requires predicted risks assuming\nthat no treatment is given. This is challenging since CPMs are often derived in\ndatasets where patients receive treatment; moreover, treatment can commence\npost-baseline - treatment drop-ins. This study presents a novel approach of\nusing marginal structural models (MSMs) to adjust for treatment drop-in.\n  Study Design and Setting: We illustrate the use of MSMs in the CPM framework\nthrough simulation studies, representing randomised controlled trials and\nobservational data. The simulations include a binary treatment and a covariate,\neach recorded at two timepoints and having a prognostic effect on a binary\noutcome. The bias in predicted risk was examined in a model ignoring treatment,\na model fitted on treatment na\\\"ive patients (at baseline), a model including\nbaseline treatment, and the MSM.\n  Results: In all simulation scenarios, all models except the MSM\nunder-estimated the risk of outcome given absence of treatment. Consequently,\nCPMs that do not acknowledge treatment drop-in can lead to under-allocation of\ntreatment.\n  Conclusion: When developing CPMs to predict treatment-na\\\"ive risk, authors\nshould consider using MSMs to adjust for treatment drop-in. MSMs also allow\nestimation of individual treatment effects.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 13:44:48 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Sperrin", "Matthew", ""], ["Martin", "Glen", ""], ["Van Staa", "Tjeerd", ""], ["Peek", "Niels", ""], ["Buchan", "Iain", ""]]}, {"id": "1709.06896", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (2, 3, 1), Julien Bect (3, 1), S\\'everine Demeyer (2),\n  Nicolas Fischer (2), Emmanuel Vazquez (3, 1) ((1) GdR MASCOT-NUM, (2) LNE,\n  (3) L2S)", "title": "Integrating hyper-parameter uncertainties in a multi-fidelity Bayesian\n  model for the estimation of a probability of failure", "comments": null, "journal-ref": null, "doi": "10.1142/9789813274303_0035", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-fidelity simulator is a numerical model, in which one of the inputs\ncontrols a trade-off between the realism and the computational cost of the\nsimulation. Our goal is to estimate the probability of exceeding a given\nthreshold on a multi-fidelity stochastic simulator. We propose a fully Bayesian\napproach based on Gaussian processes to compute the posterior probability\ndistribution of this probability. We pay special attention to the\nhyper-parameters of the model. Our methodology is illustrated on an academic\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 14:22:17 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Stroh", "R\u00e9mi", ""], ["Bect", "Julien", ""], ["Demeyer", "S\u00e9verine", ""], ["Fischer", "Nicolas", ""], ["Vazquez", "Emmanuel", ""]]}, {"id": "1709.07036", "submitter": "Junwei Lu", "authors": "Cong Ma, Junwei Lu and Han Liu", "title": "Inter-Subject Analysis: Inferring Sparse Interactions with Dense\n  Intra-Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new modeling framework for Inter-Subject Analysis (ISA). The\ngoal of ISA is to explore the dependency structure between different subjects\nwith the intra-subject dependency as nuisance. It has important applications in\nneuroscience to explore the functional connectivity between brain regions under\nnatural stimuli. Our framework is based on the Gaussian graphical models, under\nwhich ISA can be converted to the problem of estimation and inference of the\ninter-subject precision matrix. The main statistical challenge is that we do\nnot impose sparsity constraint on the whole precision matrix and we only assume\nthe inter-subject part is sparse. For estimation, we propose to estimate an\nalternative parameter to get around the non-sparse issue and it can achieve\nasymptotic consistency even if the intra-subject dependency is dense. For\ninference, we propose an \"untangle and chord\" procedure to de-bias our\nestimator. It is valid without the sparsity assumption on the inverse Hessian\nof the log-likelihood function. This inferential method is general and can be\napplied to many other statistical problems, thus it is of independent\ntheoretical interest. Numerical experiments on both simulated and brain imaging\ndata validate our methods and theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 18:57:19 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Ma", "Cong", ""], ["Lu", "Junwei", ""], ["Liu", "Han", ""]]}, {"id": "1709.07045", "submitter": "Peter Rousseeuw", "authors": "Mia Hubert, Michiel Debruyne, Peter J. Rousseeuw", "title": "Minimum Covariance Determinant and Extensions", "comments": null, "journal-ref": "WIREs Computational Statistics, 2017, wics.1421", "doi": "10.1002/wics.1421", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Covariance Determinant (MCD) method is a highly robust estimator\nof multivariate location and scatter, for which a fast algorithm is available.\nSince estimating the covariance matrix is the cornerstone of many multivariate\nstatistical methods, the MCD is an important building block when developing\nrobust multivariate techniques. It also serves as a convenient and efficient\ntool for outlier detection. The MCD estimator is reviewed, along with its main\nproperties such as affine equivariance, breakdown value, and influence\nfunction. We discuss its computation, and list applications and extensions of\nthe MCD in applied and methodological multivariate statistics. Two recent\nextensions of the MCD are described. The first one is a fast deterministic\nalgorithm which inherits the robustness of the MCD while being almost affine\nequivariant. The second is tailored to high-dimensional data, possibly with\nmore dimensions than cases, and incorporates regularization to prevent singular\nmatrices.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 19:38:08 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hubert", "Mia", ""], ["Debruyne", "Michiel", ""], ["Rousseeuw", "Peter J.", ""]]}, {"id": "1709.07064", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung, Wenjia Wang, Matthew Plumlee, Benjamin Haaland", "title": "Multi-Resolution Functional ANOVA for Large-Scale, Many-Input Computer\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process is a standard tool for building emulators for both\ndeterministic and stochastic computer experiments. However, application of\nGaussian process models is greatly limited in practice, particularly for\nlarge-scale and many-input computer experiments that have become typical. We\npropose a multi-resolution functional ANOVA model as a computationally feasible\nemulation alternative. More generally, this model can be used for large-scale\nand many-input non-linear regression problems. An overlapping group lasso\napproach is used for estimation, ensuring computational feasibility in a\nlarge-scale and many-input setting. New results on consistency and inference\nfor the (potentially overlapping) group lasso in a high-dimensional setting are\ndeveloped and applied to the proposed multi-resolution functional ANOVA model.\nImportantly, these results allow us to quantify the uncertainty in our\npredictions. Numerical examples demonstrate that the proposed model enjoys\nmarked computational advantages. Data capabilities, both in terms of sample\nsize and dimension, meet or exceed best available emulation tools while meeting\nor exceeding emulation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 20:07:23 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 13:50:26 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Sung", "Chih-Li", ""], ["Wang", "Wenjia", ""], ["Plumlee", "Matthew", ""], ["Haaland", "Benjamin", ""]]}, {"id": "1709.07238", "submitter": "Rui Paulo", "authors": "Gonzalo Garcia-Donato and Rui Paulo", "title": "Handling Factors in Variable Selection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factors are categorical variables, and the values which these variables\nassume are called levels. In this paper, we consider the variable selection\nproblem where the set of potential predictors contains both factors and\nnumerical variables. Formally, this problem is a particular case of the\nstandard variable selection problem where factors are coded using dummy\nvariables. As such, the Bayesian solution would be straightforward and,\npossibly because of this, the problem, despite its importance, has not received\nmuch attention in the literature. Nevertheless, we show that this perception is\nillusory and that in fact several inputs like the assignment of prior\nprobabilities over the model space or the parameterization adopted for factors\nmay have a large (and difficult to anticipate) impact on the results. We\nprovide a solution to these issues that extends the proposals in the standard\nvariable selection problem and does not depend on how the factors are coded\nusing dummy variables. Our approach is illustrated with a real example\nconcerning a childhood obesity study in Spain.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 09:59:44 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Garcia-Donato", "Gonzalo", ""], ["Paulo", "Rui", ""]]}, {"id": "1709.07339", "submitter": "Luke Miratrix", "authors": "Devin Caughey and Allan Dafoe and Luke Miratrix", "title": "Beyond the Sharp Null: Randomization Inference, Bounded Null Hypotheses,\n  and Confidence Intervals for Maximum Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisherian randomization inference is often dismissed as testing an\nuninteresting and implausible hypothesis: the sharp null of no effects\nwhatsoever. We show that this view is overly narrow. Many randomization tests\nare also valid under a more general \"bounded\" null hypothesis under which all\neffects are weakly negative (or positive), thus accommodating heterogenous\neffects. By inverting such tests we can form one-sided confidence intervals for\nthe maximum (or minimum) effect. These properties hold for all\neffect-increasing test statistics, which include both common statistics such as\nthe mean difference and uncommon ones such as Stephenson rank statistics. The\nlatter's sensitivity to extreme effects permits detection of positive effects\neven when the average effect is negative. We argue that bounded nulls are often\nof substantive or theoretical interest, and illustrate with two applications:\ntesting monotonicity in an IV analysis and inferring effect sizes in a small\nrandomized experiment.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 14:19:27 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Caughey", "Devin", ""], ["Dafoe", "Allan", ""], ["Miratrix", "Luke", ""]]}, {"id": "1709.07498", "submitter": "Rodney Sparapani", "authors": "Brent R. Logan, Rodney Sparapani, Robert E. McCulloch and Purushottam\n  W. Laud", "title": "Decision making and uncertainty quantification for individualized\n  treatments", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individualized treatment rules (ITR) can improve health outcomes by\nrecognizing that patients may respond differently to treatment and assigning\ntherapy with the most desirable predicted outcome for each individual. Flexible\nand efficient prediction models are desired as a basis for such ITRs to handle\npotentially complex interactions between patient factors and treatment. Modern\nBayesian semiparametric and nonparametric regression models provide an\nattractive avenue in this regard as these allow natural posterior uncertainty\nquantification of patient specific treatment decisions as well as the\npopulation wide value of the prediction-based ITR. In addition, via the use of\nsuch models, inference is also available for the value of the Optimal ITR. We\npropose such an approach and implement it using Bayesian Additive Regression\nTrees (BART) as this model has been shown to perform well in fitting\nnonparametric regression functions to continuous and binary responses, even\nwith many covariates. It is also computationally efficient for use in practice.\nWith BART we investigate a treatment strategy which utilizes individualized\npredictions of patient outcomes from BART models. Posterior distributions of\npatient outcomes under each treatment are used to assign the treatment that\nmaximizes the expected posterior utility. We also describe how to approximate\nsuch a treatment policy with a clinically interpretable ITR, and quantify its\nexpected outcome. The proposed method performs very well in extensive\nsimulation studies in comparison with several existing methods. We illustrate\nthe usage of the proposed method to identify an individualized choice of\nconditioning regimen for patients undergoing hematopoietic cell transplantation\nand quantify the value of this method of choice in relation to the Optimal ITR\nas well as non-individualized treatment strategies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 19:23:10 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Logan", "Brent R.", ""], ["Sparapani", "Rodney", ""], ["McCulloch", "Robert E.", ""], ["Laud", "Purushottam W.", ""]]}, {"id": "1709.07542", "submitter": "Robert McCulloch", "authors": "Matthew Pratola, Hugh Chipman, Edward George, Robert McCulloch", "title": "Heteroscedastic BART Using Multiplicative Regression Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BART (Bayesian Additive Regression Trees) has become increasingly popular as\na flexible and scalable nonparametric regression approach for modern applied\nstatistics problems. For the practitioner dealing with large and complex\nnonlinear response surfaces, its advantages include a matrix-free formulation\nand the lack of a requirement to prespecify a confining regression basis.\nAlthough flexible in fitting the mean, BART has been limited by its reliance on\na constant variance error model. This homoscedastic assumption is unrealistic\nin many applications. Alleviating this limitation, we propose HBART, a\nnonparametric heteroscedastic elaboration of BART. In BART, the mean function\nis modeled with a sum of trees, each of which determines an additive\ncontribution to the mean. In HBART, the variance function is further modeled\nwith a product of trees, each of which determines a multiplicative contribution\nto the variance. Like the mean model, this flexible, multidimensional variance\nmodel is entirely nonparametric with no need for the prespecification of a\nconfining basis. Moreover, with this enhancement, HBART can provide insights\ninto the potential relationships of the predictors with both the mean and the\nvariance. Practical implementations of HBART with revealing new diagnostic\nplots are demonstrated with simulated and real data on used car prices, fishing\ncatch production and alcohol consumption.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 23:29:30 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 19:03:00 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Pratola", "Matthew", ""], ["Chipman", "Hugh", ""], ["George", "Edward", ""], ["McCulloch", "Robert", ""]]}, {"id": "1709.07556", "submitter": "Kyle Vincent Ph. D", "authors": "Kyle Vincent", "title": "Recent Advances on Estimating Population Size with Link-Tracing Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to estimate population size based on a stratified link-tracing\nsampling design is presented. The method extends on the Frank and Snijders\n(1994) approach by allowing for heterogeneity in the initial sample selection\nprocedure. Rao-Blackwell estimators and corresponding resampling approximations\nsimilar to that detailed in Vincent and Thompson (2017) are explored. An\nempirical application is provided for a hard-to-reach networked population. The\nresults demonstrate that the approach has much potential for application to\nsuch populations. Supplementary materials for this article are available\nonline.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 00:59:23 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Vincent", "Kyle", ""]]}, {"id": "1709.07588", "submitter": "Blakeley McShane", "authors": "Blakeley B. McShane, David Gal, Andrew Gelman, Christian Robert and\n  Jennifer L. Tackett", "title": "Abandon Statistical Significance", "comments": null, "journal-ref": "The American Statistician 2019, Vol. 73:sup1, 235-245", "doi": "10.1080/00031305.2018.1527253", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss problems the null hypothesis significance testing (NHST) paradigm\nposes for replication and more broadly in the biomedical and social sciences as\nwell as how these problems remain unresolved by proposals involving modified\np-value thresholds, confidence intervals, and Bayes factors. We then discuss\nour own proposal, which is to abandon statistical significance. We recommend\ndropping the NHST paradigm--and the p-value thresholds intrinsic to it--as the\ndefault statistical paradigm for research, publication, and discovery in the\nbiomedical and social sciences. Specifically, we propose that the p-value be\ndemoted from its threshold screening role and instead, treated continuously, be\nconsidered along with currently subordinate factors (e.g., related prior\nevidence, plausibility of mechanism, study design and data quality, real world\ncosts and benefits, novelty of finding, and other factors that vary by research\ndomain) as just one among many pieces of evidence. We have no desire to \"ban\"\np-values or other purely statistical measures. Rather, we believe that such\nmeasures should not be thresholded and that, thresholded or not, they should\nnot take priority over the currently subordinate factors. We also argue that it\nseldom makes sense to calibrate evidence as a function of p-values or other\npurely statistical measures. We offer recommendations for how our proposal can\nbe implemented in the scientific publication process as well as in statistical\ndecision making more broadly.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 03:59:24 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 00:09:41 GMT"}, {"version": "v3", "created": "Sat, 8 Sep 2018 15:25:41 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["McShane", "Blakeley B.", ""], ["Gal", "David", ""], ["Gelman", "Andrew", ""], ["Robert", "Christian", ""], ["Tackett", "Jennifer L.", ""]]}, {"id": "1709.07616", "submitter": "Simon Lyddon", "authors": "Simon Lyddon, Chris Holmes, Stephen Walker", "title": "General Bayesian Updating and the Loss-Likelihood Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the weighted likelihood bootstrap, a method that\ngenerates samples from an approximate Bayesian posterior of a parametric model.\nWe show that the same method can be derived, without approximation, under a\nBayesian nonparametric model with the parameter of interest defined as\nminimising an expected negative log-likelihood under an unknown sampling\ndistribution. This interpretation enables us to extend the weighted likelihood\nbootstrap to posterior sampling for parameters minimizing an expected loss. We\ncall this method the loss-likelihood bootstrap. We make a connection between\nthis and general Bayesian updating, which is a way of updating prior belief\ndistributions without needing to construct a global probability model, yet\nrequires the calibration of two forms of loss function. The loss-likelihood\nbootstrap is used to calibrate the general Bayesian posterior by matching\nasymptotic Fisher information. We demonstrate the methodology on a number of\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 07:25:36 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 10:40:14 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Lyddon", "Simon", ""], ["Holmes", "Chris", ""], ["Walker", "Stephen", ""]]}, {"id": "1709.07716", "submitter": "M.I. Borrajo", "authors": "M.I. Borrajo, W. Gonz\\'alez-Manteiga, M.D. Mart\\'inez-Miranda", "title": "Testing first-order intensity model in non-homogeneous Poisson point\n  processes with covariates", "comments": "30 pages (23 main doc + 7 appendix); 9 figures; 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling the first-order intensity function is one of the main aims in point\nprocess theory, and it has been approached so far from different perspectives.\nOne appealing model describes the intensity as a function of a spatial\ncovariate. In the recent literature, estimation theory and several applications\nhave been developed assuming this model, but without formally checking this\nassumption. In this paper we address this problem for a non-homogeneous Poisson\npoint process, by proposing a new test based on an $L^2$-distance. We also\nprove the asymptotic normality of the statistic and we suggest a bootstrap\nprocedure to accomplish the calibration. Two applications with real data are\npresented and a simulation study to better understand the performance of our\nproposals is accomplished. Finally some possible extensions of the present work\nto non-Poisson processes and to a multi-dimensional covariate context are\ndetailed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 12:36:43 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 12:11:43 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Borrajo", "M. I.", ""], ["Gonz\u00e1lez-Manteiga", "W.", ""], ["Mart\u00ednez-Miranda", "M. D.", ""]]}, {"id": "1709.07778", "submitter": "Abdolnasser Sadeghkhani", "authors": "\\'Eric Marchand, Abdolnasser Sadeghkhani", "title": "On predictive density estimation with additional information", "comments": "30 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on independently distributed $X_1 \\sim N_p(\\theta_1, \\sigma^2_1 I_p)$\nand $X_2 \\sim N_p(\\theta_2, \\sigma^2_2 I_p)$, we consider the efficiency of\nvarious predictive density estimators for $Y_1 \\sim N_p(\\theta_1, \\sigma^2_Y\nI_p)$, with the additional information $\\theta_1 - \\theta_2 \\in A$ and known\n$\\sigma^2_1, \\sigma^2_2, \\sigma^2_Y$. We provide improvements on benchmark\npredictive densities such as plug-in, the maximum likelihood, and the minimum\nrisk equivariant predictive densities. Dominance results are obtained for\n$\\alpha-$divergence losses and include Bayesian improvements for reverse\nKullback-Leibler loss, and Kullback-Leibler (KL) loss in the univariate case\n($p=1$). An ensemble of techniques are exploited, including variance expansion\n(for KL loss), point estimation duality, and concave inequalities.\nRepresentations for Bayesian predictive densities, and in particular for\n$\\hat{q}_{\\pi_{U,A}}$ associated with a uniform prior for $\\theta=(\\theta_1,\n\\theta_2)$ truncated to $\\{\\theta \\in \\mathbb{R}^{2p}: \\theta_1 - \\theta_2 \\in\nA \\}$, are established and are used for the Bayesian dominance findings.\nFinally and interestingly, these Bayesian predictive densities also relate to\nskew-normal distributions, as well as new forms of such distributions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:32:51 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Marchand", "\u00c9ric", ""], ["Sadeghkhani", "Abdolnasser", ""]]}, {"id": "1709.07779", "submitter": "BaoLuo Sun", "authors": "Eric J. Tchetgen Tchetgen, BaoLuo Sun, Stefan Walter", "title": "The GENIUS Approach to Robust Mendelian Randomization Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) is a popular instrumental variable (IV)\napproach, in which one or several genetic markers serve as IVs that can\nsometimes be leveraged to recover valid inferences about a given\nexposure-outcome causal association subject to unmeasured confounding. A key IV\nidentification condition known as the exclusion restriction states that the IV\ncannot have a direct effect on the outcome which is not mediated by the\nexposure in view. In MR studies, such an assumption requires an unrealistic\nlevel of prior knowledge about the mechanism by which genetic markers causally\naffect the outcome. As a result, possible violation of the exclusion\nrestriction can seldom be ruled out in practice. To address this concern, we\nintroduce a new class of IV estimators which are robust to violation of the\nexclusion restriction under data generating mechanisms commonly assumed in MR\nliterature. The proposed approach named \"MR G-Estimation under No Interaction\nwith Unmeasured Selection\" (MR GENIUS) improves on Robins' G-estimation by\nmaking it robust to both additive unmeasured confounding and violation of the\nexclusion restriction assumption. In certain key settings, MR GENIUS reduces to\nthe estimator of Lewbel (2012) which is widely used in econometrics but appears\nlargely unappreciated in MR literature. More generally, MR GENIUS generalizes\nLewbel's estimator to several key practical MR settings, including\nmultiplicative causal models for binary outcome, multiplicative and odds ratio\nexposure models, case control study design and censored survival outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:32:55 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 14:14:26 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 13:00:56 GMT"}, {"version": "v4", "created": "Sun, 1 Oct 2017 14:46:48 GMT"}, {"version": "v5", "created": "Fri, 6 Oct 2017 18:34:29 GMT"}, {"version": "v6", "created": "Sun, 2 Jun 2019 18:50:07 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Tchetgen", "Eric J. Tchetgen", ""], ["Sun", "BaoLuo", ""], ["Walter", "Stefan", ""]]}, {"id": "1709.08031", "submitter": "Georg Zimmermann", "authors": "Georg Zimmermann, Markus Pauly, Arne C. Bathke", "title": "Small-sample performance and underlying assumptions of a bootstrap-based\n  inference method for a general analysis of covariance model with possibly\n  heteroskedastic and nonnormal errors", "comments": null, "journal-ref": null, "doi": "10.1177/0962280218817796", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the standard F test is severely affected by\nheteroskedasticity in unbalanced analysis of covariance (ANCOVA) models.\nCurrently available potential remedies for such a scenario are based on\nheteroskedasticity-consistent covariance matrix estimation (HCCME). However,\nthe HCCME approach tends to be liberal in small samples. Therefore, in the\npresent manuscript, we propose a combination of HCCME and a wild bootstrap\ntechnique, with the aim of improving the small-sample performance. We precisely\nstate a set of assumptions for the general ANCOVA model and discuss their\npractical interpretation in detail, since this issue may have been somewhat\nneglected in applied research so far. We prove that these assumptions are\nsufficient to ensure the asymptotic validity of the combined HCCME-wild\nbootstrap ANCOVA. The results of our simulation study indicate that our\nproposed test remedies the problems of the ANCOVA F test and its\nheteroskedasticity-consistent alternatives in small to moderate sample size\nscenarios. Our test only requires very mild conditions, thus being applicable\nin a broad range of real-life settings, as illustrated by the detailed\ndiscussion of a dataset from preclinical research on spinal cord injury. Our\nproposed method is ready-to-use and allows for valid hypothesis testing in\nfrequently encountered settings (e.g., comparing group means while adjusting\nfor baseline measurements in a randomized controlled clinical trial).\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 10:00:01 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 09:33:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zimmermann", "Georg", ""], ["Pauly", "Markus", ""], ["Bathke", "Arne C.", ""]]}, {"id": "1709.08036", "submitter": "Guillaume Basse", "authors": "Guillaume Basse, Avi Feller, Panos Toulis", "title": "Conditional randomization tests of causal effects with interference\n  between units", "comments": "Accepted for publication in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many causal questions involve interactions between units, also known as\ninterference, for example between individuals in households, students in\nschools, or firms in markets. In this paper, we formalize the concept of a\nconditioning mechanism, which provides a framework for constructing valid and\npowerful randomization tests under general forms of interference. We describe\nour framework in the context of two-stage randomized designs and apply our\napproach to a randomized evaluation of an intervention targeting student\nabsenteeism in the School District of Philadelphia. We show improvements over\nexisting methods in terms of computational and statistical power.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 11:22:10 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 16:48:03 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 04:17:00 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Basse", "Guillaume", ""], ["Feller", "Avi", ""], ["Toulis", "Panos", ""]]}, {"id": "1709.08089", "submitter": "Thomas Lee", "authors": "Qi Gao, Randy C. S. Lai, Thomas C. M. Lee and Yao Li", "title": "Uncertainty Quantification for High Dimensional Sparse Nonparametric\n  Additive Models", "comments": null, "journal-ref": "2019, Technometrics", "doi": "10.1080/00401706.2019.1665591", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference in high dimensional settings has recently attracted\nenormous attention within the literature. However, most published work focuses\non the parametric linear regression problem. This paper considers an important\nextension of this problem: statistical inference for high dimensional sparse\nnonparametric additive models. To be more precise, this paper develops a\nmethodology for constructing a probability density function on the set of all\ncandidate models. This methodology can also be applied to construct confidence\nintervals for various quantities of interest (such as noise variance) and\nconfidence bands for the additive functions. This methodology is derived using\na generalized fiducial inference framework. It is shown that results produced\nby the proposed methodology enjoy correct asymptotic frequentist properties.\nEmpirical results obtained from numerical experimentation verify this\ntheoretical claim. Lastly, the methodology is applied to a gene expression data\nset and discovers new findings for which most existing methods based on\nparametric linear modeling failed to observe.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 17:59:04 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 17:09:24 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Gao", "Qi", ""], ["Lai", "Randy C. S.", ""], ["Lee", "Thomas C. M.", ""], ["Li", "Yao", ""]]}, {"id": "1709.08094", "submitter": "Nhat Ho", "authors": "Nhat Ho, XuanLong Nguyen, Ya'acov Ritov", "title": "Robust estimation of mixing measures in finite mixture models", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finite mixture models, apart from underlying mixing measure, true kernel\ndensity function of each subpopulation in the data is, in many scenarios,\nunknown. Perhaps the most popular approach is to choose some kernel functions\nthat we empirically believe our data are generated from and use these kernels\nto fit our models. Nevertheless, as long as the chosen kernel and the true\nkernel are different, statistical inference of mixing measure under this\nsetting will be highly unstable. To overcome this challenge, we propose\nflexible and efficient robust estimators of the mixing measure in these models,\nwhich are inspired by the idea of minimum Hellinger distance estimator, model\nselection criteria, and superefficiency phenomenon. We demonstrate that our\nestimators consistently recover the true number of components and achieve the\noptimal convergence rates of parameter estimation under both the well- and\nmis-specified kernel settings for any fixed bandwidth. These desirable\nasymptotic properties are illustrated via careful simulation studies with both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 18:37:21 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Ho", "Nhat", ""], ["Nguyen", "XuanLong", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1709.08148", "submitter": "Tong Li", "authors": "Krishnakumar Balasubramanian, Tong Li, Ming Yuan", "title": "On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reproducing kernel Hilbert space (RKHS) embedding of distributions offers\na general and flexible framework for testing problems in arbitrary domains and\nhas attracted considerable amount of attention in recent years. To gain\ninsights into their operating characteristics, we study here the statistical\nperformance of such approaches within a minimax framework. Focusing on the case\nof goodness-of-fit tests, our analyses show that a vanilla version of the\nkernel-embedding based test could be suboptimal, and suggest a simple remedy by\nmoderating the embedding. We prove that the moderated approach provides optimal\ntests for a wide range of deviations from the null and can also be made\nadaptive over a large collection of interpolation spaces. Numerical experiments\nare presented to further demonstrate the merits of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 05:28:21 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Li", "Tong", ""], ["Yuan", "Ming", ""]]}, {"id": "1709.08221", "submitter": "Mark Steel", "authors": "Mark F.J. Steel", "title": "Model Averaging and its Use in Economics", "comments": "forthcoming; accepted version", "journal-ref": "Journal of Economic Literature, 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of model averaging has become an important tool to deal with model\nuncertainty, for example in situations where a large amount of different\ntheories exist, as are common in economics. Model averaging is a natural and\nformal response to model uncertainty in a Bayesian framework, and most of the\npaper deals with Bayesian model averaging. The important role of the prior\nassumptions in these Bayesian procedures is highlighted. In addition,\nfrequentist model averaging methods are also discussed. Numerical methods to\nimplement these methods are explained, and I point the reader to some freely\navailable computational resources. The main focus is on uncertainty regarding\nthe choice of covariates in normal linear regression models, but the paper also\ncovers other, more challenging, settings, with particular emphasis on sampling\nmodels commonly used in economics. Applications of model averaging in economics\nare reviewed and discussed in a wide range of areas, among which growth\neconomics, production modelling, finance and forecasting macroeconomic\nquantities.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 16:49:05 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 09:29:53 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 17:33:20 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Steel", "Mark F. J.", ""]]}, {"id": "1709.08251", "submitter": "Alexander Aue", "authors": "Miles Lopes, Andrew Blandino and Alexander Aue", "title": "Bootstrapping spectral statistics in high dimensions", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics derived from the eigenvalues of sample covariance matrices are\ncalled spectral statistics, and they play a central role in multivariate\ntesting. Although bootstrap methods are an established approach to\napproximating the laws of spectral statistics in low-dimensional problems,\nthese methods are relatively unexplored in the high-dimensional setting. The\naim of this paper is to focus on linear spectral statistics as a class of\nprototypes for developing a new bootstrap in high-dimensions --- and we refer\nto this method as the Spectral Bootstrap. In essence, the method originates\nfrom the parametric bootstrap, and is motivated by the notion that, in high\ndimensions, it is difficult to obtain a non-parametric approximation to the\nfull data-generating distribution. From a practical standpoint, the method is\neasy to use, and allows the user to circumvent the difficulties of complex\nasymptotic formulas for linear spectral statistics. In addition to proving the\nconsistency of the proposed method, we provide encouraging empirical results in\na variety of settings. Lastly, and perhaps most interestingly, we show through\nsimulations that the method can be applied successfully to statistics outside\nthe class of linear spectral statistics, such as the largest sample eigenvalue\nand others.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 20:04:52 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 18:25:21 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Lopes", "Miles", ""], ["Blandino", "Andrew", ""], ["Aue", "Alexander", ""]]}, {"id": "1709.08258", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "On Fractionally-Supervised Classification: Weight Selection and\n  Extension to the Multivariate t-Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on fractionally-supervised classification (FSC), an approach that\nallows classification to be carried out with a fractional amount of weight\ngiven to the unlabelled points, is further developed in two respects. The\nprimary development addresses a question of fundamental importance over how to\nchoose the amount of weight given to the unlabelled points. The resolution of\nthis matter is essential because it makes FSC more readily applicable to real\nproblems. Interestingly, the resolution of the weight selection problem opens\nup the possibility of a different approach to model selection in model-based\nclustering and classification. A secondary development demonstrates that the\nFSC approach can be effective beyond Gaussian mixture models. To this end, an\nFSC approach is illustrated using mixtures of multivariate t-distributions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 21:19:03 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1709.08281", "submitter": "Linbo Wang", "authors": "Linbo Wang, Thomas S. Richardson and James M. Robins", "title": "Congenial Causal Inference with Binary Structural Nested Mean Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural nested mean models (SNMMs) are among the fundamental tools for\ninferring causal effects of time-dependent exposures from longitudinal studies.\nWith binary outcomes, however, current methods for estimating multiplicative\nand additive SNMM parameters suffer from variation dependence between the\ncausal SNMM parameters and the non-causal nuisance parameters. Estimating\nmethods for logistic SNMMs do not suffer from this dependence. Unfortunately,\nin contrast with the multiplicative and additive models, unbiased estimation of\nthe causal parameters of a logistic SNMM rely on additional modeling\nassumptions even when the treatment probabilities are known. These difficulties\nhave hindered the uptake of SNMMs in epidemiological practice, where binary\noutcomes are common. We solve the variation dependence problem for the binary\nmultiplicative SNMM by a reparametrization of the non-causal nuisance\nparameters. Our novel nuisance parameters are variation independent of the\ncausal parameters, and hence allows the fitting of a multiplicative SNMM by\nunconstrained maximum likelihood. It also allows one to construct true (i.e.\ncongenial) doubly robust estimators of the causal parameters. Along the way, we\nprove that an additive SNMM with binary outcomes does not admit a variation\nindependent parametrization, thus explaining why we restrict ourselves to the\nmultiplicative SNMM.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 23:15:26 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Wang", "Linbo", ""], ["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""]]}, {"id": "1709.08293", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Rupert E. H. Kuveke", "title": "The large sample coverage probability of confidence intervals in general\n  regression models after a preliminary hypothesis test", "comments": "Scandinavian Journal of Statistics", "journal-ref": "Scandinavian Journal of Statistics, 46, 432-445 (2019)", "doi": "10.1111/sjos.12358", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a computationally convenient formula for the large sample coverage\nprobability of a confidence interval for a scalar parameter of interest\nfollowing a preliminary hypothesis test that a specified vector parameter takes\na given value in a general regression model. Previously, this large sample\ncoverage probability could only be estimated by simulation. Our formula only\nrequires the evaluation, by numerical integration, of either a double or triple\nintegral, irrespective of the dimension of this specified vector parameter. We\nillustrate the application of this formula to a confidence interval for the log\nodds ratio of myocardial infarction when the exposure is recent oral\ncontraceptive use, following a preliminary test that two specified interactions\nin a logistic regression model are zero. For this real-life data, we compare\nthis large sample coverage probability with the actual coverage probability of\nthis confidence interval, obtained by simulation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 02:06:00 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 04:43:11 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kabaila", "Paul", ""], ["Kuveke", "Rupert E. H.", ""]]}, {"id": "1709.08535", "submitter": "Tom Michoel", "authors": "Tom Michoel", "title": "Analytic solution and stationary phase approximation for the Bayesian\n  lasso and elastic net", "comments": "Switched to new NeurIPS style file; 11 pages, 3 figures + appendices\n  29 pages, 3 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso and elastic net linear regression models impose a\ndouble-exponential prior distribution on the model parameters to achieve\nregression shrinkage and variable selection, allowing the inference of robust\nmodels from large data sets. However, there has been limited success in\nderiving estimates for the full posterior distribution of regression\ncoefficients in these models, due to a need to evaluate analytically\nintractable partition function integrals. Here, the Fourier transform is used\nto express these integrals as complex-valued oscillatory integrals over\n\"regression frequencies\". This results in an analytic expansion and stationary\nphase approximation for the partition functions of the Bayesian lasso and\nelastic net, where the non-differentiability of the double-exponential prior\nhas so far eluded such an approach. Use of this approximation leads to highly\naccurate numerical estimates for the expectation values and marginal posterior\ndistributions of the regression coefficients, and allows for Bayesian inference\nof much higher dimensional models than previously possible.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 15:05:29 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 18:29:17 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 08:09:17 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Michoel", "Tom", ""]]}, {"id": "1709.08626", "submitter": "Bruno Sudret", "authors": "E. Torre, S. Marelli, P. Embrechts, B. Sudret", "title": "A general framework for data-driven uncertainty quantification under\n  complex input dependencies using vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-012", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems subject to uncertain inputs produce uncertain responses. Uncertainty\nquantification (UQ) deals with the estimation of statistics of the system\nresponse, given a computational model of the system and a probabilistic model\nof its inputs. In engineering applications it is common to assume that the\ninputs are mutually independent or coupled by a Gaussian or elliptical\ndependence structure (copula). In this paper we overcome such limitations by\nmodelling the dependence structure of multivariate inputs as vine copulas. Vine\ncopulas are models of multivariate dependence built from simpler pair-copulas.\nThe vine representation is flexible enough to capture complex dependencies.\nThis paper formalises the framework needed to build vine copula models of\nmultivariate inputs and to combine them with virtually any UQ method. The\nframework allows for a fully automated, data-driven inference of the\nprobabilistic input model on available input data. The procedure is exemplified\non two finite element models of truss structures, both subject to inputs with\nnon-Gaussian dependence structures. For each case, we analyse the moments of\nthe model response (using polynomial chaos expansions), and perform a\nstructural reliability analysis to calculate the probability of failure of the\nsystem (using the first order reliability method and importance sampling).\nReference solutions are obtained by Monte Carlo simulation. The results show\nthat, while the Gaussian assumption yields biased statistics, the vine copula\nrepresentation achieves significantly more precise estimates, even when its\nstructure needs to be fully inferred from a limited amount of observations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:17:21 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 12:38:20 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Torre", "E.", ""], ["Marelli", "S.", ""], ["Embrechts", "P.", ""], ["Sudret", "B.", ""]]}, {"id": "1709.08764", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Binbin Lu, Paul Harris, Chris Brunsdon, Martin\n  Charlton, Tomoki Nakaya, Daniel A. Griffith", "title": "The importance of scale in spatially varying coefficient modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While spatially varying coefficient (SVC) models have attracted considerable\nattention in applied science, they have been criticized as being unstable. The\nobjective of this study is to show that capturing the \"spatial scale\" of each\ndata relationship is crucially important to make SVC modeling more stable, and\nin doing so, adds flexibility. Here, the analytical properties of six SVC\nmodels are summarized in terms of their characterization of scale. Models are\nexamined through a series of Monte Carlo simulation experiments to assess the\nextent to which spatial scale influences model stability and the accuracy of\ntheir SVC estimates. The following models are studied: (i) geographically\nweighted regression (GWR) with a fixed distance or (ii) an adaptive distance\nbandwidth (GWRa), (iii) flexible bandwidth GWR (FB-GWR) with fixed distance or\n(iv) adaptive distance bandwidths (FB-GWRa), (v) eigenvector spatial filtering\n(ESF), and (vi) random effects ESF (RE-ESF). Results reveal that the SVC models\ndesigned to capture scale dependencies in local relationships (FB-GWR, FB-GWRa\nand RE-ESF) most accurately estimate the simulated SVCs, where RE-ESF is the\nmost computationally efficient. Conversely GWR and ESF, where SVC estimates are\nnaively assumed to operate at the same spatial scale for each relationship,\nperform poorly. Results also confirm that the adaptive bandwidth GWR models\n(GWRa and FB-GWRa) are superior to their fixed bandwidth counterparts (GWR and\nFB-GWR).\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 00:38:02 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Murakami", "Daisuke", ""], ["Lu", "Binbin", ""], ["Harris", "Paul", ""], ["Brunsdon", "Chris", ""], ["Charlton", "Martin", ""], ["Nakaya", "Tomoki", ""], ["Griffith", "Daniel A.", ""]]}, {"id": "1709.08775", "submitter": "Chen Feng", "authors": "Chen Feng and Brani Vidakovic", "title": "Estimation of the Hurst Exponent Using Trimean Estimators on\n  Nondecimated Wavelet Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurst exponent is an important feature summarizing the noisy high-frequency\ndata when the inherent scaling pattern cannot be described by standard\nstatistical models. In this paper, we study the robust estimation of Hurst\nexponent based on non-decimated wavelet transforms (NDWT). The robustness is\nachieved by applying a general trimean estimator on non-decimated wavelet\ncoefficients of the transformed data. The general trimean estimator is derived\nas a weighted average of the distribution's median and quantiles, combining the\nmedian's emphasis on central values with the quantiles' attention to the\nextremes. The properties of the proposed Hurst exponent estimators are studied\nboth theoretically and numerically. Compared with other standard wavelet-based\nmethods (Veitch $\\&$ Abry (VA) method, Soltani, Simard, $\\&$ Boichu (SSB)\nmethod, median based estimators MEDL and MEDLA), our methods reduce the\nvariance of the estimators and increase the prediction precision in most cases.\nThe proposed methods are applied to a data set in high frequency pupillary\nresponse behavior (PRB) with the goal to classify individuals according to a\ndegree of their visual impairment.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 01:18:01 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Feng", "Chen", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1709.08795", "submitter": "Krishnakumar Balasubramanian", "authors": "Zhuoran Yang, Krishnakumar Balasubramanian, Han Liu", "title": "On Stein's Identity and Near-Optimal Estimation in High-dimensional\n  Index Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the parametric components of semi-parametric multiple\nindex models in a high-dimensional and non-Gaussian setting. Such models form a\nrich class of non-linear models with applications to signal processing, machine\nlearning and statistics. Our estimators leverage the score function based first\nand second-order Stein's identities and do not require the covariates to\nsatisfy Gaussian or elliptical symmetry assumptions common in the literature.\nMoreover, to handle score functions and responses that are heavy-tailed, our\nestimators are constructed via carefully thresholding their empirical\ncounterparts. We show that our estimator achieves near-optimal statistical rate\nof convergence in several settings. We supplement our theoretical results via\nsimulation experiments that confirm the theory.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 03:04:11 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 22:48:08 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Yang", "Zhuoran", ""], ["Balasubramanian", "Krishnakumar", ""], ["Liu", "Han", ""]]}, {"id": "1709.08846", "submitter": "Yichong Zhang", "authors": "Xiaobin Liu, Thomas Tao Yang and Yichong Zhang", "title": "Quasi-Bayesian Inference for Production Frontiers", "comments": "This paper was previously circulated under the title\n  \"Simulation-based Estimation and Inference of Production Frontiers\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a quasi-Bayesian method to conduct inference for the production\nfrontier. This approach combines multiple first-stage extreme quantile\nestimates by the quasi-Bayesian method to produce the point estimate and\nconfidence interval for the production frontier. We show the asymptotic\nproperties of the proposed estimator and the validity of the inference\nprocedure. The finite sample performance of our method is illustrated through\nsimulations and an empirical application.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 05:53:34 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 05:12:28 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 04:03:43 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Xiaobin", ""], ["Yang", "Thomas Tao", ""], ["Zhang", "Yichong", ""]]}, {"id": "1709.09138", "submitter": "Kyle Vincent Ph. D", "authors": "Kyle Vincent", "title": "Rao-Blackwellization to give Improved Estimates in Multi-List Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient statistics are derived for the population size and parameters of\ncommonly used closed population mark-recapture models. Rao-Blackwellization\ndetails for improving estimators that are not functions of the statistics are\npresented. As Rao-Blackwellization entails enumerating all sample reorderings\nconsistent with the sufficient statistic, Markov chain Monte Carlo resampling\nprocedures are provided to approximate the computationally intensive\nestimators. Simulation studies demonstrate that significant improvements can be\nmade with the strategy. Supplementary materials for this article are available\nonline.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 17:17:34 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 14:28:30 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Vincent", "Kyle", ""]]}, {"id": "1709.09280", "submitter": "Yasuhiro Omori", "authors": "Naoki Awaya and Yasuhiro Omori", "title": "Particle rolling MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient simulation-based methodology is proposed for the rolling window\nestimation of state space models, called particle rolling Markov chain Monte\nCarlo (MCMC) with double block sampling. In our method, which is based on\nSequential Monte Carlo (SMC), particles are sequentially updated to approximate\nthe posterior distribution for each window by learning new information and\ndiscarding old information from observations. Th particles are refreshed with\nan MCMC algorithm when the importance weights degenerate. To avoid degeneracy,\nwhich is crucial for reducing the computation time, we introduce a block\nsampling scheme and generate multiple candidates by the algorithm based on the\nconditional SMC. The theoretical discussion shows that the proposed methodology\nwith a nested structure is expressed as SMC sampling for the augmented space to\nprovide the justification. The computational performance is evaluated in\nillustrative examples, showing that the posterior distributions of the model\nparameters are accurately estimated. The proofs and additional discussions\n(algorithms and experimental results) are provided in the Supplementary\nMaterial.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 22:57:40 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 07:14:05 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 07:02:55 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2019 00:26:52 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Awaya", "Naoki", ""], ["Omori", "Yasuhiro", ""]]}, {"id": "1709.09333", "submitter": "Jeffrey Blume", "authors": "Jeffrey D. Blume, Lucy DAgostino McGowan, William D. Dupont, Robert A.\n  Greevy", "title": "Second-generation p-values: improved rigor, reproducibility, &\n  transparency in statistical analyses", "comments": "29 pages, 29 page Supplement", "journal-ref": null, "doi": "10.1371/journal.pone.0188299", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying that a statistically significant result is scientifically\nmeaningful is not only good scientific practice, it is a natural way to control\nthe Type I error rate. Here we introduce a novel extension of the p-value - a\nsecond-generation p-value - that formally accounts for scientific relevance and\nleverages this natural Type I Error control. The approach relies on a\npre-specified interval null hypothesis that represents the collection of effect\nsizes that are scientifically uninteresting or are practically null. The\nsecond-generation p-value is the proportion of data-supported hypotheses that\nare also null hypotheses. As such, second-generation p-values indicate when the\ndata are compatible with null hypotheses, or with alternative hypotheses, or\nwhen the data are inconclusive. Moreover, second-generation p-values provide a\nproper scientific adjustment for multiple comparisons and reduce false\ndiscovery rates. This is an advance for environments rich in data, where\ntraditional p-value adjustments are needlessly punitive. Second-generation\np-values promote transparency, rigor and reproducibility of scientific results\nby a priori specifying which candidate hypotheses are practically meaningful\nand by providing a more reliable statistical summary of when the data are\ncompatible with alternative or null hypotheses.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 04:50:46 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 20:44:01 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 16:18:27 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Blume", "Jeffrey D.", ""], ["McGowan", "Lucy DAgostino", ""], ["Dupont", "William D.", ""], ["Greevy", "Robert A.", ""]]}, {"id": "1709.09512", "submitter": "Eric Blankmeyer", "authors": "Eric Blankmeyer", "title": "Simultaneous-equation Estimation without Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a single equation in a system of linear equations, estimation by\ninstrumental variables is the standard approach. In practice, however, it is\noften difficult to find valid instruments. This paper proposes a maximum\nlikelihood method that does not require instrumental variables; it is\nillustrated by simulation and with a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 13:45:22 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Blankmeyer", "Eric", ""]]}, {"id": "1709.09583", "submitter": "Stephan Smeekes", "authors": "Lenard Lieb and Stephan Smeekes", "title": "Inference for Impulse Responses under Model Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many macroeconomic applications, confidence intervals for impulse\nresponses are constructed by estimating VAR models in levels - ignoring\ncointegration rank uncertainty. We investigate the consequences of ignoring\nthis uncertainty. We adapt several methods for handling model uncertainty and\nhighlight their shortcomings. We propose a new method -\nWeighted-Inference-by-Model-Plausibility (WIMP) - that takes rank uncertainty\ninto account in a data-driven way. In simulations the WIMP outperforms all\nother methods considered, delivering intervals that are robust to rank\nuncertainty, yet not overly conservative. We also study potential ramifications\nof rank uncertainty on applied macroeconomic analysis by re-assessing the\neffects of fiscal policy shocks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 15:35:17 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 12:55:54 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 11:52:41 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Lieb", "Lenard", ""], ["Smeekes", "Stephan", ""]]}, {"id": "1709.09599", "submitter": "Selden Crary", "authors": "Nikoloz Chkonia and Selden Crary", "title": "The Nu Class of Low-Degree-Truncated, Rational, Generalized Functions.\n  Ic. IMSPE-optimal designs with circular-disk prediction domains", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an extension of Part I of a series about Nu-class\nmultifunctions. A method is presented for computing the integrated mean-squared\nprediction error (IMSPE) in the design of computer experiments, when the\nprediction domain is a circular disk. The method is extensible to more than two\nfactors, to prediction domains other than squares or disks, and to a variety of\nassumed covariance functions. Three example optimal designs, under Gaussian\ncovariance with known hyperparameters, are found using the method: an n=1\ndesign centered on the disk; an n=2 continuously rotatable design with assumed\ninversion symmetry about the center of the disk; and an n=4 twin-point design\nsimilar to the n=4 twin-point design observed previously for a square\nprediction domain [1]. The four-point design on the disk demonstrates that\nnon-round boundaries are not a prerequisite for the occurrence of twin-point\noptimal designs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 16:06:12 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 15:21:58 GMT"}, {"version": "v3", "created": "Wed, 11 Oct 2017 18:01:42 GMT"}, {"version": "v4", "created": "Wed, 18 Oct 2017 21:46:33 GMT"}, {"version": "v5", "created": "Mon, 20 May 2019 04:14:57 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Chkonia", "Nikoloz", ""], ["Crary", "Selden", ""]]}, {"id": "1709.09606", "submitter": "Matteo Iacopini", "authors": "Monica Billio, Roberto Casarin, Matteo Iacopini, Sylvia Kaufmann", "title": "Bayesian Dynamic Tensor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor-valued data are becoming increasingly available in economics and this\ncalls for suitable econometric tools. We propose a new dynamic linear model for\ntensor-valued response variables and covariates that encompasses some\nwell-known econometric models as special cases. Our contribution is manifold.\nFirst, we define a tensor autoregressive process (ART), study its properties\nand derive the associated impulse response function. Second, we exploit the\nPARAFAC low-rank decomposition for providing a parsimonious parametrization and\nto incorporate sparsity effects. We also contribute to inference methods for\ntensors by developing a Bayesian framework which allows for including\nextra-sample information and for introducing shrinking effects. We apply the\nART model to time-varying multilayer networks of international trade and\ncapital stock and study the propagation of shocks across countries, over time\nand between layers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 16:24:27 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 20:18:17 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 20:51:42 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Billio", "Monica", ""], ["Casarin", "Roberto", ""], ["Iacopini", "Matteo", ""], ["Kaufmann", "Sylvia", ""]]}, {"id": "1709.09636", "submitter": "Sean Taylor", "authors": "Sean J. Taylor and Dean Eckles", "title": "Randomized experiments to detect and estimate social influence in\n  networks", "comments": "Forthcoming in Spreading Dynamics in Social Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of social influence in networks can be substantially biased in\nobservational studies due to homophily and network correlation in exposure to\nexogenous events. Randomized experiments, in which the researcher intervenes in\nthe social system and uses randomization to determine how to do so, provide a\nmethodology for credibly estimating of causal effects of social behaviors. In\naddition to addressing questions central to the social sciences, these\nestimates can form the basis for effective marketing and public policy.\n  In this review, we discuss the design space of experiments to measure social\ninfluence through combinations of interventions and randomizations. We define\nan experiment as combination of (1) a target population of individuals\nconnected by an observed interaction network, (2) a set of treatments whereby\nthe researcher will intervene in the social system, (3) a randomization\nstrategy which maps individuals or edges to treatments, and (4) a measurement\nof an outcome of interest after treatment has been assigned. We review\nexperiments that demonstrate potential experimental designs and we evaluate\ntheir advantages and tradeoffs for answering different types of causal\nquestions about social influence. We show how randomization also provides a\nbasis for statistical inference when analyzing these experiments.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 17:20:32 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Taylor", "Sean J.", ""], ["Eckles", "Dean", ""]]}, {"id": "1709.09645", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Statistical Challenges of Big Brain Network Data", "comments": "8 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the main characteristics of big brain network data that offer\nunique statistical challenges. The brain networks are biologically expected to\nbe both sparse and hierarchical. Such unique characterizations put specific\ntopological constraints onto statistical approaches and models we can use\neffectively. We explore the limitations of the current models used in the field\nand offer alternative approaches and explain new challenges.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 17:30:10 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 07:31:43 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "1709.09659", "submitter": "Katherine Wilson", "authors": "Katherine Wilson and Jon Wakefield", "title": "Pointless Continuous Spatial Surface Reconstruction", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of area-level aggregated summary data is common in many\ndisciplines including epidemiology and the social sciences. Typically, Markov\nrandom field spatial models have been employed to acknowledge spatial\ndependence and allow data-driven smoothing. In this paper, we exploit recent\ntheoretical and computational advances in continuous spatial modeling to carry\nout the reconstruction of an underlying continuous spatial surface. In\nparticular, we focus on models based on stochastic partial differential\nequations (SPDEs). We also consider the interesting case in which the aggregate\ndata are supplemented with point data. We carry out Bayesian inference, and in\nthe language of generalized linear mixed models, if the link is linear, an\nefficient implementation of the model is available via integrated nested\nLaplace approximations. For nonlinear links, we present two approaches: a fully\nBayesian implementation using a Hamiltonian Monte Carlo algorithm, and an\nempirical Bayes implementation, that is much faster, and is based on Laplace\napproximations. We examine the properties of the approach using simulation, and\nthen estimate an underlying continuous risk surface for the classic Scottish\nlip cancer data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 17:56:15 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Wilson", "Katherine", ""], ["Wakefield", "Jon", ""]]}, {"id": "1709.09705", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel, David J. Hunter, and McKalie Drown", "title": "Better estimates from binned income data: Interpolated CDFs and\n  mean-matching", "comments": "20 pages (including Appendix), 3 tables, 2 figures (+2 in Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often estimate income statistics from summaries that report the\nnumber of incomes in bins such as \\$0-10,000, \\$10,001-20,000,...,\\$200,000+.\nSome analysts assign incomes to bin midpoints, but this treats income as\ndiscrete. Other analysts fit a continuous parametric distribution, but the\ndistribution may not fit well.\n  We fit nonparametric continuous distributions that reproduce the bin counts\nperfectly by interpolating the cumulative distribution function (CDF). We also\nshow how both midpoints and interpolated CDFs can be constrained to reproduce\nthe mean of income when it is known.\n  We compare the methods' accuracy in estimating the Gini coefficients of all\n3,221 US counties. Fitting parametric distributions is very slow. Fitting\ninterpolated CDFs is much faster and slightly more accurate. Both interpolated\nCDFs and midpoints give dramatically better estimates if constrained to match a\nknown mean.\n  We have implemented interpolated CDFs in the binsmooth package for R. We have\nimplemented the midpoint method in the rpme command for Stata. Both\nimplementations can be constrained to match a known mean.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 19:13:50 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 16:55:29 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 02:52:52 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["von Hippel", "Paul T.", ""], ["Hunter", "David J.", ""], ["Drown", "McKalie", ""]]}, {"id": "1709.09723", "submitter": "Yingzhuo Zhang", "authors": "Yingzhuo Zhang, Noa Malem-Shinitski, Stephen A Allsop, Kay Tye and\n  Demba Ba", "title": "Estimating a Separably-Markov Random Field (SMuRF) from Binary\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in neuroscience is to characterize the dynamics of\nspiking from the neurons in a circuit that is involved in learning about a\nstimulus or a contingency. A key limitation of current methods to analyze\nneural spiking data is the need to collapse neural activity over time or\ntrials, which may cause the loss of information pertinent to understanding the\nfunction of a neuron or circuit. We introduce a new method that can determine\nnot only the trial-to-trial dynamics that accompany the learning of a\ncontingency by a neuron, but also the latency of this learning with respect to\nthe onset of a conditioned stimulus. The backbone of the method is a separable\ntwo-dimensional (2D) random field (RF) model of neural spike rasters, in which\nthe joint conditional intensity function of a neuron over time and trials\ndepends on two latent Markovian state sequences that evolve separately but in\nparallel. Classical tools to estimate state-space models cannot be applied\nreadily to our 2D separable RF model. We develop efficient statistical and\ncomputational tools to estimate the parameters of the separable 2D RF model. We\napply these to data collected from neurons in the pre-frontal cortex (PFC) in\nan experiment designed to characterize the neural underpinnings of the\nassociative learning of fear in mice. Overall, the separable 2D RF model\nprovides a detailed, interpretable, characterization of the dynamics of neural\nspiking that accompany the learning of a contingency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 20:18:48 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Zhang", "Yingzhuo", ""], ["Malem-Shinitski", "Noa", ""], ["Allsop", "Stephen A", ""], ["Tye", "Kay", ""], ["Ba", "Demba", ""]]}, {"id": "1709.10012", "submitter": "Sarka Brodinova", "authors": "Sarka Brodinova, Peter Filzmoser, Thomas Ortner, Christian\n  Breiteneder, Maia Zaharieva", "title": "Robust and sparse k-means clustering for high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world application scenarios, the identification of groups poses a\nsignificant challenge due to possibly occurring outliers and existing noise\nvariables. Therefore, there is a need for a clustering method which is capable\nof revealing the group structure in data containing both outliers and noise\nvariables without any pre-knowledge. In this paper, we propose a\n$k$-means-based algorithm incorporating a weighting function which leads to an\nautomatic weight assignment for each observation. In order to cope with noise\nvariables, a lasso-type penalty is used in an objective function adjusted by\nobservation weights. We finally introduce a framework for selecting both the\nnumber of clusters and variables based on a modified gap statistic. The\nconducted experiments on simulated and real-world data demonstrate the\nadvantage of the method to identify groups, outliers, and informative variables\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 15:14:49 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Brodinova", "Sarka", ""], ["Filzmoser", "Peter", ""], ["Ortner", "Thomas", ""], ["Breiteneder", "Christian", ""], ["Zaharieva", "Maia", ""]]}, {"id": "1709.10038", "submitter": "Hyungsik Roger Moon", "authors": "Khai X. Chiong, Hyungsik Roger Moon", "title": "Estimation of Graphical Models using the $L_{1,2}$ Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are recently used in economics to obtain networks\nof dependence among agents. A widely-used estimator is the Graphical Lasso\n(GLASSO), which amounts to a maximum likelihood estimation regularized using\nthe $L_{1,1}$ matrix norm on the precision matrix $\\Omega$. The $L_{1,1}$ norm\nis a lasso penalty that controls for sparsity, or the number of zeros in\n$\\Omega$. We propose a new estimator called Structured Graphical Lasso\n(SGLASSO) that uses the $L_{1,2}$ mixed norm. The use of the $L_{1,2}$ penalty\ncontrols for the structure of the sparsity in $\\Omega$. We show that when the\nnetwork size is fixed, SGLASSO is asymptotically equivalent to an infeasible\nGLASSO problem which prioritizes the sparsity-recovery of high-degree nodes.\nMonte Carlo simulation shows that SGLASSO outperforms GLASSO in terms of\nestimating the overall precision matrix and in terms of estimating the\nstructure of the graphical model. In an empirical illustration using a classic\nfirms' investment dataset, we obtain a network of firms' dependence that\nexhibits the core-periphery structure, with General Motors, General Electric\nand U.S. Steel forming the core group of firms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 16:15:30 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 21:32:41 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chiong", "Khai X.", ""], ["Moon", "Hyungsik Roger", ""]]}, {"id": "1709.10066", "submitter": "David Gerard", "authors": "David Gerard and Matthew Stephens", "title": "Empirical Bayes Shrinkage and False Discovery Rate Estimation, Allowing\n  For Unwanted Variation", "comments": "42 pages, 11 figures, 3 tables", "journal-ref": null, "doi": "10.1093/biostatistics/kxy029", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine two important ideas in the analysis of large-scale genomics\nexperiments (e.g. experiments that aim to identify genes that are\ndifferentially expressed between two conditions). The first is use of Empirical\nBayes (EB) methods to handle the large number of potentially-sparse effects,\nand estimate false discovery rates and related quantities. The second is use of\nfactor analysis methods to deal with sources of unwanted variation such as\nbatch effects and unmeasured confounders. We describe a simple modular fitting\nprocedure that combines key ideas from both these lines of research. This\nyields new, powerful EB methods for analyzing genomics experiments that account\nfor both sparse effects and unwanted variation. In realistic simulations, these\nnew methods provide significant gains in power and calibration over competing\nmethods. In real data analysis we find that different methods, while often\nconceptually similar, can vary widely in their assessments of statistical\nsignificance. This highlights the need for care in both choice of methods and\ninterpretation of results. All methods introduced in this paper are implemented\nin the R package vicar available at https://github.com/dcgerard/vicar .\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 17:13:09 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 21:24:54 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Gerard", "David", ""], ["Stephens", "Matthew", ""]]}, {"id": "1709.10248", "submitter": "Patrick Purdon", "authors": "Patrick A. Stokes and Patrick L. Purdon", "title": "In reply to Faes et al. and Barnett et al. regarding \"A study of\n  problems encountered in Granger causality analysis from a neuroscience\n  perspective\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This reply is in response to commentaries by Barnett, Barrett, and Seth\n(arXiv:1708.08001) and Faes, Stramaglia, and Marinazzo (arXiv:1708.06990) on\nour paper entitled \"A study of problems encountered in Granger causality\nanalysis from a neuroscience perspective.\" (PNAS 114(34):7063-7072. 2017). In\nour paper, we analyzed several properties of Granger-Geweke causality (GGC) and\ndiscussed potential problems in neuroscience applications. We demonstrated: (i)\nthat GGC, estimated using separate model fits, is either severely biased,\nparticularly when the true model is known, or a high variance is introduced to\novercome the bias; and (ii) that GGC does not reflect some component dynamics\nof the system. The commentaries by both Faes et al. and Barnett et al. point\nout that the computational problems of (i) are resolved by using recent\ncomputational methods. We acknowledge that these problems are indeed resolved\nby these methods. However, the traditional computation using separate model\nfits continues to be presented and applied. More fundamentally, the\ninterpretational problems stemming from (ii) are not in anyway addressed by the\nimproved methods because they are inherent to the definition of GGC. These\nproperties are indeed acknowledged by both commentaries. We have no\nmisconception of the GGC measure and do not claim that these properties are\nfacially wrong. But we do discuss at length how these properties make it\ninappropriate and misleading for common types of scientific questions, how\npresentation of GGC results without model estimates are not decipherable, and\nhow the absence of clear statements of questions of interest present further\nopportunities for misinterpretation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 06:19:45 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Stokes", "Patrick A.", ""], ["Purdon", "Patrick L.", ""]]}, {"id": "1709.10250", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Jianbo Chen, Martin J. Wainwright, Michael I. Jordan", "title": "DAGGER: A sequential algorithm for FDR control on DAGs", "comments": "29 pages, 10 figures, accepted for publication by Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a linear-time, single-pass, top-down algorithm for multiple\ntesting on directed acyclic graphs (DAGs), where nodes represent hypotheses and\nedges specify a partial ordering in which hypotheses must be tested. The\nprocedure is guaranteed to reject a sub-DAG with bounded false discovery rate\n(FDR) while satisfying the logical constraint that a rejected node's parents\nmust also be rejected. It is designed for sequential testing settings, when the\nDAG structure is known a priori, but the $p$-values are obtained selectively\n(such as in a sequence of experiments), but the algorithm is also applicable in\nnon-sequential settings when all $p$-values can be calculated in advance (such\nas variable/model selection). Our DAGGER algorithm, shorthand for Greedily\nEvolving Rejections on DAGs, provably controls the false discovery rate under\nindependence, positive dependence or arbitrary dependence of the $p$-values.\nThe DAGGER procedure specializes to known algorithms in the special cases of\ntrees and line graphs, and simplifies to the classical Benjamini-Hochberg\nprocedure when the DAG has no edges. We explore the empirical performance of\nDAGGER using simulations, as well as a real dataset corresponding to a gene\nontology, showing favorable performance in terms of time and power.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 06:38:11 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 01:21:47 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 20:06:02 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Chen", "Jianbo", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1709.10261", "submitter": "Claudio Agostinelli", "authors": "Marina Valdora, Claudio Agostinelli and Victor J. Yohai", "title": "Robust Estimation in High Dimensional Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Linear Models are routinely used in data analysis. The classical\nprocedures for estimation are based on Maximum Likelihood and it is well known\nthat the presence of outliers can have a large impact on this estimator. Robust\nprocedures are presented in the literature but they need a robust initial\nestimate in order to be computed. This is especially important for robust\nprocedures with non convex loss function such as redescending M-estimators.\nSubsampling techniques are often used to determine a robust initial estimate;\nhowever when the number of unknown parameters is large the number of subsamples\nneeded in order to have a high probability of having one subsample free of\noutliers become infeasible. Furthermore the subsampling procedure provides a\nnon deterministic starting point. Based on ideas in Pena and Yohai (1999), we\nintroduce a deterministic robust initial estimate for M-estimators based on\ntransformations Valdora and Yohai (2014) for which we also develop an\niteratively reweighted least squares algorithm. The new methods are studied by\nMonte Carlo experiments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 07:27:21 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Valdora", "Marina", ""], ["Agostinelli", "Claudio", ""], ["Yohai", "Victor J.", ""]]}, {"id": "1709.10298", "submitter": "Nadim Ballout", "authors": "Nadim Ballout, Vivian Viallon", "title": "Structure estimation of binary graphical models on stratified data:\n  application to the description of injury tables for victims of road accidents", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are used in many applications such as medical diagnostic,\ncomputer security, etc. More and more often, the estimation of such models has\nto be performed on several predefined strata of the whole population. For\ninstance, in epidemiology and clinical research, strata are often defined\naccording to age, gender, treatment or disease type, etc. In this article, we\npropose new approaches aimed at estimating binary graphical models on such\nstrata. Our approaches are obtained by combining well-known methods when\nestimating one single binary graphical model, with penalties encouraging\nstructured sparsity, and which have recently been shown appropriate when\ndealing with stratified data. Empirical comparions on synthetic data highlight\nthat our approaches generally outperform the competitors we considered. An\napplication is provided where we study associations among injuries suffered by\nvictims of road accidents according to road user type.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 09:27:18 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Ballout", "Nadim", ""], ["Viallon", "Vivian", ""]]}, {"id": "1709.10320", "submitter": "Werner M\\\"uller", "authors": "Radoslav Harman and Werner G. M\\\"uller", "title": "A design criterion for symmetric model discrimination based on nominal\n  confidence sets", "comments": "19 pages, 5 figures, 3 tables", "journal-ref": "The Biometrical Journal, 2020", "doi": "10.1002/bimj.201900074", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental design applications for discriminating between models have been\nhampered by the assumption to know beforehand which model is the true one,\nwhich is counter to the very aim of the experiment. Previous approaches to\nalleviate this requirement were either symmetrizations of asymmetric\ntechniques, or Bayesian, minimax and sequential approaches. Here we present a\ngenuinely symmetric criterion based on a linearized distance between mean-value\nsurfaces and the newly introduced tool of flexible nominal confidence sets. We\ndemonstrate the computational efficiency of the approach using the proposed\ncriterion and provide a Monte-Carlo evaluation of its discrimination\nperformance on the basis of the likelihood ratio. An application for a pair of\ncompeting models in enzyme kinetics is given.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 10:40:34 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 18:55:58 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 13:08:56 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Harman", "Radoslav", ""], ["M\u00fcller", "Werner G.", ""]]}, {"id": "1709.10330", "submitter": "Sarka Brodinova", "authors": "Sarka Brodinova, Maia Zaharieva, Peter Filzmoser, Thomas Ortner,\n  Christian Breiteneder", "title": "Clustering of imbalanced high-dimensional media data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media content in large repositories usually exhibits multiple groups of\nstrongly varying sizes. Media of potential interest often form notably smaller\ngroups. Such media groups differ so much from the remaining data that it may be\nworthy to look at them in more detail. In contrast, media with popular content\nappear in larger groups. Identifying groups of varying sizes is addressed by\nclustering of imbalanced data. Clustering highly imbalanced media groups is\nadditionally challenged by the high dimensionality of the underlying features.\nIn this paper, we present the Imbalanced Clustering (IClust) algorithm designed\nto reveal group structures in high-dimensional media data. IClust employs an\nexisting clustering method in order to find an initial set of a large number of\npotentially highly pure clusters which are then successively merged. The main\nadvantage of IClust is that the number of clusters does not have to be\npre-specified and that no specific assumptions about the cluster or data\ncharacteristics need to be made. Experiments on real-world media data\ndemonstrate that in comparison to existing methods, IClust is able to better\nidentify media groups, especially groups of small sizes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 11:15:38 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 18:27:15 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Brodinova", "Sarka", ""], ["Zaharieva", "Maia", ""], ["Filzmoser", "Peter", ""], ["Ortner", "Thomas", ""], ["Breiteneder", "Christian", ""]]}, {"id": "1709.10467", "submitter": "Willem van den Boom", "authors": "Willem van den Boom, Callie Mao, Rebecca A. Schroeder, and David B.\n  Dunson", "title": "Extrema-weighted feature extraction for functional data", "comments": "16 pages, 9 figures", "journal-ref": "Bioinformatics 34 (2018) 2457-2464", "doi": "10.1093/bioinformatics/bty120", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Although there is a rich literature on methods for assessing the\nimpact of functional predictors, the focus has been on approaches for dimension\nreduction that can fail dramatically in certain applications. Examples of\nstandard approaches include functional linear models, functional principal\ncomponents regression, and cluster-based approaches, such as latent trajectory\nanalysis. This article is motivated by applications in which the dynamics in a\npredictor, across times when the value is relatively extreme, are particularly\ninformative about the response. For example, physicians are interested in\nrelating the dynamics of blood pressure changes during surgery to post-surgery\nadverse outcomes, and it is thought that the dynamics are more important when\nblood pressure is significantly elevated or lowered.\n  Methods: We propose a novel class of extrema-weighted feature (XWF)\nextraction models. Key components in defining XWFs include the marginal density\nof the predictor, a function up-weighting values at high quantiles of this\nmarginal, and functionals characterizing local dynamics. Algorithms are\nproposed for fitting of XWF-based regression and classification models, and are\ncompared with current methods for functional predictors in simulations and a\nblood pressure during surgery application.\n  Results: XWFs find features of intraoperative blood pressure trajectories\nthat are predictive of postoperative mortality. By their nature, most of these\nfeatures cannot be found by previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 15:58:09 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Boom", "Willem van den", ""], ["Mao", "Callie", ""], ["Schroeder", "Rebecca A.", ""], ["Dunson", "David B.", ""]]}, {"id": "1709.10505", "submitter": "Ngom Papa", "authors": "Papa Ngom, Jean de Dieu Nkurunziza, Carlos Simplice Ogouyandjou", "title": "Discriminating between two models based on Bregman divergence in small\n  samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently in [1, 2], Ali-Akbar Bromideh introduced the Kullback-Leibler\nDivergence (KLD) test statistic in discrim- inating between two models. It was\nfound that the Ratio Minimized Kulback-Leibler Divergence (RMKLD) works better\nthan the Ratio of Maximized Likelihood (RML) for small sample size. The aim of\nthis paper is to generalize the works of Ali-Akbar Bromideh by proposing a\nhypothesis testing based on Bregman divergence in order to improve the process\nof choice of the model. Our aproach differs from him. After observing n data\npoints of unknown density f ; we firstly measure the closness between the bias\nreduced kernel density estimator and the first estimated candidate model.\nSecondly between the bias reduced kernel density estimator and the second\nestimated candidate model. In these two cases Bregman Divergence (BD) and the\nbias reduced kernel estimator [3] focuses on improving the con- vergence rates\nof kernel density estimators are used. Our testing procedure for model\nselection is thus based on the comparison of the value of model selection test\nstatistic to critical values from a standard normal table. We establish the\nasymptotic properties of Bregman divergence estimator and approximations of the\npower functions are deduced. The multi-step MLE process will be used to\nestimate the parameters of the models. We explain the applicability of the BD\nby a real data set and by the data generating process (DGP). The Monte Carlo\nsimulation and then the numerical analysis will be used to interpret the\nresult.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 17:31:08 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Ngom", "Papa", ""], ["Nkurunziza", "Jean de Dieu", ""], ["Ogouyandjou", "Carlos Simplice", ""]]}]