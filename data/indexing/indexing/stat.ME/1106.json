[{"id": "1106.0539", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Michael I. Jordan, Jim Pitman", "title": "Beta processes, stick-breaking, and power laws", "comments": "37 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The beta-Bernoulli process provides a Bayesian nonparametric prior for models\ninvolving collections of binary-valued features. A draw from the beta process\nyields an infinite collection of probabilities in the unit interval, and a draw\nfrom the Bernoulli process turns these into binary-valued features. Recent work\nhas provided stick-breaking representations for the beta process analogous to\nthe well-known stick-breaking representation for the Dirichlet process. We\nderive one such stick-breaking representation directly from the\ncharacterization of the beta process as a completely random measure. This\napproach motivates a three-parameter generalization of the beta process, and we\nstudy the power laws that can be obtained from this generalized beta process.\nWe present a posterior inference algorithm for the beta-Bernoulli process that\nexploits the stick-breaking representation, and we present experimental results\nfor a discrete factor-analysis model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 00:08:20 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2011 04:28:06 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Broderick", "Tamara", ""], ["Jordan", "Michael I.", ""], ["Pitman", "Jim", ""]]}, {"id": "1106.0721", "submitter": "Jingchen Liu", "authors": "Jingchen Liu, Gongjun Xu, and Zhiliang Ying", "title": "Learning Item-Attribute Relationship in Q-Matrix Based Diagnostic\n  Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent surge of interests in cognitive assessment has led to the developments\nof novel statistical models for diagnostic classification. Central to many such\nmodels is the well-known Q-matrix, which specifies the item-attribute\nrelationship. This paper proposes a principled estimation procedure for the\nQ-matrix and related model parameters. Desirable theoretic properties are\nestablished through large sample analysis. The proposed method also provides a\nplatform under which important statistical issues, such as hypothesis testing\nand model selection, can be addressed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 18:06:06 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Liu", "Jingchen", ""], ["Xu", "Gongjun", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1106.0773", "submitter": "Jose A. Diaz-Garcia", "authors": "Jose A. Diaz-Garcia and Rogelio Ramos-Quiroga", "title": "Multivariate stratified sampling by stochastic multiobjective\n  optimisation", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the allocation problem for multivariate stratified random\nsampling as a problem of integer non-linear stochastic multiobjective\nmathematical programming. With this goal in mind the asymptotic distribution of\nthe vector of sample variances is studied. Two alternative approaches are\nsuggested for solving the allocation problem for multivariate stratified random\nsampling. An example is presented by applying the different proposed\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 22:50:49 GMT"}], "update_date": "2011-06-07", "authors_parsed": [["Diaz-Garcia", "Jose A.", ""], ["Ramos-Quiroga", "Rogelio", ""]]}, {"id": "1106.1266", "submitter": "Olaf Kouamo", "authors": "Olaf Kouamo (LTCI), Eric Moulines (LTCI), Fran\\c{c}ois Roueff (LTCI)", "title": "Testing for homogeneity of variance in the wavelet domain", "comments": null, "journal-ref": "Dependence in probability and statistics, Doukhan, Paul Lang,\n  Gabriel Surgailis, Donatas Teyssi\\`ere, Gilles (Ed.) (2010) 175-205", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The danger of confusing long-range dependence with non-stationarity has been\npointed out by many authors. Finding an answer to this difficult question is of\nimportance to model time-series showing trend-like behavior, such as river\nrun-off in hydrology, historical temperatures in the study of climates changes,\nor packet counts in network traffic engineering. The main goal of this paper is\nto develop a test procedure to detect the presence of non-stationarity for a\nclass of processes whose $K$-th order difference is stationary. Contrary to\nmost of the proposed methods, the test procedure has the same distribution for\nshort-range and long-range dependence covariance stationary processes, which\nmeans that this test is able to detect the presence of non-stationarity for\nprocesses showing long-range dependence or which are unit root. The proposed\ntest is formulated in the wavelet domain, where a change in the generalized\nspectral density results in a change in the variance of wavelet coefficients at\none or several scales. Such tests have been already proposed in\n\\cite{whitcher:2001}, but these authors do not have taken into account the\ndependence of the wavelet coefficients within scales and between scales.\nTherefore, the asymptotic distribution of the test they have proposed was\nerroneous; as a consequence, the level of the test under the null hypothesis of\nstationarity was wrong. In this contribution, we introduce two test procedures,\nboth using an estimator of the variance of the scalogram at one or several\nscales. The asymptotic distribution of the test under the null is rigorously\njustified. The pointwise consistency of the test in the presence of a single\njump in the general spectral density is also be presented. A limited\nMonte-Carlo experiment is performed to illustrate our findings.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2011 07:09:23 GMT"}], "update_date": "2011-06-08", "authors_parsed": [["Kouamo", "Olaf", "", "LTCI"], ["Moulines", "Eric", "", "LTCI"], ["Roueff", "Fran\u00e7ois", "", "LTCI"]]}, {"id": "1106.1451", "submitter": "Tsagris Michail", "authors": "Michail T. Tsagris, Simon Preston and Andrew T.A. Wood", "title": "A data-based power transformation for compositional data", "comments": "Published in the proceddings of the 4th international workshop on\n  Compositional Data Analysis.\n  http://congress.cimne.com/codawork11/frontal/default.asp", "journal-ref": "Proceedings of CoDaWork'11: 4th international workshop on\n  Compositional Data Analysis, Egozcue, J.J., Tolosana-Delgado, R. and Ortego,\n  M.I. (eds.) 2011. ISBN: 978-84-87867-76-7", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data analysis is carried out either by neglecting the\ncompositional constraint and applying standard multivariate data analysis, or\nby transforming the data using the logs of the ratios of the components. In\nthis work we examine a more general transformation which includes both\napproaches as special cases. It is a power transformation and involves a single\nparameter, {\\alpha}. The transformation has two equivalent versions. The first\nis the stay-in-the-simplex version, which is the power transformation as\ndefined by Aitchison in 1986. The second version, which is a linear\ntransformation of the power transformation, is a Box-Cox type transformation.\nWe discuss a parametric way of estimating the value of {\\alpha}, which is\nmaximization of its profile likelihood (assuming multivariate normality of the\ntransformed data) and the equivalence between the two versions is exhibited.\nOther ways include maximization of the correct classification probability in\ndiscriminant analysis and maximization of the pseudo R-squared (as defined by\nAitchison in 1986) in linear regression. We examine the relationship between\nthe {\\alpha}-transformation, the raw data approach and the isometric log-ratio\ntransformation. Furthermore, we also define a suitable family of metrics\ncorresponding to the family of {\\alpha}-transformation and consider the\ncorresponding family of Frechet means.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2011 20:35:58 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2011 11:56:33 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Tsagris", "Michail T.", ""], ["Preston", "Simon", ""], ["Wood", "Andrew T. A.", ""]]}, {"id": "1106.1916", "submitter": "Atul Mallik", "authors": "Atul Mallik, Bodhisattva Sen, Moulinath Banerjee and George\n  Michailidis", "title": "Threshold estimation based on a p-value framework in dose-response and\n  regression settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use p-values to identify the threshold level at which a regression\nfunction takes off from its baseline value, a problem motivated by applications\nin toxicological and pharmacological dose-response studies and environmental\nstatistics. We study the problem in two sampling settings: one where multiple\nresponses can be obtained at a number of different covariate-levels and the\nother the standard regression setting involving limited number of response\nvalues at each covariate. Our procedure involves testing the hypothesis that\nthe regression function is at its baseline at each covariate value and then\ncomputing the potentially approximate p-value of the test. An estimate of the\nthreshold is obtained by fitting a piecewise constant function with a single\njump discontinuity, otherwise known as a stump, to these observed p-values, as\nthey behave in markedly different ways on the two sides of the threshold. The\nestimate is shown to be consistent and its finite sample properties are studied\nthrough simulations. Our approach is computationally simple and extends to the\nestimation of the baseline value of the regression function, heteroscedastic\nerrors and to time-series. It is illustrated on some real data applications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 20:55:17 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Mallik", "Atul", ""], ["Sen", "Bodhisattva", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "1106.1993", "submitter": "Sorana D. Bolboaca", "authors": "Lorentz J\\\"antschi, Sorana D. Bolboac{\\ba}, Mugur C. B{\\ba}lan and\n  Radu E. Sestra\\c{s}", "title": "Distribution fitting 13. Analysis of independent, multiplicative effect\n  of factors. Application to effect of essential oils extracts from plant\n  species on bacterial species. Application to factors of antibacterial\n  activity of plant species", "comments": "8 pages, 1 figure, 8 tables, research to be presented at Prospects\n  for the 3rd Millennium Agriculture, Biotechnology section", "journal-ref": null, "doi": null, "report-no": "DistFit13", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A factor effect study was conducted on a set of observations at the\ncontingency of a series of plant species and bacteria species regarding the\nantibacterial activity of essential oil extracts. The study reveals a very good\nagreement between the observations and the hypothesis of independent and\nmultiplicative effect of plant and bacteria species factors on the\nantibacterial activity. Shaping of the observable to a Negative Binomial\ndistribution allowed the separation of two convoluted Gamma distributions in\nthe observable further assigned to the distribution of factors. Statistics of\nthe Gamma distribution allowed estimating the ratio between diversity of plants\nfactors and bacteria factors in the antibacterial activity of essential oils\nextracts.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 09:50:45 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["J\u00e4ntschi", "Lorentz", ""], ["Bolboac{\\ba}", "Sorana D.", ""], ["B{\\ba}lan", "Mugur C.", ""], ["Sestra\u015f", "Radu E.", ""]]}, {"id": "1106.2014", "submitter": "Emmanuel Guerre", "authors": "Alain Guay, Emmanuel Guerre and Stepana Lazarova", "title": "Robust Adaptive Rate-Optimal Testing for the White Noise Hypothesis", "comments": "Article plus Supplementary Material document which groups proofs", "journal-ref": "Journal of Econometrics Volume 176, Issue 2, October 2013, Pages\n  134-145", "doi": "10.1016/j.jeconom.2013.05.001", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new test is proposed for the weak white noise null hypothesis. The test is\nbased on a new automatic choice of the order for a Box-Pierce or Hong test\nstatistic. The test uses Lobato (2001) or Kuan and Lee (2006) HAC critical\nvalues. The data-driven order choice is tailored to detect a new class of\nalternatives with autocorrelation coefficients which can be $o(n^{-1/2})$\nprovided there are enough of them. A simulation experiment illustrates the good\nbehavior of the test both under the weak white noise null and the alternative.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 10:58:35 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2011 10:54:54 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2012 17:03:49 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Guay", "Alain", ""], ["Guerre", "Emmanuel", ""], ["Lazarova", "Stepana", ""]]}, {"id": "1106.2125", "submitter": "Art B. Owen", "authors": "Art B. Owen, Dean Eckles", "title": "Bootstrapping data arrays of arbitrary order", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS547 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 895-927", "doi": "10.1214/12-AOAS547", "report-no": "IMS-AOAS-AOAS547", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a bootstrap strategy for estimating the variance of a\nmean taken over large multifactor crossed random effects data sets. We apply\nbootstrap reweighting independently to the levels of each factor, giving each\nobservation the product of independently sampled factor weights. No exact\nbootstrap exists for this problem [McCullagh (2000) Bernoulli 6 285-301]. We\nshow that the proposed bootstrap is mildly conservative, meaning biased toward\noverestimating the variance, under sufficient conditions that allow very\nunbalanced and heteroscedastic inputs. Earlier results for a resampling\nbootstrap only apply to two factors and use multinomial weights that are poorly\nsuited to online computation. The proposed reweighting approach can be\nimplemented in parallel and online settings. The results for this method apply\nto any number of factors. The method is illustrated using a 3 factor data set\nof comment lengths from Facebook.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 17:26:33 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2012 01:28:14 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2012 11:07:10 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Owen", "Art B.", ""], ["Eckles", "Dean", ""]]}, {"id": "1106.2246", "submitter": "Salim Bouzebda", "authors": "Salim Bouzebda and Mohamed Cherfi", "title": "General bootstrap for dual phi-divergence estimates", "comments": null, "journal-ref": "Journal of Probability and Statistics - 2012- Art. ID 834107", "doi": "10.1155/2012/834107", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general notion of bootstrapped $\\phi$-divergence estimates constructed by\nexchangeably weighting sample is introduced. Asymptotic properties of these\ngeneralized bootstrapped $\\phi$-divergence estimates are obtained, by mean of\nthe empirical process theory, which are applied to construct the bootstrap\nconfidence set with asymptotically correct coverage probability. Some of\npractical problems are discussed, including in particular, the choice of escort\nparameter and several examples of divergences are investigated. Simulation\nresults are provided to illustrate the finite sample performance of the\nproposed estimators.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2011 15:14:31 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2011 20:02:38 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Bouzebda", "Salim", ""], ["Cherfi", "Mohamed", ""]]}, {"id": "1106.2442", "submitter": "Marcela Svarc", "authors": "Ricardo Fraiman and Marcela Svarc", "title": "Resistant estimates for high dimensional and functional data based on\n  random projections", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We herein propose a new robust estimation method based on random projections\nthat is adaptive and, automatically produces a robust estimate, while enabling\neasy computations for high or infinite dimensional data. Under some restricted\ncontamination models, the procedure is robust and attains full efficiency. We\ntested the method using both simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2011 13:40:55 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2011 13:28:46 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2011 14:28:23 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Fraiman", "Ricardo", ""], ["Svarc", "Marcela", ""]]}, {"id": "1106.2525", "submitter": "Sumeetpal S Singh", "authors": "Pierre Del Moral, Arnaud Doucet and Sumeetpal Singh", "title": "Uniform Stability of a Particle Approximation of the Optimal Filter\n  Derivative", "comments": null, "journal-ref": null, "doi": null, "report-no": "Cambridge University Engineering Department Technical Report\n  CUED/F-INFENG/TR.668", "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo methods, also known as particle methods, are a widely\nused set of computational tools for inference in non-linear non-Gaussian\nstate-space models. In many applications it may be necessary to compute the\nsensitivity, or derivative, of the optimal filter with respect to the static\nparameters of the state-space model; for instance, in order to obtain maximum\nlikelihood model parameters of interest, or to compute the optimal controller\nin an optimal control problem. In Poyiadjis et al. [2011] an original particle\nalgorithm to compute the filter derivative was proposed and it was shown using\nnumerical examples that the particle estimate was numerically stable in the\nsense that it did not deteriorate over time. In this paper we substantiate this\nclaim with a detailed theoretical study. Lp bounds and a central limit theorem\nfor this particle approximation of the filter derivative are presented. It is\nfurther shown that under mixing conditions these Lp bounds and the asymptotic\nvariance characterized by the central limit theorem are uniformly bounded with\nrespect to the time index. We demon- strate the performance predicted by theory\nwith several numerical examples. We also use the particle approximation of the\nfilter derivative to perform online maximum likelihood parameter estimation for\na stochastic volatility model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2011 19:09:05 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Del Moral", "Pierre", ""], ["Doucet", "Arnaud", ""], ["Singh", "Sumeetpal", ""]]}, {"id": "1106.2559", "submitter": "Jay Bartroff", "authors": "Jay Bartroff, Matthew Finkelman, Tze Leung Lai", "title": "Modern Sequential Analysis and its Applications to Computerized Adaptive\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a brief review of recent advances in sequential analysis involving\nsequential generalized likelihood ratio tests, we discuss their use in\npsychometric testing and extend the asymptotic optimality theory of these\nsequential tests to the case of sequentially generated experiments, of\nparticular interest in computerized adaptive testing. We then show how these\nmethods can be used to design adaptive mastery tests, which are asymptotically\noptimal and are also shown to provide substantial improvements over currently\nused sequential and fixed length tests.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2011 20:14:59 GMT"}], "update_date": "2011-06-15", "authors_parsed": [["Bartroff", "Jay", ""], ["Finkelman", "Matthew", ""], ["Lai", "Tze Leung", ""]]}, {"id": "1106.2697", "submitter": "Samuel Gershman", "authors": "Samuel J. Gershman and David M. Blei", "title": "A Tutorial on Bayesian Nonparametric Models", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in statistical modeling is model selection, how to choose a\nmodel at an appropriate level of complexity. This problem appears in many\nsettings, most prominently in choosing the number ofclusters in mixture models\nor the number of factors in factor analysis. In this tutorial we describe\nBayesian nonparametric methods, a class of methods that side-steps this issue\nby allowing the data to determine the complexity of the model. This tutorial is\na high-level introduction to Bayesian nonparametric methods and contains\nseveral examples of their application.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 12:51:54 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2011 04:04:45 GMT"}], "update_date": "2011-08-05", "authors_parsed": [["Gershman", "Samuel J.", ""], ["Blei", "David M.", ""]]}, {"id": "1106.2791", "submitter": "Brahimi Brahim", "authors": "Brahim Brahimi, Djamel Meraghni, and Abdelhakim Necir", "title": "Distortion risk measures for sums of dependent losses", "comments": "Accepted 25 October 2010, Journal Afrika Statistika Vol. 5, N9, 2010,\n  page 260--267", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss two distinct approaches, for distorting risk measures of sums of\ndependent random variables, which preserve the property of coherence. The\nfirst, based on distorted expectations, operates on the survival function of\nthe sum. The second, simultaneously applies the distortion on the survival\nfunction of the sum and the dependence structure of risks, represented by\ncopulas. Our goal is to propose risk measures that take into account the\nfluctuations of losses and possible correlations between risk components.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 19:16:19 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2011 06:58:04 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Brahimi", "Brahim", ""], ["Meraghni", "Djamel", ""], ["Necir", "Abdelhakim", ""]]}, {"id": "1106.2848", "submitter": "Patrick J. Wolfe", "authors": "Florian Luisier, Thierry Blu, Patrick J. Wolfe", "title": "A CURE for noisy magnetic resonance images: Chi-square unbiased risk\n  estimation", "comments": "30 double-spaced pages, 11 figures; submitted for publication", "journal-ref": "IEEE Transactions on Image Processing, vol. 21, pp. 3454-3466,\n  2012", "doi": "10.1109/TIP.2012.2191565", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we derive an unbiased expression for the expected\nmean-squared error associated with continuously differentiable estimators of\nthe noncentrality parameter of a chi-square random variable. We then consider\nthe task of denoising squared-magnitude magnetic resonance image data, which\nare well modeled as independent noncentral chi-square random variables on two\ndegrees of freedom. We consider two broad classes of linearly parameterized\nshrinkage estimators that can be optimized using our risk estimate, one in the\ngeneral context of undecimated filterbank transforms, and another in the\nspecific case of the unnormalized Haar wavelet transform. The resultant\nalgorithms are computationally tractable and improve upon state-of-the-art\nmethods for both simulated and actual magnetic resonance image data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 00:56:10 GMT"}], "update_date": "2012-10-15", "authors_parsed": [["Luisier", "Florian", ""], ["Blu", "Thierry", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1106.2881", "submitter": "Ya'acov Ritov", "authors": "Ya'acov Ritov", "title": "A Random Walk with Drift: Interview with Peter J. Bickel", "comments": "Published in at http://dx.doi.org/10.1214/09-STS300 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 150-159", "doi": "10.1214/09-STS300", "report-no": "IMS-STS-STS300", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I met Peter J. Bickel for the first time in 1981. He came to Jerusalem for a\nyear; I had just started working on my Ph.D. studies. Yossi Yahav, who was my\nadvisor at this time, busy as the Dean of Social Sciences, brought us together.\nPeter became my chief thesis advisor. A year and a half later I came to\nBerkeley as a post-doc. Since then we have continued to work together. Peter\nwas first my advisor, then a teacher, and now he is also a friend. It is\nappropriate that this interview took place in two cities. We spoke together\nfirst in Jerusalem, at Mishkenot Shaananim and the Center for Research of\nRationality, and then at the University of California at Berkeley. These\nconversations were not formal interviews, but just questions that prompted\nPeter to tell his story.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 05:57:39 GMT"}], "update_date": "2011-06-16", "authors_parsed": [["Ritov", "Ya'acov", ""]]}, {"id": "1106.2887", "submitter": "Brahimi Brahim", "authors": "Brahim Brahimi, Fateh Chebana, and Abdelhakim Necir", "title": "Copula representation of bivariate L-moments : A new estimation method\n  for multiparameter 2-dimentional copula models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Serfling and Xiao (2007) extended the L-moment theory (Hosking,\n1990) to the multivariate setting. In the present paper, we focus on the\ntwo-dimension random vectors to establish a link between the bivariate\nL-moments (BLM) and the underlying bivariate copula functions. This connection\nprovides a new estimate of dependence parameters of bivariate statistical data.\nConsistency and asymptotic normality of the proposed estimator are established.\nExtensive simulation study is carried out to compare estimators based on the\nBLM, the maximum likelihood, the minimum distance and rank approximate\nZ-estimation. The obtained results show that, when the sample size increases,\nBLM-based estimation performs better as far as the bias and computation time\nare concerned. Moreover, the root mean squared error (RMSE) is quite reasonable\nand less sensitive in general to outliers than those of the above cited\nmethods. Further, we expect that the BLM method is an easy-to-use tool for the\nestimation of multiparameter copula models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 06:33:39 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2011 11:06:09 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Brahimi", "Brahim", ""], ["Chebana", "Fateh", ""], ["Necir", "Abdelhakim", ""]]}, {"id": "1106.2895", "submitter": "Robert E. Kass", "authors": "Robert E. Kass", "title": "Statistical Inference: The Big Picture", "comments": "Published in at http://dx.doi.org/10.1214/10-STS337 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 1-9", "doi": "10.1214/10-STS337", "report-no": "IMS-STS-STS337", "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics has moved beyond the frequentist-Bayesian controversies of the\npast. Where does this leave our ability to interpret results? I suggest that a\nphilosophy compatible with statistical practice, labeled here statistical\npragmatism, serves as a foundation for inference. Statistical pragmatism is\ninclusive and emphasizes the assumptions that connect statistical models with\nobserved data. I argue that introductory courses often mischaracterize the\nprocess of statistical inference and I propose an alternative \"big picture\"\ndepiction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 07:47:09 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2011 09:07:34 GMT"}], "update_date": "2011-06-23", "authors_parsed": [["Kass", "Robert E.", ""]]}, {"id": "1106.3181", "submitter": "Terrance Savitsky", "authors": "Terrance Savitsky, Marina Vannucci, Naijun Sha", "title": "Variable Selection for Nonparametric Gaussian Process Priors: Models and\n  Computational Strategies", "comments": "Published in at http://dx.doi.org/10.1214/11-STS354 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 130-149", "doi": "10.1214/11-STS354", "report-no": "IMS-STS-STS354", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified treatment of Gaussian process models that\nextends to data from the exponential dispersion family and to survival data.\nOur specific interest is in the analysis of data sets with predictors that have\nan a priori unknown form of possibly nonlinear associations to the response.\nThe modeling approach we describe incorporates Gaussian processes in a\ngeneralized linear model framework to obtain a class of nonparametric\nregression models where the covariance matrix depends on the predictors. We\nconsider, in particular, continuous, categorical and count responses. We also\nlook into models that account for survival outcomes. We explore alternative\ncovariance formulations for the Gaussian process prior and demonstrate the\nflexibility of the construction. Next, we focus on the important problem of\nselecting variables from the set of possible predictors and describe a general\nframework that employs mixture priors. We compare alternative MCMC strategies\nfor posterior inference and achieve a computationally efficient and practical\napproach. We demonstrate performances on simulated and benchmark data sets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2011 09:29:57 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Savitsky", "Terrance", ""], ["Vannucci", "Marina", ""], ["Sha", "Naijun", ""]]}, {"id": "1106.3203", "submitter": "Mathilde Bouriga", "authors": "Mathilde Bouriga and Olivier F\\'eron", "title": "Estimation of covariance matrices based on hierarchical inverse-Wishart\n  priors", "comments": "22 pages, 3 figures, 2 annexes, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on Bayesian shrinkage for covariance matrix estimation. We\nexamine posterior properties and frequentist risks of Bayesian estimators based\non new hierarchical inverse-Wishart priors. More precisely, we give the\nexistence conditions of the posterior distributions. Advantages in terms of\nnumerical simulations of posteriors are shown. A simulation study illustrates\nthe performance of the estimation procedures under three loss functions for\nrelevant sample sizes and various covariance structures.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2011 11:39:36 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Bouriga", "Mathilde", ""], ["F\u00e9ron", "Olivier", ""]]}, {"id": "1106.3211", "submitter": "Julia Salzman", "authors": "Julia Salzman, Hui Jiang, Wing Hung Wong", "title": "Statistical Modeling of RNA-Seq Data", "comments": "Published in at http://dx.doi.org/10.1214/10-STS343 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 62-83", "doi": "10.1214/10-STS343", "report-no": "IMS-STS-STS343", "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, ultra high-throughput sequencing of RNA (RNA-Seq) has been\ndeveloped as an approach for analysis of gene expression. By obtaining tens or\neven hundreds of millions of reads of transcribed sequences, an RNA-Seq\nexperiment can offer a comprehensive survey of the population of genes\n(transcripts) in any sample of interest. This paper introduces a statistical\nmodel for estimating isoform abundance from RNA-Seq data and is flexible enough\nto accommodate both single end and paired end RNA-Seq data and sampling bias\nalong the length of the transcript. Based on the derivation of minimal\nsufficient statistics for the model, a computationally feasible implementation\nof the maximum likelihood estimator of the model is provided. Further, it is\nshown that using paired end RNA-Seq provides more accurate isoform abundance\nestimates than single end sequencing at fixed sequencing depth. Simulation\nstudies are also given.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2011 12:17:05 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Salzman", "Julia", ""], ["Jiang", "Hui", ""], ["Wong", "Wing Hung", ""]]}, {"id": "1106.3220", "submitter": "Andrew Gelman", "authors": "Andrew Gelman", "title": "Bayesian Statistical Pragmatism", "comments": "Published in at http://dx.doi.org/10.1214/11-STS337C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 10-11", "doi": "10.1214/11-STS337C", "report-no": "IMS-STS-STS337C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Statistical Inference: The Big Picture\" by R. E. Kass\n[arXiv:1106.2895]\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2011 12:56:48 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Gelman", "Andrew", ""]]}, {"id": "1106.3352", "submitter": "Ryan Martin", "authors": "Ryan Martin, Surya T. Tokdar", "title": "Semiparametric inference in mixture models with predictive recursion\n  marginal likelihood", "comments": null, "journal-ref": "Biometrika, 98(3), 567-582, 2011", "doi": "10.1093/biomet/asr030", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive recursion is an accurate and computationally efficient algorithm\nfor nonparametric estimation of mixing densities in mixture models. In\nsemiparametric mixture models, however, the algorithm fails to account for any\nuncertainty in the additional unknown structural parameter. As an alternative\nto existing profile likelihood methods, we treat predictive recursion as a\nfilter approximation to fitting a fully Bayes model, whereby an approximate\nmarginal likelihood of the structural parameter emerges and can be used for\ninference. We call this the predictive recursion marginal likelihood.\nConvergence properties of predictive recursion under model mis-specification\nalso lead to an attractive construction of this new procedure. We show\npointwise convergence of a normalized version of this marginal likelihood\nfunction. Simulations compare the performance of this new marginal likelihood\napproach that of existing profile likelihood methods as well as Dirichlet\nprocess mixtures in density estimation. Mixed-effects models and an empirical\nBayes multiple testing application in time series analysis are also considered.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2011 21:22:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Martin", "Ryan", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1106.3393", "submitter": "Steven N. Goodman", "authors": "Steven N. Goodman", "title": "Discussion of \"Statistical Inference: The Big Picture\" by R. E. Kass", "comments": "Published in at http://dx.doi.org/10.1214/11-STS337A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 12-14", "doi": "10.1214/11-STS337A", "report-no": "IMS-STS-STS337A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Statistical Inference: The Big Picture\" by R. E. Kass\n[arXiv:1106.2895]\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 06:39:21 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Goodman", "Steven N.", ""]]}, {"id": "1106.3400", "submitter": "Robert McCulloch", "authors": "Robert McCulloch", "title": "Discussion of \"Statistical Inference: The Big Picture\" by R. E. Kass", "comments": "Published in at http://dx.doi.org/10.1214/11-STS337D the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 15-16", "doi": "10.1214/11-STS337D", "report-no": "IMS-STS-STS337D", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Statistical Inference: The Big Picture\" by R. E. Kass\n[arXiv:1106.2895]\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 07:19:23 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["McCulloch", "Robert", ""]]}, {"id": "1106.3406", "submitter": "Hal Stern", "authors": "Hal Stern", "title": "Discussion of \"Statistical Inference: The Big Picture\" by R. E. Kass", "comments": "Published in at http://dx.doi.org/10.1214/11-STS337B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 17-18", "doi": "10.1214/11-STS337B", "report-no": "IMS-STS-STS337B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Statistical Inference: The Big Picture\" by R. E. Kass\n[arXiv:1106.2895]\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 07:53:25 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Stern", "Hal", ""]]}, {"id": "1106.3410", "submitter": "Robert E. Kass", "authors": "Robert E. Kass", "title": "Rejoinder", "comments": "Published in at http://dx.doi.org/10.1214/11-STS337REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 19-20", "doi": "10.1214/11-STS337REJ", "report-no": "IMS-STS-STS337REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of \"Statistical Inference: The Big Picture\" by R. E. Kass\n[arXiv:1106.2895]\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 08:13:25 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Kass", "Robert E.", ""]]}, {"id": "1106.3415", "submitter": "Florian Rohart", "authors": "Florian Rohart", "title": "Multiple Hypotheses Testing For Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been developed to estimate the set of relevant variables in\na sparse linear model Y= XB+e where the dimension p of B can be much higher\nthan the length n of Y. Here we propose two new methods based on multiple\nhypotheses testing, either for ordered or non-ordered variables. Our procedures\nare inspired by the testing procedure proposed by Baraud et al (2003). The new\nprocedures are proved to be powerful under some conditions on the signal and\ntheir properties are non asymptotic. They gave better results in estimating the\nset of relevant variables than both the False Discovery Rate (FDR) and the\nLasso, both in the common case (p<n) and in the high-dimensional case (p>n).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 08:47:57 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2012 13:31:34 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Rohart", "Florian", ""]]}, {"id": "1106.3520", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen, Dominic Schuhmacher, Richard Samworth", "title": "Stochastic Search for Semiparametric Linear Regression Models", "comments": "Technical report 75, IMSV, University of Bern", "journal-ref": "From Probability to Statistics and Back: High-Dimensional Models\n  and Processes - A Festschrift in Honor of Jon A. Wellner (2013)", "doi": "10.1214/12-IMSCOLL907", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and analyzes a stochastic search method for parameter\nestimation in linear regression models in the spirit of Beran and Millar\n(1987). The idea is to generate a random finite subset of a parameter space\nwhich will automatically contain points which are very close to an unknown true\nparameter. The motivation for this procedure comes from recent work of\nDuembgen, Samworth and Schuhmacher (2011) on regression models with log-concave\nerror distributions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 15:57:30 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2011 11:10:38 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Duembgen", "Lutz", ""], ["Schuhmacher", "Dominic", ""], ["Samworth", "Richard", ""]]}, {"id": "1106.3703", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, Luis M. Rocha", "title": "Prediction and Modularity in Dynamical Systems", "comments": "v1 published in ECAL 2011 (European Conference on Artificial Life).\n  v2 fixes error in causal risk (number of parameters should be based on\n  training distribution)", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.AI cs.IT cs.LG cs.SY math.IT q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and understanding modular organizations is centrally important in\nthe study of complex systems. Several approaches to this problem have been\nadvanced, many framed in information-theoretic terms. Our treatment starts from\nthe complementary point of view of statistical modeling and prediction of\ndynamical systems. It is known that for finite amounts of training data,\nsimpler models can have greater predictive power than more complex ones. We use\nthe trade-off between model simplicity and predictive accuracy to generate\noptimal multiscale decompositions of dynamical networks into weakly-coupled,\nsimple modules. State-dependent and causal versions of our method are also\nproposed.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2011 04:20:16 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 06:53:24 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1106.3885", "submitter": "Ryan Martin", "authors": "Ryan Martin and Surya T. Tokdar", "title": "A nonparametric empirical Bayes framework for large-scale multiple\n  testing", "comments": "18 pages, 4 figures, 3 tables", "journal-ref": "Biostatistics 13(3):427-439, 2012", "doi": "10.1093/biostatistics/kxr039", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible and identifiable version of the two-groups model,\nmotivated by hierarchical Bayes considerations, that features an empirical null\nand a semiparametric mixture model for the non-null cases. We use a\ncomputationally efficient predictive recursion marginal likelihood procedure to\nestimate the model parameters, even the nonparametric mixing distribution. This\nleads to a nonparametric empirical Bayes testing procedure, which we call\nPRtest, based on thresholding the estimated local false discovery rates.\nSimulations and real-data examples demonstrate that, compared to existing\napproaches, PRtest's careful handling of the non-null density can give a much\nbetter fit in the tails of the mixture distribution which, in turn, can lead to\nmore realistic conclusions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 13:02:46 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2011 15:56:42 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2011 14:54:14 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Martin", "Ryan", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1106.3915", "submitter": "Song Song", "authors": "Song Song and Peter J. Bickel", "title": "Large Vector Auto Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular approach for nonstructural economic and financial forecasting is\nto include a large number of economic and financial variables, which has been\nshown to lead to significant improvements for forecasting, for example, by the\ndynamic factor models. A challenging issue is to determine which variables and\n(their) lags are relevant, especially when there is a mixture of serial\ncorrelation (temporal dynamics), high dimensional (spatial) dependence\nstructure and moderate sample size (relative to dimensionality and lags). To\nthis end, an \\textit{integrated} solution that addresses these three challenges\nsimultaneously is appealing. We study the large vector auto regressions here\nwith three types of estimates. We treat each variable's own lags different from\nother variables' lags, distinguish various lags over time, and is able to\nselect the variables and lags simultaneously. We first show the consequences of\nusing Lasso type estimate directly for time series without considering the\ntemporal dependence. In contrast, our proposed method can still produce an\nestimate as efficient as an \\textit{oracle} under such scenarios. The tuning\nparameters are chosen via a data driven \"rolling scheme\" method to optimize the\nforecasting performance. A macroeconomic and financial forecasting problem is\nconsidered to illustrate its superiority over existing estimators.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 14:24:08 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Song", "Song", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1106.3921", "submitter": "Song Song", "authors": "Song Song", "title": "Dynamic Large Spatial Covariance Matrix Estimation in Application to\n  Semiparametric Model Construction via Variable Clustering: the SCE approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.RM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better understand the spatial structure of large panels of economic and\nfinancial time series and provide a guideline for constructing semiparametric\nmodels, this paper first considers estimating a large spatial covariance matrix\nof the generalized $m$-dependent and $\\beta$-mixing time series (with $J$\nvariables and $T$ observations) by hard thresholding regularization as long as\n${{\\log J \\, \\cx^*(\\ct)}}/{T} = \\Co(1)$ (the former scheme with some time\ndependence measure $\\cx^*(\\ct)$) or $\\log J /{T} = \\Co(1)$ (the latter scheme\nwith some upper bounded mixing coefficient). We quantify the interplay between\nthe estimators' consistency rate and the time dependence level, discuss an\nintuitive resampling scheme for threshold selection, and also prove a general\ncross-validation result justifying this. Given a consistently estimated\ncovariance (correlation) matrix, by utilizing its natural links with graphical\nmodels and semiparametrics, after \"screening\" the (explanatory) variables, we\nimplement a novel forward (and backward) label permutation procedure to cluster\nthe \"relevant\" variables and construct the corresponding semiparametric model,\nwhich is further estimated by the groupwise dimension reduction method with\nsign constraints. We call this the SCE (screen - cluster - estimate) approach\nfor modeling high dimensional data with complex spatial structure. Finally we\napply this method to study the spatial structure of large panels of economic\nand financial time series and find the proper semiparametric structure for\nestimating the consumer price index (CPI) to illustrate its superiority over\nthe linear models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 14:35:52 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2011 06:08:31 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Song", "Song", ""]]}, {"id": "1106.4160", "submitter": "Eno Vangjeli", "authors": "Eno Vangjeli", "title": "Essentially ML ASN-Minimax double sampling plans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subject of this paper is ASN-Minimax (AM) double sampling plans by variables\nfor a normally distributed quality characteristic with unknown standard\ndeviation and two-sided specification limits. Based on the estimator p* of the\nfraction defective p, which is essentially the Maximum-Likelihood (ML)\nestimator, AM-double sampling plans are calculated by using the random\nvariables p*_1 and p*_p relating to the first and pooled samples, respectively.\nGiven p_1, p_2, {\\alpha}, and {\\beta}, no other AM-double sampling plans based\non the same estimator feature a lower maximum of the average sample number\n(ASN) while fulfilling the classical two-point condition on the corresponding\noperation characteristic (OC).\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 10:32:41 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Vangjeli", "Eno", ""]]}, {"id": "1106.4223", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "Convergence rate for predictive recursion estimation of finite mixtures", "comments": "12 pages, 1 figure", "journal-ref": "Statistics and Probability Letters, 82 (2012), pp. 378-384", "doi": "10.1016/j.spl.2011.10.023", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive recursion (PR) is a fast stochastic algorithm for nonparametric\nestimation of mixing distributions in mixture models. It is known that the PR\nestimates of both the mixing and mixture densities are consistent under fairly\nmild conditions, but currently very little is known about the rate of\nconvergence. Here I first investigate asymptotic convergence properties of the\nPR estimate under model misspecification in the special case of finite mixtures\nwith known support. Tools from stochastic approximation theory are used to\nprove that the PR estimates converge, to the best Kullback--Leibler\napproximation, at a nearly root-$n$ rate. When the support is unknown, PR can\nbe used to construct an objective function which, when optimized, yields an\nestimate the support. I apply the known-support results to derive a rate of\nconvergence for this modified PR estimate in the unknown support case, which\ncompares favorably to known optimal rates.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 15:04:08 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2011 17:11:19 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1106.4431", "submitter": "Pasi Jyl\\\"anki M. Sc.", "authors": "Pasi Jyl\\\"anki, Jarno Vanhatalo and Aki Vehtari", "title": "Gaussian Process Regression with a Student-t Likelihood", "comments": null, "journal-ref": "Journal of Machine Learning Research 12 (2011) 3227-3257", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the robust and efficient implementation of Gaussian\nprocess regression with a Student-t observation model. The challenge with the\nStudent-t model is the analytically intractable inference which is why several\napproximative methods have been proposed. The expectation propagation (EP) has\nbeen found to be a very accurate method in many empirical studies but the\nconvergence of the EP is known to be problematic with models containing\nnon-log-concave site functions such as the Student-t distribution. In this\npaper we illustrate the situations where the standard EP fails to converge and\nreview different modifications and alternative algorithms for improving the\nconvergence. We demonstrate that convergence problems may occur during the\ntype-II maximum a posteriori (MAP) estimation of the hyperparameters and show\nthat the standard EP may not converge in the MAP values in some difficult\ncases. We present a robust implementation which relies primarily on parallel EP\nupdates and utilizes a moment-matching-based double-loop algorithm with\nadaptively selected step size in difficult cases. The predictive performance of\nthe EP is compared to the Laplace, variational Bayes, and Markov chain Monte\nCarlo approximations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 12:24:03 GMT"}], "update_date": "2012-06-28", "authors_parsed": [["Jyl\u00e4nki", "Pasi", ""], ["Vanhatalo", "Jarno", ""], ["Vehtari", "Aki", ""]]}, {"id": "1106.4432", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "An approximate Bayesian marginal likelihood approach for estimating\n  finite mixtures", "comments": "16 pages, 1 figure, 3 tables", "journal-ref": "Communications in Statistics--Computation and Simulation, 42(7),\n  1533-1548, 2013", "doi": "10.1080/03610918.2012.667476", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of finite mixture models when the mixing distribution support is\nunknown is an important problem. This paper gives a new approach based on a\nmarginal likelihood for the unknown support. Motivated by a Bayesian Dirichlet\nprior model, a computationally efficient stochastic approximation version of\nthe marginal likelihood is proposed and large-sample theory is presented. By\nrestricting the support to a finite grid, a simulated annealing method is\nemployed to maximize the marginal likelihood and estimate the support. Real and\nsimulated data examples show that this novel stochastic\napproximation--simulated annealing procedure compares favorably to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 12:25:10 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2011 17:16:21 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2011 14:52:48 GMT"}, {"version": "v4", "created": "Mon, 13 Feb 2012 14:43:18 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1106.4461", "submitter": "Marianna Pensky", "authors": "Anestis Antoniadis, Marianna Pensky and Theofanis Sapatinas", "title": "Nonparametric Regression Estimation Based on Spatially Inhomogeneous\n  Data: Minimax Global Convergence Rates and Adaptivity", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the nonparametric regression estimation problem of recovering an\nunknown response function f on the basis of spatially inhomogeneous data when\nthe design points follow a known compactly supported density g with a finite\nnumber of well separated zeros. In particular, we consider two different cases:\nwhen g has zeros of a polynomial order and when g has zeros of an exponential\norder. These two cases correspond to moderate and severe data losses,\nrespectively. We obtain asymptotic minimax lower bounds for the global risk of\nan estimator of f and construct adaptive wavelet nonlinear thresholding\nestimators of f which attain those minimax convergence rates (up to a\nlogarithmic factor in the case of a zero of a polynomial order), over a wide\nrange of Besov balls.\n  The spatially inhomogeneous ill-posed problem that we investigate is\ninherently more difficult than spatially homogeneous problems like, e.g.,\ndeconvolution. In particular, due to spatial irregularity, assessment of\nminimax global convergence rates is a much harder task than the derivation of\nminimax local convergence rates studied recently in the literature.\nFurthermore, the resulting estimators exhibit very different behavior and\nminimax global convergence rates in comparison with the solution of spatially\nhomogeneous ill-posed problems. For example, unlike in deconvolution problem,\nthe minimax global convergence rates are greatly influenced not only by the\nextent of data loss but also by the degree of spatial homogeneity of f.\nSpecifically, even if 1/g is not integrable, one can recover f as well as in\nthe case of an equispaced design (in terms of minimax global convergence rates)\nwhen it is homogeneous enough since the estimator is \"borrowing strength\" in\nthe areas where f is adequately sampled.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 14:24:34 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2011 14:10:30 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2011 22:21:16 GMT"}, {"version": "v4", "created": "Tue, 21 Feb 2012 17:46:18 GMT"}, {"version": "v5", "created": "Fri, 26 Oct 2012 15:15:33 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Antoniadis", "Anestis", ""], ["Pensky", "Marianna", ""], ["Sapatinas", "Theofanis", ""]]}, {"id": "1106.4490", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Simple estimators of false discovery rates given as few as one or two\n  p-values without strong parametric assumptions", "comments": null, "journal-ref": "Statistical Applications in Genetics and Molecular Biology 12,\n  529-543 (2013)", "doi": "10.1515/sagmb-2013-0003", "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple comparison procedures that control a family-wise error rate or false\ndiscovery rate provide an achieved error rate as the adjusted p-value for each\nhypothesis tested. However, since such p-values are not probabilities that the\nnull hypotheses are true, empirical Bayes methods have been devised to estimate\nsuch posterior probabilities, called local false discovery rates (LFDRs) to\nemphasize the frequency interpretation of their priors. The main approaches to\nLFDR estimation, relying either on numerical algorithms to maximize likelihood\nor on the selection of smoothing parameters for nonparametric density\nestimation, lack the automatic nature of the methods of error rate control. To\nbegin filling the gap, this paper introduces automatic methods of LFDR\nestimation with proven asymptotic conservatism under the independence of\np-values but without strong parametric assumptions. Simulations indicate that\nthey remain conservative even for very small numbers of hypotheses. One of the\nproposed procedures enables interpreting the original FDR control rule in terms\nof LFDR estimation, thereby facilitating practical interpretation. The most\nconservative of the new procedures is applied to measured abundance levels of\n20 proteins.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 16:00:25 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Bickel", "David R.", ""]]}, {"id": "1106.4500", "submitter": "Ya'acov Ritov", "authors": "E. Greenshtein and Y. Ritov", "title": "Re-calibration of sample means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of calibration and the GREG method as suggested and\nstudied in Deville and Sarndal (1992). We show that a GREG type estimator is\ntypically not minimal variance unbiased estimator even asymptotically. We\nsuggest a similar estimator which is unbiased but is asymptotically with a\nminimal variance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 16:49:27 GMT"}], "update_date": "2011-06-23", "authors_parsed": [["Greenshtein", "E.", ""], ["Ritov", "Y.", ""]]}, {"id": "1106.4729", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya,\n  Masashi Sugiyama", "title": "Relative Density-Ratio Estimation for Robust Distribution Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divergence estimators based on direct approximation of density-ratios without\ngoing through separate approximation of numerator and denominator densities\nhave been successfully applied to machine learning tasks that involve\ndistribution comparison such as outlier detection, transfer learning, and\ntwo-sample homogeneity test. However, since density-ratio functions often\npossess high fluctuation, divergence estimation is still a challenging task in\npractice. In this paper, we propose to use relative divergences for\ndistribution comparison, which involves approximation of relative\ndensity-ratios. Since relative density-ratios are always smoother than\ncorresponding ordinary density-ratios, our proposed method is favorable in\nterms of the non-parametric convergence speed. Furthermore, we show that the\nproposed divergence estimator has asymptotic variance independent of the model\ncomplexity under a parametric setup, implying that the proposed estimator\nhardly overfits even with complex models. Through experiments, we demonstrate\nthe usefulness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 14:05:34 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Yamada", "Makoto", ""], ["Suzuki", "Taiji", ""], ["Kanamori", "Takafumi", ""], ["Hachiya", "Hirotaka", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1106.4739", "submitter": "Krzysztof {\\L}atuszy\\'{n}ski", "authors": "Krzysztof {\\L}atuszy\\'nski, B{\\l}a\\.zej Miasojedow, Wojciech Niemiro", "title": "Nonasymptotic bounds on the estimation error of MCMC algorithms", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ442 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm). arXiv admin\n  note: text overlap with arXiv:0907.4915", "journal-ref": "Bernoulli 2013, Vol. 19, No. 5A, 2033-2066", "doi": "10.3150/12-BEJ442", "report-no": "IMS-BEJ-BEJ442", "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of upper bounding the mean square error of MCMC\nestimators. Our analysis is nonasymptotic. We first establish a general result\nvalid for essentially all ergodic Markov chains encountered in Bayesian\ncomputation and a possibly unbounded target function $f$. The bound is sharp in\nthe sense that the leading term is exactly $\\sigma_{\\mathrm {as}}^2(P,f)/n$,\nwhere $\\sigma_{\\mathrm{as}}^2(P,f)$ is the CLT asymptotic variance. Next, we\nproceed to specific additional assumptions and give explicit computable bounds\nfor geometrically and polynomially ergodic Markov chains under quantitative\ndrift conditions. As a corollary, we provide results on confidence estimation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 14:35:04 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2012 10:35:24 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2013 10:09:55 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Niemiro", "Wojciech", ""]]}, {"id": "1106.5242", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov", "title": "High Dimensional Sparse Econometric Models: An Introduction", "comments": null, "journal-ref": "Inverse Problems and High-Dimensional Estimation, Lecture Notes in\n  Statistics, Vol. 203, 2011, pp. 121-156", "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we discuss conceptually high dimensional sparse econometric\nmodels as well as estimation of these models using L1-penalization and\npost-L1-penalization methods. Focusing on linear and nonparametric regression\nframeworks, we discuss various econometric examples, present basic theoretical\nresults, and illustrate the concepts and methods with Monte Carlo simulations\nand an empirical application. In the application, we examine and confirm the\nempirical validity of the Solow-Swan model for international economic growth.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 18:21:14 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2011 02:20:42 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "1106.5348", "submitter": "Elvira Romano Dr", "authors": "Elvira Romano, Antonio Balzanella, Rosanna Verde", "title": "Revealing spatial variability structures of geostatistical functional\n  data via Dynamic Clustering", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several environmental applications data are functions of time, essentially\ncon- tinuous, observed and recorded discretely, and spatially correlated. Most\nof the methods for analyzing such data are extensions of spatial statistical\ntools which deal with spatially dependent functional data. In such framework,\nthis paper introduces a new clustering method. The main features are that it\nfinds groups of functions that are similar to each other in terms of their\nspatial functional variability and that it locates a set of centers which\nsummarize the spatial functional variability of each cluster. The method\noptimizes, through an iterative algorithm, a best fit criterion between the\npartition of the curves and the representative element of the clusters, assumed\nto be a variogram function. The performance of the proposed clustering method\nwas evaluated by studying the results obtained through the application on\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 10:24:32 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Romano", "Elvira", ""], ["Balzanella", "Antonio", ""], ["Verde", "Rosanna", ""]]}, {"id": "1106.5714", "submitter": "Oliver Johnson", "authors": "Oliver Johnson, Dino Sejdinovic, James Cruise, Ayalvadi Ganesh, Robert\n  Piechocki", "title": "Non-parametric change-point detection using string matching algorithms", "comments": null, "journal-ref": "Methodology and Computing in Applied Probability. 16(4) p.\n  987-1008 (2014)", "doi": "10.1007/s11009-013-9359-2", "report-no": null, "categories": "math.PR cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the output of a data source taking values in a finite alphabet, we wish\nto detect change-points, that is times when the statistical properties of the\nsource change. Motivated by ideas of match lengths in information theory, we\nintroduce a novel non-parametric estimator which we call CRECHE (CRossings\nEnumeration CHange Estimator). We present simulation evidence that this\nestimator performs well, both for simulated sources and for real data formed by\nconcatenating text sources. For example, we show that we can accurately detect\nthe point at which a source changes from a Markov chain to an IID source with\nthe same stationary distribution. Our estimator requires no assumptions about\nthe form of the source distribution, and avoids the need to estimate its\nprobabilities. Further, we establish consistency of the CRECHE estimator under\na related toy model, by establishing a fluid limit and using martingale\narguments.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 16:03:49 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2011 11:36:34 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Johnson", "Oliver", ""], ["Sejdinovic", "Dino", ""], ["Cruise", "James", ""], ["Ganesh", "Ayalvadi", ""], ["Piechocki", "Robert", ""]]}, {"id": "1106.5779", "submitter": "Anjishnu  Banerjee", "authors": "Anjishnu Banerjee, David Dunson and Surya Tokdar", "title": "Efficient Gaussian Process Regression for Large Data Sets", "comments": "23 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are widely used in nonparametric regression,\nclassification and spatio-temporal modeling, motivated in part by a rich\nliterature on theoretical properties. However, a well known drawback of GPs\nthat limits their use is the expensive computation, typically O($n^3$) in\nperforming the necessary matrix inversions with $n$ denoting the number of data\npoints. In large data sets, data storage and processing also lead to\ncomputational bottlenecks and numerical stability of the estimates and\npredicted values degrades with $n$. To address these problems, a rich variety\nof methods have been proposed, with recent options including predictive\nprocesses in spatial data analysis and subset of regressors in machine\nlearning. The underlying idea in these approaches is to use a subset of the\ndata, leading to questions of sensitivity to the subset and limitations in\nestimating fine scale structure in regions that are not well covered by the\nsubset. Motivated by the literature on compressive sensing, we propose an\nalternative random projection of all the data points onto a lower-dimensional\nsubspace. We demonstrate the superiority of this approach from a theoretical\nperspective and through the use of simulated and real data examples. Some\nKeywords: Bayesian; Compressive Sensing; Dimension Reduction; Gaussian\nProcesses; Random Projections; Subset Selection\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 19:55:22 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Banerjee", "Anjishnu", ""], ["Dunson", "David", ""], ["Tokdar", "Surya", ""]]}, {"id": "1106.5837", "submitter": "Tso-Jung Yen", "authors": "Tso-Jung Yen and Yu-Min Yen", "title": "Grouped Variable Selection via Nested Spike and Slab Priors", "comments": "47 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study grouped variable selection problems by proposing a\nspecified prior, called the nested spike and slab prior, to model collective\nbehavior of regression coefficients. At the group level, the nested spike and\nslab prior puts positive mass on the event that the l2-norm of the grouped\ncoefficients is equal to zero. At the individual level, each coefficient is\nassumed to follow a spike and slab prior. We carry out maximum a posteriori\nestimation for the model by applying blockwise coordinate descent algorithms to\nsolve an optimization problem involving an approximate objective modified by\nmajorization-minimization techniques. Simulation studies show that the proposed\nestimator performs relatively well in the situations in which the true and\nredundant covariates are both covered by the same group. Asymptotic analysis\nunder a frequentist's framework further shows that the l2 estimation error of\nthe proposed estimator can have a better upper bound if the group that covers\nthe true covariates does not cover too many redundant covariates. In addition,\ngiven some regular conditions hold, the proposed estimator is asymptotically\ninvariant to group structures, and its model selection consistency can be\nestablished without imposing irrepresentable-type conditions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 03:48:20 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Yen", "Tso-Jung", ""], ["Yen", "Yu-Min", ""]]}, {"id": "1106.5919", "submitter": "Oliver Ratmann", "authors": "Oliver Ratmann, Pierre Pudlo, Sylvia Richardson, Christian Robert", "title": "Monte Carlo algorithms for model assessment via conflicting summaries", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of statistical methods and numerical algorithms for model\nchoice is vital to many real-world applications. In practice, the ABC approach\ncan be instrumental for sequential model design; however, the theoretical basis\nof its use has been questioned. We present a measure-theoretic framework for\nusing the ABC error towards model choice and describe how easily existing\nrejection, Metropolis-Hastings and sequential importance sampling ABC\nalgorithms are extended for the purpose of model checking. Considering a panel\nof applications from evolutionary biology to dynamic systems, we discuss the\nchoice of summaries which differs from standard ABC approaches. The methods and\nalgorithms presented here may provide the workhorse machinery for an\nexploratory approach to ABC model choice, particularly as the application of\nstandard Bayesian tools can prove impossible.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 12:18:45 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Ratmann", "Oliver", ""], ["Pudlo", "Pierre", ""], ["Richardson", "Sylvia", ""], ["Robert", "Christian", ""]]}, {"id": "1106.5950", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet, Jonny O'Muircheartaigh, Owen G. O'Daly and Andrew\n  Simmons", "title": "Topological Randomness and Number of Edges Predict Modular Structure in\n  Functional Brain Networks", "comments": "Submitted as a letter to the Proceedings of the National Academy of\n  Sciences (PNAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, Bassett et al. (2011) have analyzed the static and dynamic\norganization of functional brain networks in humans. We here focus on the first\nclaim made in this paper, which states that the static modular structure of\nsuch networks is nested with respect to time. Bassett et al. (2011) argue that\nthis graded structure underlines a \"multiscale modular structure\". In this\nletter, however, we show that such a relationship is substantially mediated by\nan increase in the random variation of the correlation coefficients computed at\ndifferent time scales.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 14:41:20 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["O'Muircheartaigh", "Jonny", ""], ["O'Daly", "Owen G.", ""], ["Simmons", "Andrew", ""]]}, {"id": "1106.5981", "submitter": "Mikko Tuomi", "authors": "Mikko Tuomi, David Pinfield, and Hugh R. A. Jones", "title": "Application of Bayesian model inadequacy criterion for multiple data\n  sets to radial velocity models of exoplanet systems", "comments": "10 pages, accepted for publication in the Astronomy and Astrophysics", "journal-ref": null, "doi": "10.1051/0004-6361/201117278", "report-no": null, "categories": "astro-ph.EP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple mathematical criterion for determining whether a given\nstatistical model does not describe several independent sets of measurements,\nor data modes, adequately. We derive this criterion for two data sets and\ngeneralise it to several sets by using the Bayesian updating of the posterior\nprobability density. To demonstrate the usage of the criterion, we apply it to\nobservations of exoplanet host stars by re-analysing the radial velocities of\nHD 217107, Gliese 581, and \\u{psion} Andromedae and show that the currently\nused models are not necessarily adequate in describing the properties of these\nmeasurements. We show that while the two data sets of Gliese 581 can be\nmodelled reasonably well, the noise model of HD 217107 needs to be revised. We\nalso reveal some biases in the radial velocities of \\u{psion} Andromedae and\nreport updated orbital parameters for the recently proposed 4-planet model.\nBecause of the generality of our criterion, no assumptions are needed on the\nnature of the measurements, models, or model parameters. The method we propose\ncan be applied to any astronomical problems, as well as outside the field of\nastronomy, because it is a simple consequence of the Bayes' rule of conditional\nprobabilities.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 15:53:06 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Tuomi", "Mikko", ""], ["Pinfield", "David", ""], ["Jones", "Hugh R. A.", ""]]}, {"id": "1106.6002", "submitter": "Ulrike Schneider", "authors": "Benedikt M. P\\\"otscher, Ulrike Schneider", "title": "Distributional Results for Thresholding Estimators in High-Dimensional\n  Gaussian Regression Models", "comments": "minor corrections", "journal-ref": "Electron. J. Statist. 5 (2011), 1876-1934", "doi": "10.1214/11-EJS659", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of hard-, soft-, and adaptive soft-thresholding\nestimators within a linear regression model where the number of parameters k\ncan depend on sample size n and may diverge with n. In addition to the case of\nknown error-variance, we define and study versions of the estimators when the\nerror-variance is unknown. We derive the finite-sample distribution of each\nestimator and study its behavior in the large-sample limit, also investigating\nthe effects of having to estimate the variance when the degrees of freedom n-k\ndoes not tend to infinity or tends to infinity very slowly. Our analysis\nencompasses both the case where the estimators are tuned to perform consistent\nmodel selection and the case where the estimators are tuned to perform\nconservative model selection. Furthermore, we discuss consistency, uniform\nconsistency and derive the uniform convergence rate under either type of\ntuning.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 17:06:24 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2011 14:57:35 GMT"}, {"version": "v3", "created": "Fri, 16 Dec 2011 18:46:33 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Schneider", "Ulrike", ""]]}, {"id": "1106.6147", "submitter": "Etienne Roquain", "authors": "Pierre Neuvial (SG), Etienne Roquain (LPMA)", "title": "On false discovery rate thresholding for classification under sparsity", "comments": null, "journal-ref": "Annals of Statistics (2012) Vol. 40, No. 5, 2572-2600", "doi": "10.1214/12-AOS1042", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of false discovery rate (FDR) thresholding, viewed as\na classification procedure. The \"0\"-class (null) is assumed to have a known\ndensity while the \"1\"-class (alternative) is obtained from the \"0\"-class either\nby translation or by scaling. Furthermore, the \"1\"-class is assumed to have a\nsmall number of elements w.r.t. the \"0\"-class (sparsity). We focus on densities\nof the Subbotin family, including Gaussian and Laplace models. Nonasymptotic\noracle inequalities are derived for the excess risk of FDR thresholding. These\ninequalities lead to explicit rates of convergence of the excess risk to zero,\nas the number m of items to be classified tends to infinity and in a regime\nwhere the power of the Bayes rule is away from 0 and 1. Moreover, these\ntheoretical investigations suggest an explicit choice for the target level\n$\\alpha_m$ of FDR thresholding, as a function of m. Our oracle inequalities\nshow theoretically that the resulting FDR thresholding adapts to the unknown\nsparsity regime contained in the data. This property is illustrated with\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 08:55:18 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2012 12:17:28 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2013 20:49:26 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["Neuvial", "Pierre", "", "SG"], ["Roquain", "Etienne", "", "LPMA"]]}]