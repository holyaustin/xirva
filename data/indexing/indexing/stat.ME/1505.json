[{"id": "1505.00003", "submitter": "Dimitris Kugiumtzis", "authors": "G. Papadopoulos and D. Kugiumtzis", "title": "Estimation of connectivity measures in gappy time series", "comments": "20 pages, 10 figures, submitted to Physica A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to compute connectivity measures on multivariate\ntime series with gaps. Rather than removing or filling the gaps, the rows of\nthe joint data matrix containing empty entries are removed and the calculations\nare done on the remainder matrix. The method, called measure adapted gap\nremoval (MAGR), can be applied to any connectivity measure that uses a joint\ndata matrix, such as cross correlation, cross mutual information and transfer\nentropy. MAGR is favorably compared using these three measures to a number of\nknown gap-filling techniques, as well as the gap closure. The superiority of\nMAGR is illustrated on time series from synthetic systems and financial time\nseries.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 22:35:10 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Papadopoulos", "G.", ""], ["Kugiumtzis", "D.", ""]]}, {"id": "1505.00044", "submitter": "Patrick Staples", "authors": "Patrick C. Staples, Elizabeth L. Ogburn, and Jukka-Pekka Onnela", "title": "Incorporating Contact Network Structure in Cluster Randomized Trials", "comments": "20 Pages, 6 Figures, and 2 Tables. Supplement contains 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whenever possible, the efficacy of a new treatment, such as a drug or\nbehavioral intervention, is investigated by randomly assigning some individuals\nto a treatment condition and others to a control condition, and comparing the\noutcomes between the two groups. Often, when the treatment aims to slow an\ninfectious disease, groups or clusters of individuals are assigned en masse to\neach treatment arm. The structure of interactions within and between clusters\ncan reduce the power of the trial, i.e. the probability of correctly detecting\na real treatment effect. We investigate the relationships among power,\nwithin-cluster structure, between-cluster mixing, and infectivity by simulating\nan infectious process on a collection of clusters. We demonstrate that current\npower calculations may be conservative for low levels of between-cluster\nmixing, but failing to account for moderate or high amounts can result in\nseverely underpowered studies. Power also depends on within-cluster network\nstructure for certain kinds of infectious spreading. Infections that spread\nopportunistically through very highly connected individuals have unpredictable\ninfectious breakouts, which makes it harder to distinguish between random\nvariation and real treatment effects. Our approach can be used before\nconducting a trial to assess power using network information if it is\navailable, and we demonstrate how empirical data can inform the extent of\nbetween-cluster mixing.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 22:03:22 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Staples", "Patrick C.", ""], ["Ogburn", "Elizabeth L.", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1505.00045", "submitter": "Karina Yaginuma Yuriko", "authors": "Karina Y. Yaginuma", "title": "A stochastic system with infinite interacting components to model the\n  time evolution of the membrane potentials of a population of neurons", "comments": "arXiv admin note: text overlap with arXiv:0904.1845 by other authors", "journal-ref": null, "doi": "10.1007/s10955-016-1490-3", "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new class of interacting particle systems with a countable\nnumber of interacting components. The system represents the time evolution of\nthe membrane potentials of an infinite set of interacting neurons. We prove the\nexistence and uniqueness of the process, using a perfect simulation procedure.\nWe show that this algorithm is successful, that is, we show that the number of\nsteps of the algorithm is almost surely finite. We also construct a perfect\nsimulation procedure for the coupling of a process with a finite number of\nneurons and the process with a infinite number of neurons. As a consequence, we\nobtain an upper bound for the error that we make when sampling from a finite\nset of neurons instead of the infinite set of neurons.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 22:06:57 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Yaginuma", "Karina Y.", ""]]}, {"id": "1505.00202", "submitter": "Pierre Druilhet", "authors": "Pierre Druilhet", "title": "On the Flatland Paradox", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the flatland paradox proposed by \\cite{ston1976} which is an\nexample of non-conglomerability. The aim of the paper is to show that the\nimproperness of the prior is not directly involved in the inconsistency. First,\nwe show that the choice of a flat prior is not adapted to the structure of the\nparameter space and we consider an improper prior based on reference priors\nwith nuisance parameter for which the Bayesian analysis matches the intuitive\nreasoning. Then, we propose an analysis by considering the flat prior as limit\nof proper uniform priors. In order to use limiting arguments, we must make a\ndistinction between two different Bayesian paradigms. The first one is related\nto the marginal model whereas the second one is related to the conditional\nmodel. For the latter approach, we show that the inconsistency remains even\nwith proper priors provided that we reconsider the interpretation of prior\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 15:32:50 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 09:40:37 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 08:44:52 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Druilhet", "Pierre", ""]]}, {"id": "1505.00252", "submitter": "Jihnhee Yu", "authors": "Jihnhee Yu, Luge Yang, Albert Vexler, Alan D. Hutson", "title": "A Generalized Empirical Likelihood Approach for Two-Group Comparisons\n  Given a U-Statistic Constraint", "comments": "39 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a generalized empirical likelihood approach in a two-group\nsetting where the constraints on parameters have a form of U-statistics. In\nthis situation, the summands that consist of the constraints for the empirical\nlikelihood are not independent, and a weight of each summand may not have a\ndirect interpretation as a probability point mass, dissimilar to the common\nempirical likelihood constraints based on independent summands. We show that\nthe resulting empirical likelihood ratio statistic has a weighted chi-squared\ndistribution in the univariate case and a combination of weighted chi-squared\ndistributions in the multivariate case. Through an extensive Monte-Carlo study,\nwe show that the proposed methods applied for some well-known U-statistics have\nrobust Type I error control under various underlying distributions including\ncases with a violation of exchangeability under null hypotheses. For the\napplication, we employ the proposed methods to test hypotheses in crossover\ndesigns demonstrating an adaptability of the proposed methods in various\nhypothesis tests.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 19:18:25 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Yu", "Jihnhee", ""], ["Yang", "Luge", ""], ["Vexler", "Albert", ""], ["Hutson", "Alan D.", ""]]}, {"id": "1505.00314", "submitter": "Nirav Bhatt", "authors": "Shankar Narasimhan and Nirav Bhatt", "title": "Deconstructing Principal Component Analysis Using a Data Reconciliation\n  Perspective", "comments": null, "journal-ref": "Computers and Chemical Engineering 77 (2015) 74-84", "doi": "10.1016/j.compchemeng.2015.03.016", "report-no": null, "categories": "cs.LG cs.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data reconciliation (DR) and Principal Component Analysis (PCA) are two\npopular data analysis techniques in process industries. Data reconciliation is\nused to obtain accurate and consistent estimates of variables and parameters\nfrom erroneous measurements. PCA is primarily used as a method for reducing the\ndimensionality of high dimensional data and as a preprocessing technique for\ndenoising measurements. These techniques have been developed and deployed\nindependently of each other. The primary purpose of this article is to\nelucidate the close relationship between these two seemingly disparate\ntechniques. This leads to a unified framework for applying PCA and DR. Further,\nwe show how the two techniques can be deployed together in a collaborative and\nconsistent manner to process data. The framework has been extended to deal with\npartially measured systems and to incorporate partial knowledge available about\nthe process model.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 06:20:08 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Narasimhan", "Shankar", ""], ["Bhatt", "Nirav", ""]]}, {"id": "1505.00401", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to\n  LIFT, ROC & BIRD", "comments": "23 pages, 12 equations, 2 figures, 2 tables, 1 sidebar", "journal-ref": null, "doi": null, "report-no": "KIT-14-002", "categories": "cs.LG cs.AI cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation often aims to reduce the correctness or error characteristics of a\nsystem down to a single number, but that always involves trade-offs. Another\nway of dealing with this is to quote two numbers, such as Recall and Precision,\nor Sensitivity and Specificity. But it can also be useful to see more than\nthis, and a graphical approach can explore sensitivity to cost, prevalence,\nbias, noise, parameters and hyper-parameters.\n  Moreover, most techniques are implicitly based on two balanced classes, and\nour ability to visualize graphically is intrinsically two dimensional, but we\noften want to visualize in a multiclass context. We review the dichotomous\napproaches relating to Precision, Recall, and ROC as well as the related LIFT\nchart, exploring how they handle unbalanced and multiclass data, and deriving\nnew probabilistic and information theoretic variants of LIFT that help deal\nwith the issues associated with the handling of multiple and unbalanced\nclasses.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 07:27:27 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 04:02:22 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1505.00475", "submitter": "Wei Qian", "authors": "Wei Qian, Craig A. Rolling, Gang Cheng, Yuhong Yang", "title": "On the Forecast Combination Puzzle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often reported in forecast combination literature that a simple average\nof candidate forecasts is more robust than sophisticated combining methods.\nThis phenomenon is usually referred to as the \"forecast combination puzzle\".\nMotivated by this puzzle, we explore its possible explanations including\nestimation error, invalid weighting formulas and model screening. We show that\nexisting understanding of the puzzle should be complemented by the distinction\nof different forecast combination scenarios known as combining for adaptation\nand combining for improvement. Applying combining methods without consideration\nof the underlying scenario can itself cause the puzzle. Based on our new\nunderstandings, both simulations and real data evaluations are conducted to\nillustrate the causes of the puzzle. We further propose a multi-level AFTER\nstrategy that can integrate the strengths of different combining methods and\nadapt intelligently to the underlying scenario. In particular, by treating the\nsimple average as a candidate forecast, the proposed strategy is shown to avoid\nthe heavy cost of estimation error and, to a large extent, solve the forecast\ncombination puzzle.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 20:56:27 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Qian", "Wei", ""], ["Rolling", "Craig A.", ""], ["Cheng", "Gang", ""], ["Yang", "Yuhong", ""]]}, {"id": "1505.00703", "submitter": "Kshitij Khare", "authors": "Kshitij Khare, Bala Rajaratnam, Abhishek Saha", "title": "Bayesian inference for Gaussian graphical models beyond decomposable\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for graphical models has received much attention in the\nliterature in recent years. It is well known that when the graph G is\ndecomposable, Bayesian inference is significantly more tractable than in the\ngeneral non-decomposable setting. Penalized likelihood inference on the other\nhand has made tremendous gains in the past few years in terms of scalability\nand tractability. Bayesian inference, however, has not had the same level of\nsuccess, though a scalable Bayesian approach has its respective strengths,\nespecially in terms of quantifying uncertainty. To address this gap, we propose\na scalable and flexible novel Bayesian approach for estimation and model\nselection in Gaussian undirected graphical models. We first develop a class of\ngeneralized G-Wishart distributions with multiple shape parameters for an\narbitrary underlying graph. This class contains the G-Wishart distribution as a\nspecial case. We then introduce the class of Generalized Bartlett (GB) graphs,\nand derive an efficient Gibbs sampling algorithm to obtain posterior draws from\ngeneralized G-Wishart distributions corresponding to a GB graph. The class of\nGeneralized Bartlett graphs contains the class of decomposable graphs as a\nspecial case, but is substantially larger than the class of decomposable\ngraphs. We proceed to derive theoretical properties of the proposed Gibbs\nsampler. We then demonstrate that the proposed Gibbs sampler is scalable to\nsignificantly higher dimensional problems as compared to using an accept-reject\nor a Metropolis-Hasting algorithm. Finally, we show the efficacy of the\nproposed approach on simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 16:45:05 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Khare", "Kshitij", ""], ["Rajaratnam", "Bala", ""], ["Saha", "Abhishek", ""]]}, {"id": "1505.00821", "submitter": "Rm Zhang", "authors": "Rongmao Zhang, Peter Robinson and Qiwei Yao", "title": "Identifying Cointegration by Eigenanalysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new and easy-to-use method for identifying cointegrated\ncomponents of nonstationary time series, consisting of an eigenanalysis for a\ncertain non-negative definite matrix. Our setting is model-free, and we allow\nthe integer-valued integration orders of the observable series to be unknown,\nand to possibly differ. Consistency of estimates of the cointegration space and\ncointegration rank is established both when the dimension of the observable\ntime series is fixed as sample size increases, and when it diverges slowly. The\nproposed methodology is also extended and justified in a fractional setting. A\nMonte Carlo study of finite-sample performance, and a small empirical\nillustration, are reported.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 21:33:25 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 07:50:14 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2018 03:25:44 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zhang", "Rongmao", ""], ["Robinson", "Peter", ""], ["Yao", "Qiwei", ""]]}, {"id": "1505.00954", "submitter": "Ivan Kojadinovic", "authors": "Axel B\\\"ucher, Paul Kinsvater and Ivan Kojadinovic", "title": "Detecting breaks in the dependence of multivariate extreme-value\n  distributions", "comments": "32 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In environmental sciences, it is often of interest to assess whether the\ndependence between extreme measurements has changed during the observation\nperiod. The aim of this work is to propose a statistical test that is\nparticularly sensitive to such changes. The resulting procedure is also\nextended to allow the detection of changes in the extreme-value dependence\nunder the presence of known breaks in the marginal distributions. Simulations\nare carried out to study the finite-sample behavior of both versions of the\nproposed test. Illustrations on hydrological data sets conclude the work.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 11:12:06 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Kinsvater", "Paul", ""], ["Kojadinovic", "Ivan", ""]]}, {"id": "1505.01164", "submitter": "Emily Fox", "authors": "You Ren, Emily B. Fox, and Andrew Bruce", "title": "Achieving a Hyperlocal Housing Price Index: Overcoming Data Sparsity by\n  Bayesian Dynamical Modeling of Multiple Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how housing values evolve over time is important to policy\nmakers, consumers and real estate professionals. Existing methods for\nconstructing housing indices are computed at a coarse spatial granularity, such\nas metropolitan regions, which can mask or distort price dynamics apparent in\nlocal markets, such as neighborhoods and census tracts. A challenge in moving\nto estimates at, for example, the census tract level is the sparsity of\nspatiotemporally localized house sales observations. Our work aims at\naddressing this challenge by leveraging observations from multiple census\ntracts discovered to have correlated valuation dynamics. Our proposed Bayesian\nnonparametric approach builds on the framework of latent factor models to\nenable a flexible, data-driven method for inferring the clustering of\ncorrelated census tracts. We explore methods for scalability and\nparallelizability of computations, yielding a housing valuation index at the\nlevel of census tract rather than zip code, and on a monthly basis rather than\nquarterly. Our analysis is provided on a large Seattle metropolitan housing\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 20:01:08 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Ren", "You", ""], ["Fox", "Emily B.", ""], ["Bruce", "Andrew", ""]]}, {"id": "1505.01177", "submitter": "Baojun Dou", "authors": "Baojun Dou, Maria Lucia Parrella and Qiwei Yao", "title": "Generalized Yule-Walker Estimation for Spatio-Temporal Models with\n  Unknown Diagonal Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of spatio-temporal models which extend popular\neconometric spatial autoregressive panel data models by allowing the scalar\ncoefficients for each location (or panel) different from each other. To\novercome the innate endogeneity, we propose a generalized Yule-Walker\nestimation method which applies the least squares estimation to a Yule-Walker\nequation. The asymptotic theory is developed under the setting that both the\nsample size and the number of locations (or panels) tend to infinity under a\ngeneral setting for stationary and alpha-mixing processes, which includes\nspatial autoregressive panel data models driven by i.i.d. innovations as\nspecial cases. The proposed methods are illustrated using both simulated and\nreal data.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 20:13:21 GMT"}, {"version": "v2", "created": "Sat, 14 May 2016 10:57:00 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Dou", "Baojun", ""], ["Parrella", "Maria Lucia", ""], ["Yao", "Qiwei", ""]]}, {"id": "1505.01179", "submitter": "Changshuai Wei", "authors": "Changshuai Wei and Qing Lu", "title": "A Generalized Similarity U Test for Multivariate Analysis of Sequencing\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequencing-based studies are emerging as a major tool for genetic association\nstudies of complex diseases. These studies pose great challenges to the\ntraditional statistical methods (e.g., single-locus analyses based on\nregression methods) because of the high-dimensionality of data and the low\nfrequency of genetic variants. In addition, there is a great interest in\nbiology and epidemiology to identify genetic risk factors contributed to\nmultiple disease phenotypes. The multiple phenotypes can often follow different\ndistributions, which violates the assumptions of most current methods. In this\npaper, we propose a generalized similarity U test, referred to as GSU. GSU is a\nsimilarity-based test and can handle high-dimensional genotypes and phenotypes.\nWe studied the theoretical properties of GSU, and provided the efficient\np-value calculation for association test as well as the sample size and power\ncalculation for the study design. Through simulation, we found that GSU had\nadvantages over existing methods in terms of power and robustness to phenotype\ndistributions. Finally, we used GSU to perform a multivariate analysis of\nsequencing data in the Dallas Heart Study and identified a joint association of\n4 genes with 5 metabolic related phenotypes.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 20:36:43 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 23:50:14 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2015 18:10:42 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Wei", "Changshuai", ""], ["Lu", "Qing", ""]]}, {"id": "1505.01204", "submitter": "Changshuai Wei", "authors": "Changshuai Wei, Ming Li, Zihuai He, Olga Vsevolozhskaya, Daniel J.\n  Schaid, and Qing Lu", "title": "A Weighted U Statistic for Genetic Association Analyses of Sequencing\n  Data", "comments": null, "journal-ref": "Genet Epidemiol. 2014 Dec;38(8):699-708", "doi": "10.1002/gepi.21864", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advancements in next generation sequencing technology, a massive amount\nof sequencing data are generated, offering a great opportunity to\ncomprehensively investigate the role of rare variants in the genetic etiology\nof complex diseases. Nevertheless, this poses a great challenge for the\nstatistical analysis of high-dimensional sequencing data. The association\nanalyses based on traditional statistical methods suffer substantial power loss\nbecause of the low frequency of genetic variants and the extremely high\ndimensionality of the data. We developed a weighted U statistic, referred to as\nWU-seq, for the high-dimensional association analysis of sequencing data. Based\non a non-parametric U statistic, WU-SEQ makes no assumption of the underlying\ndisease model and phenotype distribution, and can be applied to a variety of\nphenotypes. Through simulation studies and an empirical study, we showed that\nWU-SEQ outperformed a commonly used SKAT method when the underlying assumptions\nwere violated (e.g., the phenotype followed a heavy-tailed distribution). Even\nwhen the assumptions were satisfied, WU-SEQ still attained comparable\nperformance to SKAT. Finally, we applied WU-seq to sequencing data from the\nDallas Heart Study (DHS), and detected an association between ANGPTL 4 and very\nlow density lipoprotein cholesterol.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 22:13:23 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Wei", "Changshuai", ""], ["Li", "Ming", ""], ["He", "Zihuai", ""], ["Vsevolozhskaya", "Olga", ""], ["Schaid", "Daniel J.", ""], ["Lu", "Qing", ""]]}, {"id": "1505.01243", "submitter": "Michael Horrell", "authors": "Michael T. Horrell and Michael L. Stein", "title": "Half-Spectral Space-Time Covariance Models", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two new classes of space-time Gaussian process models by\nspecifying covariance functions using what we call a half-spectral\nrepresentation. The half-spectral representation of a covariance function, $K$,\nis a special case of standard spectral representations. In addition to the\nintroduction of two new model classes, we also develop desirable theoretical\nproperties of certain half-spectral forms. In particular, for a half-spectral\nmodel, $K$, we determine spatial and temporal mean-square differentiability\nproperties of a Gaussian process governed by $K$, and we determine whether or\nnot the spectral density of $K$ meets a regularity condition motivated by a\nscreening effect analysis. We fit models we develop in this paper to a wind\npower dataset, and we show our models fit these data better than other\nseparable and non-separable space-time models.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 03:23:28 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Horrell", "Michael T.", ""], ["Stein", "Michael L.", ""]]}, {"id": "1505.01333", "submitter": "Damien Challet", "authors": "Damien Challet", "title": "Sharper asset ranking from total drawdown durations", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total duration of drawdowns is shown to provide a moment-free, unbiased,\nefficient and robust estimator of Sharpe ratios both for Gaussian and\nheavy-tailed price returns. We then use this quantity to infer an analytic\nexpression of the bias of moment-based Sharpe ratio estimators as a function of\nthe return distribution tail exponent. The heterogeneity of tail exponents at\nany given time among assets implies that our new method yields significantly\ndifferent asset rankings than those of moment-based methods, especially in\nperiods large volatility. This is fully confirmed by using 20 years of\nhistorical data on 3449 liquid US equities.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 12:11:37 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 12:51:27 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2015 12:10:25 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2015 17:15:23 GMT"}, {"version": "v5", "created": "Wed, 28 Sep 2016 07:04:45 GMT"}, {"version": "v6", "created": "Wed, 8 Feb 2017 16:03:56 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Challet", "Damien", ""]]}, {"id": "1505.01351", "submitter": "Ali Akbar Jafari", "authors": "Rasool Roozegar and Saeid Tahmasebi and Ali Akbar Jafari", "title": "The McDonald Gompertz Distribution: Properties and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a five-parameter lifetime model with increasing,\ndecreasing, upside -down bathtub and bathtub shaped failure rate called as the\nMcDonald Gompertz (McG) distribution. This new distribution extend the\nGompertz, generalized Gompertz, generalized exponential, beta Gompertz and\nKumaraswamy Gompertz distributions, among several other models. We obtain\nseveral properties of the McG distribution including moments, entropies,\nquantile and generating functions. We provide the density function of the order\nstatistics and their moments. The parameter estimation is based on the usual\nmaximum likelihood approach. We also provide the observed information matrix\nand discuss inferences issues. In the end, the flexibility and usefulness of\nthe new distribution is illustrated by means of application to two real data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 12:59:27 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Roozegar", "Rasool", ""], ["Tahmasebi", "Saeid", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1505.01394", "submitter": "William Kleiber", "authors": "William Kleiber", "title": "Coherence for Random Fields", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate spatial field data are increasingly common and whose modeling\ntypically relies on building cross-covariance functions to describe\ncross-process relationships. An alternative viewpoint is to model the matrix of\nspectral measures. We develop the notions of coherence, phase and gain for\nmultidimensional stationary processes. Coherence, as a function of frequency,\ncan be seen to be a measure of linear relationship between two spatial\nprocesses at that frequency band. We use the coherence function to illustrate\nfundamental limitations on a number of previously proposed constructions for\nmultivariate processes, suggesting these options are not viable for real data.\nWe also give natural interpretations to cross-covariance parameters of the\nMatern class, where the smoothness indexes dependence at low frequencies while\nthe range parameter can imply dependence at low or high frequencies. Estimation\nfollows from smoothed multivariate periodogram matrices. We illustrate the\nestimation and interpretation of these functions on two datasets, forecast and\nreanalysis sea level pressure and geopotential heights over the equatorial\nregion. Examining these functions lends insight that would otherwise be\ndifficult to detect and model using standard cross-covariance formulations.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 15:19:19 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Kleiber", "William", ""]]}, {"id": "1505.01665", "submitter": "Stanley I. M. Ko", "authors": "Stanley I. M. Ko, Terence T. L. Chong, Pulak Ghosh", "title": "Dirichlet Process Hidden Markov Multiple Change-point Model", "comments": "Published at http://dx.doi.org/10.1214/14-BA910 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 275-296", "doi": "10.1214/14-BA910", "report-no": "VTeX-BA-BA910", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Bayesian multiple change-point model which is based\non the hidden Markov approach. The Dirichlet process hidden Markov model does\nnot require the specification of the number of change-points a priori. Hence\nour model is robust to model specification in contrast to the fully parametric\nBayesian model. We propose a general Markov chain Monte Carlo algorithm which\nonly needs to sample the states around change-points. Simulations for a normal\nmean-shift model with known and unknown variance demonstrate advantages of our\napproach. Two applications, namely the coal-mining disaster data and the real\nUnited States Gross Domestic Product growth, are provided. We detect a single\nchange-point for both the disaster data and US GDP growth. All the change-point\nlocations and posterior inferences of the two applications are in line with\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 11:14:49 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ko", "Stanley I. M.", ""], ["Chong", "Terence T. L.", ""], ["Ghosh", "Pulak", ""]]}, {"id": "1505.01687", "submitter": "Hao Wang", "authors": "Hao Wang", "title": "Scaling It Up: Stochastic Search Structure Learning in Graphical Models", "comments": "Published at http://dx.doi.org/10.1214/14-BA916 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 351-377", "doi": "10.1214/14-BA916", "report-no": "VTeX-BA-BA916", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian concentration graph models and covariance graph models are two\nclasses of graphical models that are useful for uncovering latent dependence\nstructures among multivariate variables. In the Bayesian literature, graphs are\noften determined through the use of priors over the space of positive definite\nmatrices with fixed zeros, but these methods present daunting computational\nburdens in large problems. Motivated by the superior computational efficiency\nof continuous shrinkage priors for regression analysis, we propose a new\nframework for structure learning that is based on continuous spike and slab\npriors and uses latent variables to identify graphs. We discuss model\nspecification, computation, and inference for both concentration and covariance\ngraph models. The new approach produces reliable estimates of graphs and\nefficiently handles problems with hundreds of variables.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 12:53:40 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Wang", "Hao", ""]]}, {"id": "1505.01743", "submitter": "Zhuang Ma", "authors": "Zhuang Ma, Dean Foster, Robert Stine", "title": "Adaptive Monotone Shrinkage for Regression", "comments": "Appearing in Uncertainty in Artificial Intelligence (UAI) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an adaptive monotone shrinkage estimator for regression models\nwith the following characteristics: i) dense coefficients with small but\nimportant effects; ii) a priori ordering that indicates the probable predictive\nimportance of the features. We capture both properties with an empirical Bayes\nestimator that shrinks coefficients monotonically with respect to their\nanticipated importance. This estimator can be rapidly computed using a version\nof Pool-Adjacent-Violators algorithm. We show that the proposed monotone\nshrinkage approach is competitive with the class of all Bayesian estimators\nthat share the prior information. We further observe that the estimator also\nminimizes Stein's unbiased risk estimate. Along with our key result that the\nestimator mimics the oracle Bayes rule under an order assumption, we also prove\nthat the estimator is robust. Even without the order assumption, our estimator\nmimics the best performance of a large family of estimators that includes the\nleast squares estimator, constant-$\\lambda$ ridge estimator, James-Stein\nestimator, etc. All the theoretical results are non-asymptotic. Simulation\nresults and data analysis from a model for text processing are provided to\nsupport the theory.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 15:24:54 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ma", "Zhuang", ""], ["Foster", "Dean", ""], ["Stine", "Robert", ""]]}, {"id": "1505.01886", "submitter": "Waldemar Koczkodaj Prof.", "authors": "W.W. Koczkodaj, T. Kakiashvili, A. Szyma\\'nska, J. Montero-Marin, R.\n  Araya, J. Garcia-Campayo, K. Rutkowski, D. Strza{\\l}ka", "title": "How to reduce the number of rating scale items without predictability\n  loss?", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating scales are used to elicit data about qualitative entities (e.g.,\nresearch collaboration). This study presents an innovative method for reducing\nthe number of rating scale items without the predictability loss. The \"area\nunder the receiver operator curve method\" (AUC ROC) is used. The presented\nmethod has reduced the number of rating scale items (variables) to 28.57\\%\n(from 21 to 6) making over 70\\% of collected data unnecessary.\n  Results have been verified by two methods of analysis: Graded Response Model\n(GRM) and Confirmatory Factor Analysis (CFA). GRM revealed that the new method\ndifferentiates observations of high and middle scores. CFA proved that the\nreliability of the rating scale has not deteriorated by the scale item\nreduction. Both statistical analysis evidenced usefulness of the AUC ROC\nreduction method.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 22:57:20 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 13:25:52 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2017 10:01:30 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Koczkodaj", "W. W.", ""], ["Kakiashvili", "T.", ""], ["Szyma\u0144ska", "A.", ""], ["Montero-Marin", "J.", ""], ["Araya", "R.", ""], ["Garcia-Campayo", "J.", ""], ["Rutkowski", "K.", ""], ["Strza\u0142ka", "D.", ""]]}, {"id": "1505.01949", "submitter": "Florian Frommlet", "authors": "Florian Frommlet, Gregory Nuel", "title": "An adaptive Ridge procedure for L0 regularization", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0148620", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized selection criteria like AIC or BIC are among the most popular\nmethods for variable selection. Their theoretical properties have been studied\nintensively and are well understood, but making use of them in case of\nhigh-dimensional data is difficult due to the non-convex optimization problem\ninduced by L0 penalties. An elegant solution to this problem is provided by the\nmulti-step adaptive lasso, where iteratively weighted lasso problems are\nsolved, whose weights are updated in such a way that the procedure converges\ntowards selection with L0 penalties. In this paper we introduce an adaptive\nridge procedure (AR) which mimics the adaptive lasso, but is based on weighted\nRidge problems. After introducing AR its theoretical properties are studied in\nthe particular case of orthogonal linear regression. For the non-orthogonal\ncase extensive simulations are performed to assess the performance of AR. In\ncase of Poisson regression and logistic regression it is illustrated how the\niterative procedure of AR can be combined with iterative maximization\nprocedures. The paper ends with an efficient implementation of AR in the\ncontext of least-squares segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 07:57:15 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Frommlet", "Florian", ""], ["Nuel", "Gregory", ""]]}, {"id": "1505.01975", "submitter": "Abdul-Hameed Al-Ibrahim", "authors": "Abdul-Hamid Al-Ibrahim", "title": "The Statistical Analysis of Pairwise Experiments with Qualitative\n  Responses", "comments": "All tables and figures are included within text. arXiv admin note:\n  text overlap with arXiv:q-bio/0601027 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose an experiment is conducted on pairs of objects with outcome responses\na continuous variable measuring the interactions among the pairs. Furthermore,\nassume the response variable is hard to measure numerically but easy to be\ncoded into ordered categories such as low, moderate, and high levels of\ninteraction. In this paper we estimate the unknown interaction values from the\ninformation contained in the coded data and the design structure of the\nexperiment. The method of estimation is shown to enjoy several optimal\nproperties such as explaining maximum variance in the responses with minimum\nnumber of parameters and for any probability distribution underlying the\nresponses. Other properties of the method include: the interactions have the\nsimple interpretation of correlation, size of error is estimable from the\nexperiment, and only a single run of each pair is needed to carry out the\nexperiment. We also explore possible applications of the technique. Three\napplications are presented, one on protein interaction, a second on drug\ncombination, and the third on computer imaging. The first two applications are\nillustrated using real life data while for the third application the data are\ngenerated via binary coding of an image.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 09:59:46 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Al-Ibrahim", "Abdul-Hamid", ""]]}, {"id": "1505.01979", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "The efficiency of the likelihood ratio to choose between a\n  t-distribution and a normal distribution", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A decision must often be made between heavy-tailed and Gaussian errors for a\nregression or a time series model, and the t-distribution is frequently used\nwhen it is assumed that the errors are heavy-tailed distributed. The\nperformance of the likelihood ratio to choose between the two distributions is\ninvestigated using entropy properties and a simulation study. The proportion of\ntimes or probability that the likelihood of the correct assumption will be\nbigger than the likelihood of the incorrect assumption is estimated.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 10:15:51 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["van Zyl", "J. Martin", ""]]}, {"id": "1505.02005", "submitter": "Sayan Ghosh", "authors": "S. Ghosh and P. Vellaisamy", "title": "On the occurrence of boundary solutions in two-way incomplete tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of incomplete contingency tables is an important problem, which\nis also of practical interest. In this paper, we consider boundary solutions\nunder nonignorable nonresponse models in two-way incomplete tables with data on\nboth variables missing. We establish a result similar to Park {\\it et al.}\n(2014) on sufficient conditions for the occurrence of boundary solutions. We\nalso provide a new result, which connects the forms of boundary solutions under\nvarious parameterizations of the missing data models. This result helps us to\nobtain the exact form of boundary solutions in the above tables, which improves\na claim made in Baker {\\it et al.} (1992) and avoids computational burden. A\ncounterexample is provided to show that the sufficient conditions for the\noccurrence of boundary solutions are not necessary, thereby disproving a\nconjecture of Kim and Park (2014). Finally, we establish new necessary\nconditions for the occurrence of boundary solutions under nonignorable\nnonresponse models in square two-way incomplete tables, and show that they are\nnot sufficient. These conditions are simple and easy to check as they depend\nonly on the observed cell counts. They are useful and important for model\nselection also. Some real life data sets are analyzed to illustrate the\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 11:51:01 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 05:14:24 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 14:10:00 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 12:30:23 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ghosh", "S.", ""], ["Vellaisamy", "P.", ""]]}, {"id": "1505.02023", "submitter": "Shahin Tavakoli", "authors": "John A. D. Aston, Davide Pigoli and Shahin Tavakoli", "title": "Tests for separability in nonparametric covariance operators of random\n  surfaces", "comments": "47 pages, 10 figures, 4 tables", "journal-ref": "Annals of Statistics, Vol. 45, No. 4, 1431-1461 (2017)", "doi": "10.1214/16-AOS1495", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of separability of the covariance operator for a random image\nor hypersurface can be of substantial use in applications, especially in\nsituations where the accurate estimation of the full covariance structure is\nunfeasible, either for computational reasons, or due to a small sample size.\nHowever, inferential tools to verify this assumption are somewhat lacking in\nhigh-dimensional or functional {data analysis} settings, where this assumption\nis most relevant. We propose here to test separability by focusing on\n$K$-dimensional projections of the difference between the covariance operator\nand a nonparametric separable approximation. The subspace we project onto is\none generated by the eigenfunctions of the covariance operator estimated under\nthe separability hypothesis, negating the need to ever estimate the full\nnon-separable covariance. We show that the rescaled difference of the sample\ncovariance operator with its separable approximation is asymptotically\nGaussian. As a by-product of this result, we derive asymptotically pivotal\ntests under Gaussian assumptions, and propose bootstrap methods for\napproximating the distribution of the test statistics. We probe the finite\nsample performance through simulations studies, and present an application to\nlog-spectrogram images from a phonetic linguistics dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 13:01:37 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 17:04:45 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2016 15:36:04 GMT"}, {"version": "v4", "created": "Fri, 3 Jun 2016 12:24:36 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Aston", "John A. D.", ""], ["Pigoli", "Davide", ""], ["Tavakoli", "Shahin", ""]]}, {"id": "1505.02097", "submitter": "Lucas Janson", "authors": "Lucas Janson, Rina Foygel Barber, Emmanuel Cand\\`es", "title": "EigenPrism: Inference for High-Dimensional Signal-to-Noise Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following three important problems in statistical inference,\nnamely, constructing confidence intervals for (1) the error of a\nhigh-dimensional ($p>n$) regression estimator, (2) the linear regression noise\nlevel, and (3) the genetic signal-to-noise ratio of a continuous-valued trait\n(related to the heritability). All three problems turn out to be closely\nrelated to the little-studied problem of performing inference on the\n$\\ell_2$-norm of the signal in high-dimensional linear regression. We derive a\nnovel procedure for this, which is asymptotically correct when the covariates\nare multivariate Gaussian and produces valid confidence intervals in finite\nsamples as well. The procedure, called EigenPrism, is computationally fast and\nmakes no assumptions on coefficient sparsity or knowledge of the noise level.\nWe investigate the width of the EigenPrism confidence intervals, including a\ncomparison with a Bayesian setting in which our interval is just 5% wider than\nthe Bayes credible interval. We are then able to unify the three aforementioned\nproblems by showing that the EigenPrism procedure with only minor modifications\nis able to make important contributions to all three. We also investigate the\nrobustness of coverage and find that the method applies in practice and in\nfinite samples much more widely than just the case of multivariate Gaussian\ncovariates. Finally, we apply EigenPrism to a genetic dataset to estimate the\ngenetic signal-to-noise ratio for a number of continuous phenotypes.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 16:51:54 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 18:06:58 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Janson", "Lucas", ""], ["Barber", "Rina Foygel", ""], ["Cand\u00e8s", "Emmanuel", ""]]}, {"id": "1505.02114", "submitter": "David Gerard", "authors": "David Gerard and Peter Hoff", "title": "Adaptive Higher-order Spectral Estimators", "comments": "29 pages, 3 figures", "journal-ref": "Electronic Journal of Statistics 11 (2017) 3703--3737", "doi": "10.1214/17-EJS1330", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications involve estimation of a signal matrix from a noisy data\nmatrix. In such cases, it has been observed that estimators that shrink or\ntruncate the singular values of the data matrix perform well when the signal\nmatrix has approximately low rank. In this article, we generalize this approach\nto the estimation of a tensor of parameters from noisy tensor data. We develop\nnew classes of estimators that shrink or threshold the mode-specific singular\nvalues from the higher-order singular value decomposition. These classes of\nestimators are indexed by tuning parameters, which we adaptively choose from\nthe data by minimizing Stein's unbiased risk estimate. In particular, this\nprocedure provides a way to estimate the multilinear rank of the underlying\nsignal tensor. Using simulation studies under a variety of conditions, we show\nthat our estimators perform well when the mean tensor has approximately low\nmultilinear rank, and perform competitively when the signal tensor does not\nhave approximately low multilinear rank. We illustrate the use of these methods\nin an application to multivariate relational data.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 18:07:09 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 17:39:37 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Gerard", "David", ""], ["Hoff", "Peter", ""]]}, {"id": "1505.02118", "submitter": "Linbo Wang", "authors": "Linbo Wang, Thomas S. Richardson, Xiao-Hua Zhou", "title": "Causal analysis of ordinal treatments and binary outcomes under\n  truncation by death", "comments": "To appear in Journal of the Royal Statistical Society: Series B\n  (Statistical Methodology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common that in multiarm randomized trials, the outcome of interest is\n\"truncated by death,\" meaning that it is only observed or well defined\nconditioning on an intermediate outcome. In this case, in addition to pairwise\ncontrasts, the joint inference for all treatment arms is also of interest.\nUnder a monotonicity assumption we present methods for both pairwise and joint\ncausal analyses of ordinal treatments and binary outcomes in presence of\ntruncation by death. We illustrate via examples the appropriateness of our\nassumptions in different scientific contexts.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 18:25:19 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 20:16:37 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 05:38:10 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wang", "Linbo", ""], ["Richardson", "Thomas S.", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "1505.02212", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael M.\n  Mitzenmacher", "title": "Equitability, interval estimation, and statistical power", "comments": "Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together\n  with arXiv:1505.02212, subsumes arXiv:1408.4908", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG q-bio.QM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For analysis of a high-dimensional dataset, a common approach is to test a\nnull hypothesis of statistical independence on all variable pairs using a\nnon-parametric measure of dependence. However, because this approach attempts\nto identify any non-trivial relationship no matter how weak, it often\nidentifies too many relationships to be useful. What is needed is a way of\nidentifying a smaller set of relationships that merit detailed further\nanalysis.\n  Here we formally present and characterize equitability, a property of\nmeasures of dependence that aims to overcome this challenge. Notionally, an\nequitable statistic is a statistic that, given some measure of noise, assigns\nsimilar scores to equally noisy relationships of different types [Reshef et al.\n2011]. We begin by formalizing this idea via a new object called the\ninterpretable interval, which functions as an interval estimate of the amount\nof noise in a relationship of unknown type. We define an equitable statistic as\none with small interpretable intervals.\n  We then draw on the equivalence of interval estimation and hypothesis testing\nto show that under moderate assumptions an equitable statistic is one that\nyields well powered tests for distinguishing not only between trivial and\nnon-trivial relationships of all kinds but also between non-trivial\nrelationships of different strengths. This means that equitability allows us to\nspecify a threshold relationship strength $x_0$ and to search for relationships\nof all kinds with strength greater than $x_0$. Thus, equitability can be\nthought of as a strengthening of power against independence that enables\nfruitful analysis of data sets with a small number of strong, interesting\nrelationships and a large number of weaker ones. We conclude with a\ndemonstration of how our two equivalent characterizations of equitability can\nbe used to evaluate the equitability of a statistic in practice.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:31:23 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 20:05:17 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02213", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Hilary K. Finucane, Pardis C.\n  Sabeti, Michael M. Mitzenmacher", "title": "Measuring dependence powerfully and equitably", "comments": "Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together\n  with arXiv:1505.02212, subsumes arXiv:1408.4908. v3 includes new analyses and\n  exposition", "journal-ref": "J.Mach.Learn.Res. 17 (2016), 1-63", "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a high-dimensional data set we often wish to find the strongest\nrelationships within it. A common strategy is to evaluate a measure of\ndependence on every variable pair and retain the highest-scoring pairs for\nfollow-up. This strategy works well if the statistic used is equitable [Reshef\net al. 2015a], i.e., if, for some measure of noise, it assigns similar scores\nto equally noisy relationships regardless of relationship type (e.g., linear,\nexponential, periodic).\n  In this paper, we introduce and characterize a population measure of\ndependence called MIC*. We show three ways that MIC* can be viewed: as the\npopulation value of MIC, a highly equitable statistic from [Reshef et al.\n2011], as a canonical \"smoothing\" of mutual information, and as the supremum of\nan infinite sequence defined in terms of optimal one-dimensional partitions of\nthe marginals of the joint distribution. Based on this theory, we introduce an\nefficient approach for computing MIC* from the density of a pair of random\nvariables, and we define a new consistent estimator MICe for MIC* that is\nefficiently computable. In contrast, there is no known polynomial-time\nalgorithm for computing the original equitable statistic MIC. We show through\nsimulations that MICe has better bias-variance properties than MIC. We then\nintroduce and prove the consistency of a second statistic, TICe, that is a\ntrivial side-product of the computation of MICe and whose goal is powerful\nindependence testing rather than equitability.\n  We show in simulations that MICe and TICe have good equitability and power\nagainst independence respectively. The analyses here complement a more in-depth\nempirical evaluation of several leading measures of dependence [Reshef et al.\n2015b] that shows state-of-the-art performance for MICe and TICe.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:31:54 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 19:51:46 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 18:42:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Finucane", "Hilary K.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02214", "submitter": "Yakir Reshef", "authors": "David N. Reshef, Yakir A. Reshef, Pardis C. Sabeti, Michael M.\n  Mitzenmacher", "title": "An Empirical Study of Leading Measures of Dependence", "comments": "David N. Reshef and Yakir A. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors", "journal-ref": "Ann.Appl.Stat. 12 (2018) 123-155", "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory data analysis, we are often interested in identifying\npromising pairwise associations for further analysis while filtering out\nweaker, less interesting ones. This can be accomplished by computing a measure\nof dependence on all variable pairs and examining the highest-scoring pairs,\nprovided the measure of dependence used assigns similar scores to equally noisy\nrelationships of different types. This property, called equitability, is\nformalized in Reshef et al. [2015b]. In addition to equitability, measures of\ndependence can also be assessed by the power of their corresponding\nindependence tests as well as their runtime.\n  Here we present extensive empirical evaluation of the equitability, power\nagainst independence, and runtime of several leading measures of dependence.\nThese include two statistics introduced in Reshef et al. [2015a]: MICe, which\nhas equitability as its primary goal, and TICe, which has power against\nindependence as its goal. Regarding equitability, our analysis finds that MICe\nis the most equitable method on functional relationships in most of the\nsettings we considered, although mutual information estimation proves the most\nequitable at large sample sizes in some specific settings. Regarding power\nagainst independence, we find that TICe, along with Heller and Gorfine's S^DDP,\nis the state of the art on the relationships we tested. Our analyses also show\na trade-off between power against independence and equitability consistent with\nthe theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are\nsignificantly faster than many other measures of dependence tested, and\ncomputing either one makes computing the other trivial. This suggests that a\nfast and useful strategy for achieving a combination of power against\nindependence and equitability may be to filter relationships by TICe and then\nto examine the MICe of only the significant ones.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:32:20 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 19:54:34 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Reshef", "David N.", ""], ["Reshef", "Yakir A.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02324", "submitter": "Md Abul Hasnat", "authors": "Md. Abul Hasnat, Julien Velcin, St\\'ephane Bonnevay and Julien Jacques", "title": "Simultaneous Clustering and Model Selection for Multinomial\n  Distribution: A Comparative Study", "comments": "Accepted in the International Symposium on Intelligent Data Analysis\n  (IDA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study different discrete data clustering methods, which use\nthe Model-Based Clustering (MBC) framework with the Multinomial distribution.\nOur study comprises several relevant issues, such as initialization, model\nestimation and model selection. Additionally, we propose a novel MBC method by\nefficiently combining the partitional and hierarchical clustering techniques.\nWe conduct experiments on both synthetic and real data and evaluate the methods\nusing accuracy, stability and computation time. Our study identifies\nappropriate strategies to be used for discrete data analysis with the MBC\nmethods. Moreover, our proposed method is very competitive w.r.t. clustering\naccuracy and better w.r.t. stability and computation time.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 22:29:40 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2015 09:03:48 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Hasnat", "Md. Abul", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Jacques", "Julien", ""]]}, {"id": "1505.02390", "submitter": "Kari Heine", "authors": "Kari Heine, Nick Whiteley", "title": "Fluctuations, stability and instability of a distributed particle filter\n  with local exchange", "comments": "49 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a distributed particle filter proposed by Boli\\'c et al.~(2005).\nThis algorithm involves $m$ groups of $M$ particles, with interaction between\ngroups occurring through a \"local exchange\" mechanism. We establish a central\nlimit theorem in the regime where $M$ is fixed and $m\\to\\infty$. A formula we\nobtain for the asymptotic variance can be interpreted in terms of colliding\nMarkov chains, enabling analytic and numerical evaluations of how the\nasymptotic variance behaves over time, with comparison to a benchmark algorithm\nconsisting of $m$ independent particle filters. We prove that subject to\nregularity conditions, when $m$ is fixed both algorithms converge\ntime-uniformly at rate $M^{-1/2}$. Through use of our asymptotic variance\nformula we give counter-examples satisfying the same regularity conditions to\nshow that when $M$ is fixed neither algorithm, in general, converges\ntime-uniformly at rate $m^{-1/2}$.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 14:50:49 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 11:13:03 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Heine", "Kari", ""], ["Whiteley", "Nick", ""]]}, {"id": "1505.02417", "submitter": "Dustin Tran", "authors": "Panos Toulis, Dustin Tran, Edoardo M. Airoldi", "title": "Towards stability and optimality in stochastic gradient descent", "comments": "Appears in Artificial Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative procedures for parameter estimation based on stochastic gradient\ndescent allow the estimation to scale to massive data sets. However, in both\ntheory and practice, they suffer from numerical instability. Moreover, they are\nstatistically inefficient as estimators of the true parameter value. To address\nthese two issues, we propose a new iterative procedure termed averaged implicit\nSGD (AI-SGD). For statistical efficiency, AI-SGD employs averaging of the\niterates, which achieves the optimal Cram\\'{e}r-Rao bound under strong\nconvexity, i.e., it is an optimal unbiased estimator of the true parameter\nvalue. For numerical stability, AI-SGD employs an implicit update at each\niteration, which is related to proximal operators in optimization. In practice,\nAI-SGD achieves competitive performance with other state-of-the-art procedures.\nFurthermore, it is more stable than averaging procedures that do not employ\nproximal updates, and is simple to implement as it requires fewer tunable\nhyperparameters than procedures that do employ proximal updates.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 18:10:07 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 03:01:53 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 23:11:21 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 04:02:43 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Toulis", "Panos", ""], ["Tran", "Dustin", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1505.02452", "submitter": "Nanny Wermuth", "authors": "D.R. Cox and Nanny Wermuth", "title": "Design and interpretation of studies: relevant concepts from the past\n  and some extensions", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principles for the planning and analysis of observational studies, as\nsuggested by W.G Cochran in 1972, are discussed and compared to additional\nmethodological developments since then.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 22:57:27 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Cox", "D. R.", ""], ["Wermuth", "Nanny", ""]]}, {"id": "1505.02456", "submitter": "Nanny Wermuth", "authors": "Nanny Wermuth", "title": "Graphical Markov models, unifying results and their interpretation", "comments": "34 Pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Markov models combine conditional independence constraints with\ngraphical representations of stepwise data generating processes.The models\nstarted to be formulated about 40 years ago and vigorous development is\nongoing. Longitudinal observational studies as well as intervention studies are\nbest modeled via a subclass called regression graph models and, especially\ntraceable regressions. Regression graphs include two types of undirected graph\nand directed acyclic graphs in ordered sequences of joint responses. Response\ncomponents may correspond to discrete or continuous random variables and may\ndepend exclusively on variables which have been generated earlier. These\naspects are essential when causal hypothesis are the motivation for the\nplanning of empirical studies.\n  To turn the graphs into useful tools for tracing developmental pathways and\nfor predicting structure in alternative models, the generated distributions\nhave to mimic some properties of joint Gaussian distributions. Here, relevant\nresults concerning these aspects are spelled out and illustrated by examples.\nWith regression graph models, it becomes feasible, for the first time, to\nderive structural effects of (1) ignoring some of the variables, of (2)\nselecting subpopulations via fixed levels of some other variables or of (3)\nchanging the order in which the variables might get generated. Thus, the most\nimportant future applications of these models will aim at the best possible\nintegration of knowledge from related studies.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 23:45:57 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 13:09:43 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Wermuth", "Nanny", ""]]}, {"id": "1505.02488", "submitter": "Siuli Mukhopadhyay", "authors": "S. Mukhopadhyay, S.P. Singh, A. Dey", "title": "Optimal two-treatment crossover designs for binary response models", "comments": "15 pages, 0 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal two-treatment, $p$ period crossover designs for binary responses are\ndetermined. The optimal designs are obtained by minimizing the variance of the\ntreatment contrast estimator over all possible allocations of $n$ subjects to\n$2^p$ possible treatment sequences. An appropriate logistic regression model is\npostulated and the within subject covariances are modeled through a working\ncorrelation matrix. The marginal mean of the binary responses are fitted using\ngeneralized estimating equations. The efficiencies of some crossover designs\nfor $p=2,3,4$ periods are calculated. The effect of misspecified working\ncorrelation matrix on design efficiency is also studied.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 05:39:44 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Mukhopadhyay", "S.", ""], ["Singh", "S. P.", ""], ["Dey", "A.", ""]]}, {"id": "1505.02689", "submitter": "Michael Shields", "authors": "Michael D. Shields, Kirubel Teferra, Adam Hapij, Raymond P. Daddazio", "title": "Refined Stratified Sampling for efficient Monte Carlo based uncertainty\n  quantification", "comments": null, "journal-ref": "Reliability Engineering and System Safety, 2015, 142: 310-325", "doi": "10.1016/j.ress.2015.05.023", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general adaptive approach rooted in stratified sampling (SS) is proposed\nfor sample-based uncertainty quantification (UQ). To motivate its use in this\ncontext the space-filling, orthogonality, and projective properties of SS are\ncompared with simple random sampling and Latin hypercube sampling (LHS). SS is\ndemonstrated to provide attractive properties for certain classes of problems.\nThe proposed approach, Refined Stratified Sampling (RSS), capitalizes on these\nproperties through an adaptive process that adds samples sequentially by\ndividing the existing subspaces of a stratified design. RSS is proven to reduce\nvariance compared to traditional stratified sample extension methods while\nproviding comparable or enhanced variance reduction when compared to sample\nsize extension methods for LHS - which do not afford the same degree of\nflexibility to facilitate a truly adaptive UQ process. An initial investigation\nof optimal stratification is presented and motivates the potential for major\nadvances in variance reduction through optimally designed RSS. Potential paths\nfor extension of the method to high dimension are discussed. Two examples are\nprovided. The first involves UQ for a low dimensional function where\nconvergence is evaluated analytically. The second presents a study to asses the\nresponse variability of a floating structure to an underwater shock.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 16:19:28 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Shields", "Michael D.", ""], ["Teferra", "Kirubel", ""], ["Hapij", "Adam", ""], ["Daddazio", "Raymond P.", ""]]}, {"id": "1505.02827", "submitter": "R\\'emi Bardenet", "authors": "R\\'emi Bardenet, Arnaud Doucet, Chris Holmes", "title": "On Markov chain Monte Carlo methods for tall data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo methods are often deemed too computationally\nintensive to be of any practical use for big data applications, and in\nparticular for inference on datasets containing a large number $n$ of\nindividual data points, also known as tall datasets. In scenarios where data\nare assumed independent, various approaches to scale up the Metropolis-Hastings\nalgorithm in a Bayesian inference context have been recently proposed in\nmachine learning and computational statistics. These approaches can be grouped\ninto two categories: divide-and-conquer approaches and, subsampling-based\nalgorithms. The aims of this article are as follows. First, we present a\ncomprehensive review of the existing literature, commenting on the underlying\nassumptions and theoretical guarantees of each method. Second, by leveraging\nour understanding of these limitations, we propose an original\nsubsampling-based approach which samples from a distribution provably close to\nthe posterior distribution of interest, yet can require less than $O(n)$ data\npoint likelihood evaluations at each iteration for certain statistical models\nin favourable scenarios. Finally, we have only been able so far to propose\nsubsampling-based methods which display good performance in scenarios where the\nBernstein-von Mises approximation of the target posterior distribution is\nexcellent. It remains an open challenge to develop such methods in scenarios\nwhere the Bernstein-von Mises approximation is poor.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 22:51:02 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Doucet", "Arnaud", ""], ["Holmes", "Chris", ""]]}, {"id": "1505.02913", "submitter": "Mohammad Arashi", "authors": "M. Norouzirad, M. Arashi and A.K.Md.Ehsanes Saleh", "title": "Restricted LASSO and Double Shrinking", "comments": "20 pages; 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of multiple regression model, suppose that the vector\nparameter of interest \\beta is subjected to lie in the subspace hypothesis\nH\\beta = h, where this restriction is based on either additional information or\nprior knowledge. Then, the restricted estimator performs fairly well than the\nordinary least squares one. In addition, when the number of variables is\nrelatively large with respect to observations, the use of least absolute\nshrinkage and selection operator (LASSO) estimator is suggested for variable\nselection purposes. In this paper, we deffine a restricted LASSO estimator and\nconfigure three classes of LASSO-type estimators to fulfill both variable\nselection and restricted estimation. Asymptotic performance of the proposed\nestimators are studied and a simulation is conducted to analyze asymptotic\nrelative efficiencies. The application of our result is considered for the\nprostate dataset where the expected prediction errors and risks are compared.\nIt has been shown that the proposed shrunken LASSO estimators, resulted from\ndouble shrinking methodology, perform better than the classical LASSO.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 08:55:00 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Norouzirad", "M.", ""], ["Arashi", "M.", ""], ["Saleh", "A. K. Md. Ehsanes", ""]]}, {"id": "1505.03030", "submitter": "Murray Pollock", "authors": "Murray Pollock", "title": "On the Exact Simulation of (Jump) Diffusion Bridges", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR q-fin.CP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we outline methodology to efficiently simulate (jump) diffusion\nbridge sample paths without discretisation error. We achieve this by\nconsidering the simulation of conditioned (jump) diffusion bridge sample paths\nin light of recent work developing a mathematical framework for simulating\nfinite dimensional sample path skeletons (which flexibly characterise the\nentirety of sample paths).\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 14:41:08 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Pollock", "Murray", ""]]}, {"id": "1505.03118", "submitter": "Richard Kennaway", "authors": "Richard Kennaway", "title": "When causation does not imply correlation: robust violations of the\n  Faithfulness axiom", "comments": "18 pages. Matlab code for running the reported simulations and\n  generating the figures is included", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that the Faithfulness property that is assumed in much causal\nanalysis is robustly violated for a large class of systems of a type that\noccurs throughout the life and social sciences: control systems. These systems\nexhibit correlations indistinguishable from zero between variables that are\nstrongly causally connected, and can show very high correlations between\nvariables that have no direct causal connection, only a connection via causal\nlinks between uncorrelated variables. Their patterns of correlation are robust,\nin that they remain unchanged when their parameters are varied. The violation\nof Faithfulness is fundamental to what a control system does: hold some\nvariable constant despite the disturbing influences on it. No method of causal\nanalysis that requires Faithfulness is applicable to such systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 18:53:50 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Kennaway", "Richard", ""]]}, {"id": "1505.03131", "submitter": "Alexander Tank", "authors": "Alex Tank, Nicholas Foti, Emily Fox", "title": "Bayesian Structure Learning for Stationary Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While much work has explored probabilistic graphical models for independent\ndata, less attention has been paid to time series. The goal in this setting is\nto determine conditional independence relations between entire time series,\nwhich for stationary series, are encoded by zeros in the inverse spectral\ndensity matrix. We take a Bayesian approach to structure learning, placing\npriors on (i) the graph structure and (ii) spectral matrices given the graph.\nWe leverage a Whittle likelihood approximation and define a conjugate\nprior---the hyper complex inverse Wishart---on the complex-valued and\ngraph-constrained spectral matrices. Due to conjugacy, we can analytically\nmarginalize the spectral matrices and obtain a closed-form marginal likelihood\nof the time series given a graph. Importantly, our analytic marginal likelihood\nallows us to avoid inference of the complex spectral matrices themselves and\nplaces us back into the framework of standard (Bayesian) structure learning. In\nparticular, combining this marginal likelihood with our graph prior leads to\nefficient inference of the time series graph itself, which we base on a\nstochastic search procedure, though any standard approach can be\nstraightforwardly modified to our time series case. We demonstrate our methods\non analyzing stock data and neuroimaging data of brain activity during various\nauditory tasks.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 19:37:20 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 19:07:08 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Tank", "Alex", ""], ["Foti", "Nicholas", ""], ["Fox", "Emily", ""]]}, {"id": "1505.03220", "submitter": "Grzegorz A Rempala", "authors": "Maciej Pietrzak, Grzegorz A. Rempa{\\l}a, Micha{\\l} Seweryn, Jacek\n  Weso{\\l}owski", "title": "Limit Theorems for Empirical R\\'enyi Entropy and Divergence with\n  Applications to Molecular Diversity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative methods for studying biodiversity have been traditionally rooted\nin the classical theory of finite frequency tables analysis. However, with the\nhelp of modern experimental tools, like high throughput sequencing, we now\nbegin to unlock the outstanding diversity of genomic data in plants and animals\nreflective of the long evolutionary history of our planet. This molecular data\noften defies the classical frequency/contingency tables assumptions and seems\nto require sparse tables with very large number of categories and highly\nunbalanced cell counts, e.g., following heavy tailed distributions (for\ninstance, power laws). Motivated by the molecular diversity studies, we propose\nhere a frequency-based framework for biodiversity analysis in the asymptotic\nregime where the number of categories grows with sample size (an infinite\ncontingency table). Our approach is rooted in information theory and based on\nthe Gaussian limit results for the effective number of species (the Hill\nnumbers) and the empirical Renyi entropy and divergence. We argue that when\napplied to molecular biodiversity analysis our methods can properly account for\nthe complicated data frequency patterns on one hand and the practical sample\nsize limitations on the other. We illustrate this principle with two specific\nRNA sequencing examples: a comparative study of T-cell receptor populations and\na validation of some preselected molecular hepatocellular carcinoma (HCC)\nmarkers.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 02:34:30 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2015 17:17:11 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Pietrzak", "Maciej", ""], ["Rempa\u0142a", "Grzegorz A.", ""], ["Seweryn", "Micha\u0142", ""], ["Weso\u0142owski", "Jacek", ""]]}, {"id": "1505.03339", "submitter": "Sara Wade", "authors": "Sara Wade and Zoubin Ghahramani", "title": "Bayesian cluster analysis: Point estimation and credible balls", "comments": null, "journal-ref": "Bayesian Anal., Volume 13, Number 2 (2018), 559-626", "doi": "10.1214/17-BA1073", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is widely studied in statistics and machine learning, with\napplications in a variety of fields. As opposed to classical algorithms which\nreturn a single clustering solution, Bayesian nonparametric models provide a\nposterior over the entire space of partitions, allowing one to assess\nstatistical properties, such as uncertainty on the number of clusters. However,\nan important problem is how to summarize the posterior; the huge dimension of\npartition space and difficulties in visualizing it add to this problem. In a\nBayesian analysis, the posterior of a real-valued parameter of interest is\noften summarized by reporting a point estimate such as the posterior mean along\nwith 95% credible intervals to characterize uncertainty. In this paper, we\nextend these ideas to develop appropriate point estimates and credible sets to\nsummarize the posterior of clustering structure based on decision and\ninformation theoretic techniques.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 11:51:24 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 17:26:59 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Wade", "Sara", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1505.03350", "submitter": "Stefano Cabras", "authors": "Stefano Cabras, Maria Eugenia Castellanos Nueda, Erlis Ruli", "title": "Approximate Bayesian Computation by Modelling Summary Statistics in a\n  Quasi-likelihood Framework", "comments": "Published at http://dx.doi.org/10.1214/14-BA921 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 411-439", "doi": "10.1214/14-BA921", "report-no": "VTeX-BA-BA921", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a useful class of methods for\nBayesian inference when the likelihood function is computationally intractable.\nIn practice, the basic ABC algorithm may be inefficient in the presence of\ndiscrepancy between prior and posterior. Therefore, more elaborate methods,\nsuch as ABC with the Markov chain Monte Carlo algorithm (ABC-MCMC), should be\nused. However, the elaboration of a proposal density for MCMC is a sensitive\nissue and very difficult in the ABC setting, where the likelihood is\nintractable. We discuss an automatic proposal distribution useful for ABC-MCMC\nalgorithms. This proposal is inspired by the theory of quasi-likelihood (QL)\nfunctions and is obtained by modelling the distribution of the summary\nstatistics as a function of the parameters. Essentially, given a real-valued\nvector of summary statistics, we reparametrize the model by means of a\nregression function of the statistics on parameters, obtained by sampling from\nthe original model in a pilot-run simulation study. The QL theory is well\nestablished for a scalar parameter, and it is shown that when the conditional\nvariance of the summary statistic is assumed constant, the QL has a closed-form\nnormal density. This idea of constructing proposal distributions is extended to\nnon constant variance and to real-valued parameter vectors. The method is\nillustrated by several examples and by an application to a real problem in\npopulation genetics.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 12:11:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Cabras", "Stefano", ""], ["Nueda", "Maria Eugenia Castellanos", ""], ["Ruli", "Erlis", ""]]}, {"id": "1505.03372", "submitter": "Christopher C. Drovandi", "authors": "Christopher C. Drovandi, Anthony N. Pettitt, Anthony Lee", "title": "Bayesian Indirect Inference Using a Parametric Auxiliary Model", "comments": "Published at http://dx.doi.org/10.1214/14-STS498 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 1, 72-95", "doi": "10.1214/14-STS498", "report-no": "IMS-STS-STS498", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect inference (II) is a methodology for estimating the parameters of an\nintractable (generative) model on the basis of an alternative parametric\n(auxiliary) model that is both analytically and computationally easier to deal\nwith. Such an approach has been well explored in the classical literature but\nhas received substantially less attention in the Bayesian paradigm. The purpose\nof this paper is to compare and contrast a collection of what we call\nparametric Bayesian indirect inference (pBII) methods. One class of pBII\nmethods uses approximate Bayesian computation (referred to here as ABC II)\nwhere the summary statistic is formed on the basis of the auxiliary model,\nusing ideas from II. Another approach proposed in the literature, referred to\nhere as parametric Bayesian indirect likelihood (pBIL), uses the auxiliary\nlikelihood as a replacement to the intractable likelihood. We show that pBIL is\na fundamentally different approach to ABC II. We devise new theoretical results\nfor pBIL to give extra insights into its behaviour and also its differences\nwith ABC II. Furthermore, we examine in more detail the assumptions required to\nuse each pBII method. The results, insights and comparisons developed in this\npaper are illustrated on simple examples and two other substantive\napplications. The first of the substantive examples involves performing\ninference for complex quantile distributions based on simulated data while the\nsecond is for estimating the parameters of a trivariate stochastic process\ndescribing the evolution of macroparasites within a host based on real data. We\ncreate a novel framework called Bayesian indirect likelihood (BIL) that\nencompasses pBII as well as general ABC methods so that the connections between\nthe methods can be established.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 13:15:54 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Drovandi", "Christopher C.", ""], ["Pettitt", "Anthony N.", ""], ["Lee", "Anthony", ""]]}, {"id": "1505.03431", "submitter": "Zuoxiang Peng", "authors": "Xin Liao and Zuoxiang Peng", "title": "Asymptotics and statistical inferences on independent and\n  non-identically distributed bivariate Gaussian triangular arrays", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish the first and the second-order asymptotics of\ndistributions of normalized maxima of independent and non-identically\ndistributed bivariate Gaussian triangular arrays, where each vector of the\n$n$th row follows from a bivariate Gaussian distribution with correlation\ncoefficient being a monotone continuous function of $i/n$. Furthermore,\nparametric inference for this unknown function is studied. Some simulation\nstudy and real data sets analysis are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 15:40:22 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 17:04:01 GMT"}, {"version": "v3", "created": "Tue, 26 Apr 2016 08:06:52 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Liao", "Xin", ""], ["Peng", "Zuoxiang", ""]]}, {"id": "1505.03659", "submitter": "Paolo Zaffaroni", "authors": "Wei Biao Wu and Paolo Zaffaroni", "title": "Uniform Convergence of Multivariate Spectral Density Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider uniform moment convergence of lag-window spectral density\nestimates for univariate and multivariate stationary processes. Optimal rates\nof convergence are obtained under mild and easily verifiable conditions. Our\ntheory complements earlier results which primarily concern weak or\nin-probability convergence.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 09:22:54 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Wu", "Wei Biao", ""], ["Zaffaroni", "Paolo", ""]]}, {"id": "1505.03762", "submitter": "Zuoxiang Peng", "authors": "Xin Liao, Liang Peng, Zuoxiang Peng, Yanting Zheng", "title": "Dynamic Bivariate Normal Copula", "comments": "22pages, 4 figures", "journal-ref": null, "doi": "10.1007/s11425-015-5114-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normal copula with a correlation coefficient between $-1$ and $1$ is tail\nindependent and so it severely underestimates extreme probabilities. By letting\nthe correlation coefficient in a normal copula depend on the sample size,\nH\\\"usler and Reiss (1989) showed that the tail can become asymptotically\ndependent. In this paper, we extend this result by deriving the limit of the\nnormalized maximum of $n$ independent observations, where the $i$-th\nobservation follows from a normal copula with its correlation coefficient being\neither a parametric or a nonparametric function of $i/n$. Furthermore, both\nparametric and nonparametric inference for this unknown function are studied,\nwhich can be employed to test the condition in H\\\"usler and Reiss (1989). A\nsimulation study and real data analysis are presented too.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 15:43:16 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Liao", "Xin", ""], ["Peng", "Liang", ""], ["Peng", "Zuoxiang", ""], ["Zheng", "Yanting", ""]]}, {"id": "1505.03772", "submitter": "Chao Gao", "authors": "Chao Gao, Zongming Ma, Anderson Y. Zhang, Harrison H. Zhou", "title": "Achieving Optimal Misclassification Proportion in Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental statistical problem in network data\nanalysis. Many algorithms have been proposed to tackle this problem. Most of\nthese algorithms are not guaranteed to achieve the statistical optimality of\nthe problem, while procedures that achieve information theoretic limits for\ngeneral parameter spaces are not computationally tractable. In this paper, we\npresent a computationally feasible two-stage method that achieves optimal\nstatistical performance in misclassification proportion for stochastic block\nmodel under weak regularity conditions. Our two-stage procedure consists of a\ngeneric refinement step that can take a wide range of weakly consistent\ncommunity detection procedures as initializer, to which the refinement stage\napplies and outputs a community assignment achieving optimal misclassification\nproportion with high probability. The practical effectiveness of the new\nalgorithm is demonstrated by competitive numerical results.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 15:59:00 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 00:28:26 GMT"}, {"version": "v3", "created": "Thu, 21 May 2015 19:18:20 GMT"}, {"version": "v4", "created": "Fri, 29 May 2015 21:03:56 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2015 03:52:33 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Gao", "Chao", ""], ["Ma", "Zongming", ""], ["Zhang", "Anderson Y.", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1505.03810", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty and Dylan S. Small", "title": "Sensitivity Analysis for Multiple Comparisons in Matched Observational\n  Studies through Quadratically Constrained Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sensitivity analysis in an observational study assesses the robustness of\nsignificant findings to unmeasured confounding. While sensitivity analyses in\nmatched observational studies have been well addressed when there is a single\noutcome variable, accounting for multiple comparisons through the existing\nmethods yields overly conservative results when there are multiple outcome\nvariables of interest. This stems from the fact that unmeasured confounding\ncannot affect the probability of assignment to treatment differently depending\non the outcome being analyzed. Existing methods allow this to occur by\ncombining the results of individual sensitivity analyses to assess whether at\nleast one hypothesis is significant, which in turn results in an overly\npessimistic assessment of a study's sensitivity to unobserved biases. By\nsolving a quadratically constrained linear program, we are able to perform a\nsensitivity analysis while enforcing that unmeasured confounding must have the\nsame impact on the treatment assignment probabilities across outcomes for each\nindividual in the study. We show that this allows for uniform improvements in\nthe power of a sensitivity analysis not only for testing the overall null of no\neffect, but also for null hypotheses on \\textit{specific} outcome variables\nwhile strongly controlling the familywise error rate. We illustrate our method\nthrough an observational study on the effect of smoking on naphthalene\nexposure.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 17:42:17 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 18:35:46 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2015 22:17:46 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2015 22:39:27 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Fogarty", "Colin B.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1505.04015", "submitter": "James Wilson", "authors": "James D. Wilson, Matthew J. Denny, Shankar Bhamidi, Skyler Cranmer,\n  Bruce Desmarais", "title": "Stochastic Weighted Graphs: Flexible Model Specification and Simulation", "comments": "33 pages, 6 figures. To appear in Social Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most domains of network analysis researchers consider networks that arise\nin nature with weighted edges. Such networks are routinely dichotomized in the\ninterest of using available methods for statistical inference with networks.\nThe generalized exponential random graph model (GERGM) is a recently proposed\nmethod used to simulate and model the edges of a weighted graph. The GERGM\nspecifies a joint distribution for an exponential family of graphs with\ncontinuous-valued edge weights. However, current estimation algorithms for the\nGERGM only allow inference on a restricted family of model specifications. To\naddress this issue, we develop a Metropolis--Hastings method that can be used\nto estimate any GERGM specification, thereby significantly extending the family\nof weighted graphs that can be modeled with the GERGM. We show that new\nflexible model specifications are capable of avoiding likelihood degeneracy and\nefficiently capturing network structure in applications where such models were\nnot previously available. We demonstrate the utility of this new class of\nGERGMs through application to two real network data sets, and we further assess\nthe effectiveness of our proposed methodology by simulating non-degenerate\nmodel specifications from the well-studied two-stars model. A working R version\nof the GERGM code is available in the supplement and will be incorporated in\nthe gergm CRAN package.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 10:42:41 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 05:19:27 GMT"}, {"version": "v3", "created": "Wed, 9 Nov 2016 19:03:36 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Wilson", "James D.", ""], ["Denny", "Matthew J.", ""], ["Bhamidi", "Shankar", ""], ["Cranmer", "Skyler", ""], ["Desmarais", "Bruce", ""]]}, {"id": "1505.04234", "submitter": "Robert Staudte", "authors": "Luke A. Prendergast and Robert G. Staudte", "title": "Exploiting the Quantile Optimality Ratio to Obtain Better Confidence\n  Intervals for Quantiles", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": "10.1002/sta4.105", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard approach to confidence intervals for quantiles requires good\nestimates of the quantile density. The optimal bandwidth for kernel estimation\nof the quantile density depends on an underlying location-scale family only\nthrough the quantile optimality ratio (QOR), which is the starting point for\nour results. While the QOR is not distribution-free, it turns out that what is\noptimal for one family often works quite well for families having similar\nshape. This allows one to rely on a single representative QOR if one has a\nrough idea of the distributional shape. Another option that we explore assumes\nthe data can be modeled by the highly flexible generalized lambda distribution\n(GLD), already studied by others, and we show that using the QOR for the\nestimated GLD can lead to more than competitive intervals. Confidence intervals\nfor the difference between quantiles from independent populations are also\nconsidered, with an application to heart rate data.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 02:28:21 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 23:54:22 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Prendergast", "Luke A.", ""], ["Staudte", "Robert G.", ""]]}, {"id": "1505.04302", "submitter": "Nicholas James", "authors": "Nicholas A. James, David S. Matteson", "title": "Change Points via Probabilistically Pruned Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of homogeneity plays a critical role in statistics, both in its\napplications as well as its theory. Change point analysis is a statistical tool\nthat aims to attain homogeneity within time series data. This is accomplished\nthrough partitioning the time series into a number of contiguous homogeneous\nsegments. The applications of such techniques range from identifying chromosome\nalterations to solar flare detection. In this manuscript we present a general\npurpose search algorithm called cp3o that can be used to identify change points\nin multivariate time series. This new search procedure can be applied with a\nlarge class of goodness of fit measures. Additionally, a reduction in the\ncomputational time needed to identify change points is accomplish by means of\nprobabilistic pruning. With mild assumptions about the goodness of fit measure\nthis new search algorithm is shown to generate consistent estimates for both\nthe number of change points and their locations, even when the number of change\npoints increases with the time series length.\n  A change point algorithm that incorporates the cp3o search algorithm and\nE-Statistics, e-cp3o, is also presented. The only distributional assumption\nthat the e-cp3o procedure makes is that the absolute $\\alpha$th moment exists,\nfor some $\\alpha\\in(0,2)$. Due to this mild restriction, the e-cp3o procedure\ncan be applied to a majority of change point problems. Furthermore, even with\nsuch a mild restriction, the e-cp3o procedure has the ability to detect any\ntype of distributional change within a time series. Simulation studies are used\nto compare the e-cp3o procedure to other parametric and nonparametric change\npoint procedures, we highlight applications of e-cp3o to climate and financial\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 17:58:26 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["James", "Nicholas A.", ""], ["Matteson", "David S.", ""]]}, {"id": "1505.04321", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob (University of Oxford)", "title": "Sequential Bayesian inference for implicit hidden Markov models and\n  current limitations", "comments": "Review article written for ESAIM: proceedings and surveys. 25 pages,\n  10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models can describe time series arising in various fields of\nscience, by treating the data as noisy measurements of an arbitrarily complex\nMarkov process. Sequential Monte Carlo (SMC) methods have become standard tools\nto estimate the hidden Markov process given the observations and a fixed\nparameter value. We review some of the recent developments allowing the\ninclusion of parameter uncertainty as well as model uncertainty. The\nshortcomings of the currently available methodology are emphasised from an\nalgorithmic complexity perspective. The statistical objects of interest for\ntime series analysis are illustrated on a toy \"Lotka-Volterra\" model used in\npopulation ecology. Some open challenges are discussed regarding the\nscalability of the reviewed methodology to longer time series,\nhigher-dimensional state spaces and more flexible models.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 20:11:52 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Jacob", "Pierre E.", "", "University of Oxford"]]}, {"id": "1505.04493", "submitter": "Wen Zhou", "authors": "Jinyuan Chang, Wen Zhou, Wen-Xin Zhou, and Lan Wang", "title": "Comparing large covariance matrices under weak conditions on the\n  dependence structure and its application to gene clustering", "comments": "The original title dated back to May 2015 is \"Bootstrap Tests on High\n  Dimensional Covariance Matrices with Applications to Understanding Gene\n  Clustering\"", "journal-ref": "Biometrics 2017, Vol. 73, No. 1, 31-41", "doi": "10.1111/biom.12552", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing large covariance matrices has important applications in modern\ngenomics, where scientists are often interested in understanding whether\nrelationships (e.g., dependencies or co-regulations) among a large number of\ngenes vary between different biological states. We propose a computationally\nfast procedure for testing the equality of two large covariance matrices when\nthe dimensions of the covariance matrices are much larger than the sample\nsizes. A distinguishing feature of the new procedure is that it imposes no\nstructural assumptions on the unknown covariance matrices. Hence the test is\nrobust with respect to various complex dependence structures that frequently\narise in genomics. We prove that the proposed procedure is asymptotically valid\nunder weak moment conditions. As an interesting application, we derive a new\ngene clustering algorithm which shares the same nice property of avoiding\nrestrictive structural assumptions for high-dimensional genomics data. Using an\nasthma gene expression dataset, we illustrate how the new test helps compare\nthe covariance matrices of the genes across different gene sets/pathways\nbetween the disease group and the control group, and how the gene clustering\nalgorithm provides new insights on the way gene clustering patterns differ\nbetween the two groups. The proposed methods have been implemented in an\nR-package HDtest and is available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 02:45:09 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 05:21:47 GMT"}, {"version": "v3", "created": "Sun, 24 Apr 2016 19:53:55 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Chang", "Jinyuan", ""], ["Zhou", "Wen", ""], ["Zhou", "Wen-Xin", ""], ["Wang", "Lan", ""]]}, {"id": "1505.04629", "submitter": "Jiannan Lu", "authors": "Jiannan Lu, Peng Ding, Tirthankar Dasgupta", "title": "Construction of alternative hypotheses for randomization tests with\n  ordinal outcomes", "comments": null, "journal-ref": "Stat. Prob. Lett., 107:348-355 (2015)", "doi": "10.1016/j.spl.2015.09.013", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For ordinal outcomes, we construct sequences of alternative hypotheses in\nincreasing departures from the sharp null hypothesis of zero treatment effect\non each experimental unit, to help assess the powers of randomization tests in\nrandomized treatment-control experiments.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:24:15 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 17:54:34 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Lu", "Jiannan", ""], ["Ding", "Peng", ""], ["Dasgupta", "Tirthankar", ""]]}, {"id": "1505.04697", "submitter": "Adam Sales", "authors": "Adam C Sales, Ben B Hansen, Brian Rowan", "title": "Rebar: Reinforcing a Matching Estimator with Predictions from\n  High-Dimensional Covariates", "comments": "Published in Journal of Educational and Behavioral Statistics\n  (Currently 12/6/17 \"Online First\")", "journal-ref": "Journal of Educational and Behavioral Statistics, 43(1), 3-31", "doi": "10.3102/1076998617731518", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In causal matching designs, some control subjects are often left unmatched,\nand some covariates are often left unmodeled. This article introduces \"rebar,\"\na method using high-dimensional modeling to incorporate these commonly\ndiscarded data without sacrificing the integrity of the matching design. After\nconstructing a match, a researcher uses the unmatched control subjects--the\nremnant--to fit a machine learning model predicting control potential outcomes\nas a function of the full covariate matrix. The resulting predictions in the\nmatched set are used to adjust the causal estimate to reduce confounding bias.\nWe present theoretical results to justify the method's bias-reducing properties\nas well as a simulation study that demonstrates them. Additionally, we\nillustrate the method in an evaluation of a school-level comprehensive\neducational reform program in Arizona.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 16:00:25 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 19:51:20 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 16:04:31 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Sales", "Adam C", ""], ["Hansen", "Ben B", ""], ["Rowan", "Brian", ""]]}, {"id": "1505.04768", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela, Victor M. Panaretos", "title": "Statistical unfolding of elementary particle spectra: Empirical Bayes\n  estimation and bias-corrected uncertainty quantification", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS857 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). arXiv admin note:\n  substantial text overlap with arXiv:1401.8274", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1671-1705", "doi": "10.1214/15-AOAS857", "report-no": "IMS-AOAS-AOAS857", "categories": "stat.AP hep-ex physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the high energy physics unfolding problem where the goal is to\nestimate the spectrum of elementary particles given observations distorted by\nthe limited resolution of a particle detector. This important statistical\ninverse problem arising in data analysis at the Large Hadron Collider at CERN\nconsists in estimating the intensity function of an indirectly observed Poisson\npoint process. Unfolding typically proceeds in two steps: one first produces a\nregularized point estimate of the unknown intensity and then uses the\nvariability of this estimator to form frequentist confidence intervals that\nquantify the uncertainty of the solution. In this paper, we propose forming the\npoint estimate using empirical Bayes estimation which enables a data-driven\nchoice of the regularization strength through marginal maximum likelihood\nestimation. Observing that neither Bayesian credible intervals nor standard\nbootstrap confidence intervals succeed in achieving good frequentist coverage\nin this problem due to the inherent bias of the regularized point estimate, we\nintroduce an iteratively bias-corrected bootstrap technique for constructing\nimproved confidence intervals. We show using simulations that this enables us\nto achieve nearly nominal frequentist coverage with only a modest increase in\ninterval length. The proposed methodology is applied to unfolding the $Z$ boson\ninvariant mass spectrum as measured in the CMS experiment at the Large Hadron\nCollider.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:26:46 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 13:01:49 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 13:25:17 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Kuusela", "Mikael", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1505.04827", "submitter": "Roland Langrock", "authors": "Ruth King and Roland Langrock", "title": "Semi-Markov Arnason-Schwarz models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-state capture-recapture-recovery data where observed\nindividuals are recorded in a set of possible discrete states. Traditionally,\nthe Arnason-Schwarz model has been fitted to such data where the state process\nis modeled as a first-order Markov chain, though second-order models have also\nbeen proposed and fitted to data. However, low-order Markov models may not\naccurately represent the underlying biology. For example, specifying a\n(time-independent) first-order Markov process assumes that the dwell time in\neach state (i.e., the duration of a stay in a given state) has a geometric\ndistribution, and hence that the modal dwell time is one. Specifying\ntime-dependent or higher-order processes provides additional flexibility, but\nat the expense of a potentially significant number of additional model\nparameters. We extend the Arnason-Schwarz model by specifying a semi-Markov\nmodel for the state process, where the dwell-time distribution is specified\nmore generally, using for example a shifted Poisson or negative binomial\ndistribution. A state expansion technique is applied in order to represent the\nresulting semi-Markov Arnason-Schwarz model in terms of a simpler and\ncomputationally tractable hidden Markov model. Semi-Markov Arnason-Schwarz\nmodels come with only a very modest increase in the number of parameters, yet\npermit a significantly more flexible state process. Model selection can be\nperformed using standard procedures, and in particular via the use of\ninformation criteria. The semi-Markov approach allows for important biological\ninference to be drawn on the underlying state process, for example on the times\nspent in the different states. The feasibility of the approach is demonstrated\nin a simulation study, before being applied to real data corresponding to house\nfinches where the states correspond to the presence or absence of\nconjunctivitis.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 21:55:27 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["King", "Ruth", ""], ["Langrock", "Roland", ""]]}, {"id": "1505.04844", "submitter": "Piotr Dniestrzanski PhD", "authors": "Piotr Dniestrzanski", "title": "Gini index and angle measure as special cases of a wider family of\n  measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will show that the Gini coefficient and the introduced\nmeasure of angular inequality are special cases of a wider indexed family of\nmeasurements. We will discuss the properties of the defined class based, inter\nalia, on a new elementary characterization of the Gini coefficient.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 00:46:22 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 14:05:26 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Dniestrzanski", "Piotr", ""]]}, {"id": "1505.04883", "submitter": "Chandler Zuo", "authors": "Chandler Zuo, Kailei Chen, Kyle Hewitt, Emery Bresnick, Sunduz Keles", "title": "A Hierarchical Framework for State Space Matrix Inference and Clustering", "comments": "65 pages, 27 figures", "journal-ref": "Annals of Applied Statistics. Volume 10, Number 3 (2016),\n  1348-1372", "doi": "10.1214/16-AOAS938", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a large number of genomic and epigenomic studies have been\nfocusing on the integrative analysis of multiple experimental datasets measured\nover a large number of observational units. The objectives of such studies\ninclude not only inferring a hidden state of activity for each unit over\nindividual experiments, but also detecting highly associated clusters of units\nbased on their inferred states. In this paper, we develop the MBASIC (Matrix\nBased Analysis for State-space Inference and Clustering) framework. MBASIC\nconsists of two parts: state-space mapping and state-space clustering. In\nstate-space mapping, it maps observations onto a finite state-space,\nrepresenting the activation states of units across conditions. In state-space\nclustering, MBASIC incorporates a finite mixture model to cluster the units\nbased on their inferred state-space profiles across all conditions. Both the\nstate-space mapping and clustering can be simultaneously estimated through an\nExpectation-Maximization algorithm. MBASIC flexibly adapts to a large number of\nparametric distributions for the observed data, as well as the heterogeneity in\nreplicate experiments. In our data-driven simulation studies, MBASIC showed\nsignificant accuracy in recovering both the underlying state-space variables\nand clustering structures. We applied MBASIC to two genome research problems\nusing large numbers of datasets from the ENCODE project. In both studies,\nMBASIC showed higher levels of raw data fidelity than analyzing these data with\na two-step approach using ENCODE results on transcription factor occupancy\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 06:21:37 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 17:01:05 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Zuo", "Chandler", ""], ["Chen", "Kailei", ""], ["Hewitt", "Kyle", ""], ["Bresnick", "Emery", ""], ["Keles", "Sunduz", ""]]}, {"id": "1505.04898", "submitter": "Florian Pein", "authors": "Florian Pein, Hannes Sieling, Axel Munk", "title": "Heterogeneous Change Point Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HSMUCE (heterogeneous simultaneous multiscale change-point\nestimator) for the detection of multiple change-points of the signal in a\nheterogeneous gaussian regression model. A piecewise constant function is\nestimated by minimizing the number of change-points over the acceptance region\nof a multiscale test which locally adapts to changes in the variance. The\nmultiscale test is a combination of local likelihood ratio tests which are\nproperly calibrated by scale dependent critical values in order to keep a\nglobal nominal level alpha, even for finite samples. We show that HSMUCE\ncontrols the error of over- and underestimation of the number of change-points.\nTo this end, new deviation bounds for F-type statistics are derived. Moreover,\nwe obtain confidence sets for the whole signal. All results are non-asymptotic\nand uniform over a large class of heterogeneous change-point models. HSMUCE is\nfast to compute, achieves the optimal detection rate and estimates the number\nof change-points at almost optimal accuracy for vanishing signals, while still\nbeing robust. We compare HSMUCE with several state of the art methods in\nsimulations and analyse current recordings of a transmembrane protein in the\nbacterial outer membrane with pronounced heterogeneity for its states. An\nR-package is available online.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 07:56:20 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 14:19:41 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Pein", "Florian", ""], ["Sieling", "Hannes", ""], ["Munk", "Axel", ""]]}, {"id": "1505.04983", "submitter": "Paul Northrop", "authors": "Paul J. Northrop (1), Nicolas Attalides (1) ((1) University College\n  London, UK)", "title": "Posterior propriety in Bayesian extreme value analyses using reference\n  priors", "comments": "20 pages, 2 figures; typo corrected on page 5 (line -2, Euler's\n  constant corrected to approx. 0.57722). The final publication is available at\n  http://www3.stat.sinica.edu.tw/preprint/SS-14-034_preprint.pdf or\n  http://dx.doi.org/10.5705/ss.2014.034", "journal-ref": "Statistica Sinica 26 (2016), 721-743", "doi": "10.5705/ss.2014.034", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generalized Pareto (GP) and Generalized extreme value (GEV) distributions\nplay an important role in extreme value analyses, as models for threshold\nexcesses and block maxima respectively. For each of these distributions we\nconsider Bayesian inference using \"reference\" prior distributions (in the\ngeneral sense of priors constructed using formal rules) for the model\nparameters, specifically a Jeffreys prior, the maximal data information (MDI)\nprior and independent uniform priors on separate model parameters. We\ninvestigate the important issue of whether these improper priors lead to proper\nposterior distributions. We show that, in the GP and GEV cases, the MDI prior,\nunless modified, never yields a proper posterior and that in the GEV case this\nalso applies to the Jeffreys prior. We also show that a sample size of three\n(four) is sufficient for independent uniform priors to yield a proper posterior\ndistribution in the GP (GEV) case.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 13:08:42 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 20:52:41 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2015 09:27:28 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Northrop", "Paul J.", ""], ["Attalides", "Nicolas", ""]]}, {"id": "1505.05229", "submitter": "Tyler Massaro", "authors": "T. J. Massaro and H. Bozdogan", "title": "Variable subset selection via GA and information complexity in mixtures\n  of Poisson and negative binomial regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Count data, for example the number of observed cases of a disease in a city,\noften arise in the fields of healthcare analytics and epidemiology. In this\npaper, we consider performing regression on multivariate data in which our\noutcome is a count. Specifically, we derive log-likelihood functions for finite\nmixtures of regression models involving counts that come from a Poisson\ndistribution, as well as a negative binomial distribution when the counts are\nsignificantly overdispersed. Within our proposed modeling framework, we carry\nout optimal component selection using the information criteria scores AIC, BIC,\nCAIC, and ICOMP. We demonstrate applications of our approach on simulated data,\nas well as on a real data set of HIV cases in Tennessee counties from the year\n2010. Finally, using a genetic algorithm within our framework, we perform\nvariable subset selection to determine the covariates that are most responsible\nfor categorizing Tennessee counties. This leads to some interesting insights\ninto the traits of counties that have high HIV counts.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 02:36:13 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Massaro", "T. J.", ""], ["Bozdogan", "H.", ""]]}, {"id": "1505.05257", "submitter": "Shota Katayama", "authors": "Shota Katayama and Hironori Fujisawa", "title": "Sparse and Robust Linear Regression: An Optimization Algorithm and Its\n  Statistical Properties", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies sparse linear regression analysis with outliers in the\nresponses. A parameter vector for modeling outliers is added to the standard\nlinear regression model and then the sparse estimation problem for both\ncoefficients and outliers is considered. The $\\ell_{1}$ penalty is imposed for\nthe coefficients, while various penalties including redescending type penalties\nare for the outliers. To solve the sparse estimation problem, we introduce an\noptimization algorithm. Under some conditions, we show the algorithmic and\nstatistical convergence property for the coefficients obtained by the\nalgorithm. Moreover, it is shown that the algorithm can recover the true\nsupport of the coefficients with probability going to one.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 06:46:32 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Katayama", "Shota", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1505.05266", "submitter": "Holger Dette", "authors": "Holger Dette and Kathrin M\\\"ollenhoff and Stanislav Volgushev and\n  Frank Bretz", "title": "Equivalence of dose response curves", "comments": "28 pages Keywords and Phrases: dose response studies; nonlinear\n  regression; equivalence of curves; constrained parameter estimation;\n  parametric bootstrap", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem whether the difference between two\nparametric models $m_1,m_2$ describing the relation between a response variable\nand several covariates in two different groups is practically irrelevant, such\nthat inference can be performed on the basis of the pooled sample. Statistical\nmethodology is developed to test the hypotheses $H_0 : d(m_1,m_2)\\geq \\epsilon$\nversus $H_1 : d(m_1,m_2) < \\epsilon$ to demonstrate equivalence between the two\nregression curves $m_1,m_2$ for a pre-specified threshold $\\epsilon$, where $d$\ndenotes a distance measuring the distance between $m_1$ and $m_2$. Our approach\nis based on the asymptotic properties of a suitable estimator $d(\\hat{m}_1;\n\\hat{m}_2)$ of this distance. In order to improve the approximation of the\nnominal level for small sample sizes a bootstrap test is developed, which\naddresses the specific form of the interval hypotheses. In particular, data has\nto be generated under the null hypothesis, which implicitly defines a manifold\nfor the parameter vector. The results are illustrated by means of a simulation\nstudy and a data example. It is demonstrated that the new methods substantially\nimprove currently available approaches with respect to power and approximation\nof the nominal level.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 07:33:35 GMT"}, {"version": "v2", "created": "Thu, 21 May 2015 06:53:27 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 20:21:55 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Dette", "Holger", ""], ["M\u00f6llenhoff", "Kathrin", ""], ["Volgushev", "Stanislav", ""], ["Bretz", "Frank", ""]]}, {"id": "1505.05272", "submitter": "Merle Behr", "authors": "Merle Behr and Axel Munk", "title": "Identifiability for Blind Source Separation of Multiple Finite Alphabet\n  Linear Mixtures", "comments": null, "journal-ref": null, "doi": "10.1109/TIT.2017.2717586", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give under weak assumptions a complete combinatorial characterization of\nidentifiability for linear mixtures of finite alphabet sources, with unknown\nmixing weights and unknown source signals, but known alphabet. This is based on\na detailed treatment of the case of a single linear mixture. Notably, our\nidentifiability analysis applies also to the case of unknown number of sources.\nWe provide sufficient and necessary conditions for identifiability and give a\nsimple sufficient criterion together with an explicit construction to determine\nthe weights and the source signals for deterministic data by taking advantage\nof the hierarchical structure within the possible mixture values. We show that\nthe probability of identifiability is related to the distribution of a hitting\ntime and converges exponentially fast to one when the underlying sources come\nfrom a discrete Markov process. Finally, we explore our theoretical results in\na simulation study. Our work extends and clarifies the scope of scenarios for\nwhich blind source separation becomes meaningful.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 08:06:40 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 15:16:50 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 16:27:33 GMT"}, {"version": "v4", "created": "Tue, 16 May 2017 15:23:38 GMT"}, {"version": "v5", "created": "Wed, 30 Aug 2017 12:01:04 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Behr", "Merle", ""], ["Munk", "Axel", ""]]}, {"id": "1505.05290", "submitter": "Sheng Han", "authors": "Suzhen Wang, Sheng Han, Zhiguo Zhang, and Wing Shing Wong", "title": "Sparsest Error Detection via Sparsity Invariant Transformation based\n  $\\ell_1$ Minimization", "comments": "20 pages, single column. 7 Figures. To be submitted and under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method, referred to here as the sparsity invariant\ntransformation based $\\ell_1$ minimization, to solve the $\\ell_0$ minimization\nproblem for an over-determined linear system corrupted by additive sparse\nerrors with arbitrary intensity. Many previous works have shown that $\\ell_1$\nminimization can be applied to realize sparse error detection in many\nover-determined linear systems. However, performance of this approach is\nstrongly dependent on the structure of the measurement matrix, which limits\napplication possibility in practical problems. Here, we present a new approach\nbased on transforming the $\\ell_0$ minimization problem by a linear\ntransformation that keeps sparsest solutions invariant. We call such a property\na sparsity invariant property (SIP), and a linear transformation with SIP is\nreferred to as a sparsity invariant transformation (SIT). We propose the\nSIT-based $\\ell_1$ minimization method by using an SIT in conjunction with\n$\\ell_1$ relaxation on the $\\ell_0$ minimization problem. We prove that for any\nover-determined linear system, there always exists a specific class of SIT's\nthat guarantees a solution to the SIT-based $\\ell_1$ minimization is a\nsparsest-errors solution. Besides, a randomized algorithm based on Monte Carlo\nsimulation is proposed to search for a feasible SIT.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 09:09:32 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Wang", "Suzhen", ""], ["Han", "Sheng", ""], ["Zhang", "Zhiguo", ""], ["Wong", "Wing Shing", ""]]}, {"id": "1505.05314", "submitter": "Johanna F. Ziegel", "authors": "Christof Str\\\"ahl and Johanna F. Ziegel", "title": "Cross-calibration of probabilistic forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When providing probabilistic forecasts for uncertain future events, it is\ncommon to strive for calibrated forecasts, that is, the predictive distribution\nshould be compatible with the observed outcomes. Several notions of calibration\nare available in the case of a single forecaster alongside with diagnostic\ntools and statistical tests to assess calibration in practice. Often, there is\nmore than one forecaster providing predictions, and these forecasters may use\ninformation of the others and therefore influence one another. We extend common\nnotions of calibration, where each forecaster is analysed individually, to\nnotions of cross-calibration where each forecaster is analysed with respect to\nthe other forecasters in a natural way. It is shown theoretically and in\nsimulation studies that cross-calibration is a stronger requirement on a\nforecaster than calibration. Analogously to calibration for individual\nforecasters, we provide diagnostic tools and statistical tests to assess\nforecasters in terms of cross-calibration. The methods are illustrated in\nsimulation examples and applied to probabilistic forecasts for inflation rates\nby the Bank of England.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 10:54:43 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Str\u00e4hl", "Christof", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1505.05461", "submitter": "Karl Rohe", "authors": "Karl Rohe", "title": "Network driven sampling; a critical threshold for design effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web crawling, snowball sampling, and respondent-driven sampling (RDS) are\nthree types of network sampling techniques used to contact individuals in\nhard-to-reach populations. This paper studies these procedures as a Markov\nprocess on the social network that is indexed by a tree. Each node in this tree\ncorresponds to an observation and each edge in the tree corresponds to a\nreferral. Indexing with a tree (instead of a chain) allows for the sampled\nunits to refer multiple future units into the sample. In survey sampling, the\ndesign effect characterizes the additional variance induced by a novel sampling\nstrategy. If the design effect is some value $DE$, then constructing an\nestimator from the novel design makes the variance of the estimator $DE$ times\ngreater than it would be under a simple random sample with the same sample size\n$n$. Under certain assumptions on the referral tree, the design effect of\nnetwork sampling has a critical threshold that is a function of the referral\nrate $m$ and the clustering structure in the social network, represented by the\nsecond eigenvalue of the Markov transition matrix, $\\lambda_2$. If $m <\n1/\\lambda_2^2$, then the design effect is finite (i.e. the standard estimator\nis $\\sqrt{n}$-consistent). However, if $m > 1/\\lambda_2^2$, then the design\neffect grows with $n$ (i.e. the standard estimator is no longer\n$\\sqrt{n}$-consistent). Past this critical threshold, the standard error of the\nestimator converges at the slower rate of $n^{\\log_m \\lambda_2}$. The Markov\nmodel allows for nodes to be resampled; computational results show that the\nfindings hold in without-replacement sampling. To estimate confidence intervals\nthat adapt to the correct level of uncertainty, a novel resampling procedure is\nproposed. Computational experiments compare this procedure to previous\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 17:36:03 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 13:01:27 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2015 22:43:28 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 21:53:47 GMT"}, {"version": "v5", "created": "Thu, 1 Jun 2017 11:27:37 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Rohe", "Karl", ""]]}, {"id": "1505.05466", "submitter": "Ricardo Ehlers", "authors": "Felipe R. S. de Gusm\\~ao and Vera L. D. Tomazella and Ricardo S.\n  Ehlers", "title": "Bayesian Estimation of the Kumaraswamy Inverse Weibull Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kumaraswamy Inverse Weibull distribution has the ability to model failure\nrates that have unimodal shapes and are quite common in reliability and\nbiological studies. The three-parameter Kumaraswamy Inverse Weibull\ndistribution with decreasing and unimodal failure rate is introduced. We\nprovide a comprehensive treatment of the mathematical properties of the\nKumaraswany Inverse Weibull distribution and derive expressions for its moment\ngenerating function and the $r$-th generalized moment. Some properties of the\nmodel with some graphs of density and hazard function are discussed. We also\ndiscuss a Bayesian approach for this distribution and an application was made\nfor a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 17:50:08 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["de Gusm\u00e3o", "Felipe R. S.", ""], ["Tomazella", "Vera L. D.", ""], ["Ehlers", "Ricardo S.", ""]]}, {"id": "1505.05654", "submitter": "Fabien Navarro", "authors": "Fabien Navarro and Adrien Saumard", "title": "Slope heuristics and V-Fold model selection in heteroscedastic\n  regression using strongly localized bases", "comments": null, "journal-ref": null, "doi": "10.1051/ps/2017005", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the optimality for model selection of the so-called slope\nheuristics, $V$-fold cross-validation and $V$-fold penalization in a\nheteroscedastic with random design regression context. We consider a new class\nof linear models that we call strongly localized bases and that generalize\nhistograms, piecewise polynomials and compactly supported wavelets. We derive\nsharp oracle inequalities that prove the asymptotic optimality of the slope\nheuristics---when the optimal penalty shape is known---and $V$ -fold\npenalization. Furthermore, $V$-fold cross-validation seems to be suboptimal for\na fixed value of $V$ since it recovers asymptotically the oracle learned from a\nsample size equal to $1-V^{-1}$ of the original amount of data. Our results are\nbased on genuine concentration inequalities for the true and empirical excess\nrisks that are of independent interest. We show in our experiments the good\nbehavior of the slope heuristics for the selection of linear wavelet models.\nFurthermore, $V$-fold cross-validation and $V$-fold penalization have\ncomparable efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 09:29:53 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 09:57:40 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 09:32:36 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Navarro", "Fabien", ""], ["Saumard", "Adrien", ""]]}, {"id": "1505.05660", "submitter": "Stephane Robin", "authors": "Xavier Collilieux and Emilie Lebarbier and St\\'ephane Robin", "title": "A factor model approach for the joint segmentation with between-series\n  correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the segmentation of set of correlated time-series, the\ncorrelation being allowed to take an arbitrary form but being the same at each\ntime-position. We show that encoding the dependency in a factor model enables\nus to use the dynamic programming algorithm for the inference of the\nbreakpoints, which remains one the most efficient algorithm. We propose a model\nselection procedure to determine both the number of breakpoints and the number\nof factors. This proposed method is implemented in the FASeg R package, which\nis available on the CRAN. We demonstrate the performances of our procedure\nthrough simulation experiments and an application to geodesic data is\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 09:48:06 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 13:03:10 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Collilieux", "Xavier", ""], ["Lebarbier", "Emilie", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1505.05687", "submitter": "Tiejun Tong", "authors": "Dehui Luo, Xiang Wan, Jiming Liu and Tiejun Tong", "title": "Optimally estimating the sample mean from the sample size, median,\n  mid-range and/or mid-quartile range", "comments": "21 pages, 7 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The era of big data is coming, and evidence-based medicine is attracting\nincreasing attention to improve decision making in medical practice via\nintegrating evidence from well designed and conducted clinical research.\nMeta-analysis is a statistical technique widely used in evidence-based medicine\nfor analytically combining the findings from independent clinical trials to\nprovide an overall estimation of a treatment effectiveness. The sample mean and\nstandard deviation are two commonly used statistics in meta-analysis but some\ntrials use the median, the minimum and maximum values, or sometimes the first\nand third quartiles to report the results. Thus, to pool results in a\nconsistent format, researchers need to transform those information back to the\nsample mean and standard deviation. In this paper, we investigate the optimal\nestimation of the sample mean for meta-analysis from both theoretical and\nempirical perspectives. A major drawback in the literature is that the sample\nsize, needless to say its importance, is either ignored or used in a stepwise\nbut somewhat arbitrary manner, e.g., the famous method proposed by Hozo et al.\nWe solve this issue by incorporating the sample size in a smoothly changing\nweight in the estimators to reach the optimal estimation. Our proposed\nestimators not only improve the existing ones significantly but also share the\nsame virtue of the simplicity. The real data application indicates that our\nproposed estimators are capable to serve as \"rules of thumb\" and will be widely\napplied in evidence-based medicine.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 12:01:32 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2015 12:45:56 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2016 14:27:42 GMT"}, {"version": "v4", "created": "Tue, 4 Oct 2016 07:35:57 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Luo", "Dehui", ""], ["Wan", "Xiang", ""], ["Liu", "Jiming", ""], ["Tong", "Tiejun", ""]]}, {"id": "1505.05770", "submitter": "Danilo Jimenez Rezende", "authors": "Danilo Jimenez Rezende and Shakir Mohamed", "title": "Variational Inference with Normalizing Flows", "comments": "Proceedings of the 32nd International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 15:36:37 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 09:13:28 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 15:46:33 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 18:36:32 GMT"}, {"version": "v5", "created": "Mon, 13 Jun 2016 08:46:44 GMT"}, {"version": "v6", "created": "Tue, 14 Jun 2016 09:01:36 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1505.06129", "submitter": "Magalie Fromont", "authors": "M\\'elisande Albert and Yann Bouret and Magalie Fromont and Patricia\n  Reynaud-Bouret", "title": "A Distribution Free Unitary Events Method based on Delayed Coincidence\n  Count", "comments": "45 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate several distribution free dependence detection procedures,\nmainly based on bootstrap principles and their approximation properties. Thanks\nto this study, we introduce a new distribution free Unitary Events (UE) method,\nnamed Permutation UE, which consists in a multiple testing procedure based on\npermutation and delayed coincidence count. Each involved single test of this\nprocedure achieves the prescribed level, so that the corresponding multiple\ntesting procedure controls the False Discovery Rate (FDR), and this with as few\nassumptions as possible on the underneath distribution. Some simulations show\nthat this method outperforms the trial-shuffling and the MTGAUE method in terms\nof single levels and FDR, for a comparable amount of false negatives.\nApplication on real data is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 15:57:26 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Albert", "M\u00e9lisande", ""], ["Bouret", "Yann", ""], ["Fromont", "Magalie", ""], ["Reynaud-Bouret", "Patricia", ""]]}, {"id": "1505.06318", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Rachele Anderson", "title": "Approximate maximum likelihood estimation using data-cloning ABC", "comments": "25 pages. Minor revision. It includes a parametric bootstrap for the\n  exact MLE for the first example; includes mean bias and RMSE calculations for\n  the third example. Forthcoming in Computational Statistics and Data Analysis", "journal-ref": null, "doi": "10.1016/j.csda.2016.08.006", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A maximum likelihood methodology for a general class of models is presented,\nusing an approximate Bayesian computation (ABC) approach. The typical target of\nABC methods are models with intractable likelihoods, and we combine an ABC-MCMC\nsampler with so-called \"data cloning\" for maximum likelihood estimation.\nAccuracy of ABC methods relies on the use of a small threshold value for\ncomparing simulations from the model and observed data. The proposed\nmethodology shows how to use large threshold values, while the number of\ndata-clones is increased to ease convergence towards an approximate maximum\nlikelihood estimate. We show how to exploit the methodology to reduce the\nnumber of iterations of a standard ABC-MCMC algorithm and therefore reduce the\ncomputational effort, while obtaining reasonable point estimates. Simulation\nstudies show the good performance of our approach on models with intractable\nlikelihoods such as g-and-k distributions, stochastic differential equations\nand state-space models.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 12:31:00 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 05:43:32 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 12:22:30 GMT"}, {"version": "v4", "created": "Thu, 11 Aug 2016 09:30:40 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Picchini", "Umberto", ""], ["Anderson", "Rachele", ""]]}, {"id": "1505.06472", "submitter": "Ville Satopaa", "authors": "Ville A. Satop\\\"a\\\"a, Shane T. Jensen, Robin Pemantle, and Lyle H.\n  Ungar", "title": "Partial Information Framework: Model-Based Aggregation of Estimates from\n  Diverse Information Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction polling is an increasingly popular form of crowdsourcing in which\nmultiple participants estimate the probability or magnitude of some future\nevent. These estimates are then aggregated into a single forecast.\nHistorically, randomness in scientific estimation has been generally assumed to\narise from unmeasured factors which are viewed as measurement noise. However,\nwhen combining subjective estimates, heterogeneity stemming from differences in\nthe participants' information is often more important than measurement noise.\nThis paper formalizes information diversity as an alternative source of such\nheterogeneity and introduces a novel modeling framework that is particularly\nwell-suited for prediction polls. A practical specification of this framework\nis proposed and applied to the task of aggregating probability and point\nestimates from two real-world prediction polls. In both cases our model\noutperforms standard measurement-error-based aggregators, hence providing\nevidence in favor of information diversity being the more important source of\nheterogeneity.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 20:13:30 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 01:32:26 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Satop\u00e4\u00e4", "Ville A.", ""], ["Jensen", "Shane T.", ""], ["Pemantle", "Robin", ""], ["Ungar", "Lyle H.", ""]]}, {"id": "1505.06483", "submitter": "Vishal Kamat", "authors": "Vishal Kamat", "title": "On Nonparametric Inference in the Regression Discontinuity Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the validity of nonparametric tests used in the regression\ndiscontinuity design. The null hypothesis of interest is that the average\ntreatment effect at the threshold in the so-called sharp design equals a\npre-specified value. We first show that, under assumptions used in the majority\nof the literature, for \\emph{any} test the power against any alternative is\nbounded above by its size. This result implies that, under these assumptions,\nany test with nontrivial power will exhibit size distortions. We next provide a\nsufficient strengthening of the standard assumptions under which we show that a\nnovel test in the literature can control limiting size.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 21:13:27 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 21:03:21 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 01:04:21 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Kamat", "Vishal", ""]]}, {"id": "1505.06549", "submitter": "Weijie Su", "authors": "Lucas Janson and Weijie Su", "title": "Familywise Error Rate Control via Knockoffs", "comments": "15 pages, 3 figures. Updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for controlling the $k$-familywise error rate\n($k$-FWER) in the linear regression setting using the knockoffs framework first\nintroduced by Barber and Cand\\`es. Our procedure, which we also refer to as\nknockoffs, can be applied with any design matrix with at least as many\nobservations as variables, and does not require knowing the noise variance.\nUnlike other multiple testing procedures which act directly on $p$-values,\nknockoffs is specifically tailored to linear regression and implicitly accounts\nfor the statistical relationships between hypothesis tests of different\ncoefficients. We prove that knockoffs controls the $k$-FWER exactly in finite\nsamples and show in simulations that it provides superior power to alternative\nprocedures over a range of linear regression problems. We also discuss\nextensions to controlling other Type I error rates such as the false exceedance\nrate, and use it to identify candidates for mutations conferring\ndrug-resistance in HIV.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 06:28:56 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 03:53:33 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2015 18:13:56 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Janson", "Lucas", ""], ["Su", "Weijie", ""]]}, {"id": "1505.06832", "submitter": "Lilia Costa", "authors": "Lilia Costa, Jim Smith, Thomas Nichols, James Cussens, Eugene P. Duff,\n  Tamar R. Makin", "title": "Searching Multiregression Dynamic Models of Resting-State fMRI Networks\n  Using Integer Programming", "comments": "Published at http://dx.doi.org/10.1214/14-BA913 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 441-478", "doi": "10.1214/14-BA913", "report-no": "VTeX-BA-BA913", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Multiregression Dynamic Model (MDM) is a class of multivariate time series\nthat represents various dynamic causal processes in a graphical way. One of the\nadvantages of this class is that, in contrast to many other Dynamic Bayesian\nNetworks, the hypothesised relationships accommodate conditional conjugate\ninference. We demonstrate for the first time how straightforward it is to\nsearch over all possible connectivity networks with dynamically changing\nintensity of transmission to find the Maximum a Posteriori Probability (MAP)\nmodel within this class. This search method is made feasible by using a novel\napplication of an Integer Programming algorithm. The efficacy of applying this\nparticular class of dynamic models to this domain is shown and more\nspecifically the computational efficiency of a corresponding search of 11-node\nDirected Acyclic Graph (DAG) model space. We proceed to show how diagnostic\nmethods, analogous to those defined for static Bayesian Networks, can be used\nto suggest embellishment of the model class to extend the process of model\nselection. All methods are illustrated using simulated and real resting-state\nfunctional Magnetic Resonance Imaging (fMRI) data.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 07:34:35 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Costa", "Lilia", ""], ["Smith", "Jim", ""], ["Nichols", "Thomas", ""], ["Cussens", "James", ""], ["Duff", "Eugene P.", ""], ["Makin", "Tamar R.", ""]]}, {"id": "1505.06954", "submitter": "Sebastian Kurtek", "authors": "Sebastian Kurtek", "title": "A Geometric Approach to Pairwise Bayesian Alignment of Functional Data\n  Using Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian model for pairwise nonlinear registration of functional\ndata. We use the Riemannian geometry of the space of warping functions to\ndefine appropriate prior distributions and sample from the posterior using\nimportance sampling. A simple square-root transformation is used to simplify\nthe geometry of the space of warping functions, which allows for computation of\nsample statistics, such as the mean and median, and a fast implementation of a\n$k$-means clustering algorithm. These tools allow for efficient posterior\ninference, where multiple modes of the posterior distribution corresponding to\nmultiple plausible alignments of the given functions are found. We also show\npointwise $95\\%$ credible intervals to assess the uncertainty of the alignment\nin different clusters. We validate this model using simulations and present\nmultiple examples on real data from different application domains including\nbiometrics and medicine.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 13:56:46 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 16:06:07 GMT"}, {"version": "v3", "created": "Fri, 3 Feb 2017 22:23:20 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Kurtek", "Sebastian", ""]]}, {"id": "1505.06966", "submitter": "Maria Franco-Villoria", "authors": "Maria Franco-Villoria and Rosaria Ignaccolo", "title": "Bootstrap based uncertainty bands for prediction in functional kriging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing interest in spatially correlated functional data has led to\nthe development of appropriate geostatistical techniques that allow to predict\na curve at an unmonitored location using a functional kriging with external\ndrift model that takes into account the effect of exogenous variables (either\nscalar or functional). Nevertheless uncertainty evaluation for functional\nspatial prediction remains an open issue. We propose a semi-parametric\nbootstrap for spatially correlated functional data that allows to evaluate the\nuncertainty of a predicted curve, ensuring that the spatial dependence\nstructure is maintained in the bootstrap samples. The performance of the\nproposed methodology is assessed via a simulation study. Moreover, the approach\nis illustrated on a well known data set of Canadian temperature and on a real\ndata set of PM$_{10}$ concentration in the Piemonte region, Italy. Based on the\nresults it can be concluded that the method is computationally feasible and\nsuitable for quantifying the uncertainty around a predicted curve.\nSupplementary material including R code is available upon request.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 14:38:48 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 09:48:57 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 12:09:01 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Franco-Villoria", "Maria", ""], ["Ignaccolo", "Rosaria", ""]]}, {"id": "1505.07170", "submitter": "Cesar F. Caiafa", "authors": "Cesar F. Caiafa and Franco Pestilli", "title": "Sparse multiway decomposition for analysis and modeling of diffusion\n  imaging and tractography", "comments": "19 pages, 1 table, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of neuroimaging data sets publicly available is growing at fast\nrate. The increase in availability and resolution of neuroimaging data requires\nmodern approaches to signal processing for data analysis and results\nvalidation. We introduce the application of sparse multiway decomposition\nmethods (Caiafa and Cichocki, 2012) to linearized neuroimaging models. We show\nthat decomposed models are more compact but as accurate as full models and can\nbe successfully used for fast data analysis. We focus as example on a recent\nmodel for the evaluation of white matter connectomes (Pestilli et al, 2014). We\nshow that the multiway decomposed model achieves accuracy comparable to the\nfull model, while requiring only a small fraction of the memory and compute\ntime. The approach has implications for a majority of neuroimaging methods\nusing linear approximations to measured signals.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 01:21:37 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Caiafa", "Cesar F.", ""], ["Pestilli", "Franco", ""]]}, {"id": "1505.07201", "submitter": "Shyam Mohan M", "authors": "M. R. Ananthasayanam, Shyam Mohan M, Naren Naik, R. M. O. Gemson", "title": "A Heuristic Reference Recursive Recipe for the Menacing Problem of\n  Adaptively Tuning the Kalman Filter Statistics. Part-1. Formulation and\n  Simulation Studies", "comments": "arXiv admin note: substantial text overlap with arXiv:1503.04313", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the innovation of the ubiquitous Kalman filter more than five decades\nback it is well known that to obtain the best possible estimates the tuning of\nits statistics $X_0$, $P_0$, $\\Theta$, $R$ and $Q$ namely initial state and\ncovariance, unknown parameters, and the measurement and state noise covariances\nis very crucial. The earlier tweaking and other systematic approaches are\nreviewed but none has reached a simple and easily implementable approach for\nany application. The present reference recursive recipe based on multiple\nfilter passes through the data leads to a converged `statistical equilibrium'\nsolution. It utilizes the pre, post, and smoothed state estimates and their\ncorresponding measurements and the actual measurements as well as their\ncovariances to balance the state and measurement equations and form generalized\ncost functions. The filter covariance at the end of each pass is heuristically\nscaled up by the number of data points and further trimmed to provide the $P_0$\nfor subsequent passes. A simultaneous and proper choice for $Q$ and $R$ based\non the filter sample statistics and certain other covariances leads to a stable\nfilter operation providing the results after few iterations. When only $R$ is\npresent in the data by minimizing the `innovation' cost function using the non\nfilter based Newton Raphson optimization results served as an anchor for\nmatching and tuning the filter statistics. When both $R$ and $Q$ are present in\nthe data the consistency between the injected noise sequences and their\nstatistics provided a simple route and confidence in the present approach. A\ntypical simulation study of a spring, mass, damper system with a weak non\nlinear spring constant shows the present approach out performs earlier\ntechniques. The Part-2 of the paper further consolidates the present approach\nbased on an analysis of real airplane flight test data.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 06:31:15 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Ananthasayanam", "M. R.", ""], ["M", "Shyam Mohan", ""], ["Naik", "Naren", ""], ["Gemson", "R. M. O.", ""]]}, {"id": "1505.07208", "submitter": "Shyam Mohan M", "authors": "Shyam Mohan M, Naren Naik, R. M. O. Gemson, M. R. Ananthasayanam", "title": "A Heuristic Reference Recursive Recipe for the Menacing Problem of\n  Adaptively Tuning the Kalman Filter Statistics. Part-2. Real Data Studies", "comments": "arXiv admin note: substantial text overlap with arXiv:1503.04313", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the previous paper an adaptive filtering based on a reference recursive\nrecipe was developed and tested on a simulated dynamics of a spring, mass, and\ndamper with a weak nonlinear spring. In this paper the above recipe is applied\nto a more involved case of three sets of airplane data which have a larger\nnumber of state, measurements, and unknown parameters. Further the flight tests\ncannot always be conducted in an ideal situation of the process noise and the\nmeasurement noises being white and Gaussian as is generally assumed in the\nKalman filter. The measurements are not available in general with respect to\nthe center of gravity, possess scale and bias factors which will have to be\nmodelled and estimated as well. The coupling between the longitudinal and\nlateral motion brings in added difficulty but makes the problem more\ninteresting. At times the noisy measurements from the longitudinal and lateral\nmotion are input into the longitudinal states. This leads to the resulting\nequations becoming linear with the measurement noise forming the process noise\ninput. At times it turns out that even a parameter that strongly affects the\nairplane dynamics is estimated which vary widely among the approaches. This\nrequires a careful look at the estimates. We also recommend a closer look at\nthe correlation coefficients (that is generally ignored in such studies) of the\nestimated parameters which provide an insight into their subsequent uses. The\npresent recipe has been shown to be better than the earlier approaches in\nestimating the unknowns. In particular the generalized cost functions that are\nintroduced in the present work help to identify definitive results from\ndeceptive results.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 07:02:20 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["M", "Shyam Mohan", ""], ["Naik", "Naren", ""], ["Gemson", "R. M. O.", ""], ["Ananthasayanam", "M. R.", ""]]}, {"id": "1505.07215", "submitter": "Frederic Lavancier", "authors": "Fr\\'ed\\'eric Lavancier and Jesper M{\\o}ller", "title": "Modelling aggregation on the large scale and regularity on the small\n  scale in spatial point pattern datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dependent thinning of a regular point process with the aim of\nobtaining aggregation on the large scale and regularity on the small scale in\nthe resulting target point process of retained points. Various parametric\nmodels for the underlying processes are suggested and the properties of the\ntarget point process are studied. Simulation and inference procedures are\ndiscussed when a realization of the target point process is observed, depending\non whether the thinned points are observed or not. The paper extends previous\nwork by Dietrich Stoyan on interrupted point processes.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 08:14:13 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Lavancier", "Fr\u00e9d\u00e9ric", ""], ["M\u00f8ller", "Jesper", ""]]}, {"id": "1505.07281", "submitter": "Jean-Michel Becu", "authors": "Jean-Michel B\\'ecu (Heudiasyc), Yves Grandvalet (Heudiasyc),\n  Christophe Ambroise (LaMME), Cyril Dalmasso (LaMME)", "title": "Beyond Support in Two-Stage Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous variable selection methods rely on a two-stage procedure, where a\nsparsity-inducing penalty is used in the first stage to predict the support,\nwhich is then conveyed to the second stage for estimation or inference\npurposes. In this framework, the first stage screens variables to find a set of\npossibly relevant variables and the second stage operates on this set of\ncandidate variables, to improve estimation accuracy or to assess the\nuncertainty associated to the selection of variables. We advocate that more\ninformation can be conveyed from the first stage to the second one: we use the\nmagnitude of the coefficients estimated in the first stage to define an\nadaptive penalty that is applied at the second stage. We give two examples of\nprocedures that can benefit from the proposed transfer of information, in\nestimation and inference problems respectively. Extensive simulations\ndemonstrate that this transfer is particularly efficient when each stage\noperates on distinct subsamples. This separation plays a crucial role for the\ncomputation of calibrated p-values, allowing to control the False Discovery\nRate. In this setup, the proposed transfer results in sensitivity gains ranging\nfrom 50% to 100% compared to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 12:14:20 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["B\u00e9cu", "Jean-Michel", "", "Heudiasyc"], ["Grandvalet", "Yves", "", "Heudiasyc"], ["Ambroise", "Christophe", "", "LaMME"], ["Dalmasso", "Cyril", "", "LaMME"]]}, {"id": "1505.07352", "submitter": "Rina Foygel Barber", "authors": "Ang Li and Rina Foygel Barber", "title": "Accumulation tests for FDR control in ordered hypothesis testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple testing problems arising in modern scientific applications can\ninvolve simultaneously testing thousands or even millions of hypotheses, with\nrelatively few true signals. In this paper, we consider the multiple testing\nproblem where prior information is available (for instance, from an earlier\nstudy under different experimental conditions), that can allow us to test the\nhypotheses as a ranked list in order to increase the number of discoveries.\nGiven an ordered list of n hypotheses, the aim is to select a data-dependent\ncutoff k and declare the first k hypotheses to be statistically significant\nwhile bounding the false discovery rate (FDR). Generalizing several existing\nmethods, we develop a family of \"accumulation tests\" to choose a cutoff k that\nadapts to the amount of signal at the top of the ranked list. We introduce a\nnew method in this family, the HingeExp method, which offers higher power to\ndetect true signals compared to existing techniques. Our theoretical results\nprove that these methods control a modified FDR on finite samples, and\ncharacterize the power of the methods in the family. We apply the tests to\nsimulated data, including a high-dimensional model selection problem for linear\nregression. We also compare accumulation tests to existing methods for multiple\ntesting on a real data problem of identifying differential gene expression over\na dosage gradient.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 14:45:06 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 16:03:19 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Li", "Ang", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1505.07369", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Tatsuya Kubokawa", "title": "Heteroscedastic Nested Error Regression Models with Variance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nested error regression model is a useful tool for analyzing clustered\n(grouped) data, and is especially used in small area estimation. The classical\nnested error regression model assumes normality of random effects and error\nterms, and homoscedastic variances. However, these assumptions are often\nviolated in real applications and more flexible models are required. This\narticle proposes a nested error regression model with heteroscedastic\nvariances, where the normality for the underlying distributions is not assumed.\nWe propose the structure of heteroscedastic variances by using some specified\nvariance functions and some covariates with unknown parameters. Under the\nsetting, we construct the moment-type estimators of model parameters and some\nasymptotic properties including asymptotic biases and variances are derived.\nFor predicting linear quantities including random effects, we suggest the\nempirical best linear unbiased predictors and the second-order unbiased\nestimators of mean squared errors are derived in the closed form. We\ninvestigate the proposed method with simulation and empirical studies.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 15:18:17 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 06:55:33 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1505.07414", "submitter": "Lingzhou Xue", "authors": "Jianqing Fan, Lingzhou Xue and Jiawei Yao", "title": "Sufficient Forecasting Using Factor Models", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider forecasting a single time series when there is a large number of\npredictors and a possible nonlinear effect. The dimensionality was first\nreduced via a high-dimensional (approximate) factor model implemented by the\nprincipal component analysis. Using the extracted factors, we develop a novel\nforecasting method called the sufficient forecasting, which provides a set of\nsufficient predictive indices, inferred from high-dimensional predictors, to\ndeliver additional predictive power. The projected principal component analysis\nwill be employed to enhance the accuracy of inferred factors when a\nsemi-parametric (approximate) factor model is assumed. Our method is also\napplicable to cross-sectional sufficient regression using extracted factors.\nThe connection between the sufficient forecasting and the deep learning\narchitecture is explicitly stated. The sufficient forecasting correctly\nestimates projection indices of the underlying factors even in the presence of\na nonparametric forecasting function. The proposed method extends the\nsufficient dimension reduction to high-dimensional regimes by condensing the\ncross-sectional information through factor models. We derive asymptotic\nproperties for the estimate of the central subspace spanned by these projection\ndirections as well as the estimates of the sufficient predictive indices. We\nfurther show that the natural method of running multiple regression of target\non estimated factors yields a linear estimate that actually falls into this\ncentral subspace. Our method and theory allow the number of predictors to be\nlarger than the number of observations. We finally demonstrate that the\nsufficient forecasting improves upon the linear forecasting in both simulation\nstudies and an empirical study of forecasting macroeconomic variables.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 17:46:37 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 23:25:53 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Fan", "Jianqing", ""], ["Xue", "Lingzhou", ""], ["Yao", "Jiawei", ""]]}, {"id": "1505.07415", "submitter": "Bojana Milo\\v{s}evi\\'c", "authors": "Bojana Milo\\v{s}evi\\'c, Marko Obradovi\\'c", "title": "Two-dimensional Kolmogorov-type Goodness-of-fit Tests Based on\n  Characterizations and their Asymptotic Efficiencies", "comments": null, "journal-ref": null, "doi": "10.1080/10485252.2016.1163358", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper new two-dimensional goodness of fit tests are proposed. They\nare of supremum-type and are based on different types of characterizations. For\nthe first time a characterization based on independence of two statistics is\nused for goodness-of-fit testing. The asymptotics of the statistics is studied\nand Bahadur efficiencies of the tests against some close alternatives are\ncalculated. In the process a theorem on large deviations of Kolmogorov-type\nstatistics has been extended to the multidimensional case.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 17:47:41 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "1505.07517", "submitter": "Frederick Campbell", "authors": "Frederick Campbell, Genevera I. Allen", "title": "Within Group Variable Selection through the Exclusive Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sets consist of variables with an inherent group structure. The\nproblem of group selection has been well studied, but in this paper, we seek to\ndo the opposite: our goal is to select at least one variable from each group in\nthe context of predictive regression modeling. This problem is NP-hard, but we\nstudy the tightest convex relaxation: a composite penalty that is a combination\nof the $\\ell_1$ and $\\ell_2$ norms. Our so-called Exclusive Lasso method\nperforms structured variable selection by ensuring that at least one variable\nis selected from each group. We study our method's statistical properties and\ndevelop computationally scalable algorithms for fitting the Exclusive Lasso. We\nstudy the effectiveness of our method via simulations as well as using NMR\nspectroscopy data. Here, we use the Exclusive Lasso to select the appropriate\nchemical shift from a dictionary of possible chemical shifts for each molecule\nin the biological sample.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 00:42:29 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Campbell", "Frederick", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1505.07541", "submitter": "Kobayashi Genya Mr.", "authors": "Genya Kobayashi", "title": "Bayesian Endogenous Tobit Quantile Regression", "comments": null, "journal-ref": null, "doi": "10.1214/16-BA996", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes $p$-th Tobit quantile regression models with endogenous\nvariables. In the first stage regression of the endogenous variable on the\nexogenous variables, the assumption that the $\\alpha$-th quantile of the error\nterm is zero is introduced. Then, the residual of this regression model is\nincluded in the $p$-th quantile regression model in such a way that the $p$-th\nconditional quantile of the new error term is zero. The error distribution of\nthe first stage regression is modelled around the zero $\\alpha$-th quantile\nassumption by using parametric and semiparametric approaches. Since the value\nof $\\alpha$ is a priori unknown, it is treated as an additional parameter and\nis estimated from the data. The proposed models are then demonstrated by using\nsimulated data and real data on the labour supply of married women.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 03:42:48 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 07:38:34 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2015 08:42:51 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Kobayashi", "Genya", ""]]}, {"id": "1505.07710", "submitter": "Edward Roualdes", "authors": "Edward A. Roualdes", "title": "Bayesian Trend Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully Bayesian hierarchical model for trend filtering, itself a\nnew development in nonparametric, univariate regression. The framework more\nbroadly applies to the generalized lasso, but focus is on Bayesian trend\nfiltering. We compare two shrinkage priors, double exponential and generalized\ndouble Pareto. A simulation study, comparing Bayesian trend filtering to the\noriginal formulation and a number of other popular methods shows our method to\nimprove estimation error while maintaining if not improving coverage\nprobability. Two time series data sets demonstrate Bayesian trend filtering's\nrobustness to possible violations of its assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 14:58:35 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Roualdes", "Edward A.", ""]]}, {"id": "1505.07752", "submitter": "Chitta Ranjan", "authors": "Chitta Ranjan, Kamran Paynabar, Jonathan E. Helm and Julian Pan", "title": "The Impact of Estimation: A New Method for Clustering and Trajectory\n  Estimation in Patient Flow Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately forecast and control inpatient census, and thereby\nworkloads, is a critical and longstanding problem in hospital management.\nMajority of current literature focuses on optimal scheduling of inpatients, but\nlargely ignores the process of accurate estimation of the trajectory of\npatients throughout the treatment and recovery process. The result is that\ncurrent scheduling models are optimizing based on inaccurate input data. We\ndeveloped a Clustering and Scheduling Integrated (CSI) approach to capture\npatient flows through a network of hospital services. CSI functions by\nclustering patients into groups based on similarity of trajectory using a novel\nSemi-Markov model (SMM)-based clustering scheme proposed in this paper, as\nopposed to clustering by admit type or condition as in previous literature. The\nmethodology is validated by simulation and then applied to real patient data\nfrom a partner hospital where we see it outperforms current methods. Further,\nwe demonstrate that extant optimization methods achieve significantly better\nresults on key hospital performance measures under CSI, compared with\ntraditional estimation approaches, increasing elective admissions by 97% and\nutilization by 22% compared to 30% and 8% using traditional estimation\ntechniques. From a theoretical standpoint, the SMM-clustering is a novel\napproach applicable to any temporal-spatial stochastic data that is prevalent\nin many industries and application areas.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 16:47:57 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 14:13:03 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2015 19:16:52 GMT"}, {"version": "v4", "created": "Tue, 12 Jul 2016 16:36:24 GMT"}, {"version": "v5", "created": "Wed, 19 Oct 2016 03:59:24 GMT"}, {"version": "v6", "created": "Wed, 11 Jan 2017 01:17:26 GMT"}, {"version": "v7", "created": "Mon, 30 Jan 2017 02:26:23 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Ranjan", "Chitta", ""], ["Paynabar", "Kamran", ""], ["Helm", "Jonathan E.", ""], ["Pan", "Julian", ""]]}, {"id": "1505.07798", "submitter": "Maria Terres", "authors": "Maria A. Terres, Montserrat Fuentes, Dean Hesterberg, Matthew\n  Polizzotto", "title": "Bayesian Spectral Modeling of Microscale Spatial Distributions in a\n  Multivariate Soil Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological advances have enabled researchers in a variety of fields\nto collect accurately geocoded data for several variables simultaneously. In\nmany cases it may be most appropriate to jointly model these multivariate\nspatial processes without constraints on their conditional relationships. When\ndata have been collected on a regular lattice, the multivariate conditionally\nautoregressive (MCAR) models are a common choice. However, inference from these\nMCAR models relies heavily on the pre-specified neighborhood structure and\noften assumes a separable covariance structure. Here, we present a multivariate\nspatial model using a spectral analysis approach that enables inference on the\nconditional relationships between the variables that does not rely on a\npre-specified neighborhood structure, is non-separable, and is computationally\nefficient. Covariance and cross-covariance functions are defined in the\nspectral domain to obtain computational efficiency. Posterior inference on the\ncorrelation matrix allows for quantification of the conditional dependencies.\nThe approach is illustrated for the toxic element arsenic and four other soil\nelements whose relative concentrations were measured on a spatial lattice.\nUnderstanding conditional relationships between arsenic and other soil elements\nprovides insights for mitigating poisoning in southern Asia and elsewhere.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 19:00:05 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Terres", "Maria A.", ""], ["Fuentes", "Montserrat", ""], ["Hesterberg", "Dean", ""], ["Polizzotto", "Matthew", ""]]}, {"id": "1505.07917", "submitter": "Qiwei Li", "authors": "Qiwei Li", "title": "Oracally Efficient Estimation of Functional-Coefficient Autoregressive\n  Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1502.03486 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear autoregressive models are very useful for modeling many natural\nprocesses, however, the size of the class of these models is large.\nFunctional-coefficient autoregressive models (FCAR) are useful structures for\nreducing the size of the class of these models. Although this structure reduces\nthe class of nonlinear models, it is broad enough to include some common time\nseries models as specific cases. A recent development in estimating nonlinear\ntime series data is the spline backfitted kernel (SBK) method. This method\ncombines the computational speed of splines with the asymptotic properties of\nkernel smoothing. To estimate a component function in the model, all other\ncomponent functions are pre-estimated with splines and then the difference is\ntaken of the observed time series and the pre-estimates. This difference is\nthen used as pseudo-responses for which kernel smoothing is used to estimate\nthe function of interest. By constructing the estimates in this way, the method\ndoes not suffer from the curse of dimensionality. In this paper, we adapt the\nSBK method to FCAR models.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 04:25:46 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Li", "Qiwei", ""]]}, {"id": "1505.07925", "submitter": "Yun Yang", "authors": "Yun Yang, Martin J. Wainwright, Michael I. Jordan", "title": "On the Computational Complexity of High-Dimensional Bayesian Variable\n  Selection", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of Markov chain Monte Carlo (MCMC)\nmethods for high-dimensional Bayesian linear regression under sparsity\nconstraints. We first show that a Bayesian approach can achieve\nvariable-selection consistency under relatively mild conditions on the design\nmatrix. We then demonstrate that the statistical criterion of posterior\nconcentration need not imply the computational desideratum of rapid mixing of\nthe MCMC algorithm. By introducing a truncated sparsity prior for variable\nselection, we provide a set of conditions that guarantee both\nvariable-selection consistency and rapid mixing of a particular\nMetropolis-Hastings algorithm. The mixing time is linear in the number of\ncovariates up to a logarithmic factor. Our proof controls the spectral gap of\nthe Markov chain by constructing a canonical path ensemble that is inspired by\nthe steps taken by greedy algorithms for variable selection.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 05:33:22 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Yang", "Yun", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1505.08113", "submitter": "Christophe Ley", "authors": "Toshihiro Abe and Christophe Ley", "title": "A tractable, parsimonious and flexible model for cylindrical data, with\n  applications", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose cylindrical distributions obtained by combining the\nsine-skewed von Mises distribution (circular part) with the Weibull\ndistribution (linear part). This new model, the WeiSSVM, enjoys numerous\nadvantages: simple normalizing constant and hence very tractable density,\nparameter-parsimony and interpretability, good circular-linear dependence\nstructure, easy random number generation thanks to known marginal/conditional\ndistributions, flexibility illustrated via excellent fitting abilities, and a\nstraightforward extension to the case of directional-linear data. Inferential\nissues, such as independence testing, circular-linear respectively\nlinear-circular regression, can easily be tackled with our model, which we\napply on two real data sets. We conclude the paper by discussing future\napplications of our model.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 17:01:29 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 11:23:36 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Abe", "Toshihiro", ""], ["Ley", "Christophe", ""]]}, {"id": "1505.08116", "submitter": "Vincent Audigier", "authors": "Vincent Audigier, Fran\\c{c}ois Husson and Julie Josse", "title": "MIMCA: Multiple imputation for categorical variables with multiple\n  correspondence analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiple imputation method to deal with incomplete categorical\ndata. This method imputes the missing entries using the principal components\nmethod dedicated to categorical data: multiple correspondence analysis (MCA).\nThe uncertainty concerning the parameters of the imputation model is reflected\nusing a non-parametric bootstrap. Multiple imputation using MCA (MIMCA)\nrequires estimating a small number of parameters due to the dimensionality\nreduction property of MCA. It allows the user to impute a large range of data\nsets. In particular, a high number of categories per variable, a high number of\nvariables or a small the number of individuals are not an issue for MIMCA.\nThrough a simulation study based on real data sets, the method is assessed and\ncompared to the reference methods (multiple imputation using the loglinear\nmodel, multiple imputation by logistic regressions) as well to the latest works\non the topic (multiple imputation by random forests or by the Dirichlet process\nmixture of products of multinomial distributions model). The proposed method\nshows good performances in terms of bias and coverage for an analysis model\nsuch as a main effects logistic regression model. In addition, MIMCA has the\ngreat advantage that it is substantially less time consuming on data sets of\nhigh dimensions than the other multiple imputation methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 17:36:11 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Audigier", "Vincent", ""], ["Husson", "Fran\u00e7ois", ""], ["Josse", "Julie", ""]]}]