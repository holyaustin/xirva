[{"id": "1708.00040", "submitter": "Arikatla Satyanarayana Reddy", "authors": "Basheeruddin Shah Shaik, Vijay Kumar Chakka, Srikanth Goli,\n  A.Satyanarayana Reddy", "title": "Removal of Narrowband Interference (PLI in ECG Signal) Using Ramanujan\n  Periodic Transform (RPT)", "comments": "IEEE explore: Conference: International Conference on Signal\n  processing and Communication ( ICSC), At NOIDA, ISBN: 978-1-5090-2684-5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppression of interference from narrowband frequency signals play vital role\nin many signal processing and communication applications. A transform based\nmethod for suppression of narrow band interference in a biomedical signal is\nproposed. As a specific example Electrocardiogram (ECG) is considered for the\nanalysis. ECG is one of the widely used biomedical signal. ECG signal is often\ncontaminated with baseline wander noise, powerline interference (PLI) and\nartifacts (bioelectric signals), which complicates the processing of raw ECG\nsignal. This work proposes an approach using Ramanujan periodic transform for\nreducing PLI and is tested on a subject data from MIT-BIH Arrhythmia database.\nA sum ($E$) of Euclidean error per block ($e_i$) is used as measure to quantify\nthe suppression capability of RPT and notch filter based methods. The\ntransformation is performed for different lengths ($N$), namely $36$, $72$,\n$108$, $144$, $180$. Every doubling of $N$-points results in $50{\\%}$ reduction\nin error ($E$).\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 18:13:34 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Shaik", "Basheeruddin Shah", ""], ["Chakka", "Vijay Kumar", ""], ["Goli", "Srikanth", ""], ["Reddy", "A. Satyanarayana", ""]]}, {"id": "1708.00082", "submitter": "R\\'emi Bardenet", "authors": "R\\'emi Bardenet, Julien Flamant, Pierre Chainais", "title": "On the zeros of the spectrogram of white noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, Flandrin [2015] has proposed filtering based on the zeros\nof a spectrogram, using the short-time Fourier transform and a Gaussian window.\nHis results are based on empirical observations on the distribution of the\nzeros of the spectrogram of white Gaussian noise. These zeros tend to be\nuniformly spread over the time-frequency plane, and not to clutter. Our\ncontributions are threefold: we rigorously define the zeros of the spectrogram\nof continuous white Gaussian noise, we explicitly characterize their\nstatistical distribution, and we investigate the computational and statistical\nunderpinnings of the practical implementation of signal detection based on the\nstatistics of spectrogram zeros. In particular, we stress that the zeros of\nspectrograms of white Gaussian noise correspond to zeros of Gaussian analytic\nfunctions, a topic of recent independent mathematical interest [Hough et al.,\n2009].\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 22:03:57 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Flamant", "Julien", ""], ["Chainais", "Pierre", ""]]}, {"id": "1708.00085", "submitter": "Veronika Rockova", "authors": "Veronika Rockova and Kenichiro McAlinn", "title": "Dynamic Variable Selection with Spike-and-Slab Process Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of dynamic variable selection in time series\nregression with unknown residual variances, where the set of active predictors\nis allowed to evolve over time. To capture time-varying variable selection\nuncertainty, we introduce new dynamic shrinkage priors for the time series of\nregression coefficients. These priors are characterized by two main\ningredients: smooth parameter evolutions and intermittent zeroes for modeling\npredictive breaks. More formally, our proposed Dynamic Spike-and-Slab (DSS)\npriors are constructed as mixtures of two processes: a spike process for the\nirrelevant coefficients and a slab autoregressive process for the active\ncoefficients. The mixing weights are themselves time-varying and depend on\nlagged values of the series. Our DSS priors are probabilistically coherent in\nthe sense that their stationary distribution is fully known and characterized\nby spike-and-slab marginals. For posterior sampling over dynamic regression\ncoefficients, model selection indicators as well as unknown dynamic residual\nvariances, we propose a Dynamic SSVS algorithm based on forward-filtering and\nbackward-sampling. To scale our method to large data sets, we develop a Dynamic\nEMVS algorithm for MAP smoothing. We demonstrate, through simulation and a\ntopical macroeconomic dataset, that DSS priors are very effective at separating\nactive and noisy coefficients. Our fast implementation significantly extends\nthe reach of spike-and-slab methods to large time series data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 22:09:58 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 21:34:17 GMT"}, {"version": "v3", "created": "Sat, 21 Sep 2019 15:19:39 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Rockova", "Veronika", ""], ["McAlinn", "Kenichiro", ""]]}, {"id": "1708.00099", "submitter": "Leonardo Egidi", "authors": "Leonardo Egidi, Francesco Pauli, Nicola Torelli", "title": "Mixture Data-Dependent Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-component mixture of a noninformative (diffuse) and an\ninformative prior distribution, weighted through the data in such a way to\nprefer the first component if a prior-data conflict arises. The data-driven\napproach for computing the mixture weights makes this class data-dependent.\nAlthough rarely used with any theoretical motivation, data-dependent priors are\noften used for different reasons, and their use has been a lot debated over the\nlast decades. However, our approach is justified in terms of Bayesian inference\nas an approximation of a hierarchical model and as a conditioning on a data\nstatistic. This class of priors turns out to provide less information than an\ninformative prior, perhaps it represents a suitable option for not dominating\nthe inference in presence of small samples. First evidences from simulation\nstudies show that this class could also be a good proposal for reducing mean\nsquared errors.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 23:32:50 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Egidi", "Leonardo", ""], ["Pauli", "Francesco", ""], ["Torelli", "Nicola", ""]]}, {"id": "1708.00145", "submitter": "Rohit Patra", "authors": "Arun K. Kuchibhotla, Rohit K. Patra, and Bodhisattva Sen", "title": "Semiparametric Efficiency in Convexity Constrained Single Index Model", "comments": "Removed the density bounded away from zero assumption in assumption\n  (A5). Weakened assumption (B2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation and inference in a single index regression model with\nan unknown convex link function. We introduce a convex and Lipschitz\nconstrained least squares estimator (CLSE) for both the parametric and the\nnonparametric components given independent and identically distributed\nobservations. We prove the consistency and find the rates of convergence of the\nCLSE when the errors are assumed to have only $q \\ge 2$ moments and are allowed\nto depend on the covariates. When $q\\ge 5$, we establish $n^{-1/2}$-rate of\nconvergence and asymptotic normality of the estimator of the parametric\ncomponent. Moreover, the CLSE is proved to be semiparametrically efficient if\nthe errors happen to be homoscedastic. {We develop and implement a numerically\nstable and computationally fast algorithm to compute our proposed estimator in\nthe R package~\\texttt{simest}}. We illustrate our methodology through extensive\nsimulations and data analysis. Finally, our proof of efficiency is geometric\nand provides a general framework that can be used to prove efficiency of\nestimators in a wide variety of semiparametric models even when they do not\nsatisfy the efficient score equation directly.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 03:21:11 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 20:14:16 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 19:15:13 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 19:39:24 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Patra", "Rohit K.", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1708.00205", "submitter": "Binyan Jiang", "authors": "Binyan Jiang, Ziqi Chen, Chenlei Leng", "title": "Dynamic Linear Discriminant Analysis in High Dimensional Space", "comments": "34 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data that evolve dynamically feature predominantly in the\nmodern data era. As a partial response to this, recent years have seen\nincreasing emphasis to address the dimensionality challenge. However, the\nnon-static nature of these datasets is largely ignored. This paper addresses\nboth challenges by proposing a novel yet simple dynamic linear programming\ndiscriminant (DLPD) rule for binary classification. Different from the usual\nstatic linear discriminant analysis, the new method is able to capture the\nchanging distributions of the underlying populations by modeling their means\nand covariances as smooth functions of covariates of interest. Under an\napproximate sparse condition, we show that the conditional misclassification\nrate of the DLPD rule converges to the Bayes risk in probability uniformly over\nthe range of the variables used for modeling the dynamics, when the\ndimensionality is allowed to grow exponentially with the sample size. The\nminimax lower bound of the estimation of the Bayes risk is also established,\nimplying that the misclassification rate of our proposed rule is minimax-rate\noptimal. The promising performance of the DLPD rule is illustrated via\nextensive simulation studies and the analysis of a breast cancer dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 08:49:13 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 02:28:44 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 03:27:43 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Jiang", "Binyan", ""], ["Chen", "Ziqi", ""], ["Leng", "Chenlei", ""]]}, {"id": "1708.00272", "submitter": "Jessica Rees Ms", "authors": "Jessica M. B. Rees, Angela Wood and Stephen Burgess", "title": "Extending the MR-Egger method for multivariable Mendelian randomization\n  to correct for both measured and unmeasured pleiotropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods have been developed for Mendelian randomization that can obtain\nconsistent causal estimates while relaxing the instrumental variable\nassumptions. These include multivariable Mendelian randomization, in which a\ngenetic variant may be associated with multiple risk factors so long as any\nassociation with the outcome is via the measured risk factors (measured\npleiotropy), and the MR-Egger (Mendelian randomization-Egger) method, in which\na genetic variant may be directly associated with the outcome not via the risk\nfactor of interest, so long as the direct effects of the variants on the\noutcome are uncorrelated with their associations with the risk factor\n(unmeasured pleiotropy). In this paper, we extend the MR-Egger method to a\nmultivariable setting to correct for both measured and unmeasured pleiotropy.\nWe show, through theoretical arguments and a simulation study, that the\nmultivariable MR-Egger method has advantages over its univariable counterpart\nin terms of plausibility of the assumption needed for consistent causal\nestimation, and power to detect a causal effect when this assumption is\nsatisfied. The methods are compared in an applied analysis to investigate the\ncausal effect of high-density lipoprotein cholesterol on coronary heart disease\nrisk. The multivariable MR-Egger method will be useful to analyse\nhigh-dimensional data in situations where the risk factors are highly related\nand it is difficult to find genetic variants specifically associated with the\nrisk factor of interest (multivariable by design), and as a sensitivity\nanalysis when the genetic variants are known to have pleiotropic effects on\nmeasured risk factors.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 12:14:05 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Rees", "Jessica M. B.", ""], ["Wood", "Angela", ""], ["Burgess", "Stephen", ""]]}, {"id": "1708.00386", "submitter": "Andrea Ghiglietti", "authors": "Andrea Martino, Andrea Ghiglietti, Francesca Ieva, Anna M. Paganoni", "title": "A k-means procedure based on a Mahalanobis type distance for clustering\n  multivariate functional data", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a clustering procedure for samples of multivariate\nfunctions in $(L^2(I))^{J}$, with $J\\geq1$. This method is based on a k-means\nalgorithm in which the distance between the curves is measured with a metrics\nthat generalizes the Mahalanobis distance in Hilbert spaces, considering the\ncorrelation and the variability along all the components of the functional\ndata. The proposed procedure has been studied in simulation and compared with\nthe k-means based on other distances typically adopted for clustering\nmultivariate functional data. In these simulations, it is shown that the\nk-means algorithm with the generalized Mahalanobis distance provides the best\nclustering performances, both in terms of mean and standard deviation of the\nnumber of misclassified curves. Finally, the proposed method has been applied\nto two real cases studies, concerning ECG signals and growth curves, where the\nresults obtained in simulation are confirmed and strengthened.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 15:17:02 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Martino", "Andrea", ""], ["Ghiglietti", "Andrea", ""], ["Ieva", "Francesca", ""], ["Paganoni", "Anna M.", ""]]}, {"id": "1708.00427", "submitter": "Jing Lei", "authors": "Jing Lei", "title": "Fast Exact Conformalization of Lasso using Piecewise Linear Homotopy", "comments": "24 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a general method that converts almost any point\npredictor to a prediction set. The resulting set keeps good statistical\nproperties of the original estimator under standard assumptions, and guarantees\nvalid average coverage even when the model is misspecified. A main challenge in\napplying conformal prediction in modern applications is efficient computation,\nas it generally requires an exhaustive search over the entire output space. In\nthis paper we develop an exact and computationally efficient conformalization\nof the Lasso and elastic net. The method makes use of a novel piecewise linear\nhomotopy of the Lasso solution under perturbation of a single input sample\npoint. As a by-product, we provide a simpler and better justified online Lasso\nalgorithm, which may be of independent interest. Our derivation also reveals an\ninteresting accuracy-stability trade-off in conformal inference, which is\nanalogous to the bias-variance trade-off in traditional parameter estimation.\nThe practical performance of the new algorithm is demonstrated using both\nsynthetic and real data examples.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 17:29:56 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Lei", "Jing", ""]]}, {"id": "1708.00430", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Breaking the curse of dimensionality in regression", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with many signals, high-dimensional models, often impose structures on\nthe signal strengths. The common assumption is that only a few signals are\nstrong and most of the signals are zero or close (collectively) to zero.\nHowever, such a requirement might not be valid in many real-life applications.\nIn this article, we are interested in conducting large-scale inference in\nmodels that might have signals of mixed strengths. The key challenge is that\nthe signals that are not under testing might be collectively non-negligible\n(although individually small) and cannot be accurately learned. This article\ndevelops a new class of tests that arise from a moment matching formulation. A\nvirtue of these moment-matching statistics is their ability to borrow strength\nacross features, adapt to the sparsity size and exert adjustment for testing\ngrowing number of hypothesis. GRoup-level Inference of Parameter, GRIP, test\nharvests effective sparsity structures with hypothesis formulation for an\nefficient multiple testing procedure. Simulated data showcase that GRIPs error\ncontrol is far better than the alternative methods. We develop a minimax\ntheory, demonstrating optimality of GRIP for a broad range of models, including\nthose where the model is a mixture of a sparse and high-dimensional dense\nsignals.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 17:39:00 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1708.00476", "submitter": "Luis Enrique Benites S\\'anchez", "authors": "Luis Benites, Roc\\'io Maehara, Filidor Vilca, Fernando Marmolejo-Ramos", "title": "Finite Mixture of Birnbaum-Saunders distributions using the $k$ bumps\n  algorithm", "comments": "23 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models have received a great deal of attention in statistics due to\nthe wide range of applications found in recent years. This paper discusses a\nfinite mixture model of Birnbaum- Saunders distributions with G components, as\nan important supplement of the work developed by Balakrishnan et al. (2011),\nwho only considered two components. Our proposal enables the modeling of proper\nmultimodal scenarios with greater flexibility, where the identifiability of the\nmodel with G components is proven and an EM-algorithm for the maximum\nlikelihood (ML) estimation of the mixture parameters is developed, in which the\nk-bumps algorithm is used as an initialization strategy in the EM algorithm.\nThe performance of the k-bumps algorithm as an initialization tool is evaluated\nthrough simulation experiments. Moreover, the empirical information matrix is\nderived analytically to account for standard error, and bootstrap procedures\nfor testing hypotheses about the number of components in the mixture are\nimplemented. Finally, we perform simulation studies and analyze two real\ndatasets to illustrate the usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 19:01:46 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Benites", "Luis", ""], ["Maehara", "Roc\u00edo", ""], ["Vilca", "Filidor", ""], ["Marmolejo-Ramos", "Fernando", ""]]}, {"id": "1708.00535", "submitter": "William Brown", "authors": "William A. Brown", "title": "They don't look different, but they're not the same: formal resemblance\n  and interpretive disparity in the construction of temporal frequency\n  distributions", "comments": "38 pages, 7 figures, 33 numbered equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In archaeological and paleontological demographic temporal frequency analysis\n(dTFA), a handful of protocols for generating temporal frequency distributions\n(tfds) have emerged based on the aggregation not of single-point timestamps but\ninstead of constituent temporal distributions, including probability summation,\nkernel density estimation, and human occupation index calculation. While these\nprotocols bear a striking algebraic resemblance to one another, they are\nmotivated by the desire to contain fundamentally different sources of\nuncertainty, leading to detailed differences in procedure as well as\nfundamental differences in the interpretation of the resulting tfd. Rather than\nassuming that one technique can fulfil dual purposes based on its formal\nresemblance with another, the joint containment of multiple sources of\nuncertainty therefore warrants the adoption of propagation-of-uncertainty\ntechniques in tfd construction.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 22:02:12 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Brown", "William A.", ""]]}, {"id": "1708.00689", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Dirichlet Bayesian Network Scores and the Maximum Relative Entropy\n  Principle", "comments": "20 pages, 4 figures; extended version submitted to Behaviormetrika", "journal-ref": "Journal of Machine Learning Research (73, Proceedings Track, AMBN\n  2017), 8-20; extended version in Behaviormetrika, 45(2), 337-362", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic approach for learning Bayesian networks from data is to identify a\nmaximum a posteriori (MAP) network structure. In the case of discrete Bayesian\nnetworks, MAP networks are selected by maximising one of several possible\nBayesian Dirichlet (BD) scores; the most famous is the Bayesian Dirichlet\nequivalent uniform (BDeu) score from Heckerman et al (1995). The key properties\nof BDeu arise from its uniform prior over the parameters of each local\ndistribution in the network, which makes structure learning computationally\nefficient; it does not require the elicitation of prior knowledge from experts;\nand it satisfies score equivalence.\n  In this paper we will review the derivation and the properties of BD scores,\nand of BDeu in particular, and we will link them to the corresponding entropy\nestimates to study them from an information theoretic perspective. To this end,\nwe will work in the context of the foundational work of Giffin and Caticha\n(2007), who showed that Bayesian inference can be framed as a particular case\nof the maximum relative entropy principle. We will use this connection to show\nthat BDeu should not be used for structure learning from sparse data, since it\nviolates the maximum relative entropy principle; and that it is also\nproblematic from a more classic Bayesian model selection perspective, because\nit produces Bayes factors that are sensitive to the value of its only\nhyperparameter. Using a large simulation study, we found in our previous work\n(Scutari, 2016) that the Bayesian Dirichlet sparse (BDs) score seems to provide\nbetter accuracy in structure learning; in this paper we further show that BDs\ndoes not suffer from the issues above, and we recommend to use it for sparse\ndata instead of BDeu. Finally, will show that these issues are in fact\ndifferent aspects of the same problem and a consequence of the distributional\nassumptions of the prior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 10:30:21 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 21:29:36 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 22:28:50 GMT"}, {"version": "v4", "created": "Thu, 25 Jan 2018 09:27:57 GMT"}, {"version": "v5", "created": "Mon, 5 Mar 2018 11:34:07 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1708.00907", "submitter": "Adam Sales", "authors": "Adam C. Sales", "title": "Sequential Specification Tests to Choose a Model: A Change-Point\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers faced with a sequence of candidate model specifications must\noften choose the best specification that does not violate a testable\nidentification assumption. One option in this scenario is sequential\nspecification tests: hypothesis tests of the identification assumption over the\nsequence. Borrowing an idea from the change-point literature, this paper shows\nhow to use the distribution of p-values from sequential specification tests to\nestimate the point in the sequence where the identification assumption ceases\nto hold. Unlike current approaches, this method is robust to individual errant\np-values and does not require choosing a test level or tuning parameter. This\npaper demonstrates the method's properties with a simulation study, and\nillustrates it by application to the problems of choosing a bandwidth in a\nregression discontinuity design while maintaining covariate balance and of\nchoosing a lag order for a time series model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 19:47:54 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Sales", "Adam C.", ""]]}, {"id": "1708.00942", "submitter": "Fang Wan", "authors": "Fang Wan, Andrew C. Titman and Thomas F. Jaki", "title": "Subgroup analysis of treatment effects for misclassified biomarkers with\n  time-to-event data", "comments": "16 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysing subgroups defined by biomarkers is of increasing importance in\nclinical research. In some situations the biomarker is subject to\nmisclassification error, meaning the true subgroups are identified with\nimperfect sensitivity and specificity. For time-to-event data, it is improper\nto assume the Cox proportional hazards model for the effects with respect to\nthe true subgroups, since the survival distributions with respect to the\ndiagnosed subgroups will not adhere to the proportional hazards assumption.\nThis precludes the possibility of using simple adjustment procedures. Instead,\nwe present a method based on formally modelling the data as a mixture of Cox\nmodels using an EM algorithm for estimation. An estimate of the overall\npopulation treatment effect is obtained through the interpretation of the\nhazard ratio as a concordance odds. Profile likelihood is used to construct\nindividual and simultaneous confidence intervals of treatment effects. The\nresulting confidence intervals are shown to have close to nominal coverage for\nmoderately large sample sizes in simulations and the method is illustrated on\ndata from a renal-cell cancer trial.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 21:48:18 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 14:32:35 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Wan", "Fang", ""], ["Titman", "Andrew C.", ""], ["Jaki", "Thomas F.", ""]]}, {"id": "1708.00955", "submitter": "Matias Quiroz", "authors": "Khue-Dung Dang, Matias Quiroz, Robert Kohn, Minh-Ngoc Tran, Mattias\n  Villani", "title": "Hamiltonian Monte Carlo with Energy Conserving Subsampling", "comments": "Includes an experiment on the scalability of the method. Text has\n  been revised too", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional\nposterior distributions with proposed parameter draws obtained by iterating on\na discretized version of the Hamiltonian dynamics. The iterations make HMC\ncomputationally costly, especially in problems with large datasets, since it is\nnecessary to compute posterior densities and their derivatives with respect to\nthe parameters. Naively computing the Hamiltonian dynamics on a subset of the\ndata causes HMC to lose its key ability to generate distant parameter proposals\nwith high acceptance probability. The key insight in our article is that\nefficient subsampling HMC for the parameters is possible if both the dynamics\nand the acceptance probability are computed from the same data subsample in\neach complete HMC iteration. We show that this is possible to do in a\nprincipled way in a HMC-within-Gibbs framework where the subsample is updated\nusing a pseudo marginal MH step and the parameters are then updated using an\nHMC step, based on the current subsample. We show that our subsampling methods\nare fast and compare favorably to two popular sampling algorithms that utilize\ngradient estimates from data subsampling. We also explore the current\nlimitations of subsampling HMC algorithms by varying the quality of the\nvariance reducing control variates used in the estimators of the posterior\ndensity and its gradients.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 23:19:07 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 01:33:52 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 00:02:16 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Dang", "Khue-Dung", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""], ["Villani", "Mattias", ""]]}, {"id": "1708.01151", "submitter": "Benjamin Frot", "authors": "Benjamin Frot, Preetam Nandy, Marloes H. Maathuis", "title": "Robust causal structure learning with some hidden variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method to estimate the Markov equivalence class of a\ndirected acyclic graph (DAG) in the presence of hidden variables, in settings\nwhere the underlying DAG among the observed variables is sparse, and there are\na few hidden variables that have a direct effect on many of the observed ones.\nBuilding on the so-called low rank plus sparse framework, we suggest a\ntwo-stage approach which first removes the effect of the hidden variables, and\nthen estimates the Markov equivalence class of the underlying DAG under the\nassumption that there are no remaining hidden variables. This approach is\nconsistent in certain high-dimensional regimes and performs favourably when\ncompared to the state of the art, both in terms of graphical structure recovery\nand total causal effect estimation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 14:10:34 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 08:28:32 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Frot", "Benjamin", ""], ["Nandy", "Preetam", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1708.01193", "submitter": "Jeremy Oakley", "authors": "Shijie Ren, Jeremy E. Oakley, John W. Stevens", "title": "Incorporating genuine prior information about between-study\n  heterogeneity in random effects pairwise and network meta-analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Pairwise and network meta-analyses using fixed effect and random\neffects models are commonly applied to synthesise evidence from randomised\ncontrolled trials. The models differ in their assumptions and the\ninterpretation of the results. The model choice depends on the objective of the\nanalysis and knowledge of the included studies. Fixed effect models are often\nused because there are too few studies with which to estimate the between-study\nstandard deviation from the data alone. Objectives: The aim is to propose a\nframework for eliciting an informative prior distribution for the between-study\nstandard deviation in a Bayesian random effects meta-analysis model to\ngenuinely represent heterogeneity when data are sparse. Methods: We developed\nan elicitation method using external information such as empirical evidence and\nexperts' beliefs on the 'range' of treatment effects in order to infer the\nprior distribution for the between-study standard deviation. We also developed\nthe method to be implemented in R. Results: The three-stage elicitation\napproach allows uncertainty to be represented by a genuine prior distribution\nto avoid making misleading inferences. It is flexible to what judgments an\nexpert can provide, and is applicable to all types of outcome measure for which\na treatment effect can be constructed on an additive scale. Conclusions: The\nchoice between using a fixed effect or random effects meta-analysis model\ndepends on the inferences required and not on the number of available studies.\nOur elicitation framework captures external evidence about heterogeneity and\novercomes the often implausible assumption that studies are estimating the same\ntreatment effect, thereby improving the quality of inferences in decision\nmaking.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 16:17:26 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Ren", "Shijie", ""], ["Oakley", "Jeremy E.", ""], ["Stevens", "John W.", ""]]}, {"id": "1708.01481", "submitter": "Daniel Eck", "authors": "Daniel J. Eck, Christopher J. Nachtsheim, R. Dennis Cook, and Thomas\n  A. Albrecht", "title": "Multivariate Design of Experiments for Engineering Dimensional Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of dimensional analysis experiments when there is more\nthan a single response. We first give a brief overview of dimensional analysis\nexperiments and the dimensional analysis (DA) procedure. The validity of the DA\nmethod for univariate responses was established by the Buckingham $\\Pi$-Theorem\nin the early 20th century. We extend the theorem to the multivariate case,\ndevelop basic criteria for multivariate design of DA and give guidelines for\ndesign construction. Finally, we illustrate the construction of designs for DA\nexperiments for an example involving the design of a heat exchanger.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 13:09:31 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 20:38:34 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Eck", "Daniel J.", ""], ["Nachtsheim", "Christopher J.", ""], ["Cook", "R. Dennis", ""], ["Albrecht", "Thomas A.", ""]]}, {"id": "1708.01550", "submitter": "Thomas Ortner", "authors": "Thomas Ortner, Peter Filzmoser, Maia Zaharieva, Sarka Brodinova and\n  Christian Breiteneder", "title": "Local projections for high-dimensional outlier detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for outlier detection, called\nlocal projections, which is based on concepts of Local Outlier Factor (LOF)\n(Breunig et al., 2000) and RobPCA (Hubert et al., 2005). By using aspects of\nboth methods, our algorithm is robust towards noise variables and is capable of\nperforming outlier detection in multi-group situations. We are further not\nreliant on a specific underlying data distribution.\n  For each observation of a dataset, we identify a local group of dense nearby\nobservations, which we call a core, based on a modification of the k-nearest\nneighbours algorithm. By projecting the dataset onto the space spanned by those\nobservations, two aspects are revealed. First, we can analyze the distance from\nan observation to the center of the core within the projection space in order\nto provide a measure of quality of description of the observation by the\nprojection. Second, we consider the distance of the observation to the\nprojection space in order to assess the suitability of the core for describing\nthe outlyingness of the observation. These novel interpretations lead to a\nunivariate measure of outlyingness based on aggregations over all local\nprojections, which outperforms LOF and RobPCA as well as other popular methods\nlike PCOut (Filzmoser et al., 2008) and subspace-based outlier detection\n(Kriegel et al., 2009) in our simulation setups. Experiments in the context of\nreal-word applications employing datasets of various dimensionality demonstrate\nthe advantages of local projections.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 15:23:11 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Ortner", "Thomas", ""], ["Filzmoser", "Peter", ""], ["Zaharieva", "Maia", ""], ["Brodinova", "Sarka", ""], ["Breiteneder", "Christian", ""]]}, {"id": "1708.01772", "submitter": "Nikolai Slavov", "authors": "Dmitry Malioutov, Tianchi Chen, Jacob Jaffe, Edoardo Airoldi, Steven\n  Carr, Bogdan Budnik and Nikolai Slavov", "title": "Quantifying homologous proteins and proteoforms", "comments": null, "journal-ref": "Molecular & Cellular Proteomics, 2018", "doi": "10.1074/mcp.TIR118.000947", "report-no": "mcp.TIR118.000947", "categories": "q-bio.QM q-bio.GN stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many proteoforms - arising from alternative splicing, post-translational\nmodifications (PTMs), or paralogous genes - have distinct biological functions,\nsuch as histone PTM proteoforms. However, their quantification by existing\nbottom-up mass-spectrometry (MS) methods is undermined by peptide-specific\nbiases. To avoid these biases, we developed and implemented a first-principles\nmodel (HIquant) for quantifying proteoform stoichiometries. We characterized\nwhen MS data allow inferring proteoform stoichiometries by HIquant, derived an\nalgorithm for optimal inference, and demonstrated experimentally high accuracy\nin quantifying fractional PTM occupancy without using external standards, even\nin the challenging case of the histone modification code.\n  HIquant server is implemented at:\nhttps://web.northeastern.edu/slavov/2014_HIquant/\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 13:52:12 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Malioutov", "Dmitry", ""], ["Chen", "Tianchi", ""], ["Jaffe", "Jacob", ""], ["Airoldi", "Edoardo", ""], ["Carr", "Steven", ""], ["Budnik", "Bogdan", ""], ["Slavov", "Nikolai", ""]]}, {"id": "1708.01829", "submitter": "Roberto Rossi", "authors": "Roberto Rossi and \\\"Ozg\\\"ur Akg\\\"un and Steven Prestwich and S.\n  Armagan Tarim", "title": "Declarative Statistics", "comments": "The modeling framework and the examples used in this work are\n  available at https://gwr3n.github.io/syat-choco/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce declarative statistics, a suite of declarative\nmodelling tools for statistical analysis. Statistical constraints represent the\nkey building block of declarative statistics. First, we introduce a range of\nrelevant counting and matrix constraints and associated decompositions, some of\nwhich novel, that are instrumental in the design of statistical constraints.\nSecond, we introduce a selection of novel statistical constraints and\nassociated decompositions, which constitute a self-contained toolbox that can\nbe used to tackle a wide range of problems typically encountered by\nstatisticians. Finally, we deploy these statistical constraints to a wide range\nof application areas drawn from classical statistics and we contrast our\nframework against established practices.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 01:25:30 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 17:51:49 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Rossi", "Roberto", ""], ["Akg\u00fcn", "\u00d6zg\u00fcr", ""], ["Prestwich", "Steven", ""], ["Tarim", "S. Armagan", ""]]}, {"id": "1708.01974", "submitter": "David Frazier", "authors": "David T. Frazier, Christian P. Robert and Judith Rousseau", "title": "Model Misspecification in ABC: Consequences and Diagnostics", "comments": null, "journal-ref": null, "doi": "10.1111/369--7412/20/82421", "report-no": null, "categories": "math.ST q-fin.EC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the behavior of approximate Bayesian computation (ABC) when the\nmodel generating the simulated data differs from the actual data generating\nprocess; i.e., when the data simulator in ABC is misspecified. We demonstrate\nboth theoretically and in simple, but practically relevant, examples that when\nthe model is misspecified different versions of ABC can yield substantially\ndifferent results. Our theoretical results demonstrate that even though the\nmodel is misspecified, under regularity conditions, the accept/reject ABC\napproach concentrates posterior mass on an appropriately defined pseudo-true\nparameter value. However, under model misspecification the ABC posterior does\nnot yield credible sets with valid frequentist coverage and has non-standard\nasymptotic behavior. In addition, we examine the theoretical behavior of the\npopular local regression adjustment to ABC under model misspecification and\ndemonstrate that this approach concentrates posterior mass on a completely\ndifferent pseudo-true value than accept/reject ABC. Using our theoretical\nresults, we suggest two approaches to diagnose model misspecification in ABC.\nAll theoretical results and diagnostics are illustrated in a simple running\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 03:10:04 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 05:35:22 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 05:53:48 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 21:58:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Frazier", "David T.", ""], ["Robert", "Christian P.", ""], ["Rousseau", "Judith", ""]]}, {"id": "1708.02123", "submitter": "Joshua Lukemire", "authors": "Joshua Lukemire, Suprateek Kundu, Giuseppe Pagnoni, and Ying Guo", "title": "Bayesian Joint Modeling of Multiple Brain Functional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain function is organized in coordinated modes of spatio-temporal activity\n(functional networks) exhibiting an intrinsic baseline structure with\nvariations under different experimental conditions. Existing approaches for\nuncovering such network structures typically do not explicitly model shared and\ndifferential patterns across networks, thus potentially reducing the detection\npower. We develop an integrative modeling approach for jointly modeling\nmultiple brain networks across experimental conditions. The proposed Bayesian\nJoint Network Learning approach develops flexible priors on the edge\nprobabilities involving a common intrinsic baseline structure and differential\neffects specific to individual networks. Conditional on these edge\nprobabilities, connection strengths are modeled under a Bayesian spike and slab\nprior on the off-diagonal elements of the inverse covariance matrix. The model\nis fit under a posterior computation scheme based on Markov chain Monte Carlo.\nNumerical simulations illustrate that the proposed joint modeling approach has\nincreased power to detect true differential edges while providing adequate\ncontrol on false positives and achieving greater accuracy in the estimation of\nedge strengths compared to existing methods. An application of the method to\nfMRI Stroop task data provides unique insights into brain network alterations\nbetween cognitive conditions which existing graphical modeling techniques\nfailed to reveal.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 14:02:25 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 18:56:02 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Lukemire", "Joshua", ""], ["Kundu", "Suprateek", ""], ["Pagnoni", "Giuseppe", ""], ["Guo", "Ying", ""]]}, {"id": "1708.02140", "submitter": "Yotam Shem-Tov", "authors": "Jasjeet S. Sekhon, Yotam Shem-Tov", "title": "Inference on a New Class of Sample Average Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive new variance formulas for inference on a general class of estimands\nof causal average treatment effects in a Randomized Control Trial (RCT). We\ngeneralize Robins (1988) and show that when the estimand of interest is the\nSample Average Treatment Effect of the Treated (SATT or SATC for controls), a\nconsistent variance estimator exists. Although these estimands are equal to the\nSample Average Treatment Effect (SATE) in expectation, potentially large\ndifferences in both accuracy and coverage can occur by the change of estimand,\neven asymptotically. Inference on the SATE, even using a conservative\nconfidence interval, provides incorrect coverage of the SATT or SATC. We derive\nthe variance and limiting distribution of a new and general class of\nestimands---any mixing between SATT and SATC---for which the SATE is a specific\ncase. We demonstrate the applicability of the new theoretical results using\nMonte-Carlo simulations and an empirical application with hundreds of online\nexperiments with an average sample size of approximately one hundred million\nobservations per experiment. An R package, estCI, that implements all the\nproposed estimation procedures is available.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 14:48:09 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 06:17:26 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Sekhon", "Jasjeet S.", ""], ["Shem-Tov", "Yotam", ""]]}, {"id": "1708.02166", "submitter": "Lars Arne Jordanger", "authors": "Lars Arne Jordanger, Dag Tj{\\o}stheim", "title": "Nonlinear spectral analysis: A local Gaussian approach", "comments": "Version 4: Major revision from version 3, with new theory/figures.\n  135 pages (main part 32 + appendices 103), 11 + 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral distribution $f(\\omega)$ of a stationary time series\n$\\{Y_t\\}_{t\\in\\mathbb{Z}}$ can be used to investigate whether or not periodic\nstructures are present in $\\{Y_t\\}_{t\\in\\mathbb{Z}}$, but $f(\\omega)$ has some\nlimitations due to its dependence on the autocovariances $\\gamma(h)$. For\nexample, $f(\\omega)$ can not distinguish white i.i.d. noise from GARCH-type\nmodels (whose terms are dependent, but uncorrelated), which implies that\n$f(\\omega)$ can be an inadequate tool when $\\{Y_t\\}_{t\\in\\mathbb{Z}}$ contains\nasymmetries and nonlinear dependencies.\n  Asymmetries between the upper and lower tails of a time series can be\ninvestigated by means of the local Gaussian autocorrelations introduced in\nTj{\\o}stheim and Hufthammer (2013), and these local measures of dependence can\nbe used to construct the local Gaussian spectral density presented in this\npaper. A key feature of the new local spectral density is that it coincides\nwith $f(\\omega)$ for Gaussian time series, which implies that it can be used to\ndetect non-Gaussian traits in the time series under investigation. In\nparticular, if $f(\\omega)$ is flat, then peaks and troughs of the new local\nspectral density can indicate nonlinear traits, which potentially might\ndiscover local periodic phenomena that remain undetected in an ordinary\nspectral analysis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:27:51 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 14:09:50 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 07:35:38 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2020 11:44:55 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jordanger", "Lars Arne", ""], ["Tj\u00f8stheim", "Dag", ""]]}, {"id": "1708.02230", "submitter": "Richard Everitt", "authors": "Richard G. Everitt and Paulina A. Rowi\\'nska", "title": "Delayed acceptance ABC-SMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is now an established technique for\nstatistical inference used in cases where the likelihood function is\ncomputationally expensive or not available. It relies on the use of a~model\nthat is specified in the form of a~simulator, and approximates the likelihood\nat a~parameter value $\\theta$ by simulating auxiliary data sets $x$ and\nevaluating the distance of $x$ from the true data $y$. However, ABC is not\ncomputationally feasible in cases where using the simulator for each $\\theta$\nis very expensive. This paper investigates this situation in cases where\na~cheap, but approximate, simulator is available. The approach is to employ\ndelayed acceptance Markov chain Monte Carlo (MCMC) within an ABC sequential\nMonte Carlo (SMC) sampler in order to, in a~first stage of the kernel, use the\ncheap simulator to rule out parts of the parameter space that are not worth\nexploring, so that the ``true'' simulator is only run (in the second stage of\nthe kernel) where there is a~reasonable chance of accepting proposed values of\n$\\theta$. We show that this approach can be used quite automatically, with few\ntuning parameters. Applications to stochastic differential equation models and\nlatent doubly intractable distributions are presented.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:52:04 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 14:56:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Everitt", "Richard G.", ""], ["Rowi\u0144ska", "Paulina A.", ""]]}, {"id": "1708.02234", "submitter": "Gael Martin Prof", "authors": "David Harris, Gael M. Martin, Indeewara Perera and D.S. Poskitt", "title": "Construction and Visualization of Optimal Confidence Sets for\n  Frequentist Distributional Forecasts", "comments": "This paper contains animated figures that can be viewed using Adobe\n  Reader/Acrobat. The animations are not supported within a web browser or in\n  non-Acrobat document viewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is on the quantification of sampling variation in\nfrequentist probabilistic forecasts. We propose a method of constructing\nconfidence sets that respects the functional nature of the forecast\ndistribution, and use animated graphics to visualize the impact of parameter\nuncertainty on the location, dispersion and shape of the distribution. The\nconfidence sets are derived via the inversion of a Wald test and are\nasymptotically uniformly most accurate and, hence, optimal in this sense. A\nwide range of linear and non-linear time series models - encompassing long\nmemory, state space and mixture specifications - is used to demonstrate the\nprocedure, based on artificially generated data. An empirical example in which\ndistributional forecasts of both financial returns and its stochastic\nvolatility are produced is then used to illustrate the practical importance of\naccommodating sampling variation in the manner proposed.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 02:11:43 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Harris", "David", ""], ["Martin", "Gael M.", ""], ["Perera", "Indeewara", ""], ["Poskitt", "D. S.", ""]]}, {"id": "1708.02365", "submitter": "David Frazier", "authors": "David T. Frazier, Tatsushi Oka and Dan Zhu", "title": "Indirect Inference with a Non-Smooth Criterion Function", "comments": "This paper is a revision of arXiv:1708.02365 and supersedes the\n  earlier arXiv paper \"Derivative-Based Optimization with a Non-Smooth\n  Simulated Criterion\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect inference requires simulating realisations of endogenous variables\nfrom the model under study. When the endogenous variables are discontinuous\nfunctions of the model parameters, the resulting indirect inference criterion\nfunction is discontinuous and does not permit the use of derivative-based\noptimisation routines. Using a change of variables technique, we propose a\nnovel simulation algorithm that alleviates the discontinuities inherent in such\nindirect inference criterion functions, and permits the application of\nderivative-based optimisation routines to estimate the unknown model\nparameters. Unlike competing approaches, this approach does not rely on kernel\nsmoothing or bandwidth parameters. Several Monte Carlo examples that have\nfeatured in the literature on indirect inference with discontinuous outcomes\nillustrate the approach, and demonstrate the superior performance of this\napproach over existing alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 04:04:02 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 01:16:17 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 21:57:34 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Frazier", "David T.", ""], ["Oka", "Tatsushi", ""], ["Zhu", "Dan", ""]]}, {"id": "1708.02395", "submitter": "Eric Schmitt", "authors": "Eric Schmitt, Christopher Tull and Patrick Atwater", "title": "Extending Bayesian structural time-series estimates of causal impact to\n  many-household conservation initiatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Government agencies offer economic incentives to citizens for conservation\nactions, such as rebates for installing efficient appliances and compensation\nfor modifications to homes. The intention of these conservation actions is\nfrequently to reduce the consumption of a utility. Measuring the conservation\nimpact of incentives is important for guiding policy, but doing so is\ntechnically difficult. However, the methods for estimating the impact of public\noutreach efforts have seen substantial developments in marketing to consumers\nin recent years as marketers seek to substantiate the value of their services.\nOne such method uses Bayesian Stuctural Time Series (BSTS) to compare a market\nexposed to an advertising campaign with control markets identified through a\nmatching procedure. This paper introduces an extension of the matching/BSTS\nmethod for impact estimation to make it applicable for general conservation\nprogram impact estimation when multi-household data is available. This is\naccomplished by household matching/BSTS steps to obtain conservation estimates\nand then aggregating the results using a meta-regression step to aggregate the\nfindings. A case study examining the impact of rebates for household turf\nremoval on water consumption in multiple Californian water districts is\nconducted to illustrate the work flow of this method.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 07:49:04 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Schmitt", "Eric", ""], ["Tull", "Christopher", ""], ["Atwater", "Patrick", ""]]}, {"id": "1708.02426", "submitter": "Pavel Mozgunov", "authors": "Pavel Mozgunov and Thomas Jaki", "title": "An information-theoretic approach for selecting arms in clinical trials", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of selecting the \"best\" amongst different choices is a common\nproblem in statistics. In drug development, our motivating setting, the\nquestion becomes, for example: what is the dose that gives me a pre-specified\nrisk of toxicity or which treatment gives the best response rate. Motivated by\na recent development in the weighted information measures theory, we propose an\nexperimental design based on a simple and intuitive criterion which governs arm\nselection in the experiment with multinomial outcomes. The criterion leads to\naccurate arm selection without any parametric or monotonicity assumption. The\nasymptotic properties of the design are studied for different allocation rules\nand the small sample size behaviour is evaluated in simulations in the context\nof Phase I and Phase II clinical trials with binary endpoints. We compare the\nproposed design to currently used alternatives and discuss its practical\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 09:38:23 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 11:01:37 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Mozgunov", "Pavel", ""], ["Jaki", "Thomas", ""]]}, {"id": "1708.02428", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad, Jelle J. Goeman and Erik W. van Zwet", "title": "An improvement of Tukey's HSD with application to ranking institutions", "comments": "Working paper. 19 pages with R code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a ranking of institutions such as medical centers or universities is\nbased on an indicator provided with a standard error, confidence intervals\nshould be calculated to assess the quality of these ranks. We consider the\nproblem of constructing simultaneous confidence intervals (CIs) for the ranks\nof centers based on an observed sample. We present a novel method based on\nTukey's honest significant difference test (HSD) which is the first method to\nproduce valid simultaneous CIs for ranks. Moreover, we introduce a new variant\nof Tukey's HSD based on the sequential rejection principle. The new algorithm\nensures familywise error control, and produces simultaneous confidence\nintervals for the ranks uniformly shorter than those provided by Tukey's HSD\nfor the same level of significance. We illustrate the method through both\nsimulations and real data analysis from 64 hospitals in the Netherlands.\nSoftware for our new methods is available online in package \\texttt{ICRanks}\ndownloadable from CRAN. Supplementary materials include supplementary R code\nfor the simulations and proofs of the propositions presented in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 09:40:00 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 10:19:22 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 08:51:22 GMT"}, {"version": "v4", "created": "Thu, 22 Nov 2018 10:32:08 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Mohamad", "Diaa Al", ""], ["Goeman", "Jelle J.", ""], ["van Zwet", "Erik W.", ""]]}, {"id": "1708.02447", "submitter": "Thomas Opitz", "authors": "Jean-Noel Bacro, Carlo Gaetan, Thomas Opitz, Gwladys Toulemonde", "title": "Hierarchical space-time modeling of exceedances with an application to\n  rainfall data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical modeling of space-time extremes in environmental applications\nis key to understanding complex dependence structures in original event data\nand to generating realistic scenarios for impact models. In this context of\nhigh-dimensional data, we propose a novel hierarchical model for high threshold\nexceedances defined over continuous space and time by embedding a space-time\nGamma process convolution for the rate of an exponential variable, leading to\nasymptotic independence in space and time. Its physically motivated anisotropic\ndependence structure is based on geometric objects moving through space-time\naccording to a velocity vector. We demonstrate that inference based on weighted\npairwise likelihood is fast and accurate. The usefulness of our model is\nillustrated by an application to hourly precipitation data from a study region\nin Southern France, where it clearly improves on an alternative censored\nGaussian space-time random field model. While classical limit models based on\nthreshold-stability fail to appropriately capture relatively fast joint tail\ndecay rates between asymptotic dependence and classical independence, strong\nempirical evidence from our application and other recent case studies motivates\nthe use of more realistic asymptotic independence models such as ours.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 11:13:36 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 20:01:19 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Bacro", "Jean-Noel", ""], ["Gaetan", "Carlo", ""], ["Opitz", "Thomas", ""], ["Toulemonde", "Gwladys", ""]]}, {"id": "1708.02491", "submitter": "Marie-H\\'el\\`ene Descary", "authors": "Marie-H\\'el\\`ene Descary and Victor M. Panaretos", "title": "Recovering Covariance from Functional Fragments", "comments": "Biometrika, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric estimation of a covariance function on the unit\nsquare, given a sample of discretely observed fragments of functional data.\nWhen each sample path is only observed on a subinterval of length $\\delta<1$,\none has no statistical information on the unknown covariance outside a\n$\\delta$-band around the diagonal. The problem seems unidentifiable without\nparametric assumptions, but we show that nonparametric estimation is feasible\nunder suitable smoothness and rank conditions on the unknown covariance. This\nremains true even when observation is discrete, and we give precise\ndeterministic conditions on how fine the observation grid needs to be relative\nto the rank and fragment length for identifiability to hold true. We show that\nour conditions translate the estimation problem to a low-rank matrix completion\nproblem, construct a nonparametric estimator in this vein, and study its\nasymptotic properties. We illustrate the numerical performance of our method on\nreal and simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 13:52:21 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 14:29:15 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 01:46:30 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Descary", "Marie-H\u00e9l\u00e8ne", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1708.02495", "submitter": "Lars Arne Jordanger", "authors": "Lars Arne Jordanger, Dag Tj{\\o}stheim", "title": "Nonlinear cross-spectrum analysis via the local Gaussian correlation", "comments": "41 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectrum analysis can detect frequency related structures in a time series\n$\\{Y_t\\}_{t\\in\\mathbb{Z}}$, but may in general be an inadequate tool if\nasymmetries or other nonlinear phenomena are present. This limitation is a\nconsequence of the way the spectrum is based on the second order moments (auto\nand cross-covariances), and alternative approaches to spectrum analysis have\nthus been investigated based on other measures of dependence. One such approach\nwas developed for univariate time series in Jordanger and Tj{\\o}stheim (2017),\nwhere it was seen that a local Gaussian auto-spectrum $f_{v}(\\omega)$, based on\nthe local Gaussian autocorrelations $\\rho_v(\\omega)$ from Tj{\\o}stheim and\nHufthammer (2013), could detect local structures in time series that looked\nlike white noise when investigated by the ordinary auto-spectrum $f(\\omega)$.\nThe local Gaussian approach in this paper is extended to a local Gaussian\ncross-spectrum $f_{kl:v}(\\omega)$ for multivariate time series. The local\ncross-spectrum $f_{kl:v}(\\omega)$ has the desirable property that it coincides\nwith the ordinary cross-spectrum $f_{kl}(\\omega)$ for Gaussian time series,\nwhich implies that $f_{kl:v}(\\omega)$ can be used to detect non-Gaussian traits\nin the time series under investigation. In particular: If the ordinary spectrum\nis flat, then peaks and troughs of the local Gaussian spectrum can indicate\nnonlinear traits, which potentially might discover local periodic phenomena\nthat goes undetected in an ordinary spectral analysis.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 14:06:30 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Jordanger", "Lars Arne", ""], ["Tj\u00f8stheim", "Dag", ""]]}, {"id": "1708.02647", "submitter": "Alex Reinhart", "authors": "Alex Reinhart", "title": "A Review of Self-Exciting Spatio-Temporal Point Processes and Their\n  Applications", "comments": "30 pages, 3 figures", "journal-ref": "Statistical Science 33 (2018), no. 3, pp. 299-318", "doi": "10.1214/17-STS629", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-exciting spatio-temporal point process models predict the rate of events\nas a function of space, time, and the previous history of events. These models\nnaturally capture triggering and clustering behavior, and have been widely used\nin fields where spatio-temporal clustering of events is observed, such as\nearthquake modeling, infectious disease, and crime. In the past several\ndecades, advances have been made in estimation, inference, simulation, and\ndiagnostic tools for self-exciting point process models. In this review, I\ndescribe the basic theory, survey related estimation and inference techniques\nfrom each field, highlight several key applications, and suggest directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 20:44:57 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 18:39:31 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Reinhart", "Alex", ""]]}, {"id": "1708.02648", "submitter": "Luc Villandr\\'e", "authors": "Luc Villandr\\'e, Aur\\'elie Labbe, Bluma Brenner, Michel Roger, David\n  A. Stephens", "title": "DM-PhyClus: A Bayesian phylogenetic algorithm for infectious disease\n  transmission cluster inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Conventional phylogenetic clustering approaches rely on arbitrary\ncutpoints applied a posteriori to phylogenetic estimates. Although in practice,\nBayesian and bootstrap-based clustering tend to lead to similar estimates, they\noften produce conflicting measures of confidence in clusters. The current study\nproposes a new Bayesian phylogenetic clustering algorithm, which we refer to as\nDM-PhyClus, that identifies sets of sequences resulting from quick transmission\nchains, thus yielding easily-interpretable clusters, without using any ad hoc\ndistance or confidence requirement. Results. Simulations reveal that DM-PhyClus\ncan outperform conventional clustering methods, as well as the Gap procedure, a\npure distance-based algorithm, in terms of mean cluster recovery. We apply\nDM-PhyClus to a sample of real HIV-1 sequences, producing a set of clusters\nwhose inference is in line with the conclusions of a previous thorough\nanalysis. Conclusions. DM-PhyClus, by eliminating the need for cutpoints and\nproducing sensible inference for cluster configurations, can facilitate\ntransmission cluster detection. Future efforts to reduce incidence of\ninfectious diseases, like HIV-1, will need reliable estimates of transmission\nclusters. It follows that algorithms like DM-PhyClus could serve to better\ninform public health strategies.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 20:45:26 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Villandr\u00e9", "Luc", ""], ["Labbe", "Aur\u00e9lie", ""], ["Brenner", "Bluma", ""], ["Roger", "Michel", ""], ["Stephens", "David A.", ""]]}, {"id": "1708.02705", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Kengo Kato", "title": "Jackknife multiplier bootstrap: finite sample approximations to the\n  $U$-process supremum with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with finite sample approximations to the supremum of\na non-degenerate $U$-process of a general order indexed by a function class. We\nare primarily interested in situations where the function class as well as the\nunderlying distribution change with the sample size, and the $U$-process itself\nis not weakly convergent as a process. Such situations arise in a variety of\nmodern statistical problems. We first consider Gaussian approximations, namely,\napproximate the $U$-process supremum by the supremum of a Gaussian process, and\nderive coupling and Kolmogorov distance bounds. Such Gaussian approximations\nare, however, not often directly applicable in statistical problems since the\ncovariance function of the approximating Gaussian process is unknown. This\nmotivates us to study bootstrap-type approximations to the $U$-process\nsupremum. We propose a novel jackknife multiplier bootstrap (JMB) tailored to\nthe $U$-process, and derive coupling and Kolmogorov distance bounds for the\nproposed JMB method. All these results are non-asymptotic, and established\nunder fairly general conditions on function classes and underlying\ndistributions. Key technical tools in the proofs are new local maximal\ninequalities for $U$-processes, which may be useful in other problems. We also\ndiscuss applications of the general approximation results to testing for\nqualitative features of nonparametric functions based on generalized local\n$U$-processes.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 03:43:03 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 15:04:34 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 15:36:27 GMT"}, {"version": "v4", "created": "Thu, 14 Feb 2019 02:22:27 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Chen", "Xiaohui", ""], ["Kato", "Kengo", ""]]}, {"id": "1708.02723", "submitter": "Thomas Opitz", "authors": "Thomas Opitz", "title": "Latent Gaussian modeling and INLA: A review with focus on space-time\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical models with latent Gaussian layers have proven very\nflexible in capturing complex stochastic behavior and hierarchical structures\nin high-dimensional spatial and spatio-temporal data. Whereas simulation-based\nBayesian inference through Markov Chain Monte Carlo may be hampered by slow\nconvergence and numerical instabilities, the inferential framework of\nIntegrated Nested Laplace Approximation (INLA) is capable to provide accurate\nand relatively fast analytical approximations to posterior quantities of\ninterest. It heavily relies on the use of Gauss-Markov dependence structures to\navoid the numerical bottleneck of high-dimensional nonsparse matrix\ncomputations. With a view towards space-time applications, we here review the\nprincipal theoretical concepts, model classes and inference tools within the\nINLA framework. Important elements to construct space-time models are certain\nspatial Mat\\'ern-like Gauss-Markov random fields, obtained as approximate\nsolutions to a stochastic partial differential equation. Efficient\nimplementation of statistical inference tools for a large variety of models is\navailable through the INLA package of the R software. To showcase the practical\nuse of R-INLA and to illustrate its principal commands and syntax, a\ncomprehensive simulation experiment is presented using simulated non Gaussian\nspace-time count data with a first-order autoregressive dependence structure in\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 05:57:36 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Opitz", "Thomas", ""]]}, {"id": "1708.02729", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad, Erik W. van Zwet, Jelle J. Goeman and Aldo Solari", "title": "Simultaneous confidence sets for ranks using the partitioning principle\n  - Technical report", "comments": "Technical report. A reduced version will be submitted soon to JRSSB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking institutions such as medical centers or universities is based on an\nindicator accompanied with an uncertainty measure such as a standard deviation,\nand confidence intervals should be calculated to assess the quality of these\nranks. We consider the problem of constructing simultaneous confidence\nintervals for the ranks of centers based on an observed sample. We present in\nthis paper a novel method based on multiple testing which uses the partitioning\nprinciple and employs the likelihood ratio (LR) test on the partitions. The\ncomplexity of the algorithm is super exponential. We present several ways and\nshortcuts to reduce this complexity. We provide also a polynomial algorithm\nwhich produces a very good bracketing for the multiple testing by linearizing\nthe critical value of the LR test. We show that Tukey's Honest Significant\nDifference (HSD) test can be written as a partitioning procedure. The new\nmethodology has promising properties in the sens that it opens the door in a\nsimple and easy way to construct new methods which may trade the exponential\ncomplexity with power of the test or vice versa. In comparison to Tukey's HSD\ntest, the LR test seems to give better results when the centers are close to\neach others or the uncertainty in the data is high which is confirmed during a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:27:50 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Mohamad", "Diaa Al", ""], ["van Zwet", "Erik W.", ""], ["Goeman", "Jelle J.", ""], ["Solari", "Aldo", ""]]}, {"id": "1708.02736", "submitter": "Abolfazl Safikhani", "authors": "Abolfazl Safikhani and Ali Shojaie", "title": "Structural Break Detection in High-Dimensional Non-Stationary VAR models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming stationarity is unrealistic in many time series applications. A more\nrealistic alternative is to allow for piecewise stationarity, where the model\nis allowed to change at given time points. In this article, the problem of\ndetecting the change points in a high-dimensional piecewise vector\nautoregressive model (VAR) is considered. Reformulated the problem as a\nhigh-dimensional variable selection, a penalized least square estimation using\ntotal variation LASSO penalty is proposed for estimation of model parameters.\nIt is shown that the developed method over-estimates the number of change\npoints. A backward selection criterion is thus proposed in conjunction with the\npenalized least square estimator to tackle this issue. We prove that the\nproposed two-stage procedure consistently detects the number of change points\nand their locations. A block coordinate descent algorithm is developed for\nefficient computation of model parameters. The performance of the method is\nillustrated using several simulation scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 07:03:39 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Safikhani", "Abolfazl", ""], ["Shojaie", "Ali", ""]]}, {"id": "1708.02742", "submitter": "Chi Kuen Wong", "authors": "Chi Kuen Wong, Enes Makalic, Daniel F. Schmidt", "title": "Minimum message length inference of the Poisson and geometric models\n  using heavy-tailed prior distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum message length is a general Bayesian principle for model selection\nand parameter estimation that is based on information theory. This paper\napplies the minimum message length principle to a small-sample model selection\nproblem involving Poisson and geometric data models. Since MML is a Bayesian\nprinciple, it requires prior distributions for all model parameters. We\nintroduce three candidate prior distributions for the unknown model parameters\nwith both light- and heavy-tails. The performance of the MML methods is\ncompared with objective Bayesian inference and minimum description length\ntechniques based on the normalized maximum likelihood code. Simulations show\nthat our MML approach with a heavy-tail prior distribution provides an\nexcellent performance in all tests.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 07:44:29 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 02:27:00 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 19:53:44 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Wong", "Chi Kuen", ""], ["Makalic", "Enes", ""], ["Schmidt", "Daniel F.", ""]]}, {"id": "1708.02786", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi, Lorenzo Trapani", "title": "Sequential testing for structural stability in approximate factor models", "comments": null, "journal-ref": null, "doi": "10.1016/j.spa.2020.03.003", "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a monitoring procedure to detect changes in a large approximate\nfactor model. Letting $r$ be the number of common factors, we base our\nstatistics on the fact that the $\\left( r+1\\right) $-th eigenvalue of the\nsample covariance matrix is bounded under the null of no change, whereas it\nbecomes spiked under changes. Given that sample eigenvalues cannot be estimated\nconsistently under the null, we randomise the test statistic, obtaining a\nsequence of \\textit{i.i.d} statistics, which are used for the monitoring\nscheme. Numerical evidence shows a very small probability of false detections,\nand tight detection times of change-points.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 11:18:49 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 16:44:40 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 14:15:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Trapani", "Lorenzo", ""]]}, {"id": "1708.02908", "submitter": "Sylvain Sardy", "authors": "Sylvain Sardy, Caroline Giacobino and Jairo Diaz-Rodriguez", "title": "Thresholding tests", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new class of statistical tests for generalized linear models\nbased on thresholding point estimators. These tests can be employed whether the\nmodel includes more parameters than observations or not. For linear models, our\ntests rely on pivotal statistics derived from model selection techniques.\nAffine lasso, a new extension of lasso, allows to unveil new tests and to\ndevelop in the same framework parametric and nonparametric tests. Our tests for\ngeneralized linear models are based on new asymptotically pivotal statistics. A\ncomposite thresholding test attempts to achieve uniformly most power under both\nsparse and dense alternatives with success. In a simulation, we compare the\nlevel and power of these tests under sparse and dense alternative hypotheses.\nThe thresholding tests have a better control of the nominal level and higher\npower than existing tests.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:59:50 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 10:49:02 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Sardy", "Sylvain", ""], ["Giacobino", "Caroline", ""], ["Diaz-Rodriguez", "Jairo", ""]]}, {"id": "1708.03196", "submitter": "Ricardo Maronna", "authors": "Ricardo Maronna", "title": "Improving the Pe\\~na-Prieto \"KSD\" procedure", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pe\\~{n}a and Prieto (2007) proposed the \"Kurtosis plus specific directions\"\n(KSD) method for robust multivariate location and scatter estimation and\noutlier detection. Maronna and Yohai (2017) employed it as an initial estimator\nfor multivariate S- and MM-estimators, and their simulations showed that KSD\ngenerally outperforms initial estimators based on subsampling. However further\nsimulations show that KSD may become unstable and give wrong results in extreme\nsituations when the contamination rate is \"high\" (>=0.2) and the ratio n/p of\ncases to variables is \"low\" (<10). Two simple modifications of the procedure\nare proposed, which greatly improve on the method's performance as an initial\nestimator, with only a small increase in computational time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 13:00:15 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Maronna", "Ricardo", ""]]}, {"id": "1708.03272", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Leonhard Held and H{\\aa}vard Rue", "title": "Fast and accurate Bayesian model criticism and conflict diagnostics\n  using R-INLA", "comments": null, "journal-ref": "Stat 6(1):331-344, 2017", "doi": "10.1002/sta4.163", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical models are increasingly popular for realistic modelling\nand analysis of complex data. This trend is accompanied by the need for\nflexible, general, and computationally efficient methods for model criticism\nand conflict detection. Usually, a Bayesian hierarchical model incorporates a\ngrouping of the individual data points, for example individuals in repeated\nmeasurement data. In such cases, the following question arises: Are any of the\ngroups \"outliers\", or in conflict with the remaining groups? Existing general\napproaches aiming to answer such questions tend to be extremely computationally\ndemanding when model fitting is based on MCMC. We show how group-level model\ncriticism and conflict detection can be done quickly and accurately through\nintegrated nested Laplace approximations (INLA). The new method is implemented\nas a part of the open source R-INLA package for Bayesian computing\n(http://r-inla.org).\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:49:25 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 16:50:55 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 20:26:35 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 17:15:55 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Held", "Leonhard", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1708.03288", "submitter": "Peter Radchenko", "authors": "Rahul Mazumder and Peter Radchenko and Antoine Dedieu", "title": "Subset Selection with Shrinkage: Sparse Linear Modeling when the SNR is\n  low", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a seemingly unexpected and relatively less understood overfitting\naspect of a fundamental tool in sparse linear modeling - best subset selection,\nwhich minimizes the residual sum of squares subject to a constraint on the\nnumber of nonzero coefficients. While the best subset selection procedure is\noften perceived as the \"gold standard\" in sparse learning when the signal to\nnoise ratio (SNR) is high, its predictive performance deteriorates when the SNR\nis low. In particular, it is outperformed by continuous shrinkage methods, such\nas ridge regression and the Lasso. We investigate the behavior of best subset\nselection in the high-noise regimes and propose an alternative approach based\non a regularized version of the least-squares criterion. Our proposed\nestimators (a) mitigate, to a large extent, the poor predictive performance of\nbest subset selection in the high-noise regimes; and (b) perform favorably,\nwhile generally delivering substantially sparser models, relative to the best\npredictive models available via ridge regression and the Lasso. We conduct an\nextensive theoretical analysis of the predictive properties of the proposed\napproach and provide justification for its superior predictive performance\nrelative to best subset selection when the noise-level is high. Our estimators\ncan be expressed as solutions to mixed integer second order conic optimization\nproblems and, hence, are amenable to modern computational tools from\nmathematical optimization.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:28:39 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 18:30:37 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 04:23:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""], ["Dedieu", "Antoine", ""]]}, {"id": "1708.03532", "submitter": "Clemens Kreutz", "authors": "Clemens Kreutz", "title": "An easy and efficient approach for testing identifiability of parameters", "comments": "Supplement is attached at the end of the document", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feasibility of uniquely estimating parameters of dynamical systems from\nobservations is a widely discussed aspect of mathematical modelling. Several\napproaches have been published for analyzing identifiability. However, they are\ntypically computationally demanding, difficult to perform and/or not applicable\nin many application settings.\n  Here, an intuitive approach is presented which enables quickly testing of\nparameter identifiability. Numerical optimization with a penalty in radial\ndirection enforcing displacement of the parameters is used to check whether\nestimated parameters are unique, or whether the parameters can be altered\nwithout loss of agreement with the data indicating non-identifiability. This\nIdentifiability-Test by Radial Penalization (ITRP) can be employed for every\nmodel where optimization-based fitting like least-squares or maximum likelihood\nis feasible and is therefore applicable for all typical deterministic models.\n  The approach is illustrated and tested using 11 ordinary differential\nequation (ODE) models. The presented approach can be implemented without great\nefforts in any modelling framework. It is available within the free\nMatlab-based modelling toolbox Data2Dynamics. Source code is available at\nhttps://github.com/Data2Dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 13:13:08 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Kreutz", "Clemens", ""]]}, {"id": "1708.03625", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, John O'Leary and Yves F. Atchad\\'e", "title": "Unbiased Markov chain Monte Carlo with couplings", "comments": "Final version, accepted as a JRSS discussion paper; includes\n  supplementary material as appendices; 12 figures, 48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods provide consistent of integrals as\nthe number of iterations goes to infinity. MCMC estimators are generally biased\nafter any fixed number of iterations. We propose to remove this bias by using\ncouplings of Markov chains together with a telescopic sum argument of Glynn and\nRhee (2014). The resulting unbiased estimators can be computed independently in\nparallel. We discuss practical couplings for popular MCMC algorithms. We\nestablish the theoretical validity of the proposed estimators and study their\nefficiency relative to the underlying MCMC algorithms. Finally, we illustrate\nthe performance and limitations of the method on toy examples, on an Ising\nmodel around its critical temperature, on a high-dimensional variable selection\nproblem, and on an approximation of the cut distribution arising in Bayesian\ninference for models made of multiple modules.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 17:42:28 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 20:28:31 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 13:07:19 GMT"}, {"version": "v4", "created": "Wed, 31 Oct 2018 16:20:07 GMT"}, {"version": "v5", "created": "Wed, 17 Jul 2019 11:42:09 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Jacob", "Pierre E.", ""], ["O'Leary", "John", ""], ["Atchad\u00e9", "Yves F.", ""]]}, {"id": "1708.03761", "submitter": "Tim Verdonck", "authors": "Michiel Debruyne, Sebastiaan H\\\"oppner, Sven Serneels, Tim Verdonck", "title": "Outlyingness: why do outliers lie out?", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-018-9831-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is an inevitable step to most statistical data analyses.\nHowever, the mere detection of an outlying case does not always answer all\nscientific questions associated with that data point. Outlier detection\ntechniques, classical and robust alike, will typically flag the entire case as\noutlying, or attribute a specific case weight to the entire case. In practice,\nparticularly in high dimensional data, the outlier will most likely not be\noutlying along all of its variables, but just along a subset of them. If so,\nthe scientific question why the case has been flagged as an outlier becomes of\ninterest. In this article, a fast and efficient method is proposed to detect\nvariables that contribute most to an outlier's outlyingness. Thereby, it helps\nthe analyst understand why an outlier lies out. The approach pursued in this\nwork is to estimate the univariate direction of maximal outlyingness. It is\nshown that the problem of estimating that direction can be rewritten as the\nnormed solution of a classical least squares regression problem. Identifying\nthe subset of variables contributing most to outlyingness, can thus be achieved\nby estimating the associated least squares problem in a sparse manner. From a\npractical perspective, sparse partial least squares (SPLS) regression,\npreferably by the fast sparse NIPALS (SNIPLS) algorithm, is suggested to tackle\nthat problem. The proposed methodology is illustrated to perform well both on\nsimulated data and real life examples.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 10:48:58 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Debruyne", "Michiel", ""], ["H\u00f6ppner", "Sebastiaan", ""], ["Serneels", "Sven", ""], ["Verdonck", "Tim", ""]]}, {"id": "1708.03787", "submitter": "Vahe Avagyan", "authors": "Vahe Avagyan and Stijn Vansteelandt", "title": "Honest data-adaptive inference for the average treatment effect under\n  model misspecification using penalised bias-reduced double-robust estimation", "comments": "33 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of confounding by high-dimensional variables complicates\nestimation of the average effect of a point treatment. On the one hand, it\nnecessitates the use of variable selection strategies or more general\ndata-adaptive high-dimensional statistical methods. On the other hand, the use\nof such techniques tends to result in biased estimators with a non-standard\nasymptotic behaviour. Double-robust estimators are vital for offering a\nresolution because they possess a so-called small bias property (Newey et al.,\n2004). This means that their bias vanishes faster than the bias in the nuisance\nparameter estimators when the relevant smoothing parameter goes to zero, making\ntheir performance less sensitive to smoothing (Chernozhukov et al., 2016). This\nproperty has been exploited to achieve valid (uniform) inference of the average\ncausal effect when data-adaptive estimators of the propensity score and\nconditional outcome mean both converge to their respective truths at\nsufficiently fast rate (e.g., van der Laan, 2014; Farrell, 2015; Belloni et\nal., 2016). In this article, we extend this work in order to retain valid\n(uniform) inference when one of these estimators does not converge to the\ntruth, regardless of which. This is done by generalising prior work for\nlow-dimensional settings by Vermeulen and Vansteelandt (2015) to incorporate\nregularisation. The proposed penalised bias-reduced double-robust estimation\nstrategy exhibits promising performance in extensive simulation studies and a\ndata analysis, relative to competing proposals.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 15:07:37 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Avagyan", "Vahe", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1708.03796", "submitter": "ZhiMin Xiao", "authors": "ZhiMin Xiao, Steve Higgins, Adetayo Kasim", "title": "An Empirical Unravelling of Lord's Paradox", "comments": "23 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lord's Paradox occurs when a continuous covariate is statistically controlled\nfor and the relationship between a continuous outcome and group status\nindicator changes in both magnitude and direction. This phenomenon poses a\nchallenge to the notion of evidence-based policy, where data are supposed to be\nself-evident. We examined 50 effect size estimates from 34 large-scale\neducational interventions, and found that impact estimates are affected in\nmagnitude, with or without reversal in sign, when there is substantial baseline\nimbalance. We also demonstrated that multilevel modelling can ameliorate the\ndivergence in sign and/or magnitude of effect estimation, which, together with\nproject specific knowledge, promises to help those who are presented with\nconflicting or confusing evidence in decision making.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 17:04:39 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Xiao", "ZhiMin", ""], ["Higgins", "Steve", ""], ["Kasim", "Adetayo", ""]]}, {"id": "1708.03818", "submitter": "Nilanjan Chatterjee", "authors": "Prosenjit Kundu and Runlong Tang and Nilanjan Chatterjee", "title": "Generalized Meta-Analysis for Multiple Regression Models Across Studies\n  with Disparate Covariate Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis, because of both logistical convenience and statistical\nefficiency, is widely popular for synthesizing information on common parameters\nof interest across multiple studies. We propose developing a generalized\nmeta-analysis approach for combining information on multivariate regression\nparameters across multiple different studies which have varying level of\ncovariate information. Using algebraic relationships between regression\nparameters in different dimensions, we specify a set of moment equations for\nestimating parameters of a maximal model through information available from\nsets of parameter estimates from a series of reduced models available from the\ndifferent studies. The specification of the equations requires a reference\ndataset to estimate the joint distribution of the covariates. We propose to\nsolve these equations using the generalized method of moments approach, with\nthe optimal weighting of the equations taking into account uncertainty\nassociated with estimates of the parameters of the reduced models. We describe\nextensions of the iterated reweighted least square algorithm for fitting\ngeneralized linear regression models using the proposed framework. Based on the\nsame moment equations, we also propose a diagnostic test for detecting\nviolation of underlying model assumptions, such as those arising due to\nheterogeneity in the underlying study populations. Methods are illustrated\nusing extensive simulation studies and a real data example involving the\ndevelopment of a breast cancer risk prediction model using disparate risk\nfactor information from multiple studies.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 20:07:44 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 02:59:49 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kundu", "Prosenjit", ""], ["Tang", "Runlong", ""], ["Chatterjee", "Nilanjan", ""]]}, {"id": "1708.03992", "submitter": "Yuta Koike", "authors": "Takaki Hayashi, Yuta Koike", "title": "Multi-scale analysis of lead-lag relationships in high-frequency\n  financial markets", "comments": "30 pages, 1 figure. Theoretical results have been improved. Empirical\n  application has been updated (there was a minor data manipulation issue in v3\n  and it has been fixed in this version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel estimation procedure for scale-by-scale lead-lag\nrelationships of financial assets observed at high-frequency in a\nnon-synchronous manner. The proposed estimation procedure does not require any\ninterpolation processing of original datasets and is applicable to those with\nhighest time resolution available. Consistency of the proposed estimators is\nshown under the continuous-time framework that has been developed in our\nprevious work Hayashi and Koike (2018). An empirical application to a quote\ndataset of the NASDAQ-100 assets identifies two types of lead-lag relationships\nat different time scales.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 02:32:43 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 13:44:29 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 12:19:00 GMT"}, {"version": "v4", "created": "Fri, 8 May 2020 15:00:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Hayashi", "Takaki", ""], ["Koike", "Yuta", ""]]}, {"id": "1708.04060", "submitter": "Zhana Kuncheva", "authors": "Zhana Kuncheva and Giovanni Montana", "title": "Multi-scale Community Detection in Temporal Networks Using Spectral\n  Graph Wavelets", "comments": "22 pages, 8 figures, Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral graph wavelets introduce a notion of scale in networks, and are thus\nused to obtain a local view of the network from each node. By carefully\nconstructing a wavelet filter function for these wavelets, a multi-scale\ncommunity detection method for monoplex networks has already been developed.\nThis construction takes advantage of the partitioning properties of the network\nLaplacian. In this paper we elaborate on a novel method which uses spectral\ngraph wavelets to detect multi-scale communities in temporal networks. To do\nthis we extend the definition of spectral graph wavelets to temporal networks\nby adopting a multilayer framework. We use arguments from Perturbation Theory\nto investigate the spectral properties of the supra-Laplacian matrix for\nclustering purposes in temporal networks. Using these properties, we construct\na new wavelet filter function, which attenuates the influence of uninformative\neigenvalues and centres the filter around eigenvalues which contain information\non the coarsest description of prevalent community structures over time. We use\nthe spectral graph wavelets as feature vectors in a connectivity-constrained\nclustering procedure to detect multi-scale communities at different scales, and\nrefer to this method as Temporal Multi-Scale Community Detection (TMSCD). We\nvalidate the performance of TMSCD and a competing methodology on various\nbenchmarks. The advantage of TMSCD is the automated selection of relevant\nscales at which communities should be sought.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 10:14:54 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Kuncheva", "Zhana", ""], ["Montana", "Giovanni", ""]]}, {"id": "1708.04142", "submitter": "Sijia Xiang", "authors": "Sijia Xiang and Weixin Yao", "title": "Semiparametric Mixtures of Regressions with Single-index for Model Based\n  Clustering", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.06610", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose two classes of semiparametric mixture regression\nmodels with single-index for model based clustering. Unlike many\nsemiparametric/nonparametric mixture regression models that can only be applied\nto low dimensional predictors, the new semiparametric models can easily\nincorporate high dimensional predictors into the nonparametric components. The\nproposed models are very general, and many of the recently proposed\nsemiparametric/nonparametric mixture regression models are indeed special cases\nof the new models. Backfitting estimates and the corresponding modified EM\nalgorithms are proposed to achieve optimal convergence rates for both\nparametric and nonparametric parts. We establish the identifiability results of\nthe proposed two models and investigate the asymptotic properties of the\nproposed estimation procedures. Simulation studies are conducted to demonstrate\nthe finite sample performance of the proposed models. An application of NBA\ndata by new models reveals some new findings.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 02:07:51 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Xiang", "Sijia", ""], ["Yao", "Weixin", ""]]}, {"id": "1708.04221", "submitter": "Axel Finke", "authors": "Axel Finke, Ruth King, Alexandros Beskos, Petros Dellaportas", "title": "Efficient sequential Monte Carlo algorithms for integrated population\n  models", "comments": "includes supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models are commonly used to describe different forms of\necological data. We consider the case of count data with observation errors.\nFor such data the system process is typically multi-dimensional consisting of\ncoupled Markov processes, where each component corresponds to a different\ncharacterisation of the population, such as age group, gender or breeding\nstatus. The associated system process equations describe the biological\nmechanisms under which the system evolves over time. However, there is often\nlimited information in the count data alone to sensibly estimate demographic\nparameters of interest, so these are often combined with additional ecological\nobservations leading to an integrated data analysis. Unfortunately, fitting\nthese models to the data can be challenging, especially if the state-space\nmodel for the count data is non-linear or non-Gaussian. We propose an efficient\nparticle Markov chain Monte Carlo algorithm to estimate the demographic\nparameters without the need for resorting to linear or Gaussian approximations.\nIn particular, we exploit the integrated model structure to enhance the\nefficiency of the algorithm. We then incorporate the algorithm into a\nsequential Monte Carlo sampler in order to perform model comparison with\nregards to the dependence structure of the demographic parameters. Finally, we\ndemonstrate the applicability and computational efficiency of our algorithms on\ntwo real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 17:37:41 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Finke", "Axel", ""], ["King", "Ruth", ""], ["Beskos", "Alexandros", ""], ["Dellaportas", "Petros", ""]]}, {"id": "1708.04303", "submitter": "Paul Constantine", "authors": "Paul G. Constantine, Zachary del Rosario and Gianluca Iaccarino", "title": "Data-driven dimensional analysis: algorithms for unique and relevant\n  dimensionless groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical dimensional analysis has two limitations: (i) the computed\ndimensionless groups are not unique, and (ii) the analysis does not measure\nrelative importance of the dimensionless groups. We propose two algorithms for\nestimating unique and relevant dimensionless groups assuming the experimenter\ncan control the system's independent variables and evaluate the corresponding\ndependent variable; e.g., computer experiments provide such a setting. The\nfirst algorithm is based on a response surface constructed from a set of\nexperiments. The second algorithm uses many experiments to estimate finite\ndifferences over a range of the independent variables. Both algorithms are\nsemi-empirical because they use experimental data to complement the dimensional\nanalysis. We derive the algorithms by combining classical semi-empirical\nmodeling with active subspaces, which---given a probability density on the\nindependent variables---yield unique and relevant dimensionless groups. The\nconnection between active subspaces and dimensional analysis also reveals that\nall empirical models are ridge functions, which are functions that are constant\nalong low-dimensional subspaces in its domain. We demonstrate the proposed\nalgorithms on the well-studied example of viscous pipe flow---both turbulent\nand laminar cases. The results include a new set of two dimensionless groups\nfor turbulent pipe flow that are ordered by relevance to the system; the\nprecise notion of relevance is closely tied to the derivative based global\nsensitivity metric from Sobol' and Kucherenko.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 20:11:27 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Constantine", "Paul G.", ""], ["del Rosario", "Zachary", ""], ["Iaccarino", "Gianluca", ""]]}, {"id": "1708.04377", "submitter": "Vivekananda Roy", "authors": "Arnab Kumar Laha, Somak Dutta and Vivekananda Roy", "title": "A novel sandwich algorithm for empirical Bayes analysis of rank data", "comments": null, "journal-ref": null, "doi": "10.4310/SII.2017.v10.n4.a2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank data arises frequently in marketing, finance, organizational behavior,\nand psychology. Most analysis of rank data reported in the literature assumes\nthe presence of one or more variables (sometimes latent) based on whose values\nthe items are ranked. In this paper we analyze rank data using a purely\nprobabilistic model where the observed ranks are assumed to be perturbed\nversions of the true rank and each perturbation has a specific probability of\noccurring. We consider the general case when covariate information is present\nand has an impact on the rankings. An empirical Bayes approach is taken for\nestimating the model parameters. The Gibbs sampler is shown to converge very\nslowly to the target posterior distribution and we show that some of the widely\nused empirical convergence diagnostic tools may fail to detect this lack of\nconvergence. We propose a novel, fast mixing sandwich algorithm for exploring\nthe posterior distribution. An EM algorithm based on Markov chain Monte Carlo\n(MCMC) sampling is developed for estimating prior hyperparameters. A real life\nrank data set is analyzed using the methods developed in the paper. The results\nobtained indicate the usefulness of these methods in analyzing rank data with\ncovariate information.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 01:37:43 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Laha", "Arnab Kumar", ""], ["Dutta", "Somak", ""], ["Roy", "Vivekananda", ""]]}, {"id": "1708.04490", "submitter": "Giles Hooker", "authors": "David Sinclair and Giles Hooker", "title": "Sparse Inverse Covariance Estimation for High-throughput microRNA\n  Sequencing Data in the Poisson Log-Normal Graphical Model", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Poisson Log-Normal Graphical Model for count data, and\npresent a normality transformation for data arising from this distribution. The\nmodel and transformation are feasible for high-throughput microRNA (miRNA)\nsequencing data and directly account for known overdispersion relationships\npresent in this data set. The model allows for network dependencies to be\nmodeled, and we provide an algorithm which utilizes a one-step EM based result\nin order to allow for a provable increase in performance in determining the\nnetwork structure. The model is shown to provide an increase in performance in\nsimulation settings over a range of network structures. The model is applied to\nhigh-throughput miRNA sequencing data from patients with breast cancer from The\nCancer Genome Atlas (TCGA). By selecting the most highly connected miRNA\nmolecules in the fitted network we find that nearly all of them are known to be\ninvolved in the regulation of breast cancer.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 13:32:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Sinclair", "David", ""], ["Hooker", "Giles", ""]]}, {"id": "1708.04527", "submitter": "Martin Copenhaver", "authors": "Dimitris Bertsimas, Martin S. Copenhaver, and Rahul Mazumder", "title": "The Trimmed Lasso: Sparsity and Robustness", "comments": "32 pages (excluding appendix); 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex penalty methods for sparse modeling in linear regression have been\na topic of fervent interest in recent years. Herein, we study a family of\nnonconvex penalty functions that we call the trimmed Lasso and that offers\nexact control over the desired level of sparsity of estimators. We analyze its\nstructural properties and in doing so show the following:\n  1) Drawing parallels between robust statistics and robust optimization, we\nshow that the trimmed-Lasso-regularized least squares problem can be viewed as\na generalized form of total least squares under a specific model of\nuncertainty. In contrast, this same model of uncertainty, viewed instead\nthrough a robust optimization lens, leads to the convex SLOPE (or OWL) penalty.\n  2) Further, in relating the trimmed Lasso to commonly used sparsity-inducing\npenalty functions, we provide a succinct characterization of the connection\nbetween trimmed-Lasso- like approaches and penalty functions that are\ncoordinate-wise separable, showing that the trimmed penalties subsume existing\ncoordinate-wise separable penalties, with strict containment in general.\n  3) Finally, we describe a variety of exact and heuristic algorithms, both\nexisting and new, for trimmed Lasso regularized estimation problems. We include\na comparison between the different approaches and an accompanying\nimplementation of the algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 14:56:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Copenhaver", "Martin S.", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1708.04658", "submitter": "David Kaplan", "authors": "Matt Goldman, David M. Kaplan", "title": "Comparing distributions by multiple testing across quantiles or CDF\n  values", "comments": "under review", "journal-ref": "Journal of Econometrics 206 (2018) 143-166", "doi": "10.1016/j.jeconom.2018.04.003", "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When comparing two distributions, it is often helpful to learn at which\nquantiles or values there is a statistically significant difference. This\nprovides more information than the binary \"reject\" or \"do not reject\" decision\nof a global goodness-of-fit test. Framing our question as multiple testing\nacross the continuum of quantiles $\\tau\\in(0,1)$ or values $r\\in\\mathbb{R}$, we\nshow that the Kolmogorov--Smirnov test (interpreted as a multiple testing\nprocedure) achieves strong control of the familywise error rate. However, its\nwell-known flaw of low sensitivity in the tails remains. We provide an\nalternative method that retains such strong control of familywise error rate\nwhile also having even sensitivity, i.e., equal pointwise type I error rates at\neach of $n\\to\\infty$ order statistics across the distribution. Our one-sample\nmethod computes instantly, using our new formula that also instantly computes\ngoodness-of-fit $p$-values and uniform confidence bands. To improve power, we\nalso propose stepdown and pre-test procedures that maintain control of the\nasymptotic familywise error rate. One-sample and two-sample cases are\nconsidered, as well as extensions to regression discontinuity designs and\nconditional distributions. Simulations, empirical examples, and code are\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 19:30:50 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Goldman", "Matt", ""], ["Kaplan", "David M.", ""]]}, {"id": "1708.04705", "submitter": "Ezequiel Smucler", "authors": "Daniel Pe\\~na, Ezequiel Smucler, Victor J. Yohai", "title": "Forecasting Multiple Time Series with One-Sided Dynamic Principal\n  Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define one-sided dynamic principal components (ODPC) for time series as\nlinear combinations of the present and past values of the series that minimize\nthe reconstruction mean squared error. Previous definitions of dynamic\nprincipal components depend on past and future values of the series. For this\nreason, they are not appropriate for forecasting purposes. On the contrary, it\nis shown that the ODPC introduced in this paper can be successfully used for\nforecasting high-dimensional multiple time series. An alternating least squares\nalgorithm to compute the proposed ODPC is presented. We prove that for\nstationary and ergodic time series the estimated values converge to their\npopulation analogues. We also prove that asymptotically, when both the number\nof series and the sample size go to infinity, if the data follows a dynamic\nfactor model, the reconstruction obtained with ODPC converges, in mean squared\nerror, to the common part of the factor model. Monte Carlo results shows that\nforecasts obtained by the ODPC compare favourably with other forecasting\nmethods based on dynamic factor models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 22:16:35 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Pe\u00f1a", "Daniel", ""], ["Smucler", "Ezequiel", ""], ["Yohai", "Victor J.", ""]]}, {"id": "1708.04792", "submitter": "Marcio Diniz", "authors": "M\\'arcio Augusto Diniz, Mourad Tighiouart and Andr\\'e Rogatko", "title": "Comparison between continuous and discrete doses using Escalation With\n  Overdose Control", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is an extensive statistical literature showing the\ndisadvantages of discretizing continuous variables, categorization is a common\npractice in clinical research which results in substantial loss of information.\nA large collection of methods in cancer phase I clinical trial design\nestablishes dose of a new agent as a discrete variable. A noteworthy exception\nis the Escalation With Overdose Control (EWOC) design, where doses can be\ndefined either as continuous or as a grid of discrete doses. A Monte Carlo\nsimulation study was performed to compare the operating characteristics of\ncontinuous and discrete dose EWOC designs. Four equally spaced grids with\ndifferent interval lengths were considered. The loss of information was\nmeasured by several operating characteristics easier for clinicians to\ninterpret, in addition to the usual statistical measures of bias and mean\nsquared error. Based on the simulations, if there is not enough knowledge about\nthe true MTD value as commonly happens in phase I clinical trials, continuous\ndose scheme arises as an attractive option.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 07:08:57 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Diniz", "M\u00e1rcio Augusto", ""], ["Tighiouart", "Mourad", ""], ["Rogatko", "Andr\u00e9", ""]]}, {"id": "1708.04815", "submitter": "L\\'aszl\\'o N\\'emeth", "authors": "L\\'aszl\\'o N\\'emeth and Andr\\'as Zempl\\'eni", "title": "Regression estimator for the tail index", "comments": "15 pages, 4 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the tail index parameter is one of the primal objectives in\nextreme value theory. For heavy-tailed distributions the Hill estimator is the\nmost popular way to estimate the tail index parameter. Improving the Hill\nestimator was aimed by recent works with different methods, for example by\nusing bootstrap, or Kolmogorov-Smirnov metric. These methods are asymptotically\nconsistent, but for tail index $\\xi >1$ and smaller sample sizes the estimation\nfails to approach the theoretical value for realistic sample sizes. In this\npaper, we introduce a new empirical method, which can estimate high tail index\nparameters well and might also be useful for relatively small sample sizes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:56:48 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 07:16:35 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 08:45:24 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["N\u00e9meth", "L\u00e1szl\u00f3", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1708.04887", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Gerda Claeskens, Thomas Gueuning", "title": "Fixed effects testing in high-dimensional linear mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering challenges -- ranging from pharmacokinetic\ndrug dosage allocation and personalized medicine to marketing mix (4Ps)\nrecommendations -- require an understanding of the unobserved heterogeneity in\norder to develop the best decision making-processes. In this paper, we develop\na hypothesis test and the corresponding p-value for testing for the\nsignificance of the homogeneous structure in linear mixed models. A robust\nmatching moment construction is used for creating a test that adapts to the\nsize of the model sparsity. When unobserved heterogeneity at a cluster level is\nconstant, we show that our test is both consistent and unbiased even when the\ndimension of the model is extremely high. Our theoretical results rely on a new\nfamily of adaptive sparse estimators of the fixed effects that do not require\nconsistent estimation of the random effects. Moreover, our inference results do\nnot require consistent model selection. We showcase that moment matching can be\nextended to nonlinear mixed effects models and to generalized linear mixed\neffects models. In numerical and real data experiments, we find that the\ndeveloped method is extremely accurate, that it adapts to the size of the\nunderlying model and is decidedly powerful in the presence of irrelevant\ncovariates.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 21:48:46 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Bradic", "Jelena", ""], ["Claeskens", "Gerda", ""], ["Gueuning", "Thomas", ""]]}, {"id": "1708.04891", "submitter": "Karthik Bharath", "authors": "Karthik Bharath and Sebastian Kurtek", "title": "Distribution on Warp Maps for Alignment of Open and Closed Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alignment of curve data is an integral part of their statistical analysis,\nand can be achieved using model- or optimization-based approaches. The\nparameter space is usually the set of monotone, continuous warp maps of a\ndomain. Infinite-dimensional nature of the parameter space encourages sampling\nbased approaches, which require a distribution on the set of warp maps.\nMoreover, the distribution should also enable sampling in the presence of\nimportant landmark information on the curves which constrain the warp maps. For\nalignment of closed and open curves in $\\mathbb{R}^d, d=1,2,3$, possibly with\nlandmark information, we provide a constructive, point-process based definition\nof a distribution on the set of warp maps of $[0,1]$ and the unit circle\n$\\mathbb{S}^1$ that is (1) simple to sample from, and (2) possesses the\ndesiderata for decomposition of the alignment problem with landmark constraints\ninto multiple unconstrained ones. For warp maps on $[0,1]$, the distribution is\nrelated to the Dirichlet process. We demonstrate its utility by using it as a\nprior distribution on warp maps in a Bayesian model for alignment of two\nunivariate curves, and as a proposal distribution in a stochastic algorithm\nthat optimizes a suitable alignment functional for higher-dimensional curves.\nSeveral examples from simulated and real datasets are provided.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 13:41:03 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 16:12:18 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Bharath", "Karthik", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1708.04929", "submitter": "Jenny Shi", "authors": "W. Jenny Shi, Jan Hannig, Randy C.S. Lai, Thomas C.M. Lee", "title": "Covariance Estimation via Fiducial Inference", "comments": "31 pages with 5 figures, including appendix; 1 supplementary document\n  with 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a classical problem, covariance estimation has drawn much attention from\nthe statistical community for decades. Much work has been done under the\nfrequentist and the Bayesian frameworks. Aiming to quantify the uncertainty of\nthe estimators without having to choose a prior, we have developed a fiducial\napproach to the estimation of covariance matrix. Built upon the Fiducial\nBerstein-von Mises Theorem (Sonderegger and Hannig 2014), we show that the\nfiducial distribution of the covariate matrix is consistent under our\nframework. Consequently, the samples generated from this fiducial distribution\nare good estimators to the true covariance matrix, which enable us to define a\nmeaningful confidence region for the covariance matrix. Lastly, we also show\nthat the fiducial approach can be a powerful tool for identifying clique\nstructures in covariance matrices.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 15:20:01 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Shi", "W. Jenny", ""], ["Hannig", "Jan", ""], ["Lai", "Randy C. S.", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1708.04981", "submitter": "Sungkyu Jung", "authors": "Sungkyu Jung, Myung Hee Lee, Jeongyoun Ahn", "title": "On the number of principal components in high dimensions", "comments": null, "journal-ref": "Biometrika 105 (2018) 389-402", "doi": "10.1093/biomet/asy010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of how many components to retain in the application\nof principal component analysis when the dimension is much higher than the\nnumber of observations. To estimate the number of components, we propose to\nsequentially test skewness of the squared lengths of residual scores that are\nobtained by removing leading principal components. The residual lengths are\nasymptotically left-skewed if all principal components with diverging variances\nare removed, and right-skewed if not. The proposed estimator is shown to be\nconsistent, performs well in high-dimensional simulation studies, and provides\nreasonable estimates in a number of real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:13:13 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Jung", "Sungkyu", ""], ["Lee", "Myung Hee", ""], ["Ahn", "Jeongyoun", ""]]}, {"id": "1708.05017", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Adrian Dobra", "title": "Measuring Human Activity Spaces from GPS Data with Density Ranking and\n  Summary Curves", "comments": "45 pages, 13 figures. Add a mixture model for GPS data, 4 new\n  theorems, and a simulation study", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity spaces are fundamental to the assessment of individuals' dynamic\nexposure to social and environmental risk factors associated with multiple\nspatial contexts that are visited during activities of daily living. In this\npaper we survey existing approaches for measuring the geometry, size and\nstructure of activity spaces based on GPS data, and explain their limitations.\nWe propose addressing these shortcomings through a nonparametric approach\ncalled density ranking, and also through three summary curves: the mass-volume\ncurve, the Betti number curve, and the persistence curve. We introduce a novel\nmixture model for human activity spaces, and study its asymptotic properties.\nWe prove that the kernel density estimator which, at the present time, is one\nof the most widespread methods for measuring activity spaces is not a stable\nestimator of their structure. We illustrate the practical value of our methods\nwith a simulation study, and with a recently collected GPS dataset that\ncomprises the locations visited by ten individuals over a six months period.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:01:03 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 22:39:13 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Dobra", "Adrian", ""]]}, {"id": "1708.05037", "submitter": "Simon Vandekar", "authors": "Simon N. Vandekar, Theodore D. Satterthwaite, Adon Rosen, Rastko\n  Ciric, David R. Roalf, Kosha Ruparel, Ruben C. Gur, Raquel E. Gur, and\n  Russell T. Shinohara", "title": "Faster Family-wise Error Control for Neuroimaging with a Parametric\n  Bootstrap", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxx051", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroimaging, hundreds to hundreds of thousands of tests are performed\nacross a set of brain regions or all locations in an image. Recent studies have\nshown that the most common family-wise error (FWE) controlling procedures in\nimaging, which rely on classical mathematical inequalities or Gaussian random\nfield theory, yield FWE rates that are far from the nominal level. Depending on\nthe approach used, the FWER can be exceedingly small or grossly inflated. Given\nthe widespread use of neuroimaging as a tool for understanding neurological and\npsychiatric disorders, it is imperative that reliable multiple testing\nprocedures are available. To our knowledge, only permutation joint testing\nprocedures have been shown to reliably control the FWER at the nominal level.\nHowever, these procedures are computationally intensive due to the increasingly\navailable large sample sizes and dimensionality of the images, and analyses can\ntake days to complete. Here, we develop a parametric bootstrap joint testing\nprocedure. The parametric bootstrap procedure works directly with the test\nstatistics, which leads to much faster estimation of adjusted \\emph{p}-values\nthan resampling-based procedures while reliably controlling the FWER in sample\nsizes available in many neuroimaging studies. We demonstrate that the procedure\ncontrols the FWER in finite samples using simulations, and present region- and\nvoxel-wise analyses to test for sex differences in developmental trajectories\nof cerebral blood flow.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:52:35 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 14:33:55 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Vandekar", "Simon N.", ""], ["Satterthwaite", "Theodore D.", ""], ["Rosen", "Adon", ""], ["Ciric", "Rastko", ""], ["Roalf", "David R.", ""], ["Ruparel", "Kosha", ""], ["Gur", "Ruben C.", ""], ["Gur", "Raquel E.", ""], ["Shinohara", "Russell T.", ""]]}, {"id": "1708.05069", "submitter": "Joshua Brul\\'e", "authors": "Joshua Brul\\'e", "title": "A causation coefficient and taxonomy of correlation/causation\n  relationships", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a causation coefficient which is defined in terms of\nprobabilistic causal models. This coefficient is suggested as the natural\ncausal analogue of the Pearson correlation coefficient and permits comparing\ncausation and correlation to each other in a simple, yet rigorous manner.\nTogether, these coefficients provide a natural way to classify the possible\ncorrelation/causation relationships that can occur in practice and examples of\neach relationship are provided. In addition, the typical relationship between\ncorrelation and causation is analyzed to provide insight into why correlation\nand causation are often conflated. Finally, example calculations of the\ncausation coefficient are shown on a real data set.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 05:47:33 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Brul\u00e9", "Joshua", ""]]}, {"id": "1708.05084", "submitter": "Zhiguang Huo", "authors": "Zhiguang Huo, Shaowu Tang, Yongseok Park and George Tseng", "title": "P-value evaluation, variability index and biomarker categorization for\n  adaptively weighted Fisher's meta-analysis method in omics applications", "comments": "adaptive weights, Fisher's method, meta-analysis, differential\n  expression analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis methods have been widely used to combine results from multiple\nclinical or genomic studies to increase statistical power and ensure robust and\naccurate conclusion. Adaptively weighted Fisher's method (AW-Fisher) is an\neffective approach to combine p-values from $K$ independent studies and to\nprovide better biological interpretation by characterizing which studies\ncontribute to meta-analysis. Currently, AW-Fisher suffers from lack of fast,\naccurate p-value computation and variability estimate of AW weights. When the\nnumber of studies $K$ is large, the $3^K - 1$ possible differential expression\npattern categories can become intractable. In this paper, we apply an\nimportance sampling technique with spline interpolation to increase accuracy\nand speed of p-value calculation. Using resampling techniques, we propose a\nvariability index for the AW weight estimator and a co-membership matrix to\ncharacterize pattern similarities between genes. The co-membership matrix is\nfurther used to categorize differentially expressed genes based on their\nmeta-patterns for further biological investigation. The superior performance of\nthe proposed methods is shown in simulations. These methods are also applied to\ntwo real applications to demonstrate intriguing biological findings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 21:02:22 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Huo", "Zhiguang", ""], ["Tang", "Shaowu", ""], ["Park", "Yongseok", ""], ["Tseng", "George", ""]]}, {"id": "1708.05239", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Fredrik Lindsten, Maurizio Filippone and James\n  Hensman", "title": "Pseudo-extended Markov chain Monte Carlo", "comments": "Advances in Neural Information Processing Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from posterior distributions using Markov chain Monte Carlo (MCMC)\nmethods can require an exhaustive number of iterations, particularly when the\nposterior is multi-modal as the MCMC sampler can become trapped in a local mode\nfor a large number of iterations. In this paper, we introduce the\npseudo-extended MCMC method as a simple approach for improving the mixing of\nthe MCMC sampler for multi-modal posterior distributions. The pseudo-extended\nmethod augments the state-space of the posterior using pseudo-samples as\nauxiliary variables. On the extended space, the modes of the posterior are\nconnected, which allows the MCMC sampler to easily move between well-separated\nposterior modes. We demonstrate that the pseudo-extended approach delivers\nimproved MCMC sampling over the Hamiltonian Monte Carlo algorithm on\nmulti-modal posteriors, including Boltzmann machines and models with\nsparsity-inducing priors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:45:07 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 12:03:37 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 17:13:57 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Nemeth", "Christopher", ""], ["Lindsten", "Fredrik", ""], ["Filippone", "Maurizio", ""], ["Hensman", "James", ""]]}, {"id": "1708.05248", "submitter": "Anne van Delft Dr.", "authors": "Anne van Delft, Vaidotas Characiejus, Holger Dette", "title": "A nonparametric test for stationarity in functional time series", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202018.0320", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new measure for stationarity of a functional time series, which\nis based on an explicit representation of the $L^2$-distance between the\nspectral density operator of a non-stationary process and its best\n($L^2$-)approximation by a spectral density operator corresponding to a\nstationary process. This distance can easily be estimated by sums of\nHilbert-Schmidt inner products of periodogram operators (evaluated at different\nfrequencies), and asymptotic normality of an appropriately standardized version\nof the estimator can be established for the corresponding estimate under the\nnull hypothesis and alternative. As a result we obtain a simple asymptotic\nfrequency domain level $\\alpha$ test (using the quantiles of the normal\ndistribution) for the hypothesis of stationarity of functional time series.\nOther applications such as asymptotic confidence intervals for a measure of\nstationarity or the construction of tests for \"relevant deviations from\nstationarity\", are also briefly mentioned. We demonstrate in a small simulation\nstudy that the new method has very good finite sample properties. Moreover, we\napply our test to annual temperature curves.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:56:15 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 16:21:21 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["van Delft", "Anne", ""], ["Characiejus", "Vaidotas", ""], ["Dette", "Holger", ""]]}, {"id": "1708.05250", "submitter": "Philipp Frank", "authors": "Philipp Frank, Theo Steininger and Torsten A. En{\\ss}lin", "title": "Field dynamics inference via spectral density estimation", "comments": "12 pages, 9 figures", "journal-ref": "Phys. Rev. E 96, 052104 (2017)", "doi": "10.1103/PhysRevE.96.052104", "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) are of utmost importance in various\nscientific and industrial areas. They are the natural description of dynamical\nprocesses whose precise equations of motion are either not known or too\nexpensive to solve, e.g., when modeling Brownian motion. In some cases, the\nequations governing the dynamics of a physical system on macroscopic scales\noccur to be unknown since they typically cannot be deduced from general\nprinciples. In this work, we describe how the underlying laws of a stochastic\nprocess can be approximated by the spectral density of the corresponding\nprocess. Furthermore, we show how the density can be inferred from possibly\nvery noisy and incomplete measurements of the dynamical field. Generally,\ninverse problems like these can be tackled with the help of Information Field\nTheory (IFT). For now, we restrict to linear and autonomous processes. Though,\nthis is a non-conceptual limitation that may be omitted in future work. To\ndemonstrate its applicability we employ our reconstruction algorithm on a\ntime-series and spatio-temporal processes.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:57:55 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Frank", "Philipp", ""], ["Steininger", "Theo", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1708.05254", "submitter": "Bharath Sriperumbudur", "authors": "Ingo Steinwart, Bharath K. Sriperumbudur, Philipp Thomann", "title": "Adaptive Clustering Using Kernel Density Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive and analyze a generic, recursive algorithm for estimating all\nsplits in a finite cluster tree as well as the corresponding clusters. We\nfurther investigate statistical properties of this generic clustering algorithm\nwhen it receives level set estimates from a kernel density estimator. In\nparticular, we derive finite sample guarantees, consistency, rates of\nconvergence, and an adaptive data-driven strategy for choosing the kernel\nbandwidth. For these results we do not need continuity assumptions on the\ndensity such as H\\\"{o}lder continuity, but only require intuitive geometric\nassumptions of non-parametric nature.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 13:19:16 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 02:19:24 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Steinwart", "Ingo", ""], ["Sriperumbudur", "Bharath K.", ""], ["Thomann", "Philipp", ""]]}, {"id": "1708.05341", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos and Fabrizio Leisen", "title": "An Approximate Likelihood Perspective on ABC Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are living in the big data era, as current technologies and networks allow\nfor the easy and routine collection of data sets in different disciplines.\nBayesian Statistics offers a flexible modeling approach which is attractive for\ndescribing the complexity of these datasets. These models often exhibit a\nlikelihood function which is intractable due to the large sample size, high\nnumber of parameters, or functional complexity. Approximate Bayesian\nComputational (ABC) methods provides likelihood-free methods for performing\nstatistical inferences with Bayesian models defined by intractable likelihood\nfunctions. The vastity of the literature on ABC methods created a need to\nreview and relate all ABC approaches so that scientists can more readily\nunderstand and apply them for their own work. This article provides a unifying\nreview, general representation, and classification of all ABC methods from the\nview of approximate likelihood theory. This clarifies how ABC methods can be\ncharacterized, related, combined, improved, and applied for future research.\nPossible future research in ABC is then suggested.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 15:52:22 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 12:49:18 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Karabatsos", "George", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1708.05439", "submitter": "Shaobo Li", "authors": "Yichen Qin, Shaobo Li, Yang Li and Yan Yu", "title": "Penalized Maximum Tangent Likelihood Estimation and Robust Variable\n  Selection", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of mean regression estimators -- penalized maximum\ntangent likelihood estimation -- for high-dimensional regression estimation and\nvariable selection. We first explain the motivations for the key ingredient,\nmaximum tangent likelihood estimation (MTE), and establish its asymptotic\nproperties. We further propose a penalized MTE for variable selection and show\nthat it is $\\sqrt{n}$-consistent, enjoys the oracle property. The proposed\nclass of estimators consists penalized $\\ell_2$ distance, penalized exponential\nsquared loss, penalized least trimmed square and penalized least square as\nspecial cases and can be regarded as a mixture of minimum Kullback-Leibler\ndistance estimation and minimum $\\ell_2$ distance estimation. Furthermore, we\nconsider the proposed class of estimators under the high-dimensional setting\nwhen the number of variables $d$ can grow exponentially with the sample size\n$n$, and show that the entire class of estimators (including the aforementioned\nspecial cases) can achieve the optimal rate of convergence in the order of\n$\\sqrt{\\ln(d)/n}$. Finally, simulation studies and real data analysis\ndemonstrate the advantages of the penalized MTE.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 20:51:01 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 20:59:55 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Qin", "Yichen", ""], ["Li", "Shaobo", ""], ["Li", "Yang", ""], ["Yu", "Yan", ""]]}, {"id": "1708.05445", "submitter": "Catia  Scricciolo", "authors": "Catia Scricciolo", "title": "Bayes and maximum likelihood for $L^1$-Wasserstein deconvolution of\n  Laplace mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a distribution function on the real\nline from observations additively contaminated with errors following the\nstandard Laplace distribution. Assuming that the latent distribution is\ncompletely unknown leads to a nonparametric deconvolution problem. We begin by\nstudying the rates of convergence relative to the $L^2$-norm and the Hellinger\nmetric for the direct problem of estimating the sampling density, which is a\nmixture of Laplace densities with a possibly unbounded set of locations: the\nrate of convergence for the Bayes' density estimator corresponding to a\nDirichlet process prior over the space of all mixing distributions on the real\nline matches, up to a logarithmic factor, with the $n^{-3/8}\\log^{1/8}n$ rate\nfor the maximum likelihood estimator. Then, appealing to an inversion\ninequality translating the $L^2$-norm and the Hellinger distance between\ngeneral kernel mixtures, with a kernel density having polynomially decaying\nFourier transform, into any $L^p$-Wasserstein distance, $p\\geq1$, between the\ncorresponding mixing distributions, provided their Laplace transforms are\nfinite in some neighborhood of zero, we derive the rates of convergence in the\n$L^1$-Wasserstein metric for the Bayes' and maximum likelihood estimators of\nthe mixing distribution. Merging in the $L^1$-Wasserstein distance between\nBayes and maximum likelihood follows as a by-product, along with an assessment\non the stochastic order of the discrepancy between the two estimation\nprocedures.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 21:28:38 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Scricciolo", "Catia", ""]]}, {"id": "1708.05508", "submitter": "Naim Rashid", "authors": "Naim U. Rashid, Quefeng Li, Jen Jen Yeh, and Joseph G. Ibrahim", "title": "Modeling Between-Study Heterogeneity for Improved Reproducibility in\n  Gene Signature Selection and Clinical Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the genomic era, the identification of gene signatures associated with\ndisease is of significant interest. Such signatures are often used to predict\nclinical outcomes in new patients and aid clinical decision-making. However,\nrecent studies have shown that gene signatures are often not replicable. This\noccurrence has practical implications regarding the generalizability and\nclinical applicability of such signatures. To improve replicability, we\nintroduce a novel approach to select gene signatures from multiple datasets\nwhose effects are consistently non-zero and account for between-study\nheterogeneity. We build our model upon some rank-based quantities, facilitating\nintegration over different genomic datasets. A high dimensional penalized\nGeneralized Linear Mixed Model (pGLMM) is used to select gene signatures and\naddress data heterogeneity. We compare our method to some commonly used\nstrategies that select gene signatures ignoring between-study heterogeneity. We\nprovide asymptotic results justifying the performance of our method and\ndemonstrate its advantage in the presence of heterogeneity through thorough\nsimulation studies. Lastly, we motivate our method through a case study\nsubtyping pancreatic cancer patients from four gene expression studies.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 04:40:06 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 16:37:58 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Rashid", "Naim U.", ""], ["Li", "Quefeng", ""], ["Yeh", "Jen Jen", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "1708.05573", "submitter": "Soumendu Sundar Mukherjee", "authors": "Soumendu Sundar Mukherjee, Purnamrita Sarkar, and Peter J. Bickel", "title": "Two provably consistent divide and conquer clustering algorithms for\n  large networks", "comments": "41 pages, comments are most welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we advance divide-and-conquer strategies for solving the\ncommunity detection problem in networks. We propose two algorithms which\nperform clustering on a number of small subgraphs and finally patches the\nresults into a single clustering. The main advantage of these algorithms is\nthat they bring down significantly the computational cost of traditional\nalgorithms, including spectral clustering, semi-definite programs, modularity\nbased methods, likelihood based methods etc., without losing on accuracy and\neven improving accuracy at times. These algorithms are also, by nature,\nparallelizable. Thus, exploiting the facts that most traditional algorithms are\naccurate and the corresponding optimization problems are much simpler in small\nproblems, our divide-and-conquer methods provide an omnibus recipe for scaling\ntraditional algorithms up to large networks. We prove consistency of these\nalgorithms under various subgraph selection procedures and perform extensive\nsimulations and real-data analysis to understand the advantages of the\ndivide-and-conquer approach in various settings.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 12:09:10 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Mukherjee", "Soumendu Sundar", ""], ["Sarkar", "Purnamrita", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1708.05653", "submitter": "Luca Weihs", "authors": "Luca Weihs, Mathias Drton, Nicolai Meinshausen", "title": "Symmetric Rank Covariances: a Generalised Framework for Nonparametric\n  Measures of Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to test whether two random vectors are independent has spawned a\nlarge number of competing measures of dependence. We are interested in\nnonparametric measures that are invariant under strictly increasing\ntransformations, such as Kendall's tau, Hoeffding's D, and the more recently\ndiscovered Bergsma--Dassios sign covariance. Each of these measures exhibits\nsymmetries that are not readily apparent from their definitions. Making these\nsymmetries explicit, we define a new class of multivariate nonparametric\nmeasures of dependence that we refer to as Symmetric Rank Covariances. This new\nclass generalises all of the above measures and leads naturally to multivariate\nextensions of the Bergsma--Dassios sign covariance. Symmetric Rank Covariances\nmay be estimated unbiasedly using U-statistics for which we prove results on\ncomputational efficiency and large-sample behavior. The algorithms we develop\nfor their computation include, to the best of our knowledge, the first\nefficient algorithms for the well-known Hoeffding's D statistic in the\nmultivariate setting.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 15:40:02 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Weihs", "Luca", ""], ["Drton", "Mathias", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1708.05879", "submitter": "Jiahe Lin", "authors": "Jiahe Lin and George Michailidis", "title": "Regularized Estimation and Testing for High-Dimensional Multi-Block\n  Vector-Autoregressive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems comprising of multiple components that can be partitioned\ninto distinct blocks originate in many scientific areas. A pertinent example is\nthe interactions between financial assets and selected macroeconomic\nindicators, which has been studied at aggregate level---e.g. a stock index and\nan employment index---extensively in the macroeconomics literature. A key\nshortcoming of this approach is that it ignores potential influences from other\nrelated components (e.g. Gross Domestic Product) that may exert influence on\nthe system's dynamics and structure and thus produces incorrect results. To\nmitigate this issue, we consider a multi-block linear dynamical system with\nGranger-causal ordering between blocks, wherein the blocks' temporal dynamics\nare described by vector autoregressive processes and are influenced by blocks\nhigher in the system hierarchy. We derive the maximum likelihood estimator for\nthe posited model for Gaussian data in the high-dimensional setting based on\nappropriate regularization schemes for the parameters of the block components.\nTo optimize the underlying non-convex likelihood function, we develop an\niterative algorithm with convergence guarantees. We establish theoretical\nproperties of the maximum likelihood estimates, leveraging the decomposability\nof the regularizers and a careful analysis of the iterates. Finally, we develop\ntesting procedures for the null hypothesis of whether a block \"Granger-causes\"\nanother block of variables. The performance of the model and the testing\nprocedures are evaluated on synthetic data, and illustrated on a data set\ninvolving log-returns of the US S&P100 component stocks and key macroeconomic\nvariables for the 2001--16 period.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 17:35:55 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Lin", "Jiahe", ""], ["Michailidis", "George", ""]]}, {"id": "1708.05894", "submitter": "Joseph Futoma", "authors": "Joseph Futoma, Sanjay Hariharan, Mark Sendak, Nathan Brajer, Meredith\n  Clement, Armando Bedoya, Cara O'Brien, Katherine Heller", "title": "An Improved Multi-Output Gaussian Process RNN with Real-Time Validation\n  for Early Sepsis Detection", "comments": "Presented at Machine Learning for Healthcare 2017, Boston, MA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a poorly understood and potentially life-threatening complication\nthat can occur as a result of infection. Early detection and treatment improves\npatient outcomes, and as such it poses an important challenge in medicine. In\nthis work, we develop a flexible classifier that leverages streaming lab\nresults, vitals, and medications to predict sepsis before it occurs. We model\npatient clinical time series with multi-output Gaussian processes, maintaining\nuncertainty about the physiological state of a patient while also imputing\nmissing values. The mean function takes into account the effects of medications\nadministered on the trajectories of the physiological variables. Latent\nfunction values from the Gaussian process are then fed into a deep recurrent\nneural network to classify patient encounters as septic or not, and the overall\nmodel is trained end-to-end using back-propagation. We train and validate our\nmodel on a large dataset of 18 months of heterogeneous inpatient stays from the\nDuke University Health System, and develop a new \"real-time\" validation scheme\nfor simulating the performance of our model as it will actually be used. Our\nproposed method substantially outperforms clinical baselines, and improves on a\nprevious related model for detecting sepsis. Our model's predictions will be\ndisplayed in a real-time analytics dashboard to be used by a sepsis rapid\nresponse team to help detect and improve treatment of sepsis.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 20:14:07 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Futoma", "Joseph", ""], ["Hariharan", "Sanjay", ""], ["Sendak", "Mark", ""], ["Brajer", "Nathan", ""], ["Clement", "Meredith", ""], ["Bedoya", "Armando", ""], ["O'Brien", "Cara", ""], ["Heller", "Katherine", ""]]}, {"id": "1708.05931", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui, Rolando J. Biscay, Jorge Bosch-Bayard,\n  Pascal Faber, Toshihiko Kinoshita, Kieko Kochi, Patricia Milz, Keiichiro\n  Nishida, Masafumi Yoshimura", "title": "Innovations orthogonalization: a solution to the major pitfalls of\n  EEG/MEG \"leakage correction\"", "comments": "preprint, technical report, under license\n  \"Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND\n  4.0)\", https://creativecommons.org/licenses/by-nc-nd/4.0/", "journal-ref": null, "doi": "10.1101/178657", "report-no": null, "categories": "stat.ME q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The problem of interest here is the study of brain functional and effective\nconnectivity based on non-invasive EEG-MEG inverse solution time series. These\nsignals generally have low spatial resolution, such that an estimated signal at\nany one site is an instantaneous linear mixture of the true, actual, unobserved\nsignals across all cortical sites. False connectivity can result from analysis\nof these low-resolution signals. Recent efforts toward \"unmixing\" have been\ndeveloped, under the name of \"leakage correction\". One recent noteworthy\napproach is that by Colclough et al (2015 NeuroImage, 117:439-448), which\nforces the inverse solution signals to have zero cross-correlation at lag zero.\nOne goal is to show that Colclough's method produces false human connectomes\nunder very broad conditions. The second major goal is to develop a new\nsolution, that appropriately \"unmixes\" the inverse solution signals, based on\ninnovations orthogonalization. The new method first fits a multivariate\nautoregression to the inverse solution signals, giving the mixed innovations.\nSecond, the mixed innovations are orthogonalized. Third, the mixed and\northogonalized innovations allow the estimation of the \"unmixing\" matrix, which\nis then finally used to \"unmix\" the inverse solution signals. It is shown that\nunder very broad conditions, the new method produces proper human connectomes,\neven when the signals are not generated by an autoregressive model.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 04:15:13 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 04:59:52 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""], ["Biscay", "Rolando J.", ""], ["Bosch-Bayard", "Jorge", ""], ["Faber", "Pascal", ""], ["Kinoshita", "Toshihiko", ""], ["Kochi", "Kieko", ""], ["Milz", "Patricia", ""], ["Nishida", "Keiichiro", ""], ["Yoshimura", "Masafumi", ""]]}, {"id": "1708.06302", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss and Joseph Guinness", "title": "A general framework for Vecchia approximations of Gaussian processes", "comments": null, "journal-ref": "Statistical Science, 36(1), 124-141 (2021)", "doi": "10.1214/19-STS755", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are commonly used as models for functions, time\nseries, and spatial fields, but they are computationally infeasible for large\ndatasets. Focusing on the typical setting of modeling data as a GP plus an\nadditive noise term, we propose a generalization of the Vecchia (1988) approach\nas a framework for GP approximations. We show that our general Vecchia approach\ncontains many popular existing GP approximations as special cases, allowing for\ncomparisons among the different methods within a unified framework.\nRepresenting the models by directed acyclic graphs, we determine the sparsity\nof the matrices necessary for inference, which leads to new insights regarding\nthe computational properties. Based on these results, we propose a novel sparse\ngeneral Vecchia approximation, which ensures computational feasibility for\nlarge spatial datasets but can lead to considerable improvements in\napproximation accuracy over Vecchia's original approach. We provide several\ntheoretical results and conduct numerical comparisons. We conclude with\nguidelines for the use of Vecchia approximations in spatial statistics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 16:03:04 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 16:07:51 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 20:45:48 GMT"}, {"version": "v4", "created": "Fri, 21 Dec 2018 20:52:12 GMT"}, {"version": "v5", "created": "Sat, 17 Aug 2019 15:25:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Guinness", "Joseph", ""]]}, {"id": "1708.06332", "submitter": "Fran\\c{c}ois Monard", "authors": "Fran\\c{c}ois Monard and Richard Nickl and Gabriel P. Paternain", "title": "Efficient Nonparametric Bayesian Inference For X-Ray Transforms", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical inverse problem of recovering a function $f: M\n\\to \\mathbb R$, where $M$ is a smooth compact Riemannian manifold with\nboundary, from measurements of general $X$-ray transforms $I_a(f)$ of $f$,\ncorrupted by additive Gaussian noise. For $M$ equal to the unit disk with\n`flat' geometry and $a=0$ this reduces to the standard Radon transform, but our\ngeneral setting allows for anisotropic media $M$ and can further model local\n`attenuation' effects -- both highly relevant in practical imaging problems\nsuch as SPECT tomography. We propose a nonparametric Bayesian inference\napproach based on standard Gaussian process priors for $f$. The posterior\nreconstruction of $f$ corresponds to a Tikhonov regulariser with a reproducing\nkernel Hilbert space norm penalty that does not require the calculation of the\nsingular value decomposition of the forward operator $I_a$. We prove\nBernstein-von Mises theorems that entail that posterior-based inferences such\nas credible sets are valid and optimal from a frequentist point of view for a\nlarge family of semi-parametric aspects of $f$. In particular we derive the\nasymptotic distribution of smooth linear functionals of the Tikhonov\nregulariser, which is shown to attain the semi-parametric Cram\\'er-Rao\ninformation bound. The proofs rely on an invertibility result for the `Fisher\ninformation' operator $I_a^*I_a$ between suitable function spaces, a result of\nindependent interest that relies on techniques from microlocal analysis. We\nillustrate the performance of the proposed method via simulations in various\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 17:33:30 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 21:44:42 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Monard", "Fran\u00e7ois", ""], ["Nickl", "Richard", ""], ["Paternain", "Gabriel P.", ""]]}, {"id": "1708.06337", "submitter": "Meike K\\\"ohler", "authors": "Meike K\\\"ohler, Nikolaus Umlauf, Sonja Greven", "title": "Nonlinear association structures in flexible Bayesian additive joint\n  models", "comments": "Changes to initial commit: minor language editing, additional\n  information in Section 4, formatting in Supplementary Information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models of longitudinal and survival data have become an important tool\nfor modeling associations between longitudinal biomarkers and event processes.\nThe association between marker and log-hazard is assumed to be linear in\nexisting shared random effects models, with this assumption usually remaining\nunchecked. We present an extended framework of flexible additive joint models\nthat allows the estimation of nonlinear, covariate specific associations by\nmaking use of Bayesian P-splines. Our joint models are estimated in a Bayesian\nframework using structured additive predictors for all model components,\nallowing for great flexibility in the specification of smooth nonlinear,\ntime-varying and random effects terms for longitudinal submodel, survival\nsubmodel and their association. The ability to capture truly linear and\nnonlinear associations is assessed in simulations and illustrated on the widely\nstudied biomedical data on the rare fatal liver disease primary biliary\ncirrhosis. All methods are implemented in the R package bamlss to facilitate\nthe application of this flexible joint model in practice.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 17:45:04 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 09:12:05 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["K\u00f6hler", "Meike", ""], ["Umlauf", "Nikolaus", ""], ["Greven", "Sonja", ""]]}, {"id": "1708.06436", "submitter": "Jann Spiess", "authors": "Jann Spiess", "title": "Unbiased Shrinkage Estimation", "comments": "Updated title and abstract, substance unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimation usually reduces variance at the cost of bias. But when\nwe care only about some parameters of a model, I show that we can reduce\nvariance without incurring bias if we have additional information about the\ndistribution of covariates. In a linear regression model with homoscedastic\nNormal noise, I consider shrinkage estimation of the nuisance parameters\nassociated with control variables. For at least three control variables and\nexogenous treatment, I establish that the standard least-squares estimator is\ndominated with respect to squared-error loss in the treatment effect even among\nunbiased estimators and even when the target parameter is low-dimensional. I\nconstruct the dominating estimator by a variant of James-Stein shrinkage in a\nhigh-dimensional Normal-means problem. It can be interpreted as an invariant\ngeneralized Bayes estimator with an uninformative (improper) Jeffreys prior in\nthe target parameter.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 22:20:31 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:44:08 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Spiess", "Jann", ""]]}, {"id": "1708.06443", "submitter": "Jann Spiess", "authors": "Jann Spiess", "title": "Bias Reduction in Instrumental Variable Estimation through First-Stage\n  Shrinkage", "comments": "Updated title and abstract, substance unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-stage least-squares (2SLS) estimator is known to be biased when its\nfirst-stage fit is poor. I show that better first-stage prediction can\nalleviate this bias. In a two-stage linear regression model with Normal noise,\nI consider shrinkage in the estimation of the first-stage instrumental variable\ncoefficients. For at least four instrumental variables and a single endogenous\nregressor, I establish that the standard 2SLS estimator is dominated with\nrespect to bias. The dominating IV estimator applies James-Stein type shrinkage\nin a first-stage high-dimensional Normal-means problem followed by a\ncontrol-function approach in the second stage. It preserves invariances of the\nstructural instrumental variable equations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 22:38:21 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:55:40 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Spiess", "Jann", ""]]}, {"id": "1708.06483", "submitter": "Lynn Roy LaMotte", "authors": "Lynn R. LaMotte", "title": "Deconstructing Type III", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAS introduced Type III methods to address difficulties in dummy-variable\nmodels for effects of multiple factors and covariates. Type III methods are\nwidely used in practice; they are the default method in many statistical\ncomputing packages. Type III sums of squares (SSs) are defined by an algorithm,\nand an explicit mathematical formulation does not seem to exist. For that\nreason, their properties have not been rigorously proven. Some that are widely\nbelieved to be true are not always true. An explicit formulation is derived in\nthis paper. It is used as a basis to prove fundamental properties of Type III\nestimable functions and SSs. It is shown that, in any given setting, Type III\neffects include all estimable ANOVA effects, and that if all of an ANOVA effect\nis estimable then the Type III SS tests it exactly. The setting for these\nresults is general, comprising linear models for the mean vector of a response\nthat include arbitrary sets of effects of factors and covariates.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 02:37:16 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["LaMotte", "Lynn R.", ""]]}, {"id": "1708.06489", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau and Branko Ristic", "title": "Sequential Monte Carlo algorithms for a class of outer measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closed-form stochastic filtering equations can be derived in a general\nsetting where probability distributions are replaced by some specific outer\nmeasures. In this article, we study how the principles of the sequential Monte\nCarlo method can be adapted for the purpose of practical implementation of\nthese equations. In particular, we explore how sampling can be used to provide\nsupport points for the approximation of these outer measures. This step enables\npractical algorithms to be derived in the spirit of particle filters. The\nperformance of the obtained algorithms is demonstrated in simulations and their\nversatility is illustrated through various examples.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 03:45:05 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 00:39:49 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Ristic", "Branko", ""]]}, {"id": "1708.06838", "submitter": "Yuan Wu", "authors": "Yuan Wu, Christina D.Chambers and Ronghui Xu", "title": "Semiparametric Sieve Maximum Likelihood Estimation Under Cure Model with\n  Partly Interval Censored and Left Truncated Data for Application to\n  Spontaneous Abortion Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work was motivated by observational studies in pregnancy with\nspontaneous abortion (SAB) as outcome. Clearly some women experience the SAB\nevent but the rest do not. In addition, the data are left truncated due to the\nway pregnant women are recruited into these studies. For those women who do\nexperience SAB, their exact event times are sometimes unknown. Finally, a small\npercentage of the women are lost to follow-up during their pregnancy. All these\ngive rise to data that are left truncated, partly interval and right-censored,\nand with a clearly defined cured portion. We consider the non-mixture Cox\nregression cure rate model and adopt the semiparametric spline-based sieve\nmaximum likelihood approach to analyze such data. Using modern empirical\nprocess theory we show that both the parametric and the nonparametric parts of\nthe sieve estimator are consistent, and we establish the asymptotic normality\nfor both parts. Simulation studies are conducted to establish the finite sample\nperformance. Finally, we apply our method to a database of observational\nstudies on spontaneous abortion.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 22:09:39 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Wu", "Yuan", ""], ["Chambers", "Christina D.", ""], ["Xu", "Ronghui", ""]]}, {"id": "1708.06897", "submitter": "Simon Mak", "authors": "Simon Mak, V. Roshan Joseph", "title": "Projected support points: a new method for high-dimensional data\n  reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era where big and high-dimensional data is readily available, data\nscientists are inevitably faced with the challenge of reducing this data for\nexpensive downstream computation or analysis. To this end, we present here a\nnew method for reducing high-dimensional big data into a representative point\nset, called projected support points (PSPs). A key ingredient in our method is\nthe so-called sparsity-inducing (SpIn) kernel, which encourages the\npreservation of low-dimensional features when reducing high-dimensional data.\nWe begin by introducing a unifying theoretical framework for data reduction,\nconnecting PSPs with fundamental sampling principles from experimental design\nand Quasi-Monte Carlo. Through this framework, we then derive sparsity\nconditions under which the curse-of-dimensionality in data reduction can be\nlifted for our method. Next, we propose two algorithms for one-shot and\nsequential reduction via PSPs, both of which exploit big data subsampling and\nmajorization-minimization for efficient optimization. Finally, we demonstrate\nthe practical usefulness of PSPs in two real-world applications, the first for\ndata reduction in kernel learning, and the second for reducing Markov Chain\nMonte Carlo (MCMC) chains.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 06:34:06 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 19:39:05 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Mak", "Simon", ""], ["Joseph", "V. Roshan", ""]]}, {"id": "1708.06982", "submitter": "Anders Hildeman", "authors": "Anders Hildeman, David Bolin, Jonas Wallin and Janine B. Illian", "title": "Level set Cox processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-Gaussian Cox process (LGCP) is a popular point process for modeling\nnon-interacting spatial point patterns. This paper extends the LGCP model to\nhandle data exhibiting fundamentally different behaviors in different\nsubregions of the spatial domain. The aim of the analyst might be either to\nidentify and classify these regions, to perform kriging, or to derive some\nproperties of the parameters driving the random field in one or several of the\nsubregions. The extension is based on replacing the latent Gaussian random\nfield in the LGCP by a latent spatial mixture model. The mixture model is\nspecified using a latent, categorically valued, random field induced by level\nset operations on a Gaussian random field. Conditional on the classification,\nthe intensity surface for each class is modeled by a set of independent\nGaussian random fields. This allows for standard stationary covariance\nstructures, such as the Mat\\'{e}rn family, to be used to model Gaussian random\nfields with some degree of general smoothness but also occasional and\nstructured sharp discontinuities.\n  A computationally efficient MCMC method is proposed for Bayesian inference\nand we show consistency of finite dimensional approximations of the model.\nFinally, the model is fitted to point pattern data derived from a tropical\nrainforest on Barro Colorado island, Panama. We show that the proposed model is\nable to capture behavior for which inference based on the standard LGCP is\nbiased.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 13:13:30 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 12:32:17 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Hildeman", "Anders", ""], ["Bolin", "David", ""], ["Wallin", "Jonas", ""], ["Illian", "Janine B.", ""]]}, {"id": "1708.06990", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Sebastiano Stramaglia, Daniele Marinazzo", "title": "On the interpretability and computational reliability of\n  frequency-domain Granger causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment to the paper 'A study of problems encountered in Granger\ncausality analysis from a neuroscience perspective'. We agree that\ninterpretation issues of Granger Causality in Neuroscience exist (partially due\nto the historical unfortunate use of the name 'causality', as nicely described\nin previous literature). On the other hand we think that the paper uses a\nformulation of Granger causality which is outdated (albeit still used), and in\ndoing so it dismisses the measure based on a suboptimal use of it. Furthermore,\nsince data from simulated systems are used, the pitfalls that are found with\nthe used formulation are intended to be general, and not limited to\nneuroscience. It would be a pity if this paper, even written in good faith,\nbecame a wildcard against all possible applications of Granger Causality,\nregardless of the hard work of colleagues aiming to seriously address the\nmethodological and interpretation pitfalls. In order to provide a balanced\nview, we replicated their simulations used the updated State Space\nimplementation, proposed already some years ago, in which the pitfalls are\nmitigated or directly solved.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 13:32:24 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Faes", "Luca", ""], ["Stramaglia", "Sebastiano", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1708.07019", "submitter": "Anna Kiriliouk", "authors": "Anna Kiriliouk", "title": "Hypothesis testing for tail dependence parameters on the boundary of the\n  parameter space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling multivariate tail dependence is one of the key challenges in\nextreme-value theory. Multivariate extremes are usually characterized using\nparametric models, some of which have simpler submodels at the boundary of\ntheir parameter space. Hypothesis tests are proposed for tail dependence\nparameters that, under the null hypothesis, are on the boundary of the\nalternative hypothesis. The asymptotic distribution of the weighted least\nsquares estimator (Einmahl, Kiriliouk and Segers, Extremes 21, pages 205-233,\n2018) is given when the true parameter vector is on the boundary of the\nparameter space, and two test statistics are proposed. The performance of these\ntest statistics is evaluated for the Brown-Resnick model and the max-linear\nmodel. In particular, simulations show that it is possible to recover the\noptimal number of factors for a max-linear model. Finally, the methods are\napplied to characterize the dependence structure of two major stock market\nindices, the DAX and the CAC40.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 14:24:46 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 08:38:22 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kiriliouk", "Anna", ""]]}, {"id": "1708.07114", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Aaron Smith", "title": "Rapid Mixing of Hamiltonian Monte Carlo on Strongly Log-Concave\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain several quantitative bounds on the mixing properties of the\nHamiltonian Monte Carlo (HMC) algorithm for a strongly log-concave target\ndistribution $\\pi$ on $\\mathbb{R}^{d}$, showing that HMC mixes quickly in this\nsetting. One of our main results is a dimension-free bound on the mixing of an\n\"ideal\" HMC chain, which is used to show that the usual leapfrog implementation\nof HMC can sample from $\\pi$ using only $\\mathcal{O}(d^{\\frac{1}{4}})$ gradient\nevaluations. This dependence on dimension is sharp, and our results\nsignificantly extend and improve previous quantitative bounds on the mixing of\nHMC.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 17:29:10 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Mangoubi", "Oren", ""], ["Smith", "Aaron", ""]]}, {"id": "1708.07196", "submitter": "Subhajit Sengupta", "authors": "Subhajit Sengupta, Subhadip Pal, Riten Mitra, Ying Guo, Arunava\n  Banerjee and Yuan Ji", "title": "A Bayesian Mixture Model for Clustering on the Stiefel Manifold", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of a Bayesian mixture model for the Matrix Langevin distribution on\nthe Stiefel manifold is presented. The model exploits a particular\nparametrization of the Matrix Langevin distribution, various aspects of which\nare elaborated on. A general, and novel, family of conjugate priors, and an\nefficient Markov chain Monte Carlo (MCMC) sampling scheme for the corresponding\nposteriors is then developed for the mixture model. Theoretical properties of\nthe prior and posterior distributions, including posterior consistency, are\nexplored in detail. Extensive simulation experiments are presented to validate\nthe efficacy of the framework. Real-world examples, including a large scale\nneuroimaging dataset, are analyzed to demonstrate the computational\ntractability of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 21:40:52 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Sengupta", "Subhajit", ""], ["Pal", "Subhadip", ""], ["Mitra", "Riten", ""], ["Guo", "Ying", ""], ["Banerjee", "Arunava", ""], ["Ji", "Yuan", ""]]}, {"id": "1708.07352", "submitter": "Yulin Wang", "authors": "Yulin Wang, Na Lu, Hongyu Miao", "title": "Structural Identifiability of Cyclic Graphical Models of Biological\n  Networks with Latent Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient structural identifiability analysis algorithm is developed in\nthis study for a broad range of network structures. The proposed method adopts\nthe Wright's path coefficient method to generate identifiability equations in\nforms of symbolic polynomials, and then converts these symbolic equations to\nbinary matrices (called identifiability matrix). Several matrix operations are\nintroduced for identifiability matrix reduction with system equivalency\nmaintained. Based on the reduced identifiability matrices, the structural\nidentifiability of each parameter is determined. A number of benchmark models\nare used to verify the validity of the proposed approach. Finally, the network\nmodule for influenza A virus replication is employed as a real example to\nillustrate the application of the proposed approach in practice. The proposed\napproach can deal with cyclic networks with latent variables. The key advantage\nis that it intentionally avoids symbolic computation and is thus highly\nefficient. Also, this method is capable of determining the identifiability of\neach single parameter and is thus of higher resolution in comparison with many\nexisting approaches. Overall, this study provides a basis for systematic\nexamination and refinement of graphical models of biological networks from the\nidentifiability point of view, and it has a significant potential to be\nextended to more complex network structures or high-dimensional systems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 10:55:55 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Wang", "Yulin", ""], ["Lu", "Na", ""], ["Miao", "Hongyu", ""]]}, {"id": "1708.07363", "submitter": "H{\\aa}vard Wahl Kongsg{\\aa}rd HwK", "authors": "H{\\aa}vard Wahl Kongsg{\\aa}rd, Geir-Arne Fuglstad, H{\\aa}vard Rue,\n  Kristian Hveem, Steinar Krokstad", "title": "Modeling water supply networks and gastrointestinal disorder symptoms\n  with CAR models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: The direct modeling of water networks is not a common practice in\nmodern epidemiology. While space often serves as a proxy, it can be\nproblematic. There are multiple ways to directly model water networks, but\nthese methods are not straightforward and can be difficult to implement. This\nstudy suggests a simple approach for modeling water networks and diseases, and\napplies this method to a dataset of self-reported gastrointestinal conditions\nfrom a questionnaire-based population health survey in central Norway.\n  Method: Our approach is based on a standard conditional autoregressive (CAR)\nmodel. An inverse matrix was constructed, with nodes weighted based on the\ndistance to neighboring nodes within the networks. This matrix was then fitted\nas a generic model. To illustrate its possible use, we utilized data taken from\na questionnaire-based population health survey, the HUNT Study, to measure\nself-reported gastrointestinal complaints. For hypothesis testing, we used the\ndeviance information criterion (DIC) and included variables in a stepwise\nmanner.\n  Results: The full model converged after six hours. We found no relation\nbetween the water networks and the health conditions of people whose residences\nconnected to different parts of the network in the geographical area studied.\n  Conclusion: All water network models are simplifications of the real\nnetworks. Nevertheless, we suggest a valid approach for distinguishing between\nthe general spatial effect and the water network using a generic model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 11:54:51 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 07:07:32 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Kongsg\u00e5rd", "H\u00e5vard Wahl", ""], ["Fuglstad", "Geir-Arne", ""], ["Rue", "H\u00e5vard", ""], ["Hveem", "Kristian", ""], ["Krokstad", "Steinar", ""]]}, {"id": "1708.07487", "submitter": "Daniel Simpson", "authors": "Andrew Gelman, Daniel Simpson, and Michael Betancourt", "title": "The prior can generally only be understood in the context of the\n  likelihood", "comments": "13 pages", "journal-ref": null, "doi": "10.3390/e19100555", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key sticking point of Bayesian analysis is the choice of prior\ndistribution, and there is a vast literature on potential defaults including\nuniform priors, Jeffreys' priors, reference priors, maximum entropy priors, and\nweakly informative priors. These methods, however, often manifest a key\nconceptual tension in prior modeling: a model encoding true prior information\nshould be chosen without reference to the model of the measurement process, but\nalmost all common prior modeling techniques are implicitly motivated by a\nreference likelihood. In this paper we resolve this apparent paradox by placing\nthe choice of prior into the context of the entire Bayesian analysis, from\ninference to prediction to model evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 16:47:41 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 20:34:39 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Gelman", "Andrew", ""], ["Simpson", "Daniel", ""], ["Betancourt", "Michael", ""]]}, {"id": "1708.07587", "submitter": "Wilson Ye Chen", "authors": "Wilson Ye Chen, Richard H. Gerlach", "title": "Semiparametric GARCH via Bayesian model averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the dynamic structure of the financial markets is subject to dramatic\nchanges, a model capable of providing consistently accurate volatility\nestimates must not make strong assumptions on how prices change over time. Most\nvolatility models impose a particular parametric functional form that relates\nan observed price change to a volatility forecast (news impact function). We\npropose a new class of functional coefficient semiparametric volatility models\nwhere the news impact function is allowed to be any smooth function, and study\nits ability to estimate volatilities compared to the well known parametric\nproposals, in both a simulation study and an empirical study with real\nfinancial data. We estimate the news impact function using a Bayesian model\naveraging approach, implemented via a carefully developed Markov chain Monte\nCarlo (MCMC) sampling algorithm. Using simulations we show that our flexible\nsemiparametric model is able to learn the shape of the news impact function\nfrom the observed data. When applied to real financial time series, our new\nmodel suggests that the news impact functions are significantly different in\nshapes for different asset types, but are similar for the assets of the same\ntype.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 00:56:20 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Chen", "Wilson Ye", ""], ["Gerlach", "Richard H.", ""]]}, {"id": "1708.07595", "submitter": "Jianwei Hu", "authors": "Jianwei Hu, Jingfei Zhang and Ji Zhu", "title": "Detection of the number of principal components by extended AIC-type\n  method", "comments": "56 pages; AIC is re-defined; for $k=o(n^{\\frac{1}{3}})$, we\n  investigate the limiting laws for the leading eigenvalues of the sample\n  covariance matrix $S_n$ under the condition that $\\lambda_k>1+\\sqrt{c}$", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of principal components is one of the fundamental\nproblems in many scientific fields such as signal processing (or the spiked\ncovariance model). In this paper, we first demonstrate that, for fixed $p$, any\npenalty term of the form $k'(p-k'/2+1/2)C_n$ may lead to an asymptotically\nconsistent estimator under the condition that $C_n\\to\\infty$ and $C_n/n\\to0$.\nWe also extend our results to the case $n,p\\to\\infty$, with $p/n\\to c>0$. In\nthis case, for $k=o(n^{\\frac{1}{3}})$, we first investigate the limiting laws\nfor the leading eigenvalues of the sample covariance matrix $S_n$ under the\ncondition that $\\lambda_k>1+\\sqrt{c}$. At low SNR, since the AIC tends to\nunderestimate the number of signals $k$, the AIC should be re-defined in this\ncase. As a natural extension of the AIC for fixed $p$, we propose the extended\nAIC (EAIC), i.e., the AIC-type method with tuning parameter\n$\\gamma=\\varphi(c)=1/2+\\sqrt{1/c}-\\log(1+\\sqrt{c})/c$, and demonstrate that the\nEAIC-type method, i.e., the AIC-type method with tuning parameter\n$\\gamma>\\varphi(c)$, can select the number of signals $k$ consistently. In the\nfollowing two cases, (1) $p$ fixed, $n\\to\\infty$, (2) $n,p\\to\\infty$ with\n$p/n\\to 0$, if the AIC is defined as the degeneration of the EAIC in the case\n$n,p\\to\\infty$ with $p/n\\to c>0$, i.e., $\\gamma=\\lim_{c\\rightarrow\n0+0}\\varphi(c)=1$, then we have essentially demonstrated that, to achieve the\nconsistency of the AIC-type method in the above two cases, $\\gamma>1$ is\nrequired. Moreover, we show that the EAIC-type method is essentially\ntuning-free and outperforms the well-known KN estimator proposed in Kritchman\nand Nadler (2008) and the BCF estimator proposed in Bai, Choi and Fujikoshi\n(2018). Numerical studies indicate that the proposed method works well.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 01:45:36 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 11:14:36 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 07:59:28 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 10:59:44 GMT"}, {"version": "v5", "created": "Fri, 19 Jan 2018 13:45:06 GMT"}, {"version": "v6", "created": "Wed, 25 Dec 2019 04:13:06 GMT"}, {"version": "v7", "created": "Sat, 29 Aug 2020 11:05:05 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hu", "Jianwei", ""], ["Zhang", "Jingfei", ""], ["Zhu", "Ji", ""]]}, {"id": "1708.07852", "submitter": "Zheng Tracy Ke", "authors": "Jiashun Jin, Zheng Tracy Ke, Shengming Luo", "title": "Estimating network memberships by simplex vertex hunting", "comments": "84 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an undirected mixed membership network with $n$ nodes and $K$\ncommunities. For each node $i$, $1 \\leq i \\leq n$, we model the membership by a\nProbability Mass Function (PMF) $\\pi_{i} = (\\pi_{i}(1), \\pi_{i}(2), \\ldots$,\n$\\pi_{i}(K))'$, where $\\pi_{i}(k)$ is the probability that node $i$ belongs to\ncommunity $k$, $1 \\leq k \\leq K$. We call node $i$ \"pure\" if $\\pi_i$ is\ndegenerate and \"mixed\" otherwise. The primary interest is to estimate $\\pi_i$,\n$1 \\leq i \\leq n$.\n  We model the adjacency matrix $A$ with a Degree Corrected Mixed Membership\n(DCMM) model, and let $\\hat{\\xi}_1, \\hat{\\xi}_2, \\ldots, \\hat{\\xi}_K$ be the\nfirst $K$ eigenvectors. Define a matrix $\\hat{R} \\in \\mathbb{R}^{n, K-1}$ by\n$\\hat{R}(i,k) = \\hat{\\xi}_{k+1}(i)/\\hat{\\xi}_1(i)$, $1 \\leq k \\leq K-1$, $1\n\\leq i \\leq n$. The oracle counterpart of $\\hat{R}$ (denoted by $R$) under the\nDCMM model contains all information we need for the memberships. In fact, we\nhave an interesting insight: there is a simplex ${\\cal S}$ in\n$\\mathbb{R}^{K-1}$ such that row $i$ of $R$ corresponds to a vertex of ${\\cal\nS}$ if node $i$ is pure, and corresponds to an interior point of ${\\cal S}$\notherwise. Vertex Hunting (i.e., estimating the vertices of ${\\cal S}$) is\ntherefore the key to our problem.\n  We propose a new approach Mixed-SCORE to membership estimation, with an\neasy-to-use Vertex Hunting step. We derive the convergence rate of Mixed-SCORE\nusing delicate spectral analysis, especially tight row-wise deviation bounds\nfor $\\hat{R}$. We have also applied it to $4$ network data sets with\nencouraging results.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 18:16:13 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 21:25:06 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Jin", "Jiashun", ""], ["Ke", "Zheng Tracy", ""], ["Luo", "Shengming", ""]]}, {"id": "1708.07942", "submitter": "Minh Nguyen", "authors": "Minh Nguyen, Sanjay Purushotham, Hien To, Cyrus Shahabi", "title": "m-TSNE: A Framework for Visualizing High-Dimensional Multivariate Time\n  Series", "comments": "VAHC2016 Workshop on Visual Analytics in Healthcare in conjunction\n  with AMIA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series (MTS) have become increasingly common in healthcare\ndomains where human vital signs and laboratory results are collected for\npredictive diagnosis. Recently, there have been increasing efforts to visualize\nhealthcare MTS data based on star charts or parallel coordinates. However, such\ntechniques might not be ideal for visualizing a large MTS dataset, since it is\ndifficult to obtain insights or interpretations due to the inherent high\ndimensionality of MTS. In this paper, we propose 'm-TSNE': a simple and novel\nframework to visualize high-dimensional MTS data by projecting them into a\nlow-dimensional (2-D or 3-D) space while capturing the underlying data\nproperties. Our framework is easy to use and provides interpretable insights\nfor healthcare professionals to understand MTS data. We evaluate our\nvisualization framework on two real-world datasets and demonstrate that the\nresults of our m-TSNE show patterns that are easy to understand while the other\nmethods' visualization may have limitations in interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 07:21:58 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Nguyen", "Minh", ""], ["Purushotham", "Sanjay", ""], ["To", "Hien", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1708.08001", "submitter": "Lionel Barnett", "authors": "Lionel Barnett, Adam B. Barrett and Anil K. Seth", "title": "Solved problems and remaining challenges for Granger causality analysis\n  in neuroscience: A response to Stokes and Purdon (2017)", "comments": "Submitted to NeuroImage, 5 February 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger-Geweke causality (GGC) is a powerful and popular method for\nidentifying directed functional (`causal') connectivity in neuroscience. In a\nrecent paper, Stokes and Purdon [1] raise several concerns about its use. They\nmake two primary claims: (1) that GGC estimates may be severely biased or of\nhigh variance, and (2) that GGC fails to reveal the full structural/causal\nmechanisms of a system. However, these claims rest, respectively, on an\nincomplete evaluation of the literature, and a misconception about what GGC can\nbe said to measure. Here we explain how existing approaches (as implemented,\nfor example, in our popular MVGC software [2,3]) resolve the first issue, and\ndiscuss the frequently-misunderstood distinction between functional and\neffective neural connectivity which underlies Stokes and Purdon's second claim.\n  [1] Patrick A. Stokes and Patrick. L. Purdon (2017), A study of problems\nencountered in Granger causality analysis from a neuroscience perspective,\nProc. Natl. Acad. Sci. USA 114(34):7063-7072.\n  [2] Lionel Barnett and Anil K. Seth (2012), The MVGC Multivariate Granger\nCausality Matlab toolbox, http://users.sussex.ac.uk/~lionelb/MVGC/\n  [3] Lionel Barnett and Anil K. Seth (2014), The MVGC multivariate Granger\ncausality toolbox: A new approach to Granger-causal inference, J. Neurosci.\nMethods 223:50-68\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 17:49:31 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 10:24:18 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Barnett", "Lionel", ""], ["Barrett", "Adam B.", ""], ["Seth", "Anil K.", ""]]}, {"id": "1708.08045", "submitter": "Shinichi Takayanagi", "authors": "Shinichi Takayanagi, Yukito Iba", "title": "Backward Simulation of Stochastic Process using a Time Reverse Monte\n  Carlo method", "comments": null, "journal-ref": "J. Phys. Soc. Jpn., Vol.87, No.12 (2018)", "doi": "10.7566/JPSJ.87.124003", "report-no": null, "categories": "physics.data-an cond-mat.dis-nn cond-mat.stat-mech nlin.CD stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"backward simulation\" of a stochastic process is defined as the\nstochastic dynamics that trace a time-reversed path from the target region to\nthe initial configuration. If the probabilities calculated by the original\nsimulation are easily restored from those obtained by backward dynamics, we can\nuse it as a computational tool. It is shown that the naive approach to backward\nsimulation does not work as expected. As a remedy, the Time Reverse Monte Carlo\nmethod (TRMC) based on the ideas of Sequential Importance Sampling (SIS) and\nSequential Monte Carlo (SMC) is proposed and successfully tested with a\nstochastic typhoon model and the Lorenz 96 model. TRMC with SMC, which contains\nresampling steps, is shown to be more efficient for simulations with a larger\nnumber of time steps. A limitation of TRMC and its relation to the Bayes\nformula are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 02:55:55 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 01:24:41 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 00:56:22 GMT"}, {"version": "v4", "created": "Sun, 27 Jan 2019 03:37:12 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Takayanagi", "Shinichi", ""], ["Iba", "Yukito", ""]]}, {"id": "1708.08137", "submitter": "Serena Ng", "authors": "Jushan Bai and Serena Ng", "title": "Principal Components and Regularized Estimation of Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the common factors in a large panel of data can be\nconsistently estimated by the method of principal components, and principal\ncomponents can be constructed by iterative least squares regressions. Replacing\nleast squares with ridge regressions turns out to have the effect of shrinking\nthe singular values of the common component and possibly reducing its rank. The\nmethod is used in the machine learning literature to recover low-rank matrices.\nWe study the procedure from the perspective of estimating a minimum-rank\napproximate factor model. We show that the constrained factor estimates are\nbiased but can be more efficient in terms of mean-squared errors. Rank\nconsideration suggests a data-dependent penalty for selecting the number of\nfactors. The new criterion is more conservative in cases when the nominal\nnumber of factors is inflated by the presence of weak factors or large\nmeasurement noise. The framework is extended to incorporate a priori linear\nconstraints on the loadings. We provide asymptotic results that can be used to\ntest economic hypotheses.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 20:58:02 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 13:45:49 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Bai", "Jushan", ""], ["Ng", "Serena", ""]]}, {"id": "1708.08143", "submitter": "Vivien Seguy", "authors": "Elsa Cazelles, Vivien Seguy, J\\'er\\'emie Bigot, Marco Cuturi, Nicolas\n  Papadakis", "title": "Log-PCA versus Geodesic PCA of histograms in the Wasserstein space", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned by the statistical analysis of data sets whose\nelements are random histograms. For the purpose of learning principal modes of\nvariation from such data, we consider the issue of computing the PCA of\nhistograms with respect to the 2-Wasserstein distance between probability\nmeasures. To this end, we propose to compare the methods of log-PCA and\ngeodesic PCA in the Wasserstein space as introduced by Bigot et al. (2015) and\nSeguy and Cuturi (2015). Geodesic PCA involves solving a non-convex\noptimization problem. To solve it approximately, we propose a novel\nforward-backward algorithm. This allows a detailed comparison between log-PCA\nand geodesic PCA of one-dimensional histograms, which we carry out using\nvarious data sets, and stress the benefits and drawbacks of each method. We\nextend these results for two-dimensional data and compare both methods in that\nsetting.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 21:46:06 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Cazelles", "Elsa", ""], ["Seguy", "Vivien", ""], ["Bigot", "J\u00e9r\u00e9mie", ""], ["Cuturi", "Marco", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1708.08157", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo and Bharath K. Sriperumbudur", "title": "Characteristic and Universal Tensor Product Kernels", "comments": "final version appeared in JMLR", "journal-ref": "Journal of Machine Learning Research 18(233):1-29, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maximum mean discrepancy (MMD), also called energy distance or N-distance in\nstatistics and Hilbert-Schmidt independence criterion (HSIC), specifically\ndistance covariance in statistics, are among the most popular and successful\napproaches to quantify the difference and independence of random variables,\nrespectively. Thanks to their kernel-based foundations, MMD and HSIC are\napplicable on a wide variety of domains. Despite their tremendous success,\nquite little is known about when HSIC characterizes independence and when MMD\nwith tensor product kernel can discriminate probability distributions. In this\npaper, we answer these questions by studying various notions of characteristic\nproperty of the tensor product kernel.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 00:37:38 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 11:33:18 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 19:46:33 GMT"}, {"version": "v4", "created": "Thu, 2 Aug 2018 12:33:18 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Szabo", "Zoltan", ""], ["Sriperumbudur", "Bharath K.", ""]]}, {"id": "1708.08171", "submitter": "Naoki Egami", "authors": "Naoki Egami", "title": "Spillover Effects in the Presence of Unobserved Networks", "comments": null, "journal-ref": "Polit. Anal. 29 (2021) 287-316", "doi": "10.1017/pan.2020.28", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When experimental subjects can interact with each other, the outcome of one\nindividual may be affected by the treatment status of others. In many social\nscience experiments, such spillover effects may occur through multiple\nnetworks, for example, through both online and offline face-to-face networks in\na Twitter experiment. Thus, to understand how people use different networks, it\nis essential to estimate the spillover effect in each specific network\nseparately. However, the unbiased estimation of these network-specific\nspillover effects requires an often-violated assumption that researchers\nobserve all relevant networks. We show that, unlike conventional omitted\nvariable bias, bias due to unobserved networks remains even when treatment\nassignment is randomized and when unobserved networks and a network of interest\nare independently generated. We then develop parametric and nonparametric\nsensitivity analysis methods, with which researchers can assess the potential\ninfluence of unobserved networks on causal findings. We illustrate the proposed\nmethods with a simulation study based on a real-world Twitter network and an\nempirical application based on a network field experiment in China.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 03:00:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 02:12:27 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 06:23:28 GMT"}, {"version": "v4", "created": "Mon, 28 Dec 2020 04:10:04 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Egami", "Naoki", ""]]}, {"id": "1708.08218", "submitter": "Atsushi Iwasaki", "authors": "Atsushi Iwasaki and Ken Umeno", "title": "A new randomness test solving problems of Discrete Fourier Transform\n  Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete Fourier Transform Test (DFTT), which is a randomness test included\nin NIST SP800-22, has a problem. It is that theoretical reference distribution\nof the test statistic has not been derived. In this paper, we propose a new\ntest using variance of power spectrum as the test statistic, whose reference\ndistribution can be theoretically derived. The purpose of DFTT is to detect\nperiodic features and that of the proposed test is the same. We make some\nexperiments and show that the proposed test has stronger detection power than\nDFTT.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 07:25:05 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Iwasaki", "Atsushi", ""], ["Umeno", "Ken", ""]]}, {"id": "1708.08239", "submitter": "Isaac Gravestock", "authors": "Isaac Gravestock, Leonhard Held", "title": "Power Priors Based on Multiple Historical Studies for Binary Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating historical information into the design and analysis of a new\nclinical trial has been the subject of much recent discussion. For example, in\nthe context of clinical trials of antibiotics for drug resistant infections,\nwhere patients with specific infections can be difficult to recruit, there is\noften only limited and heterogeneous information available from the historical\ntrials. To make the best use of the combined information at hand, we consider\nan approach based on the multiple power prior which allows the prior weight of\neach historical study to be chosen adaptively by empirical Bayes. This choice\nof weight has advantages in that it varies commensurably with differences in\nthe historical and current data and can choose weights near 1 if the data from\nthe corresponding historical study are similar enough to the data from the\ncurrent study. Fully Bayesian approaches are also considered. The methods are\napplied to data from antibiotics trials. An analysis of the operating\ncharacteristics in a binomial setting shows that the proposed empirical Bayes\nadaptive method works well, compared to several alternative approaches,\nincluding the meta-analytic prior.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 08:50:51 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 13:50:18 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 11:57:47 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Gravestock", "Isaac", ""], ["Held", "Leonhard", ""]]}, {"id": "1708.08257", "submitter": "David Kraus", "authors": "David Kraus and Marco Stefanucci", "title": "Classification of functional fragments by regularized linear classifiers\n  with domain selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classification of functional data into two groups\nby linear classifiers based on one-dimensional projections of functions. We\nreformulate the task to find the best classifier as an optimization problem and\nsolve it by regularization techniques, namely the conjugate gradient method\nwith early stopping, the principal component method and the ridge method. We\nstudy the empirical version with finite training samples consisting of\nincomplete functions observed on different subsets of the domain and show that\nthe optimal, possibly zero, misclassification probability can be achieved in\nthe limit along a possibly non-convergent empirical regularization path. Being\nable to work with fragmentary training data we propose a domain extension and\nselection procedure that finds the best domain beyond the common observation\ndomain of all curves. In a simulation study we compare the different\nregularization methods and investigate the performance of domain selection. Our\nmethodology is illustrated on a medical data set, where we observe a\nsubstantial improvement of classification accuracy due to domain extension.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 09:54:59 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Kraus", "David", ""], ["Stefanucci", "Marco", ""]]}, {"id": "1708.08278", "submitter": "Rianne de Heide", "authors": "Rianne de Heide, Peter D. Gr\\\"unwald", "title": "Why optional stopping can be a problem for Bayesians", "comments": "Replacement of Figures 7a-7d in the appendix. There was a mistake in\n  the sampling plan. Thanks to Jorge Tendeiro for pointing this out. Replaced\n  the main text with the final (published) version. Psychonomic Bulletin &\n  Review 2020 Advance Publication", "journal-ref": null, "doi": "10.3758/s13423-020-01803-x", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, optional stopping has been a subject of debate in the Bayesian\npsychology community. Rouder (2014) argues that optional stopping is no problem\nfor Bayesians, and even recommends the use of optional stopping in practice, as\ndo Wagenmakers et al. (2012). This article addresses the question whether\noptional stopping is problematic for Bayesian methods, and specifies under\nwhich circumstances and in which sense it is and is not. By slightly varying\nand extending Rouder's (2014) experiments, we illustrate that, as soon as the\nparameters of interest are equipped with default or pragmatic priors - which\nmeans, in most practical applications of Bayes factor hypothesis testing -\nresilience to optional stopping can break down. We distinguish between three\ntypes of default priors, each having their own specific issues with optional\nstopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type\nII priors).\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 11:46:00 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 14:54:12 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 15:39:35 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2020 09:57:32 GMT"}, {"version": "v5", "created": "Thu, 25 Mar 2021 18:12:30 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["de Heide", "Rianne", ""], ["Gr\u00fcnwald", "Peter D.", ""]]}, {"id": "1708.08321", "submitter": "Gery Geenens", "authors": "Carlos Aya Moreno, Gery Geenens and Spiridon Penev", "title": "Shape-preserving wavelet-based multivariate density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet estimators for a probability density f enjoy many good properties,\nhowever they are not \"shape-preserving\" in the sense that the final estimate\nmay not be non-negative or integrate to unity. A solution to negativity issues\nmay be to estimate first the square-root of f and then square this estimate up.\nThis paper proposes and investigates such an estimation scheme, generalising to\nhigher dimensions some previous constructions which are valid only in one\ndimension. The estimation is mainly based on nearest-neighbour-balls. The\ntheoretical properties of the proposed estimator are obtained, and it is shown\nto reach the optimal rate of convergence uniformly over large classes of\ndensities under mild conditions. Simulations show that the new estimator\nperforms as well in general as the classical wavelet estimator, while\nautomatically producing estimates which are bona fide densities.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:13:11 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Moreno", "Carlos Aya", ""], ["Geenens", "Gery", ""], ["Penev", "Spiridon", ""]]}, {"id": "1708.08353", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Abhishek Kaul, and Mathieu Rosenbaum", "title": "Pivotal Estimation via Self-Normalization for High-Dimensional Linear\n  Models with Error in Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new estimator for the high-dimensional linear regression model\nwith observation error in the design where the number of coefficients is\npotentially larger than the sample size. The main novelty of our procedure is\nthat the choice of penalty parameters is pivotal. The estimator is based on\napplying a self-normalization to the constraints that characterize the\nestimator. Importantly, we show how to cast the computation of the estimator as\nthe solution of a convex program with second order cone constraints. This\nallows the use of algorithms with theoretical guarantees and reliable\nimplementation. Under sparsity assumptions, we derive $\\ell_q$-rates of\nconvergence and show that consistency can be achieved even if the number of\nregressors exceeds the sample size. We further provide a simple to implement\nrule to threshold the estimator that yields a provably sparse estimator with\nsimilar $\\ell_2$ and $\\ell_1$-rates of convergence. The thresholds are\ndata-driven and component dependents. Finally, we also study the rates of\nconvergence of estimators that refit the data based on a selected support with\npossible model selection mistakes. In addition to our finite sample theoretical\nresults that allow for non-i.i.d. data, we also present simulations to compare\nthe performance of the proposed estimators.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:52:47 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 20:15:11 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 11:35:32 GMT"}, {"version": "v4", "created": "Fri, 6 Sep 2019 16:07:44 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Belloni", "Alexandre", ""], ["Kaul", "Abhishek", ""], ["Rosenbaum", "Mathieu", ""]]}, {"id": "1708.08374", "submitter": "Xiaoou Li", "authors": "Xiaoou Li, Yunxiao Chen, Xi Chen, Jingchen Liu, Zhiliang Ying", "title": "Optimal Stopping and Worker Selection in Crowdsourcing: an Adaptive\n  Sequential Probability Ratio Test Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at solving a class of multiple testing problems under\nthe Bayesian sequential decision framework. Our motivating application comes\nfrom binary labeling tasks in crowdsourcing, where the requestor needs to\nsimultaneously decide which worker to choose to provide the label and when to\nstop collecting labels under a certain budget constraint. We start with the\nbinary hypothesis testing problem to determine the true label of a single\nobject, and provide an optimal solution by casting it under the adaptive\nsequential probability ratio test (Ada-SPRT) framework. We characterize the\nstructure of the optimal solution, i.e., optimal adaptive sequential design,\nwhich minimizes the Bayes risk through log-likelihood ratio statistic. We also\ndevelop a dynamic programming algorithm that can efficiently approximate the\noptimal solution. For the multiple testing problem, we further propose to adopt\nan empirical Bayes approach for estimating class priors and show that our\nmethod has an averaged loss that converges to the minimal Bayes risk under the\ntrue model. The experiments on both simulated and real data show the robustness\nof our method and its superiority in labeling accuracy as compared to several\nother recently proposed approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 15:28:47 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Li", "Xiaoou", ""], ["Chen", "Yunxiao", ""], ["Chen", "Xi", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1708.08522", "submitter": "Edward Kao", "authors": "Edward K. Kao", "title": "Causal Inference Under Network Interference: A Framework for Experiments\n  on Social Networks", "comments": "PhD Thesis at Harvard Department of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No man is an island, as individuals interact and influence one another daily\nin our society. When social influence takes place in experiments on a\npopulation of interconnected individuals, the treatment on a unit may affect\nthe outcomes of other units, a phenomenon known as interference. This thesis\ndevelops a causal framework and inference methodology for experiments where\ninterference takes place on a network of influence (i.e. network interference).\nIn this framework, the network potential outcomes serve as the key quantity and\nflexible building blocks for causal estimands that represent a variety of\nprimary, peer, and total treatment effects. These causal estimands are\nestimated via principled Bayesian imputation of missing outcomes. The theory on\nthe unconfoundedness assumptions leading to simplified imputation highlights\nthe importance of including relevant network covariates in the potential\noutcome model. Additionally, experimental designs that result in balanced\ncovariates and sizes across treatment exposure groups further improve the\ncausal estimate, especially by mitigating potential outcome model\nmis-specification. The true potential outcome model is not typically known in\nreal-world experiments, so the best practice is to account for interference and\nconfounding network covariates through both balanced designs and model-based\nimputation. A full factorial simulated experiment is formulated to demonstrate\nthis principle by comparing performance across different randomization schemes\nduring the design phase and estimators during the analysis phase, under varying\nnetwork topology and true potential outcome models. Overall, this thesis\nasserts that interference is not just a nuisance for analysis but rather an\nopportunity for quantifying and leveraging peer effects in real-world\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 20:50:35 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Kao", "Edward K.", ""]]}, {"id": "1708.08527", "submitter": "Longhai Li", "authors": "Cindy Feng, Alireza Sadeghpour, Longhai Li", "title": "Randomized Predictive P-values: A Versatile Model Diagnostic Tool with\n  Unified Reference Distribution", "comments": "26 pages; we've updated some figures for better visualization, and\n  fixed a few errors in text; R code for producing the results of this paper is\n  available upon request", "journal-ref": "BMC Medical Research Methodology, 20, Article number: 175 (2020)", "doi": "10.1186/s12874-020-01055-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examining residuals such as Pearson and deviance residuals, is a standard\ntool for assessing normal regression. However, for discrete response, these\nresiduals cluster on lines corresponding to distinct response values. Their\ndistributions are far from normality; graphical and quantitative inspection of\nthese residuals provides little information for model diagnosis. Marshall and\nSpiegelhalter (2003) defined a cross-validatory predictive p-value for\nidentifying outliers. Predictive p-values are uniformly distributed for\ncontinuous response but not for discrete response. We propose to use randomized\npredictive p-values (RPP) for diagnosing models with discrete responses. RPPs\ncan be transformed to \"residuals\" with normal distribution, called NRPPs by us.\nNRPPs can be used to diagnose all regression models with scalar response using\nthe same way for diagnosing normal regression. The NRPPs are nearly the same as\nthe randomized quantile residuals (RQR), which are previously proposed by Dunn\nand Smyth (1996) but remain little known by statisticians. This paper provides\nan exposition of RQR using the RPP perspective. The contributions of this\nexposition include: (1) we give a rigorous proof of uniformity of RPP and\nillustrative examples to explain the uniformity under the true model; (2) we\nconduct extensive simulation studies to demonstrate the normality of NRPPs\nunder the true model; (3) our simulation studies also show that the NRPP method\nis a versatile diagnostic tool for detecting many kinds of model inadequacies\ndue to lack of complexity. The effectiveness of NRPP is further demonstrated\nwith a health utilization dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 21:14:20 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 17:04:23 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 15:27:53 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Feng", "Cindy", ""], ["Sadeghpour", "Alireza", ""], ["Li", "Longhai", ""]]}, {"id": "1708.08543", "submitter": "Joonha Park", "authors": "Joonha Park and Edward L. Ionides", "title": "Inference on high-dimensional implicit dynamic models using a guided\n  intermediate resampling filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for inference on moderately high-dimensional, nonlinear,\nnon-Gaussian, partially observed Markov process models for which the transition\ndensity is not analytically tractable. Markov processes with intractable\ntransition densities arise in models defined implicitly by simulation\nalgorithms. Widely used particle filter methods are applicable to nonlinear,\nnon-Gaussian models but suffer from the curse of dimensionality. Improved\nscalability is provided by ensemble Kalman filter methods, but these are\ninappropriate for highly nonlinear and non-Gaussian models. We propose a\nparticle filter method having improved practical and theoretical scalability\nwith respect to the model dimension. This method is applicable to implicitly\ndefined models having analytically intractable transition densities. Our method\nis developed based on the assumption that the latent process is defined in\ncontinuous time and that a simulator of this latent process is available. In\nthis method, particles are propagated at intermediate time intervals between\nobservations and are resampled based on a forecast likelihood of future\nobservations. We combine this particle filter with parameter estimation\nmethodology to enable likelihood-based inference for highly nonlinear\nspatiotemporal systems. We demonstrate our methodology on a stochastic Lorenz\n96 model and a model for the population dynamics of infectious diseases in a\nnetwork of linked regions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 21:54:57 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 16:51:49 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 20:39:26 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 01:44:19 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Park", "Joonha", ""], ["Ionides", "Edward L.", ""]]}, {"id": "1708.08676", "submitter": "Jade Giguelay", "authors": "Jade Giguelay and Sylvie Huet", "title": "Testing k-monotonicity of a discrete distribution. Application to the\n  estimation of the number of classes in a population", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop here several goodness-of-fit tests for testing the k-monotonicity\nof a discrete density, based on the empirical distribution of the observations.\nOur tests are non-parametric, easy to implement and are proved to be\nasymptotically of the desired level and consistent. We propose an estimator of\nthe degree of k-monotonicity of the distribution based on the non-parametric\ngoodness-of-fit tests. We apply our work to the estimation of the total number\nof classes in a population. A large simulation study allows to assess the\nperformances of our procedures.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 10:03:14 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Giguelay", "Jade", ""], ["Huet", "Sylvie", ""]]}, {"id": "1708.08688", "submitter": "Benedikt M. P\\\"otscher", "authors": "Benedikt M. P\\\"otscher and David Preinerstorfer", "title": "Further Results on Size and Power of Heteroskedasticity and\n  Autocorrelation Robust Tests, with an Application to Trend Testing", "comments": "Revised version. Some restructuring, some errors corrected, new\n  results added", "journal-ref": "Electronic Journal of Statistics 13 (2019), 3893-3942", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We complement the theory developed in Preinerstorfer and P\\\"otscher (2016)\nwith further finite sample results on size and power of heteroskedasticity and\nautocorrelation robust tests. These allows us, in particular, to show that the\nsufficient conditions for the existence of size-controlling critical values\nrecently obtained in P\\\"otscher and Preinerstorfer (2018) are often also\nnecessary. We furthermore apply the results obtained to tests for hypotheses on\ndeterministic trends in stationary time series regressions, and find that many\ntests currently used are strongly size-distorted.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 10:44:22 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 10:04:22 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Preinerstorfer", "David", ""]]}, {"id": "1708.08719", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob (Harvard University), Lawrence M. Murray (Uppsala\n  University), Chris C. Holmes (University of Oxford), Christian P. Robert\n  (Universit\\'e Paris-Dauphine, PSL Research University, University of Warwick)", "title": "Better together? Statistical learning in models made of modules", "comments": "31 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern applications, statisticians are faced with integrating\nheterogeneous data modalities relevant for an inference, prediction, or\ndecision problem. In such circumstances, it is convenient to use a graphical\nmodel to represent the statistical dependencies, via a set of connected\n\"modules\", each relating to a specific data modality, and drawing on specific\ndomain expertise in their development. In principle, given data, the\nconventional statistical update then allows for coherent uncertainty\nquantification and information propagation through and across the modules.\nHowever, misspecification of any module can contaminate the estimate and update\nof others, often in unpredictable ways. In various settings, particularly when\ncertain modules are trusted more than others, practitioners have preferred to\navoid learning with the full model in favor of approaches that restrict the\ninformation propagation between modules, for example by restricting propagation\nto only particular directions along the edges of the graph. In this article, we\ninvestigate why these modular approaches might be preferable to the full model\nin misspecified settings. We propose principled criteria to choose between\nmodular and full-model approaches. The question arises in many applied\nsettings, including large stochastic dynamical systems, meta-analysis,\nepidemiological models, air pollution models,\npharmacokinetics-pharmacodynamics, and causal inference with propensity scores.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 12:08:23 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Jacob", "Pierre E.", "", "Harvard University"], ["Murray", "Lawrence M.", "", "Uppsala\n  University"], ["Holmes", "Chris C.", "", "University of Oxford"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, PSL Research University, University of Warwick"]]}, {"id": "1708.08911", "submitter": "Sameer Deshpande", "authors": "Sameer K. Deshpande and Veronika Rockova and Edward I. George", "title": "Simultaneous Variable and Covariance Selection with the Multivariate\n  Spike-and-Slab Lasso", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2019.1593179", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian procedure for simultaneous variable and covariance\nselection using continuous spike-and-slab priors in multivariate linear\nregression models where q possibly correlated responses are regressed onto p\npredictors. Rather than relying on a stochastic search through the\nhigh-dimensional model space, we develop an ECM algorithm similar to the EMVS\nprocedure of Rockova & George (2014) targeting modal estimates of the matrix of\nregression coefficients and residual precision matrix. Varying the scale of the\ncontinuous spike densities facilitates dynamic posterior exploration and allows\nus to filter out negligible regression coefficients and partial covariances\ngradually. Our method is seen to substantially outperform regularization\ncompetitors on simulated data. We demonstrate our method with a re-examination\nof data from a recent observational study of the effect of playing high school\nfootball on several later-life cognition, psychological, and socio-economic\noutcomes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 17:56:42 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 12:34:53 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Deshpande", "Sameer K.", ""], ["Rockova", "Veronika", ""], ["George", "Edward I.", ""]]}, {"id": "1708.09291", "submitter": "Lynn Roy LaMotte", "authors": "Lynn Roy LaMotte", "title": "Yates's MWSM SS in the General Linear Model", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.08956", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1934, F. Yates described a sum of squares for testing factor main effects\nin saturated unbalanced models for effects of two factors. He claimed no\nparticular properties of this sum of squares other than that it provided an\n\"efficient estimate of the variance from the A means of the sub-class means...\n.\" Although it became widely regarded as the gold standard in the two-factor\nmodel, its fundamental properties and relations to other sums of squares for\nthe same model were not established until decades later. Its method has not\nbeen extended to more general settings. This paper shows how Yates's approach\ncan be extended to construct numerator sums of squares for test statistics for\nlinear hypotheses in general linear models. It is shown that Yates's sum of\nsquares is equivalent to the restricted model - full model difference in error\nsum of squares, which in turn is shown to be the unique sum of squares that\ntests exactly the hypothesis in question.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 18:56:48 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["LaMotte", "Lynn Roy", ""]]}, {"id": "1708.09305", "submitter": "Anthony Hou", "authors": "Jiajie Chen, Anthony Hou, Thomas Y. Hou", "title": "A Pseudo Knockoff Filter for Correlated Features", "comments": "25 pages, 5 figures", "journal-ref": "Information and Inference: A Journal of the IMA, Volume 8, Issue\n  2, June 2019, Pages 313-341", "doi": "10.1093/imaiai/iay012", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2015, Barber and Candes introduced a new variable selection procedure\ncalled the knockoff filter to control the false discovery rate (FDR) and prove\nthat this method achieves exact FDR control. Inspired by the work of Barber and\nCandes (2015), we propose and analyze a pseudo-knockoff filter that inherits\nsome advantages of the original knockoff filter and has more flexibility in\nconstructing its knockoff matrix. Moreover, we perform a number of numerical\nexperiments that seem to suggest that the pseudo knockoff filter with the half\nLasso statistic has FDR control and offers more power than the original\nknockoff filter with the Lasso Path or the half Lasso Statistic for the\nnumerical examples that we consider in this paper. Although we cannot establish\nrigorous FDR control for the pseudo knockoff filter, we provide some partial\nanalysis of the pseudo knockoff filter with the half Lasso statistic and\nestablish a uniform FDP bound and an expectation inequality.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 14:44:01 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 17:00:08 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 19:47:45 GMT"}, {"version": "v4", "created": "Mon, 22 Jul 2019 04:26:39 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chen", "Jiajie", ""], ["Hou", "Anthony", ""], ["Hou", "Thomas Y.", ""]]}, {"id": "1708.09507", "submitter": "Shujie Ma", "authors": "Shujie Ma, Oliver Linton and Jiti Gao", "title": "Estimation in Semiparametric Quantile Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimation methodology for a semiparametric quantile factor\npanel model. We provide tools for inference that are robust to the existence of\nmoments and to the form of weak cross-sectional dependence in the idiosyncratic\nerror term. We apply our method to daily stock return data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 00:16:23 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Ma", "Shujie", ""], ["Linton", "Oliver", ""], ["Gao", "Jiti", ""]]}, {"id": "1708.09543", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Rheanna Mainzer", "title": "Confidence Intervals that Utilize Uncertain Prior Information about\n  Exogeneity in Panel Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider panel data modelled by a linear random intercept model that includes\na time-varying covariate. Suppose that we have uncertain prior information that\nthis covariate is exogenous. We present a new confidence interval for the slope\nparameter that utilizes this uncertain prior information. This interval has\nminimum coverage probability very close to its nominal coverage. Let the scaled\nexpected length of this new confidence interval be its expected length divided\nby the expected length of the confidence interval, with the same minimum\ncoverage, constructed using the fixed effects model. This new interval has\nscaled expected length that (a) is substantially less than 1 when the prior\ninformation is correct, (b) has a maximum value that is not too much larger\nthan 1 and (c) is close to 1 when the data strongly contradict the prior\ninformation. We illustrate the properties of this new interval using an airfare\ndata set.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 02:50:33 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Kabaila", "Paul", ""], ["Mainzer", "Rheanna", ""]]}, {"id": "1708.09608", "submitter": "Ulrike Schneider", "authors": "Karl Ewald, Ulrike Schneider", "title": "On the Distribution, Model Selection Properties and Uniqueness of the\n  Lasso Estimator in Low and High Dimensions", "comments": "restructured proofs, added examples", "journal-ref": "Electron. J. Statist. 14 (2020), 944-969", "doi": "10.1214/20-EJS1687", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive expressions for the finite-sample distribution of the Lasso\nestimator in the context of a linear regression model in low as well as in high\ndimensions by exploiting the structure of the optimization problem defining the\nestimator. In low dimensions, we assume full rank of the regressor matrix and\npresent expressions for the cumulative distribution function as well as the\ndensities of the absolutely continuous parts of the estimator. Our results are\npresented for the case of normally distributed errors, but do not hinge on this\nassumption and can easily be generalized. Additionally, we establish an\nexplicit formula for the correspondence between the Lasso and the least-squares\nestimator. We derive analogous results for the distribution in less explicit\nform in high dimensions where we make no assumptions on the regressor matrix at\nall. In this setting, we also investigate the model selection properties of the\nLasso and show that possibly only a subset of models might be selected by the\nestimator, completely independently of the observed response vector. Finally,\nwe present a condition for uniqueness of the estimator that is necessary as\nwell as sufficient.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 08:16:18 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 21:58:15 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 16:35:30 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ewald", "Karl", ""], ["Schneider", "Ulrike", ""]]}, {"id": "1708.09692", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Tuhin Majumder, Ayanendranath Basu", "title": "General Robust Bayes Pseudo-Posterior: Exponential Convergence results\n  with Applications", "comments": "To appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Bayesian inference is an immensely popular paradigm among a large\nsegment of scientists including statisticians, most applications consider\nobjective priors and need critical investigations (Efron, 2013, Science). While\nit has several optimal properties, a major drawback of Bayesian inference is\nthe lack of robustness against data contamination and model misspecification,\nwhich becomes pernicious in the use of objective priors. This paper presents\nthe general formulation of a Bayes pseudo-posterior distribution yielding\nrobust inference. Exponential convergence results related to the new\npseudo-posterior and the corresponding Bayes estimators are established under\nthe general parametric set-up and illustrations are provided for the\nindependent stationary as well as non-homogeneous models. Several additional\ndetails and properties of the procedure are described, including the estimation\nunder fixed-design regression models.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 12:58:11 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 10:02:54 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ghosh", "Abhik", ""], ["Majumder", "Tuhin", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1708.09695", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh and Ayanendranath Basu and Leandro Pardo", "title": "Robust Wald-Type Tests under Random Censoring with Applications to\n  Clinical Trial Analyses", "comments": "Pre-print, Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomly censored survival data are frequently encountered in applied\nsciences including biomedical or reliability applications and clinical trial\nanalyses. Testing the significance of statistical hypotheses is crucial in such\nanalyses to get conclusive inference but the existing likelihood based tests,\nunder a fully parametric model, are extremely non-robust against outliers in\nthe data. Although, there exists a few robust parameter estimators (e.g.,\nM-estimators and minimum density power divergence estimators) given randomly\ncensored data, there is hardly any robust testing procedure available in the\nliterature in this context. One of the major difficulties in this context is\nthe construction of a suitable consistent estimator of the asymptotic variance\nof M estimators; the latter is a function of the unknown censoring\ndistribution. In this paper, we take the first step in this direction by\nproposing a consistent estimator of asymptotic variance of the M-estimators\nbased on randomly censored data without any assumption on the form of the\ncensoring scheme. We then describe and study a class of robust Wald-type tests\nfor parametric statistical hypothesis, both simple as well as composite, under\nsuch set-up, along with their general asymptotic and robustness properties.\nRobust tests for comparing two independent randomly censored samples and robust\ntests against one sided alternatives are also discussed. Their advantages and\nusefulness are demonstrated for the tests based on the minimum density power\ndivergence estimators with specific attention to clinical trial analyses.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 13:03:33 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 18:39:52 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""], ["Pardo", "Leandro", ""]]}, {"id": "1708.09824", "submitter": "Georgios Karagiannis", "authors": "Georgios Karagiannis, Bledar A. Konomi, Guang Lin", "title": "On the Bayesian calibration of expensive computer models with input\n  dependent parameters", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2017.08.002", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models, aiming at simulating a complex real system, are often\ncalibrated in the light of data to improve performance. Standard calibration\nmethods assume that the optimal values of calibration parameters are invariant\nto the model inputs. In several real world applications where models involve\ncomplex parametrizations whose optimal values depend on the model inputs, such\nan assumption can be too restrictive and may lead to misleading results. We\npropose a fully Bayesian methodology that produces input dependent optimal\nvalues for the calibration parameters, as well as it characterizes the\nassociated uncertainties via posterior distributions. Central to methodology is\nthe idea of formulating the calibration parameter as a step function whose\nuncertain structure is modeled properly via a binary treed process. Our method\nis particularly suitable to address problems where the computer model requires\nthe selection of a sub-model from a set of competing ones, but the choice of\nthe `best' sub-model may change with the input values. The method produces a\nselection probability for each sub-model given the input. We propose suitable\nreversible jump operations to facilitate the challenging computations. We\nassess the performance of our method against benchmark examples, and use it to\nanalyze a real world application with a large-scale climate model.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 17:19:00 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Karagiannis", "Georgios", ""], ["Konomi", "Bledar A.", ""], ["Lin", "Guang", ""]]}]