[{"id": "1604.00314", "submitter": "Jairo F\\'uquene", "authors": "Jairo F\\'uquene and Mark Steel and David Rossell", "title": "On choosing mixture components via non-local priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing the number of mixture components remains an elusive challenge. Model\nselection criteria can be either overly liberal or conservative and return\npoorly-separated components of limited practical use. We formalize non-local\npriors (NLPs) for mixtures and show how they lead to well-separated components\nwith non-negligible weight, interpretable as distinct subpopulations. We also\npropose an estimator for posterior model probabilities under local and\nnon-local priors, showing that Bayes factors are ratios of posterior to prior\nempty-cluster probabilities. The estimator is widely applicable and helps set\nthresholds to drop unoccupied components in overfitted mixtures. We suggest\ndefault prior parameters based on multi-modality for Normal/T mixtures and\nminimal informativeness for categorical outcomes. We characterise theoretically\nthe NLP-induced sparsity, derive tractable expressions and algorithms. We fully\ndevelop Normal, Binomial and product Binomial mixtures but the theory,\ncomputation and principles hold more generally. We observed a serious lack of\nsensitivity of the Bayesian information criterion (BIC), insufficient parsimony\nof the AIC and a local prior, and a mixed behavior of the singular BIC. We also\nconsidered overfitted mixtures, their performance was competitive but depended\non tuning parameters. Under our default prior elicitation NLPs offered a good\ncompromise between sparsity and power to detect meaningfully-separated\ncomponents.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 16:22:00 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 22:00:51 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 14:26:00 GMT"}, {"version": "v4", "created": "Fri, 15 Jun 2018 16:37:09 GMT"}, {"version": "v5", "created": "Tue, 11 Jun 2019 16:47:19 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["F\u00faquene", "Jairo", ""], ["Steel", "Mark", ""], ["Rossell", "David", ""]]}, {"id": "1604.00376", "submitter": "Anindya Bhadra", "authors": "Anindya Bhadra, Arvind Rao and Veerabhadran Baladandayuthapani", "title": "Inferring network structure in non-normal and mixed discrete-continuous\n  genomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring dependence structure through undirected graphs is crucial for\nuncovering the major modes of multivariate interaction among high-dimensional\ngenomic markers that are potentially associated with cancer. Traditionally,\nconditional independence has been studied using sparse Gaussian graphical\nmodels for continuous data and sparse Ising models for discrete data. However,\nthere are two clear situations when these approaches are inadequate. The first\noccurs when the data are continuous but display non-normal marginal behavior\nsuch as heavy tails or skewness, rendering an assumption of normality\ninappropriate. The second occurs when a part of the data is ordinal or discrete\n(e.g., presence or absence of a mutation) and the other part is continuous\n(e.g., expression levels of genes or proteins). In this case, the existing\nBayesian approaches typically employ a latent variable framework for the\ndiscrete part that precludes inferring conditional independence among the data\nthat are actually observed. The current article overcomes these two challenges\nin a unified framework using Gaussian scale mixtures. Our framework is able to\nhandle continuous data that are not normal and data that are of mixed\ncontinuous and discrete nature, while still being able to infer a sparse\nconditional sign independence structure among the observed data. Extensive\nperformance comparison in simulations with alternative techniques and an\nanalysis of a real cancer genomics data set demonstrate the effectiveness of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 19:37:31 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Bhadra", "Anindya", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1604.00447", "submitter": "Kyungchul Song", "authors": "Kyungchul Song", "title": "Ordering-Free Inference from Locally Dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a data-rich environment where the data set has a very\nlarge cross-sectional dimension, is likely to exhibit local dependence, and yet\nis hard to determine the dependence ordering. Such a situation arises, for\nexample, when the data set is collected from the Internet, through a method of\nweb crawling. This paper proposes an approach of randomized subsampling\ninference, where one constructs a test statistic by aggregating many randomized\ntest statistics using random draws of subsamples, and uses for inference the\nconditional distribution of the test statistic given data. This paper explores\ntwo approaches of such inference: one based on an M-type statistic constructed\nfrom randomized mean statistics and the other based on a U-type statistic\nconstructed from randomized U-statistics. This paper provides conditions for\nlocal dependence, the number of the random draws, and the subsample size, under\nwhich randomized subsampling inference is asymptotically valid. From the Monte\nCarlo simulation studies, this paper finds that the randomized subsampling\ninference based on the U-type statistics performs better than that based on the\nM-type statistics.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 01:04:55 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 00:31:49 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 23:21:10 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Song", "Kyungchul", ""]]}, {"id": "1604.00540", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Rafael Izbicki, Ann B. Lee", "title": "Nonparametric Conditional Density Estimation in a High-Dimensional\n  Regression Setting", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2015.1094393", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some applications (e.g., in cosmology and economics), the regression\nE[Z|x] is not adequate to represent the association between a predictor x and a\nresponse Z because of multi-modality and asymmetry of f(z|x); using the full\ndensity instead of a single-point estimate can then lead to less bias in\nsubsequent analysis. As of now, there are no effective ways of estimating\nf(z|x) when x represents high-dimensional, complex data. In this paper, we\npropose a new nonparametric estimator of f(z|x) that adapts to sparse\n(low-dimensional) structure in x. By directly expanding f(z|x) in the\neigenfunctions of a kernel-based operator, we avoid tensor products in high\ndimensions as well as ratios of estimated densities. Our basis functions are\northogonal with respect to the underlying data distribution, allowing fast\nimplementation and tuning of parameters. We derive rates of convergence and\nshow that the method adapts to the intrinsic dimension of the data. We also\ndemonstrate the effectiveness of the series method on images, spectra, and an\napplication to photometric redshift estimation of galaxies.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 18:01:03 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Izbicki", "Rafael", ""], ["Lee", "Ann B.", ""]]}, {"id": "1604.00570", "submitter": "Florian Maire", "authors": "Florian Maire, Eric Moulines, Sidonie Lefebvre", "title": "Online EM for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach to perform unsupervised sequential learning for functional\ndata is proposed. Our goal is to extract reference shapes (referred to as\ntemplates) from noisy, deformed and censored realizations of curves and images.\nOur model generalizes the Bayesian dense deformable template model\n(Allassonni\\`ere et al., 2007), a hierarchical model in which the template is\nthe function to be estimated and the deformation is a nuisance, assumed to be\nrandom with a known prior distribution. The templates are estimated using a\nMonte Carlo version of the online Expectation-Maximization algorithm, extending\nthe work from Capp\\'e and Moulines (2009). Our sequential inference framework\nis significantly more computationally efficient than equivalent batch learning\nalgorithms, especially when the missing data is high-dimensional. Some\nnumerical illustrations on curve registration problem and templates extraction\nfrom images are provided to support our findings.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 23:00:07 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Maire", "Florian", ""], ["Moulines", "Eric", ""], ["Lefebvre", "Sidonie", ""]]}, {"id": "1604.00695", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte\n  Carlo", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When properly tuned, Hamiltonian Monte Carlo scales to some of the most\nchallenging high-dimensional problems at the frontiers of applied statistics,\nbut when that tuning is suboptimal the performance leaves much to be desired.\nIn this paper I show how suboptimal choices of one critical degree of freedom,\nthe cotangent disintegration, manifest in readily observed diagnostics that\nfacilitate the robust application of the algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 22:18:37 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1604.00860", "submitter": "Daniel Simpson", "authors": "H{\\aa}vard Rue, Andrea Riebler, Sigrunn H. S{\\o}rbye, Janine B.\n  Illian, Daniel P. Simpson, and Finn K. Lindgren", "title": "Bayesian Computing with INLA: A Review", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key operation in Bayesian inference, is to compute high-dimensional\nintegrals. An old approximate technique is the Laplace method or approximation,\nwhich dates back to Pierre- Simon Laplace (1774). This simple idea approximates\nthe integrand with a second order Taylor expansion around the mode and computes\nthe integral analytically. By developing a nested version of this classical\nidea, combined with modern numerical techniques for sparse matrices, we obtain\nthe approach of Integrated Nested Laplace Approximations (INLA) to do\napproximate Bayesian inference for latent Gaussian models (LGMs). LGMs\nrepresent an important model-abstraction for Bayesian inference and include a\nlarge proportion of the statistical models used today. In this review, we will\ndiscuss the reasons for the success of the INLA-approach, the R-INLA package,\nwhy it is so accurate, why the approximations are very quick to compute and why\nLGMs make such a useful concept for Bayesian computing.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 13:53:34 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 13:41:35 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Rue", "H\u00e5vard", ""], ["Riebler", "Andrea", ""], ["S\u00f8rbye", "Sigrunn H.", ""], ["Illian", "Janine B.", ""], ["Simpson", "Daniel P.", ""], ["Lindgren", "Finn K.", ""]]}, {"id": "1604.00912", "submitter": "Murat Bilgel", "authors": "Murat Bilgel, Jerry L. Prince, Dean F. Wong, Susan M. Resnick, Bruno\n  M. Jedynak", "title": "A multivariate nonlinear mixed effects model for longitudinal image\n  analysis: Application to amyloid imaging", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2016.04.001", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is important to characterize the temporal trajectories of disease-related\nbiomarkers in order to monitor progression and identify potential points of\nintervention. This is especially important for neurodegenerative diseases, as\ntherapeutic intervention is most likely to be effective in the preclinical\ndisease stages prior to significant neuronal damage. Longitudinal neuroimaging\nallows for the measurement of structural, functional, and metabolic integrity\nof the brain over time at the level of voxels. However, commonly used\nlongitudinal analysis approaches, such as linear mixed effects models, do not\naccount for the fact that individuals enter a study at various disease stages\nand progress at different rates, and generally consider each voxelwise measure\nindependently. We propose a multivariate nonlinear mixed effects model for\nestimating the trajectories of voxelwise neuroimaging biomarkers from\nlongitudinal data that accounts for such differences across individuals. The\nmethod involves the prediction of a progression score for each visit based on a\ncollective analysis of voxelwise biomarker data within an\nexpectation-maximization framework that efficiently handles large amounts of\nmeasurements and variable number of visits per individual, and accounts for\nspatial correlations among voxels. This score allows individuals with similar\nprogressions to be aligned and analyzed together, which enables the\nconstruction of a trajectory of brain changes as a function of an underlying\nprogression or disease stage. Application of our method to studying images of\nbeta-amyloid deposition, a hallmark of preclinical Alzheimer's disease,\nsuggests that precuneus is the earliest cortical region to accumulate amyloid.\nThe proposed method can be applied to other types of longitudinal imaging data,\nincluding metabolism, blood flow, tau, and structural imaging-derived measures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 15:28:58 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Bilgel", "Murat", ""], ["Prince", "Jerry L.", ""], ["Wong", "Dean F.", ""], ["Resnick", "Susan M.", ""], ["Jedynak", "Bruno M.", ""]]}, {"id": "1604.00954", "submitter": "Johan Segers", "authors": "R. A. Davis, H. Drees, J. Segers and M. Warcho{\\l}", "title": "Inference on the tail process with application to financial time series\n  modelling", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To draw inference on serial extremal dependence within heavy-tailed Markov\nchains, Drees, Segers and Warcho{\\l} [Extremes (2015) 18, 369--402] proposed\nnonparametric estimators of the spectral tail process. The methodology can be\nextended to the more general setting of a stationary, regularly varying time\nseries. The large-sample distribution of the estimators is derived via\nempirical process theory for cluster functionals. The finite-sample performance\nof these estimators is evaluated via Monte Carlo simulations. Moreover, two\ndifferent bootstrap schemes are employed which yield confidence intervals for\nthe pre-asymptotic spectral tail process: the stationary bootstrap and the\nmultiplier block bootstrap. The estimators are applied to stock price data to\nstudy the persistence of positive and negative shocks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 17:23:31 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 11:02:46 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Davis", "R. A.", ""], ["Drees", "H.", ""], ["Segers", "J.", ""], ["Warcho\u0142", "M.", ""]]}, {"id": "1604.01064", "submitter": "Matthew Wheeler", "authors": "Matthew W. Wheeler and David B. Dunson and Amy H. Herring", "title": "Bayesian Local Extrema Splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of shape restricted nonparametric regression on a\nclosed set X ?\\in R; where it is reasonable to assume the function has no more\nthan H local extrema interior to X: Following a Bayesian approach we develop a\nnonparametric prior over a novel class of local extrema splines. This approach\nis shown to be consistent when modeling any continuously differentiable\nfunction within the class of functions considered, and is used to develop\nmethods for hypothesis testing on the shape of the curve. Sampling algorithms\nare developed, and the method is applied in simulation studies and data\nexamples where the shape of the curve is of interest.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 21:06:40 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Wheeler", "Matthew W.", ""], ["Dunson", "David B.", ""], ["Herring", "Amy H.", ""]]}, {"id": "1604.01105", "submitter": "Amit Sharma", "authors": "Amit Sharma, Dan Cosley", "title": "Distinguishing between Personal Preferences and Social Influence in\n  Online Activity Feeds", "comments": "13 pages, ACM CSCW 2016", "journal-ref": null, "doi": "10.1145/2818048.2819982", "report-no": null, "categories": "cs.SI cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online social networks thrive on automatic sharing of friends'\nactivities to a user through activity feeds, which may influence the user's\nnext actions. However, identifying such social influence is tricky because\nthese activities are simultaneously impacted by influence and homophily. We\npropose a statistical procedure that uses commonly available network and\nobservational data about people's actions to estimate the extent of\ncopy-influence---mimicking others' actions that appear in a feed. We assume\nthat non-friends don't influence users; thus, comparing how a user's activity\ncorrelates with friends versus non-friends who have similar preferences can\nhelp tease out the effect of copy-influence.\n  Experiments on datasets from multiple social networks show that estimates\nthat don't account for homophily overestimate copy-influence by varying, often\nlarge amounts. Further, copy-influence estimates fall below 1% of total actions\nin all networks: most people, and almost all actions, are not affected by the\nfeed. Our results question common perceptions around the extent of\ncopy-influence in online social networks and suggest improvements to diffusion\nand recommendation models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 01:16:30 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Sharma", "Amit", ""], ["Cosley", "Dan", ""]]}, {"id": "1604.01120", "submitter": "Takashi Goda", "authors": "Takashi Goda", "title": "Unbiased Monte Carlo estimation for the expected value of partial\n  perfect information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected value of partial perfect information (EVPPI) denotes the value\nof eliminating uncertainty on a subset of unknown parameters involved in a\ndecision model. The EVPPI can be regarded as a decision-theoretic sensitivity\nindex, and has been widely used for identifying relatively important unknown\nparameters. It follows from Jensen's inequality, however, that the standard\nnested Monte Carlo computation of the EVPPI results in biased estimates. In\nthis paper we introduce two unbiased Monte Carlo estimators for the EVPPI based\non multilevel Monte Carlo method, introduced by Heinrich (1998) and Giles\n(2008), and its extension by Rhee and Glynn (2012, 2015). Our unbiased\nestimators are simple and straightforward to implement, and thus are of highly\npractical use. Numerical experiments show that even the convergence behaviors\nof our unbiased estimators are superior to that of the standard nested Monte\nCarlo estimator.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 03:01:15 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Goda", "Takashi", ""]]}, {"id": "1604.01191", "submitter": "Joris Chau", "authors": "Joris Chau and Rainer von Sachs", "title": "Functional mixed effects wavelet estimation for spectra of replicated\n  time series", "comments": null, "journal-ref": "Electron. J. Statist. Volume 10, Number 2 (2016), 2461-2510", "doi": "10.1214/16-EJS1181", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by spectral analysis of replicated brain signal time series, we\npropose a functional mixed effects approach to model replicate-specific\nspectral densities as random curves varying about a deterministic\npopulation-mean spectrum. In contrast to existing work, we do not assume the\nreplicate-specific spectral curves to be independent, i.e. there may exist\nexplicit correlation between different replicates in the population. By\nprojecting the replicate-specific curves onto an orthonormal wavelet basis,\nestimation and prediction is carried out under an equivalent linear mixed\neffects model in the wavelet coefficient domain. To cope with potentially very\nlocalized features of the spectral curves, we develop estimators and predictors\nbased on a combination of generalized least squares estimation and nonlinear\nwavelet thresholding, including asymptotic confidence sets for the\npopulation-mean curve. We derive risk bounds for the nonlinear wavelet\nestimator of the population-mean curve, a result that reflects the influence of\ncorrelation between different curves in the replicate-population, and we derive\nconsistency of the estimators of the inter- and intra-curve correlation\nstructure in an appropriate sparseness class of functions. To illustrate the\nproposed functional mixed effects model and our estimation and prediction\nprocedures, we present several simulated time series data examples and we\nanalyze a motivating brain signal dataset recorded during an associative\nlearning experiment.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 09:13:09 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 15:38:07 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2016 10:47:41 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Chau", "Joris", ""], ["von Sachs", "Rainer", ""]]}, {"id": "1604.01233", "submitter": "Lucy Kerns Ph.D.", "authors": "Lucy Kerns", "title": "Construction of Simultaneous Confidence Bands for Multiple Logistic\n  Regression Models over Restricted Regions", "comments": "15 pages, 2 figures in 2016, Statistics: A Journal of Theoretical and\n  Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents methods for constructing an asymptotic hyperbolic band\nunder the multiple logistic regression model when the predictor variables are\nrestricted to a specific region $\\mathscr{X}$. Scheff\\'{e}'s method yields\nunnecessarily wide, and hence conservative, bands if the predictor variables\ncan be restricted to a certain region. Piegorsch and Casella (1988) developed a\nprocedure to build an asymptotic confidence band for the multiple logistic\nregression model over particular regions. Those regions are shown to be special\ncases of the region $\\mathscr{X}$, which was first investigated by Seppanen and\nUusipaikka (1992) in the multiple linear regression context. This article also\nprovides methods for constructing conservative confidence bands when the\nrestricted region is not of the specified form. Particularly, rectangular\nrestricted regions, which are commonly encountered in practice, are considered.\nTwo examples are given to illustrate the proposed methodology, and one example\nshows that the proposed procedure outperforms the method given by Piegorsch and\nCasella (1988).\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 12:39:02 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Kerns", "Lucy", ""]]}, {"id": "1604.01242", "submitter": "Lucy Kerns Ph.D.", "authors": "Lucy Kerns", "title": "Confidence Bands for the Logistic and Probit Regression Models Over\n  Intervals", "comments": "20 pages, 2 figures in 2015, Communication in Statistics - Theory and\n  Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents methods for the construction of two-sided and one-sided\nsimultaneous hyperbolic bands for the logistic and probit regression models\nwhen the predictor variable is restricted to a given interval. The bands are\nconstructed based on the asymptotic properties of the maximum likelihood\nestimators. Past articles have considered building two-sided asymptotic\nconfidence bands for the logistic model, such as Piegorsch and Casella (1988).\nHowever, the confidence bands given by Piegorsch and Casella are conservative\nunder a single interval restriction, and it is shown in this article that their\nbands can be sharpened using the methods proposed here. Furthermore, no method\nhas yet appeared in the literature for constructing one-sided confidence bands\nfor the logistic model, and no work has been done for building confidence bands\nfor the probit model, over a limited range of the predictor variable. This\narticle provides methods for computing critical points in these areas.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 13:04:03 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Kerns", "Lucy", ""]]}, {"id": "1604.01250", "submitter": "Alvin Chua", "authors": "Christopher J. Moore, Alvin J. K. Chua, Christopher P. L. Berry,\n  Jonathan R. Gair", "title": "Fast methods for training Gaussian processes on large data sets", "comments": "Fixed missing references", "journal-ref": "R. Soc. Open Sci. 3, 160125 (2016)", "doi": "10.1098/rsos.160125", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression (GPR) is a non-parametric Bayesian technique for\ninterpolating or fitting data. The main barrier to further uptake of this\npowerful tool rests in the computational costs associated with the matrices\nwhich arise when dealing with large data sets. Here, we derive some simple\nresults which we have found useful for speeding up the learning stage in the\nGPR algorithm, and especially for performing Bayesian model comparison between\ndifferent covariance functions. We apply our techniques to both synthetic and\nreal data and quantify the speed-up relative to using nested sampling to\nnumerically evaluate model evidences.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 13:29:15 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 10:38:28 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Moore", "Christopher J.", ""], ["Chua", "Alvin J. K.", ""], ["Berry", "Christopher P. L.", ""], ["Gair", "Jonathan R.", ""]]}, {"id": "1604.01268", "submitter": "Cristiano Villa", "authors": "Cristiano Villa", "title": "Bayesian Estimation of the Threshold of a Generalised Pareto\n  Distribution for Heavy-Tailed Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss a method to define prior distributions for the\nthreshold of a generalised Pareto distribution, in particular when its\napplications are directed to heavy-tailed data. We propose to assign prior\nprobabilities to the order statistics of a given set of observations. In other\nwords, we assume that the threshold coincides to one of the data points. We\nshow two ways of defining a prior: by assigning equal mass to each order\nstatistic, that is a uniform prior, and by considering the worth that every\norder statistic has in representing the true threshold. Both proposed priors\nrepresent a scenario of minimal information, and we study their adequacy\nthrough simulation exercises and by analysing two applications from insurance\nand from finance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 14:21:14 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Villa", "Cristiano", ""]]}, {"id": "1604.01443", "submitter": "Li Ma", "authors": "Li Ma and Jacopo Soriano", "title": "Analysis of distributional variation through multi-scale Beta-Binomial\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical analyses involve the comparison of multiple data sets\ncollected under different conditions in order to identify the difference in the\nunderlying distributions. A common challenge in multi-sample comparison is the\npresence of various confounders, or extraneous causes other than the conditions\nof interest that also contribute to the difference across the distributions.\nThey result in false findings, i.e., identified differences that are not\nreplicable in follow-up investigations. We consider an ANOVA approach to\naddressing this issue in multi-sample comparison---by collecting replicate data\nsets under each condition, thereby allowing the identification of the\ninteresting distributional variation from the extraneous ones. We introduce a\nmulti-scale Bayesian hierarchical model for the analysis of distributional\nvariation (ANDOVA) under this design, based on a collection of Beta-Binomial\ntests targeting variations of different scales at different locations across\nthe sample space. Instead treating the tests independently, the model employs a\ngraphical structure to introduce dependency among the individual tests thereby\nallowing borrowing of strength among them. We derive efficient inference recipe\nthrough a combination of numerical integration and message passing, and\nevaluate the ability of our method to effectively address ANDOVA through\nextensive simulation. We utilize our method to analyze a DNase-seq data set for\nidentifying differences in transcriptional factor binding.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 22:35:22 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ma", "Li", ""], ["Soriano", "Jacopo", ""]]}, {"id": "1604.01515", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (LMO, SELECT), Robin Genuer (ISPED, SISTM)", "title": "Comments on: \"A Random Forest Guided Tour\" by G. Biau and E. Scornet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a comment on the survey paper by Biau and Scornet (2016) about\nrandom forests. We focus on the problem of quantifying the impact of each\ningredient of random forests on their performance. We show that such a\nquantification is possible for a simple pure forest , leading to conclusions\nthat could apply more generally. Then, we consider \"hold-out\" random forests,\nwhich are a good middle point between \"toy\" pure forests and Breiman's original\nrandom forests.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 07:23:17 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Arlot", "Sylvain", "", "LMO, SELECT"], ["Genuer", "Robin", "", "ISPED, SISTM"]]}, {"id": "1604.01541", "submitter": "Deepesh Bhati Mr.", "authors": "Deepesh Bhati, Subrata Chakraborty and Snober Gowhar Lateef", "title": "An Alternative Discrete Skew Logistic Distribution", "comments": "17 pages, One figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an alternative Discrete skew Logistic distribution is\nproposed, which is derived by using the general approach of discretizing a\ncontinuous distribution while retaining its survival function. The properties\nof the distribution are explored and it is compared to a discrete distribution\ndefined on integers recently proposed in the literature. The estimation of its\nparameters are discussed, with particular focus on the maximum likelihood\nmethod and the method of proportion, which is particularly suitable for such a\ndiscrete model. A Monte Carlo simulation study is carried out to assess the\nstatistical properties of these inferential techniques. Application of the\nproposed model to a real life data is given as well.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 08:36:52 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Bhati", "Deepesh", ""], ["Chakraborty", "Subrata", ""], ["Lateef", "Snober Gowhar", ""]]}, {"id": "1604.01597", "submitter": "Jon Michael Gran", "authors": "J. M. Gran, R. Hoff, K. R{\\o}ysland, B. Ledergerber, J. Young, O. O.\n  Aalen", "title": "Estimating the treatment effect on the treated under time-dependent\n  confounding in an application to the Swiss HIV Cohort Study", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When comparing time-varying treatments in a non-randomised setting, one must\noften correct for time-dependent confounders that influence treatment choice\nover time and that are themselves influenced by treatment. We present a new two\nstep procedure, based on additive hazard regression and linear increments\nmodels, for handling such confounding when estimating average treatment effects\non the treated (ATT). The approach can also be used for mediation analysis. The\nmethod is applied to data from the Swiss HIV Cohort Study, estimating the\neffect of antiretroviral treatment on time to AIDS or death. Compared to other\nmethods for estimating the ATT, the proposed method is easy to implement using\navailable software packages in R.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 12:58:26 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 08:45:55 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Gran", "J. M.", ""], ["Hoff", "R.", ""], ["R\u00f8ysland", "K.", ""], ["Ledergerber", "B.", ""], ["Young", "J.", ""], ["Aalen", "O. O.", ""]]}, {"id": "1604.01785", "submitter": "Peter Gr\\\"unwald", "authors": "Peter Gr\\\"unwald", "title": "Safe Probability", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the idea of probability distributions that lead to reliable\npredictions about some, but not all aspects of a domain. The resulting notion\nof `safety' provides a fresh perspective on foundational issues in statistics,\nproviding a middle ground between imprecise probability and multiple-prior\nmodels on the one hand and strictly Bayesian approaches on the other. It also\nallows us to formalize fiducial distributions in terms of the set of random\nvariables that they can safely predict, thus taking some of the sting out of\nthe fiducial idea. By restricting probabilistic inference to safe uses, one\nalso automatically avoids paradoxes such as the Monty Hall problem. Safety\ncomes in a variety of degrees, such as \"validity\" (the strongest notion),\n\"calibration\", \"confidence safety\" and \"unbiasedness\" (almost the weakest\nnotion).\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:01:28 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Gr\u00fcnwald", "Peter", ""]]}, {"id": "1604.01844", "submitter": "Jose D. Perezgonzalez PhD", "authors": "Jose D. Perezgonzalez", "title": "Statistical sensitiveness for science", "comments": "25 pages, 1 figure, 2 tables, 3 supplemental materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research often necessitates of samples, yet obtaining large enough samples is\nnot always possible. When it is, the researcher may use one of two methods for\ndeciding upon the required sample size: rules-of-thumb, quick yet uncertain,\nand estimations for power, mathematically precise yet with the potential to\noverestimate or underestimate sample sizes when effect sizes are unknown.\nMisestimated sample sizes have negative repercussions in the form of increased\ncosts, abandoned projects or abandoned publication of non-significant results.\nHere I describe a procedure for estimating sample sizes adequate for the\ntesting approach which is most common in the behavioural, social, and\nbiomedical sciences, that of tests of significance developed by Fisher. The\nprocedure focuses on a desired minimum effect size for the research at hand and\nfinds the minimum sample size required for capturing such effect size as a\nstatistically significant result. In a similar fashion than power analyses,\nsensitiveness analyses can also be extended to finding the minimum effect for a\ngiven sample size a priori as well as to calculating sensitiveness a\nposteriori. The article provides a full tutorial for carrying out a\nsensitiveness analysis, as well as empirical support via simulation\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 01:28:28 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Perezgonzalez", "Jose D.", ""]]}, {"id": "1604.01874", "submitter": "Lixing Zhu", "authors": "Falong Tan, Xuehu Zhu and Lixing Zhu", "title": "A projection-based adaptive-to-model test for regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longstanding problem of existing empirical process-based tests for\nregressions is that when the number of covariates is greater than one, they\neither have no tractable limiting null distributions or are not omnibus. To\nattack this problem, we in this paper propose a projection-based\nadaptive-to-model approach. When the hypothetical model is parametric\nsingle-index, the method can fully utilize the dimension reduction model\nstructure under the null hypothesis as if the covariate were one-dimensional\nsuch that the martingale transformation-based test can be asymptotically\ndistribution-free. Further, the test can automatically adapt to the underlying\nmodel structure such that the test can be omnibus and thus detect alternative\nmodels distinct from the hypothetical model at the fastest possible rate in\nhypothesis testing. The method is examined through simulation studied and is\nillustrated by a real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 04:45:09 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Tan", "Falong", ""], ["Zhu", "Xuehu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1604.01919", "submitter": "Zhichao Jiang", "authors": "Zhichao Jiang and Peng Ding", "title": "Robust Modeling Using Non-Elliptically Contoured Multivariate t\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models based on multivariate t distributions are widely applied to analyze\ndata with heavy tails. However, all the marginal distributions of the\nmultivariate t distributions are restricted to have the same degrees of\nfreedom, making these models unable to describe different marginal\nheavy-tailedness. We generalize the traditional multivariate t distributions to\nnon-elliptically contoured multivariate t distributions, allowing for different\nmarginal degrees of freedom. We apply the non-elliptically contoured\nmultivariate t distributions to three widely-used models: the Heckman selection\nmodel with different degrees of freedom for selection and outcome equations,\nthe multivariate Robit model with different degrees of freedom for marginal\nresponses, and the linear mixed-effects model with different degrees of freedom\nfor random effects and within-subject errors. Based on the Normal mixture\nrepresentation of our t distribution, we propose efficient Bayesian inferential\nprocedures for the model parameters based on data augmentation and parameter\nexpansion. We show via simulation studies and real examples that the\nconclusions are sensitive to the existence of different marginal\nheavy-tailedness.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 08:39:29 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Jiang", "Zhichao", ""], ["Ding", "Peng", ""]]}, {"id": "1604.01983", "submitter": "Cristiano Villa", "authors": "Cristiano Villa", "title": "A Property of the Kullback--Leibler Divergence for Location-scale Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss a property of the Kullback--Leibler divergence\nmeasured between two models of the family of the location-scale distributions.\nWe show that, if model $M_1$ and model $M_2$ are represented by location-scale\ndistributions, then the minimum Kullback--Leibler divergence from $M_1$ to\n$M_2$, with respect to the parameters of $M_2$, is independent from the value\nof the parameters of $M_1$. Furthermore, we show that the property holds for\nmodels that can be transformed into location-scale distributions. We illustrate\na possible application of the property in objective Bayesian model selection.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 12:56:50 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Villa", "Cristiano", ""]]}, {"id": "1604.01984", "submitter": "Brian Bader", "authors": "Brian Bader, Jun Yan, Xuebin Zhang", "title": "Automated Selection of r for the r Largest Order Statistics Approach\n  with Adjustment for Sequential Testing", "comments": "21 pages (with tables and figures), 11 pages without, 6 tables, 7\n  figures", "journal-ref": "Statistics and Computing 27.6 (2017): 1435-1451", "doi": "10.1007/s11222-016-9697-3", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The r largest order statistics approach is widely used in extreme value\nanalysis because it may use more information from the data than just the block\nmaxima. In practice, the choice of r is critical. If r is too large, bias can\noccur; if too small, the variance of the estimator can be high. The limiting\ndistribution of the r largest order statistics, denoted by GEVr, extends that\nof the block maxima. Two specification tests are proposed to select r\nsequentially. The first is a score test for the GEVr distribution. Due to the\nspecial characteristics of the GEVr distribution, the classical chi-square\nasymptotics cannot be used. The simplest approach is to use the parametric\nbootstrap, which is straightforward to implement but computationally expensive.\nAn alternative fast weighted bootstrap or multiplier procedure is developed for\ncomputational efficiency. The second test uses the difference in estimated\nentropy between the GEVr and GEV(r-1) models, applied to the r largest order\nstatistics and the r-1 largest order statistics, respectively. The asymptotic\ndistribution of the difference statistic is derived. In a large scale\nsimulation study, both tests held their size and had substantial power to\ndetect various misspecification schemes. A new approach to address the issue of\nmultiple, sequential hypotheses testing is adapted to this setting to control\nthe false discovery rate or familywise error rate. The utility of the\nprocedures is demonstrated with extreme sea level and precipitation data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 13:02:40 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Bader", "Brian", ""], ["Yan", "Jun", ""], ["Zhang", "Xuebin", ""]]}, {"id": "1604.01996", "submitter": "Victoria Nyaga", "authors": "Victoria N Nyaga, Marc Arbyn, Marc Aerts", "title": "CopulaDTA: An R Package for Copula Based Bivariate Beta-Binomial Models\n  for Diagnostic Test Accuracy Studies in a Bayesian Framework", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current statistical procedures implemented in statistical software\npackages for pooling of diagnostic test accuracy data include hSROC regression\nand the bivariate random-effects meta-analysis model (BRMA). However, these\nmodels do not report the overall mean but rather the mean for a central study\nwith random-effect equal to zero and have difficulties estimating the\ncorrelation between sensitivity and specificity when the number of studies in\nthe meta-analysis is small and/or when the between-study variance is relatively\nlarge. This tutorial on advanced statistical methods for meta-analysis of\ndiagnostic accuracy studies discusses and demonstrates Bayesian modeling using\nCopulaDTA package in R to fit different models to obtain the meta-analytic\nparameter estimates. The focus is on the joint modelling of sensitivity and\nspecificity using copula based bivariate beta distribution. Essentially, we\nextend the work of Nikoloulopoulos by: i) presenting the Bayesian approach\nwhich offers flexibility and ability to perform complex statistical modelling\neven with small data sets and ii) including covariate information, and iii)\nproviding an easy to use code. The statistical methods are illustrated by\nre-analysing data of two published meta-analyses. Modelling sensitivity and\nspecificity using the bivariate beta distribution provides marginal as well as\nstudy-specific parameter estimates as opposed to using bivariate normal\ndistribution (e.g., in BRMA) which only yields study-specific parameter\nestimates. Moreover, copula based models offer greater flexibility in modelling\ndifferent correlation structures in contrast to the normal distribution which\nallows for only one correlation structure.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 13:40:18 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Nyaga", "Victoria N", ""], ["Arbyn", "Marc", ""], ["Aerts", "Marc", ""]]}, {"id": "1604.02018", "submitter": "Victoria Nyaga", "authors": "Victoria Nyaga, Marc Aerts, Marc Arbyn", "title": "ANOVA model for network meta-analysis of diagnostic test accuracy data", "comments": "27 Pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network meta-analysis (NMA) allow combining efficacy information from\nmultiple comparisons from trials assessing different therapeutic interventions\nfor a given disease and to estimate unobserved comparisons from a network of\nobserved comparisons. Applying NMA on diagnostic accuracy studies is a\nstatistical challenge given the inherent correlation of sensitivity and\nspecificity. A conceptually simple and novel hierarchical arm-based (AB) model\nwhich expresses the logit transformed sensitivity and specificity as sum of\nfixed effects for test, correlated study-effects and a random error associated\nwith various tests evaluated in given study is proposed. We apply the model to\npreviously published meta-analyses assessing the accuracy of diverse\ncytological and molecular tests used to triage women with minor cervical\nlesions to detect cervical precancer and the results compared with those from\nthe contrast-based (CB) model which expresses the linear predictor as a\ncontrast to a comparator test. The proposed AB model is more appealing than the\nCB model in that it yields the marginal means which are easily interpreted and\nmakes use of all available data and easily accommodates more general\nvariance-covariance matrix structures.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 14:42:22 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Nyaga", "Victoria", ""], ["Aerts", "Marc", ""], ["Arbyn", "Marc", ""]]}, {"id": "1604.02024", "submitter": "Brian Bader", "authors": "Brian Bader, Jun Yan, Xuebin Zhang", "title": "Automated Threshold Selection for Extreme Value Analysis via\n  Goodness-of-Fit Tests with Application to Batched Return Level Mapping", "comments": "21 pages (text and figures), 27 pages total with reference and title,\n  9 figures, 1 table", "journal-ref": "Ann. Appl. Stat. Volume 12, Number 1 (2018), 310-329", "doi": "10.1214/17-AOAS1092", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threshold selection is a critical issue for extreme value analysis with\nthreshold-based approaches. Under suitable conditions, exceedances over a high\nthreshold have been shown to follow the generalized Pareto distribution (GPD)\nasymptotically. In practice, however, the threshold must be chosen. If the\nchosen threshold is too low, the GPD approximation may not hold and bias can\noccur. If the threshold is chosen too high, reduced sample size increases the\nvariance of parameter estimates. To process batch analyses, commonly used\nselection methods such as graphical diagnosis are subjective and cannot be\nautomated, while computational methods may not be feasible. We propose to test\na set of thresholds through the goodness-of-fit of the GPD for the exceedances,\nand select the lowest one, above which the data provides adequate fit to the\nGPD. Previous attempts in this setting are not valid due to the special feature\nthat the multiple tests are done in an ordered fashion. We apply two recently\navailable stopping rules that control the false discovery rate or familywise\nerror rate to ordered goodness-of-fit tests to automate threshold selection.\nVarious model specification tests such as the Cramer-von Mises,\nAnderson-Darling, Moran's, and a score test are investigated. The performance\nof the method is assessed in a large scale simulation study that mimics\npractical return level estimation. This procedure was repeated at hundreds of\nsites in the western US to generate return level maps of extreme precipitation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 14:53:42 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Bader", "Brian", ""], ["Yan", "Jun", ""], ["Zhang", "Xuebin", ""]]}, {"id": "1604.02158", "submitter": "Ming Yuan", "authors": "Shulei Wang, Jianqing Fan, Ginger Pocock and Ming Yuan", "title": "Structured Correlation Detection with Application to Colocalization\n  Analysis in Dual-Channel Fluorescence Microscopic Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of colocalization analysis in fluorescence\nmicroscopic imaging, we study in this paper structured detection of correlated\nregions between two random processes observed on a common domain. We argue that\nalthough intuitive, direct use of the maximum log-likelihood statistic suffers\nfrom potential bias and substantially reduced power, and introduce a simple\nsize-based normalization to overcome this problem. We show that scanning with\nthe proposed size-corrected likelihood ratio statistics leads to optimal\ncorrelation detection over a large collection of structured correlation\ndetection problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 20:04:39 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Wang", "Shulei", ""], ["Fan", "Jianqing", ""], ["Pocock", "Ginger", ""], ["Yuan", "Ming", ""]]}, {"id": "1604.02221", "submitter": "Giovana Fumes", "authors": "Silvia L. P. Ferrari and Giovana Fumes", "title": "Box-Cox symmetric distributions and applications to nutritional data", "comments": "25 pages, 4 figures", "journal-ref": "The final publication is available at Springer (2017)", "doi": "10.1007/s10182-017-0291-6", "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Box-Cox symmetric class of distributions, which is useful\nfor modeling positively skewed, possibly heavy-tailed, data. The new class of\ndistributions includes the Box-Cox t, Box-Cox Cole-Gree, Box-Cox power\nexponential distributions, and the class of the log-symmetric distributions as\nspecial cases. It provides easy parameter interpretation, which makes it\nconvenient for regression modeling purposes. Additionally, it provides enough\nflexibility to handle outliers. The usefulness of the Box-Cox symmetric models\nis illustrated in applications to nutritional data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 04:00:49 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 01:16:07 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 01:13:13 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Ferrari", "Silvia L. P.", ""], ["Fumes", "Giovana", ""]]}, {"id": "1604.02309", "submitter": "Anders Bredahl Kock", "authors": "Federico A. Bugni, Mehmet Caner, Anders Bredahl Kock, Soumendra Lahiri", "title": "Inference in partially identified models with many moment inequalities\n  using Lasso", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers inference in a partially identified moment (in)equality\nmodel with many moment inequalities. We propose a novel two-step inference\nprocedure that combines the methods proposed by Chernozhukov, Chetverikov and\nKato (2018a) (CCK18, hereafter) with a first step moment inequality selection\nbased on the Lasso. Our method controls asymptotic size uniformly, both in\nunderlying parameter and data distribution. Also, the power of our method\ncompares favorably with that of the corresponding two-step method in CCK18 for\nlarge parts of the parameter space, both in theory and in simulations. Finally,\nwe show that our Lasso-based first step can be implemented by thresholding\nstandardized sample averages, and so it is straightforward to implement.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 11:19:03 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 15:36:34 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 17:44:59 GMT"}, {"version": "v4", "created": "Sat, 29 Jun 2019 10:18:30 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bugni", "Federico A.", ""], ["Caner", "Mehmet", ""], ["Kock", "Anders Bredahl", ""], ["Lahiri", "Soumendra", ""]]}, {"id": "1604.02318", "submitter": "Saverio Ranciati", "authors": "Saverio Ranciati, Cinzia Viroli, Ernst Wit", "title": "Bayesian Smooth-and-Match strategy for ordinary differential equations\n  models that are linear in the parameters", "comments": "31 pages, 4 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields of application, dynamic processes that evolve through time are\nwell described by systems of ordinary differential equations (ODEs). The\nanalytical solution of the ODEs is often not available and different methods\nhave been proposed to infer these quantities: from numerical optimization to\nregularized (penalized) models, these procedures aim to estimate indirectly the\nparameters without solving the system. We focus on the class of techniques that\nuse smoothing to avoid direct integration and, in particular, on a Bayesian\nSmooth-and-Match strategy that allows to obtain the ODEs' solution while\nperforming inference on models that are linear in the parameters. We\nincorporate in the strategy two main sources of uncertainty: the noise level in\nthe measurements and the model error. We assess the performance of the proposed\napproach in three different simulation studies and we compare the results on a\ndataset on neuron electrical activity.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 11:55:53 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 11:12:17 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 09:49:27 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Ranciati", "Saverio", ""], ["Viroli", "Cinzia", ""], ["Wit", "Ernst", ""]]}, {"id": "1604.02335", "submitter": "Yasin Asar", "authors": "Yasin Asar", "title": "Liu-type Negative Binomial Regression: A Comparison of Recent Estimators\n  and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new biased estimator for the negative binomial\nregression model that is a generalization of Liu-type estimator proposed for\nthe linear model in [12]. Since the variance of the maximum likelihood\nestimator (MLE) is inflated when there is multicollinearity between the\nexplanatory variables, a new biased estimator is proposed to solve the problem\nand decrease the variance of MLE in order to make stable inferences. Moreover,\nwe obtain some theoretical comparisons between the new estimator and some\nothers via matrix mean squared error (MMSE) criterion. Furthermore, a Monte\nCarlo simulation study is designed to evaluate performances of the estimators\nin the sense of mean squared error. Finally, a real data application is used to\nillustrate the benefits of new estimator.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 12:49:53 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Asar", "Yasin", ""]]}, {"id": "1604.02390", "submitter": "John Duchi", "authors": "John Duchi and Martin Wainwright and Michael Jordan", "title": "Minimax Optimal Procedures for Locally Private Estimation", "comments": "64 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:1302.3203", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working under a model of privacy in which data remains private even from the\nstatistician, we study the tradeoff between privacy guarantees and the risk of\nthe resulting statistical estimators. We develop private versions of classical\ninformation-theoretic bounds, in particular those due to Le Cam, Fano, and\nAssouad. These inequalities allow for a precise characterization of statistical\nrates under local privacy constraints and the development of provably (minimax)\noptimal estimation procedures. We provide a treatment of several canonical\nfamilies of problems: mean estimation and median estimation, generalized linear\nmodels, and nonparametric density estimation. For all of these families, we\nprovide lower and upper bounds that match up to constant factors, and exhibit\nnew (optimal) privacy-preserving mechanisms and computationally efficient\nestimators that achieve the bounds. Additionally, we present a variety of\nexperimental results for estimation problems involving sensitive data,\nincluding salaries, censored blog posts and articles, and drug abuse; these\nexperiments demonstrate the importance of deriving optimal procedures.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 16:24:18 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 19:01:25 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Duchi", "John", ""], ["Wainwright", "Martin", ""], ["Jordan", "Michael", ""]]}, {"id": "1604.02573", "submitter": "Henry Lam", "authors": "Henry Lam, Enlu Zhou", "title": "The Empirical Likelihood Approach to Quantifying Uncertainty in Sample\n  Average Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the empirical likelihood approach to construct confidence intervals\nfor the optimal value and the optimality gap of a given solution, henceforth\nquantify the statistical uncertainty of sample average approximation, for\noptimization problems with expected value objectives and constraints where the\nunderlying probability distributions are observed via limited data. This\napproach relies on two distributionally robust optimization problems posited\nover the uncertain distribution, with a divergence-based uncertainty set that\nis suitably calibrated to provide asymptotic statistical guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 14:51:14 GMT"}, {"version": "v2", "created": "Sun, 23 Oct 2016 20:14:19 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Lam", "Henry", ""], ["Zhou", "Enlu", ""]]}, {"id": "1604.02598", "submitter": "Amy Willis", "authors": "Amy Willis", "title": "Species richness estimation with high diversity but spurious singletons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of uncommon taxa in high-throughput sequenced ecological samples\npose challenges to the microbial ecologist, bioinformatician and statistician.\nIt is rarely certain whether these taxa are truly present in the sample or the\nresult of sequencing errors. Unfortunately, alpha-diversity quantification\nrelies on accurate frequency counts, which can rarely be guaranteed. We present\na species richness estimation tool which predicts both the number of unobserved\ntaxa and the number of true singletons based on the non-singleton frequency\ncounts. This method can be treated as either inferential (for formally\nestimating richness) or exploratory (for assessing robustness of the richness\nestimate to the singleton count). If the estimate, called breakaway_nof1, is\ncomparable to other richness estimators, this provides evidence that the\nrichness estimate is robust to the level of quality control (eg.\nchimera-checking) employed in pre-processing. The function breakaway_nof1 is\nfreely available from CRAN via the R package breakaway.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 19:38:19 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Willis", "Amy", ""]]}, {"id": "1604.02631", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Athina P. Petropulu", "title": "Grid Based Nonlinear Filtering Revisited: Recursive Estimation &\n  Asymptotic Optimality", "comments": "38 pages. To appear in the IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2016.2557311", "report-no": null, "categories": "math.ST cs.IT math.IT math.OC stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the development of grid based recursive approximate filtering of\ngeneral Markov processes in discrete time, partially observed in conditionally\nGaussian noise. The grid based filters considered rely on two types of state\nquantization: The \\textit{Markovian} type and the \\textit{marginal} type. We\npropose a set of novel, relaxed sufficient conditions, ensuring strong and\nfully characterized pathwise convergence of these filters to the respective\nMMSE state estimator. In particular, for marginal state quantizations, we\nintroduce the notion of \\textit{conditional regularity of stochastic kernels},\nwhich, to the best of our knowledge, constitutes the most relaxed condition\nproposed, under which asymptotic optimality of the respective grid based\nfilters is guaranteed. Further, we extend our convergence results, including\nfiltering of bounded and continuous functionals of the state, as well as\nrecursive approximate state prediction. For both Markovian and marginal\nquantizations, the whole development of the respective grid based filters\nrelies more on linear-algebraic techniques and less on measure theoretic\narguments, making the presentation considerably shorter and technically\nsimpler.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 03:06:23 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1604.02634", "submitter": "Renbo Zhao", "authors": "Renbo Zhao and Vincent Y. F. Tan", "title": "Online Nonnegative Matrix Factorization with Outliers", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2620967", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified and systematic framework for performing online\nnonnegative matrix factorization in the presence of outliers. Our framework is\nparticularly suited to large-scale data. We propose two solvers based on\nprojected gradient descent and the alternating direction method of multipliers.\nWe prove that the sequence of objective values converges almost surely by\nappealing to the quasi-martingale convergence theorem. We also show the\nsequence of learned dictionaries converges to the set of stationary points of\nthe expected loss function almost surely. In addition, we extend our basic\nproblem formulation to various settings with different constraints and\nregularizers. We also adapt the solvers and analyses to each setting. We\nperform extensive experiments on both synthetic and real datasets. These\nexperiments demonstrate the computational efficiency and efficacy of our\nalgorithms on tasks such as (parts-based) basis learning, image denoising,\nshadow removal and foreground-background separation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 04:02:57 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 12:01:30 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Zhao", "Renbo", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1604.02642", "submitter": "Pedro H. C. Sant'Anna", "authors": "Pedro H. C. Sant'Anna", "title": "Program Evaluation with Right-Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a unified framework, we provide estimators and confidence bands for a\nvariety of treatment effects when the outcome of interest, typically a\nduration, is subjected to right censoring. Our methodology accommodates\naverage, distributional, and quantile treatment effects under different\nidentifying assumptions including unconfoundedness, local treatment effects,\nand nonlinear differences-in-differences. The proposed estimators are easy to\nimplement, have close-form representation, are fully data-driven upon\nestimation of nuisance parameters, and do not rely on parametric distributional\nassumptions, shape restrictions, or on restricting the potential treatment\neffect heterogeneity across different subpopulations. These treatment effects\nresults are obtained as a consequence of more general results on two-step\nKaplan-Meier estimators that are of independent interest: we provide conditions\nfor applying (i) uniform law of large numbers, (ii) functional central limit\ntheorems, and (iii) we prove the validity of the ordinary nonparametric\nbootstrap in a two-step estimation procedure where the outcome of interest may\nbe randomly censored.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 06:14:33 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Sant'Anna", "Pedro H. C.", ""]]}, {"id": "1604.02652", "submitter": "Tam\\'as Sz\\'antai", "authors": "Edith Kov\\'acs, Tam\\'as Sz\\'antai", "title": "Hypergraphs in the characterization of regular vine copula structures", "comments": "Presented on the XIII-th Conference on Mathematics and its\n  Applications, University \"Politehnica\" of Timisoara, Romania, November, 1-3,\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas constitute a flexible way for modeling of dependences using only\npair copulas as building blocks. The pair-copula constructions introduced by\nJoe (1997) are able to encode more types of dependences in the same time since\nthey can be expressed as a product of different types of bi-variate copulas.\nThe Regular-vine structures (R-vines), as pair copulas corresponding to a\nsequence of trees, have been introduced by Bedford and Cooke (2001, 2002) and\nfurther explored by Kurowicka and Cooke (2006). The complexity of these models\nstrongly increases in larger dimensions. Therefore the so called truncated\nR-vines were introduced in Brechmann et al. (2012). In this paper we express\nthe Regular-vines using a special type of hypergraphs, which encodes the\nconditional independences.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 08:02:20 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Kov\u00e1cs", "Edith", ""], ["Sz\u00e1ntai", "Tam\u00e1s", ""]]}, {"id": "1604.02668", "submitter": "ShengLi Tzeng", "authors": "ShengLi Tzeng, Christian Hennig, Yu-Fen Li, Chien-Ju Lin", "title": "Distance for Functional Data Clustering Based on Smoothing Parameter\n  Commutation", "comments": null, "journal-ref": "Statistical Methods in Medical Research, 27 (2018)", "doi": "10.1177/0962280217710050", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to determine the dissimilarity between subjects for\nfunctional data clustering. Spline smoothing or interpolation is common to deal\nwith data of such type. Instead of estimating the best-representing curve for\neach subject as fixed during clustering, we measure the dissimilarity between\nsubjects based on varying curve estimates with commutation of smoothing\nparameters pair-by-pair (of subjects). The intuitions are that smoothing\nparameters of smoothing splines reflect inverse signal-to-noise ratios and that\napplying an identical smoothing parameter the smoothed curves for two similar\nsubjects are expected to be close. The effectiveness of our proposal is shown\nthrough simulations comparing to other dissimilarity measures. It also has\nseveral pragmatic advantages. First, missing values or irregular time points\ncan be handled directly, thanks to the nature of smoothing splines. Second,\nconventional clustering method based on dissimilarity can be employed\nstraightforward, and the dissimilarity also serves as a useful tool for outlier\ndetection. Third, the implementation is almost handy since subroutines for\nsmoothing splines and numerical integration are widely available. Fourth, the\ncomputational complexity does not increase and is parallel with that in\ncalculating Euclidean distance between curves estimated by smoothing splines.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 10:55:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tzeng", "ShengLi", ""], ["Hennig", "Christian", ""], ["Li", "Yu-Fen", ""], ["Lin", "Chien-Ju", ""]]}, {"id": "1604.02736", "submitter": "Xufei Wang", "authors": "Xufei Wang, Bo Jiang and Jun S. Liu", "title": "Generalized R-squared for Detecting Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting dependence between two random variables is a fundamental problem.\nAlthough the Pearson correlation is effective for capturing linear dependency,\nit can be entirely powerless for detecting nonlinear and/or heteroscedastic\npatterns. We introduce a new measure, G-squared, to test whether two univariate\nrandom variables are independent and to measure the strength of their\nrelationship. The G-squared is almost identical to the square of the Pearson\ncorrelation coefficient, R-squared, for linear relationships with constant\nerror variance, and has the intuitive meaning of the piecewise R-squared\nbetween the variables. It is particularly effective in handling nonlinearity\nand heteroscedastic errors. We propose two estimators of G-squared and show\ntheir consistency. Simulations demonstrate that G-squared estimates are among\nthe most powerful test statistics compared with several state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 21:03:53 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 03:15:08 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 03:07:49 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Wang", "Xufei", ""], ["Jiang", "Bo", ""], ["Liu", "Jun S.", ""]]}, {"id": "1604.02928", "submitter": "Sergiy Shklyar", "authors": "Sergiy Shklyar", "title": "Equivariant adjusted least squares estimator in two-line fitting model", "comments": "Published at http://dx.doi.org/10.15559/16-VMSTA47 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2016, Vol. 3, No. 1,\n  19-45", "doi": "10.15559/16-VMSTA47", "report-no": "VTeX-VMSTA-VMSTA47", "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the two-line fitting problem. True points lie on two straight\nlines and are observed with Gaussian perturbations. For each observed point, it\nis not known on which line the corresponding true point lies. The parameters of\nthe lines are estimated. This model is a restriction of the conic section\nfitting model because a couple of two lines is a degenerate conic section. The\nfollowing estimators are constructed: two projections of the adjusted least\nsquares estimator in the conic section fitting model, orthogonal regression\nestimator, parametric maximum likelihood estimator in the Gaussian model, and\nregular best asymptotically normal moment estimator. The conditions for the\nconsistency and asymptotic normality of the projections of the adjusted least\nsquares estimator are provided. All the estimators constructed in the paper are\nequivariant. The estimators are compared numerically.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 12:24:39 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Shklyar", "Sergiy", ""]]}, {"id": "1604.03192", "submitter": "Jian Kang", "authors": "Jian Kang, Brian J. Reich and Ana-Maria Staicu", "title": "Scalar-on-Image Regression via the Soft-Thresholded Gaussian Process", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this work is on spatial variable selection for scalar-on-image\nregression. We propose a new class of Bayesian nonparametric models,\nsoft-thresholded Gaussian processes and develop the efficient posterior\ncomputation algorithms. Theoretically, soft-thresholded Gaussian processes\nprovide large prior support for the spatially varying coefficients that enjoy\npiecewise smoothness, sparsity and continuity, characterizing the important\nfeatures of imaging data. Also, under some mild regularity conditions, the\nsoft-thresholded Gaussian process leads to the posterior consistency for both\nparameter estimation and variable selection for scalar-on-image regression,\neven when the number of true predictors is larger than the sample size. The\nproposed method is illustrated via simulations, compared numerically with\nexisting alternatives and applied to Electroencephalography (EEG) study of\nalcoholism.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 01:14:38 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Kang", "Jian", ""], ["Reich", "Brian J.", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1604.03269", "submitter": "Tam\\'as Sz\\'antai", "authors": "Edith Kov\\'acs, Tam\\'as Sz\\'antai", "title": "On the connection between cherry-tree copulas and truncated R-vine\n  copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas are a flexible way for modeling dependences using only\npair-copulas as building blocks. However if the number of variables grows the\nproblem gets fast intractable. For dealing with this problem Brechmann at al.\nproposed the truncated R-vine copulas. The truncated R-vine copula has the very\nuseful property that it can be constructed by using only pair-wise copulas, and\nconditional pair-wise copulas. In our earlier papers we introduced the concept\nof cherry-tree copulas. In this paper we characterize the relation between the\ncherry-tree copulas and the truncated R-vine copulas. Both are based on\nexploiting of some conditional independences between the variables. We give a\nnecessary and sufficient condition for a cherry-tree copula to be a truncated\nR-vine copula. We introduce a new perspective for truncated R-vine modeling.\nThe new idea is finding first a good fitting cherry-tree copula of order $k$.\nThen, if this is also a truncated R-vine copula we apply the Backward Algorithm\nintroduced in this paper. This way the construction of a sequence of trees\nwhich leads to it becomes possible. So the cherry-tree copula can be expressed\nby pair-copulas and conditional pair-copulas. In the case when the fitted $k$\norder cherry-tree copula is not a truncated R-vine copula we give an algorithm\nto transform it into truncated R-vine copula at level $k+1$. Therefore this\ncherry-tree copula can also be expressed by pair-copulas.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 07:32:47 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 05:22:11 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kov\u00e1cs", "Edith", ""], ["Sz\u00e1ntai", "Tam\u00e1s", ""]]}, {"id": "1604.03611", "submitter": "Hao Chen", "authors": "Hao Chen", "title": "Sequential change-point detection based on nearest neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for the detection of change-points in online,\nsequential data analysis. The approach utilizes nearest neighbor information\nand can be applied to sequences of multivariate observations or non-Euclidean\ndata objects, such as network data. Different stopping rules are explored, and\none specific rule is recommended due to its desirable properties. An accurate\nanalytic approximation of the average run length is derived for the recommended\nrule, making it an easy off-the-shelf approach for real multivariate/object\nsequential data monitoring applications. Simulations reveal that the new\napproach has better performance than likelihood-based approaches for high\ndimensional data. The new approach is illustrated through a real dataset in\ndetecting global structural changes in social networks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 22:45:14 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 16:43:36 GMT"}, {"version": "v3", "created": "Sat, 28 Apr 2018 05:34:06 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Chen", "Hao", ""]]}, {"id": "1604.03614", "submitter": "Guanhao Feng", "authors": "Guanhao Feng, Nicholas G. Polson, Jianeng Xu", "title": "The Market for English Premier League (EPL) Odds", "comments": null, "journal-ref": "Journal of Quantitative Analysis in Sports, 12.4 (2017): 167-178", "doi": "10.1515/jqas-2016-0039", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper employs a Skellam process to represent real-time betting odds for\nEnglish Premier League (EPL) soccer games. Given a matrix of market odds on all\npossible score outcomes, we estimate the expected scoring rates for each team.\nThe expected scoring rates then define the implied volatility of an EPL game.\nAs events in the game evolve, we re-estimate the expected scoring rates and our\nimplied volatility measure to provide a dynamic representation of the market's\nexpectation of the game outcome. Using a dataset of 1520 EPL games from\n2012-2016, we show how our model calibrates well to the game outcome. We\nillustrate our methodology on real-time market odds data for a game between\nEverton and West Ham in the 2015-2016 season. We show how the implied\nvolatility for the outcome evolves as goals, red cards, and corner kicks occur.\nFinally, we conclude with directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 23:22:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 19:17:40 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 21:15:25 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 15:25:42 GMT"}, {"version": "v5", "created": "Thu, 5 Jan 2017 20:42:09 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Feng", "Guanhao", ""], ["Polson", "Nicholas G.", ""], ["Xu", "Jianeng", ""]]}, {"id": "1604.03615", "submitter": "Subharup Guha", "authors": "Subharup Guha and Veerabhadran Baladandayuthapani", "title": "A Nonparametric Bayesian Technique for High-Dimensional Regression", "comments": "arXiv admin note: substantial text overlap with arXiv:1407.5472", "journal-ref": null, "doi": "10.1214/16-EJS1184", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a nonparametric Bayesian framework called VariScan for\nsimultaneous clustering, variable selection, and prediction in high-throughput\nregression settings. Poisson-Dirichlet processes are utilized to detect\nlower-dimensional latent clusters of covariates. An adaptive nonlinear\nprediction model is constructed for the response, achieving a balance between\nmodel parsimony and flexibility. Contrary to conventional belief, cluster\ndetection is shown to be aposteriori consistent for a general class of models\nas the number of covariates and subjects grows. Simulation studies and data\nanalyses demonstrate that VariScan often outperforms several well-known\nstatistical methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 23:24:00 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Guha", "Subharup", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1604.03648", "submitter": "Wei Xiao", "authors": "Wei Xiao, Hao Helen Zhang, and Wenbin Lu", "title": "Robust regression for optimal individualized treatment rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because different patients may response quite differently to the same drug or\ntreatment, there is increasing interest in discovering individualized treatment\nrule. In particular, people are eager to find the optimal individualized\ntreatment rules, which if followed by the whole patient population would lead\nto the \"best\" outcome. In this paper, we propose new estimators based on robust\nregression with general loss functions to estimate the optimal individualized\ntreatment rules. The new estimators possess the following nice properties:\nfirst, they are robust against skewed, heterogeneous, heavy-tailed errors or\noutliers; second, they are robust against misspecification of the baseline\nfunction; third, under certain situations, the new estimator coupled with\npinball loss approximately maximizes the outcome's conditional quantile instead\nof conditional mean, which leads to a different optimal individualized\ntreatment rule comparing with traditional Q- and A-learning. Consistency and\nasymptotic normality of the proposed estimators are established. Their\nempirical performance is demonstrated via extensive simulation studies and an\nanalysis of an AIDS data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 04:05:49 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Xiao", "Wei", ""], ["Zhang", "Hao Helen", ""], ["Lu", "Wenbin", ""]]}, {"id": "1604.03776", "submitter": "Ma{\\l}gorzata Snarska", "authors": "Daniel Kosiorowski, Jerzy P. Rydlewski, Ma{\\l}gorzata Snarska", "title": "Detecting a Structural Change in Functional Time Series Using Local\n  Wilcoxon Statistic", "comments": "17 pages, 19 figures, LaTeX svjour3 class The final publication is\n  available at link.springer.com DOI: 10.1007/s00362-017-0891-y", "journal-ref": "Statistical Papers, October 2019, Volume 60, Issue 5, pp 1677 -\n  1698", "doi": "10.1007/s00362-017-0891-y", "report-no": null, "categories": "stat.ME q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis (FDA) is a part of modern multivariate statistics\nthat analyses data providing information about curves, surfaces or anything\nelse varying over a certain continuum. In economics and empirical finance we\noften have to deal with time series of functional data, where we cannot easily\ndecide, whether they are to be considered as homogeneous or heterogeneous. At\npresent a discussion on adequate tests of homogenity for functional data is\ncarried. We propose a novel statistic for detetecting a structural change in\nfunctional time series based on a local Wilcoxon statistic induced by a local\ndepth function proposed by Paindaveine and Van Bever (2013).\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 14:00:30 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 23:22:21 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 08:08:00 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Rydlewski", "Jerzy P.", ""], ["Snarska", "Ma\u0142gorzata", ""]]}, {"id": "1604.03802", "submitter": "Chang-Yun Lin", "authors": "Chang-Yun Lin", "title": "Robust designs to model uncertainty with high estimation and prediction\n  efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alphabetic optimality criteria, such as the $D$, $A$, and $I$ criteria,\nrequire specifying a model to select optimal designs. They are not model free\nand the optimal designs selected by them are not robust to model uncertainty.\nRecently, many extensions of the $D$ and $A$ criteria have been proposed for\nselecting robust designs with high estimation efficiency. However, approaches\nfor finding robust designs with high prediction efficiency are rarely studied\nin the literature. In this paper, we propose the $P_\\alpha$ criterion and\ndevelop its approximation version for two-level designs, called the ${\\tilde\nP_\\alpha}$ criterion. They are useful for selecting robust designs with high\nestimation, high prediction, or balanced estimation and prediction efficiency\nfor projective submodels. Computational studies show that the ${\\tilde\nP}_\\alpha$ criterion is a good approximation of the $P_\\alpha$ criterion and\ncan reduce great computation time when we search designs over a wide range of\nmodels. The connection between the ${\\tilde P_\\alpha}$ criterion and the\ngeneralized minimum aberration (GMA) criterion is studied. Result shows that\n${\\tilde P_\\alpha}$ plays a great role to link the alphabetic optimality\ncriteria and the aberration-based criteria.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 14:31:10 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Lin", "Chang-Yun", ""]]}, {"id": "1604.04173", "submitter": "Jing Lei", "authors": "Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani and Larry\n  Wasserman", "title": "Distribution-Free Predictive Inference For Regression", "comments": "50 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for distribution-free predictive inference in\nregression, using conformal inference. The proposed methodology allows for the\nconstruction of a prediction band for the response variable using any estimator\nof the regression function. The resulting prediction band preserves the\nconsistency properties of the original estimator under standard assumptions,\nwhile guaranteeing finite-sample marginal coverage even when these assumptions\ndo not hold. We analyze and compare, both empirically and theoretically, the\ntwo major variants of our conformal framework: full conformal inference and\nsplit conformal inference, along with a related jackknife method. These methods\noffer different tradeoffs between statistical accuracy (length of resulting\nprediction intervals) and computational efficiency. As extensions, we develop a\nmethod for constructing valid in-sample prediction intervals called {\\it\nrank-one-out} conformal inference, which has essentially the same computational\nefficiency as split conformal inference. We also describe an extension of our\nprocedures for producing prediction bands with locally varying length, in order\nto adapt to heteroskedascity in the data. Finally, we propose a model-free\nnotion of variable importance, called {\\it leave-one-covariate-out} or LOCO\ninference. Accompanying this paper is an R package {\\tt conformalInference}\nthat implements all of the proposals we have introduced. In the spirit of\nreproducibility, all of our empirical results can also be easily (re)generated\nusing this package.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 14:46:16 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 18:52:14 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Lei", "Jing", ""], ["G'Sell", "Max", ""], ["Rinaldo", "Alessandro", ""], ["Tibshirani", "Ryan J.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1604.04202", "submitter": "Dominik M\\\"uller", "authors": "Dominik M\\\"uller and Claudia Czado", "title": "Representing sparse Gaussian DAGs as sparse R-vines allowing for\n  non-Gaussian dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling dependence in high dimensional systems has become an increasingly\nimportant topic. Most approaches rely on the assumption of a multivariate\nGaussian distribution such as statistical models on directed acyclic graphs\n(DAGs). They are based on modeling conditional independencies and are scalable\nto high dimensions. In contrast, vine copula models accommodate more elaborate\nfeatures like tail dependence and asymmetry, as well as independent modeling of\nthe marginals. This flexibility comes however at the cost of exponentially\nincreasing complexity for model selection and estimation. We show a novel\nconnection between DAGs with limited number of parents and truncated vine\ncopulas under sufficient conditions. This motivates a more general procedure\nexploiting the fast model selection and estimation of sparse DAGs while\nallowing for non-Gaussian dependence using vine copulas. We demonstrate in a\nsimulation study and using a high dimensional data application that our\napproach outperforms standard methods for vine structure estimation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 16:07:50 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 16:22:39 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["M\u00fcller", "Dominik", ""], ["Czado", "Claudia", ""]]}, {"id": "1604.04211", "submitter": "Farzaneh Safavimanesh", "authors": "Farzaneh Safavimanesh, Claudia Redenbach", "title": "A comparison of functional summary statistics to detect anisotropy of\n  three-dimensional point patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing availability of three-dimensional point process data asks for a\ndevelopment of suitable analysis techniques. In this paper, we focus on two\nrecently developed summary statistics, the conical and the cylindrical\n$K$-function, which may be used to detect anisotropies in 3D point patterns. We\ngive some recommendations on choosing their arguments and investigate their\nability to detect two special types of anisotropy. Finally, both functions are\ncompared on some real data sets from neuroscience and glaciology.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 16:30:57 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Safavimanesh", "Farzaneh", ""], ["Redenbach", "Claudia", ""]]}, {"id": "1604.04242", "submitter": "Amadou Diadie Ba", "authors": "Amadou Diadie Ba", "title": "Consistency Bands for divergences measures", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By wavelets approach we estimate densities. Then by means of mean value\ntheorem we establish asymptotic consistency and normality for special\ndivergence measures and construct their consistency bands.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 18:05:24 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 13:20:37 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Ba", "Amadou Diadie", ""]]}, {"id": "1604.04264", "submitter": "Woncheol Jang", "authors": "Seok-Oh Jeong, Dongseok Choi and Woncheol Jang", "title": "A semiparametric mixture method for local false discovery rate\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semiparametric mixture model to estimate local false discovery\nrates in multiple testing problems. The two pilars of the proposed approach are\nEfron's empirical null principle and log-concave density estimation for the\nalternative distribution. Compared to existing methods, our method can be\neasily extended to high dimension. Simulation results show that our method\noutperforms other existing methods and we illustrate its use via case studies\nin astronomy and microarray.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 19:13:58 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Jeong", "Seok-Oh", ""], ["Choi", "Dongseok", ""], ["Jang", "Woncheol", ""]]}, {"id": "1604.04318", "submitter": "Zhigang Yao", "authors": "Zhigang Yao, Benjamin Eltzner, Tung Pham", "title": "Principal Sub-manifolds", "comments": "38 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We invent a novel method of finding principal components in multivariate data\nsets that lie on an embedded nonlinear Riemannian manifold within a\nhigher-dimensional space. Our aim is to extend the geometric interpretation of\nPCA, while being able to capture non-geodesic modes of variation in the data.\nWe introduce the concept of a principal sub-manifold, a manifold passing\nthrough the center of the data, and at any point on the manifold extending in\nthe direction of highest variation in the space spanned by the eigenvectors of\nthe local tangent space PCA. Compared to recent work for the case where the\nsub-manifold is of dimension one \\citep{Panaretos2014}--essentially a curve\nlying on the manifold attempting to capture one-dimensional variation--the\ncurrent setting is much more general. The principal sub-manifold is therefore\nan extension of the principal flow, accommodating to capture higher dimensional\nvariation in the data. We show the principal sub-manifold yields the ball\nspanned by the usual principal components in Euclidean space. By means of\nexamples, we illustrate how to find, use and interpret a principal sub-manifold\nand we present an application in shape analysis.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 00:12:33 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 06:01:48 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 07:12:24 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Yao", "Zhigang", ""], ["Eltzner", "Benjamin", ""], ["Pham", "Tung", ""]]}, {"id": "1604.04478", "submitter": "Kai-Lan Chang", "authors": "Kai-Lan Chang and Serge Guillas", "title": "Computer model calibration with large non-stationary spatial outputs:\n  application to the calibration of a climate model", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12309", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian calibration of computer models tunes unknown input parameters by\ncomparing outputs with observations. For model outputs that are distributed\nover space, this becomes computationally expensive because of the output size.\nTo overcome this challenge, we employ a basis representation of the model\noutputs and observations: we match these decompositions to carry out the\ncalibration efficiently. In the second step, we incorporate the non-stationary\nbehaviour, in terms of spatial variations of both variance and correlations, in\nthe calibration. We insert two integrated nested Laplace\napproximation-stochastic partial differential equation parameters into the\ncalibration. A synthetic example and a climate model illustration highlight the\nbenefits of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 12:57:47 GMT"}, {"version": "v2", "created": "Sun, 30 Oct 2016 19:44:22 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 15:27:20 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Chang", "Kai-Lan", ""], ["Guillas", "Serge", ""]]}, {"id": "1604.04656", "submitter": "Khanh To Duc", "authors": "Khanh To Duc, Monica Chiogna and Gianfranco Adimari", "title": "Nonparametric Estimation of ROC Surfaces Under Verification Bias", "comments": "arXiv admin note: text overlap with arXiv:1510.03225", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification bias is a well known problem when the predictive ability of a\ndiagnostic test has to be evaluated. In this paper, we discuss how to assess\nthe accuracy of continuous-scale diagnostic tests in the presence of\nverification bias, when a three-class disease status is considered. In\nparticular, we propose a fully nonparametric verification bias-corrected\nestimator of the ROC surface. Our approach is based on nearest-neighbor\nimputation and adopts generic smooth regression models for both the disease and\nthe verification processes. Consistency and asymptotic normality of the\nproposed estimator are proved and its finite sample behavior is investigated by\nmeans of several Monte Carlo simulation studies. Variance estimation is also\ndiscussed and an illustrative example is presented.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 22:24:05 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Duc", "Khanh To", ""], ["Chiogna", "Monica", ""], ["Adimari", "Gianfranco", ""]]}, {"id": "1604.04732", "submitter": "Stephanie Thiemichen", "authors": "Stephanie Thiemichen and G\\\"oran Kauermann", "title": "Stable Exponential Random Graph Models with Non-parametric Components\n  for Large Dense Networks", "comments": "26 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential Random Graph Models (ERGM) behave peculiar in large networks with\nthousand(s) of actors (nodes). Standard models containing two-star or triangle\ncounts as statistics are often unstable leading to completely full or empty\nnetworks. Moreover, numerical methods break down which makes it complicated to\napply ERGMs to large networks. In this paper we propose two strategies to\ncircumvent these obstacles. First, we fit a model to a subsampled network and\nsecondly, we show how linear statistics (like two-stars etc.) can be replaced\nby smooth functional components. These two steps in combination allow to fit\nstable models to large network data, which is illustrated by a data example\nincluding a residual analysis.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 12:03:51 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Thiemichen", "Stephanie", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1604.04768", "submitter": "Nicola Sartori", "authors": "Euloge Clovis, Kenne Pagui and Alessandra Salvan and Nicola Sartori", "title": "Median bias reduction of maximum likelihood estimates", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For regular parametric problems, we show how median centering of the maximum\nlikelihood estimate can be achieved by a simple modification of the score\nequation. For a scalar parameter of interest, the estimator is equivariant\nunder interest respecting parameterizations and third-order median unbiased.\nWith a vector parameter of interest, componentwise equivariance and third-order\nmedian centering are obtained. Like Firth's (1993, Biometrika) implicit method\nfor bias reduction, the new method does not require finiteness of the maximum\nlikelihood estimate and is effective in preventing infinite estimates.\nSimulation results for continuous and discrete models, including binary and\nbeta regression, confirm that the method succeeds in achieving componentwise\nmedian centering and in solving the infinite estimate problem, while keeping\ncomparable dispersion and the same approximate distribution as its main\ncompetitors.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 16:02:16 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 23:10:20 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 12:08:46 GMT"}, {"version": "v4", "created": "Thu, 2 Mar 2017 14:18:20 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Clovis", "Euloge", ""], ["Pagui", "Kenne", ""], ["Salvan", "Alessandra", ""], ["Sartori", "Nicola", ""]]}, {"id": "1604.04793", "submitter": "Gane Samb Lo", "authors": "Modou Ngom, Gane Samb Lo", "title": "A double-indexed functional Hill process and applications", "comments": "33 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1111.3988 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{1,n} \\leq .... \\leq X_{n,n}$ be the order statistics associated with\na sample $X_{1}, ...., X_{n}$ whose pertaining distribution function (%\n\\textit{df}) is $F$. We are concerned with the functional asymptotic behaviour\nof the sequence of stochastic processes \\begin{equation}\nT_{n}(f,s)=\\sum_{j=1}^{j=k}f(j)\\left(\\log X_{n-j+1,n}-\\log\nX_{n-j,n}\\right)^{s}, \\label{fme} \\end{equation} indexed by some classes\n$\\mathcal{F}$ of functions $f:\\mathbb{N}% ^{\\ast}\\longmapsto \\mathbb{R}_{+}$\nand $s \\in ]0,+\\infty[$ and where $k=k(n)$ satisfies \\begin{equation*} 1\\leq\nk\\leq n,k/n\\rightarrow 0\\text{as}n\\rightarrow \\infty . \\end{equation*}\n  \\noindent We show that this is a stochastic process whose margins generate\nestimators of the extreme value index when $F$ is in the extreme domain of\nattraction. We focus in this paper on its finite-dimension asymptotic law and\nprovide a class of new estimators of the extreme value index whose performances\nare compared to analogous ones. The results are next particularized for one\nexplicit class $\\mathcal{F}$.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 20:31:16 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Ngom", "Modou", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1604.04899", "submitter": "Tian Zheng", "authors": "Lu Meng and Tian Zheng", "title": "Phase-Aligned Spectral Filtering for Decomposing Spatiotemporal Dynamics", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal dynamics is central to a wide range of applications from\nclimatology, computer vision to neural sciences. From temporal observations\ntaken on a high-dimensional vector of spatial locations, we seek to derive\nknowledge about such dynamics via data assimilation and modeling. It is assumed\nthat the observed spatiotemporal data represent superimposed lower-rank smooth\noscillations and movements from a generative dynamic system, mixed with\nhigher-rank random noises. Separating the signals from noises is essential for\nus to visualize, model and understand these lower-rank dynamic systems. It is\nalso often the case that such a lower-rank dynamic system have multiple\nindependent components, corresponding to different trends or functionalities of\nthe system under study. In this paper, we present a novel filtering framework\nfor identifying lower-rank dynamics and its components embedded in a high\ndimensional spatiotemporal system. It is based on an approach of structural\ndecomposition and phase-aligned construction in the frequency domain. In both\nour simulated examples and real data applications, we illustrate that the\nproposed method is able to separate and identify meaningful lower-rank\nmovements, while existing methods fail.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 17:15:39 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Meng", "Lu", ""], ["Zheng", "Tian", ""]]}, {"id": "1604.04980", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung, Robert B. Gramacy, Benjamin Haaland", "title": "Potentially Predictive Variance Reducing Subsample Locations in Local\n  Gaussian Process Regression", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": "10.5705/ss.202016.0138", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process models are commonly used as emulators for computer\nexperiments. However, developing a Gaussian process emulator can be\ncomputationally prohibitive when the number of experimental samples is even\nmoderately large. Local Gaussian process approximation (Gramacy and Apley,\n2015) was proposed as an accurate and computationally feasible emulation\nalternative. However, constructing local sub-designs specific to predictions at\na particular location of interest remains a substantial computational\nbottleneck to the technique. In this paper, two computationally efficient\nneighborhood search limiting techniques are proposed, a maximum distance method\nand a feature approximation method. Two examples demonstrate that the proposed\nmethods indeed save substantial computation while retaining emulation accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 04:05:59 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 20:05:41 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Sung", "Chih-Li", ""], ["Gramacy", "Robert B.", ""], ["Haaland", "Benjamin", ""]]}, {"id": "1604.05180", "submitter": "Louis Aslett", "authors": "Louis J. M. Aslett", "title": "Cryptographically secure multiparty evaluation of system reliability", "comments": "13 pages; supplemental material at http://www.louisaslett.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precise design of a system may be considered a trade secret which should\nbe protected, whilst at the same time component manufacturers are sometimes\nreluctant to release full test data (perhaps only providing mean time to\nfailure data). In this situation it seems impractical to both produce an\naccurate reliability assessment and satisfy all parties' privacy requirements.\nHowever, we present recent developments in cryptography which, when combined\nwith the recently developed survival signature in reliability theory, allows\nalmost total privacy to be maintained in a cryptographically strong manner in\nprecisely this setting. Thus, the system designer does not have to reveal their\ntrade secret design and the component manufacturer can retain component test\ndata in-house.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 14:32:57 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Aslett", "Louis J. M.", ""]]}, {"id": "1604.05278", "submitter": "Selden Crary", "authors": "Selden Crary", "title": "The Nu Class of Low-Degree-Truncated, Rational, Generalized Functions.\n  I. IMSPE in Design of Computer Experiments: Integrals and Very-Low-N,\n  Single-Factor, Free-Ranging Designs", "comments": "61 pages, 5 figures, 20 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide detailed algebra for determining the integrated mean-squared\nprediction error (IMSPE) of designs of computer experiments, with one factor\nand one or two points, under the exponential, Gaussian, or either of two Matern\ncorrelation functions. This algebra shall provide the basis for the\nidentification of the IMSPE as a member of a special class of\nlow-degree-truncated rational functions, which we name, here, the Nu class. We\nshall detail this function class in a series of papers, of which this is the\nfirst.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 19:01:30 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 02:28:59 GMT"}, {"version": "v3", "created": "Wed, 4 May 2016 17:51:35 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 19:39:21 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Crary", "Selden", ""]]}, {"id": "1604.05375", "submitter": "Hao Ji", "authors": "Hao Ji and Hans-Georg M\\\"uller", "title": "Optimal Designs for Longitudinal and Functional Data", "comments": "23 pages, 11 figures. Under 2nd revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel optimal designs for longitudinal data for the common\nsituation where the resources for longitudinal data collection are limited, by\ndetermining the optimal locations in time where measurements should be taken.\nAs for all optimal designs, some prior information is needed to implement the\nproposed optimal designs. We demonstrate that this prior information may come\nfrom a pilot longitudinal study that has irregularly measured and noisy\nmeasurements, where for each subject one has available a small random number of\nrepeated measurements that are randomly located on the domain. A second\npossibility of interest is that a pilot study consists of densely measured\nfunctional data and one intends to take only a few measurements at\nstrategically placed locations in the domain for the future collection of\nsimilar data. We construct optimal designs by targeting two criteria: (a)\nOptimal designs to recover the unknown underlying smooth random trajectory for\neach subject from a few optimally placed measurements such that squared\nprediction errors are minimized; (b) Optimal designs that minimize prediction\nerrors for functional linear regression with functional or longitudinal\npredictors and scalar responses, again from a few optimally placed\nmeasurements. The proposed optimal designs address the need for sparse data\ncollection when planning longitudinal studies, by taking advantage of the close\nconnections between longitudinal and functional data analysis. We demonstrate\nin simulations that the proposed designs perform considerably better than\nrandomly chosen design points and include a motivating data example from the\nBaltimore longitudinal study of aging. The proposed designs are shown to have\nan asymptotic optimality property.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 23:10:52 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Ji", "Hao", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1604.05453", "submitter": "Yiming  Ding", "authors": "Yiming Ding and Xuyan Xiang", "title": "An entropic characterization of long memory stationary process", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long memory or long range dependency is an important phenomenon that may\narise in the analysis of time series or spatial data. Most of the definitions\nof long memory of a stationary process $X=\\{X_1, X_2,\\cdots,\\}$ are based on\nthe second-order properties of the process. The excess entropy of a stationary\nprocess is the summation of redundancies which relates to the rate of\nconvergence of the conditional entropy $H(X_n|X_{n-1},\\cdots, X_1)$ to the\nentropy rate. It is proved that the excess entropy is identical to the mutual\ninformation between the past and the future when the entropy $H(X_1)$ is\nfinite. We suggest the definition that a stationary process is long memory if\nthe excess entropy is infinite. Since the definition of excess entropy of a\nstationary process requires very weak moment condition on the distribution of\nthe process, it can be applied to processes whose distributions without bounded\nsecond moment. A significant property of excess entropy is that it is invariant\nunder invertible transformation, which enables us to know the excess entropy of\na stationary process from the excess entropy of other process. For stationary\nGuassian process, the excess entropy characterization of long memory relates to\npopular characterization well. It is proved that the excess entropy of\nfractional Gaussian noise is infinite if the Hurst parameter $H \\in (1/2, 1)$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 07:16:36 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Ding", "Yiming", ""], ["Xiang", "Xuyan", ""]]}, {"id": "1604.05456", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "Hybrid copula mixed models for combining case-control and cohort studies\n  in meta-analysis of diagnostic tests", "comments": "arXiv admin note: text overlap with arXiv:1506.03920,\n  arXiv:1502.07505", "journal-ref": "Statistical Methods in Medical Research, 2018, 27 (8), 2540--2553", "doi": "10.1177/0962280216682376", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula mixed models for trivariate (or bivariate) meta-analysis of diagnostic\ntest accuracy studies accounting (or not) for disease prevalence have been\nproposed in the biostatistics literature to synthesize information. However,\nmany systematic reviews often include case-control and cohort studies, so one\ncan either focus on the bivariate meta-analysis of the case control studies or\nthe trivariate meta-analysis of the cohort studies, as only the latter contains\ninformation on disease prevalence. In order to remedy this situation of wasting\ndata we propose a hybrid copula mixed model via a combination of the bivariate\nand trivariate copula mixed model for the data from the case-control studies\nand cohort studies, respectively. Hence, this hybrid model can account for\nstudy design and also due its generality can deal with dependence in the joint\ntails. We apply the proposed hybrid copula mixed model to a review of the\nperformance of contemporary diagnostic imaging modalities for detecting\nmetastases in patients with melanoma.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 07:24:55 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 09:24:41 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1604.05478", "submitter": "Mattia Molinaro", "authors": "Mattia Molinaro, Reinhard Furrer", "title": "Valid parameter space of a bivariate Gaussian Markov random field with a\n  generalized block-Toeplitz precision matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Markov random fields (GMRFs) are extensively used in statistics to\nmodel area-based data and usually depend on several parameters in order to\ncapture complex spatial correlations. In this context, it is important to\ndetermine the valid parameter space, namely the domain ensuring (semi)\npositive-definiteness of the precision matrix. Depending on the structure of\nthe latter, this task can be challenging. While univari- ate GMRFs with\nblock-Toeplitz precision are well studied in the literature, not much is\nanalytically known about bivariate GMRFs. So far, only restrictive sufficient\nconditions and brute-force approaches were proposed, which are computationally\nexpensive for the size of modern datasets. In this paper, we consider a\nbivariate GMRF, which is part of a hierarchical model used in spatial\nstatistics to analyze data coming from projec- tions of regional climate\nchange. By extending classical convergence results of univariate fields with\ntoroidal boundary conditions to fields without boundary conditions, we pro-\nvide asymptotically closed-form expressions of the valid parameter space. We\ndevelop a general methodology that can be used to determine the valid parameter\nspace of bivariate GMRFs whose precision matrix has a generalized\nblock-Toeplitz structure and for which classical convergence results are not\ndirectly applicable. Finally, we quantify the rate of convergence of our\napproach through a numerical study in R.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 09:01:20 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Molinaro", "Mattia", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1604.05589", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos and Peter G. Moffatt", "title": "Coupling couples with copulas: analysis of assortative matching on risk\n  attitude", "comments": null, "journal-ref": "Economic Inquiry, 2019, 57 (1), 654--666", "doi": "10.1111/ecin.12726", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate patterns of assortative matching on risk attitude, using\nself-reported (ordinal) data on risk attitudes for males and females within\nmarried couples, from the German Socio-Economic Panel over the period\n2004-2012. We apply a novel copula-based bivariate panel ordinal model.\nEstimation is in two steps: firstly, a copula-based Markov model is used to\nrelate the marginal distribution of the response in different time periods,\nseparately for males and females; secondly, another copula is used to couple\nthe males' and females' conditional (on the past) distributions. We find\npositive dependence, both in the middle of the distribution, and in the joint\ntails, and we interpret this as positive assortative matching (PAM). Hence we\nreject standard assortative matching theories based on risk-sharing\nassumptions, and favour models based on alternative assumptions such as the\nability of agents to control income risk. We also find evidence of\n\"assimilation\"; that is, PAM appearing to increase with years of marriage.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:26:29 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 21:18:06 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 19:00:46 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""], ["Moffatt", "Peter G.", ""]]}, {"id": "1604.05643", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos and Emmanouil Mentzakis", "title": "A copula-based model for multivariate ordinal panel data: application to\n  well-being composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel copula-based multivariate panel ordinal model is developed to\nestimate structural relations among components of well-being. Each ordinal\ntime-series is modelled using a copula-based Markov model to relate the\nmarginal distributions of the response at each time of observation and then, at\neach observation time, the conditional distributions of each ordinal\ntime-series are joined using a multivariate t copula. Maximum simulated\nlikelihood based on evaluating the multidimensional integrals of the likelihood\nwith randomized quasi Monte Carlo methods is used for the estimation.\nAsymptotic calculations show that our method is nearly as efficient as maximum\nlikelihood for fully specified multivariate copula models. Our findings\nhighlight the importance of one's relative position in evaluating their\nwell-being with no direct effects of socio-economic characteristics on\nwell-being but strong indirect effects through their impact on components of\nwell-being. Temporal resilience, habit formation and behavioural traits can\nexplain the dependence in the joint tails over time and across well-being\ncomponents.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 16:31:25 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 20:11:28 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""], ["Mentzakis", "Emmanouil", ""]]}, {"id": "1604.05661", "submitter": "Luca Rossini", "authors": "Fabrizio Leisen and Luca Rossini and Cristiano Villa", "title": "Objective Bayesian Analysis of the Yule-Simon Distribution with\n  Applications", "comments": "24 pages, 11 Figures, 7 Tables", "journal-ref": null, "doi": "10.1007/s00180-017-0735-1", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Yule-Simon distribution is usually employed in the analysis of frequency\ndata. As the Bayesian literature, so far, ignored this distribution, here we\nshow the derivation of two objective priors for the parameter of the Yule-Simon\ndistribution. In particular, we discuss the Jeffreys prior and a loss-based\nprior, which has recently appeared in the literature. We illustrate the\nperformance of the derived priors through a simulation study and the analysis\nof real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 17:30:45 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""], ["Villa", "Cristiano", ""]]}, {"id": "1604.05868", "submitter": "Edith Gabriel", "authors": "Edith Gabriel, Florent Bonneu, Pascal Monestiez, Joel Chadoeuf", "title": "Predicting the intensity of partially observed data from a revisited\n  kriging for point processes", "comments": "arXiv admin note: text overlap with arXiv:1409.6441", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary and isotropic spatial point process whose a\nrealisation is observed within a large window. We assume it to be driven by a\nstationary random field $U$. In order to predict the local intensity of the\npoint process, $\\lambda (x|U)$, we propose to define the first- and\nsecond-order characteristics of a random field, defined as the regularized\ncounting process, from the ones of the point process and to interpolate the\nlocal intensity by using a kriging adapted to the regularized process.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 09:24:58 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Gabriel", "Edith", ""], ["Bonneu", "Florent", ""], ["Monestiez", "Pascal", ""], ["Chadoeuf", "Joel", ""]]}, {"id": "1604.06013", "submitter": "Yuan Yuan", "authors": "Y. Yuan and F. E. Bachl and F. Lindgren and D. L. Brochers and J. B.\n  Illian and S. T. Buckland and H. Rue and T. Gerrodette", "title": "Point process models for spatio-temporal distance sampling data from a\n  large-scale survey of blue whales", "comments": "33 pages 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance sampling is a widely used method for estimating wildlife population\nabundance. The fact that conventional distance sampling methods are partly\ndesign-based constrains the spatial resolution at which animal density can be\nestimated using these methods. Estimates are usually obtained at survey stratum\nlevel. For an endangered species such as the blue whale, it is desirable to\nestimate density and abundance at a finer spatial scale than stratum. Temporal\nvariation in the spatial structure is also important. We formulate the process\ngenerating distance sampling data as a thinned spatial point process and\npropose model-based inference using a spatial log-Gaussian Cox process. The\nmethod adopts a flexible stochastic partial differential equation (SPDE)\napproach to model spatial structure in density that is not accounted for by\nexplanatory variables, and integrated nested Laplace approximation (INLA) for\nBayesian inference. It allows simultaneous fitting of detection and density\nmodels and permits prediction of density at an arbitrarily fine scale. We\nestimate blue whale density in the Eastern Tropical Pacific Ocean from thirteen\nshipboard surveys conducted over 22 years. We find that higher blue whale\ndensity is associated with colder sea surface temperatures in space, and\nalthough there is some positive association between density and mean annual\ntemperature, our estimates are consitent with no trend in density across years.\nOur analysis also indicates that there is substantial spatially structured\nvariation in density that is not explained by available covariates.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 16:02:27 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 12:56:35 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 21:34:14 GMT"}, {"version": "v4", "created": "Thu, 22 Jun 2017 16:43:12 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Yuan", "Y.", ""], ["Bachl", "F. E.", ""], ["Lindgren", "F.", ""], ["Brochers", "D. L.", ""], ["Illian", "J. B.", ""], ["Buckland", "S. T.", ""], ["Rue", "H.", ""], ["Gerrodette", "T.", ""]]}, {"id": "1604.06132", "submitter": "Ekkehard Beck", "authors": "Ekkehard Beck, Benjamin Armbruster", "title": "Optimal timing of cross-sectional network samples in longitudinal\n  network studies", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When choosing the timing of cross-sectional network snapshots in longitudinal\nsocial network studies, the effect on the precision of parameter estimates\ngenerally plays a minor role. Often the timing is opportunistic or determined\nby a variety of considerations such as organizational constraints, funding, and\navailability of study participants. Theory to guide the timing of network\nsnapshots is also missing. We use a statistical framework to relate the timing\nto the precision of the parameter estimates, specifically, the sum of the\nrelative widths of their confidence intervals. We illustrate this\ncomputationally using the STERGM suite of the statnet package to estimate the\nparameters of the network dynamics. Analytically, we derive simple\napproximations for the optimal timing when the parameters correspond to the\nrates for different network events. We find that the optimal time depends the\nmost on the network processes with short expected durations and few expected\nevents such as the dissolution of ties. We apply our approximations to a simple\nexample of a dynamic network with formation and dissolution of iid ties.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 22:16:26 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Beck", "Ekkehard", ""], ["Armbruster", "Benjamin", ""]]}, {"id": "1604.06145", "submitter": "Matthew Shum", "authors": "Xiaoxia Shi, Matthew Shum, Wei Song", "title": "Estimating Semi-parametric Panel Multinomial Choice Models using Cyclic\n  Monotonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new semi-parametric identification and estimation\napproach to multinomial choice models in a panel data setting with individual\nfixed effects. Our approach is based on cyclic monotonicity, which is a\ndefining feature of the random utility framework underlying multinomial choice\nmodels. From the cyclic monotonicity property, we derive identifying\ninequalities without requiring any shape restrictions for the distribution of\nthe random utility shocks. These inequalities point identify model parameters\nunder straightforward assumptions on the covariates. We propose a consistent\nestimator based on these inequalities.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 23:38:35 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Shi", "Xiaoxia", ""], ["Shum", "Matthew", ""], ["Song", "Wei", ""]]}, {"id": "1604.06170", "submitter": "Wei Ning", "authors": "Ramadha D. Piyadi Gamage, Wei Ning and Arjun K. Gupta", "title": "Adjusted Empirical Likelihood for Long-memory Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical likelihood method has been applied to short-memory time series\nmodels by Monti (1997) through the Whittle's estimation method. Yau (2012)\nextended this idea to long-memory time series models. Asymptotic distributions\nof the empirical likelihood ratio statistic for short and long-memory time\nseries have been derived to construct confidence regions for the corresponding\nmodel parameters. However, computing profile empirical likelihood function\ninvolving constrained maximization does not always have a solution which leads\nto several drawbacks. In this paper, we propose an adjusted empirical\nlikelihood procedure to modify the one proposed by Yau (2012) for\nautoregressive fractionally integrated moving average (ARFIMA) model. It\nguarantees the existence of a solution to the required maximization problem as\nwell as maintains same asymptotic properties obtained by Yau (2012).\nSimulations have been carried out to illustrate that the adjusted empirical\nlikelihood method for different long-time series models provides better\nconfidence regions and coverage probabilities than the unadjusted ones,\nespecially for small sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 03:38:38 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Gamage", "Ramadha D. Piyadi", ""], ["Ning", "Wei", ""], ["Gupta", "Arjun K.", ""]]}, {"id": "1604.06308", "submitter": "Sudhansu Sekhar Maiti", "authors": "Sudhansu S. Maiti and Indrani Mukherjee", "title": "Some estimators of the PDF and CDF of the Lindley Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the different methods of estimation of the probability\ndensity function (PDF) and the cumulative distribution function (CDF) for the\nLindley distribution. Following estimation methods are considered: uniformly\nminimum variance unbiased estimator (UMVUE), maximum likelihood estimator\n(MLE), percentile estimator (PCE), least square estimator (LSE), weighted least\nsquare estimator (WLSE), Cram\\'{e}r-von-Mises estimator (CVME),\nAnderson-Darling estimator (ADE). Monte Carlo simulations are performed to\ncompare the performances of the proposed methods of estimation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 13:54:54 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Maiti", "Sudhansu S.", ""], ["Mukherjee", "Indrani", ""]]}, {"id": "1604.06310", "submitter": "Adam Kashlak", "authors": "Adam B. Kashlak, John A. D. Aston, Richard Nickl", "title": "Inference on covariance operators via concentration inequalities:\n  k-sample tests, classification, and clustering via Rademacher complexities", "comments": "15 pages, 2 figures, 6 tables", "journal-ref": "Sankhya A 81 (2019) 214-243", "doi": "10.1007/s13171-018-0143-9", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to the analysis of covariance operators making\nuse of concentration inequalities. First, non-asymptotic confidence sets are\nconstructed for such operators. Then, subsequent applications including a k\nsample test for equality of covariance, a functional data classifier, and an\nexpectation-maximization style clustering algorithm are derived and tested on\nboth simulated and phoneme data.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 13:59:32 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Kashlak", "Adam B.", ""], ["Aston", "John A. D.", ""], ["Nickl", "Richard", ""]]}, {"id": "1604.06380", "submitter": "Seok Young Hong", "authors": "Seok Young Hong, Oliver Linton", "title": "Asymptotic properties of a Nadaraya-Watson type estimator for regression\n  functions of infinite order", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of nonparametric time series regression models in which\nthe regressor takes values in a sequence space. Technical challenges that\nhampered theoretical advances in these models include the lack of associated\nLebesgue density and difficulties with regard to the choice of dependence\nstructure in the autoregressive framework. We propose an infinite-dimensional\nNadaraya-Watson type estimator, and investigate its asymptotic properties in\ndetail under both static regressive and autoregressive contexts, aiming to\nanswer the open questions left by Linton and Sancetta (2009). First we show\npointwise consistency of the estimator under a set of mild regularity\nconditions. Furthermore, the asymptotic normality of the estimator is\nestablished, and then its uniform strong consistency is shown over a compact\nset of logarithmically increasing dimension with respect to $\\alpha$-mixing and\nnear epoch dependent (NED) samples. We specify the explicit rates of\nconvergence in terms of the Lambert W function, and show that the optimal rate\nis of logarithmic order, confirming the existence of the curse of infinite\ndimensionality.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 16:47:27 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Hong", "Seok Young", ""], ["Linton", "Oliver", ""]]}, {"id": "1604.06515", "submitter": "Hao Chen", "authors": "Hao Chen, Xu Chen, Yi Su", "title": "A weighted edge-count two-sample test for multivariate and object data", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2017.1307757", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample tests for multivariate data and non-Euclidean data are widely used\nin many fields. Parametric tests are mostly restrained to certain types of data\nthat meets the assumptions of the parametric models. In this paper, we study a\nnonparametric testing procedure that utilizes graphs representing the\nsimilarity among observations. It can be applied to any data types as long as\nan informative similarity measure on the sample space can be defined. The\nclassic test based on a similarity graph has a problem when the two sample\nsizes are different. We solve the problem by applying appropriate weights to\ndifferent components of the classic test statistic. The new test exhibits\nsubstantial power gains in simulation studies. Its asymptotic permutation null\ndistribution is derived and shown to work well under finite samples,\nfacilitating its application to large datasets. The new test is illustrated\nthrough an analysis on a real dataset of network data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 00:20:04 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Chen", "Hao", ""], ["Chen", "Xu", ""], ["Su", "Yi", ""]]}, {"id": "1604.06637", "submitter": "Takayuki Kawashima", "authors": "Takayuki Kawashima, Hironori Fujisawa", "title": "Robust and Sparse Regression via $\\gamma$-divergence", "comments": "25 pages", "journal-ref": null, "doi": "10.3390/e19110608", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data, many sparse regression methods have been proposed.\nHowever, they may not be robust against outliers. Recently, the use of density\npower weight has been studied for robust parameter estimation and the\ncorresponding divergences have been discussed. One of such divergences is the\n$\\gamma$-divergence and the robust estimator using the $\\gamma$-divergence is\nknown for having a strong robustness. In this paper, we consider the robust and\nsparse regression based on $\\gamma$-divergence. We extend the\n$\\gamma$-divergence to the regression problem and show that it has a strong\nrobustness under heavy contamination even when outliers are heterogeneous. The\nloss function is constructed by an empirical estimate of the\n$\\gamma$-divergence with sparse regularization and the parameter estimate is\ndefined as the minimizer of the loss function. To obtain the robust and sparse\nestimate, we propose an efficient update algorithm which has a monotone\ndecreasing property of the loss function. Particularly, we discuss a linear\nregression problem with $L_1$ regularization in detail. In numerical\nexperiments and real data analyses, we see that the proposed method outperforms\npast robust and sparse methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 12:53:27 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 09:07:10 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 06:52:11 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Kawashima", "Takayuki", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1604.06716", "submitter": "Guy Nason Prof.", "authors": "Guy P. Nason, Ben Powell, Duncan Elliott and Paul A. Smith", "title": "Supplementary Material for \"Should we sample a time series more\n  frequently? Decision support via multirate spectrum estimation (with\n  discussion)\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report includes an assortment of technical details and\nextended discussions related to paper \"Should we sample a time series more\nfrequently? Decision support via multirate spectrum estimation (with\ndiscussion)\", which introduces a model for estimating the log-spectral density\nof a stationary discrete time process given systematically missing data and\nmodels the cost implication for changing the sampling rate.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 15:42:25 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Nason", "Guy P.", ""], ["Powell", "Ben", ""], ["Elliott", "Duncan", ""], ["Smith", "Paul A.", ""]]}, {"id": "1604.06815", "submitter": "Christian M\\\"uller", "authors": "Jacob Bien, Irina Gaynanova, Johannes Lederer, Christian M\\\"uller", "title": "Non-convex Global Minimization and False Discovery Rate Control for the\n  TREX", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 2017, Vol. 27,\n  No. 1, 23-33", "doi": "10.1080/10618600.2017.1341414", "report-no": null, "categories": "stat.ML cs.OH stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREX is a recently introduced method for performing sparse\nhigh-dimensional regression. Despite its statistical promise as an alternative\nto the lasso, square-root lasso, and scaled lasso, the TREX is computationally\nchallenging in that it requires solving a non-convex optimization problem. This\npaper shows a remarkable result: despite the non-convexity of the TREX problem,\nthere exists a polynomial-time algorithm that is guaranteed to find the global\nminimum. This result adds the TREX to a very short list of non-convex\noptimization problems that can be globally optimized (principal components\nanalysis being a famous example). After deriving and developing this new\napproach, we demonstrate that (i) the ability of the preexisting TREX heuristic\nto reach the global minimum is strongly dependent on the difficulty of the\nunderlying statistical problem, (ii) the new polynomial-time algorithm for TREX\npermits a novel variable ranking and selection scheme, (iii) this scheme can be\nincorporated into a rule that controls the false discovery rate (FDR) of\nincluded features in the model. To achieve this last aim, we provide an\nextension of the results of Barber & Candes (2015) to establish that the\nknockoff filter framework can be applied to the TREX. This investigation thus\nprovides both a rare case study of a heuristic for non-convex optimization and\na novel way of exploiting non-convexity for statistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 20:28:55 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 20:07:35 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bien", "Jacob", ""], ["Gaynanova", "Irina", ""], ["Lederer", "Johannes", ""], ["M\u00fcller", "Christian", ""]]}, {"id": "1604.06837", "submitter": "Martin Copenhaver", "authors": "Dimitris Bertsimas, Martin S. Copenhaver, Rahul Mazumder", "title": "Certifiably Optimal Low Rank Factor Analysis", "comments": null, "journal-ref": "JMLR 18(29) (2017)", "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor Analysis (FA) is a technique of fundamental importance that is widely\nused in classical and modern multivariate statistics, psychometrics and\neconometrics. In this paper, we revisit the classical rank-constrained FA\nproblem, which seeks to approximate an observed covariance matrix\n($\\boldsymbol\\Sigma$), by the sum of a Positive Semidefinite (PSD) low-rank\ncomponent ($\\boldsymbol\\Theta$) and a diagonal matrix ($\\boldsymbol\\Phi$) (with\nnonnegative entries) subject to $\\boldsymbol\\Sigma - \\boldsymbol\\Phi$ being\nPSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite\nOptimization based formulations for this task. We introduce a reformulation of\nthe problem as a smooth optimization problem with convex compact constraints\nand propose a unified algorithmic framework, utilizing state of the art\ntechniques in nonlinear optimization to obtain high-quality feasible solutions\nfor our proposed formulation. At the same time, by using a variety of\ntechniques from discrete and global optimization, we show that these solutions\nare certifiably optimal in many cases, even for problems with thousands of\nvariables. Our techniques are general and make no assumption on the underlying\nproblem data. The estimator proposed herein, aids statistical interpretability,\nprovides computational scalability and significantly improved accuracy when\ncompared to current, publicly available popular methods for rank-constrained\nFA. We demonstrate the effectiveness of our proposal on an array of synthetic\nand real-life datasets. To our knowledge, this is the first paper that\ndemonstrates how a previously intractable rank-constrained optimization problem\ncan be solved to provable optimality by coupling developments in convex\nanalysis and in discrete optimization.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 00:24:14 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Copenhaver", "Martin S.", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1604.07027", "submitter": "Shinichiro Shirota", "authors": "Shinichiro Shirota and Alan. E. Gelfand", "title": "Approximate Bayesian Computation and Model Validation for Repulsive\n  Spatial Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications involving spatial point patterns, we find evidence of\ninhibition or repulsion. The most commonly used class of models for such\nsettings are the Gibbs point processes. A recent alternative, at least to the\nstatistical community, is the determinantal point process. Here, we examine\nmodel fitting and inference for both of these classes of processes in a\nBayesian framework. While usual MCMC model fitting can be available, the\nalgorithms are complex and are not always well behaved. We propose using\napproximate Bayesian computation (ABC) for such fitting. This approach becomes\nattractive because, though likelihoods are very challenging to work with for\nthese processes, generation of realizations given parameter values is\nrelatively straightforward. As a result, the ABC fitting approach is\nwell-suited for these models. In addition, such simulation makes them\nwell-suited for posterior predictive inference as well as for model assessment.\nWe provide details for all of the above along with some simulation\ninvestigation and an illustrative analysis of a point pattern of tree data\nexhibiting repulsion. R-code and datasets are included in the supplementary\nmaterial.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 13:20:23 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 19:23:55 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Gelfand", "Alan. E.", ""]]}, {"id": "1604.07031", "submitter": "Elizabeth Lorenzi", "authors": "Elizabeth C. Lorenzi, Stephanie L. Brown, Zhifei Sun, Katherine Heller", "title": "Predictive Hierarchical Clustering: Learning clusters of CPT codes for\n  improving surgical outcomes", "comments": "Accepted at MLHC 2017 to appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel algorithm, Predictive Hierarchical Clustering (PHC), for\nagglomerative hierarchical clustering of current procedural terminology (CPT)\ncodes. Our predictive hierarchical clustering aims to cluster subgroups, not\nindividual observations, found within our data, such that the clusters\ndiscovered result in optimal performance of a classification model. Therefore,\nmerges are chosen based on a Bayesian hypothesis test, which chooses pairings\nof the subgroups that result in the best model fit, as measured by held out\npredictive likelihoods. We place a Dirichlet prior on the probability of\nmerging clusters, allowing us to adjust the size and sparsity of clusters. The\nmotivation is to predict patient-specific surgical outcomes using data from ACS\nNSQIP (American College of Surgeon's National Surgical Quality Improvement\nProgram). An important predictor of surgical outcomes is the actual surgical\nprocedure performed as described by a CPT code. We use PHC to cluster CPT\ncodes, represented as subgroups, together in a way that enables us to better\npredict patient-specific outcomes compared to currently used clusters based on\nclinical judgment.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 13:49:23 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 19:02:01 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Lorenzi", "Elizabeth C.", ""], ["Brown", "Stephanie L.", ""], ["Sun", "Zhifei", ""], ["Heller", "Katherine", ""]]}, {"id": "1604.07087", "submitter": "Xin Lu Tan", "authors": "Xin Lu Tan", "title": "Optimal Estimation of Slope Vector in High-dimensional Linear\n  Transformation Model", "comments": "25 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a linear transformation model, there exists an unknown monotone nonlinear\ntransformation function such that the transformed response variable and the\npredictor variables satisfy a linear regression model. In this paper, we\npresent CENet, a new method for estimating the slope vector and simultaneously\nperforming variable selection in the high-dimensional sparse linear\ntransformation model. CENet is the solution to a convex optimization problem\nand can be computed efficiently from an algorithm with guaranteed convergence\nto the global optimum. We show that under a pairwise elliptical distribution\nassumption on each predictor-transformed-response pair and some regularity\nconditions, CENet attains the same optimal rate of convergence as the best\nregression method in the high-dimensional sparse linear regression model. To\nthe best of our limited knowledge, this is the first such result in the\nliterature. We demonstrate the empirical performance of CENet on both simulated\nand real datasets. We also discuss the connection of CENet with some nonlinear\nregression/multivariate methods proposed in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 22:17:19 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Tan", "Xin Lu", ""]]}, {"id": "1604.07125", "submitter": "Stefan Wager", "authors": "Susan Athey, Guido W. Imbens, and Stefan Wager", "title": "Approximate Residual Balancing: De-Biased Inference of Average Treatment\n  Effects in High Dimensions", "comments": "Forthcoming in the Journal of the Royal Statistical Society, Series B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many settings where researchers are interested in estimating\naverage treatment effects and are willing to rely on the unconfoundedness\nassumption, which requires that the treatment assignment be as good as random\nconditional on pre-treatment variables. The unconfoundedness assumption is\noften more plausible if a large number of pre-treatment variables are included\nin the analysis, but this can worsen the performance of standard approaches to\ntreatment effect estimation. In this paper, we develop a method for de-biasing\npenalized regression adjustments to allow sparse regression methods like the\nlasso to be used for sqrt{n}-consistent inference of average treatment effects\nin high-dimensional linear models. Given linearity, we do not need to assume\nthat the treatment propensities are estimable, or that the average treatment\neffect is a sparse contrast of the outcome model parameters. Rather, in\naddition standard assumptions used to make lasso regression on the outcome\nmodel consistent under 1-norm error, we only require overlap, i.e., that the\npropensity score be uniformly bounded away from 0 and 1. Procedurally, our\nmethod combines balancing weights with a regularized regression adjustment.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 04:29:31 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 20:11:04 GMT"}, {"version": "v3", "created": "Mon, 14 Nov 2016 18:04:25 GMT"}, {"version": "v4", "created": "Thu, 17 Aug 2017 05:04:14 GMT"}, {"version": "v5", "created": "Wed, 31 Jan 2018 19:36:51 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Athey", "Susan", ""], ["Imbens", "Guido W.", ""], ["Wager", "Stefan", ""]]}, {"id": "1604.07177", "submitter": "Sinan Y{\\i}ld{\\i}r{\\i}m", "authors": "Sinan Y{\\i}ld{\\i}r{\\i}m", "title": "On the Use of Penalty MCMC for Differential Privacy", "comments": "15 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We view the penalty algorithm of Ceperley and Dewing (1999), a Markov chain\nMonte Carlo (MCMC) algorithm for Bayesian inference, in the context of data\nprivacy. Specifically, we study differential privacy of the penalty algorithm\nand advocate its use for data privacy. We show that in the simple model of\nindependent observations the algorithm has desirable convergence and privacy\nproperties that scale with data size. Two special cases are also investigated\nand privacy preserving schemes are proposed for those cases: (i) Data are\ndistributed among several data owners who are interested in the inference of a\ncommon parameter while preserving their data privacy. (ii) The data likelihood\nbelongs to an exponential family.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 09:19:22 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Y\u0131ld\u0131r\u0131m", "Sinan", ""]]}, {"id": "1604.07212", "submitter": "Jenny H\\\"aggstr\\\"om", "authors": "Jenny H\\\"aggstr\\\"om", "title": "Data-Driven Confounder Selection via Markov and Bayesian Networks", "comments": "To appear in Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To unbiasedly estimate a causal effect on an outcome unconfoundedness is\noften assumed. If there is sufficient knowledge on the underlying causal\nstructure then existing confounder selection criteria can be used to select\nsubsets of the observed pretreatment covariates, $X$, sufficient for\nunconfoundedness, if such subsets exist. Here, estimation of these target\nsubsets is considered when the underlying causal structure is unknown. The\nproposed method is to model the causal structure by a probabilistic graphical\nmodel, e.g., a Markov or Bayesian network, estimate this graph from observed\ndata and select the target subsets given the estimated graph. The approach is\nevaluated by simulation both in a high-dimensional setting where\nunconfoundedness holds given $X$ and in a setting where unconfoundedness only\nholds given subsets of $X$. Several common target subsets are investigated and\nthe selected subsets are compared with respect to accuracy in estimating the\naverage causal effect. The proposed method is implemented with existing\nsoftware that can easily handle high-dimensional data, in terms of large\nsamples and large number of covariates. The results from the simulation study\nshow that, if unconfoundedness holds given $X$, this approach is very\nsuccessful in selecting the target subsets, outperforming alternative\napproaches based on random forests and LASSO, and that the subset estimating\nthe target subset containing all causes of outcome yields smallest MSE in the\naverage causal effect estimation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 11:43:53 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 12:38:14 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["H\u00e4ggstr\u00f6m", "Jenny", ""]]}, {"id": "1604.07264", "submitter": "Changgee Chang", "authors": "Changgee Chang, Suprateek Kundu, and Qi Long", "title": "Scalable Bayesian Variable Selection for Structured High-dimensional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for structured covariates lying on an underlying known\ngraph is a problem motivated by practical applications, and has been a topic of\nincreasing interest. However, most of the existing methods may not be scalable\nto high dimensional settings involving tens of thousands of variables lying on\nknown pathways such as the case in genomics studies. We propose an adaptive\nBayesian shrinkage approach which incorporates prior network information by\nsmoothing the shrinkage parameters for connected variables in the graph, so\nthat the corresponding coefficients have a similar degree of shrinkage. We fit\nour model via a computationally efficient expectation maximization algorithm\nwhich scalable to high dimensional settings (p~100,000). Theoretical properties\nfor fixed as well as increasing dimensions are established, even when the\nnumber of variables increases faster than the sample size. We demonstrate the\nadvantages of our approach in terms of variable selection, prediction, and\ncomputational scalability via a simulation study, and apply the method to a\ncancer genomics study.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 14:01:12 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 00:48:43 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Chang", "Changgee", ""], ["Kundu", "Suprateek", ""], ["Long", "Qi", ""]]}, {"id": "1604.07304", "submitter": "Luca Rossini", "authors": "Fabrizio Leisen and Luca Rossini and Cristiano Villa", "title": "A Note on the Posterior Inference for the Yule-Simon Distribution", "comments": "Forthcoming in the \"Journal of Statistical Computation and\n  Simulation\" - 12 pages, 4 Figures, 3 Tables", "journal-ref": "Journal of Statistical Computation and Simulation (2017), 87:6,\n  1179-1188", "doi": "10.1080/00949655.2016.1255741", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Yule--Simon distribution has been out of the radar of the Bayesian\ncommunity, so far. In this note, we propose an explicit Gibbs sampling scheme\nwhen a Gamma prior is chosen for the shape parameter. The performance of the\nalgorithm is illustrated with simulation studies, including count data\nregression, and a real data application to text analysis. We compare our\nproposal to the frequentist counterparts showing better performance of our\nalgorithm when a small sample size is considered.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 15:14:28 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 16:23:51 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""], ["Villa", "Cristiano", ""]]}, {"id": "1604.07311", "submitter": "Hasinur Rahaman Khan", "authors": "Md Hasinur Rahaman Khan, Anamika Bhadra, Tamanna Howlader", "title": "Stability Selection for Lasso, Ridge and Elastic Net Implemented with\n  AFT Models", "comments": "24 pages, 6 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:0809.2932 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instability in the selection of models is a major concern with data sets\ncontaining a large number of covariates. We focus on stability selection which\nis used as a technique to improve variable selection performance for a range of\nselection methods, based on aggregating the results of applying a selection\nprocedure to sub-samples of the data where the observations are subject to\nright censoring. The accelerated failure time (AFT) models have proved useful\nin many contexts including the heavy censoring (as for example in cancer\nsurvival) and the high dimensionality (as for example in micro-array data). We\nimplement the stability selection approach using three variable selection\ntechniques--Lasso, ridge regression, and elastic net applied to censored data\nusing AFT models. We compare the performances of these regularized techniques\nwith and without stability selection approaches with simulation studies and a\nbreast cancer data analysis. The results suggest that stability selection gives\nalways stable scenario about the selection of variables and that as the\ndimension of data increases the performance of methods with stability selection\nalso improves compared to methods without stability selection irrespective of\nthe collinearity between the covariates.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 15:34:32 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Khan", "Md Hasinur Rahaman", ""], ["Bhadra", "Anamika", ""], ["Howlader", "Tamanna", ""]]}, {"id": "1604.07354", "submitter": "Tianqi Liu", "authors": "Tianqi Liu, Kuang-Yao Lee, and Hongyu Zhao", "title": "Ultrahigh Dimensional Feature Selection via Kernel Canonical Correlation\n  Analysis", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional variable selection is an important issue in many scientific\nfields, such as genomics. In this paper, we develop a sure independence feature\nscreening pro- cedure based on kernel canonical correlation analysis (KCCA-SIS,\nfor short). KCCA- SIS is easy to be implemented and applied. Compared to the\nsure independence screen- ing procedure based on the Pearson correlation (SIS,\nfor short) developed by Fan and Lv [2008], KCCA-SIS can handle nonlinear\ndependencies among variables. Compared to the sure independence screening\nprocedure based on the distance correlation (DC- SIS, for short) proposed by Li\net al. [2012], KCCA-SIS is scale free, distribution free and has better\napproximation results based on the universal characteristic of Gaussian Kernel\n(Micchelli et al. [2006]). KCCA-SIS is more general than SIS and DC-SIS in the\nsense that SIS and DC-SIS correspond to certain choice of kernels. Compared to\nsupremum of Hilbert Schmidt independence criterion-Sure independence screening\n(sup-HSIC-SIS, for short) developed by Balasubramanian et al. [2013], KCCA-SIS\nis scale free removing the marginal variation of features and response\nvariables. No model assumption is needed between response and predictors to\napply KCCA-SIS and it can be used in ultrahigh dimensional data analysis.\nSimilar to DC-SIS and sup- HSIC-SIS, KCCA-SIS can also be used directly to\nscreen grouped predictors and for multivariate response variables. We show that\nKCCA-SIS has the sure screening prop- erty, and has better performance through\nsimulation studies. We applied KCCA-SIS to study Autism genes in a\nspatiotemporal gene expression dataset for human brain development, and\nobtained better results based on gene ontology enrichment analysis comparing to\nthe other existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 18:29:24 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 15:19:32 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Liu", "Tianqi", ""], ["Lee", "Kuang-Yao", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1604.07449", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and Yuchao Liu", "title": "Distribution-free Detection of a Submatrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting the presence of a submatrix with\nlarger-than-usual values in a large data matrix. This problem was considered in\n(Butucea and Ingster, 2013) under a one-parameter exponential family, and one\nof the test they analyzed is the scan test. Taking a nonparametric stance, we\nshow that a calibration by permutation leads to the same (first-order)\nasymptotic performance. This is true for the two types of permutations we\nconsider. We also study the corresponding rank-based variants and precisely\nquantify the loss in asymptotic power.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 21:08:48 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Liu", "Yuchao", ""]]}, {"id": "1604.07451", "submitter": "Guo Yu", "authors": "Guo Yu and Jacob Bien", "title": "Learning Local Dependence In Ordered Data", "comments": null, "journal-ref": "Journal of Machine Learning (2017) 18(42) 1-60", "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, data come with a natural ordering. This ordering can\noften induce local dependence among nearby variables. However, in complex data,\nthe width of this dependence may vary, making simple assumptions such as a\nconstant neighborhood size unrealistic. We propose a framework for learning\nthis local dependence based on estimating the inverse of the Cholesky factor of\nthe covariance matrix. Penalized maximum likelihood estimation of this matrix\nyields a simple regression interpretation for local dependence in which\nvariables are predicted by their neighbors. Our proposed method involves\nsolving a convex, penalized Gaussian likelihood problem with a hierarchical\ngroup lasso penalty. The problem decomposes into independent subproblems which\ncan be solved efficiently in parallel using first-order methods. Our method\nyields a sparse, symmetric, positive definite estimator of the precision\nmatrix, encoding a Gaussian graphical model. We derive theoretical results not\nfound in existing methods attaining this structure. In particular, our\nconditions for signed support recovery and estimation consistency rates in\nmultiple norms are as mild as those in a regression problem. Empirical results\nshow our method performing favorably compared to existing methods. We apply our\nmethod to genomic data to flexibly model linkage disequilibrium. Our method is\nalso applied to improve the performance of discriminant analysis in sound\nrecording classification.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 21:20:51 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 21:42:25 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 03:15:08 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Yu", "Guo", ""], ["Bien", "Jacob", ""]]}, {"id": "1604.07464", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Nonparametric Bayesian Negative Binomial Factor Analysis", "comments": "To appear in Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to analyze a covariate-sample count matrix, an element of\nwhich represents how many times a covariate appears in a sample, is to\nfactorize it under the Poisson likelihood. We show its limitation in capturing\nthe tendency for a covariate present in a sample to both repeat itself and\nexcite related ones. To address this limitation, we construct negative binomial\nfactor analysis (NBFA) to factorize the matrix under the negative binomial\nlikelihood, and relate it to a Dirichlet-multinomial distribution based\nmixed-membership model. To support countably infinite factors, we propose the\nhierarchical gamma-negative binomial process. By exploiting newly proved\nconnections between discrete distributions, we construct two blocked and a\ncollapsed Gibbs sampler that all adaptively truncate their number of factors,\nand demonstrate that the blocked Gibbs sampler developed under a compound\nPoisson representation converges fast and has low computational complexity.\nExample results show that NBFA has a distinct mechanism in adjusting its number\nof inferred factors according to the sample lengths, and provides clear\nadvantages in parsimonious representation, predictive power, and computational\ncomplexity over previously proposed discrete latent variable models, which\neither completely ignore burstiness, or model only the burstiness of the\ncovariates but not that of the factors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 22:27:25 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 15:54:43 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1604.07627", "submitter": "Bruno Sudret", "authors": "C. V. Mai and M. D. Spiridonakos and E. N. Chatzi and B. Sudret", "title": "Surrogate modelling for stochastic dynamical systems by combining NARX\n  models and polynomial chaos expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of polynomial chaos expansions (PCEs) to the propagation of\nuncertainties in stochastic dynamical models is well-known to face challenging\nissues. The accuracy of PCEs degenerates quickly in time. Thus maintaining a\nsufficient level of long term accuracy requires the use of high-order\npolynomials. In numerous cases, it is even infeasible to obtain accurate\nmetamodels with regular PCEs due to the fact that PCEs cannot represent the\ndynamics. To overcome the problem, an original numerical approach was recently\nproposed that combines PCEs and non-linear autoregressive with exogenous input\n(NARX) models, which are a universal tool in the field of system\nidentification. The approach relies on using NARX models to mimic the dynamical\nbehaviour of the system and dealing with the uncertainties using PCEs. The\nPC-NARX model was built by means of heuristic genetic algorithms. This paper\naims at introducing the least angle regression (LAR) technique for computing\nPC-NARX models, which consists in solving two linear regression problems. The\nproposed approach is validated with structural mechanics case studies, in which\nuncertainties arising from both structures and excitations are taken into\naccount. Comparison with Monte Carlo simulation and regular PCEs is also\ncarried out to demonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 11:34:53 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Mai", "C. V.", ""], ["Spiridonakos", "M. D.", ""], ["Chatzi", "E. N.", ""], ["Sudret", "B.", ""]]}, {"id": "1604.07949", "submitter": "Worapree Ole Maneesoonthorn", "authors": "Gael M. Martin, Brendan P.M. McCabe, David T. Frazier, Worapree\n  Maneesoonthorn and Christian P. Robert", "title": "Auxiliary Likelihood-Based Approximate Bayesian Computation in State\n  Space Models", "comments": "This paper is forthcoming at the Journal of Computational and\n  Graphical Statistics. It also supersedes the earlier arXiv paper \"Approximate\n  Bayesian Computation in State Space Models\" (arXiv:1409.8363)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computationally simple approach to inference in state space models is\nproposed, using approximate Bayesian computation (ABC). ABC avoids evaluation\nof an intractable likelihood by matching summary statistics for the observed\ndata with statistics computed from data simulated from the true process, based\non parameter draws from the prior. Draws that produce a 'match' between\nobserved and simulated summaries are retained, and used to estimate the\ninaccessible posterior. With no reduction to a low-dimensional set of\nsufficient statistics being possible in the state space setting, we define the\nsummaries as the maximum of an auxiliary likelihood function, and thereby\nexploit the asymptotic sufficiency of this estimator for the auxiliary\nparameter vector. We derive conditions under which this approach - including a\ncomputationally efficient version based on the auxiliary score - achieves\nBayesian consistency. To reduce the well-documented inaccuracy of ABC in\nmulti-parameter settings, we propose the separate treatment of each parameter\ndimension using an integrated likelihood technique. Three stochastic volatility\nmodels for which exact Bayesian inference is either computationally\nchallenging, or infeasible, are used for illustration. We demonstrate that our\napproach compares favorably against an extensive set of approximate and exact\ncomparators. An empirical illustration completes the paper.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 06:55:26 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 05:46:58 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2018 22:53:41 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Martin", "Gael M.", ""], ["McCabe", "Brendan P. M.", ""], ["Frazier", "David T.", ""], ["Maneesoonthorn", "Worapree", ""], ["Robert", "Christian P.", ""]]}, {"id": "1604.08016", "submitter": "Florian Maire", "authors": "Florian Maire, Nial Friel, Antonietta Mira, Adrian Raftery", "title": "Adaptive Incremental Mixture Markov chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Adaptive Incremental Mixture Markov chain Monte Carlo (AIMM), a\nnovel approach to sample from challenging probability distributions defined on\na general state-space. While adaptive MCMC methods usually update a parametric\nproposal kernel with a global rule, AIMM locally adapts a semiparametric\nkernel. AIMM is based on an independent Metropolis-Hastings proposal\ndistribution which takes the form of a finite mixture of Gaussian\ndistributions. Central to this approach is the idea that the proposal\ndistribution adapts to the target by locally adding a mixture component when\nthe discrepancy between the proposal mixture and the target is deemed to be too\nlarge. As a result, the number of components in the mixture proposal is not\nfixed in advance. Theoretically, we prove that there exists a process that can\nbe made arbitrarily close to AIMM and that converges to the correct target\ndistribution. We also illustrate that it performs well in practice in a variety\nof challenging situations, including high-dimensional and multimodal target\ndistributions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 10:59:56 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 09:50:06 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Maire", "Florian", ""], ["Friel", "Nial", ""], ["Mira", "Antonietta", ""], ["Raftery", "Adrian", ""]]}, {"id": "1604.08045", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp, Joost Kruis and Maarten Marsman", "title": "Estimating psychopathological networks: be careful what you wish for", "comments": "Published in PlosOne", "journal-ref": null, "doi": "10.1371/journal.pone.0179891", "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models, in which psychopathological disorders are conceptualized as a\ncomplex interplay of psychological and biological components, have become\nincreasingly popular in the recent psychopathological literature. These network\nmodels often contain significant numbers of unknown parameters, yet the sample\nsizes available in psychological research are limited. As such, general\nassumptions about the true network are introduced to reduce the number of free\nparameters. Incorporating these assumptions, however, means that the resulting\nnetwork will lead to reflect the particular structure assumed by the estimation\nmethod---a crucial and often ignored aspect of psychopathological networks. For\nexample, observing a sparse structure and simultaneously assuming a sparse\nstructure does not imply that the true model is, in fact, sparse. To illustrate\nthis point, we discuss recent literature and show the effect of the assumption\nof sparsity in three simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 12:57:51 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 15:52:45 GMT"}, {"version": "v3", "created": "Sun, 18 Sep 2016 10:53:07 GMT"}, {"version": "v4", "created": "Thu, 1 Jun 2017 09:18:25 GMT"}, {"version": "v5", "created": "Mon, 11 Sep 2017 20:49:10 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Epskamp", "Sacha", ""], ["Kruis", "Joost", ""], ["Marsman", "Maarten", ""]]}, {"id": "1604.08085", "submitter": "Sarah Filippi", "authors": "Sarah Filippi, Chris C. Holmes and Luis E. Nieto-Barajas", "title": "Scalable Bayesian nonparametric measures for exploring pairwise\n  dependence via Dirichlet Process Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose novel Bayesian nonparametric methods using\nDirichlet Process Mixture (DPM) models for detecting pairwise dependence\nbetween random variables while accounting for uncertainty in the form of the\nunderlying distributions. A key criteria is that the procedures should scale to\nlarge data sets. In this regard we find that the formal calculation of the\nBayes factor for a dependent-vs.-independent DPM joint probability measure is\nnot feasible computationally. To address this we present Bayesian diagnostic\nmeasures for characterising evidence against a \"null model\" of pairwise\nindependence. In simulation studies, as well as for a real data analysis, we\nshow that our approach provides a useful tool for the exploratory nonparametric\nBayesian analysis of large multivariate data sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 14:25:17 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Filippi", "Sarah", ""], ["Holmes", "Chris C.", ""], ["Nieto-Barajas", "Luis E.", ""]]}, {"id": "1604.08294", "submitter": "Lixing Zhu", "authors": "Hira L. Koul, Chuanlong Xie, Lixing Zhu", "title": "An Adaptive-to-Model Test for Parametric Single-Index\n  Errors-in-Variables Models", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides some useful tests for fitting a parametric single-index\nregression model when covariates are measured with error and validation data is\navailable. We propose two tests whose consistency rates do not depend on the\ndimension of the covariate vector when an adaptive-to-model strategy is\napplied. One of these tests has a bias term that becomes arbitrarily large with\nincreasing sample size but its asymptotic variance is smaller, and the other is\nasymptotically unbiased with larger asymptotic variance. Compared with the\nexisting local smoothing tests, the new tests behave like a classical local\nsmoothing test with only one covariate, and still are omnibus against general\nalternatives. This avoids the difficulty associated with the curse of\ndimensionality. Further, a systematic study is conducted to give an insight on\nthe effect of the values of the ratio between the sample size and the size of\nvalidation data on the asymptotic behavior of these tests. Simulations are\nconducted to examine the performance in several finite sample scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 02:51:07 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Koul", "Hira L.", ""], ["Xie", "Chuanlong", ""], ["Zhu", "Lixing", ""]]}, {"id": "1604.08320", "submitter": "Xun Huan", "authors": "Xun Huan, Youssef M. Marzouk", "title": "Sequential Bayesian optimal experimental design via approximate dynamic\n  programming", "comments": "Preprint 34 pages, 12 figures (36 small figures). v1 submitted to the\n  SIAM/ASA Journal on Uncertainty Quantification on April 27, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of multiple experiments is commonly undertaken via suboptimal\nstrategies, such as batch (open-loop) design that omits feedback or greedy\n(myopic) design that does not account for future effects. This paper introduces\nnew strategies for the optimal design of sequential experiments. First, we\nrigorously formulate the general sequential optimal experimental design (sOED)\nproblem as a dynamic program. Batch and greedy designs are shown to result from\nspecial cases of this formulation. We then focus on sOED for parameter\ninference, adopting a Bayesian formulation with an information theoretic design\nobjective. To make the problem tractable, we develop new numerical approaches\nfor nonlinear design with continuous parameter, design, and observation spaces.\nWe approximate the optimal policy by using backward induction with regression\nto construct and refine value function approximations in the dynamic program.\nThe proposed algorithm iteratively generates trajectories via exploration and\nexploitation to improve approximation accuracy in frequently visited regions of\nthe state space. Numerical results are verified against analytical solutions in\na linear-Gaussian setting. Advantages over batch and greedy design are then\ndemonstrated on a nonlinear source inversion problem where we seek an optimal\npolicy for sequential sensing.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 06:32:27 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Huan", "Xun", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1604.08403", "submitter": "Pierre Pudlo", "authors": "Paul-Marie Grollemund, Christophe Abraham, Me\\\"ili Baragatti, Pierre\n  Pudlo", "title": "Bayesian functional linear regression with sparse step functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional linear regression model is a common tool to determine the\nrelationship between a scalar outcome and a functional predictor seen as a\nfunction of time. This paper focuses on the Bayesian estimation of the support\nof the coefficient function. To this aim we propose a parsimonious and adaptive\ndecomposition of the coefficient function as a step function, and a model\nincluding a prior distribution that we name Bayesian functional Linear\nregression with Sparse Step functions (Bliss). The aim of the method is to\nrecover areas of time which influences the most the outcome. A Bayes estimator\nof the support is built with a specific loss function, as well as two Bayes\nestimators of the coefficient function, a first one which is smooth and a\nsecond one which is a step function. The performance of the proposed\nmethodology is analysed on various synthetic datasets and is illustrated on a\nblack P\\'erigord truffle dataset to study the influence of rainfall on the\nproduction.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 13:13:21 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 11:09:34 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Grollemund", "Paul-Marie", ""], ["Abraham", "Christophe", ""], ["Baragatti", "Me\u00efli", ""], ["Pudlo", "Pierre", ""]]}, {"id": "1604.08462", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp, Denny Borsboom and Eiko I. Fried", "title": "Estimating Psychological Networks and their Accuracy: A Tutorial Paper", "comments": "Accepted for publication in Behavior Research Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of psychological networks that conceptualize psychological behavior\nas a complex interplay of psychological and other components has gained\nincreasing popularity in various fields of psychology. While prior publications\nhave tackled the topics of estimating and interpreting such networks, little\nwork has been conducted to check how accurate (i.e., prone to sampling\nvariation) networks are estimated, and how stable (i.e., interpretation remains\nsimilar with less observations) inferences from the network structure (such as\ncentrality indices) are. In this tutorial paper, we aim to introduce the reader\nto this field and tackle the problem of accuracy under sampling variation. We\nfirst introduce the current state-of-the-art of network estimation. Second, we\nprovide a rationale why researchers should investigate the accuracy of\npsychological networks. Third, we describe how bootstrap routines can be used\nto (A) assess the accuracy of estimated network connections, (B) investigate\nthe stability of centrality indices, and (C) test whether network connections\nand centrality estimates for different variables differ from each other. We\nintroduce two novel statistical methods: for (B) the correlation stability\ncoefficient, and for (C) the bootstrapped difference test for edge-weights and\ncentrality indices. We conducted and present simulation studies to assess the\nperformance of both methods. Finally, we developed the free R-package bootnet\nthat allows for estimating psychological networks in a generalized framework in\naddition to the proposed bootstrap methods. We showcase bootnet in a tutorial,\naccompanied by R syntax, in which we analyze a dataset of 359 women with\nposttraumatic stress disorder available online.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 15:24:48 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 23:27:29 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 10:02:17 GMT"}, {"version": "v4", "created": "Fri, 20 Jan 2017 09:52:22 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Epskamp", "Sacha", ""], ["Borsboom", "Denny", ""], ["Fried", "Eiko I.", ""]]}, {"id": "1604.08636", "submitter": "Priyam Das", "authors": "Priyam Das", "title": "Recursive Modified Pattern Search on High-dimensional Simplex : A\n  Blackbox Optimization Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel derivative-free pattern search based algorithm for\nBlack-box optimization is proposed over a simplex constrained parameter space.\nAt each iteration, starting from the current solution, new possible set of\nsolutions are found by adding a set of derived step-size vectors to the initial\nstarting point. While deriving these step-size vectors, precautions and\nadjustments are considered so that the set of new possible solution points\nstill remain within the simplex constrained space. Thus, no extra time is spent\nin evaluating the (possibly expensive) objective function at infeasible points\n(points outside the unit-simplex space). While minimizing any objective\nfunction of m parameters, within each iteration, the objective function is\nevaluated at 2m new possible solution points. So, upto 2m parallel threads can\nbe incorporated which makes the computation even faster while optimizing\nexpensive objective functions over high-dimensional parameter space. Once a\nlocal minimum is discovered, in order to find a better solution, a novel\n`re-start' strategy is considered to increase the likelihood of finding a\nbetter solution. Unlike existing pattern search based methods, a sparsity\ncontrol parameter is introduced which can be used to induce sparsity in the\nsolution in case the solution is expected to be sparse in prior. A comparative\nstudy of the performances of the proposed algorithm and other existing\nalgorithms are shown for a few low, moderate and high-dimensional optimization\nproblems. Upto 338 folds improvement in computation time is achieved using the\nproposed algorithm over Genetic algorithm along with better solution. The\nproposed algorithm is used to estimate the simultaneous quantiles of North\nAtlantic Hurricane velocities during 1981-2006 by maximizing a non-closed form\nlikelihood function with (possibly) multiple maximums.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 22:20:09 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 00:03:08 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Das", "Priyam", ""]]}, {"id": "1604.08654", "submitter": "Eric Lock", "authors": "Eric F. Lock and David B. Dunson", "title": "Bayesian Genome- and Epigenome-wide Association Studies with Gene Level\n  Dependence", "comments": "23 pages, 7 figures", "journal-ref": "Biometrics 73(3), 1018-1028, 2017", "doi": "10.1111/biom.12649", "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput genetic and epigenetic data are often screened for\nassociations with an observed phenotype. For example, one may wish to test\nhundreds of thousands of genetic variants, or DNA methylation sites, for an\nassociation with disease status. These genomic variables can naturally be\ngrouped by the gene they encode, among other criteria. However, standard\npractice in such applications is independent screening with a universal\ncorrection for multiplicity. We propose a Bayesian approach in which the prior\nprobability of an association for a given genomic variable depends on its gene,\nand the gene-specific probabilities are modeled nonparametrically. This\nhierarchical model allows for appropriate gene and genome-wide multiplicity\nadjustments, and can be incorporated into a variety of Bayesian association\nscreening methodologies with negligible increase in computational complexity.\nWe describe an application to screening for differences in DNA methylation\nbetween lower grade glioma and glioblastoma multiforme tumor samples from The\nCancer Genome Atlas. Software is available via the package BayesianScreening\nfor R at https://github.com/lockEF/BayesianScreening .\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 00:12:38 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Lock", "Eric F.", ""], ["Dunson", "David B.", ""]]}, {"id": "1604.08720", "submitter": "Torsten Hothorn", "authors": "Heidi Seibold and Achim Zeileis and Torsten Hothorn", "title": "Individual Treatment Effect Prediction for ALS Patients", "comments": null, "journal-ref": "Statistical Methods in Medical Research 2017", "doi": "10.1177/0962280217693034", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A treatment for a complicated disease may be helpful for some but not all\npatients, which makes predicting the treatment effect for new patients\nimportant yet challenging. Here we develop a method for predicting the\ntreatment effect based on patient char- acteristics and use it for predicting\nthe effect of the only drug (Riluzole) approved for treating Amyotrophic\nLateral Sclerosis (ALS). Our proposed method of model-based ran- dom forests\ndetects similarities in the treatment effect among patients and on this basis\ncomputes personalised models for new patients. The entire procedure focuses on\na base model, which usually contains the treatment indicator as a single\ncovariate and takes the survival time or a health or treatment success\nmeasurement as primary outcome. This base model is used both to grow the\nmodel-based trees within the forest, in which the patient characteristics that\ninteract with the treatment are split variables, and to com- pute the\npersonalised models, in which the similarity measurements enter as weights. We\napplied the personalised models using data from several clinical trials for ALS\nfrom the PRO-ACT database. Our results indicate that some ALS patients benefit\nmore from the drug Riluzole than others. Our method allows shifting from\nstratified medicine to person- alised medicine and can also be used in\nassessing the treatment effect for other diseases studied in a clinical trial.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 07:53:10 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Seibold", "Heidi", ""], ["Zeileis", "Achim", ""], ["Hothorn", "Torsten", ""]]}]