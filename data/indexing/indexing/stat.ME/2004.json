[{"id": "2004.00041", "submitter": "Zhou Fan", "authors": "Zhou Fan, Yi Sun, Tianhao Wang, and Yihong Wu", "title": "Likelihood landscape and maximum likelihood estimation for the discrete\n  orbit recovery model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-convex optimization landscape for maximum likelihood\nestimation in the discrete orbit recovery model with Gaussian noise. This model\nis motivated by applications in molecular microscopy and image processing,\nwhere each measurement of an unknown object is subject to an independent random\nrotation from a rotational group. Equivalently, it is a Gaussian mixture model\nwhere the mixture centers belong to a group orbit.\n  We show that fundamental properties of the likelihood landscape depend on the\nsignal-to-noise ratio and the group structure. At low noise, this landscape is\n\"benign\" for any discrete group, possessing no spurious local optima and only\nstrict saddle points. At high noise, this landscape may develop spurious local\noptima, depending on the specific group. We discuss several positive and\nnegative examples, and provide a general condition that ensures a globally\nbenign landscape. For cyclic permutations of coordinates on $\\mathbb{R}^d$\n(multi-reference alignment), there may be spurious local optima when $d \\geq\n6$, and we establish a correspondence between these local optima and those of a\nsurrogate function of the phase variables in the Fourier domain.\n  We show that the Fisher information matrix transitions from resembling that\nof a single Gaussian in low noise to having a graded eigenvalue structure in\nhigh noise, which is determined by the graded algebra of invariant polynomials\nunder the group action. In a local neighborhood of the true object, the\nlikelihood landscape is strongly convex in a reparametrized system of variables\ngiven by a transcendence basis of this polynomial algebra. We discuss\nimplications for optimization algorithms, including slow convergence of\nexpectation-maximization, and possible advantages of momentum-based\nacceleration and variable reparametrization for first- and second-order descent\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:13:18 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 01:11:32 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 20:37:00 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 00:30:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fan", "Zhou", ""], ["Sun", "Yi", ""], ["Wang", "Tianhao", ""], ["Wu", "Yihong", ""]]}, {"id": "2004.00076", "submitter": "Hau-tieng Wu", "authors": "Ziyu Chen and Hau-Tieng Wu", "title": "When Ramanujan meets time-frequency analysis in complicated time series\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To handle time series with complicated oscillatory structure, we propose a\nnovel time-frequency (TF) analysis tool that fuses the short time Fourier\ntransform (STFT) and periodic transform (PT). Since many time series oscillate\nwith time-varying frequency, amplitude and non-sinusoidal oscillatory pattern,\na direct application of PT or STFT might not be suitable. However, we show that\nby combining them in a proper way, we obtain a powerful TF analysis tool. We\nfirst combine the Ramanujan sums and $l_1$ penalization to implement the PT. We\ncall the algorithm Ramanujan PT (RPT). The RPT is of its own interest for other\napplications, like analyzing short signal composed of components with integer\nperiods, but that is not the focus of this paper. Second, the RPT is applied to\nmodify the STFT and generate a novel TF representation of the complicated time\nseries that faithfully reflect the instantaneous frequency information of each\noscillatory components. We coin the proposed TF analysis the Ramanujan de-shape\n(RDS) and vectorized RDS (vRDS). In addition to showing some preliminary\nanalysis results on complicated biomedical signals, we provide theoretical\nanalysis about RPT. Specifically, we show that the RPT is robust to three\ncommonly encountered noises, including envelop fluctuation, jitter and additive\nnoise.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:47:52 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Chen", "Ziyu", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "2004.00160", "submitter": "Chaoran Hu", "authors": "Chaoran Hu, Mark Elbroch, Thomas Meyer, Vladimir Pozdnyakov, Jun Yan", "title": "Moving-Resting Process with Measurement Error in Animal Movement\n  Modeling", "comments": "26 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of animal movement is of critical importance. The\ncontinuous trajectory of an animal's movements is only observed at discrete,\noften irregularly spaced time points. Most existing models cannot handle the\nunequal sampling interval naturally and/or do not allow inactivity periods such\nas resting or sleeping. The recently proposed moving-resting (MR) model is a\nBrownian motion governed by a telegraph process, which allows periods of\ninactivity in one state of the telegraph process. The MR model shows promise in\nmodeling the movements of predators with long inactive periods such as many\nfelids, but the lack of accommodation of measurement errors seriously prohibits\nits application in practice. Here we incorporate measurement errors in the MR\nmodel and derive basic properties of the model. Inferences are based on a\ncomposite likelihood using the Markov property of the chain composed by every\nother observed increments. The performance of the method is validated in finite\nsample simulation studies. Application to the movement data of a mountain lion\nin Wyoming illustrates the utility of the method.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 23:22:27 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 16:59:29 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 01:38:55 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Hu", "Chaoran", ""], ["Elbroch", "Mark", ""], ["Meyer", "Thomas", ""], ["Pozdnyakov", "Vladimir", ""], ["Yan", "Jun", ""]]}, {"id": "2004.00386", "submitter": "Linda Mhalla", "authors": "Daniela Castro-Camilo, Linda Mhalla, and Thomas Opitz", "title": "Bayesian space-time gap filling for inference on extreme hot-spots: an\n  application to Red Sea surface temperatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for probabilistic prediction of extreme value hot-spots\nin a spatio-temporal framework, tailored to big datasets containing important\ngaps. In this setting, direct calculation of summaries from data, such as the\nminimum over a space-time domain, is not possible. To obtain predictive\ndistributions for such cluster summaries, we propose a two-step approach. We\nfirst model marginal distributions with a focus on accurate modeling of the\nright tail and then, after transforming the data to a standard Gaussian scale,\nwe estimate a Gaussian space-time dependence model defined locally in the time\ndomain for the space-time subregions where we want to predict. In the first\nstep, we detrend the mean and standard deviation of the data and fit a\nspatially resolved generalized Pareto distribution to apply a correction of the\nupper tail. To ensure spatial smoothness of the estimated trends, we either\npool data using nearest-neighbor techniques, or apply generalized additive\nregression modeling. To cope with high space-time resolution of data, the local\nGaussian models use a Markov representation of the Mat\\'ern correlation\nfunction based on the stochastic partial differential equations (SPDE)\napproach. In the second step, they are fitted in a Bayesian framework through\nthe integrated nested Laplace approximation implemented in R-INLA. Finally,\nposterior samples are generated to provide statistical inferences through\nMonte-Carlo estimation. Motivated by the 2019 Extreme Value Analysis data\nchallenge, we illustrate our approach to predict the distribution of local\nspace-time minima in anomalies of Red Sea surface temperatures, using a gridded\ndataset (11315 days, 16703 pixels) with artificially generated gaps. In\nparticular, we show the improved performance of our two-step approach over a\npurely Gaussian model without tail transformations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:40:38 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Castro-Camilo", "Daniela", ""], ["Mhalla", "Linda", ""], ["Opitz", "Thomas", ""]]}, {"id": "2004.00527", "submitter": "Thomas Shaw", "authors": "Thomas Shaw, Jesper M{\\o}ller and Rasmus Waagepetersen", "title": "Globally intensity-reweighted estimators for $K$- and pair correlation\n  functions", "comments": "31 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new estimators of the inhomogeneous $K$-function and the pair\ncorrelation function of a spatial point process as well as the cross\n$K$-function and the cross pair correlation function of a bivariate spatial\npoint process under the assumption of second-order intensity-reweighted\nstationarity. These estimators rely on a 'global' normalization factor which\ndepends on an aggregation of the intensity function, whilst the existing\nestimators depend 'locally' on the intensity function at the individual\nobserved points. The advantages of our new global estimators over the existing\nlocal estimators are demonstrated by theoretical considerations and a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 15:51:58 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 20:12:58 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Shaw", "Thomas", ""], ["M\u00f8ller", "Jesper", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "2004.00623", "submitter": "Filip Tronarp", "authors": "Filip Tronarp, Simo Sarkka, Philipp Hennig", "title": "Bayesian ODE Solvers: The Maximum A Posteriori Estimate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been established that the numerical solution of ordinary\ndifferential equations can be posed as a nonlinear Bayesian inference problem,\nwhich can be approximately solved via Gaussian filtering and smoothing,\nwhenever a Gauss--Markov prior is used. In this paper the class of $\\nu$ times\ndifferentiable linear time invariant Gauss--Markov priors is considered. A\ntaxonomy of Gaussian estimators is established, with the maximum a posteriori\nestimate at the top of the hierarchy, which can be computed with the iterated\nextended Kalman smoother. The remaining three classes are termed explicit,\nsemi-implicit, and implicit, which are in similarity with the classical notions\ncorresponding to conditions on the vector field, under which the filter update\nproduces a local maximum a posteriori estimate. The maximum a posteriori\nestimate corresponds to an optimal interpolant in the reproducing Hilbert space\nassociated with the prior, which in the present case is equivalent to a Sobolev\nspace of smoothness $\\nu+1$. Consequently, using methods from scattered data\napproximation and nonlinear analysis in Sobolev spaces, it is shown that the\nmaximum a posteriori estimate converges to the true solution at a polynomial\nrate in the fill-distance (maximum step size) subject to mild conditions on the\nvector field. The methodology developed provides a novel and more natural\napproach to study the convergence of these estimators than classical methods of\nconvergence analysis. The methods and theoretical results are demonstrated in\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 11:39:59 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 16:12:21 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Tronarp", "Filip", ""], ["Sarkka", "Simo", ""], ["Hennig", "Philipp", ""]]}, {"id": "2004.00744", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Pattern graphs: a graphical approach to nonmonotone missing data", "comments": "Main paper: 25 pages. We added semi-parametric theory of pattern\n  graphs in Section 3.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the concept of pattern graphs--directed acyclic graphs\nrepresenting how response patterns are associated. A pattern graph represents\nan identifying restriction that is nonparametrically identified/saturated and\nis often a missing not at random restriction. We introduce a selection model\nand a pattern mixture model formulations using the pattern graphs and show that\nthey are equivalent. A pattern graph leads to an inverse probability weighting\nestimator as well as an imputation-based estimator. We also study the\nsemi-parametric efficiency theory and derive a multiply-robust estimator using\npattern graphs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 23:48:42 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 18:56:40 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "2004.00764", "submitter": "Qiong Li", "authors": "Qiong Li, Xin Gao, Helene Massam", "title": "Bayesian model selection approach for colored graphical Gaussian models", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of colored graphical Gaussian models obtained by placing\nsymmetry constraints on the precision matrix in a Bayesian framework. The prior\ndistribution on the precision matrix is the colored $G$-Wishart prior which is\nthe Diaconis-Ylvisaker conjugate prior. In this paper, we develop a\ncomputationally efficient model search algorithm which combines linear\nregression with a double reversible jump Markov chain Monte Carlo (MCMC)\nmethod. The latter is to estimate the Bayes factors expressed as the ratio of\nposterior probabilities of two competing models. We also establish the\nasymptotic consistency property of the model selection procedure based on the\nBayes factors. Our procedure avoids an exhaustive search which is\ncomputationally impossible. Our method is illustrated with simulations and a\nreal-world application with a protein signalling data set.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 01:15:54 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Li", "Qiong", ""], ["Gao", "Xin", ""], ["Massam", "Helene", ""]]}, {"id": "2004.00766", "submitter": "Siyu Heng", "authors": "Siyu Heng, Dylan S. Small", "title": "Sharpening the Rosenbaum Sensitivity Bounds to Address Concerns About\n  Interactions Between Observed and Unobserved Covariates", "comments": "32 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, it is typically unrealistic to assume that\ntreatments are randomly assigned, even conditional on adjusting for all\nobserved covariates. Therefore, a sensitivity analysis is often needed to\nexamine how hidden biases due to unobserved covariates would affect inferences\non treatment effects. In matched observational studies where each treated unit\nis matched to one or multiple untreated controls for observed covariates, the\nRosenbaum bounds sensitivity analysis is one of the most popular sensitivity\nanalysis models. In this paper, we show that in the presence of interactions\nbetween observed and unobserved covariates, directly applying the Rosenbaum\nbounds will almost inevitably exaggerate the report of sensitivity of causal\nconclusions to hidden bias. We give sharper odds ratio bounds to fix this\ndeficiency. We illustrate our new method through studying the effect of\nanger/hostility tendency on the risk of having heart problems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 01:28:13 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 18:39:58 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 01:14:24 GMT"}, {"version": "v4", "created": "Sat, 15 May 2021 02:55:34 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Heng", "Siyu", ""], ["Small", "Dylan S.", ""]]}, {"id": "2004.00792", "submitter": "HaiYing Wang", "authors": "Luc Pronzato and HaiYing Wang", "title": "Sequential online subsampling for thinning experimental designs", "comments": "35 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a design problem where experimental conditions (design points\n$X_i$) are presented in the form of a sequence of i.i.d.\\ random variables,\ngenerated with an unknown probability measure $\\mu$, and only a given\nproportion $\\alpha\\in(0,1)$ can be selected. The objective is to select good\ncandidates $X_i$ on the fly and maximize a concave function $\\Phi$ of the\ncorresponding information matrix. The optimal solution corresponds to the\nconstruction of an optimal bounded design measure $\\xi_\\alpha^*\\leq\n\\mu/\\alpha$, with the difficulty that $\\mu$ is unknown and $\\xi_\\alpha^*$ must\nbe constructed online. The construction proposed relies on the definition of a\nthreshold $\\tau$ on the directional derivative of $\\Phi$ at the current\ninformation matrix, the value of $\\tau$ being fixed by a certain quantile of\nthe distribution of this directional derivative. Combination with recursive\nquantile estimation yields a nonlinear two-time-scale stochastic approximation\nmethod. It can be applied to very long design sequences since only the current\ninformation matrix and estimated quantile need to be stored. Convergence to an\noptimum design is proved. Various illustrative examples are presented.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 03:19:11 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 12:58:58 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Pronzato", "Luc", ""], ["Wang", "HaiYing", ""]]}, {"id": "2004.00796", "submitter": "Chaitanya Joshi Dr.", "authors": "Chaitanya Joshi and Fabrizio Ruggeri", "title": "Duality between Approximate Bayesian Methods and Prior Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that there is a link between approximate Bayesian\nmethods and prior robustness. We show that what is typically recognized as an\napproximation to the likelihood, either due to the simulated data as in the\nApproximate Bayesian Computation (ABC) methods or due to the functional\napproximation to the likelihood, can instead also be viewed upon as an implicit\nexercise in prior robustness. We first define two new classes of priors for the\ncases where the sufficient statistics is available, establish their\nmathematical properties and show, for a simple illustrative example, that these\nclasses of priors can also be used to obtain the posterior distribution that\nwould be obtained by implementing ABC. We then generalize and define two\nfurther classes of priors that are applicable in very general scenarios; one\nwhere the sufficient statistics is not available and another where the\nlikelihood is approximated using a functional approximation. We then discuss\nthe interpretation and elicitation aspects of the classes proposed here as well\nas their potential applications and possible computational benefits. These\nclasses establish the duality between approximate Bayesian inference and prior\nrobustness for a wide category of Bayesian inference methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 03:32:51 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Joshi", "Chaitanya", ""], ["Ruggeri", "Fabrizio", ""]]}, {"id": "2004.00816", "submitter": "Molei Liu", "authors": "Molei Liu, Yin Xia, Kelly Cho and Tianxi Cai", "title": "Integrative High Dimensional Multiple Testing with Heterogeneity under\n  Data Sharing Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying informative predictors in a high dimensional regression model is\na critical step for association analysis and predictive modeling. Signal\ndetection in the high dimensional setting often fails due to the limited sample\nsize. One approach to improve power is through meta-analyzing multiple studies\non the same scientific question. However, integrative analysis of high\ndimensional data from multiple studies is challenging in the presence of\nbetween study heterogeneity. The challenge is even more pronounced with\nadditional data sharing constraints under which only summary data but not\nindividual level data can be shared across different sites. In this paper, we\npropose a novel data shielding integrative large-scale testing (DSILT) approach\nto signal detection by allowing between study heterogeneity and not requiring\nsharing of individual level data. Assuming the underlying high dimensional\nregression models of the data differ across studies yet share similar support,\nthe DSILT approach incorporates proper integrative estimation and debiasing\nprocedures to construct test statistics for the overall effects of specific\ncovariates. We also develop a multiple testing procedure to identify\nsignificant effects while controlling for false discovery rate (FDR) and false\ndiscovery proportion (FDP). Theoretical comparisons of the DSILT procedure with\nthe ideal individual--level meta--analysis (ILMA) approach and other\ndistributed inference methods are investigated. Simulation studies demonstrate\nthat the DSILT procedure performs well in both false discovery control and\nattaining power. The proposed method is applied to a real example on detecting\ninteraction effect of the genetic variants for statins and obesity on the risk\nfor Type 2 Diabetes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 05:01:45 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 20:52:45 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 04:10:16 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Liu", "Molei", ""], ["Xia", "Yin", ""], ["Cho", "Kelly", ""], ["Cai", "Tianxi", ""]]}, {"id": "2004.00855", "submitter": "Shuhao Jiao", "authors": "Shuhao Jiao, Ron D. Frostig, Hernando Ombao", "title": "Variation Pattern Classification of Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new classification method for functional data is developed for the case\nwhere different groups or classes of functions have similar mean functions but\npotentially different second moments. The proposed method, second moment-based\nfunctional classifier (SMFC), uses the Hilbert-Schmidt norm to measure the\ndiscrepancy between the second moment structure of the different groups. The\nproposed SMFC method is demonstrated to be sensitive to the discrepancy in the\nsecond moment structure and thus produces lower rate of misclassification\ncompared to competitor methods. One important innovation lies in the dimension\nreduction step where the SMFC method data-adaptively determines the basis\nfunctions that account for most of the discrepancy. Consequently, the\nmisclassification rate is reduced because it removes components of the\nfunctional data that are only weakly discriminatory. In addition, the selected\ndiscriminative basis functions provide insights on the discrepancy between\ngroups as the basis functions reveal the features of variation pattern that\ndifferentiate groups. Consistency properties are established and, furthermore,\nsimulation studies and analysis of phoneme and rat brain activity trajectories\nempirically demonstrate the advantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 07:50:48 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 23:10:23 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Jiao", "Shuhao", ""], ["Frostig", "Ron D.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2004.00941", "submitter": "DDimitar Atanasov", "authors": "Nikolay M. Yanev, Vessela K. Stoimenova, Dimitar V. Atanasov", "title": "Stochastic modeling and estimation of COVID-19 population dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the paper is to describe a model of the development of the\nCovid-19 contamination of the population of a country or a region. For this\npurpose a special branching process with two types of individuals is\nconsidered. This model is intended to use only the observed daily statistics to\nestimate the main parameter of the contamination and to give a prediction of\nthe mean value of the non-observed population of the contaminated individuals.\nThis is a serious advantage in comparison with other more complicated models\nwhere the observed official statistics are not sufficient. In this way the\nspecific development of the Covid-19 epidemics is considered for different\ncountries.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 11:23:53 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Yanev", "Nikolay M.", ""], ["Stoimenova", "Vessela K.", ""], ["Atanasov", "Dimitar V.", ""]]}, {"id": "2004.00973", "submitter": "Abdulaziz Alenazi Dr", "authors": "Abdulaziz Alenazi", "title": "A Monte Carlo comparison of categorical tests of independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $X^2$ and $G^2$ tests are the most frequently applied tests for testing\nthe independence of two categorical variables. However, no one, to the best of\nour knowledge has compared them, extensively, and ultimately answer the\nquestion of which to use and when. Further, their applicability in cases with\nzero frequencies has been debated and (non parametric) permutation tests are\nsuggested. In this work we perform extensive Monte Carlo simulation studies\nattempting to answer both aforementioned points. As expected, in large sample\nsized cases ($>1,000$) the $X^2$ and $G^2$ are indistinguishable. In the small\nsample sized cases ($\\leq 1,000$) though, we provide strong evidence supporting\nthe use of the $X^2$ test regardless of zero frequencies for the case of\nunconditional independence. Also, we suggest the use of the permutation based\n$G^2$ test for testing conditional independence, at the cost of being\ncomputationally more expensive. The $G^2$ test exhibited inferior performance\nand its use should be limited.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 13:18:51 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Alenazi", "Abdulaziz", ""]]}, {"id": "2004.01112", "submitter": "Lillian Boe", "authors": "Lillian A. Boe (1), Lesley F. Tinker (2), and Pamela A. Shaw (1) ((1)\n  Department of Biostatistics, Epidemiology, and Informatics, University of\n  Pennsylvania Perelman School of Medicine, (2) WHI Clinical Coordinating\n  Center, Fred Hutchinson Cancer Research Center)", "title": "An Approximate Quasi-Likelihood Approach for Error-Prone Failure Time\n  Outcomes and Exposures", "comments": "61 pages, 1 figure, 14 tables in total. Main manuscript: first 38\n  pages including references and 6 tables, followed by supplementary materials\n  with remaining 23 pages including 1 figure and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement error arises commonly in clinical research settings that rely on\ndata from electronic health records or large observational cohorts. In\nparticular, self-reported outcomes are typical in cohort studies for chronic\ndiseases such as diabetes in order to avoid the burden of expensive diagnostic\ntests. Dietary intake, which is also commonly collected by self-report and\nsubject to measurement error, is a major factor linked to diabetes and other\nchronic diseases. These errors can bias exposure-disease associations that\nultimately can mislead clinical decision-making. We have extended an existing\nsemiparametric likelihood-based method for handling error-prone, discrete\nfailure time outcomes to also address covariate error. We conduct an extensive\nnumerical study to compare the proposed method to the naive approach that\nignores measurement error in terms of bias and efficiency in the estimation of\nthe regression parameter of interest. In all settings considered, the proposed\nmethod showed minimal bias and maintained coverage probability, thus\noutperforming the naive analysis which showed extreme bias and low coverage.\nThis method is applied to data from the Women's Health Initiative to assess the\nassociation between energy and protein intake and the risk of incident diabetes\nmellitus. Our results show that correcting for errors in both the self-reported\noutcome and dietary exposures leads to considerably different hazard ratio\nestimates than those from analyses that ignore measurement error, which\ndemonstrates the importance of correcting for both outcome and covariate error.\nComputational details and R code for implementing the proposed method are\npresented in Section S1 of the Supplementary Materials.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:24:08 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 14:32:50 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 15:54:57 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2021 21:46:48 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Boe", "Lillian A.", ""], ["Tinker", "Lesley F.", ""], ["Shaw", "Pamela A.", ""]]}, {"id": "2004.01218", "submitter": "Eli Sherman", "authors": "Eli Sherman, David Arbour, Ilya Shpitser", "title": "General Identification of Dynamic Treatment Regimes Under Interference", "comments": "2020 Conference on Artificial Intelligence and Statistics (AIStats)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applied fields, researchers are often interested in tailoring\ntreatments to unit-level characteristics in order to optimize an outcome of\ninterest. Methods for identifying and estimating treatment policies are the\nsubject of the dynamic treatment regime literature. Separately, in many\nsettings the assumption that data are independent and identically distributed\ndoes not hold due to inter-subject dependence. The phenomenon where a subject's\noutcome is dependent on his neighbor's exposure is known as interference. These\nareas intersect in myriad real-world settings. In this paper we consider the\nproblem of identifying optimal treatment policies in the presence of\ninterference. Using a general representation of interference, via\nLauritzen-Wermuth-Freydenburg chain graphs (Lauritzen and Richardson, 2002), we\nformalize a variety of policy interventions under interference and extend\nexisting identification theory (Tian, 2008; Sherman and Shpitser, 2018).\nFinally, we illustrate the efficacy of policy maximization under interference\nin a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 18:22:59 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Sherman", "Eli", ""], ["Arbour", "David", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2004.01249", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh", "title": "Robust Parametric Inference for Finite Markov Chains", "comments": "To appear in TEST", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of statistical inference in a parametric finite\nMarkov chain model and develop a robust estimator of the parameters defining\nthe transition probabilities via minimization of a suitable (empirical) version\nof the popular density power divergence. Based on a long sequence of\nobservations from a first-order stationary Markov chain, we have defined the\nminimum density power divergence estimator (MDPDE) of the underlying parameter\nand rigorously derived its asymptotic and robustness properties under\nappropriate conditions. Performance of the MDPDEs is illustrated theoretically\nas well as empirically for some common examples of finite Markov chain models.\nIts applications in robust testing of statistical hypotheses are also discussed\nalong with (parametric) comparison of two Markov chain sequences. Several\ndirections for extending the MDPDE and related inference are also briefly\ndiscussed for multiple sequences of Markov chains, higher order Markov chains\nand non-stationary Markov chains with time-dependent transition probabilities.\nFinally, our proposal is applied to analyze corporate credit rating migration\ndata of three international markets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 20:29:15 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 05:11:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ghosh", "Abhik", ""]]}, {"id": "2004.01292", "submitter": "Wagner Barreto-Souza", "authors": "Luiza Sette C\\^amara Piancastelli, Wagner Barreto-Souza and Vin\\'icius\n  Diniz Mayrink", "title": "Generalized inverse-Gaussian frailty models with application to TARGET\n  neuroblastoma data", "comments": null, "journal-ref": "Annals of the Institute of Statistical Mathematics (2021)", "doi": "10.1007/s10463-020-00774-z", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of survival frailty models based on the Generalized\nInverse-Gaussian (GIG) distributions is proposed. We show that the GIG frailty\nmodels are flexible and mathematically convenient like the popular gamma\nfrailty model. Furthermore, our proposed class is robust and does not present\nsome computational issues experienced by the gamma model. By assuming a\npiecewise-exponential baseline hazard function, which gives a semiparametric\nflavour for our frailty class, we propose an EM-algorithm for estimating the\nmodel parameters and provide an explicit expression for the information matrix.\nSimulated results are addressed to check the finite sample behavior of the\nEM-estimators and also to study the performance of the GIG models under\nmisspecification. We apply our methodology to a TARGET (Therapeutically\nApplicable Research to Generate Effective Treatments) data about survival time\nof patients with neuroblastoma cancer and show some advantages of the GIG\nfrailties over existing models in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 22:35:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Piancastelli", "Luiza Sette C\u00e2mara", ""], ["Barreto-Souza", "Wagner", ""], ["Mayrink", "Vin\u00edcius Diniz", ""]]}, {"id": "2004.01328", "submitter": "Qiong Li", "authors": "Qiong Li, Xiaoying Sun, Nanwei Wang", "title": "Penalized composite likelihood for colored graphical Gaussian models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a penalized composite likelihood method for model\nselection in colored graphical Gaussian models. The method provides a sparse\nand symmetry-constrained estimator of the precision matrix, and thus conducts\nmodel selection and precision matrix estimation simultaneously. In particular,\nthe method uses penalty terms to constrain the elements of the precision\nmatrix, which enables us to transform the model selection problem into a\nconstrained optimization problem. Further, computer experiments are conducted\nto illustrate the performance of the proposed new methodology. It is shown that\nthe proposed method performs well in both the selection of nonzero elements in\nthe precision matrix and the identification of symmetry structures in graphical\nmodels. The feasibility and potential clinical application of the proposed\nmethod are demonstrated on a microarray gene expression data set.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 01:45:28 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Li", "Qiong", ""], ["Sun", "Xiaoying", ""], ["Wang", "Nanwei", ""]]}, {"id": "2004.01462", "submitter": "Emanuele Aliverti", "authors": "Emanuele Aliverti and David B. Dunson", "title": "Composite mixture of log-linear models for categorical data", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate categorical data are routinely collected in many application\nareas. As the number of cells in the table grows exponentially with the number\nof variables, many or even most cells will contain zero observations. This\nsevere sparsity motivates appropriate statistical methodologies that\neffectively reduce the number of free parameters, with penalized log-linear\nmodels and latent structure analysis being popular options. This article\nproposes a fundamentally new class of methods, which we refer to as Mixture of\nLog Linear models (mills). Combining latent class analysis and log-linear\nmodels, mills defines a novel Bayesian methodology to model complex\nmultivariate categorical with flexibility and interpretability. Mills is shown\nto have key advantages over alternative methods for contingency tables in\nsimulations and an application investigating the relation among suicide\nattempts and empathy.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 10:26:04 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Aliverti", "Emanuele", ""], ["Dunson", "David B.", ""]]}, {"id": "2004.01563", "submitter": "Michel Broniatowski", "authors": "Michel Broniatowski (LPSM (UMR\\_8001)), Emilie Miranda (LPSM\n  (UMR\\_8001))", "title": "A sequential design for extreme quantiles estimation under binary\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequential design method aiming at the estimation of an extreme\nquantile based on a sample of dichotomic data corresponding to peaks over a\ngiven threshold. This study is motivated by an industrial challenge in material\nreliability and consists in estimating a failure quantile from trials whose\noutcomes are reduced to indicators of whether the specimen have failed at the\ntested stress levels. The solution proposed is a sequential design making use\nof a splitting approach, decomposing the target probability level into a\nproduct of probabilities of conditional events of higher order. The method\nconsists in gradually targeting the tail of the distribution and sampling under\ntruncated distributions. The model is GEV or Weibull, and sequential estimation\nof its parameters involves an improved maximum likelihood procedure for binary\ndata, due to the large uncertainty associated with such a restricted\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:42:01 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Broniatowski", "Michel", "", "LPSM"], ["Miranda", "Emilie", "", "LPSM"]]}, {"id": "2004.01623", "submitter": "Philipp Bach", "authors": "Philipp Bach, Sven Klaassen, Jannis Kueck, Martin Spindler", "title": "Uniform Inference in High-Dimensional Generalized Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for uniform valid confidence bands of a nonparametric\ncomponent $f_1$ in the general additive model $Y=f_1(X_1)+\\ldots + f_p(X_p) +\n\\varepsilon$ in a high-dimensional setting. We employ sieve estimation and\nembed it in a high-dimensional Z-estimation framework allowing us to construct\nuniformly valid confidence bands for the first component $f_1$. As usual in\nhigh-dimensional settings where the number of regressors $p$ may increase with\nsample, a sparsity assumption is critical for the analysis. We also run\nsimulations studies which show that our proposed method gives reliable results\nconcerning the estimation properties and coverage properties even in small\nsamples. Finally, we illustrate our procedure with an empirical application\ndemonstrating the implementation and the use of the proposed method in\npractice.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 15:30:29 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Bach", "Philipp", ""], ["Klaassen", "Sven", ""], ["Kueck", "Jannis", ""], ["Spindler", "Martin", ""]]}, {"id": "2004.01684", "submitter": "Ravi Kumar", "authors": "Andrei Z. Broder and Ravi Kumar", "title": "A Note on Double Pooling Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present double pooling, a simple, easy-to-implement variation on test\npooling, that in certain ranges for the a priori probability of a positive\ntest, is significantly more efficient than the standard single pooling approach\n(the Dorfman method).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:29:31 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Broder", "Andrei Z.", ""], ["Kumar", "Ravi", ""]]}, {"id": "2004.01757", "submitter": "Harlan Campbell", "authors": "Harlan Campbell", "title": "Equivalence testing for standardized effect sizes in linear regression", "comments": "30 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce equivalence testing procedures for standardized\neffect sizes in a linear regression. We show how to define valid hypotheses and\ncalculate p-values for these tests. Such tests are necessary to confirm the\nlack of a meaningful association between an outcome and predictors. A\nsimulation study is conducted to examine type I error rates and statistical\npower. We also compare using equivalence testing as part of a frequentist\ntesting scheme with an alternative Bayesian testing approach. The results\nindicate that the proposed equivalence test is a potentially useful tool for\n\"testing the null.\"\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 20:05:32 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 16:57:26 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 17:37:35 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2021 18:58:13 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Campbell", "Harlan", ""]]}, {"id": "2004.01759", "submitter": "David Robertson", "authors": "David S. Robertson, James M. S. Wason, Frank Bretz", "title": "Graphical approaches for the control of generalised error rates", "comments": "Accepted for publication in Statistics in Medicine", "journal-ref": null, "doi": "10.1002/sim.8595", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When simultaneously testing multiple hypotheses, the usual approach in the\ncontext of confirmatory clinical trials is to control the familywise error rate\n(FWER), which bounds the probability of making at least one false rejection. In\nmany trial settings, these hypotheses will additionally have a hierarchical\nstructure that reflects the relative importance and links between different\nclinical objectives. The graphical approach of Bretz et al. (2009) is a\nflexible and easily communicable way of controlling the FWER while respecting\ncomplex trial objectives and multiple structured hypotheses. However, the FWER\ncan be a very stringent criterion that leads to procedures with low power, and\nmay not be appropriate in exploratory trial settings. This motivates\ncontrolling generalised error rates, particularly when the number of hypotheses\ntested is no longer small. We consider the generalised familywise error rate\n(k-FWER), which is the probability of making k or more false rejections, as\nwell as the tail probability of the false discovery proportion (FDP), which is\nthe probability that the proportion of false rejections is greater than some\nthreshold. We also consider asymptotic control of the false discovery rate\n(FDR), which is the expectation of the FDP. In this paper, we show how to\ncontrol these generalised error rates when using the graphical approach and its\nextensions. We demonstrate the utility of the resulting graphical procedures on\nthree clinical trial case studies.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 20:18:42 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 10:57:44 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Robertson", "David S.", ""], ["Wason", "James M. S.", ""], ["Bretz", "Frank", ""]]}, {"id": "2004.01893", "submitter": "Neeraj Bokde", "authors": "Neeraj Dhanraj Bokde and Zaher Mundher Yaseen and Gorm Bruun Andersen", "title": "ForecastTB An R Package as a Test-Bench for Time Series Forecasting\n  Application of Wind Speed and Solar Radiation Modeling", "comments": "Published in Energies", "journal-ref": "2020", "doi": "10.3390/en13102578", "report-no": null, "categories": "stat.ME cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces an R package ForecastTB that can be used to compare the\naccuracy of different forecasting methods as related to the characteristics of\na time series dataset. The ForecastTB is a plug-and-play structured module, and\nseveral forecasting methods can be included with simple instructions. The\nproposed test-bench is not limited to the default forecasting and error metric\nfunctions, and users are able to append, remove, or choose the desired methods\nas per requirements. Besides, several plotting functions and statistical\nperformance metrics are provided to visualize the comparative performance and\naccuracy of different forecasting methods. Furthermore, this paper presents\nreal application examples with natural time series datasets (i.e., wind speed\nand solar radiation) to exhibit the features of the ForecastTB package to\nevaluate forecasting comparison analysis as affected by the characteristics of\na dataset. Modeling results indicated the applicability and robustness of the\nproposed R package ForecastTB for time series forecasting.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 08:52:19 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 13:49:21 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Bokde", "Neeraj Dhanraj", ""], ["Yaseen", "Zaher Mundher", ""], ["Andersen", "Gorm Bruun", ""]]}, {"id": "2004.01927", "submitter": "Pavlo Mozharovskyi", "authors": "Karl Mosler and Pavlo Mozharovskyi", "title": "Choosing among notions of multivariate depth statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Classical multivariate statistics measures the outlyingness of a point by its\nMahalanobis distance from the mean, which is based on the mean and the\ncovariance matrix of the data. A multivariate depth function is a function\nwhich, given a point and a distribution in d-space, measures centrality by a\nnumber between 0 and 1, while satisfying certain postulates regarding\ninvariance, monotonicity, convexity and continuity. Accordingly, numerous\nnotions of multivariate depth have been proposed in the literature, some of\nwhich are also robust against extremely outlying data. The departure from\nclassical Mahalanobis distance does not come without cost. There is a trade-off\nbetween invariance, robustness and computational feasibility. In the last few\nyears, efficient exact algorithms as well as approximate ones have been\nconstructed and made available in R-packages. Consequently, in practical\napplications the choice of a depth statistic is no more restricted to one or\ntwo notions due to computational limits; rather often more notions are\nfeasible, among which the researcher has to decide. The article debates\ntheoretical and practical aspects of this choice, including invariance and\nuniqueness, robustness and computational feasibility. Complexity and speed of\nexact algorithms are compared. The accuracy of approximate approaches like the\nrandom Tukey depth is discussed as well as the application to large and\nhigh-dimensional data. Extensions to local and functional depths and\nconnections to regression depth are shortly addressed.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 13:06:44 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 16:28:07 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 12:29:52 GMT"}, {"version": "v4", "created": "Wed, 5 May 2021 17:17:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mosler", "Karl", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "2004.01975", "submitter": "Wenshuo Wang", "authors": "Yichao Li, Wenshuo Wang, Ke Deng and Jun S Liu", "title": "Stratification and Optimal Resampling for Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC), also known as particle filters, has been widely\naccepted as a powerful computational tool for making inference with dynamical\nsystems. A key step in SMC is resampling, which plays the role of steering the\nalgorithm towards the future dynamics. Several strategies have been proposed\nand used in practice, including multinomial resampling, residual resampling\n(Liu and Chen 1998), optimal resampling (Fearnhead and Clifford 2003),\nstratified resampling (Kitagawa 1996), and optimal transport resampling (Reich\n2013). We show that, in the one dimensional case, optimal transport resampling\nis equivalent to stratified resampling on the sorted particles, and they both\nminimize the resampling variance as well as the expected squared energy\ndistance between the original and resampled empirical distributions; in the\nmultidimensional case, the variance of stratified resampling after sorting\nparticles using Hilbert curve (Gerber et al. 2019) in $\\mathbb{R}^d$ is\n$O(m^{-(1+2/d)})$, an improved rate compared to the original $O(m^{-(1+1/d)})$,\nwhere $m$ is the number of resampled particles. This improved rate is the\nlowest for ordered stratified resampling schemes, as conjectured in Gerber et\nal. (2019). We also present an almost sure bound on the Wasserstein distance\nbetween the original and Hilbert-curve-resampled empirical distributions. In\nlight of these theoretical results, we propose the stratified\nmultiple-descendant growth (SMG) algorithm, which allows us to explore the\nsample space more efficiently compared to the standard i.i.d.\nmultiple-descendant sampling-resampling approach as measured by the Wasserstein\nmetric. Numerical evidence is provided to demonstrate the effectiveness of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 17:12:09 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 08:41:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Li", "Yichao", ""], ["Wang", "Wenshuo", ""], ["Deng", "Ke", ""], ["Liu", "Jun S", ""]]}, {"id": "2004.02007", "submitter": "Anastasios Papanikos Tasos", "authors": "Tasos Papanikos, John R Thompson, Keith R Abrams, Sylwia Bujkiewicz", "title": "A novel approach to bivariate meta-analysis of binary outcomes and its\n  application in the context of surrogate endpoints", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bivariate meta-analysis provides a useful framework for combining information\nacross related studies and has been widely utilised to combine evidence from\nclinical studies in order to evaluate treatment efficacy. Bivariate\nmeta-analysis has also been used to investigate surrogacy patterns between\ntreatment effects on the surrogate and the final outcome. Surrogate endpoints\nplay an important role in drug development when they can be used to measure\ntreatment effect early compared to the final clinical outcome and to predict\nclinical benefit or harm. The standard bivariate meta-analytic approach models\nthe observed treatment effects on the surrogate and final outcomes jointly, at\nboth the within-study and between-studies levels, using a bivariate normal\ndistribution. For binomial data a normal approximation can be used on log odds\nratio scale, however, this method may lead to biased results when the\nproportions of events are close to one or zero, affecting the validation of\nsurrogate endpoints. In this paper, two Bayesian meta-analytic approaches are\nintroduced which allow for modelling the within-study variability using\nbinomial data directly. The first uses independent binomial likelihoods to\nmodel the within-study variability avoiding to approximate the observed\ntreatment effects, however, ignores the within-study association. The second,\nmodels the summarised events in each arm jointly using a bivariate copula with\nbinomial marginals. This allows the model to take into account the within-study\nassociation through the copula dependence parameter. We applied the methods to\nan illustrative example in chronic myeloid leukemia to investigate the\nsurrogate relationship between complete cytogenetic response (CCyR) and\nevent-free-survival (EFS).\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:05:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Papanikos", "Tasos", ""], ["Thompson", "John R", ""], ["Abrams", "Keith R", ""], ["Bujkiewicz", "Sylwia", ""]]}, {"id": "2004.02008", "submitter": "Brenda Betancourt", "authors": "Brenda Betancourt, Giacomo Zanella and Rebecca C. Steorts", "title": "Random Partition Models for Microclustering Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Bayesian random partition models assume that the size of each\ncluster grows linearly with the number of data points. While this is appealing\nfor some applications, this assumption is not appropriate for other tasks such\nas entity resolution, modeling of sparse networks, and DNA sequencing tasks.\nSuch applications require models that yield clusters whose sizes grow\nsublinearly with the total number of data points -- the microclustering\nproperty. Motivated by these issues, we propose a general class of random\npartition models that satisfy the microclustering property with\nwell-characterized theoretical properties. Our proposed models overcome major\nlimitations in the existing literature on microclustering models, namely a lack\nof interpretability, identifiability, and full characterization of model\nasymptotic properties. Crucially, we drop the classical assumption of having an\nexchangeable sequence of data points, and instead assume an exchangeable\nsequence of clusters. In addition, our framework provides flexibility in terms\nof the prior distribution of cluster sizes, computational tractability, and\napplicability to a large number of microclustering tasks. We establish\ntheoretical properties of the resulting class of priors, where we characterize\nthe asymptotic behavior of the number of clusters and of the proportion of\nclusters of a given size. Our framework allows a simple and efficient Markov\nchain Monte Carlo algorithm to perform statistical inference. We illustrate our\nproposed methodology on the microclustering task of entity resolution, where we\nprovide a simulation study and real experiments on survey panel data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:06:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Betancourt", "Brenda", ""], ["Zanella", "Giacomo", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "2004.02051", "submitter": "Xinwei Deng", "authors": "Xiaoning Kang, Xiaoyu Chen, Ran Jin, Hao Wu and Xinwei Deng", "title": "Multivariate Regression of Mixed Responses for Evaluation of\n  Visualization Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information visualization significantly enhances human perception by\ngraphically representing complex data sets. The variety of visualization\ndesigns makes it challenging to efficiently evaluate all possible designs\ncatering to users' preferences and characteristics. Most of existing evaluation\nmethods perform user studies to obtain multivariate qualitative responses from\nusers via questionnaires and interviews. However, these methods cannot support\nonline evaluation of designs as they are often time-consuming. A statistical\nmodel is desired to predict users' preferences on visualization designs based\non non-interference measurements (i.e., wearable sensor signals). In this work,\nwe propose a multivariate regression of mixed responses (MRMR) to facilitate\nquantitative evaluation of visualization designs. The proposed MRMR method is\nable to provide accurate model prediction with meaningful variable selection. A\nsimulation study and a user study of evaluating visualization designs with 14\neffective participants are conducted to illustrate the merits of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 23:54:35 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kang", "Xiaoning", ""], ["Chen", "Xiaoyu", ""], ["Jin", "Ran", ""], ["Wu", "Hao", ""], ["Deng", "Xinwei", ""]]}, {"id": "2004.02281", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy and Sayar Karmakar", "title": "Analyzing initial stage of COVID-19 transmission through Bayesian\n  time-varying model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent outbreak of the novel coronavirus COVID-19 has affected all of our\nlives in one way or the other. While medical researchers are working hard to\nfind a cure and doctors/nurses to attend the affected individuals, measures\nsuch as `lockdown', `stay-at-home', `social distancing' are being implemented\nin different parts of the world to curb its further spread. To model the\nnon-stationary spread, we propose a novel time-varying semiparametric AR$(p)$\nmodel for the count valued time-series of newly affected cases, collected every\nday and also extend it to propose a novel time-varying INGARCH model. Our\nproposed structures of the models are amenable to Hamiltonian Monte Carlo (HMC)\nsampling for efficient computation. We substantiate our methods by simulations\nthat show superiority compared to some of the close existing methods. Finally\nwe analyze the daily time series data of newly confirmed cases to study its\nspread through different government interventions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 19:01:51 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 15:36:02 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 01:00:31 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 19:50:34 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Roy", "Arkaprava", ""], ["Karmakar", "Sayar", ""]]}, {"id": "2004.02336", "submitter": "He Li", "authors": "Xi Chen and Jason D. Lee and He Li and Yun Yang", "title": "Distributed Estimation for Principal Component Analysis: an Enlarged\n  Eigenspace Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing size of modern data sets brings many challenges to the existing\nstatistical estimation approaches, which calls for new distributed\nmethodologies. This paper studies distributed estimation for a fundamental\nstatistical machine learning problem, principal component analysis (PCA).\nDespite the massive literature on top eigenvector estimation, much less is\npresented for the top-$L$-dim ($L>1$) eigenspace estimation, especially in a\ndistributed manner. We propose a novel multi-round algorithm for constructing\ntop-$L$-dim eigenspace for distributed data. Our algorithm takes advantage of\nshift-and-invert preconditioning and convex optimization. Our estimator is\ncommunication-efficient and achieves a fast convergence rate. In contrast to\nthe existing divide-and-conquer algorithm, our approach has no restriction on\nthe number of machines. Theoretically, the traditional Davis-Kahan theorem\nrequires the explicit eigengap assumption to estimate the top-$L$-dim\neigenspace. To abandon this eigengap assumption, we consider a new route in our\nanalysis: instead of exactly identifying the top-$L$-dim eigenspace, we show\nthat our estimator is able to cover the targeted top-$L$-dim population\neigenspace. Our distributed algorithm can be applied to a wide range of\nstatistical problems based on PCA, such as principal component regression and\nsingle index model. Finally, We provide simulation studies to demonstrate the\nperformance of the proposed distributed estimator.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:28:08 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 19:43:28 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 02:24:02 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Chen", "Xi", ""], ["Lee", "Jason D.", ""], ["Li", "He", ""], ["Yang", "Yun", ""]]}, {"id": "2004.02359", "submitter": "Ranadeep Daw", "authors": "Ranadeep Daw, Zhuoqiong He", "title": "Deep Neural Network in Cusp Catastrophe Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophe theory was originally proposed to study dynamical systems that\nexhibit sudden shifts in behavior arising from small changes in input. These\nmodels can generate reasonable explanation behind abrupt jumps in nonlinear\ndynamic models. Among the different catastrophe models, the Cusp Catastrophe\nmodel attracted the most attention due to it's relatively simpler dynamics and\nrich domain of application. Due to the complex behavior of the response, the\nparameter space becomes highly non-convex and hence it becomes very hard to\noptimize to figure out the generating parameters. Instead of solving for these\ngenerating parameters, we demonstrated how a Machine learning model can be\ntrained to learn the dynamics of the Cusp catastrophe models, without ever\nreally solving for the generating model parameters. Simulation studies and\napplication on a few famous datasets are used to validate our approach. To our\nknowledge, this is the first paper of such kind where a neural network based\napproach has been applied in Cusp Catastrophe model.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 00:25:41 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 02:30:54 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Daw", "Ranadeep", ""], ["He", "Zhuoqiong", ""]]}, {"id": "2004.02414", "submitter": "Feifei Wang", "authors": "Feifei Wang, Danyang Huang, Yingqiu Zhu, Hansheng Wang", "title": "Efficient Estimation for Generalized Linear Models on a Distributed\n  System with Nonrandomly Distributed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems have been widely used in practice to accomplish data\nanalysis tasks of huge scales. In this work, we target on the estimation\nproblem of generalized linear models on a distributed system with nonrandomly\ndistributed data. We develop a Pseudo-Newton-Raphson algorithm for efficient\nestimation. In this algorithm, we first obtain a pilot estimator based on a\nsmall random sample collected from different Workers. Then conduct one-step\nupdating based on the computed derivatives of log-likelihood functions in each\nWorker at the pilot estimator. The final one-step estimator is proved to be\nstatistically efficient as the global estimator, even with nonrandomly\ndistributed data. In addition, it is computationally efficient, in terms of\nboth communication cost and storage usage. Based on the one-step estimator, we\nalso develop a likelihood ratio test for hypothesis testing. The theoretical\nproperties of the one-step estimator and the corresponding likelihood ratio\ntest are investigated. The finite sample performances are assessed through\nsimulations. Finally, an American Airline dataset is analyzed on a Spark\ncluster for illustration purpose.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 06:10:29 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Feifei", ""], ["Huang", "Danyang", ""], ["Zhu", "Yingqiu", ""], ["Wang", "Hansheng", ""]]}, {"id": "2004.02525", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over, Tim Friede", "title": "Bounds for the weight of external data in shrinkage estimation", "comments": "14 pages, 1 figure, 5 tables", "journal-ref": "Biometrical Journal, 65(5):1131-1143, 2021", "doi": "10.1002/bimj.202000227", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimation in a meta-analysis framework may be used to facilitate\ndynamical borrowing of information. This framework might be used to analyze a\nnew study in the light of previous data, which might differ in their design\n(e.g., a randomized controlled trial (RCT) and a clinical registry). We show\nhow the common study weights arise in effect and shrinkage estimation, and how\nthese may be generalized to the case of Bayesian meta-analysis. Next we develop\nsimple ways to compute bounds on the weights, so that the contribution of the\nexternal evidence may be assessed a priori. These considerations are\nillustrated and discussed using numerical examples, including applications in\nthe treatment of Creutzfeldt-Jakob disease and in fetal monitoring to prevent\nthe occurrence of metabolic acidosis. The target study's contribution to the\nresulting estimate is shown to be bounded below. Therefore, concerns of\nevidence being easily overwhelmed by external data are largely unwarranted.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 09:56:20 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 07:34:55 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 15:52:17 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Friede", "Tim", ""]]}, {"id": "2004.02653", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist", "title": "Gaussian Process Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel way to combine boosting with Gaussian process and mixed\neffects models. This allows for relaxing, first, the linearity assumption for\nthe mean function in Gaussian process and grouped random effects models in a\nflexible non-parametric way and, second, the independence assumption made in\nmost boosting algorithms. The former is advantageous for predictive accuracy\nand for avoiding model misspecifications. The latter is important for more\nefficient learning of the mean function and for obtaining probabilistic\npredictions. In addition, we present an extension that scales to large data\nusing a Vecchia approximation for the Gaussian process model relying on novel\nresults for covariance parameter inference. We obtain increased predictive\naccuracy compared to existing approaches on several simulated and real-world\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:19:54 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 20:00:36 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 12:06:59 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Sigrist", "Fabio", ""]]}, {"id": "2004.02670", "submitter": "Olivier Scaillet", "authors": "Stelios Arvanitis, Olivier Scaillet, Nikolas Topaloglou", "title": "Spanning analysis of stock market anomalies under Prospect Stochastic\n  Dominance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM econ.EM q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and implement methods for determining whether introducing new\nsecurities or relaxing investment constraints improves the investment\nopportunity set for prospect investors. We formulate a new testing procedure\nfor prospect spanning for two nested portfolio sets based on subsampling and\nLinear Programming. In an application, we use the prospect spanning framework\nto evaluate whether well-known anomalies are spanned by standard factors. We\nfind that of the strategies considered, many expand the opportunity set of the\nprospect type investors, thus have real economic value for them. In-sample and\nout-of-sample results prove remarkably consistent in identifying genuine\nanomalies for prospect investors.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:41:32 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Arvanitis", "Stelios", ""], ["Scaillet", "Olivier", ""], ["Topaloglou", "Nikolas", ""]]}, {"id": "2004.02689", "submitter": "Junan Zhu", "authors": "Junan Zhu, Kristina Rivera, Dror Baron", "title": "Noisy Pooled PCR for Virus Testing", "comments": "5 pages, 3 figures; we welcome new collaborators to reach out and\n  help improve this work!", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.IT eess.SP math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast testing can help mitigate the coronavirus disease 2019 (COVID-19)\npandemic. Despite their accuracy for single sample analysis, infectious\ndiseases diagnostic tools, like RT-PCR, require substantial resources to test\nlarge populations. We develop a scalable approach for determining the viral\nstatus of pooled patient samples. Our approach converts group testing to a\nlinear inverse problem, where false positives and negatives are interpreted as\ngenerated by a noisy communication channel, and a message passing algorithm\nestimates the illness status of patients. Numerical results reveal that our\napproach estimates patient illness using fewer pooled measurements than\nexisting noisy group testing algorithms. Our approach can easily be extended to\nvarious applications, including where false negatives must be minimized.\nFinally, in a Utopian world we would have collaborated with RT-PCR experts; it\nis difficult to form such connections during a pandemic. We welcome new\ncollaborators to reach out and help improve this work!\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:12:20 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhu", "Junan", ""], ["Rivera", "Kristina", ""], ["Baron", "Dror", ""]]}, {"id": "2004.02692", "submitter": "Claudia Kirch", "authors": "Idris Eckley, Claudia Kirch, Silke Weber", "title": "A novel change point approach for the detection of gas emission sources\n  using remotely contained concentration data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an example from remote sensing of gas emission sources, we\nderive two novel change point procedures for multivariate time series where, in\ncontrast to classical change point literature, the changes are not required to\nbe aligned in the different components of the time series. Instead the change\npoints are described by a functional relationship where the precise shape\ndepends on unknown parameters of interest such as the source of the gas\nemission in the above example. Two different types of tests and the\ncorresponding estimators for the unknown parameters describing the change\nlocations are proposed. We derive the null asymptotics for both tests under\nweak assumptions on the error time series and show asymptotic consistency under\nalternatives. Furthermore, we prove consistency for the corresponding\nestimators of the parameters of interest. The small sample behavior of the\nmethodology is assessed by means of a simulation study and the above remote\nsensing example analyzed in detail.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:17:53 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Eckley", "Idris", ""], ["Kirch", "Claudia", ""], ["Weber", "Silke", ""]]}, {"id": "2004.02708", "submitter": "Fulvia Mecatti Dr", "authors": "Fulvia Mecatti and Charalambos Sismanidis and Emanuela Furfaro", "title": "Sequential adaptive strategy for population-based sampling of a rare and\n  clustered disease", "comments": "21 pages, 1 tenable, 3 figures - Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An innovative sampling strategy is proposed, which applies to large-scale\npopulation-based surveys targeting a rare trait that is unevenly spread over a\ngeographical area of interest. Our proposal is characterised by the ability to\ntailor the data collection to specific features and challenges of the survey at\nhand. It is based on integrating an adaptive component into a sequential\nselection, which aims to both intensify detection of positive cases, upon\nexploiting the spatial clusterisation, and provide a flexible framework for\nmanaging logistical and budget constraints. To account for the selection bias,\na ready-to-implement weighting system is provided to release unbiased and\naccurate estimates. Empirical evidence is illustrated from tuberculosis\nprevalence surveys, which are recommended in many countries and supported by\nthe WHO as an emblematic example of the need for an improved sampling design.\nSimulation results are also given to illustrate strengths and weaknesses of the\nproposed sampling strategy with respect to traditional cross-sectional\nsampling.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:46:22 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Mecatti", "Fulvia", ""], ["Sismanidis", "Charalambos", ""], ["Furfaro", "Emanuela", ""]]}, {"id": "2004.02799", "submitter": "Mike Pereira", "authors": "Mike Pereira, Nicolas Desassis, C\\'edric Magneron, Nathan Palmer", "title": "A matrix-free approach to geostatistical filtering", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to geostatistical filtering which\ntackles two challenges encountered when applying this method to complex spatial\ndatasets: modeling the non-stationarity of the data while still being able to\nwork with large datasets. The approach is based on a finite element\napproximation of Gaussian random fields expressed as an expansion of the\neigenfunctions of a Laplace--Beltrami operator defined to account for local\nanisotropies. The numerical approximation of the resulting random fields using\na finite element approach is then leveraged to solve the scalability issue\nthrough a matrix-free approach. Finally, two cases of application of this\napproach, on simulated and real seismic data are presented.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:44:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Pereira", "Mike", ""], ["Desassis", "Nicolas", ""], ["Magneron", "C\u00e9dric", ""], ["Palmer", "Nathan", ""]]}, {"id": "2004.02994", "submitter": "Feifang Hu Dr", "authors": "Feifang Hu and Li-Xin Zhang", "title": "On the Theory of Covariate-Adaptive Designs", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pocock and Simon's marginal procedure (Pocock and Simon, 1975) is often\nimplemented forbalancing treatment allocation over influential covariates in\nclinical trials. However, the theoretical properties of Pocock and Simion's\nprocedure have remained largely elusive for decades. In this paper, we propose\na general framework for covariate-adaptive designs and establish the\ncorresponding theory under widely satisfied conditions. As a special case, we\nobtain the theoretical properties of Pocock and Simon's marginal procedure: the\nmarginal imbalances and overall imbalance are bounded in probability, but the\nwithin-stratum imbalances increase with the rate of $\\sqrt{n}$ as the sample\nsize increases. The theoretical results provide new insights about balance\nproperties of covariate-adaptive randomization procedures and open a door to\nstudy the theoretical properties of statistical inference for clinical trials\nbased on covariate-adaptive randomization procedures.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:53:31 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hu", "Feifang", ""], ["Zhang", "Li-Xin", ""]]}, {"id": "2004.03127", "submitter": "Tracy Qi Dong", "authors": "Tracy Qi Dong and Jon Wakefield", "title": "Modeling and presentation of vaccination coverage estimates using data\n  from household surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly popular to produce high-resolution maps of\nvaccination coverage by fitting Bayesian geostatistical models to data from\nhousehold surveys. Often, the surveys adopt a stratified cluster sampling\ndesign. We discuss a number of crucial choices with respect to two key aspects\nof the map production process: the acknowledgment of the survey design in\nmodeling, and the appropriate presentation of estimates and their\nuncertainties. Specifically, we consider the importance of accounting for\nsurvey stratification and cluster-level non-spatial excess variation in survey\noutcomes when fitting geostatistical models. We also discuss the trade-off\nbetween the geographical scale and precision of model-based estimates, and\ndemonstrate visualization methods for mapping and ranking that emphasize the\nprobabilistic interpretation of results. A novel approach to coverage map\npresentation is proposed to allow comparison and control of the overall map\nuncertainty level. We use measles vaccination coverage in Nigeria as a\nmotivating example and illustrate the different issues using data from the 2018\nNigeria Demographic and Health Survey.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 04:52:37 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Dong", "Tracy Qi", ""], ["Wakefield", "Jon", ""]]}, {"id": "2004.03130", "submitter": "Soudeep Deb", "authors": "Soudeep Deb", "title": "Analyzing count data using a time series model with an exponentially\n  decaying covariance structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data appears in various disciplines. In this work, a new method to\nanalyze time series count data has been proposed. The method assumes\nexponentially decaying covariance structure, a special class of the Mat\\'ern\ncovariance function, for the latent variable in a Poisson regression model. It\nis implemented in a Bayesian framework, with the help of Gibbs sampling and\nARMS sampling techniques. The proposed approach provides reliable estimates for\nthe covariate effects and estimates the extent of variability explained by the\ntemporally dependent process and the white noise process. The method is\nflexible, allows irregular spaced data, and can be extended naturally to bigger\ndatasets. The Bayesian implementation helps us to compute the posterior\npredictive distribution and hence is more appropriate and attractive for count\ndata forecasting problems. Two real life applications of different flavors are\nincluded in the paper. These two examples and a short simulation study\nestablish that the proposed approach has good inferential and predictive\nabilities and performs better than the other competing models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 04:57:54 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 12:19:12 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Deb", "Soudeep", ""]]}, {"id": "2004.03165", "submitter": "Christian Bongiorno", "authors": "Christian Bongiorno", "title": "Bootstraps Regularize Singular Correlation Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.SP q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I show analytically that the average of $k$ bootstrapped correlation matrices\nrapidly becomes positive-definite as $k$ increases, which provides a simple\napproach to regularize singular Pearson correlation matrices. If $n$ is the\nnumber of objects and $t$ the number of features, the averaged correlation\nmatrix is almost surely positive-definite if $k> \\frac{e}{e-1}\\frac{n}{t}\\simeq\n1.58\\frac{n}{t}$ in the limit of large $t$ and $n$. The probability of\nobtaining a positive-definite correlation matrix with $k$ bootstraps is also\nderived for finite $n$ and $t$. Finally, I demonstrate that the number of\nrequired bootstraps is always smaller than $n$. This method is particularly\nrelevant in fields where $n$ is orders of magnitude larger than the size of\ndata points $t$, e.g., in finance, genetics, social science, or image\nprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 07:26:35 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bongiorno", "Christian", ""]]}, {"id": "2004.03187", "submitter": "Erlis Ruli", "authors": "Paolo Girardi, Luca Greco, Valentina Mameli, Monica Musio, Walter\n  Racugno, Erlis Ruli, Laura Ventura", "title": "Robust inference for nonlinear regression models from the Tsallis score:\n  application to Covid-19 contagion in Italy", "comments": "15 pages, 6 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an approach for fitting robust nonlinear regression models, which\ncan be employed to model and predict the contagion dynamics of the Covid-19 in\nItaly. The focus is on the analysis of epidemic data using robust dose-response\ncurves, but the functionality is applicable to arbitrary nonlinear regression\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:15:33 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 07:38:14 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Girardi", "Paolo", ""], ["Greco", "Luca", ""], ["Mameli", "Valentina", ""], ["Musio", "Monica", ""], ["Racugno", "Walter", ""], ["Ruli", "Erlis", ""], ["Ventura", "Laura", ""]]}, {"id": "2004.03294", "submitter": "David Hofmeyr", "authors": "David P. Hofmeyr, Francois Kamper and Michail C. Melonas", "title": "Optimal Projections for Gaussian Discriminants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of obtaining optimal projections for performing discriminant\nanalysis with Gaussian class densities is studied. Unlike in most existing\napproaches to the problem, the focus of the optimisation is on the multinomial\nlikelihood based on posterior probability estimates, which directly captures\ndiscriminability of classes. In addition to the more commonly considered\nproblem, in this context, of classification, the unsupervised clustering\ncounterpart is also considered. Finding optimal projections offers utility for\ndimension reduction and regularisation, as well as instructive visualisation\nfor better model interpretability. Practical applications of the proposed\napproach show considerable promise for both classification and clustering. Code\nto implement the proposed method is available in the form of an R package from\nhttps://github.com/DavidHofmeyr/OPGD.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:59:44 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hofmeyr", "David P.", ""], ["Kamper", "Francois", ""], ["Melonas", "Michail C.", ""]]}, {"id": "2004.03448", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong and Michal Koles\\'ar and Mikkel\n  Plagborg-M{\\o}ller", "title": "Robust Empirical Bayes Confidence Intervals", "comments": "45 pages and a 24-page supplemental appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct robust empirical Bayes confidence intervals (EBCIs) in a normal\nmeans problem. The intervals are centered at the usual linear empirical Bayes\nestimator, but use a critical value accounting for shrinkage. Parametric EBCIs\nthat assume a normal distribution for the means (Morris, 1983) may\nsubstantially undercover when this assumption is violated, and we derive a\nsimple rule of thumb for gauging the potential coverage distortion. In\ncontrast, our EBCIs control coverage regardless of the means distribution,\nwhile remaining close in length to the parametric EBCIs when the means are\nindeed Gaussian. If the means are treated as fixed, our EBCIs have an average\ncoverage guarantee: the coverage probability is at least $1 - \\alpha$ on\naverage across the $n$ EBCIs for each of the means. Our empirical applications\nconsider effects of U.S. neighborhoods on intergenerational mobility, and\nstructural changes in a large dynamic factor model for the Eurozone.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:54:04 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 00:42:21 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""], ["Plagborg-M\u00f8ller", "Mikkel", ""]]}, {"id": "2004.03480", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya and Shirshendu Chatterjee", "title": "General Community Detection with Optimal Recovery Conditions for\n  Multi-relational Sparse Networks with Dependent Layers", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer and multiplex networks are becoming common network data sets in\nrecent times. We consider the problem of identifying the common community\nstructure for a special type of multilayer networks called multi-relational\nnetworks. We consider extensions of the spectral clustering methods for\nmulti-relational networks and give theoretical guarantees that the spectral\nclustering methods recover community structure consistently for\nmulti-relational networks generated from multilayer versions of both stochastic\nand degree-corrected block models even with dependence between network layers.\nThe methods are shown to work under optimal conditions on the degree parameter\nof the networks to detect both assortative and disassortative community\nstructures with vanishing error proportions even if individual layers of the\nmulti-relational network has the network structures below community\ndetectability threshold. We reinforce the validity of the theoretical results\nvia simulations too.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 03:19:45 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Chatterjee", "Shirshendu", ""]]}, {"id": "2004.03569", "submitter": "Emma Jingfei Zhang", "authors": "Biao Cai, Jingfei Zhang, Yongtao Guan", "title": "Latent Network Structure Learning from High Dimensional Multivariate\n  Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the latent network structure from large scale multivariate point\nprocess data is an important task in a wide range of scientific and business\napplications. For instance, we might wish to estimate the neuronal functional\nconnectivity network based on spiking times recorded from a collection of\nneurons. To characterize the complex processes underlying the observed data, we\npropose a new and flexible class of nonstationary Hawkes processes that allow\nboth excitatory and inhibitory effects. We estimate the latent network\nstructure using an efficient sparse least squares estimation approach. Using a\nthinning representation, we establish concentration inequalities for the first\nand second order statistics of the proposed Hawkes process. Such theoretical\nresults enable us to establish the non-asymptotic error bound and the selection\nconsistency of the estimated parameters. Furthermore, we describe a least\nsquares loss based statistic for testing if the background intensity is\nconstant in time. We demonstrate the efficacy of our proposed method through\nsimulation studies and an application to a neuron spike train data set.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:48:01 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 20:07:22 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Cai", "Biao", ""], ["Zhang", "Jingfei", ""], ["Guan", "Yongtao", ""]]}, {"id": "2004.03683", "submitter": "Brian Williamson", "authors": "Brian D. Williamson, Peter B. Gilbert, Noah R. Simon, Marco Carone", "title": "A unified approach for inference on algorithm-agnostic variable\n  importance", "comments": "55 total pages (31 in the main document, 24 supplementary), 14\n  figures (4 in the main document, 10 supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, it is of interest to assess the relative contribution\nof features (or subsets of features) toward the goal of predicting a response\n-- in other words, to gauge the variable importance of features. Most recent\nwork on variable importance assessment has focused on describing the importance\nof features within the confines of a given prediction algorithm. However, such\nassessment does not necessarily characterize the prediction potential of\nfeatures, and may provide a misleading reflection of the intrinsic value of\nthese features. To address this limitation, we propose a general framework for\nnonparametric inference on interpretable algorithm-agnostic variable\nimportance. We define variable importance as a population-level contrast\nbetween the oracle predictiveness of all available features versus all features\nexcept those under consideration. We propose a nonparametric efficient\nestimation procedure that allows the construction of valid confidence\nintervals, even when machine learning techniques are used. We also outline a\nvalid strategy for testing the null importance hypothesis. Through simulations,\nwe show that our proposal has good operating characteristics, and we illustrate\nits use with data from a study of an antibody against HIV-1 infection.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:09:21 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Williamson", "Brian D.", ""], ["Gilbert", "Peter B.", ""], ["Simon", "Noah R.", ""], ["Carone", "Marco", ""]]}, {"id": "2004.03751", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Genya Kobayashi", "title": "Robust Mixture Modeling using Weighted Complete Estimating Equations", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture modeling that takes account of potential heterogeneity in data is\nwidely adopted for classification and clustering problems. However, it can be\nsensitive to outliers especially when the mixture components are Gaussian. In\nthis paper, we introduce the robust estimating method using the weighted\ncomplete estimating equations for the robust fitting of multivariate mixture\nmodels. The proposed approach is based on a simple modification of the complete\nestimating equation given the latent variables of grouping indicators with the\nweights that depend on the components of mixture distributions. The weights are\ndesigned in such a way that outliers are downweighted and a simple computing\nstrategy is facilitated. We develop a simple expectation-estimating-equation\n(EEE) algorithm to solve the weighted complete estimating equations. As\nexamples, the multivariate Gaussian mixture, the mixture of experts, and\nmultivariate skew normal mixture are considered. In particular, we derive a\nnovel EEE algorithm for the skew normal mixture which results in the\nclosed-form expressions for both the E- and EE-steps by slightly extending the\nproposed method. The numerical performance of the proposed method is examined\nthrough simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 00:10:43 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 12:44:27 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 12:54:09 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kobayashi", "Genya", ""]]}, {"id": "2004.03758", "submitter": "Zijian Guo", "authors": "Zijian Guo, Domagoj \\'Cevid and Peter B\\\"uhlmann", "title": "Doubly Debiased Lasso: High-Dimensional Inference under Hidden\n  Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal relationships or related associations from observational\ndata can be invalidated by the existence of hidden confounding. We focus on a\nhigh-dimensional linear regression setting, where the measured covariates are\naffected by hidden confounding and propose the {\\em Doubly Debiased Lasso}\nestimator for individual components of the regression coefficient vector. Our\nadvocated method simultaneously corrects both the bias due to estimation of\nhigh-dimensional parameters as well as the bias caused by the hidden\nconfounding. We establish its asymptotic normality and also prove that it is\nefficient in the Gauss-Markov sense. The validity of our methodology relies on\na dense confounding assumption, i.e. that every confounding variable affects\nmany covariates. The finite sample performance is illustrated with an extensive\nsimulation study and a genomic application.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 00:30:19 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 03:12:35 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 20:15:45 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Guo", "Zijian", ""], ["\u0106evid", "Domagoj", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2004.03854", "submitter": "Fran\\c{c}ois Bachoc", "authors": "Fran\\c{c}ois Bachoc (IMT), Th\\'eo Barthe, Thomas Santner (OSU), Yann\n  Richet (IRSN)", "title": "Sequential Design of Mixture Experiments with an Empirically Determined\n  Input Domain and an Application to Burn-up Credit Penalization of Nuclear\n  Fuel Rods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a sequential design for maximizing a stochastic computer\nsimulator output, y(x), over an unknown optimization domain. The training data\nused to estimate the optimization domain are a set of (historical) inputs,\noften from a physical system modeled by the simulator. Two methods are provided\nfor estimating the simulator input domain. An extension of the well-known\nefficient global optimization algorithm is presented to maximize y(x). The\ndomain estimation/maximization procedure is applied to two readily understood\nanalytic examples. It is also used to solve a problem in nuclear safety by\nmaximizing the k-effective \"criticality coefficient\" of spent fuel rods,\nconsidered as one-dimensional heterogeneous fissile media. One of the two\ndomain estimation methods relies on expertise-type constraints. We show that\nthese constraints, initially chosen to address the spent fuel rod example, are\nrobust in that they also lead to good results in the second analytic\noptimization example. Of course, in other applications, it could be necessary\nto design alternative constraints that are more suitable for these\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:44:49 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 13:45:50 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "IMT"], ["Barthe", "Th\u00e9o", "", "OSU"], ["Santner", "Thomas", "", "OSU"], ["Richet", "Yann", "", "IRSN"]]}, {"id": "2004.03864", "submitter": "Tommaso Di Fonzo", "authors": "Luisa Bisaglia, Tommaso Di Fonzo, Daniele Girolimetto", "title": "Fully reconciled GDP forecasts from Income and Expenditure sides", "comments": "7 pages, 3 figures. Accepted for presentation at the Italian\n  Statistical Society Meeting - SIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a complete reconciliation procedure, resulting in a 'one number\nforecast' of the GDP figure, coherent with both Income and Expenditure sides'\nforecasted series, and evaluate its performance on the Australian quarterly GDP\nseries, as compared to the original proposal by Athanasopoulos et al. (2019).\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 08:03:47 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 09:44:08 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Bisaglia", "Luisa", ""], ["Di Fonzo", "Tommaso", ""], ["Girolimetto", "Daniele", ""]]}, {"id": "2004.04078", "submitter": "Simone Padoan PhD", "authors": "Simone A. Padoan and Gilles Stupfler", "title": "Extreme expectile estimation for heavy-tailed time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectiles are a least squares analogue of quantiles which have lately\nreceived substantial attention in actuarial and financial risk management\ncontexts. Unlike quantiles, expectiles define coherent risk measures and are\ndetermined by tail expectations rather than tail probabilities; unlike the\nExpected Shortfall, they define elicitable risk measures. This has motivated\nrecent studies of the behaviour and estimation of extreme expectile-based risk\nmeasures. The case of stationary but weakly dependent observations has,\nhowever, been left largely untouched, even though correctly accounting for the\nuncertainty present in typical financial applications requires the\nconsideration of dependent data. We investigate the estimation of, and\nconstruction of accurate confidence intervals for, extreme expectiles and\nexpectile-based Marginal Expected Shortfall in a general $\\beta-$mixing\ncontext, containing the classes of ARMA, ARCH and GARCH models with\nheavy-tailed innovations that are of interest in financial applications. The\nmethods are showcased in a numerical simulation study and on real financial\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:00:36 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 14:00:34 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Padoan", "Simone A.", ""], ["Stupfler", "Gilles", ""]]}, {"id": "2004.04150", "submitter": "Shiqing Yu", "authors": "Shiqing Yu, Mathias Drton, Ali Shojaie", "title": "Directed Graphical Models and Causal Discovery for Zero-Inflated Data", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern RNA sequencing technologies provide gene expression measurements from\nsingle cells that promise refined insights on regulatory relationships among\ngenes. Directed graphical models are well-suited to explore such (cause-effect)\nrelationships. However, statistical analyses of single cell data are\ncomplicated by the fact that the data often show zero-inflated expression\npatterns. To address this challenge, we propose directed graphical models that\nare based on Hurdle conditional distributions parametrized in terms of\npolynomials in parent variables and their 0/1 indicators of being zero or\nnonzero. While directed graphs for Gaussian models are only identifiable up to\nan equivalence class in general, we show that, under a natural and weak\nassumption, the exact directed acyclic graph of our zero-inflated models can be\nidentified. We propose methods for graph recovery, apply our model to real\nsingle-cell RNA-seq data on T helper cells, and show simulated experiments that\nvalidate the identifiability and graph estimation methods in practice.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:59:12 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Yu", "Shiqing", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "2004.04154", "submitter": "Jin Liu", "authors": "Jin Liu, Robert A. Perera", "title": "Estimating Knots and Their Association in Parallel Bilinear Spline\n  Growth Curve Models in the Framework of Individual Measurement Occasions", "comments": "\\c{opyright} 2020, American Psychological Association. This paper is\n  not the copy of record and may not exactly replicate the final, authoritative\n  version of the article. Please do not copy or cite without authors'\n  permission. The final article will be available, upon publication, via its\n  DOI: 10.1037/met0000309", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent growth curve models with spline functions are flexible and accessible\nstatistical tools for investigating nonlinear change patterns that exhibit\ndistinct phases of development in manifested variables. Among such models, the\nbilinear spline growth model (BLSGM) is the most straightforward and intuitive\nbut useful. An existing study has demonstrated that the BLSGM allows the knot\n(or change-point), at which two linear segments join together, to be an\nadditional growth factor other than the intercept and slopes so that\nresearchers can estimate the knot and its variability in the framework of\nindividual measurement occasions. However, developmental processes usually\nunfold in a joint development where two or more outcomes and their change\npatterns are correlated over time. As an extension of the existing BLSGM with\nan unknown knot, this study considers a parallel BLSGM (PBLSGM) for\ninvestigating multiple nonlinear growth processes and estimating the knot with\nits variability of each process as well as the knot-knot association in the\nframework of individual measurement occasions. We present the proposed model by\nsimulation studies and a real-world data analysis. Our simulation studies\ndemonstrate that the proposed PBLSGM generally estimate the parameters of\ninterest unbiasedly, precisely and exhibit appropriate confidence interval\ncoverage. An empirical example using longitudinal reading scores, mathematics\nscores, and science scores shows that the model can estimate the knot with its\nvariance for each growth curve and the covariance between two knots. We also\nprovide the corresponding code for the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 21:19:07 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 20:19:42 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2020 22:59:53 GMT"}, {"version": "v4", "created": "Thu, 30 Apr 2020 02:15:09 GMT"}, {"version": "v5", "created": "Fri, 1 May 2020 02:39:36 GMT"}, {"version": "v6", "created": "Sun, 19 Jul 2020 00:14:59 GMT"}, {"version": "v7", "created": "Sat, 10 Oct 2020 23:44:06 GMT"}, {"version": "v8", "created": "Thu, 29 Oct 2020 14:26:42 GMT"}, {"version": "v9", "created": "Tue, 8 Dec 2020 00:21:09 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liu", "Jin", ""], ["Perera", "Robert A.", ""]]}, {"id": "2004.04251", "submitter": "Noah Haber", "authors": "Noah A Haber, Mollie E Wood, Sarah Wieten, Alexander Breskin", "title": "DAG With Omitted Objects Displayed (DAGWOOD): A framework for revealing\n  causal assumptions in DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs (DAGs) are frequently used in epidemiology as a guide\nto encode causal inference assumptions. We propose the DAGWOOD framework to\nbring many of those encoded assumptions to the forefront.\n  DAGWOOD combines a root DAG (the DAG in the proposed analysis) and a set of\nbranch DAGs (alternative hidden assumptions to the root DAG). All branch DAGs\nshare a common ruleset, and must 1) change the root DAG, 2) be a valid DAG, and\neither 3a) change the minimally sufficient adjustment set or 3b) change the\nnumber of frontdoor paths. Branch DAGs comprise a list of assumptions which\nmust be justified as negligible. We define two types of branch DAGs: exclusion\nbranch DAGs add a single- or bidirectional pathway between two nodes in the\nroot DAG (e.g. direct pathways and colliders), while misdirection branch DAGs\nrepresent alternative pathways that could be drawn between objects (e.g.,\ncreating a collider by reversing the direction of causation for a controlled\nconfounder).\n  The DAGWOOD framework 1) organizes causal model assumptions, 2) reinforces\nbest DAG practices, 3) provides a framework for evaluation of causal models,\nand 4) can be used for generating causal models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:59:42 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 23:27:51 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 02:14:21 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 22:30:25 GMT"}, {"version": "v5", "created": "Fri, 2 Oct 2020 16:50:27 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Haber", "Noah A", ""], ["Wood", "Mollie E", ""], ["Wieten", "Sarah", ""], ["Breskin", "Alexander", ""]]}, {"id": "2004.04402", "submitter": "Quentin Duchemin", "authors": "Quentin Duchemin (LAMA)", "title": "Inference in the Stochastic Block Model with a Markovian assignment of\n  the communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the community detection problem in the Stochastic Block Model (SBM)\nwhen the communities of the nodes of the graph are assigned with a Markovian\ndynamic. To recover the partition of the nodes, we adapt the relaxed K-means\nSDP program presented in [11]. We identify the relevant signal-to-noise ratio\n(SNR) in our framework and we prove that the misclassification error decays\nexponentially fast with respect to this SNR. We provide infinity norm\nconsistent estimation of the parameters of our model and we discuss our results\nthrough the prism of classical degree regimes of the SBMs' literature. MSC 2010\nsubject classifications: Primary 68Q32; secondary 68R10, 90C35.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 07:58:02 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Duchemin", "Quentin", "", "LAMA"]]}, {"id": "2004.04480", "submitter": "Bruno Sudret", "authors": "S. Marelli, P.-R. Wagner, C. Lataniotis and B. Sudret", "title": "Stochastic spectral embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2020-003", "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing approximations that can accurately mimic the behavior of complex\nmodels at reduced computational costs is an important aspect of uncertainty\nquantification. Despite their flexibility and efficiency, classical surrogate\nmodels such as Kriging or polynomial chaos expansions tend to struggle with\nhighly non-linear, localized or non-stationary computational models. We hereby\npropose a novel sequential adaptive surrogate modeling method based on\nrecursively embedding locally spectral expansions. It is achieved by means of\ndisjoint recursive partitioning of the input domain, which consists in\nsequentially splitting the latter into smaller subdomains, and constructing a\nsimpler local spectral expansions in each, exploiting the trade-off complexity\nvs. locality. The resulting expansion, which we refer to as \"stochastic\nspectral embedding\" (SSE), is a piece-wise continuous approximation of the\nmodel response that shows promising approximation capabilities, and good\nscaling with both the problem dimension and the size of the training set. We\nfinally show how the method compares favorably against state-of-the-art sparse\npolynomial chaos expansions on a set of models with different complexity and\ninput dimension.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:00:07 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:02:44 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Marelli", "S.", ""], ["Wagner", "P. -R.", ""], ["Lataniotis", "C.", ""], ["Sudret", "B.", ""]]}, {"id": "2004.04512", "submitter": "Fernando Morales", "authors": "Fernando A. Morales, Jorge M. Ram\\'irez, and Edgar A. Ramos", "title": "A Mathematical Assessment of the Isolation Tree Method for Outliers\n  Detection in Big Data", "comments": "21 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the mathematical analysis of the Isolation Random Forest\nMethod (IRF Method) for anomaly detection is presented. We show that the IRF\nspace can be endowed with a probability induced by the Isolation Tree algorithm\n(iTree). In this setting, the convergence of the IRF method is proved using the\nLaw of Large Numbers. A couple of counterexamples are presented to show that\nthe original method is inconclusive and no quality certificate can be given,\nwhen using it as a means to detect anomalies. Hence, an alternative version of\nIRF is proposed, whose mathematical foundation, as well as its limitations, are\nfully justified. Finally, numerical experiments are presented to compare the\nperformance of the classic IRF with the proposed one.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 12:17:21 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 21:50:44 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 14:15:09 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2020 21:47:52 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2021 11:03:02 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Morales", "Fernando A.", ""], ["Ram\u00edrez", "Jorge M.", ""], ["Ramos", "Edgar A.", ""]]}, {"id": "2004.04558", "submitter": "Umberto Picchini", "authors": "Umberto Picchini, Umberto Simola, Jukka Corander", "title": "Sequentially guided MCMC proposals for synthetic likelihoods and\n  correlated synthetic likelihoods", "comments": "37 pages; 17 figures. Major revision: ASL only used to provide an\n  initialization to other adaptive MCMC proposals. New case study with\n  perturbed alpha-stable model", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Synthetic likelihood (SL) is a strategy for parameter inference when the\nlikelihood function is analytically or computationally intractable. In SL, the\nlikelihood function of the data is replaced by a multivariate Gaussian density\nover summary statistics of the data. SL requires simulation of many replicate\ndatasets at every parameter value considered by a sampling algorithm, such as\nMCMC, making the method computationally-intensive. We propose two strategies to\nalleviate the computational burden imposed by SL algorithms. We first introduce\na novel MCMC algorithm for SL where the proposal distribution is sequentially\ntuned and is also made conditional to data, thus it rapidly \"guides\" the\nproposed parameters towards high posterior probability regions. Second, we\nexploit strategies borrowed from the correlated pseudo-marginal MCMC\nliterature, to improve the chains mixing in a SL framework. Our methods enable\ninference for challenging case studies when the chain is initialised in low\nposterior probability regions of the parameter space, where standard samplers\nfailed. Our guided sampler can also be potentially used with MCMC samplers for\napproximate Bayesian computation (ABC). Our goal is to provide ways to make the\nbest out of each expensive MCMC iteration, which will broaden the scope of\nlikelihood-free inference for models with costly simulators. To illustrate the\nadvantages stemming from our framework we consider four benchmark examples,\nincluding estimation of parameters for a cosmological model and a stochastic\nmodel with highly non-Gaussian summary statistics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 14:32:12 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 19:28:33 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 10:04:31 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Picchini", "Umberto", ""], ["Simola", "Umberto", ""], ["Corander", "Jukka", ""]]}, {"id": "2004.04724", "submitter": "Anne Van Delft Dr.", "authors": "Anne van Delft and Holger Dette", "title": "Pivotal tests for relevant differences in the second order dynamics of\n  functional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need to statistically quantify differences between modern\n(complex) data-sets which commonly result as high-resolution measurements of\nstochastic processes varying over a continuum, we propose novel testing\nprocedures to detect relevant differences between the second order dynamics of\ntwo functional time series. In order to take the between-function dynamics into\naccount that characterize this type of functional data, a frequency domain\napproach is taken. Test statistics are developed to compare differences in the\nspectral density operators and in the primary modes of variation as encoded in\nthe associated eigenelements. Under mild moment conditions, we show convergence\nof the underlying statistics to Brownian motions and construct pivotal test\nstatistics. The latter is essential because the nuisance parameters can be\nunwieldy and their robust estimation infeasible, especially if the two\nfunctional time series are dependent. Besides from these novel features, the\nproperties of the tests are robust to any choice of frequency band enabling\nalso to compare energy contents at a single frequency. The finite sample\nperformance of the tests are verified through a simulation study and are\nillustrated with an application to fMRI data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:56:54 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 22:49:28 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["van Delft", "Anne", ""], ["Dette", "Holger", ""]]}, {"id": "2004.04837", "submitter": "Yaakov Malinovsky", "authors": "Gregory Haber, Yaakov Malinovsky, Paul S. Albert", "title": "Is Group Testing Ready for Prime-time in Disease Identification?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale disease screening is a complicated process in which high costs\nmust be balanced against pressing public health needs. When the goal is\nscreening for infectious disease, one approach is group testing in which\nsamples are initially tested in pools and individual samples are retested only\nif the initial pooled test was positive. Intuitively, if the prevalence of\ninfection is small, this could result in a large reduction of the total number\nof tests required. Despite this, the use of group testing in medical studies\nhas been limited, largely due to skepticism about the impact of pooling on the\naccuracy of a given assay. While there is a large body of research addressing\nthe issue of testing errors in group testing studies, it is customary to assume\nthat the misclassification parameters are known from an external population\nand/or that the values do not change with the group size. Both of these\nassumptions are highly questionable for many medical practitioners considering\ngroup testing in their study design. In this article, we explore how the\nfailure of these assumptions might impact the efficacy of a group testing\ndesign and, consequently, whether group testing is currently feasible for\nmedical screening. Specifically, we look at how incorrect assumptions about the\nsensitivity function at the design stage can lead to poor estimation of a\nprocedure's overall sensitivity and expected number of tests. Furthermore, if a\nvalidation study is used to estimate the pooled misclassification parameters of\na given assay, we show that the sample sizes required are so large as to be\nprohibitive in all but the largest screening programs\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 22:33:42 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 10:29:51 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 22:25:18 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Haber", "Gregory", ""], ["Malinovsky", "Yaakov", ""], ["Albert", "Paul S.", ""]]}, {"id": "2004.04856", "submitter": "Rong Ma", "authors": "Rong Ma and Ian Barnett", "title": "The Asymptotic Distribution of Modularity in Weighted Signed Networks", "comments": null, "journal-ref": "Biometrika (2020)", "doi": "10.1093/biomet/asaa059", "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity is a popular metric for quantifying the degree of community\nstructure within a network. The distribution of the largest eigenvalue of a\nnetwork's edge weight or adjacency matrix is well studied and is frequently\nused as a substitute for modularity when performing statistical inference.\nHowever, we show that the largest eigenvalue and modularity are asymptotically\nuncorrelated, which suggests the need for inference directly on modularity\nitself when the network size is large. To this end, we derive the asymptotic\ndistributions of modularity in the case where the network's edge weight matrix\nbelongs to the Gaussian Orthogonal Ensemble, and study the statistical power of\nthe corresponding test for community structure under some alternative model. We\nempirically explore universality extensions of the limiting distribution and\ndemonstrate the accuracy of these asymptotic distributions through type I error\nsimulations. We also compare the empirical powers of the modularity based tests\nwith some existing methods. Our method is then used to test for the presence of\ncommunity structure in two real data applications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 23:37:12 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ma", "Rong", ""], ["Barnett", "Ian", ""]]}, {"id": "2004.04872", "submitter": "Razieh Nabi", "authors": "Razieh Nabi, Rohit Bhattacharya, Ilya Shpitser", "title": "Full Law Identification In Graphical Models Of Missing Data:\n  Completeness Results", "comments": "Camera ready version published at ICML 2020", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning, PMLR 119, 2020", "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data has the potential to affect analyses conducted in all fields of\nscientific study, including healthcare, economics, and the social sciences.\nSeveral approaches to unbiased inference in the presence of non-ignorable\nmissingness rely on the specification of the target distribution and its\nmissingness process as a probability distribution that factorizes with respect\nto a directed acyclic graph. In this paper, we address the longstanding\nquestion of the characterization of models that are identifiable within this\nclass of missing data distributions. We provide the first completeness result\nin this field of study -- necessary and sufficient graphical conditions under\nwhich, the full data distribution can be recovered from the observed data\ndistribution. We then simultaneously address issues that may arise due to the\npresence of both missing data and unmeasured confounding, by extending these\ngraphical conditions and proofs of completeness, to settings where some\nvariables are not just missing, but completely unobserved.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 01:31:10 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 02:33:55 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 14:28:45 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Nabi", "Razieh", ""], ["Bhattacharya", "Rohit", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2004.05023", "submitter": "William Aeberhard", "authors": "William H. Aeberhard and Eva Cantoni and Chris Field and Hans R.\n  Kuensch and Joanna Mills Flemming and Ximing Xu", "title": "Robust Estimation for Discrete-Time State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space models (SSMs) are now ubiquitous in many fields and increasingly\ncomplicated with observed and unobserved variables often interacting in\nnon-linear fashions. The crucial task of validating model assumptions thus\nbecomes difficult, particularly since some assumptions are formulated about\nunobserved states and thus cannot be checked with data. Motivated by the\ncomplex SSMs used for the assessment of fish stocks, we introduce a robust\nestimation method for SSMs. We prove the Fisher consistency of our estimator\nand propose an implementation based on automatic differentiation and the\nLaplace approximation of integrals which yields fast computations. Simulation\nstudies demonstrate that our robust procedure performs well both with and\nwithout deviations from model assumptions. Applying it to the stock assessment\nmodel for pollock in the North Sea highlights the ability of our procedure to\nidentify years with atypical observations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 13:12:44 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Aeberhard", "William H.", ""], ["Cantoni", "Eva", ""], ["Field", "Chris", ""], ["Kuensch", "Hans R.", ""], ["Flemming", "Joanna Mills", ""], ["Xu", "Ximing", ""]]}, {"id": "2004.05027", "submitter": "Marco Mariani", "authors": "Giulio Grossi, Patrizia Lattarulo, Marco Mariani, Alessandra Mattei,\n  \\\"Ozge \\\"Oner", "title": "Synthetic Control Group Methods in the Presence of Interference: The\n  Direct and Spillover Effects of Light Rail on Neighborhood Retail Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Synthetic Control Group (SCG) methods have received great\nattention from scholars and have been subject to extensions and comparisons\nwith alternative approaches for program evaluation. However, the existing\nmethodological literature mainly relies on the assumption of non-interference.\nWe investigate the use of the SCG method in panel comparative case studies\nwhere interference between the treated and the untreated units is plausible. We\nframe our discussion in the potential outcomes approach. Under a partial\ninterference assumption, we formally define relevant direct and spillover\neffects. We also consider the \"unrealized\" spillover effect on the treated unit\nin the hypothetical scenario that another unit in the treated unit's\nneighborhood had been assigned to the intervention. Then we investigate the\nassumptions under which we can identify and estimate the causal effects of\ninterest, and show how they can be estimated using the SCG method. We apply our\napproach to the analysis of an observational study, where the focus is on\nassessing direct and spillover causal effects of a new light rail line recently\nbuilt in Florence (Italy) on the retail density of the street where it was\nbuilt and of the streets in the treated street's neighborhood.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 13:24:15 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 08:52:52 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 10:30:56 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Grossi", "Giulio", ""], ["Lattarulo", "Patrizia", ""], ["Mariani", "Marco", ""], ["Mattei", "Alessandra", ""], ["\u00d6ner", "\u00d6zge", ""]]}, {"id": "2004.05102", "submitter": "Toshihiro Hirano", "authors": "Toshihiro Hirano", "title": "A multi-resolution approximation via linear projection for large spatial\n  datasets", "comments": "44 pages, 3 figure, 7 tables", "journal-ref": "Japanese Journal of Statistics and Data Science (2021), Vol. 4,\n  215-256", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technical advances in collecting spatial data have been increasing the\ndemand for methods to analyze large spatial datasets. The statistical analysis\nfor these types of datasets can provide useful knowledge in various fields.\nHowever, conventional spatial statistical methods, such as maximum likelihood\nestimation and kriging, are impractically time-consuming for large spatial\ndatasets due to the necessary matrix inversions. To cope with this problem, we\npropose a multi-resolution approximation via linear projection ($M$-RA-lp). The\n$M$-RA-lp conducts a linear projection approach on each subregion whenever a\nspatial domain is subdivided, which leads to an approximated covariance\nfunction capturing both the large- and small-scale spatial variations.\nMoreover, we elicit the algorithms for fast computation of the log-likelihood\nfunction and predictive distribution with the approximated covariance function\nobtained by the $M$-RA-lp. Simulation studies and a real data analysis for air\ndose rates demonstrate that our proposed $M$-RA-lp works well relative to the\nrelated existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:40:14 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 17:53:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hirano", "Toshihiro", ""]]}, {"id": "2004.05105", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis and Ioannis Ntzoufras", "title": "On the identifiability of Bayesian factor analytic models", "comments": "36 pages 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well known identifiability issue in factor analytic models is the\ninvariance with respect to orthogonal transformations. This problem burdens the\ninference under a Bayesian setup, where Markov chain Monte Carlo (MCMC) methods\nare used to generate samples from the posterior distribution. We introduce a\npost-processing scheme in order to deal with rotation, sign and permutation\ninvariance of the MCMC sample. The exact version of the contributed algorithm\nrequires to solve $2^q$ assignment problems per (retained) MCMC iteration,\nwhere $q$ denotes the number of factors of the fitted model. For large numbers\nof factors two approximate schemes based on simulated annealing are also\ndiscussed. We demonstrate that the proposed method leads to interpretable\nposterior distributions using synthetic and publicly available data from\ntypical factor analytic models as well as mixtures of factor analyzers. An R\npackage is available online at CRAN web-page.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:43:15 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Papastamoulis", "Panagiotis", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "2004.05192", "submitter": "Helena Ferreira", "authors": "Helena Ferreira, Marta Ferreira", "title": "Multivariate Medial Correlation with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a multivariate medial correlation coefficient that extends the\nprobabilistic interpretation and properties of Blomqvist's $\\beta$ coefficient,\nincorporates multivariate marginal dependencies and it preserves a stronger\nmultivariate concordance relation. We determine the maximum and minimum values\nattainable and illustrate the results in some models. We end with an\napplication on real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 18:16:34 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 11:43:39 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 16:37:58 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ferreira", "Helena", ""], ["Ferreira", "Marta", ""]]}, {"id": "2004.05281", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Weining Shen, Dehan Kong", "title": "Covariance Estimation for Matrix-valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance estimation for matrix-valued data has received an increasing\ninterest in applications including neuroscience and environmental studies.\nUnlike previous works that rely heavily on matrix normal distribution\nassumption and the requirement of fixed matrix size, we propose a class of\ndistribution-free regularized covariance estimation methods for\nhigh-dimensional matrix data under a separability condition and a bandable\ncovariance structure. Under these conditions, the original covariance matrix is\ndecomposed into a Kronecker product of two bandable small covariance matrices\nrepresenting the variability over row and column directions. We formulate a\nunified framework for estimating the banded and tapering covariance, and\nintroduce an efficient algorithm based on rank one unconstrained Kronecker\nproduct approximation. The convergence rates of the proposed estimators are\nstudied and compared to the ones for the usual vector-valued data. We further\nintroduce a class of robust covariance estimators and provide theoretical\nguarantees to deal with the potential heavy-tailed data. We demonstrate the\nsuperior finite-sample performance of our methods using simulations and real\napplications from an electroencephalography study and a gridded temperature\nanomalies dataset.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 02:15:26 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Yichi", ""], ["Shen", "Weining", ""], ["Kong", "Dehan", ""]]}, {"id": "2004.05374", "submitter": "Simone Riggi", "authors": "S. Riggi, D. Riggi and F. Riggi", "title": "Handling missing data in a neural network approach for the\n  identification of charged particles in a multilayer detector", "comments": "14 pages, 8 figures", "journal-ref": "Nucl. Instr. and Methods in Physics Research A 780 (2015) 81-90", "doi": "10.1016/j.nima.2015.01.063", "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an physics.ins-det stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of charged particles in a multilayer detector by the energy\nloss technique may also be achieved by the use of a neural network. The\nperformance of the network becomes worse when a large fraction of information\nis missing, for instance due to detector inefficiencies. Algorithms which\nprovide a way to impute missing information have been developed over the past\nyears. Among the various approaches, we focused on normal mixtures models in\ncomparison with standard mean imputation and multiple imputation methods.\nFurther, to account for the intrinsic asymmetry of the energy loss data, we\nconsidered skew-normal mixture models and provided a closed form implementation\nin the Expectation-Maximization (EM) algorithm framework to handle missing\npatterns. The method has been applied to a test case where the energy losses of\npions, kaons and protons in a six-layers Silicon detector are considered as\ninput neurons to a neural network. Results are given in terms of reconstruction\nefficiency and purity of the various species in different momentum bins.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 11:43:21 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Riggi", "S.", ""], ["Riggi", "D.", ""], ["Riggi", "F.", ""]]}, {"id": "2004.05387", "submitter": "Karl Rohe", "authors": "Karl Rohe and Muzhe Zeng", "title": "Vintage Factor Analysis with Varimax Performs Statistical Inference", "comments": "All comments welcome <3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychologists developed Multiple Factor Analysis to decompose multivariate\ndata into a small number of interpretable factors without any a priori\nknowledge about those factors. In this form of factor analysis, the Varimax\n\"factor rotation\" is a key step to make the factors interpretable. Charles\nSpearman and many others objected to factor rotations because the factors seem\nto be rotationally invariant. These objections are still reported in all\ncontemporary multivariate statistics textbooks. This is an engima because this\nvintage form of factor analysis has survived and is widely popular because,\nempirically, the factor rotation often makes the factors easier to interpret.\nWe argue that the rotation makes the factors easier to interpret because, in\nfact, the Varimax factor rotation performs statistical inference. We show that\nPrincipal Components Analysis (PCA) with the Varimax rotation provides a\nunified spectral estimation strategy for a broad class of modern factor models,\nincluding the Stochastic Blockmodel and a natural variation of Latent Dirichlet\nAllocation (i.e., \"topic modeling\"). In addition, we show that Thurstone's\nwidely employed sparsity diagnostics implicitly assess a key \"leptokurtic\"\ncondition that makes the rotation statistically identifiable in these models.\nTaken together, this shows that the know-how of Vintage Factor Analysis\nperforms statistical inference, reversing nearly a century of statistical\nthinking on the topic. With a sparse eigensolver, PCA with Varimax is both fast\nand stable. Combined with Thurstone's straightforward diagnostics, this vintage\napproach is suitable for a wide array of modern applications.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 12:45:06 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 12:24:06 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Rohe", "Karl", ""], ["Zeng", "Muzhe", ""]]}, {"id": "2004.05426", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues", "title": "Scaling Bayesian inference of mixed multinomial logit models to very\n  large datasets", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference methods have been shown to lead to significant\nimprovements in the computational efficiency of approximate Bayesian inference\nin mixed multinomial logit models when compared to standard Markov-chain Monte\nCarlo (MCMC) methods without compromising accuracy. However, despite their\ndemonstrated efficiency gains, existing methods still suffer from important\nlimitations that prevent them to scale to very large datasets, while providing\nthe flexibility to allow for rich prior distributions and to capture complex\nposterior distributions. In this paper, we propose an Amortized Variational\nInference approach that leverages stochastic backpropagation, automatic\ndifferentiation and GPU-accelerated computation, for effectively scaling\nBayesian inference in Mixed Multinomial Logit models to very large datasets.\nMoreover, we show how normalizing flows can be used to increase the flexibility\nof the variational posterior approximations. Through an extensive simulation\nstudy, we empirically show that the proposed approach is able to achieve\ncomputational speedups of multiple orders of magnitude over traditional MSLE\nand MCMC approaches for large datasets without compromising estimation\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 15:30:47 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Rodrigues", "Filipe", ""]]}, {"id": "2004.05470", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Maria Jaenada, Leandro Pardo", "title": "Robust adaptive variable selection in ultra-high dimensional linear\n  regression models", "comments": "Pre-print, Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simultaneous variable selection and estimation of\nthe corresponding regression coefficients in an ultra-high dimensional linear\nregression models, an extremely important problem in the recent era. The\nadaptive penalty functions are used in this regard to achieve the oracle\nvariable selection property along with easier computational burden. However,\nthe usual adaptive procedures (e.g., adaptive LASSO) based on the squared error\nloss function is extremely non-robust in the presence of data contamination\nwhich are quite common with large-scale data (e.g., noisy gene expression data,\nspectra and spectral data). In this paper, we present a regularization\nprocedure for the ultra-high dimensional data using a robust loss function\nbased on the popular density power divergence (DPD) measure along with the\nadaptive LASSO penalty. We theoretically study the robustness and the\nlarge-sample properties of the proposed adaptive robust estimators for a\ngeneral class of error distributions; in particular, we show that the proposed\nadaptive DPD-LASSO estimator is highly robust, satisfies the oracle variable\nselection property, and the corresponding estimators of the regression\ncoefficients are consistent and asymptotically normal under easily verifiable\nset of assumptions. Numerical illustrations are provided for the mostly used\nnormal error density. Finally, the proposal is applied to analyze an\ninteresting spectral dataset, in the field of chemometrics, regarding the\nelectron-probe X-ray microanalysis (EPXMA) of archaeological glass vessels from\nthe 16th and 17th centuries.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 19:39:42 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 09:06:24 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Ghosh", "Abhik", ""], ["Jaenada", "Maria", ""], ["Pardo", "Leandro", ""]]}, {"id": "2004.05487", "submitter": "Wei Jin", "authors": "Wei Jin, Yang Ni, Leah H. Rubin, Amanda B. Spence, Yanxun Xu", "title": "A Bayesian Nonparametric Approach for Inferring Drug Combination Effects\n  on Mental Health in People with HIV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although combination antiretroviral therapy (ART) is highly effective in\nsuppressing viral load for people with HIV (PWH), many ART agents may\nexacerbate central nervous system (CNS)-related adverse effects including\ndepression. Therefore, understanding the effects of ART drugs on the CNS\nfunction, especially mental health, can help clinicians personalize medicine\nwith less adverse effects for PWH and prevent them from discontinuing their ART\nto avoid undesirable health outcomes and increased likelihood of HIV\ntransmission. The emergence of electronic health records offers researchers\nunprecedented access to HIV data including individuals' mental health records,\ndrug prescriptions, and clinical information over time. However, modeling such\ndata is very challenging due to high-dimensionality of the drug combination\nspace, the individual heterogeneity, and sparseness of the observed drug\ncombinations. We develop a Bayesian nonparametric approach to learn drug\ncombination effect on mental health in PWH adjusting for socio-demographic,\nbehavioral, and clinical factors. The proposed method is built upon the\nsubset-tree kernel method that represents drug combinations in a way that\nsynthesizes known regimen structure into a single mathematical representation.\nIt also utilizes a distance-dependent Chinese restaurant process to cluster\nheterogeneous population while taking into account individuals' treatment\nhistories. We evaluate the proposed approach through simulation studies, and\napply the method to a dataset from the Women's Interagency HIV Study, yielding\ninterpretable and promising results. Our method has clinical utility in guiding\nclinicians to prescribe more informed and effective personalized treatment\nbased on individuals' treatment histories and clinical characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 21:02:44 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jin", "Wei", ""], ["Ni", "Yang", ""], ["Rubin", "Leah H.", ""], ["Spence", "Amanda B.", ""], ["Xu", "Yanxun", ""]]}, {"id": "2004.05641", "submitter": "Jose R. Zubizarreta", "authors": "Juan D. Diaz, Jose R. Zubizarreta", "title": "Complex Discontinuity Designs Using Covariates for Policy Impact\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression discontinuity designs are extensively used for causal inference in\nobservational studies. However, they are usually confined to settings with\nsimple treatment rules, determined by a single running variable, with a single\ncutoff. Motivated by the problem of estimating the impact of grade retention on\neducational and juvenile crime outcomes, in this paper we propose a framework\nand methods for complex discontinuity designs that encompasses multiple\ntreatment rules. In this framework, the observed covariates play a central role\nfor identification, estimation, and generalization of causal effects.\nIdentification is non-parametric and relies on a local strong ignorability\nassumption. Estimation proceeds as in any observational study under strong\nignorability, yet in a neighborhood of the cutoffs of the running variables. We\ndiscuss estimation approaches based on matching and weighting, including\ncomplementary regression modeling adjustments. We present assumptions for\ngeneralization; that is, for identification and estimation of average treatment\neffects for target populations. We also describe two approaches to select the\nneighborhood for analysis. We find that grade retention has a negative impact\non future grade retention, but is not associated with dropping out of school or\ncommitting a juvenile crime.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 16:06:23 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 22:15:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Diaz", "Juan D.", ""], ["Zubizarreta", "Jose R.", ""]]}, {"id": "2004.05730", "submitter": "Hyokyoung G. Hong Dr.", "authors": "Hyokyoung G. Hong and Yi Li", "title": "Estimation of time-varying reproduction numbers underlying\n  epidemiological processes: a new statistical tool for the COVID-19 pandemic", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0236464", "report-no": null, "categories": "q-bio.PE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus pandemic has rapidly evolved into an unprecedented crisis.\nThe susceptible-infectious-removed (SIR) model and its variants have been used\nfor modeling the pandemic. However, time-independent parameters in the\nclassical models may not capture the dynamic transmission and removal\nprocesses, governed by virus containment strategies taken at various phases of\nthe epidemic. Moreover, very few models account for possible inaccuracies of\nthe reported cases. We propose a Poisson model with time-dependent transmission\nand removal rates to account for possible random errors in reporting and\nestimate a time-dependent disease reproduction number, which may be used to\nassess the effectiveness of virus control strategies. We apply our method to\nstudy the pandemic in several severely impacted countries, and analyze and\nforecast the evolving spread of the coronavirus. We have developed an\ninteractive web application to facilitate readers' use of our method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 00:14:15 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:22:36 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 03:24:55 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Hong", "Hyokyoung G.", ""], ["Li", "Yi", ""]]}, {"id": "2004.05964", "submitter": "Shusei Eshima", "authors": "Shusei Eshima, Kosuke Imai and Tomoya Sasaki", "title": "Keyword Assisted Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, fully automated content analysis based on probabilistic\ntopic models has become popular among social scientists because of their\nscalability. The unsupervised nature of the models makes them suitable for\nexploring topics in a corpus without prior knowledge. However, researchers find\nthat these models often fail to measure specific concepts of substantive\ninterest by inadvertently creating multiple topics with similar content and\ncombining distinct themes into a single topic. In this paper, we empirically\ndemonstrate that providing a small number of keywords can substantially enhance\nthe measurement performance of topic models. An important advantage of the\nproposed keyword assisted topic model (keyATM) is that the specification of\nkeywords requires researchers to label topics prior to fitting a model to the\ndata. This contrasts with a widespread practice of post-hoc topic\ninterpretation and adjustments that compromises the objectivity of empirical\nfindings. In our application, we find that keyATM provides more interpretable\nresults, has better document classification performance, and is less sensitive\nto the number of topics than the standard topic models. Finally, we show that\nkeyATM can also incorporate covariates and model time trends. An open-source\nsoftware package is available for implementing the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:35:28 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:24:52 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Eshima", "Shusei", ""], ["Imai", "Kosuke", ""], ["Sasaki", "Tomoya", ""]]}, {"id": "2004.06022", "submitter": "Jong-Hyeon Jeong Prof.", "authors": "Lauren C. Balmert and Ruosha Li and Limin Peng and Jong-Hyeon Jeong", "title": "Quantile regression on inactivity time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inactivity time, or lost lifespan specifically for mortality data,\nconcerns time from occurrence of an event of interest to the current time point\nand has recently emerged as a new summary measure for cumulative information\ninherent in time-to-event data. This summary measure provides several benefits\nover the traditional methods, including more straightforward interpretation yet\nless sensitivity to heavy censoring. However, there exists no systematic\nmodeling approach to inferring the quantile inactivity time in the literature.\nIn this paper, we propose a regression method for the quantiles of the\ninactivity time distribution under right censoring. The consistency and\nasymptotic normality of the regression parameters are established. To avoid\nestimation of the probability density function of the inactivity time\ndistribution under censoring, we propose a computationally efficient method for\nestimating the variance-covariance matrix of the regression coefficient\nestimates. Simulation results are presented to validate the finite sample\nproperties of the proposed estimators and test statistics. The proposed method\nis illustrated with a real dataset from a clinical trial on breast cancer.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:54:59 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Balmert", "Lauren C.", ""], ["Li", "Ruosha", ""], ["Peng", "Limin", ""], ["Jeong", "Jong-Hyeon", ""]]}, {"id": "2004.06054", "submitter": "Xin Gao", "authors": "Xin Gao, Li Li, Li Luo", "title": "Decomposition of Total Effect with the Notion of Natural Counterfactual\n  Interaction Effect", "comments": "72 pages in total, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis serves as a crucial tool to obtain causal inference based\non directed acyclic graphs, which has been widely employed in the areas of\nbiomedical science, social science, epidemiology and psychology. Decomposition\nof total effect provides a deep insight to fully understand the casual\ncontribution from each path and interaction term. Since the four-way\ndecomposition method was proposed to identify the mediated interaction effect\nin counterfactual framework, the idea had been extended to a more sophisticated\nscenario with non-sequential multiple mediators. However, the method exhibits\nlimitations as the causal structure contains direct causal edges between\nmediators, such as inappropriate modeling of dependence and\nnon-identifiability. We develop the notion of natural counterfactual\ninteraction effect and find that the decomposition of total effect can be\nconsistently realized with our proposed notion. Furthermore, natural\ncounterfactual interaction effect overcomes the drawbacks and possesses a clear\nand significant interpretation, which may largely improve the capacity of\nresearchers to analyze highly complex causal structures.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:29:58 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Gao", "Xin", ""], ["Li", "Li", ""], ["Luo", "Li", ""]]}, {"id": "2004.06070", "submitter": "Alexis Comber", "authors": "Alexis Comber, Chris Brunsdon, Martin Charlton, Guanpeng Dong, Rich\n  Harris, Binbin Lu, Yihe L\\\"u, Daisuke Murakami, Tomoki Nakaya, Yunqiang Wang,\n  Paul Harris", "title": "The GWR route map: a guide to the informed application of Geographically\n  Weighted Regression", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geographically Weighted Regression (GWR) is increasingly used in spatial\nanalyses of social and environmental data. It allows spatial heterogeneities in\nprocesses and relationships to be investigated through a series of local\nregression models rather than a global one. Standard GWR assumes that the\nrelationships between the response and predictor variables operate at the same\nspatial scale, which is frequently not the case. To address this, several GWR\nvariants have been proposed. This paper describes a route map to inform the\nchoice of whether to use a GWR model or not, and if so which of three core\nvariants to apply: a standard GWR, a mixed GWR or a multiscale GWR (MS-GWR).\nThe route map comprises primary steps: a basic linear regression, a MS-GWR, and\ninvestigations of the results of these. The paper provides guidance for\ndeciding whether to use a GWR approach, and if so for determining the\nappropriate GWR variant. It describes the importance of investigating a number\nof secondary issues at global and local scales including collinearity, the\ninfluence of outliers, and dependent error terms. Code and data for the case\nstudy used to illustrate the route map are provided, and further considerations\nare described in an extensive Appendix.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 17:09:57 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 07:44:43 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Comber", "Alexis", ""], ["Brunsdon", "Chris", ""], ["Charlton", "Martin", ""], ["Dong", "Guanpeng", ""], ["Harris", "Rich", ""], ["Lu", "Binbin", ""], ["L\u00fc", "Yihe", ""], ["Murakami", "Daisuke", ""], ["Nakaya", "Tomoki", ""], ["Wang", "Yunqiang", ""], ["Harris", "Paul", ""]]}, {"id": "2004.06092", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej", "title": "mFLICA: An R package for Inferring Leadership of Coordination From Time\n  Series", "comments": "The latest version of R package can be found at\n  https://github.com/DarkEyes/mFLICA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI econ.EM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leadership is a process that leaders influence followers to achieve\ncollective goals. One of special cases of leadership is the coordinated pattern\ninitiation. In this context, leaders are initiators who initiate coordinated\npatterns that everyone follows. Given a set of individual-multivariate time\nseries of real numbers, the mFLICA package provides a framework for R users to\ninfer coordination events within time series, initiators and followers of these\ncoordination events, as well as dynamics of group merging and splitting. The\nmFLICA package also has a visualization function to make results of leadership\ninference more understandable. The package is available on Comprehensive R\nArchive Network (CRAN) at https://CRAN.R-project.org/package=mFLICA.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:14:06 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""]]}, {"id": "2004.06139", "submitter": "Brady West PhD", "authors": "Brady T. West, Roderick J.A. Little, Rebecca R. Andridge, Philip S.\n  Boonstra, Erin B. Ware, Anita Pandit, Fernanda Alvarado-Leiton", "title": "Assessing Selection Bias in Regression Coefficients Estimated from\n  Non-Probability Samples, with Applications to Genetics and Demographic\n  Surveys", "comments": "29 pages, 4 figures, 2 tables, supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection bias is a serious potential problem for inference about\nrelationships of scientific interest based on samples without well-defined\nprobability sampling mechanisms. Motivated by the potential for selection bias\nin (a) estimated relationships of polygenic scores (PGSs) with phenotypes in\ngenetic studies of volunteers, and (b) estimated differences in subgroup means\nin surveys of smartphone users, we derive novel measures of selection bias for\nestimates of the coefficients in linear and probit regression models fitted to\nnon-probability samples, when aggregate-level auxiliary data are available for\nthe selected sample and the target population. The measures arise from normal\npattern-mixture models that allow analysts to examine the sensitivity of their\ninferences to assumptions about non-ignorable selection in these samples. We\nexamine the effectiveness of the proposed measures in a simulation study, and\nthen use them to quantify the selection bias in (a) estimated PGS-phenotype\nrelationships in a large study of volunteers recruited via Facebook, and (b)\nestimated subgroup differences in mean past-year employment duration in a\nnon-probability sample of low-educated smartphone users. We evaluate the\nperformance of the measures in these applications using benchmark estimates\nfrom large probability samples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:18:53 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 17:09:54 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 17:17:20 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 17:18:26 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["West", "Brady T.", ""], ["Little", "Roderick J. A.", ""], ["Andridge", "Rebecca R.", ""], ["Boonstra", "Philip S.", ""], ["Ware", "Erin B.", ""], ["Pandit", "Anita", ""], ["Alvarado-Leiton", "Fernanda", ""]]}, {"id": "2004.06150", "submitter": "Richard Minkah", "authors": "S. K-B. Dzidzornu and R. Minkah", "title": "Assessing the Performance of the Discrete Generalised Pareto\n  Distribution in Modelling Non-Life Insurance Claims", "comments": "14 pages, 1 figure,", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, non-life insurance claims were modelled under the three\nparameter discrete generalised Pareto distribution. Data from the National\nInsurance Commission of Ghana on reported and settled claims were considered\nfor the period 2012-2016. The maximum likelihood estimation principle was\nadopted in fitting the discrete Pareto distribution to the yearly and\naggregated data. The estimation involved two steps. Firstly, the $\\mu$ and\n$(\\mu+1)$ frequency method of \\citet{Prieto2014} was modified to suit the\ncharacteristics of the data under study. Secondly, a bootstrap algorithm was\nimplemented to obtain the standard errors of the estimators of the parameters\nof the discrete generalised Pareto distribution. The performance of the\ndiscrete generalised Pareto distribution is compared to the negative binomial\ndistribution in modelling the non-life insurance claims data using the\ninformation criteria of Akaike and Bayesian. The results show that the discrete\ngeneralised Pareto distribution provides a better fit to the non-life claims\ndata.\n  Keywords: Non-life insurance claims, discrete generalised Pareto\ndistribution, negative binomial distribution, maximum likelihood estimation,\ninformation criteria.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:44:04 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Dzidzornu", "S. K-B.", ""], ["Minkah", "R.", ""]]}, {"id": "2004.06156", "submitter": "Chengyuan Lu", "authors": "Chengyuan Lu and Jelle Goeman and Hein Putter", "title": "Maximum likelihood estimation in the additive hazards model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The additive hazards model specifies the effect of covariates on the hazard\nin an additive way, in contrast to the popular Cox model, in which it is\nmultiplicative. As non-parametric model, it offers a very flexible way of\nmodeling time-varying covariate effects. It is most commonly estimated by\nordinary least squares. In this paper we consider the case where covariates are\nbounded, and derive the maximum likelihood estimator under the constraint that\nthe hazard is non-negative for all covariate values in their domain. We\ndescribe an efficient algorithm to find the maximum likelihood estimator. The\nmethod is contrasted with the ordinary least squares approach in a simulation\nstudy, and the method is illustrated on a realistic data set.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:04:03 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 19:50:43 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lu", "Chengyuan", ""], ["Goeman", "Jelle", ""], ["Putter", "Hein", ""]]}, {"id": "2004.06166", "submitter": "Xiaoke Zhang", "authors": "Rui Miao, Wu Xue, Xiaoke Zhang", "title": "Average Treatment Effect Estimation in Observational Studies with\n  Functional Covariates", "comments": "Section 3.1.1: added discussions and Remark 1.3; Section 3.1.2: added\n  Eq. (5) and related discussions; Sections 5 and 6: added discussions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is an important area in modern statistics and has\nbeen successfully applied in many fields. Although many scientific studies aim\nto find causations, a predominant majority of functional data analysis\napproaches can only reveal correlations. In this paper, average treatment\neffect estimation is studied for observational data with functional covariates.\nThis paper generalizes various state-of-art propensity score estimation methods\nfor multivariate data to functional data. The resulting average treatment\neffect estimators via propensity score weighting are numerically evaluated by a\nsimulation study and applied to a real-world dataset to study the causal effect\nof duloxitine on the pain relief of chronic knee osteoarthritis patients.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:18:11 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 19:53:41 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Miao", "Rui", ""], ["Xue", "Wu", ""], ["Zhang", "Xiaoke", ""]]}, {"id": "2004.06191", "submitter": "Terrance Savitsky", "authors": "Terrance D. Savitsky and Matthew R. Williams", "title": "Pseudo Bayesian Estimation of One-way ANOVA Model in Complex Surveys", "comments": "46 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise survey-weighted pseudo posterior distribution estimators under\n2-stage informative sampling of both primary clusters and secondary nested\nunits for a one-way ANOVA population generating model as a simple canonical\ncase where population model random effects are defined to be coincident with\nthe primary clusters. We consider estimation on an observed informative sample\nunder both an augmented pseudo likelihood that co-samples random effects, as\nwell as an integrated likelihood that marginalizes out the random effects from\nthe survey-weighted augmented pseudo likelihood. This paper includes a\ntheoretical exposition that enumerates easily verified conditions for which\nestimation under the augmented pseudo posterior is guaranteed to be consistent\nat the true generating parameters. We reveal in simulation that both approaches\nproduce asymptotically unbiased estimation of the generating hyperparameters\nfor the random effects when a key condition on the sum of within cluster\nweighted residuals is met. We present a comparison with frequentist EM and a\nmethods that requires pairwise sampling weights.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 20:42:33 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 21:40:46 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 16:41:50 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Williams", "Matthew R.", ""]]}, {"id": "2004.06306", "submitter": "Lakshmi Narasimhan Theagarajan", "authors": "Lakshmi N. Theagarajan", "title": "Group Testing for COVID-19: How to Stop Worrying and Test More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The corona virus disease 2019 (COVID-19) caused by the novel corona virus has\nan exponential rate of infection. COVID-19 is particularly notorious as the\nonset of symptoms in infected patients are usually delayed and there exists a\nlarge number of asymptomatic carriers. In order to prevent overwhelming of\nmedical facilities and large fatality rate, early stage testing and diagnosis\nare key requirements. In this article, we discuss the methodologies from the\ngroup testing literature and its relevance to COVID-19 diagnosis. Specifically,\nwe investigate the efficiency of group testing using polymerase chain reaction\n(PCR) for COVID-19. Group testing is a method in which multiple samples are\npooled together in groups and fewer tests are performed on these groups to\ndiscern all the infected samples. We study the effect of dilution due to\npooling in group testing and show that group tests can perform well even in the\npresence of dilution effects. We present multiple group testing algorithms that\ncould reduce the number of tests performed for COVID-19 diagnosis. We analyze\nthe efficiency of these tests and provide insights on their practical\nrelevance. With the use of algorithms described here, test plans can be\ndeveloped that can enable testing centers to increase the number of diagnosis\nperformed without increasing the number of PCR tests. The codes for generating\ntest plans are available online at [1].\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 05:27:12 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 04:50:51 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Theagarajan", "Lakshmi N.", ""]]}, {"id": "2004.06328", "submitter": "Tin Lok James Ng", "authors": "Tin Lok James Ng, Kwok-Kun Kwong", "title": "Universal Approximation on the Hypersphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that any continuous probability density function on\n$\\mathbb{R}^m$ can be approximated arbitrarily well by a finite mixture of\nnormal distributions, provided that the number of mixture components is\nsufficiently large. The von-Mises-Fisher distribution, defined on the unit\nhypersphere $S^m$ in $\\mathbb{R}^{m+1}$, has properties that are analogous to\nthose of the multivariate normal on $\\mathbb{R}^{m+1}$. We prove that any\ncontinuous probability density function on $S^m$ can be approximated to\narbitrary degrees of accuracy by a finite mixture of von-Mises-Fisher\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 07:20:11 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ng", "Tin Lok James", ""], ["Kwong", "Kwok-Kun", ""]]}, {"id": "2004.06425", "submitter": "Gael Martin Prof", "authors": "Gael M. Martin (Monash University), David T. Frazier (Monash\n  University), and Christian P. Robert (Universit\\'e Paris Dauphine, University\n  of Warwick, and CREST)", "title": "Computing Bayes: Bayesian Computation from 1763 to the 21st Century", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Bayesian statistical paradigm uses the language of probability to express\nuncertainty about the phenomena that generate observed data. Probability\ndistributions thus characterize Bayesian analysis, with the rules of\nprobability used to transform prior probability distributions for all unknowns\n- parameters, latent variables, models - into posterior distributions,\nsubsequent to the observation of data. Conducting Bayesian analysis requires\nthe evaluation of integrals in which these probability distributions appear.\nBayesian computation is all about evaluating such integrals in the typical case\nwhere no analytical solution exists. This paper takes the reader on a\nchronological tour of Bayesian computation over the past two and a half\ncenturies. Beginning with the one-dimensional integral first confronted by\nBayes in 1763, through to recent problems in which the unknowns number in the\nmillions, we place all computational problems into a common framework, and\ndescribe all computational methods using a common notation. The aim is to help\nnew researchers in particular - and more generally those interested in adopting\na Bayesian approach to empirical work - make sense of the plethora of\ncomputational techniques that are now on offer; understand when and why\ndifferent methods are useful; and see the links that do exist, between them\nall.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 11:24:27 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 21:41:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Martin", "Gael M.", "", "Monash University"], ["Frazier", "David T.", "", "Monash\n  University"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris Dauphine, University\n  of Warwick, and CREST"]]}, {"id": "2004.06448", "submitter": "Huimin Peng", "authors": "Huimin Peng", "title": "Measurement Error in Nutritional Epidemiology: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews bias-correction models for measurement error of exposure\nvariables in the field of nutritional epidemiology. Measurement error usually\nattenuates estimated slope towards zero. Due to the influence of measurement\nerror, inference of parameter estimate is conservative and confidence interval\nof the slope parameter is too narrow. Bias-correction in estimators and\nconfidence intervals are of primary interest. We review the following\nbias-correction models: regression calibration methods, likelihood based\nmodels, missing data models, simulation based methods, nonparametric models and\nsampling based procedures.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 12:31:32 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 09:35:18 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Peng", "Huimin", ""]]}, {"id": "2004.06459", "submitter": "Gherardo Varando", "authors": "Federico Carli, Manuele Leonelli, Eva Riccomagno, Gherardo Varando", "title": "The R Package stagedtrees for Structural Learning of Stratified Staged\n  Trees", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  stagedtrees is an R package which includes several algorithms for learning\nthe structure of staged trees and chain event graphs from data. Score-based and\nclustering-based algorithms are implemented, as well as various functionalities\nto plot the models and perform inference. The capabilities of stagedtrees are\nillustrated using mainly two datasets both included in the package or bundled\nin R.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:02:59 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 17:11:54 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Carli", "Federico", ""], ["Leonelli", "Manuele", ""], ["Riccomagno", "Eva", ""], ["Varando", "Gherardo", ""]]}, {"id": "2004.06514", "submitter": "Alexandra Niessl", "authors": "Alexandra Niessl, Arthur Allignol, Carina M\u007fueller, Jan Beyersmann", "title": "Estimating state occupation and transition probabilities in non-Markov\n  multi-state models subject to both random left-truncation and right-censoring", "comments": "21 pages with 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Aalen-Johansen estimator generalizes the Kaplan-Meier estimator for\nindependently left-truncated and right-censored survival data to estimating the\ntransition probability matrix of a time-inhomogeneous Markov model with finite\nstate space. Such multi-state models have a wide range of applications for\nmodelling complex courses of a disease over the course of time, but the Markov\nassumption may often be in doubt. If censoring is entirely unrelated to the\nmulti-state data, it has been noted that the Aalen-Johansen estimator,\nstandardized by the initial empirical distribution of the multi-state model,\nstill consistently estimates the state occupation probabilities. Recently, this\nresult has been extended to transition probabilities using landmarking, which\nis, inter alia, useful for dynamic prediction. We complement these results in\nthree ways. Firstly, delayed study entry is a common phenomenon in\nobservational studies, and we extend the earlier results to multi-state data\nalso subject to left-truncation. Secondly, we present a rigorous proof of\nconsistency of the Aalen-Johansen estimator for state occupation probabilities,\non which also correctness of the landmarking approach hinges, correcting,\nsimplifying and extending the earlier result. Thirdly, our rigorous proof\nmotivates wild bootstrap resampling. Our developments for left-truncation are\nmotivated by a prospective observational study on the occurrence and the impact\nof a multi-resistant infectious organism in patients undergoing surgery. Both\nthe real data example and simulation studies are presented. Studying wild\nbootstrap is motivated by the fact that, unlike drawing with replacement from\nthe data, it is desirable to have a technique that works both with non-Markov\nmodels subject to random left-truncation and right-censoring and with Markov\nmodels where left-truncation and right-censoring need not be entirely random.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:55:00 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Niessl", "Alexandra", ""], ["Allignol", "Arthur", ""], ["M\u007fueller", "Carina", ""], ["Beyersmann", "Jan", ""]]}, {"id": "2004.06568", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Rita SahaRay, Sayan Chakrabarty, Sayan Bhadra", "title": "Robust Generalised Quadratic Discriminant Analysis", "comments": "Pre-print. Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic discriminant analysis (QDA) is a widely used statistical tool to\nclassify observations from different multivariate Normal populations. The\ngeneralized quadratic discriminant analysis (GQDA) classification\nrule/classifier, which generalizes the QDA and the minimum Mahalanobis distance\n(MMD) classifiers to discriminate between populations with underlying\nelliptically symmetric distributions competes quite favorably with the QDA\nclassifier when it is optimal and performs much better when QDA fails under\nnon-Normal underlying distributions, e.g. Cauchy distribution. However, the\nclassification rule in GQDA is based on the sample mean vector and the sample\ndispersion matrix of a training sample, which are extremely non-robust under\ndata contamination. In real world, since it is quite common to face data highly\nvulnerable to outliers, the lack of robustness of the classical estimators of\nthe mean vector and the dispersion matrix reduces the efficiency of the GQDA\nclassifier significantly, increasing the misclassification errors. The present\npaper investigates the performance of the GQDA classifier when the classical\nestimators of the mean vector and the dispersion matrix used therein are\nreplaced by various robust counterparts. Applications to various real data sets\nas well as simulation studies reveal far better performance of the proposed\nrobust versions of the GQDA classifier. A Comparative study has been made to\nadvocate the appropriate choice of the robust estimators to be used in a\nspecific situation of the degree of contamination of the data sets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 18:21:06 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ghosh", "Abhik", ""], ["SahaRay", "Rita", ""], ["Chakrabarty", "Sayan", ""], ["Bhadra", "Sayan", ""]]}, {"id": "2004.06615", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Dong Xia", "title": "Edgeworth expansions for network moments", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network method of moments arXiv:1202.5101 is an important tool for\nnonparametric network inference. However, there has been little investigation\non accurate descriptions of the sampling distributions of network moment\nstatistics. In this paper, we present the first higher-order accurate\napproximation to the sampling CDF of a studentized network moment by Edgeworth\nexpansion. In sharp contrast to classical literature on noiseless U-statistics,\nwe show that the Edgeworth expansion of a network moment statistic as a noisy\nU-statistic can achieve higher-order accuracy without non-lattice or smoothness\nassumptions but just requiring weak regularity conditions. Behind this result\nis our surprising discovery that the two typically-hated factors in network\nanalysis, namely, sparsity and edge-wise observational errors, jointly play a\nblessing role, contributing a crucial self-smoothing effect in the network\nmoment statistic and making it analytically tractable. Our assumptions match\nthe minimum requirements in related literature. For sparse networks, our theory\nshows a simple normal approximation achieves a gradually depreciating\nBerry-Esseen bound as the network becomes sparser. This result also refines the\nbest previous theoretical result.\n  For practitioners, our empirical Edgeworth expansion is highly accurate, fast\nand easy to implement. We demonstrate the clear advantage of our method by\ncomprehensive simulation studies.\n  We showcase three applications of our results in network inference. We prove,\nto our knowledge, the first theoretical guarantee of higher-order accuracy for\nsome network bootstrap schemes, and moreover, the first theoretical guidance\nfor selecting the sub-sample size for network sub-sampling. We also derive\none-sample test and Cornish-Fisher confidence interval for a given moment with\nhigher-order accurate controls of confidence level and type I error,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:02:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 05:08:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Yuan", ""], ["Xia", "Dong", ""]]}, {"id": "2004.06630", "submitter": "Chinchin Wang", "authors": "Chinchin Wang, Tyrel Stokes, Russell Steele, Niels Wedderkopp, Ian\n  Shrier", "title": "Implementing multiple imputation for missing data in longitudinal\n  studies when models are not feasible: A tutorial on the random hot deck\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: Researchers often use model-based multiple imputation to handle\nmissing at random data to minimize bias while making the best use of all\navailable data. However, there are sometimes constraints within the data that\nmake model-based imputation difficult and may result in implausible values. In\nthese contexts, we describe how to use random hot deck imputation to allow for\nplausible multiple imputation in longitudinal studies.\n  Study Design and Setting: We illustrate random hot deck multiple imputation\nusing The Childhood Health, Activity, and Motor Performance School Study\nDenmark (CHAMPS-DK), a prospective cohort study that measured weekly sports\nparticipation for 1700 Danish schoolchildren. We matched records with missing\ndata to several observed records, generated probabilities for matched records\nusing observed data, and sampled from these records based on the probability of\neach occurring. Because imputed values are generated randomly, multiple\ncomplete datasets can be created and analyzed similar to model-based multiple\nimputation.\n  Conclusion: Multiple imputation using random hot deck imputation is an\nalternative method when model-based approaches are infeasible, specifically\nwhere there are constraints within and between covariates.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:21:42 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 18:05:20 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 13:47:45 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2020 19:43:39 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wang", "Chinchin", ""], ["Stokes", "Tyrel", ""], ["Steele", "Russell", ""], ["Wedderkopp", "Niels", ""], ["Shrier", "Ian", ""]]}, {"id": "2004.06826", "submitter": "Lorenzo Cappello", "authors": "Lorenzo Cappello, Amandine Veber, Julia A. Palacios", "title": "The Tajima heterochronous n-coalescent: inference from heterochronously\n  sampled molecular data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The observed sequence variation at a locus informs about the evolutionary\nhistory of the sample and past population size dynamics. The Kingman coalescent\nis used in a generative model of molecular sequence variation to infer\nevolutionary parameters. However, it is well understood that inference under\nthis model does not scale well with sample size. Here, we build on recent work\nbased on a lower resolution coalescent process, the Tajima coalescent, to model\nlongitudinal samples. While the Kingman coalescent models the ancestry of\nlabeled individuals, the heterochronous Tajima coalescent models the ancestry\nof individuals labeled by their sampling time. We propose a new inference\nscheme for the reconstruction of effective population size trajectories based\non this model with the potential to improve computational efficiency. Modeling\nof longitudinal samples is necessary for applications (e.g. ancient DNA and RNA\nfrom rapidly evolving pathogens like viruses) and statistically desirable\n(variance reduction and parameter identifiability). We propose an efficient\nalgorithm to calculate the likelihood and employ a Bayesian nonparametric\nprocedure to infer the population size trajectory. We provide a new MCMC\nsampler to explore the space of heterochronous Tajima's genealogies and model\nparameters. We compare our procedure with state-of-the-art methodologies in\nsimulations and applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 22:47:51 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:35:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Cappello", "Lorenzo", ""], ["Veber", "Amandine", ""], ["Palacios", "Julia A.", ""]]}, {"id": "2004.06831", "submitter": "Ariel Cintron-Arias", "authors": "Ariel Cintr\\'on-Arias, H. T. Banks, Alex Capaldi, Alun L. Lloyd", "title": "A Sensitivity Matrix Based Methodology for Inverse Problem Formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to select parameter subset combinations that can be\nestimated using an ordinary least-squares (OLS) inverse problem formulation\nwith a given data set. First, the algorithm selects the parameter combinations\nthat correspond to sensitivity matrices with full rank. Second, the algorithm\ninvolves uncertainty quantification by using the inverse of the Fisher\nInformation Matrix. Nominal values of parameters are used to construct\nsynthetic data sets, and explore the effects of removing certain parameters\nfrom those to be estimated using OLS procedures. We quantify these effects in a\nscore for a vector parameter defined using the norm of the vector of standard\nerrors for components of estimates divided by the estimates. In some cases the\nmethod leads to reduction of the standard error for a parameter to less than\n1\\% of the estimate.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 23:14:39 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Cintr\u00f3n-Arias", "Ariel", ""], ["Banks", "H. T.", ""], ["Capaldi", "Alex", ""], ["Lloyd", "Alun L.", ""]]}, {"id": "2004.06998", "submitter": "Nan Van Geloven PhD", "authors": "Nan van Geloven, Sonja Swanson, Chava Ramspek, Kim Luijken, Merel van\n  Diepen, Tim Morris, Rolf Groenwold, Hans van Houwelingen, Hein Putter, Saskia\n  le Cessie", "title": "Prediction meets causal inference: the role of treatment in clinical\n  prediction models", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study approaches for dealing with treatment when developing\na clinical prediction model. Analogous to the estimand framework recently\nproposed by the European Medicines Agency for clinical trials, we propose a\n`predictimand' framework of different questions that may be of interest when\npredicting risk in relation to treatment started after baseline. We provide a\nformal definition of the estimands matching these questions, give examples of\nsettings in which each is useful and discuss appropriate estimators including\ntheir assumptions. We illustrate the impact of the predictimand choice in a\ndataset of patients with end-stage kidney disease. We argue that clearly\ndefining the estimand is equally important in prediction research as in causal\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 10:52:51 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["van Geloven", "Nan", ""], ["Swanson", "Sonja", ""], ["Ramspek", "Chava", ""], ["Luijken", "Kim", ""], ["van Diepen", "Merel", ""], ["Morris", "Tim", ""], ["Groenwold", "Rolf", ""], ["van Houwelingen", "Hans", ""], ["Putter", "Hein", ""], ["Cessie", "Saskia le", ""]]}, {"id": "2004.07356", "submitter": "Tianyu Zhan", "authors": "Tianyu Zhan, Lu Cui, Ziqian Geng, Lanju Zhang, Yihua Gu, Ivan S.F.\n  Chan", "title": "A Practical Response Adaptive Block Randomization Design with Analytic\n  Type I Error Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response adaptive randomization is appealing in confirmatory adaptive\nclinical trials from statistical, ethical, and pragmatic perspectives, in the\nsense that subjects are more likely to be randomized to better performing\ntreatment groups based on accumulating data. The Doubly Adaptive Biased Coin\nDesign (DBCD) is a popular solution due to its asymptotic normal property of\nfinal allocations, which further justifies its asymptotic type I error rate\ncontrol. As an alternative, we propose a Response Adaptive Block Randomization\n(RABR) design with pre-specified randomization ratios for the control and\nhigh-performing groups to robustly achieve desired final sample size per group\nunder different underlying responses, which is usually required in\nindustry-sponsored clinical studies. We show that the usual test statistic has\na controlled type I error rate. Our simulations further highlight the\nadvantages of the proposed design over the DBCD in terms of consistently\nachieving final sample allocations and of power performance. We further apply\nthis design to a Phase III study evaluating the efficacy of two dosing regimens\nof adjunctive everolimus in treating tuberous sclerosis complex but with no\nprevious dose-finding studies in this indication.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 21:36:15 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zhan", "Tianyu", ""], ["Cui", "Lu", ""], ["Geng", "Ziqian", ""], ["Zhang", "Lanju", ""], ["Gu", "Yihua", ""], ["Chan", "Ivan S. F.", ""]]}, {"id": "2004.07375", "submitter": "Arman Oganisian", "authors": "Arman Oganisian, Jason A. Roy", "title": "A Practical Introduction to Bayesian Estimation of Causal Effects:\n  Parametric and Nonparametric Approaches", "comments": "Currently under second-round revision. This version included edits\n  from first round", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial advances in Bayesian methods for causal inference have been\ndeveloped in recent years. We provide an introduction to Bayesian inference for\ncausal effects for practicing statisticians who have some familiarity with\nBayesian models and would like an overview of what it can add to causal\nestimation in practical settings. In the paper, we demonstrate how priors can\ninduce shrinkage and sparsity on parametric models and be used to perform\nprobabilistic sensitivity analyses around causal assumptions. We provide an\noverview of nonparametric Bayesian estimation and survey their applications in\nthe causal inference literature. Inference in the point-treatment and\ntime-varying treatment settings are considered. For the latter, we explore both\nstatic and dynamic treatment regimes. Throughout, we illustrate implementation\nusing off-the-shelf open source software. We hope the reader will walk away\nwith implementation-level knowledge of Bayesian causal inference using both\nparametric and nonparametric models. All synthetic examples and code used in\nthe paper are publicly available on a companion GitHub repository.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 22:32:16 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 17:29:08 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Oganisian", "Arman", ""], ["Roy", "Jason A.", ""]]}, {"id": "2004.07429", "submitter": "Uri Keich", "authors": "Noah Peres, Andrew Lee, Uri Keich", "title": "Exactly computing the tail of the Poisson-Binomial Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer ShiftConvolvePoibin, a fast exact method to compute the tail of a\nPoisson-Binomial distribution (PBD). Our method employs an exponential shift to\nretain its accuracy when computing a tail probability, and in practice we find\nthat it is immune to the significant relative errors that other methods, exact\nor approximate, can suffer from when computing very small tail probabilities of\nthe PBD. The accompanying R package is also competitive with the fastest\nimplementations for computing the entire PBD.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:04:27 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Peres", "Noah", ""], ["Lee", "Andrew", ""], ["Keich", "Uri", ""]]}, {"id": "2004.07471", "submitter": "Dootika Vats", "authors": "Dootika Vats, Fl\\'avio Gon\\c{c}alves, Krzysztof {\\L}atuszy\\'nski,\n  Gareth O. Roberts", "title": "Efficient Bernoulli factory MCMC for intractable posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accept-reject based Markov chain Monte Carlo (MCMC) algorithms have\ntraditionally utilised acceptance probabilities that can be explicitly written\nas a function of the ratio of the target density at the two contested points.\nThis feature is rendered almost useless in Bayesian posteriors with unknown\nfunctional forms. We introduce a new family of MCMC acceptance probabilities\nthat has the distinguishing feature of not being a function of the ratio of the\ntarget density at the two points. We present two stable Bernoulli factories\nthat generate events within this class of acceptance probabilities. The\nefficiency of our methods rely on obtaining reasonable local upper or lower\nbounds on the target density and we present two classes of problems where such\nbounds are viable: Bayesian inference for diffusions and MCMC on constrained\nspaces. The resulting portkey Barker's algorithms are exact and computationally\nmore efficient that the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 06:07:04 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 18:47:42 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 09:26:19 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Vats", "Dootika", ""], ["Gon\u00e7alves", "Fl\u00e1vio", ""], ["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2004.07542", "submitter": "Katrin Madjar", "authors": "Katrin Madjar, Manuela Zucknick, Katja Ickstadt, and J\\\"org\n  Rahnenf\\\"uhrer", "title": "Combining heterogeneous subgroups with graph-structured variable\n  selection priors for Cox regression", "comments": "under review, 19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important objectives in cancer research are the prediction of a patient's\nrisk based on molecular measurements such as gene expression data and the\nidentification of new prognostic biomarkers (e.g. genes). In clinical practice,\nthis is often challenging because patient cohorts are typically small and can\nbe heterogeneous. In classical subgroup analysis, a separate prediction model\nis fitted using only the data of one specific cohort. However, this can lead to\na loss of power when the sample size is small. Simple pooling of all cohorts,\non the other hand, can lead to biased results, especially when the cohorts are\nheterogeneous. For this situation, we propose a new Bayesian approach suitable\nfor continuous molecular measurements and survival outcome that identifies the\nimportant predictors and provides a separate risk prediction model for each\ncohort. It allows sharing information between cohorts to increase power by\nassuming a graph linking predictors within and across different cohorts. The\ngraph helps to identify pathways of functionally related genes and genes that\nare simultaneously prognostic in different cohorts. Results demonstrate that\nour proposed approach is superior to the standard approaches in terms of\nprediction performance and increased power in variable selection when the\nsample size is small.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 09:13:08 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Madjar", "Katrin", ""], ["Zucknick", "Manuela", ""], ["Ickstadt", "Katja", ""], ["Rahnenf\u00fchrer", "J\u00f6rg", ""]]}, {"id": "2004.07579", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen and Siliang Zhang", "title": "Estimation Methods for Item Factor Analysis: An Overview", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item factor analysis (IFA) refers to the factor models and statistical\ninference procedures for analyzing multivariate categorical data. IFA\ntechniques are commonly used in social and behavioral sciences for analyzing\nitem-level response data. Such models summarize and interpret the dependence\nstructure among a set of categorical variables by a small number of latent\nfactors. In this chapter, we review the IFA modeling technique and commonly\nused IFA models. Then we discuss estimation methods for IFA models and their\ncomputation, with a focus on the situation where the sample size, the number of\nitems, and the number of factors are all large. Existing statistical softwares\nfor IFA are surveyed. This chapter is concluded with suggestions for practical\napplications of IFA methods and discussions of future directions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:39:35 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Chen", "Yunxiao", ""], ["Zhang", "Siliang", ""]]}, {"id": "2004.07649", "submitter": "Bj\\\"orn B\\\"ottcher", "authors": "Bj\\\"orn B\\\"ottcher", "title": "Notes on the interpretation of dependence measures", "comments": "20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the classical distinction of correlation and dependence, many\ndependence measures bear further pitfalls in their application and\ninterpretation. The aim of this paper is to raise and recall awareness of some\nof these limitations by explicitly discussing Pearson's correlation and the\nmultivariate dependence measures: distance correlation, distance\nmulticorrelations and their copula versions. The discussed aspects include\ntypes of dependence, bias of empirical measures, influence of marginal\ndistributions and dimensions.\n  In general it is recommended to use a proper dependence measure instead of\nPearson's correlation. Moreover, a measure which is distribution-free (at least\nin some sense) can help to avoid certain systematic errors. Nevertheless, in a\ntruly multivariate setting only the p-values of the corresponding independence\ntests provide always values with indubitable interpretation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:38:08 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["B\u00f6ttcher", "Bj\u00f6rn", ""]]}, {"id": "2004.07725", "submitter": "Alassane Aw", "authors": "Alassane Aw and Emmanuel Nicolas Cabral", "title": "Functional SAC model: With application to spatial econometrics", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial autoregressive combined (SAC) model has been widely studied in the\nliterature for the analysis of spatial data in various areas such as geography,\neconomics, demography, regional sciences. This is a linear model with scalar\nresponse, scalar explanatory variables and which allows for spatial\ninteractions in the dependent variable and the disturbances. In this work we\nextend this modeling approach from scalar to functional covariate. The\nparameters of the model are estimated via the maximum likelihood estimation\nmethod. A simulation study is conducted to evaluate the performance of the\nproposed methodology. As an illustration, the model is used to establish the\nrelationship between unemployment and illiteracy in Senegal.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:05:17 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Aw", "Alassane", ""], ["Cabral", "Emmanuel Nicolas", ""]]}, {"id": "2004.07743", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Nianqiao Ju, Sergio Bacallado, Rajen D. Shah", "title": "BETS: The dangers of selection bias in early analyses of the coronavirus\n  disease (COVID-19) pandemic", "comments": "33 pages, 8 figures, 5 tables; Accepted for publication in The Annals\n  of Applied Statistics on 24th September, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) has quickly grown from a regional\noutbreak in Wuhan, China to a global pandemic. Early estimates of the epidemic\ngrowth and incubation period of COVID-19 may have been biased due to sample\nselection. Using detailed case reports from 14 locations in and outside\nmainland China, we obtained 378 Wuhan-exported cases who left Wuhan before an\nabrupt travel quarantine. We developed a generative model we call BETS for four\nkey epidemiological events---Beginning of exposure, End of exposure, time of\nTransmission, and time of Symptom onset (BETS)---and derived explicit formulas\nto correct for the sample selection. We gave a detailed illustration of why\nsome early and highly influential analyses of the COVID-19 pandemic were\nseverely biased. All our analyses, regardless of which subsample and model were\nbeing used, point to an epidemic doubling time of 2 to 2.5 days during the\nearly outbreak in Wuhan. A Bayesian nonparametric analysis further suggests\nthat about 5% of the symptomatic cases may not develop symptoms within 14 days\nof infection and that men may be much more likely than women to develop\nsymptoms within 2 days of infection.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:27:24 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 17:16:17 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 10:39:14 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 10:34:14 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Ju", "Nianqiao", ""], ["Bacallado", "Sergio", ""], ["Shah", "Rajen D.", ""]]}, {"id": "2004.07881", "submitter": "Jacob Fiksel", "authors": "Jacob Fiksel, Scott Zeger and Abhirup Datta", "title": "A Transformation-free Linear Regression for Compositional Outcomes and\n  Predictors", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data are common in many fields, both as outcomes and predictor\nvariables. The inventory of models for the case when both the outcome and\npredictor variables are compositional is limited and the existing models are\ndifficult to interpret, due to their use of complex log-ratio transformations.\nWe develop a transformation-free linear regression model where the expected\nvalue of the compositional outcome is expressed as a single Markov transition\nfrom the compositional predictor. Our approach is based on generalized method\nof moments thereby not requiring complete specification of data likelihood and\nis robust to different data generating mechanism. Our model is simple to\ninterpret, allows for 0s and 1s in both the compositional outcome and\ncovariates, and subsumes several interesting subcases of interest. We also\ndevelop a permutation test for linear independence. Finally, we show that\ndespite its simplicity, our model accurately captures the relationship between\ncompositional data from education and medical research.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 19:02:00 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Fiksel", "Jacob", ""], ["Zeger", "Scott", ""], ["Datta", "Abhirup", ""]]}, {"id": "2004.07887", "submitter": "Jordan Bryan", "authors": "Jordan G. Bryan and Peter D. Hoff", "title": "Smaller $p$-values in genomics studies using distilled historical\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical research institutions have generated massive amounts of biological\ndata by genetically profiling hundreds of cancer cell lines. In parallel,\nacademic biology labs have conducted genetic screens on small numbers of cancer\ncell lines under custom experimental conditions. In order to share information\nbetween these two approaches to scientific discovery, this article proposes a\n\"frequentist assisted by Bayes\" (FAB) procedure for hypothesis testing that\nallows historical information from massive genomics datasets to increase the\npower of hypothesis tests in specialized studies. The exchange of information\ntakes place through a novel probability model for multimodal genomics data,\nwhich distills historical information pertaining to cancer cell lines and genes\nacross a wide variety of experimental contexts. If the relevance of the\nhistorical information for a given study is high, then the resulting FAB tests\ncan be more powerful than the corresponding classical tests. If the relevance\nis low, then the FAB tests yield as many discoveries as the classical tests.\nSimulations and practical investigations demonstrate that the FAB testing\nprocedure can increase the number of effects discovered in genomics studies\nwhile still maintaining strict control of type I error and false discovery\nrates.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 19:21:09 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Bryan", "Jordan G.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "2004.07977", "submitter": "Marcelo Medeiros", "authors": "Marcelo Medeiros, Alexandre Street, Davi Vallad\\~ao, Gabriel\n  Vasconcelos, Eduardo Zilberman", "title": "Short-Term Covid-19 Forecast for Latecomers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of Covid-19 cases is increasing dramatically worldwide. Therefore,\nthe availability of reliable forecasts for the number of cases in the coming\ndays is of fundamental importance. We propose a simple statistical method for\nshort-term real-time forecasting of the number of Covid-19 cases and fatalities\nin countries that are latecomers -- i.e., countries where cases of the disease\nstarted to appear some time after others. In particular, we propose a penalized\n(LASSO) regression with an error correction mechanism to construct a model of a\nlatecomer in terms of the other countries that were at a similar stage of the\npandemic some days before. By tracking the number of cases and deaths in those\ncountries, we forecast through an adaptive rolling-window scheme the number of\ncases and deaths in the latecomer. We apply this methodology to Brazil, and\nshow that (so far) it has been performing very well. These forecasts aim to\nfoster a better short-run management of the health system capacity.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 22:09:14 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 14:30:50 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Medeiros", "Marcelo", ""], ["Street", "Alexandre", ""], ["Vallad\u00e3o", "Davi", ""], ["Vasconcelos", "Gabriel", ""], ["Zilberman", "Eduardo", ""]]}, {"id": "2004.08000", "submitter": "Ruiyi Yang", "authors": "Daniel Sanz-Alonso and Ruiyi Yang", "title": "The SPDE Approach to Mat\\'ern Fields: Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates Gaussian Markov random field approximations to\nnonstationary Gaussian fields using graph representations of stochastic partial\ndifferential equations. We establish approximation error guarantees building on\nthe theory of spectral convergence of graph Laplacians. The proposed graph\nrepresentations provide a generalization of the Mat\\'ern model to unstructured\npoint clouds, and facilitate inference and sampling using linear algebra\nmethods for sparse matrices. In addition, they bridge and unify several models\nin Bayesian inverse problems, spatial statistics and graph-based machine\nlearning. We demonstrate through examples in these three disciplines that the\nunity revealed by graph representations facilitates the exchange of ideas\nacross them.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 23:58:36 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 19:44:58 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 02:28:41 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Sanz-Alonso", "Daniel", ""], ["Yang", "Ruiyi", ""]]}, {"id": "2004.08032", "submitter": "Emiliano Valdez", "authors": "Himchan Jeong and Hyunwoong Chang and Emiliano A. Valdez", "title": "A non-convex regularization approach for stable estimation of loss\n  development factors", "comments": "23 pages, 11 Tables, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we apply non-convex regularization methods in order to\nobtain stable estimation of loss development factors in insurance claims\nreserving. Among the non-convex regularization methods, we focus on the use of\nthe log-adjusted absolute deviation (LAAD) penalty and provide discussion on\noptimization of LAAD penalized regression model, which we prove to converge\nwith a coordinate descent algorithm under mild conditions. This has the\nadvantage of obtaining a consistent estimator for the regression coefficients\nwhile allowing for the variable selection, which is linked to the stable\nestimation of loss development factors. We calibrate our proposed model using a\nmulti-line insurance dataset from a property and casualty insurer where we\nobserved reported aggregate loss along accident years and development periods.\nWhen compared to other regression models, our LAAD penalized regression model\nprovides very promising results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 02:05:43 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 13:21:55 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 02:53:24 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Jeong", "Himchan", ""], ["Chang", "Hyunwoong", ""], ["Valdez", "Emiliano A.", ""]]}, {"id": "2004.08159", "submitter": "Xiao Fang", "authors": "Xiao Fang, David Siegmund", "title": "Detection and Estimation of Local Signals", "comments": "50 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum score statistic to detect and estimate local signals in\nthe form of change-points in the level, slope, or other property of a sequence\nof observations, and to segment the sequence when there appear to be multiple\nchanges. We find that when observations are serially dependent, the\nchange-points can lead to upwardly biased estimates of autocorrelations,\nresulting in a sometimes serious loss of power. Examples involving temperature\nvariations, the level of atmospheric greenhouse gases, suicide rates and daily\nincidence of COVID-19 illustrate the general theory.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 10:32:31 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Fang", "Xiao", ""], ["Siegmund", "David", ""]]}, {"id": "2004.08256", "submitter": "Thorsten Dickhaus", "authors": "Anh-Tuan Hoang and Thorsten Dickhaus", "title": "On the usage of randomized p-values in the Schweder-Spjotvoll estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with multiple test problems with composite null hypotheses\nand the estimation of the proportion $\\pi_{0}$ of true null hypotheses. The\nSchweder-Spj\\o tvoll estimator $\\hat{\\pi}_0$ utilizes marginal $p$-values and\nonly works properly if the $p$-values that correspond to the true null\nhypotheses are uniformly distributed on $[0,1]$\n($\\mathrm{Uni}[0,1]$-distributed). In the case of composite null hypotheses,\nmarginal $p$-values are usually computed under least favorable parameter\nconfigurations (LFCs). Thus, they are stochastically larger than\n$\\mathrm{Uni}[0,1]$ under non-LFCs in the null hypotheses. When using these\nLFC-based $p$-values, $\\hat{\\pi}_0$ tends to overestimate $\\pi_{0}$. We\nintroduce a new way of randomizing $p$-values that depends on a tuning\nparameter $c\\in[0,1]$, such that $c=0$ and $c=1$ lead to\n$\\mathrm{Uni}[0,1]$-distributed $p$-values, which are independent of the data,\nand to the original LFC-based $p$-values, respectively. For a certain value\n$c=c^{\\star}$ the bias of $\\hat{\\pi}_0$ is minimized when using our randomized\n$p$-values. This often also entails a smaller mean squared error of the\nestimator as compared to the usage of the LFC-based $p$-values. We analyze\nthese points theoretically, and we demonstrate them numerically in computer\nsimulations under various standard statistical models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 14:12:37 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Hoang", "Anh-Tuan", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "2004.08309", "submitter": "Antik Chakraborty", "authors": "Antik Chakraborty, Otso Ovaskainen, David B. Dunson", "title": "Bayesian semiparametric long memory models for discretized event data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of semiparametric latent variable models for long\nmemory discretized event data. The proposed methodology is motivated by a study\nof bird vocalizations in the Amazon rain forest; the timings of vocalizations\nexhibit self-similarity and long range dependence ruling out models based on\nPoisson processes. The proposed class of FRActional Probit (FRAP) models is\nbased on thresholding of a latent process consisting of an additive expansion\nof a smooth Gaussian process with a fractional Brownian motion. We develop a\nBayesian approach to inference using Markov chain Monte Carlo, and show good\nperformance in simulation studies. Applying the methods to the Amazon bird\nvocalization data, we find substantial evidence for self-similarity and\nnon-Markovian/Poisson dynamics. To accommodate the bird vocalization data, in\nwhich there are many different species of birds exhibiting their own\nvocalization dynamics, a hierarchical expansion of FRAP is provided in\nSupplementary Materials.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 15:42:27 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 23:16:21 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Chakraborty", "Antik", ""], ["Ovaskainen", "Otso", ""], ["Dunson", "David B.", ""]]}, {"id": "2004.08318", "submitter": "Sokbae Lee", "authors": "Sung Jae Jun and Sokbae Lee", "title": "Causal Inference in Case-Control Studies", "comments": "58 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate partial identification of causal relative and attributable\nrisk---the ratio of two counterfactual proportions and the difference between\nthem---in case-control and case-population studies. The odds ratio is shown to\nbe a sharp upper bound on causal relative risk under the monotone treatment\nresponse and monotone treatment selection assumptions, without resorting to\nstrong ignorability, nor to the rare-disease assumption. Sharp bounds on causal\nattributable risk are also obtained under the same assumptions. Paying special\nattention to the (conditional) odds ratio, we propose a semiparametrically\nefficient estimator of the aggregated (log) odds ratio. Further, we develop\neasy-to-implement causal inference procedures for relative and attributable\nrisk. Finally, we showcase our methodology by applying it to two unique\ndatasets in the literature. We find that attending private school may have\nlittle effect on entering a very selective university in Pakistan and that\ndropping out of school could substantially increase relative and attributable\nrisk of joining a criminal gang in Brazil.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 16:01:34 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 17:48:47 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Jun", "Sung Jae", ""], ["Lee", "Sokbae", ""]]}, {"id": "2004.08416", "submitter": "Fekadu L. Bayisa Dr.", "authors": "Fekadu L. Bayisa, Markus {\\AA}dahl, Patrik Ryd\\'en, Ottmar Cronie", "title": "Large-scale modelling and forecasting of ambulance calls in northern\n  Sweden using spatio-temporal log-Gaussian Cox processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although ambulance call data typically come in the form of spatio-temporal\npoint patterns, point process-based modelling approaches presented in the\nliterature are scarce. In this paper, we study a unique set of Swedish\nspatio-temporal ambulance call data, which consist of the spatial (GPS)\nlocations of the calls (within the four northernmost regions of Sweden) and the\nassociated days of occurrence of the calls (January 1, 2014, to December 31,\n2018). Motivated by the nature of the data, we here employ log-Gaussian Cox\nprocesses (LGCPs) for the spatio-temporal modelling and forecasting of the\ncalls. To this end, we propose a K-means clustering based bandwidth selection\nmethod for the kernel estimation of the spatial component of the separable\nspatio-temporal intensity function. The temporal component of the intensity\nfunction is modelled using Poisson regression, using different calendar\ncovariates, and the spatio-temporal random field component of the random\nintensity of the LGCP is fitted using the Metropolis-adjusted Langevin\nalgorithm. Spatial hot-spots have been found in the south-eastern part of the\nstudy region, where most people in the region live and our fitted\nmodel/forecasts manage to capture this behavior quite well. Also, there is a\nsignificant association between the expected number of calls and the\nday-of-the-week and the season-of-the-year. A non-parametric second-order\nanalysis indicates that LGCPs seem to be reasonable models for the data.\nFinally, we find that the fitted forecasts generate simulated future spatial\nevent patterns that quite well resemble the actual future data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 18:12:32 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 19:18:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bayisa", "Fekadu L.", ""], ["\u00c5dahl", "Markus", ""], ["Ryd\u00e9n", "Patrik", ""], ["Cronie", "Ottmar", ""]]}, {"id": "2004.08458", "submitter": "Keaven Anderson PhD", "authors": "Ting-Yu Chen, Jing Zhao, Linda Sun, Keaven Anderson", "title": "Multiplicity for a Group Sequential Trial with Biomarker Subpopulations", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomarker subpopulations have become increasingly important for drug\ndevelopment in targeted therapies. The use of biomarkers has the potential to\nfacilitate more effective outcomes by guiding patient selection appropriately,\nthus enhancing the benefit-risk profile and improving trial power. Studying a\nbroad population simultaneously with a more targeted one allows the trial to\ndetermine the population for which a treatment is effective and allows a goal\nof making approved regulatory labeling as inclusive as is appropriate. We\nexamine new methods accounting for the complete correlation structure in group\nsequential designs with hypotheses in nested subgroups. The designs provide\nfull control of family-wise Type I error rate. This extension of previous\nmethods accounting for either group sequential design or correlation between\nsubgroups improves efficiency (power or sample size) over a typical Bonferroni\napproach for testing nested populations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 21:20:46 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 16:16:59 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Chen", "Ting-Yu", ""], ["Zhao", "Jing", ""], ["Sun", "Linda", ""], ["Anderson", "Keaven", ""]]}, {"id": "2004.08472", "submitter": "Tirthankar Dasgupta", "authors": "Xiaokang Luo, Tirthankar Dasgupta, Minge Xie, Regina Liu", "title": "Leveraging the Fisher randomization test using confidence distributions:\n  inference, combination and fusion learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flexibility and wide applicability of the Fisher randomization test (FRT)\nmakes it an attractive tool for assessment of causal effects of interventions\nfrom modern-day randomized experiments that are increasing in size and\ncomplexity. This paper provides a theoretical inferential framework for FRT by\nestablishing its connection with confidence distributions Such a connection\nleads to development of (i) an unambiguous procedure for inversion of FRTs to\ngenerate confidence intervals with guaranteed coverage, (ii) generic and\nspecific methods to combine FRTs from multiple independent experiments with\ntheoretical guarantees and (iii) new insights on the effect of size of the\nMonte Carlo sample on the results of FRT. Our developments pertain to finite\nsample settings but have direct extensions to large samples. Simulations and a\ncase example demonstrate the benefit of these new developments.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 22:20:35 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Luo", "Xiaokang", ""], ["Dasgupta", "Tirthankar", ""], ["Xie", "Minge", ""], ["Liu", "Regina", ""]]}, {"id": "2004.08492", "submitter": "Zhishi Wang", "authors": "Edwin Ng, Zhishi Wang, Huigang Chen, Steve Yang, Slawek Smyl", "title": "Orbit: Probabilistic Forecast with Exponential Smoothing", "comments": "arXiv admin note: text overlap with arXiv:1909.13316 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is an active research topic in academia as well as\nindustry. Although we see an increasing amount of adoptions of machine learning\nmethods in solving some of those forecasting challenges, statistical methods\nremain powerful while dealing with low granularity data. This paper introduces\na refined Bayesian exponential smoothing model with the help of probabilistic\nprogramming languages including Stan. Our model refinements include additional\nglobal trend, transformation for multiplicative form, noise distribution and\nchoice of priors. A benchmark study is conducted on a rich set of time-series\ndata sets for our models along with other well-known time series models.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 00:21:53 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 20:24:22 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 00:47:46 GMT"}, {"version": "v4", "created": "Fri, 22 Jan 2021 22:05:11 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ng", "Edwin", ""], ["Wang", "Zhishi", ""], ["Chen", "Huigang", ""], ["Yang", "Steve", ""], ["Smyl", "Slawek", ""]]}, {"id": "2004.08557", "submitter": "Thomas Hotz", "authors": "Thomas Hotz, Matthias Glock, Stefan Heyder, Sebastian Semper, Anne\n  B\\\"ohle, Alexander Kr\\\"amer", "title": "Monitoring the spread of COVID-19 by estimating reproduction numbers\n  over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To control the current outbreak of the Coronavirus Disease 2019, constant\nmonitoring of the epidemic is required since, as of today, no vaccines or\nantiviral drugs against it are known. We provide daily updated estimates of the\nreproduction number over time at\nhttps://stochastik-tu-ilmenau.github.io/COVID-19/. In this document, we\ndescribe the estimator we are using which was developed in (Fraser 2007),\nderive its asymptotic properties, and we give details on its implementation.\nFurthermore, we validate the estimator on simulated data, demonstrate that\nestimates on real data lead to plausible results, and perform a sensitivity\nanalysis. Finally, we discuss why the estimates obtained need to be interpreted\nwith care.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:39:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hotz", "Thomas", ""], ["Glock", "Matthias", ""], ["Heyder", "Stefan", ""], ["Semper", "Sebastian", ""], ["B\u00f6hle", "Anne", ""], ["Kr\u00e4mer", "Alexander", ""]]}, {"id": "2004.08561", "submitter": "Mark Balenzuela", "authors": "Mark P. Balenzuela, Adrian G. Wills, Christopher Renton, and Brett\n  Ninness", "title": "A New Smoothing Algorithm for Jump Markov Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for calculating the smoothed state distribution\nfor Jump Markov Linear Systems. More specifically, the paper details a novel\ntwo-filter smoother that provides closed-form expressions for the smoothed\nhybrid state distribution. This distribution can be expressed as a Gaussian\nmixture with a known, but exponentially increasing, number of Gaussian\ncomponents as the time index increases. This is accompanied by exponential\ngrowth in memory and computational requirements, which rapidly becomes\nintractable. To ameliorate this, we limit the number of allowed mixture terms\nby employing a Gaussian mixture reduction strategy, which results in a\ncomputationally tractable, but approximate smoothed distribution. The\napproximation error can be balanced against computational complexity in order\nto provide an accurate and practical smoothing algorithm that compares\nfavourably to existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:52:38 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Balenzuela", "Mark P.", ""], ["Wills", "Adrian G.", ""], ["Renton", "Christopher", ""], ["Ninness", "Brett", ""]]}, {"id": "2004.08565", "submitter": "Mark Balenzuela", "authors": "Mark P. Balenzuela, Adrian G. Wills, Christopher Renton, and Brett\n  Ninness", "title": "Bayesian Parameter Identification for Jump Markov Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian method for identification of jump Markov\nlinear system parameters. A primary motivation is to provide accurate\nquantification of parameter uncertainty without relying on asymptotic in\ndata-length arguments. To achieve this, the paper details a particle-Gibbs\nsampling approach that provides samples from the desired posterior\ndistribution. These samples are produced by utilising a modified discrete\nparticle filter and carefully chosen conjugate priors.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 09:13:38 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 03:03:01 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Balenzuela", "Mark P.", ""], ["Wills", "Adrian G.", ""], ["Renton", "Christopher", ""], ["Ninness", "Brett", ""]]}, {"id": "2004.08580", "submitter": "Wang Zhou", "authors": "Xuejun Ma, Shaochen Wang, Wang Zhou", "title": "Statistical inference in massive datasets by empirical likelihood", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new statistical inference method for massive data\nsets, which is very simple and efficient by combining divide-and-conquer method\nand empirical likelihood. Compared with two popular methods (the bag of little\nbootstrap and the subsampled double bootstrap), we make full use of data sets,\nand reduce the computation burden. Extensive numerical studies and real data\nanalysis demonstrate the effectiveness and flexibility of our proposed method.\nFurthermore, the asymptotic property of our method is derived.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 10:18:07 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ma", "Xuejun", ""], ["Wang", "Shaochen", ""], ["Zhou", "Wang", ""]]}, {"id": "2004.08630", "submitter": "Euloge Clovis Kenne Pagui", "authors": "Euloge Clovis Kenne Pagui, Alessandra Salvan and Nicola Sartori", "title": "Efficient implementation of median bias reduction with applications to\n  general regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In numerous regular statistical models, median bias reduction (Kenne Pagui et\nal., 2017) has proven to be a noteworthy improvement over maximum likelihood,\nalternative to mean bias reduction. The estimator is obtained as solution to a\nmodified score equation ensuring smaller asymptotic median bias than the\nmaximum likelihood estimator. This paper provides a simplified algebraic form\nof the adjustment term for general regular models. With the new formula, the\nestimation procedure benefits from a considerable computational gain by\navoiding multiple summations and thus allows an efficient implementation. More\nimportantly, the new formulation allows to highlight how the median bias\nreduction adjustment can be obtained by adding an extra term to the mean bias\nreduction adjustment. Illustrations are provided through new applications of\nmedian bias reduction to two regression models not belonging to the generalized\nlinear models class, extended beta regression and beta-binomial regression.\nMean bias reduction is also provided here for the latter model. Simulation\nstudies show remarkable componentwise median centering of the median bias\nreduced estimator, while variability and coverage of related confidence\nintervals are comparable with those of mean bias reduction. Moreover, empirical\nresults for the beta-binomial model show that the method is successful in\nsolving maximum likelihood boundary estimate problem.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 14:22:54 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 13:51:55 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Pagui", "Euloge Clovis Kenne", ""], ["Salvan", "Alessandra", ""], ["Sartori", "Nicola", ""]]}, {"id": "2004.08667", "submitter": "Matheus Guerrero", "authors": "Matheus B. Guerrero and Wagner Barreto-Souza and Hernando Ombao", "title": "Integer-valued autoregressive process with flexible marginal and\n  innovation distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  INteger Auto-Regressive (INAR) processes are usually defined by specifying\nthe innovations and the operator, which often leads to difficulties in deriving\nmarginal properties of the process. In many practical situations, a major\nmodeling limitation is that it is difficult to justify the choice of the\noperator. To overcome these drawbacks, we propose a new flexible approach to\nbuild an INAR model: we pre-specify the marginal and innovation distributions.\nHence, the operator is a consequence of specifying the desired marginal and\ninnovation distributions. Our new INAR model has both marginal and innovations\ngeometric distributed, being a direct alternative to the classical Poisson INAR\nmodel. Our proposed process has interesting stochastic properties such as an\nMA($\\infty$) representation, time-reversibility, and closed-forms for the\ntransition probabilities $h$-steps ahead, allowing for coherent forecasting. We\nanalyze time-series counts of skin lesions using our proposed approach,\ncomparing it with existing INAR and INGARCH models. Our model gives more\nadherence to the data and better forecasting performance.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 17:31:14 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Guerrero", "Matheus B.", ""], ["Barreto-Souza", "Wagner", ""], ["Ombao", "Hernando", ""]]}, {"id": "2004.08724", "submitter": "Quan Vu", "authors": "Quan Vu, Andrew Zammit-Mangion, Noel Cressie", "title": "Modeling Nonstationary and Asymmetric Multivariate Spatial Covariances\n  via Deformations", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202020.0156", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate spatial-statistical models are often used when modeling\nenvironmental and socio-demographic processes. The most commonly used models\nfor multivariate spatial covariances assume both stationarity and symmetry for\nthe cross-covariances, but these assumptions are rarely tenable in practice. In\nthis article we introduce a new and highly flexible class of nonstationary and\nasymmetric multivariate spatial covariance models that are constructed by\nmodeling the simpler and more familiar stationary and symmetric multivariate\ncovariances on a warped domain. Inspired by recent developments in the\nunivariate case, we propose modeling the warping function as a composition of a\nnumber of simple injective warping functions in a deep-learning framework.\nImportantly, covariance-model validity is guaranteed by construction. We\nestablish the types of warpings that allow for cross-covariance symmetry and\nasymmetry, and we use likelihood-based methods for inference that are\ncomputationally efficient. The utility of this new class of models is shown\nthrough two data illustrations: a simulation study on nonstationary data and an\napplication on ocean temperatures at two different depths.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 22:58:24 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 00:23:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Vu", "Quan", ""], ["Zammit-Mangion", "Andrew", ""], ["Cressie", "Noel", ""]]}, {"id": "2004.08807", "submitter": "Jere Koskela", "authors": "Jere Koskela", "title": "Zig-zag sampling for discrete structures and non-reversible phylogenetic\n  MCMC", "comments": "21 pages, 10 figures, 3 tables. This is a major revision which\n  introduces a generic zig-zag process for a hybrid target with both discrete\n  and continuous parameters. Applications to the coalescent from earlier\n  versions are retained as examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST q-bio.PE stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a zig-zag process targeting a posterior distribution defined on\na hybrid state space consisting of both discrete and continuous variables. The\nconstruction does not require any assumptions on the structure among discrete\nvariables. We demonstrate our method on two examples in genetics based on the\nKingman coalescent, showing that the zig-zag process can lead to efficiency\ngains of up to several orders of magnitude over classical Metropolis--Hastings\nalgorithms, and that it is well suited to parallel computation. Our\nconstruction resembles existing techniques for Hamiltonian Monte Carlo on a\nhybrid state space, which suffers from implementationally and analytically\ncomplex boundary crossings when applied to the coalescent. We demonstrate that\nthe continuous-time zig-zag process avoids these complications.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 10:30:34 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 14:19:59 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 15:14:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Koskela", "Jere", ""]]}, {"id": "2004.08889", "submitter": "Indranil SenGupta", "authors": "Michael Roberts, Indranil SenGupta", "title": "Sequential hypothesis testing in machine learning, and crude oil price\n  jump size detection", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.MF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a sequential hypothesis test for the detection of\ngeneral jump size distrubution. Infinitesimal generators for the corresponding\nlog-likelihood ratios are presented and analyzed. Bounds for infinitesimal\ngenerators in terms of super-solutions and sub-solutions are computed. This is\nshown to be implementable in relation to various classification problems for a\ncrude oil price data set. Machine and deep learning algorithms are implemented\nto extract a specific deterministic component from the crude oil data set, and\nthe deterministic component is implemented to improve the Barndorff-Nielsen and\nShephard model, a commonly used stochastic model for derivative and commodity\nmarket analysis.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 16:02:08 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 19:20:37 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 15:51:02 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Roberts", "Michael", ""], ["SenGupta", "Indranil", ""]]}, {"id": "2004.08935", "submitter": "Robert Lunde", "authors": "Qiaohui Lin, Robert Lunde, Purnamrita Sarkar", "title": "On the Theoretical Properties of the Network Jackknife", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of a leave-node-out jackknife procedure for network\ndata. Under the sparse graphon model, we prove an Efron-Stein-type inequality,\nshowing that the network jackknife leads to conservative estimates of the\nvariance (in expectation) for any network functional that is invariant to node\npermutation. For a general class of count functionals, we also establish\nconsistency of the network jackknife. We complement our theoretical analysis\nwith a range of simulated and real-data examples and show that the network\njackknife offers competitive performance in cases where other resampling\nmethods are known to be valid. In fact, for several network statistics, we see\nthat the jackknife provides more accurate inferences compared to related\nmethods such as subsampling.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 19:03:32 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 08:53:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lin", "Qiaohui", ""], ["Lunde", "Robert", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "2004.08950", "submitter": "Chan Park", "authors": "Chan Park and Hyunseung Kang", "title": "Efficient Semiparametric Estimation of Network Treatment Effects Under\n  Partial Interference", "comments": "19 page main paper, 2 figures, 71 page supplement, 6 page reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interest in causal inference to study treatment\neffects under interference. While many estimators have been proposed, there is\nlittle work on studying efficiency-related optimality properties of these\nestimators. To this end, the paper presents semiparametrically efficient and\ndoubly robust estimation of network treatment effects under interference. We\nfocus on partial interference where study units are partitioned into\nnon-overlapping clusters and there is interference within clusters, but not\nacross clusters. We derive the efficient influence function and the\nsemiparametric efficiency bound for a family of network causal effects that\ninclude the direct and the indirect/spillover effects. We also adapt\nM-estimation theory to interference settings and propose M-estimators which are\nlocally efficient and doubly robust. We conclude by presenting some limited\nresults on adaptive estimation to interference patterns, or commonly referred\nto as exposure mapping.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 20:04:40 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 15:48:28 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Park", "Chan", ""], ["Kang", "Hyunseung", ""]]}, {"id": "2004.08978", "submitter": "Jacobo De U\\~na-\\'Alvarez", "authors": "Jacobo de U\\~na-\\'Alvarez", "title": "R packages for the statistical analysis of doubly truncated data: a\n  review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random double truncation refers a situation in which the variable of interest\nis observed only when it falls within two random limits. Such phenomenon occurs\nin many applications of Survival Analysis and Epidemiology, among many other\nfields. There exist several R packages to analyze doubly truncated data which\nimplement, for instance, estimators for the cumulative distribution, the\ncumulative incidences of competing risks, or the regression coefficients in the\nCox model. In this paper the main features of these packages are reviewed. The\nrelative merits of the libraries are illustrated through the analysis of\nsimulated and real data. This includes the study of the statistical accuracy of\nthe implemented techniques as well as of their computational speed. Practical\nrecommendations are given.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 22:38:21 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["de U\u00f1a-\u00c1lvarez", "Jacobo", ""]]}, {"id": "2004.09017", "submitter": "Qiao Liu", "authors": "Qiao Liu, Jiaze Xu, Rui Jiang, Wing Hung Wong", "title": "Roundtrip: A Deep Generative Neural Density Estimator", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences, 2021, 118(15)", "doi": "10.1073/pnas.2101344118", "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density estimation is a fundamental problem in both statistics and machine\nlearning. In this study, we proposed Roundtrip as a general-purpose neural\ndensity estimator based on deep generative models. Roundtrip retains the\ngenerative power of generative adversarial networks (GANs) but also provides\nestimates of density values. Unlike previous neural density estimators that put\nstringent conditions on the transformation from the latent space to the data\nspace, Roundtrip enables the use of much more general mappings. In a series of\nexperiments, Roundtrip achieves state-of-the-art performance in a diverse range\nof density estimation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 01:47:00 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 02:30:16 GMT"}, {"version": "v3", "created": "Sun, 10 May 2020 06:30:26 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2020 07:17:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liu", "Qiao", ""], ["Xu", "Jiaze", ""], ["Jiang", "Rui", ""], ["Wong", "Wing Hung", ""]]}, {"id": "2004.09018", "submitter": "Yong He", "authors": "Yong He, Pengfei Liu, Xinsheng Zhang, and Wang Zhou", "title": "Robust Covariance Estimation for High-dimensional Compositional Data\n  with Application to Microbial Communities Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbial communities analysis is drawing growing attention due to the rapid\ndevelopment of high-throughput sequencing techniques nowadays. The observed\ndata has the following typical characteristics: it is high-dimensional,\ncompositional (lying in a simplex) and even would be leptokurtic and highly\nskewed due to the existence of overly abundant taxa, which makes the\nconventional correlation analysis infeasible to study the co-occurrence and\nco-exclusion relationship between microbial taxa. In this article, we address\nthe challenges of covariance estimation for this kind of data. Assuming the\nbasis covariance matrix lying in a well-recognized class of sparse covariance\nmatrices, we adopt a proxy matrix known as centered log-ratio covariance matrix\nin the literature, which is approximately indistinguishable from the real basis\ncovariance matrix as the dimensionality tends to infinity. We construct a\nMedian-of-Means (MOM) estimator for the centered log-ratio covariance matrix\nand propose a thresholding procedure that is adaptive to the variability of\nindividual entries. By imposing a much weaker finite fourth moment condition\ncompared with the sub-Gaussianity condition in the literature, we derive the\noptimal rate of convergence under the spectral norm. In addition, we also\nprovide theoretical guarantee on support recovery. The adaptive thresholding\nprocedure of the MOM estimator is easy to implement and gains robustness when\noutliers or heavy-tailedness exist. Thorough simulation studies are conducted\nto show the advantages of the proposed procedure over some state-of-the-arts\nmethods. At last, we apply the proposed method to analyze a microbiome dataset\nin human gut. The R script for implementing the method is available at\nhttps://github.com/heyongstat/RCEC.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 01:51:42 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 04:22:29 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["He", "Yong", ""], ["Liu", "Pengfei", ""], ["Zhang", "Xinsheng", ""], ["Zhou", "Wang", ""]]}, {"id": "2004.09065", "submitter": "Luis Martinez Lomeli", "authors": "Luis Martinez Lomeli, Abdon Iniguez, Babak Shahbaba, John S Lowengrub,\n  Vladimir Minin", "title": "Optimal Experimental Design for Mathematical Models of Hematopoiesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hematopoietic system has a highly regulated and complex structure in\nwhich cells are organized to successfully create and maintain new blood cells.\nFeedback regulation is crucial to tightly control this system, but the specific\nmechanisms by which control is exerted are not completely understood. In this\nwork, we aim to uncover the underlying mechanisms in hematopoiesis by\nconducting perturbation experiments, where animal subjects are exposed to an\nexternal agent in order to observe the system response and evolution.\nDeveloping a proper experimental design for these studies is an extremely\nchallenging task. To address this issue, we have developed a novel Bayesian\nframework for optimal design of perturbation experiments. We model the numbers\nof hematopoietic stem and progenitor cells in mice that are exposed to a low\ndose of radiation. We use a differential equations model that accounts for\nfeedback and feedforward regulation. A significant obstacle is that the\nexperimental data are not longitudinal, rather each data point corresponds to a\ndifferent animal. This model is embedded in a hierarchical framework with\nlatent variables that capture unobserved cellular population levels. We select\nthe optimum design based on the amount of information gain, measured by the\nKullback-Leibler divergence between the probability distributions before and\nafter observing the data. We evaluate our approach using synthetic and\nexperimental data. We show that a proper design can lead to better estimates of\nmodel parameters even with relatively few subjects. Additionally, we\ndemonstrate that the model parameters show a wide range of sensitivities to\ndesign options. Our method should allow scientists to find the optimal design\nby focusing on their specific parameters of interest and provide insight to\nhematopoiesis. Our approach can be extended to more complex models where latent\ncomponents are used.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 05:44:28 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 03:16:58 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Lomeli", "Luis Martinez", ""], ["Iniguez", "Abdon", ""], ["Shahbaba", "Babak", ""], ["Lowengrub", "John S", ""], ["Minin", "Vladimir", ""]]}, {"id": "2004.09161", "submitter": "Ke Zhu", "authors": "Mengya Liu, Fukan Zhu, Ke Zhu", "title": "Multi-frequency-band tests for white noise under heteroskedasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new family of multi-frequency-band (MFB) tests for the\nwhite noise hypothesis by using the maximum overlap discrete wavelet packet\ntransform (MODWPT). The MODWPT allows the variance of a process to be\ndecomposed into the variance of its components on different equal-length\nfrequency sub-bands, and the MFB tests then measure the distance between the\nMODWPT-based variance ratio and its theoretical null value jointly over several\nfrequency sub-bands. The resulting MFB tests have the chi-squared asymptotic\nnull distributions under mild conditions, which allow the data to be\nheteroskedastic. The MFB tests are shown to have the desirable size and power\nperformance by simulation studies, and their usefulness is further illustrated\nby two applications.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:40:58 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Liu", "Mengya", ""], ["Zhu", "Fukan", ""], ["Zhu", "Ke", ""]]}, {"id": "2004.09306", "submitter": "Xuan Cao", "authors": "Xuan Cao and Kyoungjae Lee", "title": "Joint Bayesian Variable and DAG Selection Consistency for\n  High-dimensional Regression Models with Network-structured Covariates", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202019.0202", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the joint sparse estimation of regression coefficients and the\ncovariance matrix for covariates in a high-dimensional regression model, where\nthe predictors are both relevant to a response variable of interest and\nfunctionally related to one another via a Gaussian directed acyclic graph (DAG)\nmodel. Gaussian DAG models introduce sparsity in the Cholesky factor of the\ninverse covariance matrix, and the sparsity pattern in turn corresponds to\nspecific conditional independence assumptions on the underlying predictors. A\nvariety of methods have been developed in recent years for Bayesian inference\nin identifying such network-structured predictors in regression setting, yet\ncrucial sparsity selection properties for these models have not been thoroughly\ninvestigated. In this paper, we consider a hierarchical model with spike and\nslab priors on the regression coefficients and a flexible and general class of\nDAG-Wishart distributions with multiple shape parameters on the Cholesky\nfactors of the inverse covariance matrix. Under mild regularity assumptions, we\nestablish the joint selection consistency for both the variable and the\nunderlying DAG of the covariates when the dimension of predictors is allowed to\ngrow much larger than the sample size. We demonstrate that our method\noutperforms existing methods in selecting network-structured predictors in\nseveral simulation settings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:00:17 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Cao", "Xuan", ""], ["Lee", "Kyoungjae", ""]]}, {"id": "2004.09335", "submitter": "Filip Tronarp", "authors": "Filip Tronarp and Simo S\\\"arkk\\\"a", "title": "Continuous-Discrete Filtering and Smoothing on Submanifolds of Euclidean\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the issue of filtering and smoothing in continuous discrete\ntime is studied when the state variable evolves in some submanifold of\nEuclidean space, which may not have the usual Lebesgue measure. Formal\nexpressions for prediction and smoothing problems are derived, which agree with\nthe classical results except that the formal adjoint of the generator is\ndifferent in general. For approximate filtering and smoothing the projection\napproach is taken, where it turns out that the prediction and smoothing\nequations are the same as in the case when the state variable evolves in\nEuclidean space. The approach is used to develop projection filters and\nsmoothers based on the von Mises-Fisher distribution.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:36:06 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Tronarp", "Filip", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2004.09366", "submitter": "Marco Ballin", "authors": "Marco Ballin, Giulio Barcaroli", "title": "R package SamplingStrata: new developments and extension to Spatial\n  Sampling", "comments": "28 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package SamplingStrata was developed in 2011 as an instrument to\noptimize the design of stratified samples. The optimization is performed by\nconsidering the stratification variables available in the sampling frame, and\nthe precision constraints on target estimates of the survey (Ballin &\nBarcaroli, 2014). The genetic algorithm at the basis of the optimization step\nexplores the universe of the possible alternative stratifications determining\nfor each of them the best allocation, that is the one of minumum total size\nthat allows to satisfy the precision constraints: the final optimal solution is\nthe one that ensures the global minimum sample size. One fundamental\nrequirement to make this approach feasible is the possibility to estimate the\nvariability of target variables in generated strata; in general, as target\nvariable values are not available in the frame, but only proxy ones,\nanticipated variance is calculated by modelling the relations between target\nand proxy variables. In case of spatial sampling, it is important to consider\nnot only the total model variance, but also the co-variance derived by the\nspatial auto-correlation. The last release of SamplingStrata enables to\nconsider both components of variance, thus allowing to harness spatial\nauto-correlation in order to obtain more efficient samples.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:11:53 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ballin", "Marco", ""], ["Barcaroli", "Giulio", ""]]}, {"id": "2004.09455", "submitter": "Sarah Elizabeth Heaps", "authors": "Sarah E. Heaps", "title": "Enforcing stationarity through the prior in vector autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationarity is a very common assumption in time series analysis. A vector\nautoregressive (VAR) process is stationary if and only if the roots of its\ncharacteristic equation lie outside the unit circle, constraining the\nautoregressive coefficient matrices to lie in the stationary region. However,\nthe stationary region has a highly complex geometry which impedes specification\nof a prior distribution. In this work, an unconstrained reparameterisation of a\nstationary VAR model is presented. The new parameters are based on partial\nautocorrelation matrices, which are interpretable, and can be transformed\nbijectively to the space of unconstrained square matrices. This transformation\npreserves various structural forms of the partial autocorrelation matrices and\nreadily facilitates specification of a prior. Properties of this prior are\ndescribed along with an important special case which is exchangeable with\nrespect to the order of the elements in the observation vector. Posterior\ninference and computation are described and implemented using Hamiltonian Monte\nCarlo via Stan. The prior and inferential procedures are illustrated with an\napplication to a macroeconomic time series which highlights the benefits of\nenforcing stationarity.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:11:58 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Heaps", "Sarah E.", ""]]}, {"id": "2004.09458", "submitter": "Stefan Wager", "authors": "Dean Eckles, Nikolaos Ignatiadis, Stefan Wager, Han Wu", "title": "Noise-Induced Randomization in Regression Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression discontinuity designs are used to estimate causal effects in\nsettings where treatment is determined by whether an observed running variable\ncrosses a pre-specified threshold. While the resulting sampling design is\nsometimes described as akin to a locally randomized experiment in a\nneighborhood of the threshold, standard formal analyses do not make reference\nto probabilistic treatment assignment and instead identify treatment effects\nvia continuity arguments. Here we propose a new approach to identification,\nestimation, and inference in regression discontinuity designs that exploits\nmeasurement error in the running variable. Under an assumption that the\nmeasurement error is exogenous, we show how to consistently estimate causal\neffects using a class of linear estimators that weight treated and control\nunits so as to balance a latent variable of which the running variable is a\nnoisy measure. We find this approach to facilitate identification of both\nfamiliar estimands from the literature, as well as policy-relevant estimands\nthat correspond to the effects of realistic changes to the existing treatment\nassignment rule. We demonstrate the method with a study of retention of HIV\npatients and evaluate its performance using simulated data and a regression\ndiscontinuity design artificially constructed from test scores in early\nchildhood.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:24:38 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 16:58:49 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Eckles", "Dean", ""], ["Ignatiadis", "Nikolaos", ""], ["Wager", "Stefan", ""], ["Wu", "Han", ""]]}, {"id": "2004.09588", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay, Kaijun Wang", "title": "On The Problem of Relevance in Statistical Inference", "comments": "Revised (much-improved) version. The procedure (including all the\n  datasets) is implemented in the R-package LPRelevance", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is dedicated to the \"50 Years of the Relevance Problem\" - a\nlong-neglected topic that begs attention from practical statisticians who are\nconcerned with the problem of drawing inference from large-scale heterogeneous\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 19:31:00 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 00:32:32 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 18:24:47 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Wang", "Kaijun", ""]]}, {"id": "2004.09623", "submitter": "Bryan W. Ting", "authors": "Bryan W. Ting, Fred A. Wright, Yi-Hui Zhou", "title": "Fast Multivariate Probit Estimation via a Two-Stage Composite Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate probit is popular for modeling correlated binary data, with\nan attractive balance of flexibility and simplicity. However, considerable\nchallenges remain in computation and in devising a clear statistical framework.\nInterest in the multivariate probit has increased in recent years. Current\napplications include genomics and precision medicine, where simultaneous\nmodeling of multiple traits may be of interest, and computational efficiency is\nan important consideration. We propose a fast method for multivariate probit\nestimation via a two-stage composite likelihood. We explore computational and\nstatistical efficiency, and note that the approach sets the stage for\nextensions beyond the purely binary setting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:37:16 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Ting", "Bryan W.", ""], ["Wright", "Fred A.", ""], ["Zhou", "Yi-Hui", ""]]}, {"id": "2004.09709", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao, Peter Bickel and Charles Weko", "title": "Identifiability and consistency of network inference using the hub model\n  and variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical network analysis primarily focuses on inferring the parameters of\nan observed network. In many applications, especially in the social sciences,\nthe observed data is the groups formed by individual subjects. In these\napplications, the network is itself a parameter of a statistical model. Zhao\nand Weko (2019) propose a model-based approach, called the hub model, to infer\nimplicit networks from grouping behavior. The hub model assumes that each\nmember of the group is brought together by a member of the group called the\nhub. The hub model belongs to the family of Bernoulli mixture models.\nIdentifiability of parameters is a notoriously difficult problem for Bernoulli\nmixture models. This paper proves identifiability of the hub model parameters\nand estimation consistency under mild conditions. Furthermore, this paper\ngeneralizes the hub model by introducing a model component that allows hubless\ngroups in which individual nodes spontaneously appear independent of any other\nindividual. We refer to this additional component as the null component. The\nnew model bridges the gap between the hub model and the degenerate case of the\nmixture model -- the Bernoulli product. Identifiability and consistency are\nalso proved for the new model. Numerical studies are provided to demonstrate\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:01:00 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 17:10:21 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 23:52:37 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zhao", "Yunpeng", ""], ["Bickel", "Peter", ""], ["Weko", "Charles", ""]]}, {"id": "2004.09770", "submitter": "Yeonwoo Rho", "authors": "Yeonwoo Rho, Yun Liu, and Hie Joo Ahn", "title": "Revealing Cluster Structures Based on Mixed Sampling Frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new linearized mixed data sampling (MIDAS) model and\ndevelops a framework to infer clusters in a panel regression with mixed\nfrequency data. The linearized MIDAS estimation method is more flexible and\nsubstantially simpler to implement than competing approaches. We show that the\nproposed clustering algorithm successfully recovers true membership in the\ncross-section, both in theory and in simulations, without requiring prior\nknowledge of the number of clusters. This methodology is applied to a\nmixed-frequency Okun's law model for state-level data in the U.S. and uncovers\nfour meaningful clusters based on the dynamic features of state-level labor\nmarkets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 06:20:15 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 21:32:29 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Rho", "Yeonwoo", ""], ["Liu", "Yun", ""], ["Ahn", "Hie Joo", ""]]}, {"id": "2004.09899", "submitter": "Joris Mulder", "authors": "J. Mulder, E.-J. Wagenmakers, and M. Marsman", "title": "A Generalization of the Savage-Dickey Density Ratio for Testing Equality\n  and Order Constrained Hypotheses", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Savage-Dickey density ratio is a specific expression of the Bayes factor\nwhen testing a precise (equality constrained) hypothesis against an\nunrestricted alternative. The expression greatly simplifies the computation of\nthe Bayes factor at the cost of assuming a specific form of the prior under the\nprecise hypothesis as a function of the unrestricted prior. A generalization\nwas proposed by Verdinelli and Wasserman (1995) such that the priors can be\nfreely specified under both hypotheses while keeping the computational\nadvantage. This paper presents an extension of this generalization when the\nhypothesis has equality as well as order constraints on the parameters of\ninterest. The methodology is used for a constrained multivariate t test using\nthe JZS Bayes factor and a constrained hypothesis test under the multinomial\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 10:51:17 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 14:06:14 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mulder", "J.", ""], ["Wagenmakers", "E. -J.", ""], ["Marsman", "M.", ""]]}, {"id": "2004.10075", "submitter": "Shuxi Zeng", "authors": "Shuxi Zeng, Fan Li, Rui Wang, Fan Li", "title": "Propensity Score Weighting for Covariate Adjustment in Randomized\n  Clinical Trials", "comments": "18 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chance imbalance in baseline characteristics is common in randomized clinical\ntrials. Regression adjustment such as the analysis of covariance (ANCOVA) is\noften used to account for imbalance and increase precision of the treatment\neffect estimate. An objective alternative is through inverse probability\nweighting (IPW) of the propensity scores. Although IPW and ANCOVA are\nasymptotically equivalent, the former may demonstrate inferior performance in\nfinite samples. In this article, we point out that IPW is a special case of the\ngeneral class of balancing weights, and advocate to use overlap weighting (OW)\nfor covariate adjustment. The OW method has a unique advantage of completely\nremoving chance imbalance when the propensity score is estimated by logistic\nregression. We show that the OW estimator attains the same semiparametric\nvariance lower bound as the most efficient ANCOVA estimator and the IPW\nestimator for a continuous outcome, and derive closed-form variance estimators\nfor OW when estimating additive and ratio estimands. Through extensive\nsimulations, we demonstrate OW consistently outperforms IPW in finite samples\nand improves the efficiency over ANCOVA and augmented IPW when the degree of\ntreatment effect heterogeneity is moderate or when the outcome model is\nincorrectly specified. We apply the proposed OW estimator to the Best Apnea\nInterventions for Research (BestAIR) randomized trial to evaluate the effect of\ncontinuous positive airway pressure on patient health outcomes. All the\ndiscussed propensity score weighting methods are implemented in the R package\nPSweight.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:02:08 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 19:03:38 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zeng", "Shuxi", ""], ["Li", "Fan", ""], ["Wang", "Rui", ""], ["Li", "Fan", ""]]}, {"id": "2004.10108", "submitter": "Haomiao Meng", "authors": "Haomiao Meng, Xingye Qiao", "title": "Doubly Robust Direct Learning for Estimating Conditional Average\n  Treatment Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the heterogeneous treatment effect is a fundamental problem in the\nsciences and commercial applications. In this paper, we focus on estimating\nConditional Average Treatment Effect (CATE), that is, the difference in the\nconditional mean outcome between treatments given covariates. Traditionally,\nQ-Learning based approaches rely on the estimation of conditional mean outcome\ngiven treatment and covariates. However, they are subject to misspecification\nof the main effect model. Recently, simple and flexible one-step methods to\ndirectly learn (D-Learning) the CATE without model specifications have been\nproposed. However, these methods are not robust against misspecification of the\npropensity score model. We propose a new framework for CATE estimation, robust\ndirect learning (RD-Learning), leading to doubly robust estimators of the\ntreatment effect. The consistency for our CATE estimator is guaranteed if\neither the main effect model or the propensity score model is correctly\nspecified. The framework can be used in both the binary and the multi-arm\nsettings and is general enough to allow different function spaces and\nincorporate different generic learning algorithms. As a by-product, we develop\na competitive statistical inference tool for the treatment effect, assuming the\npropensity score is known. We provide theoretical insights to the proposed\nmethod using risk bounds under both linear and non-linear settings. The\neffectiveness of our proposed method is demonstrated by simulation studies and\na real data example about an AIDS Clinical Trials study.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:36:50 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 21:12:32 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Meng", "Haomiao", ""], ["Qiao", "Xingye", ""]]}, {"id": "2004.10181", "submitter": "Suyash Gupta", "authors": "Maxime Cauchois and Suyash Gupta and John Duchi", "title": "Knowing what you know: valid and validated confidence sets in multiclass\n  and multilabel prediction", "comments": "Updated section on multilabel settings addressing the cases when\n  labels may repel each other", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop conformal prediction methods for constructing valid predictive\nconfidence sets in multiclass and multilabel problems without assumptions on\nthe data generating distribution. A challenge here is that typical conformal\nprediction methods---which give marginal validity (coverage)\nguarantees---provide uneven coverage, in that they address easy examples at the\nexpense of essentially ignoring difficult examples. By leveraging ideas from\nquantile regression, we build methods that always guarantee correct coverage\nbut additionally provide (asymptotically optimal) conditional coverage for both\nmulticlass and multilabel prediction problems. To address the potential\nchallenge of exponentially large confidence sets in multilabel prediction, we\nbuild tree-structured classifiers that efficiently account for interactions\nbetween labels. Our methods can be bolted on top of any classification\nmodel---neural network, random forest, boosted tree---to guarantee its\nvalidity. We also provide an empirical evaluation, simultaneously providing new\nvalidation methods, that suggests the more robust coverage of our confidence\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:45:38 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 22:53:23 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 18:22:12 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Cauchois", "Maxime", ""], ["Gupta", "Suyash", ""], ["Duchi", "John", ""]]}, {"id": "2004.10231", "submitter": "Wenjia Wang", "authors": "Wenjia Wang, Yi-Hui Zhou", "title": "Eigenvector-based sparse canonical correlation analysis: Fast\n  computation for estimation of multiple canonical vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical canonical correlation analysis (CCA) requires matrices to be low\ndimensional, i.e. the number of features cannot exceed the sample size. Recent\ndevelopments in CCA have mainly focused on the high-dimensional setting, where\nthe number of features in both matrices under analysis greatly exceeds the\nsample size. These approaches impose penalties in the optimization problems\nthat are needed to be solve iteratively, and estimate multiple canonical\nvectors sequentially. In this work, we provide an explicit link between sparse\nmultiple regression with sparse canonical correlation analysis, and an\nefficient algorithm that can estimate multiple canonical pairs simultaneously\nrather than sequentially. Furthermore, the algorithm naturally allows parallel\ncomputing. These properties make the algorithm much efficient. We provide\ntheoretical results on the consistency of canonical pairs. The algorithm and\ntheoretical development are based on solving an eigenvectors problem, which\nsignificantly differentiate our method with existing methods. Simulation\nresults support the improved performance of the proposed approach. We apply\neigenvector-based CCA to analysis of the GTEx thyroid histology images,\nanalysis of SNPs and RNA-seq gene expression data, and a microbiome study. The\nreal data analysis also shows improved performance compared to traditional\nsparse CCA.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 18:32:28 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:10:19 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Wenjia", ""], ["Zhou", "Yi-Hui", ""]]}, {"id": "2004.10264", "submitter": "Benjamin Nguyen-Van-Yen", "authors": "Benjamin Nguyen-Van-Yen, Pierre Del Moral, Bernard Cazelles", "title": "Stochastic Epidemic Models inference and diagnosis with Poisson Random\n  Measure Data Augmentation", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Bayesian inference method for compartmental models that\ntakes into account the intrinsic stochasticity of the process. We show how to\nformulate a SIR-type Markov jump process as the solution of a stochastic\ndifferential equation with respect to a Poisson Random Measure (PRM), and how\nto simulate the process trajectory deterministically from a parameter value and\na PRM realisation.\n  This forms the basis of our Data Augmented MCMC, which consists in augmenting\nparameter space with the unobserved PRM value. The resulting simple\nMetropolis-Hastings sampler acts as an efficient simulation-based inference\nmethod, that can easily be transferred from model to model. Compared with a\nrecent Data Augmentation method based on Gibbs sampling of individual infection\nhistories, PRM-augmented MCMC scales much better with epidemic size and is far\nmore flexible.\n  PRM-augmented MCMC also yields a posteriori estimates of the PRM, that\nrepresent process stochasticity, and which can be used to validate the model.\nIf the model is good, the posterior distribution should exhibit no pattern and\nbe close to the PRM prior distribution. We illustrate this by fitting a\nnon-seasonal model to some simulated seasonal case count data. Applied to the\nZika epidemic of 2013 in French Polynesia, our approach shows that a simple\nSEIR model cannot correctly reproduce both the initial sharp increase in the\nnumber of cases as well as the final proportion of seropositive.\n  PRM-augmentation thus provides a coherent story for Stochastic Epidemic Model\ninference, where explicitly inferring process stochasticity helps with model\nvalidation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 19:58:05 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Nguyen-Van-Yen", "Benjamin", ""], ["Del Moral", "Pierre", ""], ["Cazelles", "Bernard", ""]]}, {"id": "2004.10271", "submitter": "Xiaoxiao Sun", "authors": "Xiaoxiao Sun, Wenxuan Zhong, Ping Ma", "title": "An Asympirical Smoothing Parameters Selection Approach for Smoothing\n  Spline ANOVA Models in Large Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large samples have been generated routinely from various sources. Classic\nstatistical models, such as smoothing spline ANOVA models, are not well\nequipped to analyze such large samples due to expensive computational costs. In\nparticular, the daunting computational costs of selecting smoothing parameters\nrender smoothing spline ANOVA models impractical. In this article, we develop\nan asympirical, i.e., asymptotic and empirical, smoothing parameters selection\napproach for smoothing spline ANOVA models in large samples. The idea of this\napproach is to use asymptotic analysis to show that the optimal smoothing\nparameter is a polynomial function of the sample size and an unknown constant.\nThe unknown constant is then estimated through empirical subsample\nextrapolation. The proposed method significantly reduces the computational\ncosts of selecting smoothing parameters in high-dimensional and large samples.\nWe show smoothing parameters chosen by the proposed method tend to the optimal\nsmoothing parameters that minimise a specific risk function. In addition, the\nestimator based on the proposed smoothing parameters achieves the optimal\nconvergence rate. Extensive simulation studies demonstrate the numerical\nadvantage of the proposed method over competing methods in terms of relative\nefficacies and running time. On an application to molecular dynamics data with\nnearly one million observations, the proposed method has the best prediction\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:06:08 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sun", "Xiaoxiao", ""], ["Zhong", "Wenxuan", ""], ["Ma", "Ping", ""]]}, {"id": "2004.10337", "submitter": "Paul Zivich", "authors": "Paul N Zivich, Alexander Breskin", "title": "Machine learning for causal inference: on the use of cross-fit\n  estimators", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": "10.1097/EDE.0000000000001332", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern causal inference methods allow machine learning to be used to weaken\nparametric modeling assumptions. However, the use of machine learning may\nresult in complications for inference. Doubly-robust cross-fit estimators have\nbeen proposed to yield better statistical properties.\n  We conducted a simulation study to assess the performance of several\ndifferent estimators for the average causal effect (ACE). The data generating\nmechanisms for the simulated treatment and outcome included log-transforms,\npolynomial terms, and discontinuities. We compared singly-robust estimators\n(g-computation, inverse probability weighting) and doubly-robust estimators\n(augmented inverse probability weighting, targeted maximum likelihood\nestimation). Nuisance functions were estimated with parametric models and\nensemble machine learning, separately. We further assessed doubly-robust\ncross-fit estimators.\n  With correctly specified parametric models, all of the estimators were\nunbiased and confidence intervals achieved nominal coverage. When used with\nmachine learning, the doubly-robust cross-fit estimators substantially\noutperformed all of the other estimators in terms of bias, variance, and\nconfidence interval coverage.\n  Due to the difficulty of properly specifying parametric models in high\ndimensional data, doubly-robust estimators with ensemble learning and\ncross-fitting may be the preferred approach for estimation of the ACE in most\nepidemiologic studies. However, these approaches may require larger sample\nsizes to avoid finite-sample issues.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 23:09:55 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 15:40:04 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 15:21:23 GMT"}, {"version": "v4", "created": "Fri, 28 Aug 2020 19:30:14 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zivich", "Paul N", ""], ["Breskin", "Alexander", ""]]}, {"id": "2004.10814", "submitter": "Charlotte Micheloud", "authors": "Charlotte Micheloud and Leonhard Held", "title": "Power Calculations for Replication Studies", "comments": "28 pages, 4 figures (+ 1 in Appendix), 3 tables (+2 in Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reproducibility crisis has led to an increasing number of replication\nstudies being conducted. Sample sizes for replication studies are often\ncalculated using conditional power based on the effect estimate from the\noriginal study. However, this approach is not well suited as it ignores the\nuncertainty of the original result. Bayesian methods are used in clinical\ntrials to incorporate prior information into power calculations. We propose to\nadapt this methodology to the replication framework and promote the use of\npredictive instead of conditional power in the design of replication studies.\nMoreover, we describe how extensions of the methodology to sequential clinical\ntrials can be tailored to replication studies. Conditional and predictive power\ncalculated at an interim analysis are compared and we argue that predictive\npower is a useful tool to decide whether to stop a replication study\nprematurely. A recent project on the replicability of social sciences is used\nto illustrate the properties of the different methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:31:17 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:08:04 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 07:57:09 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 12:37:17 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Micheloud", "Charlotte", ""], ["Held", "Leonhard", ""]]}, {"id": "2004.10818", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus, Arnold Janssen and Markus Pauly", "title": "Permutation inference in factorial survival designs with the CASANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose inference procedures for general nonparametric factorial survival\ndesigns with possibly right-censored data. Similar to additive Aalen models,\nnull hypotheses are formulated in terms of cumulative hazards. Thereby,\ndeviations are measured in terms of quadratic forms in Nelson-Aalen-type\nintegrals. Different to existing approaches this allows to work without\nrestrictive model assumptions as proportional hazards. In particular, crossing\nsurvival or hazard curves can be detected without a significant loss of power.\nFor a distribution-free application of the method, a permutation strategy is\nsuggested. The resulting procedures' asymptotic validity as well as their\nconsistency are proven and their small sample performances are analyzed in\nextensive simulations. Their applicability is finally illustrated by analyzing\nan oncology data set.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:35:44 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 11:46:53 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Janssen", "Arnold", ""], ["Pauly", "Markus", ""]]}, {"id": "2004.11193", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Pietro Spitali, Roula Tsonaka", "title": "Poisson-Tweedie mixed-effects model: a flexible approach for the\n  analysis of longitudinal RNA-seq data", "comments": "The final (published) version of the article can be downloaded for\n  free (Open Access) from the editor's website (click on the DOI link below).\n  Link to the R package ptmixed:\n  https://cran.r-project.org/web/packages/ptmixed/index.html", "journal-ref": "Statistical Modelling (2020)", "doi": "10.1177/1471082X20936017", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new modelling approach for longitudinal count data that is\nmotivated by the increasing availability of longitudinal RNA-sequencing\nexperiments. The distribution of RNA-seq counts typically exhibits\noverdispersion, zero-inflation and heavy tails; moreover, in longitudinal\ndesigns repeated measurements from the same subject are typically (positively)\ncorrelated. We propose a generalized linear mixed model based on the\nPoisson-Tweedie distribution that can flexibly handle each of the\naforementioned features of longitudinal overdispersed counts. We develop a\ncomputational approach to accurately evaluate the likelihood of the proposed\nmodel and to perform maximum likelihood estimation. Our approach is implemented\nin the R package ptmixed, which can be freely downloaded from CRAN. We assess\nthe performance of ptmixed on simulated data and we present an application to a\ndataset with longitudinal RNA-sequencing measurements from healthy and\ndystrophic mice. The applicability of the Poisson-Tweedie mixed-effects model\nis not restricted to longitudinal RNA-seq data, but it extends to any scenario\nwhere non-independent measurements of a discrete overdispersed response\nvariable are available.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:38:02 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 08:13:01 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Signorelli", "Mirko", ""], ["Spitali", "Pietro", ""], ["Tsonaka", "Roula", ""]]}, {"id": "2004.11241", "submitter": "Hossein-Ali Mohtashami-Borzadaran", "authors": "H. A. Mohtashami-Borzadaran, M. Amini, H. Jabbari and A. Dolati", "title": "Marshall-Olkin exponential shock model covering all range of dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new Marshall-Olkin exponential shock model. The\nnew construction method gives the proposed model further ability to allocate\nthe common joint shock on each of the components, making it suitable for\napplication in fields like reliability and credit risk. The given model has a\nsingular part and supports both positive and negative dependence structure.\nMain dependence properties of the model is given and an analysis of\nstress-strength is presented. After a performance analysis on the estimator of\nparameters, a real data is studied. Finally, we give the multivariate version\nof the proposed model and its main properties.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 15:29:04 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 15:56:49 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Mohtashami-Borzadaran", "H. A.", ""], ["Amini", "M.", ""], ["Jabbari", "H.", ""], ["Dolati", "A.", ""]]}, {"id": "2004.11292", "submitter": "Mohamad Elmasri", "authors": "Mohamad Elmasri, Aurelie Labbe, Denis Larocque and Laurent Charlin", "title": "Predictive inference for travel time on transportation networks", "comments": "27 main pages, 4 figures and 3 tables. This version includes many\n  changes to the previous one", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent statistical methods fitted on large-scale GPS data are getting close\nto answering the proverbial \"When are we there?\" question. Unfortunately,\ncurrent methods often only provide point predictions for travel time.\nUnderstanding travel time distribution is key for decision-making and\ndownstream applications (e.g., ride share pricing decisions). Empirically,\nsingle road-segment travel time is well-studied, understanding how to aggregate\nsuch information over many segments to arrive at the distribution of travel\ntime over a route is challenging. We develop a novel statistical approach to\nthis problem, where we show that, under general conditions, without assuming a\ndistribution of speed, travel time normalized by distance follows a Gaussian\ndistribution with route-invariant population mean and variance. We develop\nefficient inference methods for such parameters, with which we propose\npopulation prediction intervals for travel time. Our population intervals are\nasymptotically tight and require only two parameter estimates. Using road-level\ninformation (e.g.~traffic density), we further develop a catered trips-specific\nGaussian-based predictive distribution, resulting in tight prediction intervals\nfor short and long trips. Our methods, implemented in an R-package, are\nillustrated in a real-world case study using mobile GPS data, showing that our\ntrip-specific and population intervals both achieve the 95\\% theoretical\ncoverage levels. Compared to alternative approaches, our trip-specific\npredictive distribution achieves (a) the theoretical coverage at every level of\nsignificance, (b) tighter prediction intervals, (c) less predictive bias, and\n(d) more efficient estimation and prediction procedures that only rely on the\nfirst and second moment estimates of speed on edges of the network. This makes\nour approach promising for low latency large-scale transportation applications.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 16:18:19 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 02:09:15 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 00:29:16 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Elmasri", "Mohamad", ""], ["Labbe", "Aurelie", ""], ["Larocque", "Denis", ""], ["Charlin", "Laurent", ""]]}, {"id": "2004.11342", "submitter": "Seth Flaxman", "authors": "Seth Flaxman, Swapnil Mishra, Axel Gandy, H Juliette T Unwin, Helen\n  Coupland, Thomas A Mellan, Harrison Zhu, Tresnia Berah, Jeffrey W Eaton,\n  Pablo N P Guzman, Nora Schmit, Lucia Callizo, Imperial College COVID-19\n  Response Team, Charles Whittaker, Peter Winskill, Xiaoyue Xi, Azra Ghani,\n  Christl A. Donnelly, Steven Riley, Lucy C Okell, Michaela A C Vollmer, Neil\n  M. Ferguson and Samir Bhatt", "title": "Estimating the number of infections and the impact of non-pharmaceutical\n  interventions on COVID-19 in European countries: technical description update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the emergence of a novel coronavirus (SARS-CoV-2) and its spread\noutside of China, Europe has experienced large epidemics. In response, many\nEuropean countries have implemented unprecedented non-pharmaceutical\ninterventions including case isolation, the closure of schools and\nuniversities, banning of mass gatherings and/or public events, and most\nrecently, wide-scale social distancing including local and national lockdowns.\n  In this technical update, we extend a semi-mechanistic Bayesian hierarchical\nmodel that infers the impact of these interventions and estimates the number of\ninfections over time. Our methods assume that changes in the reproductive\nnumber - a measure of transmission - are an immediate response to these\ninterventions being implemented rather than broader gradual changes in\nbehaviour. Our model estimates these changes by calculating backwards from\ntemporal data on observed to estimate the number of infections and rate of\ntransmission that occurred several weeks prior, allowing for a probabilistic\ntime lag between infection and death.\n  In this update we extend our original model [Flaxman, Mishra, Gandy et al\n2020, Report #13, Imperial College London] to include (a) population saturation\neffects, (b) prior uncertainty on the infection fatality ratio, (c) a more\nbalanced prior on intervention effects and (d) partial pooling of the lockdown\nintervention covariate. We also (e) included another 3 countries (Greece, the\nNetherlands and Portugal).\n  The model code is available at\nhttps://github.com/ImperialCollegeLondon/covid19model/\n  We are now reporting the results of our updated model online at\nhttps://mrc-ide.github.io/covid19estimates/\n  We estimated parameters jointly for all M=14 countries in a single\nhierarchical model. Inference is performed in the probabilistic programming\nlanguage Stan using an adaptive Hamiltonian Monte Carlo (HMC) sampler.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:38:23 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Flaxman", "Seth", ""], ["Mishra", "Swapnil", ""], ["Gandy", "Axel", ""], ["Unwin", "H Juliette T", ""], ["Coupland", "Helen", ""], ["Mellan", "Thomas A", ""], ["Zhu", "Harrison", ""], ["Berah", "Tresnia", ""], ["Eaton", "Jeffrey W", ""], ["Guzman", "Pablo N P", ""], ["Schmit", "Nora", ""], ["Callizo", "Lucia", ""], ["Team", "Imperial College COVID-19 Response", ""], ["Whittaker", "Charles", ""], ["Winskill", "Peter", ""], ["Xi", "Xiaoyue", ""], ["Ghani", "Azra", ""], ["Donnelly", "Christl A.", ""], ["Riley", "Steven", ""], ["Okell", "Lucy C", ""], ["Vollmer", "Michaela A C", ""], ["Ferguson", "Neil M.", ""], ["Bhatt", "Samir", ""]]}, {"id": "2004.11408", "submitter": "Paul-Christian B\\\"urkner", "authors": "Gabriel Riutort-Mayol, Paul-Christian B\\\"urkner, Michael R. Andersen,\n  Arno Solin, Aki Vehtari", "title": "Practical Hilbert space approximate Bayesian Gaussian processes for\n  probabilistic programming", "comments": "33 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are powerful non-parametric probabilistic models for\nstochastic functions. However they entail a complexity that is computationally\nintractable when the number of observations is large, especially when estimated\nwith fully Bayesian methods such as Markov chain Monte Carlo. In this paper, we\nfocus on a novel approach for low-rank approximate Bayesian Gaussian processes,\nbased on a basis function approximation via Laplace eigenfunctions for\nstationary covariance functions. The main contribution of this paper is a\ndetailed analysis of the performance and practical implementation of the method\nin relation to key factors such as the number of basis functions, domain of the\nprediction space, and smoothness of the latent function. We provide intuitive\nvisualizations and recommendations for choosing the values of these factors,\nwhich make it easier for users to improve approximation accuracy and\ncomputational performance. We also propose diagnostics for checking that the\nnumber of basis functions and the domain of the prediction space are adequate\ngiven the data. The proposed approach is simple and exhibits an attractive\ncomputational complexity due to its linear structure, and it is easy to\nimplement in probabilistic programming frameworks. Several illustrative\nexamples of the performance and applicability of the method in the\nprobabilistic programming language Stan are presented together with the\nunderlying Stan model code.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 18:07:24 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Riutort-Mayol", "Gabriel", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Andersen", "Michael R.", ""], ["Solin", "Arno", ""], ["Vehtari", "Aki", ""]]}, {"id": "2004.11470", "submitter": "Wagner Barreto-Souza", "authors": "Gisele O. Maia, Wagner Barreto-Souza, Fernando S. Bastos and Hernando\n  Ombao", "title": "Semiparametric time series models driven by latent factor", "comments": null, "journal-ref": "International Journal of Forecasting (2021)", "doi": "10.1016/j.ijforecast.2020.12.007", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of semiparametric time series models by assuming a\nquasi-likelihood approach driven by a latent factor process. More specifically,\ngiven the latent process, we only specify the conditional mean and variance of\nthe time series and enjoy a quasi-likelihood function for estimating parameters\nrelated to the mean. This proposed methodology has three remarkable features:\n(i) no parametric form is assumed for the conditional distribution of the time\nseries given the latent process; (ii) able for modelling non-negative, count,\nbounded/binary and real-valued time series; (iii) dispersion parameter is not\nassumed to be known. Further, we obtain explicit expressions for the marginal\nmoments and for the autocorrelation function of the time series process so that\na method of moments can be employed for estimating the dispersion parameter and\nalso parameters related to the latent process. Simulated results aiming to\ncheck the proposed estimation procedure are presented. Real data analysis on\nunemployment rate and precipitation time series illustrate the potencial for\npractice of our methodology.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 21:39:14 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Maia", "Gisele O.", ""], ["Barreto-Souza", "Wagner", ""], ["Bastos", "Fernando S.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2004.11485", "submitter": "Dimitris Korobilis Prof", "authors": "Dimitris Korobilis", "title": "High-dimensional macroeconomic forecasting using message passing\n  algorithms", "comments": "89 pages; to appear in Journal of Business and Economic Statistics", "journal-ref": null, "doi": "10.1080/07350015.2019.1677472", "report-no": null, "categories": "stat.ME econ.EM q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes two distinct contributions to econometric analysis of\nlarge information sets and structural instabilities. First, it treats a\nregression model with time-varying coefficients, stochastic volatility and\nexogenous predictors, as an equivalent high-dimensional static regression\nproblem with thousands of covariates. Inference in this specification proceeds\nusing Bayesian hierarchical priors that shrink the high-dimensional vector of\ncoefficients either towards zero or time-invariance. Second, it introduces the\nframeworks of factor graphs and message passing as a means of designing\nefficient Bayesian estimation algorithms. In particular, a Generalized\nApproximate Message Passing (GAMP) algorithm is derived that has low\nalgorithmic complexity and is trivially parallelizable. The result is a\ncomprehensive methodology that can be used to estimate time-varying parameter\nregressions with arbitrarily large number of exogenous predictors. In a\nforecasting exercise for U.S. price inflation this methodology is shown to work\nvery well.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 23:10:04 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Korobilis", "Dimitris", ""]]}, {"id": "2004.11532", "submitter": "Carlos Fern\\'andez-Lor\\'ia", "authors": "Carlos Fern\\'andez-Lor\\'ia, Foster Provost, Jesse Anderton, Benjamin\n  Carterette, Praveen Chandar", "title": "A Comparison of Methods for Treatment Assignment with an Application to\n  Playlist Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a systematic comparison of methods for individual\ntreatment assignment, a general problem that arises in many applications and\nhas received significant attention from economists, computer scientists, and\nsocial scientists. We characterize the various methods proposed in the\nliterature into three general approaches: learning models to predict outcomes,\nlearning models to predict causal effects, and learning models to predict\noptimal treatment assignments. We show analytically that optimizing for outcome\nor causal effect prediction is not the same as optimizing for treatment\nassignments, and thus we should prefer learning models that optimize for\ntreatment assignments. We then compare and contrast the three approaches\nempirically in the context of choosing, for each user, the best algorithm for\nplaylist generation in order to optimize engagement. This is the first\ncomparison of the different treatment assignment approaches on a real-world\napplication at scale (based on more than half a billion individual treatment\nassignments). Our results show (i) that applying different algorithms to\ndifferent users can improve streams substantially compared to deploying the\nsame algorithm for everyone, (ii) that personalized assignments improve\nsubstantially with larger data sets, and (iii) that learning models by\noptimizing for treatment assignment can increase engagement by 28% more than\nwhen optimizing for outcome or causal effect predictions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 04:56:15 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 03:27:10 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 14:33:52 GMT"}, {"version": "v4", "created": "Fri, 23 Apr 2021 17:51:45 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Fern\u00e1ndez-Lor\u00eda", "Carlos", ""], ["Provost", "Foster", ""], ["Anderton", "Jesse", ""], ["Carterette", "Benjamin", ""], ["Chandar", "Praveen", ""]]}, {"id": "2004.11554", "submitter": "Michael Vogt", "authors": "Johannes Lederer and Michael Vogt", "title": "Estimating the Lasso's Effective Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the theory for the lasso in the linear model $Y = X \\beta^* +\n\\varepsilon$ hinges on the quantity $2 \\| X^\\top \\varepsilon \\|_{\\infty} / n$,\nwhich we call the lasso's effective noise. Among other things, the effective\nnoise plays an important role in finite-sample bounds for the lasso, the\ncalibration of the lasso's tuning parameter, and inference on the parameter\nvector $\\beta^*$. In this paper, we develop a bootstrap-based estimator of the\nquantiles of the effective noise. The estimator is fully data-driven, that is,\ndoes not require any additional tuning parameters. We equip our estimator with\nfinite-sample guarantees and apply it to tuning parameter calibration for the\nlasso and to high-dimensional inference on the parameter vector $\\beta^*$.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 06:49:23 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Lederer", "Johannes", ""], ["Vogt", "Michael", ""]]}, {"id": "2004.11615", "submitter": "Guillaume Basse", "authors": "Kevin Guo and Guillaume Basse", "title": "The Generalized Oaxaca-Blinder Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After performing a randomized experiment, researchers often use\nordinary-least squares (OLS) regression to adjust for baseline covariates when\nestimating the average treatment effect. It is widely known that the resulting\nconfidence interval is valid even if the linear model is misspecified. In this\npaper, we generalize that conclusion to covariate adjustment with nonlinear\nmodels. We introduce an intuitive way to use any \"simple\" nonlinear model to\nconstruct a covariate-adjusted confidence interval for the average treatment\neffect. The confidence interval derives its validity from randomization alone,\nand when nonlinear models fit the data better than linear models, it is\nnarrower than the usual interval from OLS adjustment.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:32:18 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Guo", "Kevin", ""], ["Basse", "Guillaume", ""]]}, {"id": "2004.11698", "submitter": "Kai Zhou", "authors": "Kai Zhou and Jiong Tang", "title": "Structural Model Updating Using Adaptive Multi-Response Gaussian Process\n  Meta-modeling", "comments": null, "journal-ref": null, "doi": "10.1016/j.ymssp.2020.107121", "report-no": null, "categories": "cs.CE eess.SP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite element model updating utilizing frequency response functions as\ninputs is an important procedure in structural analysis, design and control.\nThis paper presents a highly efficient framework that is built upon Gaussian\nprocess emulation to inversely identify model parameters through sampling. In\nparticular, a multi-response Gaussian process (MRGP) meta-modeling approach is\nformulated that can accurately construct the error response surface, i.e., the\ndiscrepancies between the frequency response predictions and actual\nmeasurement. In order to reduce the computational cost of repeated finite\nelement simulations, an adaptive sampling strategy is established, where the\nsearch of unknown parameters is guided by the response surface features.\nMeanwhile, the information of previously sampled model parameters and the\ncorresponding errors is utilized as additional training data to refine the MRGP\nmeta-model. Two stochastic optimization techniques, i.e., particle swarm and\nsimulated annealing, are employed to train the MRGP meta-model for comparison.\nSystematic case studies are conducted to examine the accuracy and robustness of\nthe new framework of model updating.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:26:33 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zhou", "Kai", ""], ["Tang", "Jiong", ""]]}, {"id": "2004.11710", "submitter": "Hao Yan", "authors": "Yujie Zhao, Hao Yan, Sarah Holte, and Yajun Mei", "title": "Rapid Detection of Hot-spots via Tensor Decomposition with applications\n  to Crime Rate Data", "comments": "arXiv admin note: text overlap with arXiv:2001.11685", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient statistical method (denoted as SSR-Tensor) to\nrobustly and quickly detect hot-spots that are sparse and temporal-consistent\nin a spatial-temporal dataset through the tensor decomposition. Our main idea\nis first to build an SSR model to decompose the tensor data into a Smooth\nglobal trend mean, Sparse local hot-spots, and Residuals. Next, tensor\ndecomposition is utilized as follows: bases are introduced to describe\nwithin-dimension correlation, and tensor products are used for\nbetween-dimension interaction. Then, a combination of LASSO and fused LASSO is\nused to estimate the model parameters, where an efficient recursive estimation\nprocedure is developed based on the large-scale convex optimization, where we\nfirst transform the general LASSO optimization into regular LASSO optimization\nand apply FISTA to solve it with the fastest convergence rate. Finally, a CUSUM\nprocedure is applied to detect when and where the hot-spot event occurs. We\ncompare the performance of the proposed method in a numerical simulation study\nand a real-world case study, which contains a dataset including a collection of\nthree types of crime rates for U.S. mainland states during the year 1965-2014.\nIn both cases, the proposed SSR-Tensor is able to achieve the fast detection\nand accurate localization of the hot-spots.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 05:13:47 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 04:07:46 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 21:20:11 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Zhao", "Yujie", ""], ["Yan", "Hao", ""], ["Holte", "Sarah", ""], ["Mei", "Yajun", ""]]}, {"id": "2004.11760", "submitter": "Angeliki Papana", "authors": "Angeliki Papana, Ariadni Papana-Dagiasis, Elsa Siggiridou", "title": "Shortcomings of transfer entropy and partial transfer entropy: Extending\n  them to escape the curse of dimensionality", "comments": "13 pages, 14 tables, 2 figures", "journal-ref": "International Journal of Bifurcation and Chaos, Vol. 30, No. 16,\n  2020", "doi": "10.1142/S0218127420502508", "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer entropy (TE) captures the directed relationships between two\nvariables. Partial transfer entropy (PTE) accounts for the presence of all\nconfounding variables of a multivariate system and infers only about direct\ncausality. However, the computation of PTE involves high dimensional\ndistributions and thus may not be robust in case of many variables. In this\nwork, different variants of PTE are introduced, by building a reduced number of\nconfounding variables based on different scenarios in terms of their\ninterrelationships with the driving or response variable. Connectivity-based\nPTE variants and utilizing the random forests (RF) methodology are evaluated on\nsynthetic time series. The empirical findings indicate the superiority of the\nsuggested variants over TE and PTE, especially in case of high dimensional\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:02:18 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Papana", "Angeliki", ""], ["Papana-Dagiasis", "Ariadni", ""], ["Siggiridou", "Elsa", ""]]}, {"id": "2004.11769", "submitter": "Haben Michael", "authors": "Haben Michael and Yifan Cui and Scott Lorch and Eric Tchetgen Tchetgen", "title": "Instrumental Variable Estimation of Marginal Structural Mean Models for\n  Time-Varying Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robins 1997 introduced marginal structural models (MSMs), a general class of\ncounterfactual models for the joint effects of time-varying treatment regimes\nin complex longitudinal studies subject to time-varying confounding. In his\nwork, identification of MSM parameters is established under a sequential\nrandomization assumption (SRA), which rules out unmeasured confounding of\ntreatment assignment over time. We consider sufficient conditions for\nidentification of the parameters of a subclass, Marginal Structural Mean Models\n(MSMMs), when sequential randomization fails to hold due to unmeasured\nconfounding, using instead a time-varying instrumental variable. Our\nidentification conditions require that no unobserved confounder predicts\ncompliance type for the time-varying treatment. We describe a simple weighted\nestimator and examine its finite-sample properties in a simulation study. We\napply the proposed estimator to examine the effect of delivery hospital on\nneonatal survival probability.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:19:47 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 04:53:57 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Michael", "Haben", ""], ["Cui", "Yifan", ""], ["Lorch", "Scott", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2004.11770", "submitter": "Alfred M\\\"uller", "authors": "Carole Bernard, Alfred M\\\"uller", "title": "Dependence uncertainty bounds for the energy score and the multivariate\n  Gini mean difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy distance and energy scores became important tools in multivariate\nstatistics and multivariate probabilistic forecasting in recent years. They are\nboth based on the expected distance of two independent samples. In this paper\nwe study dependence uncertainty bounds for these quantities under the\nassumption that we know the marginals but do not know the dependence structure.\nWe find some interesting sharp analytic bounds, where one of them is obtained\nfor an unusual spherically symmetric copula. These results should help to\nbetter understand the sensitivity of these measures to misspecifications in the\ncopula.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:23:02 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Bernard", "Carole", ""], ["M\u00fcller", "Alfred", ""]]}, {"id": "2004.11810", "submitter": "Suneel Babu Chatla", "authors": "Suneel Babu Chatla and Galit Shmueli", "title": "A Tree-based Semi-Varying Coefficient Model for the COM-Poisson\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a tree-based semi-varying coefficient model for the\nConway-Maxwell- Poisson (CMP or COM-Poisson) distribution which is a\ntwo-parameter generalization of the Poisson distribution and is flexible enough\nto capture both under-dispersion and over-dispersion in count data. The\nadvantage of tree-based methods is their scalability to high-dimensional data.\nWe develop CMPMOB, an estimation procedure for a semi-varying coefficient\nmodel, using model-based recursive partitioning (MOB). The proposed framework\nis broader than the existing MOB framework as it allows node-invariant effects\nto be included in the model. To simplify the computational burden of the\nexhaustive search employed in the original MOB algorithm, a new split point\nestimation procedure is proposed by borrowing tools from change point\nestimation methodology. The proposed method uses only the estimated score\nfunctions without fitting models for each split point and, therefore, is\ncomputationally simpler. Since the tree-based methods only provide a piece-wise\nconstant approximation to the underlying smooth function, we propose the\nCMPBoost semi-varying coefficient model which uses the gradient boosting\nprocedure for estimation. The usefulness of the proposed methods are\nillustrated using simulation studies and a real example from a bike sharing\nsystem in Washington, DC.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 15:45:44 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Chatla", "Suneel Babu", ""], ["Shmueli", "Galit", ""]]}, {"id": "2004.11816", "submitter": "Suneel Babu Chatla", "authors": "Suneel Babu Chatla, Chun-houh Chen and Galit Shmueli", "title": "Selected Topics in Statistical Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of computational statistics refers to statistical methods or tools\nthat are computationally intensive. Due to the recent advances in computing\npower some of these methods have become prominent and central to modern data\nanalysis. In this article we focus on several of the main methods including\ndensity estimation, kernel smoothing, smoothing splines, and additive models.\nWhile the field of computational statistics includes many more methods, this\narticle serves as a brief introduction to selected popular topics.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 15:56:51 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Chatla", "Suneel Babu", ""], ["Chen", "Chun-houh", ""], ["Shmueli", "Galit", ""]]}, {"id": "2004.12012", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Shariq Mohammed, Arvind Rao, Veerabhadran\n  Baladandayuthapani", "title": "Integrative Bayesian models using Post-selective Inference: a case study\n  in Radiogenomics", "comments": "44 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying direct links between genomic pathways and clinical endpoints for\nhighly fatal diseases such as cancer is a formidable task. By selecting\nstatistically relevant associations between a wealth of intermediary variables\nsuch as imaging and genomic measurements, integrative analyses can potentially\nresult in sharper clinical models with interpretable parameters, in terms of\ntheir mechanisms. Estimates of uncertainty in the resulting models are however\nunreliable unless inference accounts for the preceding steps of selection. In\nthis article, we develop selection-aware Bayesian methods which are: (i)\namenable to a flexible class of integrative Bayesian models post a selection of\npromising variables via $\\ell_1$-regularized algorithms; (ii) enjoy\ncomputational efficiency due to a focus on sharp models with meaning; (iii)\nstrike a crucial tradeoff between the quality of model selection and\ninferential power. Central to our selection-aware workflow, a conditional\nlikelihood constructed with a reparameterization map is deployed for obtaining\nuncertainty estimates in integrative models. Investigating the potential of our\nmethods in a radiogenomic analysis, we successfully recover several important\ngene pathways and calibrate uncertainties for their associations with patient\nsurvival times.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:39:26 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 16:19:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Mohammed", "Shariq", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "2004.12013", "submitter": "Nelson Walker", "authors": "Nelson B. Walker, Trevor J. Hefley, Anne E. Ballmann, Robin E.\n  Russell, Daniel P. Walsh", "title": "Recovering individual-level spatial inference from aggregated binary\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary regression models are commonly used in disciplines such as\nepidemiology and ecology to determine how spatial covariates influence\nindividuals. In many studies, binary data are shared in a spatially aggregated\nform to protect privacy. For example, rather than reporting the location and\nresult for each individual that was tested for a disease, researchers may\nreport that a disease was detected or not detected within geopolitical units.\nOften, the spatial aggregation process obscures the values of response\nvariables, spatial covariates, and locations of each individual, which makes\nrecovering individual-level inference difficult. We show that applying a series\nof transformations, including a change of support, to a bivariate point process\nmodel allows researchers to recover individual-level inference for spatial\ncovariates from spatially aggregated binary data. The series of transformations\npreserves the convenient interpretation of desirable binary regression models\nthat are commonly applied to individual-level data. Using a simulation\nexperiment, we compare the performance of our proposed method under varying\ntypes of spatial aggregation against the performance of standard approaches\nusing the original individual-level data. We illustrate our method by modeling\nindividual-level probability of infection using a data set that has been\naggregated to protect an at-risk and endangered species of bats. Our simulation\nexperiment and data illustration demonstrate the utility of the proposed method\nwhen access to original non-aggregated data is impractical or prohibited.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:41:07 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 19:57:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Walker", "Nelson B.", ""], ["Hefley", "Trevor J.", ""], ["Ballmann", "Anne E.", ""], ["Russell", "Robin E.", ""], ["Walsh", "Daniel P.", ""]]}, {"id": "2004.12022", "submitter": "Yishu Xue", "authors": "Guanyu Hu, Yishu Xue, Zhihua Ma", "title": "Bayesian Clustered Coefficients Regression with Auxiliary Covariates\n  Assistant Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In regional economics research, a problem of interest is to detect\nsimilarities between regions, and estimate their shared coefficients in\neconomics models. In this article, we propose a mixture of finite mixtures\n(MFM) clustered regression model with auxiliary covariates that account for\nsimilarities in demographic or economic characteristics over a spatial domain.\nOur Bayesian construction provides both inference for number of clusters and\nclustering configurations, and estimation for parameters for each cluster.\nEmpirical performance of the proposed model is illustrated through simulation\nexperiments, and further applied to a study of influential factors for monthly\nhousing cost in Georgia.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 00:21:33 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hu", "Guanyu", ""], ["Xue", "Yishu", ""], ["Ma", "Zhihua", ""]]}, {"id": "2004.12028", "submitter": "Jixiong Wang", "authors": "Jixiong Wang, Ashish Patel, James M.S. Wason, Paul J. Newcombe", "title": "Two-Stage Penalized Regression Screening to Detect Biomarker-Treatment\n  Interactions in Randomized Clinical Trials", "comments": "Accepted version, to be published in Biometrics", "journal-ref": null, "doi": "10.1111/biom.13424", "report-no": null, "categories": "stat.ME cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional biomarkers such as genomics are increasingly being measured\nin randomized clinical trials. Consequently, there is a growing interest in\ndeveloping methods that improve the power to detect biomarker-treatment\ninteractions. We adapt recently proposed two-stage interaction detecting\nprocedures in the setting of randomized clinical trials. We also propose a new\nstage 1 multivariate screening strategy using ridge regression to account for\ncorrelations among biomarkers. For this multivariate screening, we prove the\nasymptotic between-stage independence, required for family-wise error rate\ncontrol, under biomarker-treatment independence. Simulation results show that\nin various scenarios, the ridge regression screening procedure can provide\nsubstantially greater power than the traditional one-biomarker-at-a-time\nscreening procedure in highly correlated data. We also exemplify our approach\nin two real clinical trial data applications.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 00:50:09 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 19:48:30 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wang", "Jixiong", ""], ["Patel", "Ashish", ""], ["Wason", "James M. S.", ""], ["Newcombe", "Paul J.", ""]]}, {"id": "2004.12130", "submitter": "Philip Nadler", "authors": "Philip Nadler, Shuo Wang, Rossella Arcucci, Xian Yang, Yike Guo", "title": "An Epidemiological Modelling Approach for Covid19 via Data Assimilation", "comments": "Initial conference version accepted at International Conference of\n  Machine Learning(ICML) workshop. Extended journal version was published in\n  the European Journal of Epidemiology\n  (https://doi.org/10.1007/s10654-020-00676-7). Please cite as accordingly", "journal-ref": null, "doi": "10.1007/s10654-020-00676-7", "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global pandemic of the 2019-nCov requires the evaluation of policy\ninterventions to mitigate future social and economic costs of quarantine\nmeasures worldwide. We propose an epidemiological model for forecasting and\npolicy evaluation which incorporates new data in real-time through variational\ndata assimilation. We analyze and discuss infection rates in China, the US and\nItaly. In particular, we develop a custom compartmental SIR model fit to\nvariables related to the epidemic in Chinese cities, named SITR model. We\ncompare and discuss model results which conducts updates as new observations\nbecome available. A hybrid data assimilation approach is applied to make\nresults robust to initial conditions. We use the model to do inference on\ninfection numbers as well as parameters such as the disease transmissibility\nrate or the rate of recovery. The parameterisation of the model is parsimonious\nand extendable, allowing for the incorporation of additional data and\nparameters of interest. This allows for scalability and the extension of the\nmodel to other locations or the adaption of novel data sources.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 12:46:36 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 16:11:32 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 12:48:52 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Nadler", "Philip", ""], ["Wang", "Shuo", ""], ["Arcucci", "Rossella", ""], ["Yang", "Xian", ""], ["Guo", "Yike", ""]]}, {"id": "2004.12162", "submitter": "David Holtz", "authors": "David Holtz, Sinan Aral", "title": "Limiting Bias from Test-Control Interference in Online Marketplace\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an A/B test, the typical objective is to measure the total average\ntreatment effect (TATE), which measures the difference between the average\noutcome if all users were treated and the average outcome if all users were\nuntreated. However, a simple difference-in-means estimator will give a biased\nestimate of the TATE when outcomes of control units depend on the outcomes of\ntreatment units, an issue we refer to as test-control interference. Using a\nsimulation built on top of data from Airbnb, this paper considers the use of\nmethods from the network interference literature for online marketplace\nexperimentation. We model the marketplace as a network in which an edge exists\nbetween two sellers if their goods substitute for one another. We then simulate\nseller outcomes, specifically considering a \"status quo\" context and\n\"treatment\" context that forces all sellers to lower their prices. We use the\nsame simulation framework to approximate TATE distributions produced by using\nblocked graph cluster randomization, exposure modeling, and the Hajek estimator\nfor the difference in means. We find that while blocked graph cluster\nrandomization reduces the bias of the naive difference-in-means estimator by as\nmuch as 62%, it also significantly increases the variance of the estimator. On\nthe other hand, the use of more sophisticated estimators produces mixed\nresults. While some provide (small) additional reductions in bias and small\nreductions in variance, others lead to increased bias and variance. Overall,\nour results suggest that experiment design and analysis techniques from the\nnetwork experimentation literature are promising tools for reducing bias due to\ntest-control interference in marketplace experiments.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 14:54:49 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Holtz", "David", ""], ["Aral", "Sinan", ""]]}, {"id": "2004.12164", "submitter": "Xiao Guo", "authors": "Xiao Guo, Yixuan Qiu, Hai Zhang, Xiangyu Chang", "title": "Randomized spectral co-clustering for large-scale directed networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed networks are generally used to represent asymmetric relationships\namong units. Co-clustering aims to cluster the senders and receivers of\ndirected networks simultaneously. In particular, the well-known spectral\nclustering algorithm could be modified as the spectral co-clustering to\nco-cluster directed networks. However, large-scale networks pose computational\nchallenge to it. In this paper, we leverage randomized sketching techniques to\naccelerate the spectral co-clustering algorithms in order to co-cluster\nlarge-scale directed networks more efficiently. Specifically, we derive two\nseries of randomized spectral co-clustering algorithms, one is\nrandom-projection-based and the other is random-sampling-based. Theoretically,\nwe analyze the resulting algorithms under two generative models\\textendash the\n\\emph{stochastic co-block model} and the \\emph{degree corrected stochastic\nco-block model}. The approximation error rates and misclustering error rates of\nproposed two randomized spectral co-clustering algorithms are established,\nwhich indicate better bounds than the state-of-the-art results of co-clustering\nliterature. Numerically, we conduct simulations to support our theoretical\nresults and test the efficiency of the algorithms on real networks with up to\ntens of millions of nodes.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 15:00:55 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:09:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Guo", "Xiao", ""], ["Qiu", "Yixuan", ""], ["Zhang", "Hai", ""], ["Chang", "Xiangyu", ""]]}, {"id": "2004.12182", "submitter": "Sebastian Engelke", "authors": "Sebastian Engelke and Jevgenijs Ivanovs", "title": "Sparse Structures for Multivariate Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme value statistics provides accurate estimates for the small occurrence\nprobabilities of rare events. While theory and statistical tools for univariate\nextremes are well-developed, methods for high-dimensional and complex data sets\nare still scarce. Appropriate notions of sparsity and connections to other\nfields such as machine learning, graphical models and high-dimensional\nstatistics have only recently been established. This article reviews the new\ndomain of research concerned with the detection and modeling of sparse patterns\nin rare events. We first describe the different forms of extremal dependence\nthat can arise between the largest observations of a multivariate random\nvector. We then discuss the current research topics including clustering,\nprincipal component analysis and graphical modeling for extremes.\nIdentification of groups of variables which can be concomitantly extreme is\nalso addressed. The methods are illustrated with an application to flood risk\nassessment.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 16:32:21 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 06:12:07 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Engelke", "Sebastian", ""], ["Ivanovs", "Jevgenijs", ""]]}, {"id": "2004.12211", "submitter": "Kamran Javid Mr", "authors": "Kamran Javid, Will Handley, Mike Hobson, Anthony Lasenby", "title": "Compromise-free Bayesian neural networks", "comments": "https://github.com/PolyChord/PolyChordLite;\n  https://github.com/SuperKam91/bnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a thorough analysis of the relationship between the out-of-sample\nperformance and the Bayesian evidence (marginal likelihood) of Bayesian neural\nnetworks (BNNs), as well as looking at the performance of ensembles of BNNs,\nboth using the Boston housing dataset. Using the state-of-the-art in nested\nsampling, we numerically sample the full (non-Gaussian and multimodal) network\nposterior and obtain numerical estimates of the Bayesian evidence, considering\nnetwork models with up to 156 trainable parameters. The networks have between\nzero and four hidden layers, either $\\tanh$ or $ReLU$ activation functions, and\nwith and without hierarchical priors. The ensembles of BNNs are obtained by\ndetermining the posterior distribution over networks, from the posterior\nsamples of individual BNNs re-weighted by the associated Bayesian evidence\nvalues. There is good correlation between out-of-sample performance and\nevidence, as well as a remarkable symmetry between the evidence versus model\nsize and out-of-sample performance versus model size planes. Networks with\n$ReLU$ activation functions have consistently higher evidences than those with\n$\\tanh$ functions, and this is reflected in their out-of-sample performance.\nEnsembling over architectures acts to further improve performance relative to\nthe individual BNNs.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:12:56 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:23:29 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 12:03:28 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Javid", "Kamran", ""], ["Handley", "Will", ""], ["Hobson", "Mike", ""], ["Lasenby", "Anthony", ""]]}, {"id": "2004.12234", "submitter": "Yifei Sun", "authors": "Yifei Sun, Charles E. McCulloch, Kieren A. Marr, Chiung-Yu Huang", "title": "Recurrent Events Analysis With Data Collected at Informative Clinical\n  Visits in Electronic Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although increasingly used as a data resource for assembling cohorts,\nelectronic health records (EHRs) pose many analytic challenges. In particular,\na patient's health status influences when and what data are recorded,\ngenerating sampling bias in the collected data. In this paper, we consider\nrecurrent event analysis using EHR data. Conventional regression methods for\nevent risk analysis usually require the values of covariates to be observed\nthroughout the follow-up period. In EHR databases, time-dependent covariates\nare intermittently measured during clinical visits, and the timing of these\nvisits is informative in the sense that it depends on the disease course.\nSimple methods, such as the last-observation-carried-forward approach, can lead\nto biased estimation. On the other hand, complex joint models require\nadditional assumptions on the covariate process and cannot be easily extended\nto handle multiple longitudinal predictors. By incorporating sampling weights\nderived from estimating the observation time process, we develop a novel\nestimation procedure based on inverse-rate-weighting and kernel-smoothing for\nthe semiparametric proportional rate model of recurrent events. The proposed\nmethods do not require model specifications for the covariate processes and can\neasily handle multiple time-dependent covariates. Our methods are applied to a\nkidney transplant study for illustration.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 21:12:41 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Sun", "Yifei", ""], ["McCulloch", "Charles E.", ""], ["Marr", "Kieren A.", ""], ["Huang", "Chiung-Yu", ""]]}, {"id": "2004.12293", "submitter": "Yichen Zhu", "authors": "Yichen Zhu, Cheng Li and David B. Dunson", "title": "Classification Trees for Imbalanced and Sparse Data: Surface-to-Volume\n  Regularization", "comments": "Submitted to Journal of American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification algorithms face difficulties when one or more classes have\nlimited training data. We are particularly interested in classification trees,\ndue to their interpretability and flexibility. When data are limited in one or\nmore of the classes, the estimated decision boundaries are often irregularly\nshaped due to the limited sample size, leading to poor generalization error. We\npropose a novel approach that penalizes the Surface-to-Volume Ratio (SVR) of\nthe decision set, obtaining a new class of SVR-Tree algorithms. We develop a\nsimple and computationally efficient implementation while proving estimation\nconsistency for SVR-Tree and rate of convergence for an idealized empirical\nrisk minimizer of SVR-Tree. SVR-Tree is compared with multiple algorithms that\nare designed to deal with imbalance through real data applications.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 06:22:47 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:41:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhu", "Yichen", ""], ["Li", "Cheng", ""], ["Dunson", "David B.", ""]]}, {"id": "2004.12322", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic and Ghislain Verdier", "title": "Nonparametric sequential change-point detection for multivariate time\n  series based on empirical distribution functions", "comments": "55 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of sequential change-point detection is to issue an alarm when it is\nthought that certain probabilistic properties of the monitored observations\nhave changed. This work is concerned with nonparametric, closed-end testing\nprocedures based on differences of empirical distribution functions that are\ndesigned to be particularly sensitive to changes in the comtemporary\ndistribution of multivariate time series. The proposed detectors are\nadaptations of statistics used in a posteriori (offline) change-point testing\nand involve a weighting allowing to give more importance to recent\nobservations. The resulting sequential change-point detection procedures are\ncarried out by comparing the detectors to threshold functions estimated through\nresampling such that the probability of false alarm remains approximately\nconstant over the monitoring period. A generic result on the asymptotic\nvalidity of such a way of estimating a threshold function is stated. As a\ncorollary, the asymptotic validity of the studied sequential tests based on\nempirical distribution functions is proven when these are carried out using a\ndependent multiplier bootstrap for multivariate time series. Large-scale Monte\nCarlo experiments demonstrate the good finite-sample properties of the\nresulting procedures. The application of the derived sequential tests is\nillustrated on financial data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:11:36 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 11:57:03 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 12:53:58 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Verdier", "Ghislain", ""]]}, {"id": "2004.12489", "submitter": "David Holtz", "authors": "David Holtz, Ruben Lobel, Inessa Liskovich, Sinan Aral", "title": "Reducing Interference Bias in Online Marketplace Pricing Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online marketplace designers frequently run A/B tests to measure the impact\nof proposed product changes. However, given that marketplaces are inherently\nconnected, total average treatment effect estimates obtained through Bernoulli\nrandomized experiments are often biased due to violations of the stable unit\ntreatment value assumption. This can be particularly problematic for\nexperiments that impact sellers' strategic choices, affect buyers' preferences\nover items in their consideration set, or change buyers' consideration sets\naltogether. In this work, we measure and reduce bias due to interference in\nonline marketplace experiments by using observational data to create clusters\nof similar listings, and then using those clusters to conduct\ncluster-randomized field experiments. We provide a lower bound on the magnitude\nof bias due to interference by conducting a meta-experiment that randomizes\nover two experiment designs: one Bernoulli randomized, one cluster randomized.\nIn both meta-experiment arms, treatment sellers are subject to a different\nplatform fee policy than control sellers, resulting in different prices for\nbuyers. By conducting a joint analysis of the two meta-experiment arms, we find\na large and statistically significant difference between the total average\ntreatment effect estimates obtained with the two designs, and estimate that\n32.60% of the Bernoulli-randomized treatment effect estimate is due to\ninterference bias. We also find weak evidence that the magnitude and/or\ndirection of interference bias depends on extent to which a marketplace is\nsupply- or demand-constrained, and analyze a second meta-experiment to\nhighlight the difficulty of detecting interference bias when treatment\ninterventions require intention-to-treat analysis.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 22:09:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Holtz", "David", ""], ["Lobel", "Ruben", ""], ["Liskovich", "Inessa", ""], ["Aral", "Sinan", ""]]}, {"id": "2004.12491", "submitter": "Roberto Vila Gabriel", "authors": "R. Vila, L. Ferreira, H. Saulo, F. Prataviera and E.M.M. Ortega", "title": "A bimodal gamma distribution: Properties, regression model and\n  applications", "comments": "26 pages, 13 figures. Accepted for publication in Statistics: A\n  Journal of Theoretical and Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a bimodal gamma distribution using a quadratic\ntransformation based on the alpha-skew-normal model. We discuss several\nproperties of this distribution such as mean, variance, moments, hazard rate\nand entropy measures. Further, we propose a new regression model with censored\ndata based on the bimodal gamma distribution. This regression model can be very\nuseful to the analysis of real data and could give more realistic fits than\nother special regression models. Monte Carlo simulations were performed to\ncheck the bias in the maximum likelihood estimation. The proposed models are\napplied to two real data sets found in literature.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 22:17:52 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 18:27:52 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Vila", "R.", ""], ["Ferreira", "L.", ""], ["Saulo", "H.", ""], ["Prataviera", "F.", ""], ["Ortega", "E. M. M.", ""]]}, {"id": "2004.12508", "submitter": "Marco Cuturi", "authors": "Marco Cuturi, Olivier Teboul, Quentin Berthet, Arnaud Doucet,\n  Jean-Philippe Vert", "title": "Noisy Adaptive Group Testing using Bayesian Sequential Experimental\n  Design", "comments": "Latest version, with updated experiments, new conclusions on LBP vs\n  SMC decoding and new approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the infection prevalence of a disease is low, Dorfman showed 80 years\nago that testing groups of people can prove more efficient than testing people\nindividually. Our goal in this paper is to propose new group testing algorithms\nthat can operate in a noisy setting (tests can be mistaken) to decide\nadaptively (looking at past results) which groups to test next, with the goal\nto converge to a good detection, as quickly, and with as few tests as possible.\nWe cast this problem as a Bayesian sequential experimental design problem.\nUsing the posterior distribution of infection status vectors for $n$ patients,\ngiven observed tests carried out so far, we seek to form groups that have a\nmaximal utility. We consider utilities such as mutual information, but also\nquantities that have a more direct relevance to testing, such as the AUC of the\nROC curve of the test. Practically, the posterior distributions on $\\{0,1\\}^n$\nare approximated by sequential Monte Carlo (SMC) samplers and the utility\nmaximized by a greedy optimizer. Our procedures show in simulations significant\nimprovements over both adaptive and non-adaptive baselines, and are far more\nefficient than individual tests when disease prevalence is low. Additionally,\nwe show empirically that loopy belief propagation (LBP), widely regarded as the\nSoTA decoder to decide whether an individual is infected or not given previous\ntests, can be unreliable and exhibit oscillatory behavior. Our SMC decoder is\nmore reliable, and can improve the performance of other group testing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 23:41:33 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 22:49:38 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 21:10:58 GMT"}, {"version": "v4", "created": "Fri, 15 May 2020 17:55:11 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 15:10:39 GMT"}, {"version": "v6", "created": "Wed, 22 Jul 2020 08:19:33 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Cuturi", "Marco", ""], ["Teboul", "Olivier", ""], ["Berthet", "Quentin", ""], ["Doucet", "Arnaud", ""], ["Vert", "Jean-Philippe", ""]]}, {"id": "2004.12588", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo L. Hammer, Anis Yazidi, Michael A. Riegler and H{\\aa}vard Rue", "title": "Efficient Quantile Tracking Using an Oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For incremental quantile estimators the step size and possibly other tuning\nparameters must be carefully set. However, little attention has been given on\nhow to set these values in an online manner. In this article we suggest two\nnovel procedures that address this issue.\n  The core part of the procedures is to estimate the current tracking mean\nsquared error (MSE). The MSE is decomposed in tracking variance and bias and\nnovel and efficient procedures to estimate these quantities are presented. It\nis shown that estimation bias can be tracked by associating it with the portion\nof observations below the quantile estimates.\n  The first procedure runs an ensemble of $L$ quantile estimators for wide\nrange of values of the tuning parameters and typically around $L = 100$. In\neach iteration an oracle selects the best estimate by the guidance of the\nestimated MSEs. The second method only runs an ensemble of $L = 3$ estimators\nand thus the values of the tuning parameters need from time to time to be\nadjusted for the running estimators. The procedures have a low memory foot\nprint of $8L$ and a computational complexity of $8L$ per iteration.\n  The experiments show that the procedures are highly efficient and track\nquantiles with an error close to the theoretical optimum. The Oracle approach\nperforms best, but comes with higher computational cost. The procedures were\nfurther applied to a massive real-life data stream of tweets and proofed real\nworld applicability of them.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 05:49:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hammer", "Hugo L.", ""], ["Yazidi", "Anis", ""], ["Riegler", "Michael A.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "2004.12639", "submitter": "Mervyn Silvapulle", "authors": "Svetlana Litvinova and Mervyn J. Silvapulle", "title": "Consistency of full-sample bootstrap for estimating high-quantile, tail\n  probability, and tail index", "comments": "Main paper is 12 pages long and contains 2 figures. The Supplementary\n  Materials containing the proofs is 20 pages long", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the full-sample bootstrap is asymptotically valid for\nconstructing confidence intervals for high-quantiles, tail probabilities, and\nother tail parameters of a univariate distribution. This resolves the doubts\nthat have been raised about the validity of such bootstrap methods. In our\nextensive simulation study, the overall performance of the bootstrap method was\nbetter than that of the standard asymptotic method, indicating that the\nbootstrap method is at least as good, if not better than, the asymptotic method\nfor inference. This paper also lays the foundation for developing bootstrap\nmethods for inference about tail events in multivariate statistics; this is\nparticularly important because some of the non-bootstrap methods are complex.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:37:40 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Litvinova", "Svetlana", ""], ["Silvapulle", "Mervyn J.", ""]]}, {"id": "2004.12716", "submitter": "Guy Nason Prof.", "authors": "Rebecca Killick, Marina I. Knight, Guy P. Nason and Idris A. Eckley", "title": "The Local Partial Autocorrelation Function and Some Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical regular and partial autocorrelation functions are powerful\ntools for stationary time series modelling and analysis. However, it is\nincreasingly recognized that many time series are not stationary and the use of\nclassical global autocorrelations can give misleading answers. This article\nintroduces two estimators of the local partial autocorrelation function and\nestablishes their asymptotic properties. The article then illustrates the use\nof these new estimators on both simulated and real time series. The examples\nclearly demonstrate the strong practical benefits of local estimators for time\nseries that exhibit nonstationarities.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:32:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Killick", "Rebecca", ""], ["Knight", "Marina I.", ""], ["Nason", "Guy P.", ""], ["Eckley", "Idris A.", ""]]}, {"id": "2004.12737", "submitter": "Tasnim Hamza", "authors": "Tasnim Hamza, Andrea Cipriani, Toshi A. Furukawa, Matthias Egger,\n  Nicola Orsini, Georgia Salanti", "title": "A Bayesian dose-response meta-analysis model: simulation study and\n  application", "comments": null, "journal-ref": "Vol 30, Issue 5, 2021", "doi": "10.1177/0962280220982643", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dose-response models express the effect of different dose or exposure levels\non a specific outcome. In meta-analysis, where aggregated-level data is\navailable, dose-response evidence is synthesized using either one-stage or\ntwo-stage models in a frequentist setting. We propose a hierarchical\ndose-response model implemented in a Bayesian framework. We present the model\nwith cubic dose-response shapes for a dichotomous outcome and take into account\nheterogeneity due to variability in the dose-response shape. We develop our\nBayesian model assuming normal or binomial likelihood and accounting for\nexposures grouped in clusters. We implement these models in R using JAGS and we\ncompare our approach to the one-stage dose-response meta-analysis model in a\nsimulation study. We found that the Bayesian dose-response model with binomial\nlikelihood has slightly lower bias than the Bayesian model with the normal\nlikelihood and the frequentist one-stage model. However, all three models\nperform very well and give practically identical results. We also re-analyze\nthe data from 60 randomized controlled trials (15,984 participants) examining\nthe efficacy (response) of various doses of antidepressant drugs. All models\nsuggest that the dose-response curve increases between zero dose and 40 mg of\nfluoxetine-equivalent dose, and thereafter is constant. We draw the same\nconclusion when we take into account the fact that five different\nantidepressants have been studied in the included trials. We show that\nimplementation of the hierarchical model in Bayesian framework has similar\nperformance to, but overcomes some of the limitations of the frequentist\napproaches and offers maximum flexibility to accommodate features of the data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:22:31 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Hamza", "Tasnim", ""], ["Cipriani", "Andrea", ""], ["Furukawa", "Toshi A.", ""], ["Egger", "Matthias", ""], ["Orsini", "Nicola", ""], ["Salanti", "Georgia", ""]]}, {"id": "2004.12755", "submitter": "David Norris", "authors": "David C. Norris", "title": "Retrospective analysis of a fatal dose-finding trial", "comments": "5 pages, 4 figures, 3 tables, 19 references with added DOI hyperlinks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The commonplace description of phase 1 clinical trials in oncology as\n\"primarily concerned with safety\" is belied by their near universal adoption of\ndose-escalation practices which are inherently unsafe. In contrast with dose\ntitration, cohort-wise dose escalation regards patients as exchangeable, an\nindefensible assumption in the face of widely appreciated inter-individual\nheterogeneity in pharmacokinetics and pharmacodynamics (PKPD). I have\npreviously advanced this argument in terms of a precautionary coherence\nprinciple that brings the well-known coherence notion of Cheung (2005) into\ncontact with modern imperatives of patient-centeredness and precision dosing.\nHere, however, I explore these matters in some mechanistic detail by analyzing\na trial of the bispecific T cell engager AFM11, in which a fatal toxicity\noccurred. To this end, I develop a Bayesian dose-response model for a single\nordinal toxicity. By constructing this model's priors to align with the AFM11\ntrial as designed and conducted, I demonstrate the incompatibility of that\ndesign with any reasonable expectation of safety. Indeed, the model readily\nyields prospective estimates of toxic response probabilities that suggest the\nfatality in this trial could have been foreseen as likely.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:58:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Norris", "David C.", ""]]}, {"id": "2004.12972", "submitter": "Yanhong Zhou", "authors": "Yanhong Zhou, J.Jack Lee, Shunguang Wang, Stuart Bailey, and Ying Yuan", "title": "Incorporating historical information to improve phase I clinical trial\n  designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating historical data or real-world evidence has a great potential to\nimprove the efficiency of phase I clinical trials and to accelerate drug\ndevelopment. For model-based designs, such as the continuous reassessment\nmethod (CRM), this can be conveniently carried out by specifying a \"skeleton,\"\ni.e., the prior estimate of dose limiting toxicity (DLT) probability at each\ndose. In contrast, little work has been done to incorporate historical data or\nreal-world evidence into model-assisted designs, such as the Bayesian optimal\ninterval (BOIN), keyboard, and modified toxicity probability interval (mTPI)\ndesigns. This has led to the misconception that model-assisted designs cannot\nincorporate prior information. In this paper, we propose a unified framework\nthat allows for incorporating historical data or real-world evidence into\nmodel-assisted designs. The proposed approach uses the well-established\n\"skeleton\" approach, combined with the concept of prior effective sample size,\nthus it is easy to understand and use. More importantly, our approach maintains\nthe hallmark of model-assisted designs: simplicity---the dose\nescalation/de-escalation rule can be tabulated prior to the trial conduct.\nExtensive simulation studies show that the proposed method can effectively\nincorporate prior information to improve the operating characteristics of\nmodel-assisted designs, similarly to model-based designs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 17:31:29 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 23:06:54 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 18:34:26 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhou", "Yanhong", ""], ["Lee", "J. Jack", ""], ["Wang", "Shunguang", ""], ["Bailey", "Stuart", ""], ["Yuan", "Ying", ""]]}, {"id": "2004.13105", "submitter": "Dallas Foster", "authors": "Dallas Foster, Darin Comeau, Nathan M. Urban", "title": "A Bayesian approach to regional decadal predictability: Sparse parameter\n  estimation in high-dimensional linear inverse models of high-latitude sea\n  surface temperature variability", "comments": "This work has been accepted to the Journal of Climate. The AMS does\n  not guarantee that the copy provided here is an accurate copy of the final\n  published work", "journal-ref": null, "doi": "10.1175/JCLI-D-19-0769.1", "report-no": "LA-UR-19-30206", "categories": "stat.ME physics.ao-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic reduced models are an important tool in climate systems whose many\nspatial and temporal scales cannot be fully discretized or underlying physics\nmay not be fully accounted for. One form of reduced model, the linear inverse\nmodel (LIM), has been widely used for regional climate predictability studies -\ntypically focusing more on tropical or mid-latitude studies. However, most LIM\nfitting techniques rely on point estimation techniques deriving from\nfluctuation-dissipation theory. In this methodological study we explore the use\nof Bayesian inference techniques for LIM parameter estimation of sea surface\ntemperature (SST), to quantify the skillful decadal predictability of Bayesian\nLIM models at high latitudes. We show that Bayesian methods, when compared to\ntraditional point estimation methods for LIM-type models, provide better\ncalibrated probabilistic skill, while simultaneously providing better point\nestimates due to the regularization effect of the prior distribution in\nhigh-dimensional problems. We compare the effect of several priors, as well as\nmaximum likelihood estimates, on (1) estimating parameter values on a perfect\nmodel experiment and (2) producing calibrated 1-year SST anomaly forecast\ndistributions using a pre-industrial control run of the Community Earth System\nModel (CESM). Finally, we employ a host of probabilistic skill metrics to\ndetermine the extent to which a LIM can forecast SST anomalies at high\nlatitudes. We find that the choice of prior distribution has an appreciable\nimpact on estimation outcomes, and priors that emphasize physically relevant\nproperties enhance the model's ability to capture variability of SST anomalies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:10:29 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Foster", "Dallas", ""], ["Comeau", "Darin", ""], ["Urban", "Nathan M.", ""]]}, {"id": "2004.13118", "submitter": "Federico Pavone", "authors": "Federico Pavone, Juho Piironen, Paul-Christian B\\\"urkner and Aki\n  Vehtari", "title": "Using reference models in variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection, or more generally, model reduction is an important aspect\nof the statistical workflow aiming to provide insights from data. In this\npaper, we discuss and demonstrate the benefits of using a reference model in\nvariable selection. A reference model acts as a noise-filter on the target\nvariable by modeling its data generating mechanism. As a result, using the\nreference model predictions in the model selection procedure reduces the\nvariability and improves stability leading to improved model selection\nperformance. Assuming that a Bayesian reference model describes the true\ndistribution of future data well, the theoretically preferred usage of the\nreference model is to project its predictive distribution to a reduced model\nleading to projection predictive variable selection approach. Alternatively,\nreference models may also be used in an ad-hoc manner in combination with\ncommon variable selection methods. In several numerical experiments, we\ninvestigate the performance of the projective prediction approach as well as\nalternative variable selection methods with and without reference models. Our\nresults indicate that the use of reference models generally translates into\nbetter and more stable variable selection. Additionally, we demonstrate that\nthe projection predictive approach shows superior performance as compared to\nalternative variable selection methods independently of whether or not they use\nreference models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:31:20 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Pavone", "Federico", ""], ["Piironen", "Juho", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "2004.13151", "submitter": "Hajo Holzmann", "authors": "Isaia Albisetti, Fadoua Balabdaoui and Hajo Holzmann", "title": "Testing for spherical and elliptical symmetry", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct new testing procedures for spherical and elliptical symmetry\nbased on the characterization that a random vector $X$ with finite mean has a\nspherical distribution if and only if $\\Ex[u^\\top X | v^\\top X] = 0$ holds for\nany two perpendicular vectors $u$ and $v$. Our test is based on the\nKolmogorov-Smirnov statistic, and its rejection region is found via the\nspherically symmetric bootstrap. We show the consistency of the spherically\nsymmetric bootstrap test using a general Donsker theorem which is of some\nindependent interest. For the case of testing for elliptical symmetry, the\nKolmogorov-Smirnov statistic has an asymptotic drift term due to the estimated\nlocation and scale parameters. Therefore, an additional standardization is\nrequired in the bootstrap procedure. In a simulation study, the size and the\npower properties of our tests are assessed for several distributions and the\nperformance is compared to that of several competing procedures.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:15:41 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Albisetti", "Isaia", ""], ["Balabdaoui", "Fadoua", ""], ["Holzmann", "Hajo", ""]]}, {"id": "2004.13446", "submitter": "Ankit Sharma", "authors": "Ankit Sharma, Garima Gupta, Ranjitha Prasad, Arnab Chatterjee,\n  Lovekesh Vig, Gautam Shroff", "title": "MultiMBNN: Matched and Balanced Causal Inference with Neural Networks", "comments": "7 pages, 3 figures, Accepted in ESANN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference (CI) in observational studies has received a lot of\nattention in healthcare, education, ad attribution, policy evaluation, etc.\nConfounding is a typical hazard, where the context affects both, the treatment\nassignment and response. In a multiple treatment scenario, we propose the\nneural network based MultiMBNN, where we overcome confounding by employing\ngeneralized propensity score based matching, and learning balanced\nrepresentations. We benchmark the performance on synthetic and real-world\ndatasets using PEHE, and mean absolute percentage error over ATE as metrics.\nMultiMBNN outperforms the state-of-the-art algorithms for CI such as TARNet and\nPerfect Match (PM).\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 11:58:38 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 05:34:17 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 10:58:56 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Sharma", "Ankit", ""], ["Gupta", "Garima", ""], ["Prasad", "Ranjitha", ""], ["Chatterjee", "Arnab", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "2004.13459", "submitter": "Laura Forastiere", "authors": "Davide Del Prete, Laura Forastiere, Valerio Leone Sciabolazza", "title": "Causal Inference on Networks under Continuous Treatment Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a methodology to draw causal inference in a\nnon-experimental setting subject to network interference. Specifically, we\ndevelop a generalized propensity score-based estimator that allows us to\nestimate both direct and spillover effects of a continuous treatment, which\nspreads through weighted and directed edges of a network. To showcase this\nmethodology, we investigate whether and how spillover effects shape the optimal\nlevel of policy interventions in agricultural markets. Our results show that,\nin this context, neglecting interference may lead to a downward bias when\nassessing policy effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:28:30 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Del Prete", "Davide", ""], ["Forastiere", "Laura", ""], ["Sciabolazza", "Valerio Leone", ""]]}, {"id": "2004.13464", "submitter": "Konstantina Chalkou", "authors": "Konstantina Chalkou, Ewout Steyerberg, Matthias Egger, Andrea Manca,\n  Fabio Pellegrini, Georgia Salanti", "title": "A two-stage prediction model for heterogeneous effects of many treatment\n  options: application to drugs for Multiple Sclerosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment effects vary across different patients and estimation of this\nvariability is important for clinical decisions. The aim is to develop a model\nto estimate the benefit of alternative treatment options for individual\npatients. Hence, we developed a two-stage prediction model for heterogeneous\ntreatment effects, by combining prognosis research and network meta-analysis\nmethods when individual patient data is available. In a first stage, we develop\na prognostic model and we predict the baseline risk of the outcome. In the\nsecond stage, we use this baseline risk score from the first stage as a single\nprognostic factor and effect modifier in a network meta-regression model. We\napply the approach to a network meta-analysis of three randomized clinical\ntrials comparing the relapse rate in Natalizumab, Glatiramer Acetate and\nDimethyl Fumarate including 3590 patients diagnosed with relapsing-remitting\nmultiple sclerosis. We find that the baseline risk score modifies the relative\nand absolute treatment effects. Several patient characteristics such as age and\ndisability status impact on the baseline risk of relapse, and this in turn\nmoderates the benefit that may be expected for each of the treatments. For\nhigh-risk patients, the treatment that minimizes the risk to relapse in two\nyears is Natalizumab, whereas for low-risk patients Dimethyl Fumarate Fumarate\nmight be a better option. Our approach can be easily extended to all outcomes\nof interest and has the potential to inform a personalised treatment approach.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:57:26 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 17:59:05 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chalkou", "Konstantina", ""], ["Steyerberg", "Ewout", ""], ["Egger", "Matthias", ""], ["Manca", "Andrea", ""], ["Pellegrini", "Fabio", ""], ["Salanti", "Georgia", ""]]}, {"id": "2004.13556", "submitter": "Ramin Moradi", "authors": "Seyed Fouad Karimian, Ramin Moradi, Sergio Cofre-Martel, Katrina M.\n  Groth, Mohammad Modarres", "title": "Neural Network and Particle Filtering: A Hybrid Framework for Crack\n  Propagation Prediction", "comments": "10 Pages, 9 figures, Accepted for publication in proceedings of\n  Structural Health Monitoring & NonDestructive Testing (SHM-NDT) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crack detection, length estimation, and Remaining Useful Life (RUL)\nprediction are among the most studied topics in reliability engineering.\nSeveral research efforts have studied physics of failure (PoF) of different\nmaterials, along with data-driven approaches as an alternative to the\ntraditional PoF studies. To bridge the gap between these two techniques, we\npropose a novel hybrid framework for fatigue crack length estimation and\nprediction. Physics-based modeling is performed on the fracture mechanics\ndegradation data by estimating parameters of the Paris Law, including the\nassociated uncertainties. Crack length estimations are inferred by feeding\nmanually extracted features from ultrasonic signals to a Neural Network (NN).\nThe crack length prediction is then performed using the Particle Filter (PF)\napproach, which takes the Paris Law as a move function and uses the NN's output\nas observation to update the crack growth path. This hybrid framework combines\nmachine learning, physics-based modeling, and Bayesian updating with promising\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 02:45:38 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Karimian", "Seyed Fouad", ""], ["Moradi", "Ramin", ""], ["Cofre-Martel", "Sergio", ""], ["Groth", "Katrina M.", ""], ["Modarres", "Mohammad", ""]]}, {"id": "2004.13611", "submitter": "Rachel Marceau West", "authors": "Devan V. Mehrotra and Rachel Marceau West", "title": "Survival Analysis Using a 5-Step Stratified Testing and Amalgamation\n  Routine in Randomized Clinical Trials", "comments": "40 pages; 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized clinical trials are often designed to assess whether a test\ntreatment prolongs survival relative to a control treatment. Increased patient\nheterogeneity, while desirable for generalizability of results, can weaken the\nability of common statistical approaches to detect treatment differences,\npotentially hampering the regulatory approval of safe and efficacious\ntherapies. A novel solution to this problem is proposed. A list of baseline\ncovariates that have the potential to be prognostic for survival under either\ntreatment is pre-specified in the analysis plan. At the analysis stage, using\nall observed survival times but blinded to patient-level treatment assignment,\n'noise' covariates are removed with elastic net Cox regression. The shortened\ncovariate list is used by a conditional inference tree algorithm to segment the\nheterogeneous trial population into subpopulations of prognostically\nhomogeneous patients (risk strata). After patient-level treatment unblinding, a\ntreatment comparison is done within each formed risk stratum and stratum-level\nresults are combined for overall statistical inference. The impressive\npower-boosting performance of our proposed 5-step stratified testing and\namalgamation routine (5-STAR), relative to that of the logrank test and other\ncommon approaches that do not leverage inherently structured patient\nheterogeneity, is illustrated using a hypothetical and two real datasets along\nwith simulation results. Furthermore, the importance of reporting stratum-level\ncomparative treatment effects (time ratios from accelerated failure time model\nfits in conjunction with model averaging and, as needed, hazard ratios from Cox\nproportional hazard model fits) is highlighted as a potential enabler of\npersonalized medicine. A fiveSTAR R package is available at\nhttps://github.com/rmarceauwest/fiveSTAR.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:44:41 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Mehrotra", "Devan V.", ""], ["West", "Rachel Marceau", ""]]}, {"id": "2004.13689", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir O Storvik, Paul E Grini, Melinka A Butenko", "title": "A Bayesian binomial regression model with latent Gaussian processes for\n  modelling DNA methylation", "comments": "10 pages, 3 tables, 3 figures", "journal-ref": "Austrian Journal of Statistics, 2020, 49(4), 46-56", "doi": "10.17713/ajs.v49i4.1124", "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epigenetic observations are represented by the total number of reads from a\ngiven pool of cells and the number of methylated reads, making it reasonable to\nmodel this data by a binomial distribution. There are numerous factors that can\ninfluence the probability of success in a particular region. Moreover, there is\na strong spatial (alongside the genome) dependence of these probabilities. We\nincorporate dependence on the covariates and the spatial dependence of the\nmethylation probability for observations from a pool of cells by means of a\nbinomial regression model with a latent Gaussian field and a logit link\nfunction. We apply a Bayesian approach including prior specifications on model\nconfigurations. We run a mode jumping Markov chain Monte Carlo algorithm\n(MJMCMC) across different choices of covariates in order to obtain the joint\nposterior distribution of parameters and models. This also allows finding the\nbest set of covariates to model methylation probability within the genomic\nregion of interest and individual marginal inclusion probabilities of the\ncovariates.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:44:50 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir O", ""], ["Grini", "Paul E", ""], ["Butenko", "Melinka A", ""]]}, {"id": "2004.13775", "submitter": "Erich Greene", "authors": "E. J. Greene (1), P. Peduzzi (1), J. Dziura (2 and 1), C. Meng, M. E.\n  Miller (3), T. G. Travison (4), D. Esserman (1) ((1) Department of\n  Biostatistics, Yale School of Public Health, (2) Department of Emergency\n  Medicine, Yale School of Medicine, (3) Department of Emergency Medicine, Wake\n  Forest School of Medicine, (4) Marcus Institute for Aging Research, Hebrew\n  SeniorLife, Harvard Medical School)", "title": "Estimation of ascertainment bias and its effect on power in clinical\n  trials with time-to-event outcomes", "comments": "31 pages, 11 figures; submitted to Statistics in Medicine", "journal-ref": null, "doi": "10.1002/sim.8842", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the gold standard for clinical trials is to blind all parties --\nparticipants, researchers, and evaluators -- to treatment assignment, this is\nnot always a possibility. When some or all of the above individuals know the\ntreatment assignment, this leaves the study open to the introduction of\npost-randomization biases. In the Strategies to Reduce Injuries and Develop\nConfidence in Elders (STRIDE) trial, we were presented with the potential for\nthe unblinded clinicians administering the treatment, as well as the\nindividuals enrolled in the study, to introduce ascertainment bias into some\nbut not all events comprising the primary outcome. In this manuscript, we\npresent ways to estimate the ascertainment bias for a time-to-event outcome,\nand discuss its impact on the overall power of a trial versus changing of the\noutcome definition to a more stringent unbiased definition that restricts\nattention to measurements less subject to potentially differential assessment.\nWe found that for the majority of situations, it is better to revise the\ndefinition to a more stringent definition, as was done in STRIDE, even though\nfewer events may be observed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 18:56:43 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 17:18:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Greene", "E. J.", "", "2 and 1"], ["Peduzzi", "P.", "", "2 and 1"], ["Dziura", "J.", "", "2 and 1"], ["Meng", "C.", ""], ["Miller", "M. E.", ""], ["Travison", "T. G.", ""], ["Esserman", "D.", ""]]}, {"id": "2004.13880", "submitter": "Marios Papamichalis V.", "authors": "Papamichalis Marios", "title": "Bayesian Model Selection on Random Networks", "comments": "18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A general Bayesian framework for model selection on random network models\nregarding their features is considered. The goal is to develop a principle\nBayesian model selection approach to compare different fittable, not\nnecessarily nested, models for inference on those network realisations. The\ncriterion for random network models regarding the comparison is formulated via\nBayes factors and penalizing using the mostwidely used loss functions.\nParametrizations are different in different spaces. To overcome this problem we\nincorporate and encode different aspects of complexities in terms of observable\nspaces. Thus, given a range of values for a feature, network realisationsare\nextracted. The proposed principle approach is based on finding random network\nmodels, such that a reasonable trade off between the interested feature and the\ncomplexity of the model is preserved, avoiding overfitting problems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 22:39:27 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Marios", "Papamichalis", ""]]}, {"id": "2004.13892", "submitter": "Jiahua Chen", "authors": "Jiahua Chen, Yukun Liu, and James Zidek", "title": "Permutation tests under a rotating sampling plan with clustered data", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a population consisting of clusters of sampling units, evolving\ntemporally, spatially, or according to other dynamics. We wish to monitor the\nevolution of its means, medians, or other parameters. For administrative\nconvenience and informativeness, clustered data are often collected via a\nrotating plan. Under rotating plans, the observations in the same clusters are\ncorrelated, and observations on the same unit collected on different occasions\nare also correlated. Ignoring this correlation structure may lead to invalid\ninference procedures. Accommodating cluster structure in parametric models is\ndifficult or will have a high level of misspecification risk. In this paper, we\nexplore exchangeability in clustered data collected via a rotating sampling\nplan to develop a permutation scheme for testing various hypotheses of\ninterest. We also introduce a semiparametric density ratio model to facilitate\nthe multiple population structure in rotating sampling plans. The combination\nensures the validity of the inference methods while extracting maximum\ninformation from the sampling plan. A simulation study indicates that the\nproposed tests firmly control the type I error whether or not the data are\nclustered. The use of the density ratio model improves the power of the tests.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 23:29:34 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Chen", "Jiahua", ""], ["Liu", "Yukun", ""], ["Zidek", "James", ""]]}, {"id": "2004.13914", "submitter": "Hung Hung", "authors": "Hung Hung, Su-Yun Huang, and Ching-Kang Ing", "title": "A generalized information criterion for high-dimensional PCA rank\n  selection", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is the most commonly used statistical\nprocedure for dimension reduction. An important issue for applying PCA is to\ndetermine the rank, which is the number of dominant eigenvalues of the\ncovariance matrix. The Akaike information criterion (AIC) and Bayesian\ninformation criterion (BIC) are among the most widely used rank selection\nmethods. Both use the number of free parameters for assessing model complexity.\nIn this work, we adopt the generalized information criterion (GIC) to propose a\nnew method for PCA rank selection under the high-dimensional framework. The GIC\nmodel complexity takes into account the sizes of covariance eigenvalues and can\nbe better adaptive to practical applications. Asymptotic properties of GIC are\nderived and the selection consistency is established under the generalized\nspiked covariance model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 01:44:25 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 01:32:04 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Hung", "Hung", ""], ["Huang", "Su-Yun", ""], ["Ing", "Ching-Kang", ""]]}, {"id": "2004.13962", "submitter": "Simon Mak", "authors": "Jared D. Huling, Simon Mak", "title": "Energy Balancing of Covariate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias in causal comparisons has a direct correspondence with distributional\nimbalance of covariates between treatment groups. Weighting strategies such as\ninverse propensity score weighting attempt to mitigate bias by either modeling\nthe treatment assignment mechanism or balancing specified covariate moments.\nThis paper introduces a new weighting method, called energy balancing, which\ninstead aims to balance weighted covariate distributions. By directly targeting\ndistributional imbalance, the proposed weighting strategy can be flexibly\nutilized in a wide variety of causal analyses, including the estimation of\naverage treatment effects and individualized treatment rules. Our energy\nbalancing weights (EBW) approach has several advantages over existing weighting\ntechniques. First, it offers a model-free and robust approach for obtaining\ncovariate balance that does not require tuning parameters, obviating the need\nfor modeling decisions of secondary nature to the scientific question at hand.\nSecond, since this approach is based on a genuine measure of distributional\nbalance, it provides a means for assessing the balance induced by a given set\nof weights for a given dataset. Finally, the proposed method is computationally\nefficient and has desirable theoretical guarantees under mild conditions. We\ndemonstrate the effectiveness of this EBW approach in a suite of simulation\nexperiments, and in studies on the safety of right heart catheterization and\nthe effect of indwelling arterial catheters.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 05:25:35 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 04:08:05 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 04:55:36 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 18:35:22 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Huling", "Jared D.", ""], ["Mak", "Simon", ""]]}, {"id": "2004.13963", "submitter": "Xuekui Zhang", "authors": "Li Xing, Xuekui Zhang, Ardo van den Hout, Scott Hofer, Graciela Muniz\n  Terrera", "title": "Optimal Study Design for Reducing Variances of Coefficient Estimators in\n  Change-Point Models", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In longitudinal studies, we observe measurements of the same variables at\ndifferent time points to track the changes in their pattern over time. In such\nstudies, scheduling of the data collection waves (i.e. time of participants'\nvisits) is often pre-determined to accommodate ease of project management and\ncompliance. Hence, it is common to schedule those visits at equally spaced time\nintervals. However, recent publications based on simulated experiments indicate\nthat the power of studies and the precision of model parameter estimators is\nrelated to the participants' visiting schemes.\n  In this paper, we consider the longitudinal studies that investigate the\nchanging pattern of a disease outcome, (e.g. the accelerated cognitive decline\nof senior adults). Such studies are often analyzed by the broken-stick model,\nconsisting of two segments of linear models connected at an unknown\nchange-point. We formulate this design problem into a high-dimensional\noptimization problem and derive its analytical solution. Based on this\nsolution, we propose an optimal design of the visiting scheme that maximizes\nthe power (i.e. reduce the variance of estimators) to identify the onset of\naccelerated decline. Using both simulation studies and evidence from real data,\nwe demonstrate our optimal design outperforms the standard equally-spaced\ndesign.\n  Applying our novel design to plan the longitudinal studies, researchers can\nimprove the power of detecting pattern change without collecting extra data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 05:34:57 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Xing", "Li", ""], ["Zhang", "Xuekui", ""], ["Hout", "Ardo van den", ""], ["Hofer", "Scott", ""], ["Terrera", "Graciela Muniz", ""]]}, {"id": "2004.13967", "submitter": "Subhadra Dasgupta", "authors": "Subhadra Dasgupta, Siuli Mukhopadhyay and Jonathan Keith", "title": "Optimal designs for some bivariate cokriging models", "comments": "37 Pages, 1 figure. Page 26 and 27 typo corrected for calculating the\n  risk functions. Page 27 typo in Table 3 header", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the estimation and design aspects of a bivariate\ncollocated cokriging experiment. For a large class of covariance matrices a\nlinear dependency criterion is identified, which allows the best linear\nunbiased estimator of the primary variable in a bivariate collocated cokriging\nsetup to reduce to a univariate kriging estimator. Exact optimal designs for\nefficient prediction for such simple and ordinary cokriging models, with one\ndimensional inputs are determined. Designs are found by minimizing the maximum\nand integrated prediction variance. For simple and ordinary cokriging models\nwith known covariance parameters, the equispaced design is shown to be optimal\nfor both criterion functions. The more realistic scenario of unknown covariance\nparameters is addressed by assuming prior distributions on the parameter\nvector, thus adopting a Bayesian approach to the design problem. The equispaced\ndesign is proved to be the Bayesian optimal design for both criteria. The work\nis motivated by designing an optimal water monitoring system for an Indian\nriver.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:03:44 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 10:19:54 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 06:01:46 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dasgupta", "Subhadra", ""], ["Mukhopadhyay", "Siuli", ""], ["Keith", "Jonathan", ""]]}, {"id": "2004.13991", "submitter": "Tomoyuki Nakagawa", "authors": "Tomoyuki Nakagawa, Shintaro Hashimoto", "title": "On default priors for robust Bayesian estimation with divergences", "comments": "22pages", "journal-ref": null, "doi": "10.3390/e23010029", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents objective priors for robust Bayesian estimation against\noutliers based on divergences. The minimum $\\gamma$-divergence estimator is\nwell-known to work well estimation against heavy contamination. The robust\nBayesian methods by using quasi-posterior distributions based on divergences\nhave been also proposed in recent years. In objective Bayesian framework, the\nselection of default prior distributions under such quasi-posterior\ndistributions is an important problem. In this study, we provide some\nproperties of reference and moment matching priors under the quasi-posterior\ndistribution based on the $\\gamma$-divergence. In particular, we show that the\nproposed priors are approximately robust under the condition on the\ncontamination distribution without assuming any conditions on the contamination\nratio. Some simulation studies are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 07:08:57 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 03:26:29 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 05:00:02 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Nakagawa", "Tomoyuki", ""], ["Hashimoto", "Shintaro", ""]]}, {"id": "2004.14009", "submitter": "Karl Oskar Ekvall", "authors": "Karl Oskar Ekvall", "title": "Targeted Principal Components Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a principal components regression method based on maximizing a\njoint pseudo-likelihood for responses and predictors. Our method uses both\nresponses and predictors to select linear combinations of the predictors\nrelevant for the regression, thereby addressing an oft-cited deficiency of\nconventional principal components regression. The proposed estimator is shown\nto be consistent in a wide range of settings, including ones with non-normal\nand dependent observations; conditions on the first and second moments suffice\nif the number of predictors ($p$) is fixed and the number of observations ($n$)\ntends to infinity and dependence is weak, while stronger distributional\nassumptions are needed when $p \\to \\infty$ with $n$. We obtain the estimator's\nasymptotic distribution as the projection of a multivariate normal random\nvector onto a tangent cone of the parameter set at the true parameter, and find\nthe estimator is asymptotically more efficient than competing ones. In\nsimulations our method is substantially more accurate than conventional\nprincipal components regression and compares favorably to partial least squares\nand predictor envelopes. The method is illustrated in a data example with\ncross-sectional prediction of stock returns.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:09:31 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 11:34:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ekvall", "Karl Oskar", ""]]}, {"id": "2004.14066", "submitter": "Katherine Lee J", "authors": "Katherine J Lee, Kate Tilling, Rosie P Cornish, Roderick JA Little,\n  Melanie L Bell, Els Goetghebeur, Joseph W Hogan and James R Carpenter (for\n  the STRATOS initiative)", "title": "Framework for the Treatment And Reporting of Missing data in\n  Observational Studies: The TARMOS framework", "comments": "37 pages, including 3 Figures, 1 table and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data are ubiquitous in medical research. Although there is increasing\nguidance on how to handle missing data, practice is changing slowly and\nmisapprehensions abound, particularly in observational research. We present a\npractical framework for handling and reporting the analysis of incomplete data\nin observational studies, which we illustrate using a case study from the Avon\nLongitudinal Study of Parents and Children. The framework consists of three\nsteps: 1) Develop an analysis plan specifying the analysis model and how\nmissing data are going to be addressed. An important consideration is whether a\ncomplete records analysis is likely to be valid, whether multiple imputation or\nan alternative approach is likely to offer benefits, and whether a sensitivity\nanalysis regarding the missingness mechanism is required. 2) Explore the data,\nchecking the methods outlined in the analysis plan are appropriate, and conduct\nthe pre-planned analysis. 3) Report the results, including a description of the\nmissing data, details on how the missing data were addressed, and the results\nfrom all analyses, interpreted in light of the missing data and the clinical\nrelevance. This framework seeks to support researchers in thinking\nsystematically about missing data, and transparently reporting the potential\neffect on the study results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:38:54 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Lee", "Katherine J", "", "for\n  the STRATOS initiative"], ["Tilling", "Kate", "", "for\n  the STRATOS initiative"], ["Cornish", "Rosie P", "", "for\n  the STRATOS initiative"], ["Little", "Roderick JA", "", "for\n  the STRATOS initiative"], ["Bell", "Melanie L", "", "for\n  the STRATOS initiative"], ["Goetghebeur", "Els", "", "for\n  the STRATOS initiative"], ["Hogan", "Joseph W", "", "for\n  the STRATOS initiative"], ["Carpenter", "James R", "", "for\n  the STRATOS initiative"]]}, {"id": "2004.14108", "submitter": "Carol Alexander Prof.", "authors": "Carol Alexander and Yang Han", "title": "Static and Dynamic Models for Multivariate Distribution Forecasts:\n  Proper Scoring Rule Tests of Factor-Quantile vs. Multivariate GARCH Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of static and dynamic models exist to forecast Value-at-Risk and\nother quantile-related metrics used in financial risk management. Industry\npractice tends to favour simpler, static models such as historical simulation\nor its variants whereas most academic research centres on dynamic models in the\nGARCH family. While numerous studies examine the accuracy of multivariate\nmodels for forecasting risk metrics, there is little research on accurately\npredicting the entire multivariate distribution. Yet this is an essential\nelement of asset pricing or portfolio optimization problems having non-analytic\nsolutions. We approach this highly complex problem using a variety of proper\nmultivariate scoring rules to evaluate over 100,000 forecasts of\neight-dimensional multivariate distributions: of exchange rates, interest rates\nand commodity futures. This way we test the performance of static models, viz.\nempirical distribution functions and a new factor-quantile model, with commonly\nused dynamic models in the asymmetric multivariate GARCH class.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:04:37 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Alexander", "Carol", ""], ["Han", "Yang", ""]]}, {"id": "2004.14479", "submitter": "Marco Henrique de Almeida In\\'acio", "authors": "Marco H A In\\'acio", "title": "Monte Carlo simulation studies on Python using the sstudy package with\n  SQL databases as storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performance assessment is a key issue in the process of proposing new machine\nlearning/statistical estimators. A possible method to complete such task is by\nusing simulation studies, which can be defined as the procedure of estimating\nand comparing properties (such as predictive power) of estimators (and other\nstatistics) by averaging over many replications given a true distribution;\ni.e.: generating a dataset, fitting the estimator, calculating and storing the\npredictive power, and then repeating the procedure many times and finally\naveraging over the stored predictive powers. Given that, in this paper, we\npresent sstudy: a Python package designed to simplify the preparation of\nsimulation studies using SQL database engines as the storage system; more\nspecifically, we present its basic features, usage examples and references to\nthe its documentation. We also present a short statistical description of the\nsimulation study procedure with a simplified explanation of what is being\nestimated by it, as well as some examples of applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:49:43 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 14:23:15 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 12:49:31 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["In\u00e1cio", "Marco H A", ""]]}, {"id": "2004.14522", "submitter": "Andriy Olenko", "authors": "Nikolai Leonenko, Ravindi Nanayakkara, Andriy Olenko", "title": "Analysis of Spherical Monofractal and Multifractal Random Fields", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": "10.1007/s00477-020-01911-z", "report-no": null, "categories": "math.PR stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R\\'enyi function plays an important role in the analysis of multifractal\nrandom fields. For random fields on the sphere, there are three models in the\nliterature where the R\\'enyi function is known explicitly. The theoretical part\nof the article presents multifractal random fields on the sphere and develops\nspecific models where the R\\'enyi function can be computed explicitly. Then\nthese results are applied to the Cosmic Microwave Background Radiation data\ncollected from the Planck mission. The main statistical model used to describe\nthese data in the literature is isotropic Gaussian fields. We present numerical\nmultifractality studies and methodology based on simulating random fields,\ncomputing the R\\'enyi function and the multifractal spectrum for different\nscenarios and actual CMB data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:10:16 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 01:56:41 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 04:14:19 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Leonenko", "Nikolai", ""], ["Nanayakkara", "Ravindi", ""], ["Olenko", "Andriy", ""]]}, {"id": "2004.14700", "submitter": "Jennifer Pohle", "authors": "Jennifer Pohle, Roland Langrock, Mihaela van der Schaar, Ruth King and\n  Frants Havmand Jensen", "title": "A primer on coupled state-switching models for multiple interacting time\n  series", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-switching models such as hidden Markov models or Markov-switching\nregression models are routinely applied to analyse sequences of observations\nthat are driven by underlying non-observable states. Coupled state-switching\nmodels extend these approaches to address the case of multiple observation\nsequences whose underlying state variables interact. In this paper, we provide\nan overview of the modelling techniques related to coupling in state-switching\nmodels, thereby forming a rich and flexible statistical framework particularly\nuseful for modelling correlated time series. Simulation experiments demonstrate\nthe relevance of being able to account for an asynchronous evolution as well as\ninteractions between the underlying latent processes. The models are further\nillustrated using two case studies related to a) interactions between a dolphin\nmother and her calf as inferred from movement data; and b) electronic health\nrecord data collected on 696 patients within an intensive care unit.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 11:31:29 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Pohle", "Jennifer", ""], ["Langrock", "Roland", ""], ["van der Schaar", "Mihaela", ""], ["King", "Ruth", ""], ["Jensen", "Frants Havmand", ""]]}, {"id": "2004.14733", "submitter": "Erik Scharw\\\"achter", "authors": "Erik Scharw\\\"achter and Emmanuel M\\\"uller", "title": "Does Terrorism Trigger Online Hate Speech? On the Association of Events\n  and Time Series", "comments": "19 pages, 8 figures, to appear in the Annals of Applied Statistics,\n  source code available at https://github.com/diozaka/pECA", "journal-ref": "Annals of Applied Statistics 14 (3) 2020, pp. 1285-1303", "doi": "10.1214/20-AOAS1338", "report-no": null, "categories": "stat.AP cs.CY cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech is ubiquitous on the Web. Recently, the offline causes that\ncontribute to online hate speech have received increasing attention. A\nrecurring question is whether the occurrence of extreme events offline\nsystematically triggers bursts of hate speech online, indicated by peaks in the\nvolume of hateful social media posts. Formally, this question translates into\nmeasuring the association between a sparse event series and a time series. We\npropose a novel statistical methodology to measure, test and visualize the\nsystematic association between rare events and peaks in a time series. In\ncontrast to previous methods for causal inference or independence tests on time\nseries, our approach focuses only on the timing of events and peaks, and no\nother distributional characteristics. We follow the framework of event\ncoincidence analysis (ECA) that was originally developed to correlate point\nprocesses. We formulate a discrete-time variant of ECA and derive all required\ndistributions to enable analyses of peaks in time series, with a special focus\non serial dependencies and peaks over multiple thresholds. The analysis gives\nrise to a novel visualization of the association via quantile-trigger rate\nplots. We demonstrate the utility of our approach by analyzing whether Islamist\nterrorist attacks in Western Europe and North America systematically trigger\nbursts of hate speech and counter-hate speech on Twitter.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:47:22 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 10:15:18 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Scharw\u00e4chter", "Erik", ""], ["M\u00fcller", "Emmanuel", ""]]}, {"id": "2004.14817", "submitter": "Matthew Koslovsky", "authors": "Matthew D. Koslovsky, Kristi L. Hoffman, Carrie R. Daniel, Marina\n  Vannucci", "title": "A Bayesian model of microbiome data for simultaneous identification of\n  covariate associations and prediction of phenotypic outcomes", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major research questions regarding human microbiome studies is the\nfeasibility of designing interventions that modulate the composition of the\nmicrobiome to promote health and cure disease. This requires extensive\nunderstanding of the modulating factors of the microbiome, such as dietary\nintake, as well as the relation between microbial composition and phenotypic\noutcomes, such as body mass index (BMI). Previous efforts have modeled these\ndata separately, employing two-step approaches that can produce biased\ninterpretations of the results. Here, we propose a Bayesian joint model that\nsimultaneously identifies clinical covariates associated with microbial\ncomposition data and predicts a phenotypic response using information contained\nin the compositional data. Using spike-and-slab priors, our approach can handle\nhigh-dimensional compositional as well as clinical data. Additionally, we\naccommodate the compositional structure of the data via balances and\noverdispersion typically found in microbial samples. We apply our model to\nunderstand the relations between dietary intake, microbial samples, and BMI. In\nthis analysis, we find numerous associations between microbial taxa and dietary\nfactors that may lead to a microbiome that is generally more hospitable to the\ndevelopment of chronic diseases, such as obesity. Additionally, we demonstrate\non simulated data how our method outperforms two-step approaches and also\npresent a sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:21:07 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Koslovsky", "Matthew D.", ""], ["Hoffman", "Kristi L.", ""], ["Daniel", "Carrie R.", ""], ["Vannucci", "Marina", ""]]}, {"id": "2004.14823", "submitter": "Shangzhi Hong", "authors": "Shangzhi Hong, Yuqi Sun, Hanying Li, Henry S. Lynn", "title": "Multiple imputation using chained random forests: a preliminary study\n  based on the empirical distribution of out-of-bag prediction errors", "comments": "Initial version, 6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data are common in data analyses in biomedical fields, and imputation\nmethods based on random forests (RF) have become widely accepted, as the RF\nalgorithm can achieve high accuracy without the need for specification of data\ndistributions or relationships. However, the predictions from RF do not contain\ninformation about prediction uncertainty, which was unacceptable for multiple\nimputation. Available RF-based multiple imputation methods tried to do proper\nmultiple imputation either by sampling directly from observations under\npredicting nodes without accounting for the prediction error or by making\nnormality assumption about the prediction error distribution. In this study, a\nnovel RF-based multiple imputation method was proposed by constructing\nconditional distributions the empirical distribution of out-of-bag prediction\nerrors. The proposed method was compared with previous method with parametric\nassumptions about RF's prediction errors and predictive mean matching based on\nsimulation studies on data with presence of interaction term. The proposed\nnon-parametric method can deliver valid multiple imputation results. The\naccompanying R package for this study is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:29:56 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Hong", "Shangzhi", ""], ["Sun", "Yuqi", ""], ["Li", "Hanying", ""], ["Lynn", "Henry S.", ""]]}, {"id": "2004.14824", "submitter": "Mats Julius Stensrud", "authors": "Mats J. Stensrud, Miguel A. Hern\\'an, Eric J. Tchetgen Tchetgen, James\n  M. Robins, Vanessa Didelez, Jessica G. Young", "title": "Generalized interpretation and identification of separable effects in\n  competing event settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In competing event settings, a counterfactual contrast of cause-specific\ncumulative incidences quantifies the total causal effect of a treatment on the\nevent of interest. However, effects of treatment on the competing event may\nindirectly contribute to this total effect, complicating its interpretation. We\npreviously proposed the separable effects (Stensrud et al, 2019) to define\ndirect and indirect effects of the treatment on the event of interest. This\ndefinition presupposes a treatment decomposition into two components acting\nalong two separate causal pathways, one exclusively outside of the competing\nevent and the other exclusively through it. Unlike previous definitions of\ndirect and indirect effects, the separable effects can be subject to empirical\nscrutiny in a study where separate interventions on the treatment components\nare available. Here we extend and generalize the notion of the separable\neffects in several ways, allowing for interpretation, identification and\nestimation under considerably weaker assumptions. We propose and discuss a\ndefinition of separable effects that is applicable to general time-varying\nstructures, where the separable effects can still be meaningfully interpreted,\neven when they cannot be regarded as direct and indirect effects. We further\nderive weaker conditions for identification of separable effects in\nobservational studies where decomposed treatments are not yet available; in\nparticular, these conditions allow for time-varying common causes of the event\nof interest, the competing events and loss to follow-up. For these general\nsettings, we propose semi-parametric weighted estimators that are\nstraightforward to implement. As an illustration, we apply the estimators to\nstudy the separable effects of intensive blood pressure therapy on acute kidney\ninjury, using data from a randomized clinical trial.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:32:29 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 02:47:07 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Stensrud", "Mats J.", ""], ["Hern\u00e1n", "Miguel A.", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["Robins", "James M.", ""], ["Didelez", "Vanessa", ""], ["Young", "Jessica G.", ""]]}, {"id": "2004.14851", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh and Magne Thoresen", "title": "A robust variable screening procedure for ultra-high dimensional data", "comments": "Pre-print, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in ultra-high dimensional regression problems has become\nan important issue. In such situations, penalized regression models may face\ncomputational problems and some pre screening of the variables may be\nnecessary. A number of procedures for such pre-screening has been developed;\namong them the sure independence screening (SIS) enjoys some popularity.\nHowever, SIS is vulnerable to outliers in the data, and in particular in small\nsamples this may lead to faulty inference. In this paper, we develop a new\nrobust screening procedure. We build on the density power divergence (DPD)\nestimation approach and introduce DPD-SIS and its extension iterative DPD-SIS.\nWe illustrate the behavior of the methods through extensive simulation studies\nand show that they are superior to both the original SIS and other robust\nmethods when there are outliers in the data. We demonstrate the claimed\nrobustness through use of influence functions, and we discuss appropriate\nchoice of the tuning parameter $\\alpha$. Finally, we illustrate its use on a\nsmall dataset from a study on regulation of lipid metabolism.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:05:15 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ghosh", "Abhik", ""], ["Thoresen", "Magne", ""]]}, {"id": "2004.14912", "submitter": "Luiz Max Carvalho PhD", "authors": "Luiz Max Carvalho and Joseph G. Ibrahim", "title": "On the normalized power prior", "comments": "Code available at\n  https://github.com/maxbiostat/propriety_power_priors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power prior is a popular tool for constructing informative prior\ndistributions based on historical data. The method consists of raising the\nlikelihood to a discounting factor in order to control the amount of\ninformation borrowed from the historical data. It is customary to perform a\nsensitivity analysis reporting results for a range of values of the discounting\nfactor. However, one often wishes to assign it a prior distribution and\nestimate it jointly with the parameters, which in turn necessitates the\ncomputation of a normalising constant. In this paper we are concerned with how\nto recycle computations from a sensitivity analysis in order to approximately\nsample from joint posterior of the parameters and the discounting factor. We\nfirst show a few important properties of the normalising constant and then use\nthese results to motivate a bisection-type algorithm for computing it on a\nfixed budget of evaluations. We give a large array of illustrations and discuss\ncases where the normalising constant is known in closed-form and where it is\nnot. We show that the proposed method produces approximate posteriors that are\nvery close to the exact distributions when those are available and also\nproduces posteriors that cover the data-generating parameters with higher\nprobability in the intractable case. Our results show that proper inclusion the\nnormalising constant is crucial to the correct quantification of uncertainty\nand that the proposed method is an accurate and easy to implement technique to\ninclude this normalisation, being applicable to a large class of models.\n  Key-words: Doubly-intractable; elicitation; historical data; normalisation;\npower prior; sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:11:59 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Carvalho", "Luiz Max", ""], ["Ibrahim", "Joseph G.", ""]]}]