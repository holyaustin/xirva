[{"id": "0905.0116", "submitter": "Paul Seed", "authors": "Paul T Seed", "title": "Correspondence: The use of cost information when defining critical\n  values for prediction of rare events using logistic regression and similar\n  methods", "comments": "Response to Berk (2009) Forecasting murder within a population of\n  probationers and parolees: a high stakes application of statistical learning.\n  J.R. Statist. Soc. A, 172, 191-211", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing a rare and serious possibility against a more common and less\nserious one is a familiar problem in many situations, such as the prediction of\nrare diseases. The relative costs of forecasting errors can be used for any\nprediction method that gives an estimated probability of a future event. The\nprobability at which the likely cost (defined as cost x probability) of a\npossible false negative is exactly equal to that of a possible false positive\ngives the relevant cutpoint and all subjects with probability of disease\ngreater than this have a positive test result. All standard methods of logistic\nregression will give the log-odds and hence the predicted probability of a\npositive outcome for every subject:\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2009 16:41:33 GMT"}], "update_date": "2009-05-04", "authors_parsed": [["Seed", "Paul T", ""]]}, {"id": "0905.0436", "submitter": "Radu Craiu Dr", "authors": "Fang Yao, Radu V. Craiu and Benjamin Reiser", "title": "Nonparametric Covariate Adjustment for Receiver Operating Characteristic\n  Curves", "comments": null, "journal-ref": "Canadian Journal of Statistics, vol 38, No. 1, p 27-46, 2010", "doi": "10.1002/cjs.10044", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of a diagnostic test is typically characterised using the\nreceiver operating characteristic (ROC) curve. Summarising indexes such as the\narea under the ROC curve (AUC) are used to compare different tests as well as\nto measure the difference between two populations. Often additional information\nis available on some of the covariates which are known to influence the\naccuracy of such measures. We propose nonparametric methods for covariate\nadjustment of the AUC. Models with normal errors and non-normal errors are\ndiscussed and analysed separately. Nonparametric regression is used for\nestimating mean and variance functions in both scenarios. In the general noise\ncase we propose a covariate-adjusted Mann-Whitney estimator for AUC estimation\nwhich effectively uses available data to construct working samples at any\ncovariate value of interest and is computationally efficient for\nimplementation. This provides a generalisation of the Mann-Whitney approach for\ncomparing two populations by taking covariate effects into account. We derive\nasymptotic properties for the AUC estimators in both settings, including\nasymptotic normality, optimal strong uniform convergence rates and MSE\nconsistency. The usefulness of the proposed methods is demonstrated through\nsimulated and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2009 17:43:22 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Yao", "Fang", ""], ["Craiu", "Radu V.", ""], ["Reiser", "Benjamin", ""]]}, {"id": "0905.0603", "submitter": "Nicole Kraemer", "authors": "Nicole Kraemer, Juliane Schaefer, Anne-Laure Boulesteix", "title": "Regularized estimation of large-scale gene association networks using\n  graphical Gaussian models", "comments": "added additional experiments", "journal-ref": "BMC Bioinformatics, 10:384, 2010", "doi": "10.1186/1471-2105-10-384", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models are popular tools for the estimation of\n(undirected) gene association networks from microarray data. A key issue when\nthe number of variables greatly exceeds the number of samples is the estimation\nof the matrix of partial correlations. Since the (Moore-Penrose) inverse of the\nsample covariance matrix leads to poor estimates in this scenario, standard\nmethods are inappropriate and adequate regularization techniques are needed. In\nthis article, we investigate a general framework for combining regularized\nregression methods with the estimation of Graphical Gaussian models. This\nframework includes various existing methods as well as two new approaches based\non ridge regression and adaptive lasso, respectively. These methods are\nextensively compared both qualitatively and quantitatively within a simulation\nstudy and through an application to six diverse real data sets. In addition,\nall proposed algorithms are implemented in the R package \"parcor\", available\nfrom the R repository CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2009 13:30:23 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2009 21:54:02 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Kraemer", "Nicole", ""], ["Schaefer", "Juliane", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "0905.0668", "submitter": "Artur Lemonte", "authors": "Artur J. Lemonte and Silvia L.P. Ferrari", "title": "Small-sample corrections for score tests in Birnbaum-Saunders\n  regressions", "comments": "To appear in the Communications in Statistics - Theory and Methods,\n  http://www.informaworld.com/smpp/title~content=t713597238", "journal-ref": null, "doi": "10.1080/03610920903402613", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the issue of performing accurate small-sample\ninference in the Birnbaum-Saunders regression model, which can be useful for\nmodeling lifetime or reliability data. We derive a Bartlett-type correction for\nthe score test and numerically compare the corrected test with the usual score\ntest, the likelihood ratio test and its Bartlett-corrected version. Our\nsimulation results suggest that the corrected test we propose is more reliable\nthan the other tests.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2009 18:48:50 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2010 16:35:08 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Lemonte", "Artur J.", ""], ["Ferrari", "Silvia L. P.", ""]]}, {"id": "0905.1422", "submitter": "Philip Stark", "authors": "Philip B. Stark", "title": "Auditing a collection of races simultaneously", "comments": "9pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A collection of races in a single election can be audited as a group by\nauditing a random sample of batches of ballots and combining observed\ndiscrepancies in the races represented in those batches in a particular way:\nthe maximum across-race relative overstatement of pairwise margins (MARROP). A\nrisk-limiting audit for the entire collection of races can be built on this\nballot-based auditing using a variety of probability sampling schemes. The\naudit controls the familywise error rate (the chance that one or more incorrect\noutcomes fails to be corrected by a full hand count) at a cost that can be\nlower than that of controlling the per-comparison error rate with independent\naudits. The approach is particularly efficient if batches are drawn with\nprobability proportional to a bound on the MARROP (PPEB sampling).\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2009 18:11:00 GMT"}], "update_date": "2009-05-12", "authors_parsed": [["Stark", "Philip B.", ""]]}, {"id": "0905.1437", "submitter": "Andrey Novikov", "authors": "Andrey Novikov, Petr Novikov", "title": "Locally most powerful sequential tests of a simple hypothesis vs\n  one-sided alternatives", "comments": "30 pages", "journal-ref": "Journal of Statistical Planning and Inference, v. 140 (2010), no.\n  3, 750--765", "doi": "10.1016/j.jspi.2009.09.004", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1,X_2,...$ be a discrete-time stochastic process with a distribution\n$P_\\theta$, $\\theta\\in\\Theta$, where $\\Theta$ is an open subset of the real\nline. We consider the problem of testing a simple hypothesis $H_0:$\n$\\theta=\\theta_0$ versus a composite alternative $H_1:$ $\\theta>\\theta_0$,\nwhere $\\theta_0\\in\\Theta$ is some fixed point. The main goal of this article is\nto characterize the structure of locally most powerful sequential tests in this\nproblem.\n  For any sequential test $(\\psi,\\phi)$ with a (randomized) stopping rule\n$\\psi$ and a (randomized) decision rule $\\phi$ let $\\alpha(\\psi,\\phi)$ be the\ntype I error probability, $\\dot \\beta_0(\\psi,\\phi)$ the derivative, at\n$\\theta=\\theta_0$, of the power function, and $\\mathscr N(\\psi)$ an average\nsample number of the test $(\\psi,\\phi)$. Then we are concerned with the problem\nof maximizing $\\dot \\beta_0(\\psi,\\phi)$ in the class of all sequential tests\nsuch that $$ \\alpha(\\psi,\\phi)\\leq \\alpha\\quad{and}\\quad \\mathscr N(\\psi)\\leq\n\\mathscr N, $$ where $\\alpha\\in[0,1]$ and $\\mathscr N\\geq 1$ are some\nrestrictions. It is supposed that $\\mathscr N(\\psi)$ is calculated under some\nfixed (not necessarily coinciding with one of $P_\\theta$) distribution of the\nprocess $X_1,X_2...$.\n  The structure of optimal sequential tests is characterized.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2009 12:23:53 GMT"}], "update_date": "2009-12-23", "authors_parsed": [["Novikov", "Andrey", ""], ["Novikov", "Petr", ""]]}, {"id": "0905.2042", "submitter": "Lixing Zhu", "authors": "Jane-Ling Wang, Liugen Xue, Lixing Zhu, and Yun Sam Chong", "title": "Estimation for a Partial-Linear Single-Index Model", "comments": "43 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the estimation for a partial-linear single-index\nmodel. A two-stage estimation procedure is proposed to estimate the link\nfunction for the single index and the parameters in the single index, as well\nas the parameters in the linear component of the model. Asymptotic normality is\nestablished for both parametric components. For the index, a constrained\nestimating equation leads to an asymptotically more efficient estimator than\nexisting estimators in the sense that it is of a smaller limiting variance. The\nestimator of the nonparametric link function achieves optimal convergence\nrates; and the structural error variance is obtained. In addition, the results\nfacilitate the construction of confidence regions and hypothesis testing for\nthe unknown parameters. A simulation study is performed and an application to a\nreal dataset is illustrated. The extension to multiple indices is briefly\nsketched.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2009 09:18:50 GMT"}], "update_date": "2009-05-14", "authors_parsed": [["Wang", "Jane-Ling", ""], ["Xue", "Liugen", ""], ["Zhu", "Lixing", ""], ["Chong", "Yun Sam", ""]]}, {"id": "0905.2327", "submitter": "Nadine Hilgert", "authors": "Nadine Hilgert and Bruno Portier", "title": "Strong uniform consistency and asymptotic normality of a kernel based\n  error density estimator in functional autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the innovation probability density is an important issue in any\nregression analysis. This paper focuses on functional autoregressive models. A\nresidual-based kernel estimator is proposed for the innovation density.\nAsymptotic properties of this estimator depend on the average prediction error\nof the functional autoregressive function. Sufficient conditions are studied to\nprovide strong uniform consistency and asymptotic normality of the kernel\ndensity estimator.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2009 13:31:56 GMT"}, {"version": "v2", "created": "Thu, 6 May 2010 12:34:05 GMT"}], "update_date": "2010-05-07", "authors_parsed": [["Hilgert", "Nadine", ""], ["Portier", "Bruno", ""]]}, {"id": "0905.2592", "submitter": "Emily B. Fox", "authors": "Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, Alan S. Willsky", "title": "A sticky HDP-HMM with application to speaker diarization", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS395 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 1020-1056", "doi": "10.1214/10-AOAS395", "report-no": "IMS-AOAS-AOAS395", "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of speaker diarization, the problem of segmenting an\naudio recording of a meeting into temporal segments corresponding to individual\nspeakers. The problem is rendered particularly difficult by the fact that we\nare not allowed to assume knowledge of the number of people participating in\nthe meeting. To address this problem, we take a Bayesian nonparametric approach\nto speaker diarization that builds on the hierarchical Dirichlet process hidden\nMarkov model (HDP-HMM) of Teh et al. [J. Amer. Statist. Assoc. 101 (2006)\n1566--1581]. Although the basic HDP-HMM tends to over-segment the audio\ndata---creating redundant states and rapidly switching among them---we describe\nan augmented HDP-HMM that provides effective control over the switching rate.\nWe also show that this augmentation makes it possible to treat emission\ndistributions nonparametrically. To scale the resulting architecture to\nrealistic diarization problems, we develop a sampling algorithm that employs a\ntruncated approximation of the Dirichlet process to jointly resample the full\nstate sequence, greatly improving mixing rates. Working with a benchmark NIST\ndata set, we show that our Bayesian nonparametric architecture yields\nstate-of-the-art speaker diarization results.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2009 18:06:13 GMT"}, {"version": "v2", "created": "Tue, 19 May 2009 13:26:48 GMT"}, {"version": "v3", "created": "Wed, 11 Aug 2010 22:50:10 GMT"}, {"version": "v4", "created": "Tue, 16 Aug 2011 09:16:24 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Fox", "Emily B.", ""], ["Sudderth", "Erik B.", ""], ["Jordan", "Michael I.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "0905.2646", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "Monotonic convergence of a general algorithm for computing optimal\n  designs", "comments": "Published in at http://dx.doi.org/10.1214/09-AOS761 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2010, Vol. 38, No. 3, 1593-1606", "doi": "10.1214/09-AOS761", "report-no": "IMS-AOS-AOS761", "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotonic convergence is established for a general class of multiplicative\nalgorithms introduced by Silvey, Titterington and Torsney [Comm. Statist.\nTheory Methods 14 (1978) 1379--1389] for computing optimal designs. A\nconjecture of Titterington [Appl. Stat. 27 (1978) 227--234] is confirmed as a\nconsequence. Optimal designs for logistic regression are used as an\nillustration.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 18:53:53 GMT"}, {"version": "v2", "created": "Tue, 19 May 2009 06:54:14 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2010 18:13:54 GMT"}, {"version": "v4", "created": "Tue, 5 Oct 2010 07:24:29 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "0905.2864", "submitter": "Franck Corset", "authors": "Gilles Celeux (INRIA Futurs), Franck Corset (LJK), A. Lannoy, Benoit\n  Ricard", "title": "Designing a Bayesian Network for Preventive Maintenance from Expert\n  Opinions in a Rapid and Reliable Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a Bayesian Network (BN) is considered to represent a nuclear\nplant mechanical system degradation. It describes a causal representation of\nthe phenomena involved in the degradation process. Inference from such a BN\nneeds to specify a great number of marginal and conditional probabilities. As,\nin the present context, information is based essentially on expert knowledge,\nthis task becomes very complex and rapidly impossible. We present a solution\nwhich consists of considering the BN as a log-linear model on which\nsimplification constraints are assumed. This approach results in a considerable\ndecrease in the number of probabilities to be given by experts. In addition, we\ngive some simple rules to choose the most reliable probabilities. We show that\nmaking use of those rules allows to check the consistency of the derived\nprobabilities. Moreover, we propose a feedback procedure to eliminate\ninconsistent probabilities. Finally, the derived probabilities that we propose\nto solve the equations involved in a realistic Bayesian network are expected to\nbe reliable. The resulting methodology to design a significant and powerful BN\nis applied to a reactor coolant sub-component in EDF Nuclear plants in an\nillustrative purpose.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 11:24:05 GMT"}], "update_date": "2009-05-19", "authors_parsed": [["Celeux", "Gilles", "", "INRIA Futurs"], ["Corset", "Franck", "", "LJK"], ["Lannoy", "A.", ""], ["Ricard", "Benoit", ""]]}, {"id": "0905.2979", "submitter": "Jo Bovy", "authors": "Jo Bovy, David W. Hogg, Sam T. Roweis", "title": "Extreme deconvolution: Inferring complete distribution functions from\n  noisy, heterogeneous and incomplete observations", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS439 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2B, 1657-1677", "doi": "10.1214/10-AOAS439", "report-no": "IMS-AOAS-AOAS439", "categories": "stat.ME astro-ph.GA physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the well-known mixtures of Gaussians approach to density\nestimation and the accompanying Expectation--Maximization technique for finding\nthe maximum likelihood parameters of the mixture to the case where each data\npoint carries an individual $d$-dimensional uncertainty covariance and has\nunique missing data properties. This algorithm reconstructs the\nerror-deconvolved or \"underlying\" distribution function common to all samples,\neven when the individual data points are samples from different distributions,\nobtained by convolving the underlying distribution with the heteroskedastic\nuncertainty distribution of the data point and projecting out the missing data\ndirections. We show how this basic algorithm can be extended with conjugate\npriors on all of the model parameters and a \"split-and-merge\" procedure\ndesigned to avoid local maxima of the likelihood. We demonstrate the full\nmethod by applying it to the problem of inferring the three-dimensional\nvelocity distribution of stars near the Sun from noisy two-dimensional,\ntransverse velocity measurements from the Hipparcos satellite.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2009 16:26:26 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2011 10:31:54 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Bovy", "Jo", ""], ["Hogg", "David W.", ""], ["Roweis", "Sam T.", ""]]}, {"id": "0905.3091", "submitter": "Andrada Ivanescu", "authors": "F. Bunea, M. H. Wegkamp, A. E. Ivanescu", "title": "Adaptive inference for the mean of a stochastic process in functional\n  data", "comments": "33 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": "Department of Statistics, Florida State University, Technical Report\n  number M989", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and analyzes fully data driven methods for inference\nabout the mean function of a stochastic process from a sample of independent\ntrajectories of the process, observed at discrete time points and corrupted by\nadditive random error. The proposed method uses thresholded least squares\nestimators relative to an approximating function basis. The variable threshold\nlevels are estimated from the data and the basis is chosen via cross-validation\nfrom a library of bases. The resulting estimates adapt to the unknown sparsity\nof the mean function relative to the selected approximating basis, both in\nterms of the mean squared error and supremum norm. These results are based on\nnovel oracle inequalities. In addition, uniform confidence bands for the mean\nfunction of the process are constructed. The bands also adapt to the unknown\nregularity of the mean function, are easy to compute, and do not require\nexplicit estimation of the covariance operator of the process. The simulation\nstudy that complements the theoretical results shows that the new method\nperforms very well in practice, and is robust against large variations\nintroduced by the random error terms.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2009 12:41:45 GMT"}], "update_date": "2009-05-20", "authors_parsed": [["Bunea", "F.", ""], ["Wegkamp", "M. H.", ""], ["Ivanescu", "A. E.", ""]]}, {"id": "0905.3217", "submitter": "Patrick J. Wolfe", "authors": "Keigo Hirakawa and Patrick J. Wolfe", "title": "Skellam shrinkage: Wavelet-based intensity estimation for inhomogeneous\n  Poisson data", "comments": "27 pages, 8 figures, slight formatting changes; submitted for\n  publication", "journal-ref": "IEEE Transactions on Information Theory, vol. 58, pp. 1080-1093,\n  2012", "doi": "10.1109/TIT.2011.2165933", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of integrating detectors in imaging and other applications\nimplies that a variety of real-world data are well modeled as Poisson random\nvariables whose means are in turn proportional to an underlying vector-valued\nsignal of interest. In this article, we first show how the so-called Skellam\ndistribution arises from the fact that Haar wavelet and filterbank transform\ncoefficients corresponding to measurements of this type are distributed as sums\nand differences of Poisson counts. We then provide two main theorems on Skellam\nshrinkage, one showing the near-optimality of shrinkage in the Bayesian setting\nand the other providing for unbiased risk estimation in a frequentist context.\nThese results serve to yield new estimators in the Haar transform domain,\nincluding an unbiased risk estimate for shrinkage of Haar-Fisz\nvariance-stabilized data, along with accompanying low-complexity algorithms for\ninference. We conclude with a simulation study demonstrating the efficacy of\nour Skellam shrinkage estimators both for the standard univariate wavelet test\nfunctions as well as a variety of test images taken from the image processing\nliterature, confirming that they offer substantial performance improvements\nover existing alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 05:05:14 GMT"}, {"version": "v2", "created": "Fri, 29 May 2009 17:33:23 GMT"}], "update_date": "2012-10-15", "authors_parsed": [["Hirakawa", "Keigo", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0905.3463", "submitter": "Carrie A. Hosman", "authors": "Carrie A. Hosman, Ben B. Hansen, Paul W. Holland", "title": "The sensitivity of linear regression coefficients' confidence limits to\n  the omission of a confounder", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS315 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 849-870", "doi": "10.1214/09-AOAS315", "report-no": "IMS-AOAS-AOAS315", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omitted variable bias can affect treatment effect estimates obtained from\nobservational data due to the lack of random assignment to treatment groups.\nSensitivity analyses adjust these estimates to quantify the impact of potential\nomitted variables. This paper presents methods of sensitivity analysis to\nadjust interval estimates of treatment effect---both the point estimate and\nstandard error---obtained using multiple linear regression. Central to our\napproach is what we term benchmarking, the use of data to establish reference\npoints for speculation about omitted confounders. The method adapts to\ntreatment effects that may differ by subgroup, to scenarios involving omission\nof multiple variables, and to combinations of covariance adjustment with\npropensity score stratification. We illustrate it using data from an\ninfluential study of health outcomes of patients admitted to critical care.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2009 09:36:56 GMT"}, {"version": "v2", "created": "Tue, 9 Nov 2010 09:36:17 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Hosman", "Carrie A.", ""], ["Hansen", "Ben B.", ""], ["Holland", "Paul W.", ""]]}]