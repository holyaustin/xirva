[{"id": "1302.0115", "submitter": "Igor Pr\\\"{u}nster", "authors": "Igor Pr\\\"unster, Matteo Ruggiero", "title": "A Bayesian nonparametric approach to modeling market share dynamics", "comments": "Published in at http://dx.doi.org/10.3150/11-BEJ392 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 1, 64-92", "doi": "10.3150/11-BEJ392", "report-no": "IMS-BEJ-BEJ392", "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible stochastic framework for modeling the market share\ndynamics over time in a multiple markets setting, where firms interact within\nand between markets. Firms undergo stochastic idiosyncratic shocks, which\ncontract their shares, and compete to consolidate their position by acquiring\nnew ones in both the market where they operate and in new markets. The model\nparameters can meaningfully account for phenomena such as barriers to entry and\nexit, fixed and sunk costs, costs of expanding to new sectors with different\ntechnologies and competitive advantage among firms. The construction is\nobtained in a Bayesian framework by means of a collection of nonparametric\nhierarchical mixtures, which induce the dependence between markets and provide\na generalization of the Blackwell-MacQueen P\\'{o}lya urn scheme, which in turn\nis used to generate a partially exchangeable dynamical particle system. A\nMarkov Chain Monte Carlo algorithm is provided for simulating trajectories of\nthe system, by means of which we perform a simulation study for transitions to\ndifferent economic regimes. Moreover, it is shown that the infinite-dimensional\nproperties of the system, when appropriately transformed and rescaled, are\nthose of a collection of interacting Fleming-Viot diffusions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 09:34:20 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Pr\u00fcnster", "Igor", ""], ["Ruggiero", "Matteo", ""]]}, {"id": "1302.0355", "submitter": "Weiming Li", "authors": "Weiming Li, Jiaqi Chen, Yingli Qin, Jianfeng Yao, Zhidong Bai", "title": "Estimation of the population spectral distribution from a large\n  dimensional sample covariance matrix", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method to estimate the spectral distribution of a\npopulation covariance matrix from high-dimensional data. The method is founded\non a meaningful generalization of the seminal Marcenko-Pastur equation,\noriginally defined in the complex plan, to the real line. Beyond its easy\nimplementation and the established asymptotic consistency, the new estimator\noutperforms two existing estimators from the literature in almost all the\nsituations tested in a simulation experiment. An application to the analysis of\nthe correlation matrix of S&P stocks data is also given.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 08:51:50 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Li", "Weiming", ""], ["Chen", "Jiaqi", ""], ["Qin", "Yingli", ""], ["Yao", "Jianfeng", ""], ["Bai", "Zhidong", ""]]}, {"id": "1302.0356", "submitter": "Weiming Li", "authors": "Weiming Li, Jianfeng Yao", "title": "A local moment estimator of the spectrum of a large dimensional\n  covariance matrix", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating the population spectral\ndistribution from a sample covariance matrix in large dimensional situations.\nWe generalize the contour-integral based method in Mestre (2008) and present a\nlocal moment estimation procedure. Compared with the original one, the new\nprocedure can be applied successfully to models where the asymptotic clusters\nof sample eigenvalues generated by different population eigenvalues are not all\nseparate. The proposed estimates are proved to be consistent. Numerical results\nillustrate the implementation of the estimation procedure and demonstrate its\nefficiency in various cases.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 08:56:55 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Li", "Weiming", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1302.0583", "submitter": "Huei-Wen Teng", "authors": "Cheng-Der Fuh and Huei-Wen Teng and Ren-Her Wang", "title": "Efficient Importance Sampling for Rare Event Simulation with\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling has been known as a powerful tool to reduce the variance\nof Monte Carlo estimator for rare event simulation. Based on the criterion of\nminimizing the variance of Monte Carlo estimator within a parametric family, we\npropose a general account for finding the optimal tilting measure. To this end,\nwhen the moment generating function of the underlying distribution exists, we\nobtain a simple and explicit expression of the optimal alternative\ndistribution. The proposed algorithm is quite general to cover many interesting\nexamples, such as normal distribution, noncentral $\\chi^2$ distribution, and\ncompound Poisson processes. To illustrate the broad applicability of our\nmethod, we study value-at-risk (VaR) computation in financial risk management\nand bootstrap confidence regions in statistical inferences.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 05:03:08 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Fuh", "Cheng-Der", ""], ["Teng", "Huei-Wen", ""], ["Wang", "Ren-Her", ""]]}, {"id": "1302.0681", "submitter": "Simo S\\\"arkk\\\"a", "authors": "Simo S\\\"arkk\\\"a Jouni Hartikainen", "title": "Variational Bayesian Adaptation of Noise Covariances in Non-Linear\n  Kalman Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is considered with joint estimation of state and time-varying\nnoise covariance matrices in non-linear stochastic state space models. We\npresent a variational Bayes and Gaussian filtering based algorithm for\nefficient computation of the approximate filtering posterior distributions. The\nGaussian filtering based formulation of the non-linear state space model\ncomputation allows usage of efficient Gaussian integration methods such as\nunscented transform, cubature integration and Gauss-Hermite integration along\nwith the classical Taylor series approximations. The performance of the\nalgorithm is illustrated in a simulated application.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 13:24:37 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Hartikainen", "Simo S\u00e4rkk\u00e4 Jouni", ""]]}, {"id": "1302.0730", "submitter": "Koby Todros", "authors": "Koby Todros and Alfred O. Hero", "title": "Measure Transformed Independent Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive a new framework for independent component analysis\n(ICA), called measure-transformed ICA (MTICA), that is based on applying a\nstructured transform to the probability distribution of the observation vector,\ni.e., transformation of the probability measure defined on its observation\nspace. By judicious choice of the transform we show that the separation matrix\ncan be uniquely determined via diagonalization of several measure-transformed\ncovariance matrices. In MTICA the separation matrix is estimated via\napproximate joint diagonalization of several empirical measure-transformed\ncovariance matrices. Unlike kernel based ICA techniques where the\ntransformation is applied repetitively to some affine mappings of the\nobservation vector, in MTICA the transformation is applied only once to the\nprobability distribution of the observations. This results in performance\nadvantages and reduced implementation complexity. Simulations demonstrate the\nadvantages of the proposed approach as compared to other existing\nstate-of-the-art methods for ICA.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 15:53:01 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 15:50:33 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Todros", "Koby", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1302.0817", "submitter": "Edgard Nyssen", "authors": "Edgard Nyssen and Wolfgang Jacquet", "title": "A statistical testing framework for evaluating the quality of\n  measurement processes", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper in which we address the evaluation of measurement process\nquality, we mainly focus on the evaluation procedure, as far as it is based on\nthe numerical measurement outcomes. We challenge the approach where the \"exact\"\nvalue of the observed quantity is compared to the error interval obtained from\nthe measurements under test and we propose a procedure where reference\nmeasurements are used as \"gold standard\". To this purpose, we designed a\nspecific t-test procedure for this purpose, explained here. We also describe\nand discuss a numerical simulation experiment demonstrating the behaviour of\nour procedure.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 19:46:56 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Nyssen", "Edgard", ""], ["Jacquet", "Wolfgang", ""]]}, {"id": "1302.0890", "submitter": "Zachary Kurtz", "authors": "Zachary T. Kurtz", "title": "Local Log-linear Models for Capture-Recapture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-linear models are often used to estimate the size of a closed population\nusing capture-recapture data. When capture probabilities are related to\nauxiliary covariates, one may select a separate model based on each of several\npost-strata. We extend post-stratification to its logical extreme by selecting\na local log-linear model for each observed unit, while smoothing to achieve\nstability. Our local models serve a dual purpose: In addition to estimating the\nsize of the population, we estimate the rate of missingness as a function of\ncovariates. A simulation demonstrates the superiority of our method when the\ngenerating model varies over the covariate space. Data from the Breeding Bird\nSurvey is used to illustrate the method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 22:32:33 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 19:55:22 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2013 17:07:29 GMT"}, {"version": "v4", "created": "Tue, 29 Apr 2014 22:30:42 GMT"}, {"version": "v5", "created": "Tue, 10 Jun 2014 01:09:44 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Kurtz", "Zachary T.", ""]]}, {"id": "1302.0907", "submitter": "Simon DeDeo", "authors": "Simon DeDeo, Robert X. D. Hawkins, Sara Klingenstein, Tim Hitchcock", "title": "Bootstrap Methods for the Empirical Study of Decision-Making and\n  Information Flows in Social Systems", "comments": "32 pages, 8 figures, 5 tables. Matched published version. Code for\n  NSB, naive, and bootstrap estimation of entropy, mutual information, and\n  other quantities available at http://thoth-python.org", "journal-ref": "Entropy 2013, 15(6), 2246-2276", "doi": "10.3390/e15062246", "report-no": null, "categories": "cs.IT cs.SI math.IT physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the statistical bootstrap for the estimation of\ninformation-theoretic quantities from data, with particular reference to its\nuse in the study of large-scale social phenomena. Our methods allow one to\npreserve, approximately, the underlying axiomatic relationships of information\ntheory---in particular, consistency under arbitrary coarse-graining---that\nmotivate use of these quantities in the first place, while providing\nreliability comparable to the state of the art for Bayesian estimators. We show\nhow information-theoretic quantities allow for rigorous empirical study of the\ndecision-making capacities of rational agents and the time-asymmetric flows of\ninformation in distributed systems. We provide illustrative examples by\nreference to ongoing collaborative work on the semantic structure of the\nBritish Criminal Court system and the conflict dynamics of the contemporary\nAfghanistan insurgency.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 00:08:38 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 17:07:58 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["DeDeo", "Simon", ""], ["Hawkins", "Robert X. D.", ""], ["Klingenstein", "Sara", ""], ["Hitchcock", "Tim", ""]]}, {"id": "1302.0977", "submitter": "Antonio Parisi", "authors": "Brunero Liseo and Antonio Parisi", "title": "Bayesian inference for the multivariate skew-normal model: a Population\n  Monte Carlo approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequentist and likelihood methods of inference based on the multivariate\nskew-normal model encounter several technical difficulties with this model. In\nspite of the popularity of this class of densities, there are no broadly\nsatisfactory solutions for estimation and testing problems. A general\npopulation Monte Carlo algorithm is proposed which: 1) exploits the latent\nstructure stochastic representation of skew-normal random variables to provide\na full Bayesian analysis of the model and 2) accounts for the presence of\nconstraints in the parameter space. The proposed approach can be defined as\nweakly informative, since the prior distribution approximates the actual\nreference prior for the shape parameter vector. Results are compared with the\nexisting classical solutions and the practical implementation of the algorithm\nis illustrated via a simulation study and a real data example. A generalization\nto the matrix variate regression model with skew-normal error is also\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 09:56:47 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Liseo", "Brunero", ""], ["Parisi", "Antonio", ""]]}, {"id": "1302.1047", "submitter": "Xinghua Zheng", "authors": "Jean Jacod, Yingying Li, and Xinghua Zheng", "title": "Statistical Properties of Microstructure Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of moments and joint moments of microstructure noise.\nEstimators of arbitrary order of (joint) moments are provided, for which we\nestablish consistency as well as central limit theorems. In particular, we\nprovide estimators of auto-covariances and auto-correlations of the noise.\nSimulation studies demonstrate excellent performance of our estimators even in\nthe presence of jumps and irregular observation times. Empirical studies reveal\n(moderate) positive auto-correlation of the noise for the stocks tested.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 14:34:34 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Jacod", "Jean", ""], ["Li", "Yingying", ""], ["Zheng", "Xinghua", ""]]}, {"id": "1302.1129", "submitter": "Saskia Becker", "authors": "Saskia Becker and Peter Math\\'e", "title": "A new perspective on the Propagation-Separation approach: Taking\n  advantage of the propagation condition", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Propagation-Separation approach is an iterative procedure for pointwise\nestimation of local constant and local polynomial functions. The estimator is\ndefined as a weighted mean of the observations with data-driven weights. Within\nhomogeneous regions it ensures a similar behavior as non-adaptive smoothing\n(propagation), while avoiding smoothing among distinct regions (separation). In\norder to enable a proof of stability of estimates, the authors of the original\nstudy introduced an additional memory step aggregating the estimators of the\nsuccessive iteration steps. Here, we study theoretical properties of the\nsimplified algorithm, where the memory step is omitted. In particular, we\nintroduce a new strategy for the choice of the adaptation parameter yielding\npropagation and stability for local constant functions with sharp\ndiscontinuities.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 17:37:54 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Becker", "Saskia", ""], ["Math\u00e9", "Peter", ""]]}, {"id": "1302.1154", "submitter": "Zuofeng Shang", "authors": "Zuofeng Shang and Ping Li", "title": "Bayesian Ultrahigh-Dimensional Screening Via MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the theoretical and numerical property of a fully Bayesian model\nselection method in sparse ultrahigh-dimensional settings, i.e., $p\\gg n$,\nwhere $p$ is the number of covariates and $n$ is the sample size. Our method\nconsists of (1) a hierarchical Bayesian model with a novel prior placed over\nthe model space which includes a hyperparameter $t_n$ controlling the model\nsize, and (2) an efficient MCMC algorithm for automatic and stochastic search\nof the models. Our theory shows that, when specifying $t_n$ correctly, the\nproposed method yields selection consistency, i.e., the posterior probability\nof the true model asymptotically approaches one; when $t_n$ is misspecified,\nthe selected model is still asymptotically nested in the true model. The theory\nalso reveals insensitivity of the selection result with respect to the choice\nof $t_n$. In implementations, a reasonable prior is further assumed on $t_n$\nwhich allows us to draw its samples stochastically. Our approach conducts\nselection, estimation and even inference in a unified framework. No additional\nprescreening or dimension reduction step is needed. Two novel $g$-priors are\nproposed to make our approach more flexible. A simulation study is given to\ndisplay the numerical advantage of our method.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 19:11:46 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 22:23:44 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2013 15:05:39 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2013 14:47:51 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Shang", "Zuofeng", ""], ["Li", "Ping", ""]]}, {"id": "1302.1500", "submitter": "Shawn Pethel Ph.D", "authors": "Shawn D. Pethel and Daniel W. Hahs", "title": "Exact test for Markov order", "comments": "7 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an exact test of the null hypothesis that a Markov chain is nth\norder versus the alternate hypothesis that it is $(n+1)$-th order. The\nprocedure does not rely on asymptotic properties, but instead builds up the\ntest statistic distribution via surrogate data and is valid for any sample\nsize. Surrogate data are generated using a novel algorithm that guarantees, per\nshot, a uniform sampling from the set of sequences that exactly match the nth\norder properties of the observed data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 20:28:42 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Pethel", "Shawn D.", ""], ["Hahs", "Daniel W.", ""]]}, {"id": "1302.1566", "submitter": "James M. Robins", "authors": "James M. Robins, Larry A. Wasserman", "title": "Estimation of Effects of Sequential Treatments by Reparameterizing\n  Directed Acyclic Graphs", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-409-420", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard way to parameterize the distributions represented by a directed\nacyclic graph is to insert a parametric family for the conditional distribution\nof each random variable given its parents. We show that when one's goal is to\ntest for or estimate an effect of a sequentially applied treatment, this\nnatural parameterization has serious deficiencies. By reparameterizing the\ngraph using structural nested models, these deficiencies can be avoided.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:58:51 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Robins", "James M.", ""], ["Wasserman", "Larry A.", ""]]}, {"id": "1302.1571", "submitter": "Bo Thiesson", "authors": "Bo Thiesson", "title": "Score and Information for Recursive Exponential Models with Incomplete\n  Data", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-453-463", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive graphical models usually underlie the statistical modelling\nconcerning probabilistic expert systems based on Bayesian networks. This paper\ndefines a version of these models, denoted as recursive exponential models,\nwhich have evolved by the desire to impose sophisticated domain knowledge onto\nlocal fragments of a model. Besides the structural knowledge, as specified by a\ngiven model, the statistical modelling may also include expert opinion about\nthe values of parameters in the model. It is shown how to translate imprecise\nexpert knowledge into approximately conjugate prior distributions. Based on\npossibly incomplete data, the score and the observed information are derived\nfor these models. This accounts for both the traditional score and observed\ninformation, derived as derivatives of the log-likelihood, and the posterior\nscore and observed information, derived as derivatives of the log-posterior\ndistribution. Throughout the paper the specialization into recursive graphical\nmodels is accounted for by a simple example.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:59:19 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Thiesson", "Bo", ""]]}, {"id": "1302.1805", "submitter": "Jiaying Gu", "authors": "Jiaying Gu and Roger Koenker and Stanislav Volgushev", "title": "Testing for Homogeneity in Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models of unobserved heterogeneity are typically formalized as\nmixtures of simple parametric models and interest naturally focuses on testing\nfor homogeneity versus general mixture alternatives. Many tests of this type\ncan be interpreted as $C(\\alpha)$ tests, as in Neyman (1959), and shown to be\nlocally, asymptotically optimal. These $C(\\alpha)$ tests will be contrasted\nwith a new approach to likelihood ratio testing for general mixture models. The\nlatter tests are based on estimation of general nonparametric mixing\ndistribution with the Kiefer and Wolfowitz (1956) maximum likelihood estimator.\nRecent developments in convex optimization have dramatically improved upon\nearlier EM methods for computation of these estimators, and recent results on\nthe large sample behavior of likelihood ratios involving such estimators yield\na tractable form of asymptotic inference. Improvement in computation efficiency\nalso facilitates the use of a bootstrap methods to determine critical values\nthat are shown to work better than the asymptotic critical values in finite\nsamples. Consistency of the bootstrap procedure is also formally established.\nWe compare performance of the two approaches identifying circumstances in which\neach is preferred.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2013 17:17:49 GMT"}, {"version": "v2", "created": "Thu, 16 May 2013 22:46:06 GMT"}, {"version": "v3", "created": "Mon, 27 May 2013 22:53:25 GMT"}, {"version": "v4", "created": "Wed, 3 Jul 2013 12:34:31 GMT"}, {"version": "v5", "created": "Sun, 16 Aug 2015 18:15:42 GMT"}, {"version": "v6", "created": "Mon, 21 Mar 2016 14:07:31 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Gu", "Jiaying", ""], ["Koenker", "Roger", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "1302.1969", "submitter": "Chris J. Oates", "authors": "Chris J. Oates, Jim Korkola, Joe W. Gray, Sach Mukherjee", "title": "Joint estimation of multiple related biological networks", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS761 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1892-1919", "doi": "10.1214/14-AOAS761", "report-no": "IMS-AOAS-AOAS761", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are widely used to make inferences concerning interplay in\nmultivariate systems. In many applications, data are collected from multiple\nrelated but nonidentical units whose underlying networks may differ but are\nlikely to share features. Here we present a hierarchical Bayesian formulation\nfor joint estimation of multiple networks in this nonidentically distributed\nsetting. The approach is general: given a suitable class of graphical models,\nit uses an exchangeability assumption on networks to provide a corresponding\njoint formulation. Motivated by emerging experimental designs in molecular\nbiology, we focus on time-course data with interventions, using dynamic\nBayesian networks as the graphical models. We introduce a computationally\nefficient, deterministic algorithm for exact joint inference in this setting.\nWe provide an upper bound on the gains that joint estimation offers relative to\nseparate estimation for each network and empirical results that support and\nextend the theory, including an extensive simulation study and an application\nto proteomic data from human cancer cell lines. Finally, we describe\napproximations that are still more computationally efficient than the exact\nalgorithm and that also demonstrate good empirical performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 08:51:14 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 08:21:35 GMT"}, {"version": "v3", "created": "Tue, 1 Jul 2014 13:43:35 GMT"}, {"version": "v4", "created": "Wed, 3 Dec 2014 12:01:02 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Oates", "Chris J.", ""], ["Korkola", "Jim", ""], ["Gray", "Joe W.", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1302.2126", "submitter": "Leif Ellingson", "authors": "Leif Ellingson, Vic Patrangenaru, Frits Ruymgaart", "title": "Nonparametric Estimation of Means on Hilbert Manifolds and Extrinsic\n  Analysis of Mean Shapes of Contours", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of nonparametric inference in high level digital\nimage analysis, we introduce a general extrinsic approach for data analysis on\nHilbert manifolds with a focus on means of probability distributions on such\nsample spaces. To perform inference on these means, we appeal to the concept of\nneighborhood hypotheses from functional data analysis and derive a one-sample\ntest. We then consider analysis of shapes of contours lying in the plane. By\nembedding the corresponding sample space of such shapes, which is a Hilbert\nmanifold, into a space of Hilbert-Schmidt operators, we can define extrinsic\nmean shapes of planar contours and their sample analogues. We apply the general\nmethods to this problem while considering the computational restrictions faced\nwhen utilizing digital imaging data. Comparisons of computational cost are\nprovided to another method for analyzing shapes of contours.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 19:40:08 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Ellingson", "Leif", ""], ["Patrangenaru", "Vic", ""], ["Ruymgaart", "Frits", ""]]}, {"id": "1302.2303", "submitter": "Maxwell Grazier G'Sell", "authors": "Max Grazier G'Sell, Trevor Hastie, Robert Tibshirani", "title": "False Variable Selection Rates in Regression", "comments": "14 figures, 21 pages. Submitted to Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been recent interest in extending the ideas of False Discovery\nRates (FDR) to variable selection in regression settings. Traditionally the FDR\nin these settings has been defined in terms of the coefficients of the full\nregression model. Recent papers have struggled with controlling this quantity\nwhen the predictors are correlated. This paper shows that this full model\ndefinition of FDR suffers from unintuitive and potentially undesirable behavior\nin the presence of correlated predictors. We propose a new false selection\nerror criterion, the False Variable Rate (FVR), that avoids these problems and\nbehaves in a more intuitive manner. We discuss the behavior of this criterion\nand how it compares with the traditional FDR, as well as presenting guidelines\nfor determining which is appropriate in a particular setting. Finally, we\npresent a simple estimation procedure for FVR in stepwise variable selection.\nWe analyze the performance of this estimator and draw connections to recent\nestimators in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2013 07:49:32 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["G'Sell", "Max Grazier", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1302.2373", "submitter": "Paul McNicholas", "authors": "Irene Vrbik and Paul D. McNicholas", "title": "Parsimonious Skew Mixture Models for Model-Based Clustering and\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2013.07.008", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work, robust mixture modelling approaches using skewed\ndistributions have been explored to accommodate asymmetric data. We introduce\nparsimony by developing skew-t and skew-normal analogues of the popular GPCM\nfamily that employ an eigenvalue decomposition of a positive-semidefinite\nmatrix. The methods developed in this paper are compared to existing models in\nboth an unsupervised and semi-supervised classification framework. Parameter\nestimation is carried out using the expectation-maximization algorithm and\nmodels are selected using the Bayesian information criterion. The efficacy of\nthese extensions is illustrated on simulated and benchmark clustering data\nsets.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2013 22:19:30 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Vrbik", "Irene", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1302.2429", "submitter": "J\\\"org Paul Rachen", "authors": "J\\\"org P. Rachen ((1) Department of Astrophysics/IMAPP, Radboud\n  University Nijmegen, The Netherlands, (2) Max-Planck-Institute for\n  Astrophysics, Garching b. M\\\"unchen, Germany)", "title": "Bayesian Classification of Astronomical Objects -- and what is behind it", "comments": "8 pages; presented at the 32nd International Workshop on Bayesian\n  Inference and Maximum Entropy Methods in Science and Engineering, Garching b.\n  M\\\"unchen, Germany, July 15.-20., 2012", "journal-ref": "Proc. MaxEnt 32, AIP Conference Proceedings, Vol. 1553, Page 254\n  (2013)", "doi": "10.1063/1.4820007", "report-no": null, "categories": "astro-ph.IM physics.data-an physics.hist-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian method for the identification and classification of\nobjects from sets of astronomical catalogs, given a predefined classification\nscheme. Identification refers here to the association of entries in different\ncatalogs to a single object, and classification refers to the matching of the\nassociated data set to a model selected from a set of parametrized models of\ndifferent complexity. By the virtue of Bayes' theorem, we can combine both\ntasks in an efficient way, which allows a largely automated and still reliable\nway to generate classified astronomical catalogs. A problem to the Bayesian\napproach is hereby the handling of exceptions, for which no likelihoods can be\nspecified. We present and discuss a simple and practical solution to this\nproblem, emphasizing the role of the \"evidence\" term in Bayes' theorem for the\nidentification of exceptions. Comparing the practice and logic of Bayesian\nclassification to Bayesian inference, we finally note some interesting links to\nconcepts of the philosophy of science.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 09:57:14 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Rachen", "J\u00f6rg P.", ""]]}, {"id": "1302.2686", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis and Alfred O. Hero III", "title": "Covariance Estimation in High Dimensions via Kronecker Product\n  Expansions", "comments": "47 pages, accepted to IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, Vol. 61, No. 21, pp. 5347\n  - 5360, November 2013", "doi": "10.1109/TSP.2013.2279355", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for estimating high dimensional covariance\nmatrices. The method, permuted rank-penalized least-squares (PRLS), is based on\na Kronecker product series expansion of the true covariance matrix. Assuming an\ni.i.d. Gaussian random sample, we establish high dimensional rates of\nconvergence to the true covariance as both the number of samples and the number\nof variables go to infinity. For covariance matrices of low separation rank,\nour results establish that PRLS has significantly faster convergence than the\nstandard sample covariance matrix (SCM) estimator. The convergence rate\ncaptures a fundamental tradeoff between estimation error and approximation\nerror, thus providing a scalable covariance estimation framework in terms of\nseparation rank, similar to low rank approximation of covariance matrices. The\nMSE convergence rates generalize the high dimensional rates recently obtained\nfor the ML Flip-flop algorithm for Kronecker product covariance estimation. We\nshow that a class of block Toeplitz covariance matrices is approximatable by\nlow separation rank and give bounds on the minimal separation rank $r$ that\nensures a given level of bias. Simulations are presented to validate the\ntheoretical bounds. As a real world application, we illustrate the utility of\nthe proposed Kronecker covariance estimator for spatio-temporal linear least\nsquares prediction of multivariate wind speed measurements.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 02:40:43 GMT"}, {"version": "v10", "created": "Mon, 23 Dec 2013 22:16:13 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2013 19:40:19 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2013 01:42:11 GMT"}, {"version": "v4", "created": "Fri, 31 May 2013 21:59:13 GMT"}, {"version": "v5", "created": "Tue, 9 Jul 2013 17:32:16 GMT"}, {"version": "v6", "created": "Fri, 1 Nov 2013 18:47:11 GMT"}, {"version": "v7", "created": "Mon, 4 Nov 2013 15:06:28 GMT"}, {"version": "v8", "created": "Tue, 19 Nov 2013 01:47:27 GMT"}, {"version": "v9", "created": "Thu, 21 Nov 2013 21:35:59 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1302.2799", "submitter": "Will Buttinger", "authors": "Will Buttinger", "title": "Asymptotic Formula for a General Double-Bounded Custom-Sided Likelihood\n  Based Test Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the asymptotic distributions of a general\nlikelihood-based test statistic, derived using results of Wilks and Wald. The\ngeneral form of the test statistic incorporates the test statistics and\nassociated asymptotic formulae previously derived by Cowan, Cranmer, Gross and\nVitells, which are seen to be special cases of the likelihood-based test\nstatistic described here.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 14:16:34 GMT"}], "update_date": "2013-02-13", "authors_parsed": [["Buttinger", "Will", ""]]}, {"id": "1302.3053", "submitter": "Adriano Polpo", "authors": "Felipe L. Bhering, Carlos A. de B. Pereira, Adriano Polpo", "title": "Reliability estimators for the components of series and parallel\n  systems: The Weibull model", "comments": null, "journal-ref": "Applied Mathematics, v. 05, p. 1633-1640, 2014", "doi": "10.4236/am.2014.511157", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hierarchical Bayesian approach to the estimation of\ncomponents' reliability (survival) using a Weibull model for each of them. The\nproposed method can be used to estimation with general survival censored data,\nbecause the estimation of a component's reliability in a series (parallel)\nsystem is equivalent to the estimation of its survival function with right-\n(left-) censored data. Besides the Weibull parametric model for reliability\ndata, independent gamma distributions are considered at the first hierarchical\nlevel for the Weibull parameters and independent uniform distributions over the\nreal line as priors for the parameters of the gammas. In order to evaluate the\nmodel, an example and a simulation study are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 11:21:42 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Bhering", "Felipe L.", ""], ["Pereira", "Carlos A. de B.", ""], ["Polpo", "Adriano", ""]]}, {"id": "1302.3065", "submitter": "Andrea Riebler", "authors": "Stefanie Muff, Andrea Riebler, Havard Rue, Philippe Saner, and\n  Leonhard Held", "title": "Bayesian analysis of measurement error models using INLA", "comments": "37 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To account for measurement error (ME) in explanatory variables, Bayesian\napproaches provide a flexible framework, as expert knowledge about unobserved\ncovariates can be incorporated in the prior distributions. However, given the\nanalytic intractability of the posterior distribution, model inference so far\nhas to be performed via time-consuming and complex Markov chain Monte Carlo\nimplementations. In this paper we extend the Integrated nested Laplace\napproximations (INLA) approach to formulate Gaussian ME models in generalized\nlinear mixed models. We present three applications, and show how parameter\nestimates are obtained for common ME models, such as the classical and Berkson\nerror model including heteroscedastic variances. To illustrate the practical\nfeasibility, R-code is provided.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 12:39:52 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2013 10:21:06 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Muff", "Stefanie", ""], ["Riebler", "Andrea", ""], ["Rue", "Havard", ""], ["Saner", "Philippe", ""], ["Held", "Leonhard", ""]]}, {"id": "1302.3330", "submitter": "Debasish Roy", "authors": "S Sarkar, S R Chowdhury, M Venugopal, R M Vasu and D Roy", "title": "A Kushner-Stratonovich Monte Carlo Filter Applied to Nonlinear Dynamical\n  System Identification", "comments": "51 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.physd.2013.12.007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Monte Carlo filter, based on the idea of averaging over characteristics and\nfashioned after a particle-based time-discretized approximation to the\nKushner-Stratonovich (KS) nonlinear filtering equation, is proposed. A key\naspect of the new filter is the gain-like additive update, designed to\napproximate the innovation integral in the KS equation and implemented through\nan annealing-type iterative procedure, which is aimed at rendering the\ninnovation (observation-prediction mismatch) for a given time-step to a\nzero-mean Brownian increment corresponding to the measurement noise. This may\nbe contrasted with the weight- based multiplicative updates in most particle\nfilters that are known to precipitate the numerical problem of weight collapse\nwithin a finite-ensemble setting. A study to estimate the a-priori error bounds\nin the proposed scheme is undertaken. The numerical evidence, presently\ngathered from the assessed performance of the proposed and a few other\ncompeting filters on a class of nonlinear dynamic system identification and\ntarget tracking problems, is suggestive of the remarkably improved convergence\nand accuracy of the new filter.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 07:57:22 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 07:04:59 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Sarkar", "S", ""], ["Chowdhury", "S R", ""], ["Venugopal", "M", ""], ["Vasu", "R M", ""], ["Roy", "D", ""]]}, {"id": "1302.3414", "submitter": "Johan Elkink", "authors": "Raffaella Calabrese and Johan A. Elkink", "title": "Estimators of Binary Spatial Autoregressive Models: A Monte Carlo Study", "comments": "22 pages, 1 figure, submitted for review and available as Working\n  Papers 201215, Geary Institute, University College Dublin", "journal-ref": null, "doi": "10.1111/jors.12116", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to provide a cohesive description and a critical\ncomparison of the main estimators proposed in the literature for spatial binary\nchoice models. The properties of such estimators are investigated using a\ntheoretical and simulation study. To the authors' knowledge, this is the first\npaper that provides a comprehensive Monte Carlo study of the estimators'\nproperties. This simulation study shows that the Gibbs estimator Le Sage (2000)\nperforms best for low spatial autocorrelation, while the Recursive Importance\nSampler Beron & Vijverberg (2004) performs best for high spatial\nautocorrelation. The same results are obtained by increasing the sample size.\nFinally, the linearized General Method of Moments estimator Klier & McMillen\n(2008) is the fastest algorithm that provides accurate estimates for low\nspatial autocorrelation and large sample size.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 14:43:00 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Calabrese", "Raffaella", ""], ["Elkink", "Johan A.", ""]]}, {"id": "1302.3488", "submitter": "Jairo Cugliari", "authors": "Jairo Cugliari (INRIA Saclay - Ile de France)", "title": "Conditional Autoregressive Hilbertian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering the problem of forecasting a continuous-time stochastic\nprocess over an entire time-interval in terms of its recent past, the notion of\nAutoregressive Hilbert space processes (ARH) arises. This model can be seen as\na generalization of the classical autoregressive processes to Hilbert space\nvalued random variables. Its estimation presents several challenges that were\naddressed by many authors in recent years. In this paper, we propose an\nextension based on this model by introducing a conditioning process on the arh.\nIn this way, we are aiming a double objective. First, the intrinsic linearity\nof arh is overwhelm. Second, we allow the introduction of exogenous covariates\non this function- valued time series model. We begin defining a new kind of\nprocesses that we call Conditional arh. We then propose estimators for the\ninfinite dimensional parameters associated to such processes. Using two classes\nof predictors defined within the arh framework, we extend these to our case.\nConsistency results are provided as well as a real data application related to\nelectricity load forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 17:46:14 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Cugliari", "Jairo", "", "INRIA Saclay - Ile de France"]]}, {"id": "1302.3919", "submitter": "Elizabeth Holmes", "authors": "Elizabeth E. Holmes", "title": "Derivation of an EM algorithm for constrained and unconstrained\n  multivariate autoregressive state-space (MARSS) models", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This report presents an Expectation-Maximization (EM) algorithm for\nestimation of the maximum-likelihood parameter values of constrained\nmultivariate autoregressive Gaussian state-space (MARSS) models. The MARSS\nmodel can be written: x(t)=Bx(t-1)+u+w(t), y(t)=Zx(t)+a+v(t), where w(t) and\nv(t) are multivariate normal error-terms with variance-covariance matrices Q\nand R respectively. MARSS models are a class of dynamic linear model and vector\nautoregressive state-space model. Shumway and Stoffer presented an\nunconstrained EM algorithm for this class of models in 1982, and a number of\nresearchers have presented EM algorithms for specific types of constrained\nMARSS models since then. In this report, I present a general EM algorithm for\nconstrained MARSS models, where the constraints are on the elements within the\nparameter matrices (B,u,Q,Z,a,R). The constraints take the form vec(M)=f+Dm,\nwhere M is the parameter matrix, f is a column vector of fixed values, D is a\nmatrix of multipliers, and m is the column vector of estimated values. This\nallows a wide variety of constrained parameter matrix forms. The presentation\nis for a time-varying MARSS model, where time-variation enters through the\nfixed (meaning not estimated) f(t) and D(t) matrices for each parameter. The\nalgorithm allows missing values in y and partially deterministic systems where\n0s appear on the diagonals of Q or R. Open source code for estimating MARSS\nmodels with this algorithm is provided in the MARSS R package on the\nComprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 02:00:03 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Holmes", "Elizabeth E.", ""]]}, {"id": "1302.3979", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Jos\\'e Miguel Hern\\'andez-Lobato, Zoubin Ghahramani", "title": "Gaussian Process Vine Copulas for Multivariate Dependence", "comments": "Accepted to International Conference in Machine Learning (ICML 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas allow to learn marginal distributions separately from the\nmultivariate dependence structure (copula) that links them together into a\ndensity function. Vine factorizations ease the learning of high-dimensional\ncopulas by constructing a hierarchy of conditional bivariate copulas. However,\nto simplify inference, it is common to assume that each of these conditional\nbivariate copulas is independent from its conditioning variables. In this\npaper, we relax this assumption by discovering the latent functions that\nspecify the shape of a conditional copula given its conditioning variables We\nlearn these functions by following a Bayesian approach based on sparse Gaussian\nprocesses with expectation propagation for scalable, approximate inference.\nExperiments on real-world datasets show that, when modeling all conditional\ndependencies, we obtain better estimates of the underlying copula of the data.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 17:29:33 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Lopez-Paz", "David", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1302.4245", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson and Ryan Prescott Adams", "title": "Gaussian Process Kernels for Pattern Discovery and Extrapolation", "comments": "10 pages, 5 figures, 1 table. Minor edits and titled changed from\n  \"Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation\"\n  to \"Gaussian Process Kernels for Pattern Discovery and Extrapolation\".\n  Appears at the International Conference on Machine Learning (ICML), JMLR W&CP\n  28(3):1067-1075, 2013", "journal-ref": "International Conference on Machine Learning (ICML), JMLR W&CP\n  28(3):1067-1075, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are rich distributions over functions, which provide a\nBayesian nonparametric approach to smoothing and interpolation. We introduce\nsimple closed form kernels that can be used with Gaussian processes to discover\npatterns and enable extrapolation. These kernels are derived by modelling a\nspectral density -- the Fourier transform of a kernel -- with a Gaussian\nmixture. The proposed kernels support a broad class of stationary covariances,\nbut Gaussian process inference remains simple and analytic. We demonstrate the\nproposed kernels by discovering patterns and performing long range\nextrapolation on synthetic examples, as well as atmospheric CO2 trends and\nairline passenger data. We also show that we can reconstruct standard\ncovariances within our framework.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 12:41:50 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 12:52:04 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 16:41:30 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Adams", "Ryan Prescott", ""]]}, {"id": "1302.4390", "submitter": "Wagner Barreto-Souza", "authors": "Wagner Barreto-Souza", "title": "Bivariate gamma-geometric law and its induced L\\'evy process", "comments": "22 pages, 9 figures", "journal-ref": "Journal of Multivariate Analysis, volume 109, August 2012, pages\n  130-145", "doi": "10.1016/j.jmva.2012.03.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we introduce a three-parameter extension of the bivariate\nexponential-geometric (BEG) law (Kozubowski and Panorska, 2005). We refer to\nthis new distribution as bivariate gamma-geometric (BGG) law. A bivariate\nrandom vector $(X,N)$ follows BGG law if $N$ has geometric distribution and $X$\nmay be represented (in law) as a sum of $N$ independent and identically\ndistributed gamma variables, where these variables are independent of $N$.\nStatistical properties such as moment generation and characteristic functions,\nmoments and variance-covariance matrix are provided. The marginal and\nconditional laws are also studied. We show that BBG distribution is infinitely\ndivisible, just as BEG model is. Further, we provide alternative\nrepresentations for the BGG distribution and show that it enjoys a geometric\nstability property. Maximum likelihood estimation and inference are discussed\nand a reparametrization is proposed in order to obtain orthogonality of the\nparameters. We present an application to the real data set where our model\nprovides a better fit than BEG model. Our bivariate distribution induces a\nbivariate L\\'evy process with correlated gamma and negative binomial processes,\nwhich extends the bivariate L\\'evy motion proposed by Kozubowski et al. (2008).\nThe marginals of our L\\'evy motion are mixture of gamma and negative binomial\nprocesses and we named it ${BMixGNB}$ motion. Basic properties such as\nstochastic self-similarity and covariance matrix of the process are presented.\nThe bivariate distribution at fixed time of our ${BMixGNB}$ process is also\nstudied and some results are derived, including a discussion about maximum\nlikelihood estimation and inference.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 19:13:27 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Barreto-Souza", "Wagner", ""]]}, {"id": "1302.4404", "submitter": "Steffen Lauritzen", "authors": "R. G. Cowell and T. Graversen and S. Lauritzen and J. Mortera", "title": "Analysis of Forensic DNA Mixtures with Artefacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA is now routinely used in criminal investigations and court cases,\nalthough DNA samples taken at crime scenes are of varying quality and therefore\npresent challenging problems for their interpretation. We present a statistical\nmodel for the quantitative peak information obtained from an electropherogram\n(EPG) of a forensic DNA sample and illustrate its potential use for the\nanalysis of criminal cases. In contrast to most previously used methods, we\ndirectly model the peak height information and incorporates important artefacts\nassociated with the production of the EPG. Our model has a number of unknown\nparameters, and we show that these can be estimated by the method of maximum\nlikelihood in the presence of multiple unknown contributors, and their\napproximate standard errors calculated; the computations exploit a Bayesian\nnetwork representation of the model. A case example from a UK trial, as\nreported in the literature, is used to illustrate the efficacy and use of the\nmodel, both in finding likelihood ratios to quantify the strength of evidence,\nand in the deconvolution of mixtures for the purpose of finding likely profiles\nof one or more unknown contributors to a DNA sample. Our model is readily\nextended to simultaneous analysis of more than one mixture as illustrated in a\ncase example. We show that combination of evidence from several samples may\ngive an evidential strength close to that of a single source trace and thus\nmodelling of peak height information provides for a potentially very efficient\nmixture analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 20:01:09 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 19:05:09 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Cowell", "R. G.", ""], ["Graversen", "T.", ""], ["Lauritzen", "S.", ""], ["Mortera", "J.", ""]]}, {"id": "1302.4641", "submitter": "Alberto Roverato", "authors": "Alberto Roverato", "title": "Dichotomization invariant log-mean linear parameterization for discrete\n  graphical models of marginal independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the log-mean linear parameterization introduced by Roverato et al.\n(2013) for binary data to discrete variables with arbitrary number of levels,\nand show that also in this case it can be used to parameterize bi-directed\ngraph models. Furthermore, we show that the log-mean linear parameterization\nallows one to simultaneously represent marginal independencies among variables\nand marginal independencies that only appear when certain levels are collapsed\ninto a single one. We illustrate the application of this property by means of\nan example based on genetic association studies involving single-nucleotide\npolymorphisms. More generally, this feature provides a natural way to reduce\nthe parameter count, while preserving the independence structure, by means of\nsubstantive constraints that give additional insight into the association\nstructure of the variables.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 15:22:55 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2013 06:55:50 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Roverato", "Alberto", ""]]}, {"id": "1302.4881", "submitter": "Michael Friendly", "authors": "Michael Friendly, Georges Monette, John Fox", "title": "Elliptical Insights: Understanding Statistical Methods through\n  Elliptical Geometry", "comments": "Published in at http://dx.doi.org/10.1214/12-STS402 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 1, 1-39", "doi": "10.1214/12-STS402", "report-no": "IMS-STS-STS402", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual insights into a wide variety of statistical methods, for both didactic\nand data analytic purposes, can often be achieved through geometric diagrams\nand geometrically based statistical graphs. This paper extols and illustrates\nthe virtues of the ellipse and her higher-dimensional cousins for both these\npurposes in a variety of contexts, including linear models, multivariate linear\nmodels and mixed-effect models. We emphasize the strong relationships among\nstatistical methods, matrix-algebraic solutions and geometry that can often be\neasily understood in terms of ellipses.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 11:40:04 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Friendly", "Michael", ""], ["Monette", "Georges", ""], ["Fox", "John", ""]]}, {"id": "1302.4907", "submitter": "Danny Pfeffermann", "authors": "Danny Pfeffermann", "title": "New Important Developments in Small Area Estimation", "comments": "Published in at http://dx.doi.org/10.1214/12-STS395 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 1, 40-68", "doi": "10.1214/12-STS395", "report-no": "IMS-STS-STS395", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of small area estimation (SAE) is how to produce reliable\nestimates of characteristics of interest such as means, counts, quantiles,\netc., for areas or domains for which only small samples or no samples are\navailable, and how to assess their precision. The purpose of this paper is to\nreview and discuss some of the new important developments in small area\nestimation methods. Rao [Small Area Estimation (2003)] wrote a very\ncomprehensive book, which covers all the main developments in this topic until\nthat time. A few review papers have been written after 2003, but they are\nlimited in scope. Hence, the focus of this review is on new developments in the\nlast 7-8 years, but to make the review more self-contained, I also mention\nshortly some of the older developments. The review covers both design-based and\nmodel-dependent methods, with the latter methods further classified into\nfrequentist and Bayesian methods. The style of the paper is similar to the\nstyle of my previous review on SAE published in 2002, explaining the new\nproblems investigated and describing the proposed solutions, but without\ndwelling on theoretical details, which can be found in the original articles. I\nhope that this paper will be useful both to researchers who like to learn more\non the research carried out in SAE and to practitioners who might be interested\nin the application of the new methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 14:10:36 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Pfeffermann", "Danny", ""]]}, {"id": "1302.4922", "submitter": "David Duvenaud", "authors": "David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum,\n  Zoubin Ghahramani", "title": "Structure Discovery in Nonparametric Regression through Compositional\n  Kernel Search", "comments": "9 pages, 7 figures, To appear in proceedings of the 2013\n  International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its importance, choosing the structural form of the kernel in\nnonparametric regression remains a black art. We define a space of kernel\nstructures which are built compositionally by adding and multiplying a small\nnumber of base kernels. We present a method for searching over this space of\nstructures which mirrors the scientific discovery process. The learned\nstructures can often decompose functions into interpretable components and\nenable long-range extrapolation on time-series datasets. Our structure search\nmethod outperforms many widely used kernels and kernel combination methods on a\nvariety of prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 14:53:13 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 11:48:12 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2013 16:53:30 GMT"}, {"version": "v4", "created": "Mon, 13 May 2013 13:10:31 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Duvenaud", "David", ""], ["Lloyd", "James Robert", ""], ["Grosse", "Roger", ""], ["Tenenbaum", "Joshua B.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1302.5206", "submitter": "Rong Chen", "authors": "Ming Lin, Rong Chen, Jun S. Liu", "title": "Lookahead Strategies for Sequential Monte Carlo", "comments": "Published in at http://dx.doi.org/10.1214/12-STS401 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 1, 69-94", "doi": "10.1214/12-STS401", "report-no": "IMS-STS-STS401", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the principles of importance sampling and resampling, sequential\nMonte Carlo (SMC) encompasses a large set of powerful techniques dealing with\ncomplex stochastic dynamic systems. Many of these systems possess strong\nmemory, with which future information can help sharpen the inference about the\ncurrent state. By providing theoretical justification of several existing\nalgorithms and introducing several new ones, we study systematically how to\nconstruct efficient SMC algorithms to take advantage of the \"future\"\ninformation without creating a substantially high computational burden. The\nmain idea is to allow for lookahead in the Monte Carlo process so that future\ninformation can be utilized in weighting and generating Monte Carlo samples, or\nresampling from samples of the current state.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 07:42:49 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Lin", "Ming", ""], ["Chen", "Rong", ""], ["Liu", "Jun S.", ""]]}, {"id": "1302.5233", "submitter": "Matthew Reimherr", "authors": "Matthew Reimherr, Dan L. Nicolae", "title": "On Quantifying Dependence: A Framework for Developing Interpretable\n  Measures", "comments": "Published in at http://dx.doi.org/10.1214/12-STS405 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 1, 116-130", "doi": "10.1214/12-STS405", "report-no": "IMS-STS-STS405", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for selecting and developing measures of dependence\nwhen the goal is the quantification of a relationship between two variables,\nnot simply the establishment of its existence. Much of the literature on\ndependence measures is focused, at least implicitly, on detection or revolves\naround the inclusion/exclusion of particular axioms and discussing which\nmeasures satisfy said axioms. In contrast, we start with only a few\nnonrestrictive guidelines focused on existence, range and interpretability,\nwhich provide a very open and flexible framework. For quantification, the most\ncrucial is the notion of interpretability, whose foundation can be found in the\nwork of Goodman and Kruskal [Measures of Association for Cross Classifications\n(1979) Springer], and whose importance can be seen in the popularity of tools\nsuch as the $R^2$ in linear regression. While Goodman and Kruskal focused on\nprobabilistic interpretations for their measures, we demonstrate how more\ngeneral measures of information can be used to achieve the same goal. To that\nend, we present a strategy for building dependence measures that is designed to\nallow practitioners to tailor measures to their needs. We demonstrate how many\nwell-known measures fit in with our framework and conclude the paper by\npresenting two real data examples. Our first example explores U.S. income and\neducation where we demonstrate how this methodology can help guide the\nselection and development of a dependence measure. Our second example examines\nmeasures of dependence for functional data, and illustrates them using data on\ngeomagnetic storms.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 09:59:11 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Reimherr", "Matthew", ""], ["Nicolae", "Dan L.", ""]]}, {"id": "1302.5237", "submitter": "Murad S. Taqqu", "authors": "Murad S. Taqqu", "title": "Beno\\^{i}t Mandelbrot and Fractional Brownian Motion", "comments": "Published in at http://dx.doi.org/10.1214/12-STS389 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 1, 131-134", "doi": "10.1214/12-STS389", "report-no": "IMS-STS-STS389", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although fractional Brownian motion was not invented by Benoit Mandelbrot, it\nwas he who recognized the importance of this random process and gave it the\nname by which it is known today. This is a personal account of the history\nbehind fractional Brownian motion and some subsequent developments.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 10:15:33 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Taqqu", "Murad S.", ""]]}, {"id": "1302.5475", "submitter": "Kei Hirose", "authors": "Kei Hirose and Michio Yamamoto", "title": "Estimation of oblique structure via penalized likelihood factor analysis", "comments": "19 pages. arXiv admin note: substantial text overlap with\n  arXiv:1205.5868", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse estimation via a lasso-type penalized\nlikelihood procedure in a factor analysis model. Typically, the model\nestimation is done under the assumption that the common factors are orthogonal\n(uncorrelated). However, the lasso-type penalization method based on the\northogonal model can often estimate a completely different model from that with\nthe true factor structure when the common factors are correlated. In order to\novercome this problem, we propose to incorporate a factor correlation into the\nmodel, and estimate the factor correlation along with parameters included in\nthe orthogonal model by maximum penalized likelihood procedure. An entire\nsolution path is computed by the EM algorithm with coordinate descent, which\npermits the application to a wide variety of convex and nonconvex penalties.\nThe proposed method can provide sufficiently sparse solutions, and be applied\nto the data where the number of variables is larger than the number of\nobservations. Monte Carlo simulations are conducted to investigate the\neffectiveness of our modeling strategies. The results show that the lasso-type\npenalization based on the orthogonal model cannot often approximate the true\nfactor structure, whereas our approach performs well in various situations. The\nusefulness of the proposed procedure is also illustrated through the analysis\nof real data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 03:54:40 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Hirose", "Kei", ""], ["Yamamoto", "Michio", ""]]}, {"id": "1302.5493", "submitter": "Zihuai He", "authors": "Zihuai He, Min Zhang, Xiaowei Zhan and Qing Lu", "title": "Modeling and Testing for Joint Association Using a Genetic Random Field\n  Model", "comments": "17 pages, 4 tables, the paper has been published on Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial progress has been made in identifying single genetic variants\npredisposing to common complex diseases. Nonetheless, the genetic etiology of\nhuman diseases remains largely unknown. Human complex diseases are likely\ninfluenced by the joint effect of a large number of genetic variants instead of\na single variant. The joint analysis of multiple genetic variants considering\nlinkage disequilibrium (LD) and potential interactions can further enhance the\ndiscovery process, leading to the identification of new disease-susceptibility\ngenetic variants. Motivated by the recent development in spatial statistics, we\npropose a new statistical model based on the random field theory, referred to\nas a genetic random field model (GenRF), for joint association analysis with\nthe consideration of possible gene-gene interactions and LD. Using a\npseudo-likelihood approach, a GenRF test for the joint association of multiple\ngenetic variants is developed, which has the following advantages: 1.\nconsidering complex interactions for improved performance; 2. natural dimension\nreduction; 3. boosting power in the presence of LD; 4. computationally\nefficient. Simulation studies are conducted under various scenarios. Compared\nwith a commonly adopted kernel machine approach, SKAT, GenRF shows overall\ncomparable performance and better performance in the presence of complex\ninteractions. The method is further illustrated by an application to the Dallas\nHeart Study.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 06:39:02 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 03:47:13 GMT"}, {"version": "v3", "created": "Mon, 26 May 2014 18:07:50 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["He", "Zihuai", ""], ["Zhang", "Min", ""], ["Zhan", "Xiaowei", ""], ["Lu", "Qing", ""]]}, {"id": "1302.5589", "submitter": "Florencia Leonardi", "authors": "Andr\\'e J. Bianchi, Suely R. Giolo, J\\'ulia P. Soler and Florencia\n  Leonardi", "title": "Finding the basic neighborhood in variable range Markov random fields:\n  application in SNP association studies", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SNPs (Single Nucleotide Polymorphisms) genotyping platforms are of great\nvalue for gene mapping of complex diseases. Nowadays, the high-density of these\nmolecular markers enables studies of dependence patterns between loci over the\ngenome, allowing a simultaneous inference of dependence structure and disease\nassociation. In this paper we propose a method based on the theory of variable\nrange Markov random fields to estimate the extent of dependence among SNPs\nallowing variable windows along the genome. The advantage of this method is\nthat it allows the simultaneous prediction of dependence and independence\nregions among SNPs, without restricting a priori the range of dependence. We\nintroduce an estimator based on the idea of penalized maximum likelihood to\nfind the conditional dependence neighborhood of each SNP in the sample and we\nprove its consistency. We apply our method to autosomal SNPs genotypic data\nwith unknown phase in the context of case-control association studies. By\nexamining rheumatoid arthritis data from the Genetic Analysis Workshop 16\n(GAW16), we show the utility of the Markov model under variable range\ndependence.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 13:45:49 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Bianchi", "Andr\u00e9 J.", ""], ["Giolo", "Suely R.", ""], ["Soler", "J\u00falia P.", ""], ["Leonardi", "Florencia", ""]]}, {"id": "1302.5644", "submitter": "Stanislav Volgushev", "authors": "Stanislav Volgushev", "title": "Smoothed quantile regression processes for binary response models", "comments": null, "journal-ref": "Econom. Theory 36 (2020) 292-330", "doi": "10.1017/S0266466619000124", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider binary response models with linear quantile\nrestrictions. Considerably generalizing previous research on this topic, our\nanalysis focuses on an infinite collection of quantile estimators. We derive a\nuniform linearisation for the properly standardized empirical quantile process\nand discover some surprising differences with the setting of continuously\nobserved responses. Moreover, we show that considering quantile processes\nprovides an effective way of estimating binary choice probabilities without\nrestrictive assumptions on the form of the link function, heteroskedasticity or\nthe need for high dimensional non-parametric smoothing necessary for approaches\navailable so far. A uniform linear representation and results on asymptotic\nnormality are provided, and the connection to rearrangements is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 17:03:09 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 02:23:05 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Volgushev", "Stanislav", ""]]}, {"id": "1302.5714", "submitter": "David Randell Dr", "authors": "David Randell, Michael Goldstein, Philip Jonathan", "title": "Bayes linear variance structure learning for inspection of large scale\n  physical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling of inspection data for large scale physical systems is critical to\nassessment of their integrity. We present a general method for inference about\nsystem state and associated model variance structure from spatially distributed\ntime series which are typically short, irregular, incomplete and not directly\nobservable. Bayes linear analysis simplifies parameter estimation and avoids\noften-unrealistic distributional assumptions. Second-order exchangeability\njudgements facilitate variance learning for sparse inspection time-series. The\nmodel is applied to inspection data for minimum wall thickness from corroding\npipe-work networks on a full-scale offshore platform, and shown to give\nmaterially different forecasts of remnant life compared to an equivalent model\nneglecting variance learning.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 21:20:22 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Randell", "David", ""], ["Goldstein", "Michael", ""], ["Jonathan", "Philip", ""]]}, {"id": "1302.5721", "submitter": "Sean Simpson", "authors": "Sean L. Simpson, F. DuBois Bowman, Paul J. Laurienti", "title": "Analyzing complex functional brain networks: fusing statistics and\n  network science to understand the brain", "comments": "Statistics Surveys, In Press", "journal-ref": "Statistics Surveys (2013) 7, 1-36", "doi": "10.1214/13-SS103", "report-no": null, "categories": "stat.ME q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex functional brain network analyses have exploded over the last eight\nyears, gaining traction due to their profound clinical implications. The\napplication of network science (an interdisciplinary offshoot of graph theory)\nhas facilitated these analyses and enabled examining the brain as an integrated\nsystem that produces complex behaviors. While the field of statistics has been\nintegral in advancing activation analyses and some connectivity analyses in\nfunctional neuroimaging research, it has yet to play a commensurate role in\ncomplex network analyses. Fusing novel statistical methods with network-based\nfunctional neuroimage analysis will engender powerful analytical tools that\nwill aid in our understanding of normal brain function as well as alterations\ndue to various brain disorders. Here we survey widely used statistical and\nnetwork science tools for analyzing fMRI network data and discuss the\nchallenges faced in filling some of the remaining methodological gaps. When\napplied and interpreted correctly, the fusion of network scientific and\nstatistical methods has a chance to revolutionize the understanding of brain\nfunction.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 21:50:37 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 14:02:12 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2013 20:40:21 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Simpson", "Sean L.", ""], ["Bowman", "F. DuBois", ""], ["Laurienti", "Paul J.", ""]]}, {"id": "1302.5831", "submitter": "Bodhisattva Sen", "authors": "Arnab Sen and Bodhisattva Sen", "title": "On Testing Independence and Goodness-of-fit in Linear Models", "comments": "31 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a linear regression model and propose an omnibus test to\nsimultaneously check the assumption of independence between the error and the\npredictor variables, and the goodness-of-fit of the parametric model. Our\napproach is based on testing for independence between the residual obtained\nfrom the parametric fit and the predictor using the Hilbert--Schmidt\nindependence criterion (Gretton et al. (2008)). The proposed method requires no\nuser-defined regularization, is simple to compute, based merely on pairwise\ndistances between points in the sample, and is consistent against all\nalternatives. We develop distribution theory for the proposed test statistic,\nboth under the null and the alternative hypotheses, and devise a bootstrap\nscheme to approximate its null distribution. We prove the consistency of the\nbootstrap scheme. A simulation study shows that our method has better power\nthan its main competitors. Two real datasets are analyzed to demonstrate the\nscope and usefulness of our method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 18:16:50 GMT"}, {"version": "v2", "created": "Mon, 5 May 2014 01:50:09 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Sen", "Arnab", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1302.5849", "submitter": "Giovanni Montana", "authors": "M. Silver, P. Chen, L. Ruoying, C.Y. Cheng, T.Y. Wong, E. Tai, Y.Y.\n  Teo, G. Montana", "title": "Pathways-driven Sparse Regression Identifies Pathways and Genes\n  Associated with High-density Lipoprotein Cholesterol in Two Asian Cohorts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard approaches to analysing data in genome-wide association studies\n(GWAS) ignore any potential functional relationships between genetic markers.\nIn contrast gene pathways analysis uses prior information on functional\nstructure within the genome to identify pathways associated with a trait of\ninterest. In a second step, important single nucleotide polymorphisms (SNPs) or\ngenes may be identified within associated pathways. Most pathways methods begin\nby testing SNPs one at a time, and so fail to capitalise on the potential\nadvantages inherent in a multi-SNP, joint modelling approach. Here we describe\na dual-level, sparse regression model for the simultaneous identification of\npathways, genes and SNPs associated with a quantitative trait. Our method takes\naccount of various factors specific to the joint modelling of pathways with\ngenome-wide data, including widespread correlation between genetic predictors,\nand the fact that variants may overlap multiple pathways. We use a resampling\nstrategy that exploits finite sample variability to provide robust rankings for\npathways, SNPs and genes. We test our method through simulation, and use it to\nperform pathways-driven SNP selection in a search for pathways, genes and SNPs\nassociated with variation in serum high-density lipoprotein cholesterol (HDLC)\nlevels in two separate GWAS cohorts of Asian adults. By comparing results from\nboth cohorts we identify a number of candidate pathways including those\nassociated with cardiomyopathy, and T cell receptor and PPAR signalling.\nHighlighted genes include those associated with the L-type calcium channel,\nadenylate cyclase, integrin, laminin, MAPK signalling and immune function.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 22:10:01 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Silver", "M.", ""], ["Chen", "P.", ""], ["Ruoying", "L.", ""], ["Cheng", "C. Y.", ""], ["Wong", "T. Y.", ""], ["Tai", "E.", ""], ["Teo", "Y. Y.", ""], ["Montana", "G.", ""]]}, {"id": "1302.5856", "submitter": "Giovanni Montana", "authors": "Brian McWilliams and Giovanni Montana", "title": "A PRESS statistic for two-block partial least squares regression", "comments": null, "journal-ref": "Workshop on Computational Intelligence (UKCI), 2010 UK, pp.1-6,\n  8-10 Sept. 2010", "doi": "10.1109/UKCI.2010.5625583", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modelling of multivariate data where both the covariates and\nresponses are high-dimensional is becoming an increasingly popular task in many\ndata mining applications. Partial Least Squares (PLS) regression often turns\nout to be a useful model in these situations since it performs dimensionality\nreduction by assuming the existence of a small number of latent factors that\nmay explain the linear dependence between input and output. In practice, the\nnumber of latent factors to be retained, which controls the complexity of the\nmodel and its predictive ability, has to be carefully selected. Typically this\nis done by cross validating a performance measure, such as the predictive\nerror. Although cross validation works well in many practical settings, it can\nbe computationally expensive. Various extensions to PLS have also been proposed\nfor regularising the PLS solution and performing simultaneous dimensionality\nreduction and variable selection, but these come at the expense of additional\ncomplexity parameters that also need to be tuned by cross-validation. In this\npaper we derive a computationally efficient alternative to leave-one-out cross\nvalidation (LOOCV), a predicted sum of squares (PRESS) statistic for two-block\nPLS. We show that the PRESS is nearly identical to LOOCV but has the\ncomputational expense of only a single PLS model fit. Examples of the PRESS for\nselecting the number of latent factors and regularisation parameters are\nprovided.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 23:05:40 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["McWilliams", "Brian", ""], ["Montana", "Giovanni", ""]]}, {"id": "1302.6073", "submitter": "Sylvain Sardy", "authors": "Sylvain Sardy", "title": "Blockwise and coordinatewise thresholding to combine tests of different\n  natures in modern ANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive new tests for fixed and random ANOVA based on a thresholded point\nestimate. The pivotal quantity is the threshold that sets all the coefficients\nof the null hypothesis to zero. Thresholding can be employed coordinatewise or\nblockwise, or both, which leads to tests with good power properties under\nalternative hypotheses that are either sparse or dense.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 12:22:38 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 14:07:22 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2013 16:33:17 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Sardy", "Sylvain", ""]]}, {"id": "1302.6088", "submitter": "Ignace Loris", "authors": "Vahid Nassiri and Ignace Loris", "title": "An efficient algorithm for structured sparse quantile regression", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is studied in combination with a penalty which promotes\nstructured (or group) sparsity. A mixed $\\ell_{1,\\infty}$-norm on the parameter\nvector is used to impose structured sparsity on the traditional quantile\nregression problem. An algorithm is derived to calculate the piece-wise linear\nsolution path of the corresponding minimization problem. A Matlab\nimplementation of the proposed algorithm is provided and some applications of\nthe methods are also studied.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 13:34:48 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Nassiri", "Vahid", ""], ["Loris", "Ignace", ""]]}, {"id": "1302.6320", "submitter": "Min Wang", "authors": "Min Wang and Xiaoqian Sun", "title": "Bayes Factor Consistency for One-way Random Effects Model", "comments": "25 pages, 2 figures, 6 tables. To appear in 'Communications in\n  Statistics - Theory and Methods'", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Bayesian hypothesis testing for the balanced\none-way random effects model. A special choice of the prior formulation for the\nratio of variance components is shown to yield an explicit closed-form Bayes\nfactor without integral representation. Furthermore, we study the consistency\nissue of the resulting Bayes factor under three asymptotic scenarios: either\nthe number of units goes to infinity, the number of observations per unit goes\nto infinity, or both go to infinity. Finally, the behavior of the proposed\napproach is illustrated by simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 05:48:05 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Wang", "Min", ""], ["Sun", "Xiaoqian", ""]]}, {"id": "1302.6390", "submitter": "Mohammed El anbari", "authors": "Mohammed El Anbari and Abdallah Mkhadri", "title": "The adaptive Gril estimator with a diverging number of parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variables selection and estimation in linear\nregression model in situations where the number of parameters diverges with the\nsample size. We propose the adaptive Generalized Ridge-Lasso (\\mbox{AdaGril})\nwhich is an extension of the the adaptive Elastic Net. AdaGril incorporates\ninformation redundancy among correlated variables for model selection and\nestimation. It combines the strengths of the quadratic regularization and the\nadaptively weighted Lasso shrinkage. In this paper, we highlight the grouped\nselection property for AdaCnet method (one type of AdaGril) in the equal\ncorrelation case. Under weak conditions, we establish the oracle property of\nAdaGril which ensures the optimal large performance when the dimension is high.\nConsequently, it achieves both goals of handling the problem of collinearity in\nhigh dimension and enjoys the oracle property. Moreover, we show that AdaGril\nestimator achieves a Sparsity Inequality, i. e., a bound in terms of the number\nof non-zero components of the 'true' regression coefficient. This bound is\nobtained under a similar weak Restricted Eigenvalue (RE) condition used for\nLasso. Simulations studies show that some particular cases of AdaGril\noutperform its competitors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 10:50:38 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Anbari", "Mohammed El", ""], ["Mkhadri", "Abdallah", ""]]}, {"id": "1302.6427", "submitter": "Clint Scovel", "authors": "Clint Scovel and Ingo Steinwart", "title": "Hypothesis Testing for Validation and Certification", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-10-02355", "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a hypothesis testing framework for the formulation of the problems\nof 1) the validation of a simulation model and 2) using modeling to certify the\nperformance of a physical system. These results are used to solve the\nextrapolative validation and certification problems, namely problems where the\nregime of interest is different than the regime for which we have experimental\ndata. We use concentration of measure theory to develop the tests and analyze\ntheir errors. This work was stimulated by the work of Lucas, Owhadi, and Ortiz\nwhere a rigorous method of validation and certification is described and\ntested. In a remark we describe the connection between the two approaches.\nMoreover, as mentioned in that work these results have important implications\nin the Quantification of Margins and Uncertainties (QMU) framework. In\nparticular, in a remark we describe how it provides a rigorous interpretation\nof the notion of confidence and new notions of margins and uncertainties which\nallow this interpretation. Since certain concentration parameters used in the\nabove tests may be unkown, we furthermore show, in the last half of the paper,\nhow to derive equally powerful tests which estimate them from sample data, thus\nreplacing the assumption of the values of the concentration parameters with\nweaker assumptions. This paper is an essentially exact copy of one dated April\n10, 2010.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 13:24:30 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Scovel", "Clint", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1302.6651", "submitter": "Junyi Zhang", "authors": "Junyi Zhang, Zhezhen Jin, Yongzhao Shao and Zhiliang Ying", "title": "Statistical Inference on Transformation Models: a Self-induced Smoothing\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a general class of transformation models that contains\nmany important semiparametric regression models as special cases. It develops a\nself-induced smoothing for the maximum rank correlation estimator, resulting in\nsimultaneous point and variance estimation. The self-induced smoothing does not\nrequire bandwidth selection, yet provides the right amount of smoothness so\nthat the estimator is asymptotically normal with mean zero (unbiased) and\nvariance-covariance matrix consistently estimated by the usual sandwich-type\nestimator. An iterative algorithm is given for the variance estimation and\nshown to numerically converge to a consistent limiting variance estimator. The\napproach is applied to a data set involving survival times of primary biliary\ncirrhosis patients. Simulations results are reported, showing that the new\nmethod performs well under a variety of scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 02:48:52 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Zhang", "Junyi", ""], ["Jin", "Zhezhen", ""], ["Shao", "Yongzhao", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1302.6838", "submitter": "William B. Poland", "authors": "William B. Poland, Ross D. Shachter", "title": "Three Approaches to Probability Model Selection", "comments": "Appears in Proceedings of the Tenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1994)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1994-PG-478-483", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares three approaches to the problem of selecting among\nprobability models to fit data (1) use of statistical criteria such as Akaike's\ninformation criterion and Schwarz's \"Bayesian information criterion,\" (2)\nmaximization of the posterior probability of the model, and (3) maximization of\nan effectiveness ratio? trading off accuracy and computational cost. The\nunifying characteristic of the approaches is that all can be viewed as\nmaximizing a penalized likelihood function. The second approach with suitable\nprior distributions has been shown to reduce to the first. This paper shows\nthat the third approach reduces to the second for a particular form of the\neffectiveness ratio, and illustrates all three approaches with the problem of\nselecting the number of components in a mixture of Gaussian distributions.\nUnlike the first two approaches, the third can be used even when the candidate\nmodels are chosen for computational efficiency, without regard to physical\ninterpretation, so that the likelihood and the prior distribution over models\ncannot be interpreted literally. As the most general and computationally\noriented of the approaches, it is especially useful for artificial intelligence\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 14:19:04 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Poland", "William B.", ""], ["Shachter", "Ross D.", ""]]}, {"id": "1302.6964", "submitter": "Murray Pollock", "authors": "Murray Pollock, Adam M. Johansen, Gareth O. Roberts", "title": "On the exact and $\\varepsilon$-strong simulation of (jump) diffusions", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ676 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 794-856", "doi": "10.3150/14-BEJ676", "report-no": "IMS-BEJ-BEJ676", "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a framework for simulating finite dimensional\nrepresentations of (jump) diffusion sample paths over finite intervals, without\ndiscretisation error (exactly), in such a way that the sample path can be\nrestored at any finite collection of time points. Within this framework we\nextend existing exact algorithms and introduce novel adaptive approaches. We\nconsider an application of the methodology developed within this paper which\nallows the simulation of upper and lower bounding processes which almost surely\nconstrain (jump) diffusion sample paths to any specified tolerance. We\ndemonstrate the efficacy of our approach by showing that with finite\ncomputation it is possible to determine whether or not sample paths cross\nvarious irregular barriers, simulate to any specified tolerance the first\nhitting time of the irregular barrier and simulate killed diffusion sample\npaths.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 19:21:12 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 15:09:36 GMT"}, {"version": "v3", "created": "Tue, 9 Sep 2014 12:14:16 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2016 10:53:15 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Pollock", "Murray", ""], ["Johansen", "Adam M.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1302.7021", "submitter": "Deborah G. Mayo", "authors": "Deborah G. Mayo", "title": "On the Birnbaum Argument for the Strong Likelihood Principle", "comments": "Published in at http://dx.doi.org/10.1214/13-STS457 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 227-239", "doi": "10.1214/13-STS457", "report-no": "IMS-STS-STS457", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An essential component of inference based on familiar frequentist notions,\nsuch as $p$-values, significance and confidence levels, is the relevant\nsampling distribution. This feature results in violations of a principle known\nas the strong likelihood principle (SLP), the focus of this paper. In\nparticular, if outcomes $\\mathbf{x}^*$ and $\\mathbf{y}^*$ from experiments\n$E_1$ and $E_2$ (both with unknown parameter $\\theta$) have different\nprobability models $f_1(\\cdot),f_2(\\cdot)$, then even though\n$f_1(\\mathbf{x}^*;\\theta)=cf_2(\\mathbf{y}^*;\\theta)$ for all $\\theta$, outcomes\n$\\mathbf{x}^*$ and $\\mathbf{y}^*$ may have different implications for an\ninference about $\\theta$. Although such violations stem from considering\noutcomes other than the one observed, we argue this does not require us to\nconsider experiments other than the one performed to produce the data. David\nCox [Ann. Math. Statist. 29 (1958) 357-372] proposes the Weak Conditionality\nPrinciple (WCP) to justify restricting the space of relevant repetitions. The\nWCP says that once it is known which $E_i$ produced the measurement, the\nassessment should be in terms of the properties of $E_i$. The surprising upshot\nof Allan Birnbaum's [J. Amer. Statist. Assoc. 57 (1962) 269-306] argument is\nthat the SLP appears to follow from applying the WCP in the case of mixtures,\nand so uncontroversial a principle as sufficiency (SP). But this would preclude\nthe use of sampling distributions. The goal of this article is to provide a new\nclarification and critique of Birnbaum's argument. Although his argument\npurports that [(WCP and SP) entails SLP], we show how data may violate the SLP\nwhile holding both the WCP and SP. Such cases also refute [WCP entails SLP].\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 22:45:29 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2013 12:39:36 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 10:17:30 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Mayo", "Deborah G.", ""]]}, {"id": "1302.7083", "submitter": "Lionel Mathelin", "authors": "Lionel Mathelin", "title": "Quantification of uncertainty from high-dimensional scattered data via\n  polynomial approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a methodology for determining a functional\nrepresentation of a random process from a collection of scattered pointwise\nsamples. The present work specifically focuses onto random quantities lying in\na high dimensional stochastic space in the context of limited amount of\ninformation. The proposed approach involves a procedure for the selection of an\napproximation basis and the evaluation of the associated coefficients. The\nselection of the approximation basis relies on the a priori choice of the\nHigh-Dimensional Model Representation format combined with a modified Least\nAngle Regression technique. The resulting basis then provides the structure for\nthe actual approximation basis, possibly using different functions, more\nparsimonious and nonlinear in its coefficients. To evaluate the coefficients,\nboth an alternate least squares and an alternate weighted total least squares\nmethods are employed. Examples are provided for the approximation of a random\nvariable in a high-dimensional space as well as the estimation of a random\nfield. Stochastic dimensions up to 100 are considered, with an amount of\ninformation as low as about 3 samples per dimension, and robustness of the\napproximation is demonstrated w.r.t. noise in the dataset. The computational\ncost of the solution method is shown to scale only linearly with the\ncardinality of the a priori basis and exhibits a (N_q)^s, 2 <= s <= 3,\ndependence with the number N_q of samples in the dataset. The provided\nnumerical experiments illustrate the ability of the present approach to derive\nan accurate approximation from scarce scattered data even in the presence of\nnoise.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 04:50:56 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2013 11:49:58 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2014 11:11:55 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Mathelin", "Lionel", ""]]}, {"id": "1302.7149", "submitter": "Roman Schefzik", "authors": "Roman Schefzik, Thordis L. Thorarinsdottir, Tilmann Gneiting", "title": "Uncertainty Quantification in Complex Simulation Models Using Ensemble\n  Copula Coupling", "comments": "Published in at http://dx.doi.org/10.1214/13-STS443 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 616-640", "doi": "10.1214/13-STS443", "report-no": "IMS-STS-STS443", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical decisions frequently rely on high-dimensional output from complex\ncomputer simulation models that show intricate cross-variable, spatial and\ntemporal dependence structures, with weather and climate predictions being key\nexamples. There is a strongly increasing recognition of the need for\nuncertainty quantification in such settings, for which we propose and review a\ngeneral multi-stage procedure called ensemble copula coupling (ECC), proceeding\nas follows: 1. Generate a raw ensemble, consisting of multiple runs of the\ncomputer model that differ in the inputs or model parameters in suitable ways.\n2. Apply statistical postprocessing techniques, such as Bayesian model\naveraging or nonhomogeneous regression, to correct for systematic errors in the\nraw ensemble, to obtain calibrated and sharp predictive distributions for each\nunivariate output variable individually. 3. Draw a sample from each\npostprocessed predictive distribution. 4. Rearrange the sampled values in the\nrank order structure of the raw ensemble to obtain the ECC postprocessed\nensemble. The use of ensembles and statistical postprocessing have become\nroutine in weather forecasting over the past decade. We show that seemingly\nunrelated, recent advances can be interpreted, fused and consolidated within\nthe framework of ECC, the common thread being the adoption of the empirical\ncopula of the raw ensemble. Depending on the use of Quantiles, Random draws or\nTransformations at the sampling stage, we distinguish the ECC-Q, ECC-R and\nECC-T variants, respectively. We also describe relations to the Schaake shuffle\nand extant copula-based techniques. In a case study, the ECC approach is\napplied to predictions of temperature, pressure, precipitation and wind over\nGermany, based on the 50-member European Centre for Medium-Range Weather\nForecasts (ECMWF) ensemble.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 10:56:43 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 10:43:06 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Schefzik", "Roman", ""], ["Thorarinsdottir", "Thordis L.", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1302.7175", "submitter": "Hado van Hasselt", "authors": "Hado van Hasselt", "title": "Estimating the Maximum Expected Value: An Analysis of (Nested) Cross\n  Validation and the Maximum Sample Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the accuracy of the two most common estimators for the maximum\nexpected value of a general set of random variables: a generalization of the\nmaximum sample average, and cross validation. No unbiased estimator exists and\nwe show that it is non-trivial to select a good estimator without knowledge\nabout the distributions of the random variables. We investigate and bound the\nbias and variance of the aforementioned estimators and prove consistency. The\nvariance of cross validation can be significantly reduced, but not without\nrisking a large bias. The bias and variance of different variants of cross\nvalidation are shown to be very problem-dependent, and a wrong choice can lead\nto very inaccurate estimates.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 12:48:32 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2013 15:04:48 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["van Hasselt", "Hado", ""]]}]