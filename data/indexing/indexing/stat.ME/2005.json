[{"id": "2005.00221", "submitter": "Zachary Shahn", "authors": "Zach Shahn, Miguel A. Hernan, James M. Robins", "title": "A Formal Causal Interpretation of the Case-Crossover Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The case-crossover design (Maclure, 1991) is widely used in epidemiology and\nother fields to study causal effects of transient treatments on acute outcomes.\nHowever, its validity and causal interpretation have only been justified under\ninformal conditions. Here, we place the design in a formal counterfactual\nframework for the first time. Doing so helps to clarify its assumptions and\ninterpretation. In particular, when the treatment effect is non-null, we\nidentify a previously unnoticed bias arising from common causes of the outcome\nat different person-times. We analytically characterize the direction and size\nof this bias and demonstrate its potential importance with a simulation. We\nalso use our derivation of the limit of the case-crossover estimator to analyze\nits sensitivity to treatment effect heterogeneity, a violation of one of the\ninformal criteria for validity. The upshot of this work for practitioners is\nthat, while the case-crossover design can be useful for testing the causal null\nhypothesis in the presence of baseline confounders, extra caution is warranted\nwhen using the case-crossover design for point estimation of causal effects.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 04:41:57 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 22:03:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shahn", "Zach", ""], ["Hernan", "Miguel A.", ""], ["Robins", "James M.", ""]]}, {"id": "2005.00248", "submitter": "Xingdong Feng", "authors": "Chao Cheng and Xingdong Feng", "title": "Parallel subgroup analysis of high-dimensional data via M-regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It becomes an interesting problem to identify subgroup structures in data\nanalysis as populations are probably heterogeneous in practice. In this paper,\nwe consider M-estimators together with both concave and pairwise fusion\npenalties, which can deal with high-dimensional data containing some outliers.\nThe penalties are applied both on covariates and treatment effects, where the\nestimation is expected to achieve both variable selection and data clustering\nsimultaneously. An algorithm is proposed to process relatively large datasets\nbased on parallel computing. We establish the convergence analysis of the\nproposed algorithm, the oracle property of the penalized M-estimators, and the\nselection consistency of the proposed criterion. Our numerical study\ndemonstrates that the proposed method is promising to efficiently identify\nsubgroups hidden in high-dimensional data.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 07:06:07 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Cheng", "Chao", ""], ["Feng", "Xingdong", ""]]}, {"id": "2005.00386", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Joseph Guinness, Earl Lawrence", "title": "Scaled Vecchia approximation for fast computer-model emulation", "comments": "R code available at https://github.com/katzfuss-group/scaledVecchia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific phenomena are studied using computer experiments consisting\nof multiple runs of a computer model while varying the input settings. Gaussian\nprocesses (GPs) are a popular tool for the analysis of computer experiments,\nenabling interpolation between input settings, but direct GP inference is\ncomputationally infeasible for large datasets. We adapt and extend a powerful\nclass of GP methods from spatial statistics to enable the scalable analysis and\nemulation of large computer experiments. Specifically, we apply Vecchia's\nordered conditional approximation in a transformed input space, with each input\nscaled according to how strongly it relates to the computer-model response. The\nscaling is learned from the data, by estimating parameters in the GP covariance\nfunction using Fisher scoring. Our methods are highly scalable, enabling\nestimation, joint prediction and simulation in near-linear time in the number\nof model runs. In several numerical examples, our approach substantially\noutperformed existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:08:31 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 16:03:08 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 21:07:03 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 15:43:56 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Guinness", "Joseph", ""], ["Lawrence", "Earl", ""]]}, {"id": "2005.00466", "submitter": "Mike Laszkiewicz", "authors": "Mike Laszkiewicz, Asja Fischer, Johannes Lederer", "title": "Thresholded Adaptive Validation: Tuning the Graphical Lasso for Graph\n  Recovery", "comments": "To appear in the proceedings of Artificial Intelligence and\n  Statistics (AISTATS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Machine Learning algorithms are formulated as regularized optimization\nproblems, but their performance hinges on a regularization parameter that needs\nto be calibrated to each application at hand. In this paper, we propose a\ngeneral calibration scheme for regularized optimization problems and apply it\nto the graphical lasso, which is a method for Gaussian graphical modeling. The\nscheme is equipped with theoretical guarantees and motivates a thresholding\npipeline that can improve graph recovery. Moreover, requiring at most one line\nsearch over the regularization path, the calibration scheme is computationally\nmore efficient than competing schemes that are based on resampling. Finally, we\nshow in simulations that our approach can improve on the graph recovery of\nother approaches considerably.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:59:47 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 09:35:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Laszkiewicz", "Mike", ""], ["Fischer", "Asja", ""], ["Lederer", "Johannes", ""]]}, {"id": "2005.00501", "submitter": "Roger Silva Ph.d", "authors": "Marina M. de Queiroz, Rosangela H. Loschi and Roger W. C. Silva", "title": "Multivariate Log-Skewed Distributions with normal kernel and their\n  Applications", "comments": "20 pages", "journal-ref": "Statistics (Berlin), 2016", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two classes of multivariate log skewed distributions with normal\nkernel: the log canonical fundamental skew-normal (log-CFUSN) and the log\nunified skew-normal (log-SUN). We also discuss some properties of the log-CFUSN\nfamily of distributions. These new classes of log-skewed distributions include\nthe log-normal and multivariate log-skew normal families as particular cases.\nWe discuss some issues related to Bayesian inference in the log-CFUSN family of\ndistributions, mainly we focus on how to model the prior uncertainty about the\nskewing parameter. Based on the stochastic representation of the log-CFUSN\nfamily, we propose a data augmentation strategy for sampling from the posterior\ndistributions. This proposed family is used to analyze the US national monthly\nprecipitation data. We conclude that a high dimensional skewing function lead\nto a better model fit.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:16:16 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["de Queiroz", "Marina M.", ""], ["Loschi", "Rosangela H.", ""], ["Silva", "Roger W. C.", ""]]}, {"id": "2005.00564", "submitter": "David Robertson", "authors": "David S. Robertson, Kim May Lee, Boryana C. Lopez-Kolkovska and Sofia\n  S. Villar", "title": "Response-adaptive randomization in clinical trials: from myths to\n  practical considerations", "comments": "Major update in response to reviewers' comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response-adaptive randomization (RAR) is part of a wider class of\ndata-dependent sampling algorithms, for which clinical trials are used as a\nmotivating application. In that context, patient allocation to treatments is\ndetermined by randomization probabilities that are altered based on the accrued\nresponse data in order to achieve experimental goals. RAR has received abundant\ntheoretical attention from the biostatistical literature since the 1930's and\nhas been the subject of numerous debates. In the last decade, it has received\nrenewed consideration from the applied and methodological communities, driven\nby successful practical examples and its widespread use in machine learning.\nPapers on the subject can give different views on its usefulness, and\nreconciling these may be difficult. This work aims to address this gap by\nproviding a unified, broad and up-to-date review of methodological and\npractical issues to consider when debating the use of RAR in clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:34:19 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 13:54:30 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 17:29:11 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Robertson", "David S.", ""], ["Lee", "Kim May", ""], ["Lopez-Kolkovska", "Boryana C.", ""], ["Villar", "Sofia S.", ""]]}, {"id": "2005.00597", "submitter": "Irina Gaynanova", "authors": "Benjamin Risk and Irina Gaynanova", "title": "Simultaneous Non-Gaussian Component Analysis (SING) for Data Integration\n  in Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As advances in technology allow the acquisition of complementary information,\nit is increasingly common for scientific studies to collect multiple datasets.\nLarge-scale neuroimaging studies often include multiple modalities (e.g., task\nfunctional MRI, resting-state fMRI, diffusion MRI, and/or structural MRI), with\nthe aim to understand the relationships between datasets. In this study, we\nseek to understand whether regions of the brain activated in a working memory\ntask relate to resting-state correlations. In neuroimaging, a popular approach\nuses principal component analysis for dimension reduction prior to canonical\ncorrelation analysis with joint independent component analysis, but this may\ndiscard biological features with low variance and/or spuriously associate\nstructure unique to a dataset with joint structure. We introduce Simultaneous\nNon-Gaussian component analysis (SING) in which dimension reduction and feature\nextraction are achieved simultaneously, and shared information is captured via\nsubject scores. We apply our method to a working memory task and resting-state\ncorrelations from the Human Connectome Project. We find joint structure as\nevident from joint scores whose loadings highlight resting-state correlations\ninvolving regions associated with working memory. Moreover, some of the subject\nscores are related to fluid intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:35:00 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 00:54:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Risk", "Benjamin", ""], ["Gaynanova", "Irina", ""]]}, {"id": "2005.00605", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir Storvik, Florian Frommlet", "title": "Rejoinder for the discussion of the paper \"A novel algorithmic approach\n  to Bayesian Logic Regression\"", "comments": "published in Bayesian Analysis, Volume 15, Number 1 (2020)", "journal-ref": "Bayesian Analysis, Volume 15, Number 1 (2020)", "doi": null, "report-no": null, "categories": "stat.ME math.LO stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this rejoinder we summarize the comments, questions and remarks on the\npaper \"A novel algorithmic approach to Bayesian Logic Regression\" from the\ndiscussants. We then respond to those comments, questions and remarks, provide\nseveral extensions of the original model and give a tutorial on our R-package\nEMJMCMC (http://aliaksah.github.io/EMJMCMC2016/)\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:59:56 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""], ["Frommlet", "Florian", ""]]}, {"id": "2005.00662", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee, Bowen Lei, and Bani K. Mallick", "title": "Estimation of COVID-19 spread curves integrating global data and\n  borrowing information", "comments": null, "journal-ref": "PLOS ONE 15 (2020) 1- 17", "doi": "10.1371/journal.pone.0236860", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, novel coronavirus disease 2019 (COVID-19) is a big threat to\nglobal health. The rapid spread of the virus has created pandemic, and\ncountries all over the world are struggling with a surge in COVID-19 infected\ncases. There are no drugs or other therapeutics approved by the US Food and\nDrug Administration to prevent or treat COVID-19: information on the disease is\nvery limited and scattered even if it exists. This motivates the use of data\nintegration, combining data from diverse sources and eliciting useful\ninformation with a unified view of them. In this paper, we propose a Bayesian\nhierarchical model that integrates global data for real-time prediction of\ninfection trajectory for multiple countries. Because the proposed model takes\nadvantage of borrowing information across multiple countries, it outperforms an\nexisting individual country-based model. As fully Bayesian way has been\nadopted, the model provides a powerful predictive tool endowed with uncertainty\nquantification. Additionally, a joint variable selection technique has been\nintegrated into the proposed modeling scheme, which aimed to identify possible\ncountry-level risk factors for severe disease due to COVID-19.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:13:48 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 18:09:04 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 03:12:43 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 20:01:36 GMT"}, {"version": "v5", "created": "Fri, 10 Jul 2020 17:26:20 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lee", "Se Yoon", ""], ["Lei", "Bowen", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2005.00869", "submitter": "Philip Pavlik Jr.", "authors": "Philip I. Pavlik, Jr., Luke G. Eglington, and Leigh M.\n  Harrell-Williams", "title": "Logistic Knowledge Tracing: A Constrained Framework for Learner Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive learning technology solutions often use a learner model to trace\nlearning and make pedagogical decisions. The present research introduces a\nformalized methodology for specifying learner models, Logistic Knowledge\nTracing (LKT), that consolidates many extant learner modeling methods. The\nstrength of LKT is the specification of a symbolic notation system for\nalternative logistic regression models that is powerful enough to specify many\nextant models in the literature and many new models. To demonstrate the\ngenerality of LKT, we fit 12 models, some variants of well-known models and\nsome newly devised, to 6 learning technology datasets. The results indicated\nthat no single learner model was best in all cases, further justifying a broad\napproach that considers multiple learner model features and the learning\ncontext. The models presented here avoid student-level fixed parameters to\nincrease generalizability. We also introduce features to stand in for these\nintercepts. We argue that to be maximally applicable, a learner model needs to\nadapt to student differences, rather than needing to be pre-parameterized with\nthe level of each student's ability.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 15:59:24 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 21:31:48 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Pavlik,", "Philip I.", "Jr."], ["Eglington", "Luke G.", ""], ["Harrell-Williams", "Leigh M.", ""]]}, {"id": "2005.00905", "submitter": "Hong Zhang", "authors": "Hong Zhang, Judong Shen and Zheyang Wu", "title": "An efficient and accurate approximation to the distribution of quadratic\n  forms of Gaussian variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational and applied statistics, it is of great interest to get fast\nand accurate calculation for the distributions of the quadratic forms of\nGaussian random variables. This paper presents a novel approximation strategy\nthat contains two developments. First, we propose a faster numerical procedure\nin computing the moments of the quadratic forms. Second, we establish a general\nmoment-matching framework for distribution approximation, which covers existing\napproximation methods for the distributions of the quadratic forms of Gaussian\nvariables. Under this framework, a novel moment-ratio method (MR) is proposed\nto match the ratio of skewness and kurtosis based on the gamma distribution.\nOur extensive simulations show that 1) MR is almost as accurate as the exact\ndistribution calculation and is much more efficient; 2) comparing with existing\napproximation methods, MR significantly improves the accuracy of approximating\nfar right tail probabilities. The proposed method has wide applications. For\nexample, it is a better choice than existing methods for facilitating\nhypothesis testing in big data analysis, where efficient and accurate\ncalculation of very small $p$-values is desired. An R package Qapprox that\nimplements related methods is available on CRAN.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 19:07:19 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 13:26:42 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhang", "Hong", ""], ["Shen", "Judong", ""], ["Wu", "Zheyang", ""]]}, {"id": "2005.00926", "submitter": "Elyas Sabeti", "authors": "Elyas Sabeti, Peter X.K. Song, Alfred O. Hero", "title": "Pattern-Based Analysis of Time Series: Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Internet of Things (IoT) devices and sensors create continuous streams\nof information, Big Data infrastructures are deemed to handle the influx of\ndata in real-time. One type of such a continuous stream of information is time\nseries data. Due to the richness of information in time series and inadequacy\nof summary statistics to encapsulate structures and patterns in such data,\ndevelopment of new approaches to learn time series is of interest. In this\npaper, we propose a novel method, called pattern tree, to learn patterns in the\ntimes-series using a binary-structured tree. While a pattern tree can be used\nfor many purposes such as lossless compression, prediction and anomaly\ndetection, in this paper we focus on its application in time series estimation\nand forecasting. In comparison to other methods, our proposed pattern tree\nmethod improves the mean squared error of estimation.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 21:37:14 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Sabeti", "Elyas", ""], ["Song", "Peter X. K.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "2005.00952", "submitter": "Michael Dumelle", "authors": "Michael Dumelle and Jay M. Ver Hoef and Claudio Fuentes and Alix\n  Gitelman", "title": "A Linear Mixed Model Formulation for Spatio-Temporal Random Processes\n  with Computational Advances for the Separable and Product-Sum Covariances", "comments": "43 pages (including an Appendix) and 8 figures", "journal-ref": "Spatial Staistics, Volume 43, 2021", "doi": "10.1016/j.spasta.2021.100510", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe spatio-temporal random processes using linear mixed models. We\nshow how many commonly used models can be viewed as special cases of this\ngeneral framework and pay close attention to models with separable or\nproduct-sum covariances. The proposed linear mixed model formulation\nfacilitates the implementation of a novel algorithm using Stegle\neigendecompositions, a recursive application of the Sherman-Morrison-Woodbury\nformula, and Helmert-Wolf blocking to efficiently invert separable and\nproduct-sum covariance matrices, even when every spatial location is not\nobserved at every time point. We show our algorithm provides noticeable\nimprovements over the standard Cholesky decomposition approach. Via\nsimulations, we assess the performance of the separable and product-sum\ncovariances and identify scenarios where separable covariances are noticeably\ninferior to product-sum covariances. We also compare likelihood-based and\nsemivariogram-based estimation and discuss benefits and drawbacks of both. We\nuse the proposed approach to analyze daily maximum temperature data in Oregon,\nUSA, during the 2019 summer. We end by offering guidelines for choosing among\nthese covariances and estimation methods based on properties of observed data.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:11:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Dumelle", "Michael", ""], ["Hoef", "Jay M. Ver", ""], ["Fuentes", "Claudio", ""], ["Gitelman", "Alix", ""]]}, {"id": "2005.01019", "submitter": "Ji\\v{r}\\'i Dvo\\v{r}\\'ak", "authors": "Ji\\v{r}\\'i Dvo\\v{r}\\'ak, Tom\\'a\\v{s} Mrkvi\\v{c}ka, Jorge Mateu and\n  Jonatan Gonz\\'alez", "title": "Nonparametric testing of the dependence structure among\n  points-marks-covariates in spatial point patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate testing of the hypothesis of independence between a covariate\nand the marks in a marked point process. It would be rather straightforward if\nthe (unmarked) point process were independent of the covariate and the marks.\nIn practice, however, such an assumption is questionable, and possible\ndependence between the point process and the covariate or the marks may lead to\nincorrect conclusions. Hence we propose to investigate the complete dependence\nstructure in the triangle points-marks-covariates together. We take advantage\nof the recent development of the nonparametric random shift methods, namely the\nnew variance correction approach, and propose tests of the null hypothesis of\nindependence between the marks and the covariate and between the points and the\ncovariate. We present a detailed simulation study showing the performance of\nthe methods, and provide two theorems establishing the appropriate form of the\ncorrection factors for the variance correction. Finally, we illustrate the use\nof the proposed methods in two real applications.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 08:32:50 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 12:39:43 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Dvo\u0159\u00e1k", "Ji\u0159\u00ed", ""], ["Mrkvi\u010dka", "Tom\u00e1\u0161", ""], ["Mateu", "Jorge", ""], ["Gonz\u00e1lez", "Jonatan", ""]]}, {"id": "2005.01058", "submitter": "Emmanuel Caron", "authors": "Emmanuel Caron, J\\'er\\^ome Dedecker, Bertrand Michel", "title": "Gaussian linear model selection in a dependent context", "comments": "30 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the nonparametric linear model, when the error\nprocess is a dependent Gaussian process. We focus on the estimation of the mean\nvector via a model selection approach. We first give the general theoretical\nform of the penalty function, ensuring that the penalized estimator among a\ncollection of models satisfies an oracle inequality. Then we derive a penalty\nshape involving the spectral radius of the covariance matrix of the errors,\nwhich can be chosen proportional to the dimension when the error process is\nstationary and short range dependent. However, this penalty can be too rough in\nsome cases, in particular when the error process is long range dependent. In a\nsecond part, we focus on the fixed-design regression model assuming that the\nerror process is a stationary Gaussian process. We propose a model selection\nprocedure in order to estimate the mean function via piecewise polynomials on a\nregular partition, when the error process is either short range dependent, long\nrange dependent or anti-persistent. We present different kinds of penalties,\ndepending on the memory of the process. For each case, an adaptive estimator is\nbuilt, and the rates of convergence are computed. Thanks to several sets of\nsimulations, we study the performance of these different penalties for all\ntypes of errors (short memory, long memory and anti-persistent errors).\nFinally, we give an application of our method to the well-known Nile data,\nwhich clearly shows that the type of dependence of the error process must be\ntaken into account.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 11:33:06 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Caron", "Emmanuel", ""], ["Dedecker", "J\u00e9r\u00f4me", ""], ["Michel", "Bertrand", ""]]}, {"id": "2005.01160", "submitter": "Piero Mazzarisi", "authors": "Piero Mazzarisi, Silvia Zaoli, Carlo Campajola, Fabrizio Lillo", "title": "Tail Granger causalities and where to find them: extreme risk spillovers\n  vs. spurious linkages", "comments": "22 pages, 7 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying risk spillovers in financial markets is of great importance for\nassessing systemic risk and portfolio management. Granger causality in tail (or\nin risk) tests whether past extreme events of a time series help predicting\nfuture extreme events of another time series. The topology and connectedness of\nnetworks built with Granger causality in tail can be used to measure systemic\nrisk and to identify risk transmitters. Here we introduce a novel test of\nGranger causality in tail which adopts the likelihood ratio statistic and is\nbased on the multivariate generalization of a discrete autoregressive process\nfor binary time series describing the sequence of extreme events of the\nunderlying price dynamics. The proposed test has very good size and power in\nfinite samples, especially for large sample size, allows inferring the correct\ntime scale at which the causal interaction takes place, and it is flexible\nenough for multivariate extension when more than two time series are considered\nin order to decrease false detections as spurious effect of neglected\nvariables. An extensive simulation study shows the performances of the proposed\nmethod with a large variety of data generating processes and it introduces also\nthe comparison with the test of Granger causality in tail by [Hong et al.,\n2009]. We report both advantages and drawbacks of the different approaches,\npointing out some crucial aspects related to the false detections of Granger\ncausality for tail events. An empirical application to high frequency data of a\nportfolio of US stocks highlights the merits of our novel approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 18:27:50 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 16:46:15 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Mazzarisi", "Piero", ""], ["Zaoli", "Silvia", ""], ["Campajola", "Carlo", ""], ["Lillo", "Fabrizio", ""]]}, {"id": "2005.01168", "submitter": "Liangliang Zhang", "authors": "Yingjie Li, Liangliang Zhang and Tapabrata Maiti", "title": "High Dimensional Classification for Spatially Dependent Data with\n  Application to Neuroimaging", "comments": "58 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminating patients with Alzheimer's disease (AD) from healthy subjects\nis a crucial task in the research of Alzheimer's disease. The task can be\npotentially achieved by linear discriminant analysis (LDA), which is one of the\nmost classical and popular classification techniques. However, the\nclassification problem becomes challenging for LDA because of the\nhigh-dimensionally and the spatial dependency of the brain imaging data. To\naddress the challenges, researchers have proposed various ways to generalize\nLDA into high-dimensional context in recent years. However, these existing\nmethods did not reach any consensus on how to incorporate spatially dependent\nstructure. In light of the current needs and limitations, we propose a new\nclassification method, named as Penalized Maximum Likelihood Estimation LDA\n(PMLE-LDA). The proposed method uses $Mat\\acute{e}rn$ covariance function to\ndescribe the spatial correlation of brain regions. Additionally, PMLE is\ndesigned to model the sparsity of high-dimensional features. The spatial\nlocation information is used to address the singularity of the covariance.\nTapering technique is introduced to reduce computational burden. We show in\ntheory that the proposed method can not only provide consistent results of\nparameter estimation and feature selection, but also generate an asymptotically\noptimal classifier driven by high dimensional data with specific spatially\ndependent structure. Finally, the method is validated through simulations and\nan application into ADNI data for classifying Alzheimer's patients.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 19:23:28 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Li", "Yingjie", ""], ["Zhang", "Liangliang", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2005.01169", "submitter": "Liangliang Zhang", "authors": "Liangliang Zhang, Yushu Shi, Kim-Anh Do, Christine B. Peterson and\n  Robert R. Jenq", "title": "ProgPermute: Progressive permutation for a dynamic representation of the\n  robustness of microbiome discoveries", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of features is a critical task in microbiome studies that is\ncomplicated by the fact that microbial data are high dimensional and\nheterogeneous. Masked by the complexity of the data, the problem of separating\nsignals from noise becomes challenging and troublesome. For instance, when\nperforming differential abundance tests, multiple testing adjustments tend to\nbe overconservative, as the probability of a type I error (false positive)\nincreases dramatically with the large numbers of hypotheses. Moreover, the\ngrouping effect of interest can be obscured by heterogeneity. These factors can\nincorrectly lead to the conclusion that there are no differences in the\nmicrobiome compositions. We translate and represent the problem of identifying\ndifferential features as a dynamic layout of separating the signal from its\nrandom background. We propose progressive permutation as a method to achieve\nthis process and show converging patterns. More specifically, we progressively\npermute the grouping factor labels of the microbiome samples and perform\nmultiple differential abundance tests in each scenario. We then compare the\nsignal strength of the top features from the original data with their\nperformance in permutations, and observe an apparent decreasing trend if these\ntop features are true positives identified from the data. We have developed\nthis into a user-friendly RShiny tool and R package, which consist of functions\nthat can convey the overall association between the microbiome and the grouping\nfactor, rank the robustness of the discovered microbes, and list the\ndiscoveries, their effect sizes, and individual abundances.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 19:25:57 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 14:45:47 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Zhang", "Liangliang", ""], ["Shi", "Yushu", ""], ["Do", "Kim-Anh", ""], ["Peterson", "Christine B.", ""], ["Jenq", "Robert R.", ""]]}, {"id": "2005.01262", "submitter": "Yijun Zuo", "authors": "Yijun Zuo", "title": "Exact computation of projection regression depth and fast computation of\n  its induced median and other estimators", "comments": "21 pages, 1 figure, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1905.11846", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zuo (2019) (Z19) addressed the computation of the projection regression depth\n(PRD) and its induced median (the maximum depth estimator). Z19 achieved the\nexact computation of PRD via a modified version of regular univariate sample\nmedian, which resulted in the loss of invariance of PRD and the equivariance of\ndepth induced median. This article achieves the exact computation without\nscarifying the invariance of PRD and the equivariance of the regression median.\nZ19 also addressed the approximate computation of PRD induced median, the naive\nalgorithm in Z19 is very slow. This article modifies the approximation in Z19\nand adopts Rcpp package and consequently obtains a much (could be $100$ times)\nfaster algorithm with an even better level of accuracy meanwhile. Furthermore,\nas the third major contribution, this article introduces three new depth\ninduced estimators which can run $300$ times faster than that of Z19 meanwhile\nmaintaining the same (or a bit better) level of accuracy. Real as well as\nsimulated data examples are presented to illustrate the difference between the\nalgorithms of Z19 and the ones proposed in this article. Findings support the\nstatements above and manifest the major contributions of the article.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 04:21:33 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zuo", "Yijun", ""]]}, {"id": "2005.01285", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe", "title": "Connecting the Dots: Towards Continuous Time Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous time Hamiltonian Monte Carlo is introduced, as a powerful\nalternative to Markov chain Monte Carlo methods for continuous target\ndistributions. The method is constructed in two steps: First Hamiltonian\ndynamics are chosen as the deterministic dynamics in a continuous time\npiecewise deterministic Markov process. Under very mild restrictions, such a\nprocess will have the desired target distribution as an invariant distribution.\nSecondly, the numerical implementation of such processes, based on adaptive\nnumerical integration of second order ordinary differential equations is\nconsidered. The numerical implementation yields an approximate, yet highly\nrobust algorithm that, unlike conventional Hamiltonian Monte Carlo, enables the\nexploitation of the complete Hamiltonian trajectories (hence the title). The\nproposed algorithm may yield large speedups and improvements in stability\nrelative to relevant benchmarks, while incurring numerical errors that are\nnegligible relative to the overall Monte Carlo errors.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 06:23:13 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 14:00:54 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kleppe", "Tore Selland", ""]]}, {"id": "2005.01309", "submitter": "Bruno Sudret", "authors": "X. Zhu and B. Sudret", "title": "Global sensitivity analysis for stochastic simulators based on\n  generalized lambda surrogate models", "comments": null, "journal-ref": "Reliability Engineering and System Safety, #107815, 2021", "doi": "10.1016/j.ress.2021.107815", "report-no": "RSUQ-2020-004D", "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis aims at quantifying the impact of input\nvariability onto the variation of the response of a computational model. It has\nbeen widely applied to deterministic simulators, for which a set of input\nparameters has a unique corresponding output value. Stochastic simulators,\nhowever, have intrinsic randomness due to their use of (pseudo)random numbers,\nso they give different results when run twice with the same input parameters\nbut non-common random numbers. Due to this random nature, conventional Sobol'\nindices, used in global sensitivity analysis, can be extended to stochastic\nsimulators in different ways. In this paper, we discuss three possible\nextensions and focus on those that depend only on the statistical dependence\nbetween input and output. This choice ignores the detailed data generating\nprocess involving the internal randomness, and can thus be applied to a wider\nclass of problems. We propose to use the generalized lambda model to emulate\nthe response distribution of stochastic simulators. Such a surrogate can be\nconstructed without the need for replications. The proposed method is applied\nto three examples including two case studies in finance and epidemiology. The\nresults confirm the convergence of the approach for estimating the sensitivity\nindices even with the presence of strong heteroskedasticity and small\nsignal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 08:03:31 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 16:45:43 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 13:32:39 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 09:56:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhu", "X.", ""], ["Sudret", "B.", ""]]}, {"id": "2005.01379", "submitter": "Gaetano Romano", "authors": "Gaetano Romano, Guillem Rigaill, Vincent Runge, Paul Fearnhead", "title": "Detecting Abrupt Changes in the Presence of Local Fluctuations and\n  Autocorrelated Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there are a plethora of algorithms for detecting changes in mean in\nunivariate time-series, almost all struggle in real applications where there is\nautocorrelated noise or where the mean fluctuates locally between the abrupt\nchanges that one wishes to detect. In these cases, default implementations,\nwhich are often based on assumptions of a constant mean between changes and\nindependent noise, can lead to substantial over-estimation of the number of\nchanges. We propose a principled approach to detect such abrupt changes that\nmodels local fluctuations as a random walk process and autocorrelated noise via\nan AR(1) process. We then estimate the number and location of changepoints by\nminimising a penalised cost based on this model. We develop a novel and\nefficient dynamic programming algorithm, DeCAFS, that can solve this\nminimisation problem; despite the additional challenge of dependence across\nsegments, due to the autocorrelated noise, which makes existing algorithms\ninapplicable. Theory and empirical results show that our approach has greater\npower at detecting abrupt changes than existing approaches. We apply our method\nto measuring gene expression levels in bacteria.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:51:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Romano", "Gaetano", ""], ["Rigaill", "Guillem", ""], ["Runge", "Vincent", ""], ["Fearnhead", "Paul", ""]]}, {"id": "2005.01457", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Tomohiro Shinozaki, Katsuhiro Iba, Satoshi Teramukai and\n  Toshi A. Furukawa", "title": "Confidence intervals of prediction accuracy measures for multivariable\n  prediction models based on the bootstrap-based optimism correction methods", "comments": null, "journal-ref": "Stat Med. 2021", "doi": "10.1002/sim.9148", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In assessing prediction accuracy of multivariable prediction models, optimism\ncorrections are essential for preventing biased results. However, in most\npublished papers of clinical prediction models, the point estimates of the\nprediction accuracy measures are corrected by adequate bootstrap-based\ncorrection methods, but their confidence intervals are not corrected, e.g., the\nDeLong's confidence interval is usually used for assessing the C-statistic.\nThese naive methods do not adjust for the optimism bias and do not account for\nstatistical variability in the estimation of parameters in the prediction\nmodels. Therefore, their coverage probabilities of the true value of the\nprediction accuracy measure can be seriously below the nominal level (e.g.,\n95%). In this article, we provide two generic bootstrap methods, namely (1)\nlocation-shifted bootstrap confidence intervals and (2) two-stage bootstrap\nconfidence intervals, that can be generally applied to the bootstrap-based\noptimism correction methods, i.e., the Harrell's bias correction, 0.632, and\n0.632+ methods. In addition, they can be widely applied to various methods for\nprediction model development involving modern shrinkage methods such as the\nridge and lasso regressions. Through numerical evaluations by simulations, the\nproposed confidence intervals showed favourable coverage performances. Besides,\nthe current standard practices based on the optimism-uncorrected methods showed\nserious undercoverage properties. To avoid erroneous results, the\noptimism-uncorrected confidence intervals should not be used in practice, and\nthe adjusted methods are recommended instead. We also developed the R package\npredboot for implementing these methods (https://github.com/nomahi/predboot).\nThe effectiveness of the proposed methods are illustrated via applications to\nthe GUSTO-I clinical trial.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:15:54 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 11:33:59 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 08:40:44 GMT"}, {"version": "v4", "created": "Wed, 30 Jun 2021 10:18:57 GMT"}, {"version": "v5", "created": "Sun, 25 Jul 2021 12:56:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Noma", "Hisashi", ""], ["Shinozaki", "Tomohiro", ""], ["Iba", "Katsuhiro", ""], ["Teramukai", "Satoshi", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "2005.01517", "submitter": "Mikko Kuronen", "authors": "Mikko Kuronen, Mari Myllym\\\"aki, Adam Loavenbruck, Aila S\\\"arkk\\\"a", "title": "Point process models for sweat gland activation observed with noise", "comments": "27 pages, 12 figures", "journal-ref": "Statistics in Medicine. 2021", "doi": "10.1002/sim.8891", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the paper is to construct spatial models for the activation of\nsweat glands for healthy subjects and subjects suffering from peripheral\nneuropathy by using videos of sweating recorded from the subjects. The sweat\npatterns are regarded as realizations of spatial point processes and two point\nprocess models for the sweat gland activation and two methods for inference are\nproposed. Several image analysis steps are needed to extract the point patterns\nfrom the videos and some incorrectly identified sweat gland locations may be\npresent in the data. To take into account the errors we either include an error\nterm in the point process model or use an estimation procedure that is robust\nwith respect to the errors.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:31:36 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kuronen", "Mikko", ""], ["Myllym\u00e4ki", "Mari", ""], ["Loavenbruck", "Adam", ""], ["S\u00e4rkk\u00e4", "Aila", ""]]}, {"id": "2005.01559", "submitter": "Wenjia Wang", "authors": "Wenjia Wang, Yi-Hui Zhou", "title": "Reduced Rank Multivariate Kernel Ridge Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multivariate regression, also referred to as multi-task learning in\nmachine learning, the goal is to recover a vector-valued function based on\nnoisy observations. The vector-valued function is often assumed to be of low\nrank. Although the multivariate linear regression is extensively studied in the\nliterature, a theoretical study on the multivariate nonlinear regression is\nlacking. In this paper, we study reduced rank multivariate kernel ridge\nregression, proposed by \\cite{mukherjee2011reduced}. We prove the consistency\nof the function predictor and provide the convergence rate. An algorithm based\non nuclear norm relaxation is proposed. A few numerical examples are presented\nto show the smaller mean squared prediction error comparing with the\nelementwise univariate kernel ridge regression.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:19:04 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Wenjia", ""], ["Zhou", "Yi-Hui", ""]]}, {"id": "2005.01765", "submitter": "Ashish Patel", "authors": "Ashish Patel, Stephen Burgess, Dipender Gill, Paul J. Newcombe", "title": "Inference with many correlated weak instruments and summary statistics", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns inference in instrumental variable models with a\nhigh-dimensional set of correlated weak instruments. Our focus is motivated by\nMendelian randomization, the use of genetic variants as instrumental variables\nto identify the unconfounded effect of an exposure on disease. In particular,\nwe consider the scenario where a large number of genetic instruments may be\nexogenous, but collectively they explain a low proportion of exposure\nvariation. Additionally, we assume that individual-level data are not\navailable, but rather summary statistics on genetic associations with the\nexposure and outcome, as typically published from meta-analyses of genome-wide\nassociation studies. In a two-stage approach, we first use factor analysis to\nexploit structured correlations of genetic instruments as expected in candidate\ngene analysis, and estimate an unknown vector of optimal instruments. The\nsecond stage conducts inference on the parameter of interest under scenarios of\nstrong and weak identification. Under strong identification, we consider point\nestimation based on minimization of a limited information maximum likelihood\ncriterion. Under weak instrument asymptotics, we generalize conditional\nlikelihood ratio and other identification-robust statistics to account for\nestimated instruments and summary data as inputs. Simulation results illustrate\nfavourable finite-sample properties of the factor-based conditional likelihood\nratio test, and we demonstrate use of our method by studying the effect of\ninterleukin-6 signaling on glycated hemoglobin levels.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 18:05:08 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Patel", "Ashish", ""], ["Burgess", "Stephen", ""], ["Gill", "Dipender", ""], ["Newcombe", "Paul J.", ""]]}, {"id": "2005.01835", "submitter": "Marc-Oliver Pohle", "authors": "Marc-Oliver Pohle", "title": "The Murphy Decomposition and the Calibration-Resolution Principle: A New\n  Perspective on Forecast Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I provide a unifying perspective on forecast evaluation, characterizing\naccurate forecasts of all types, from simple point to complete probabilistic\nforecasts, in terms of two fundamental underlying properties, autocalibration\nand resolution, which can be interpreted as describing a lack of systematic\nmistakes and a high information content. This \"calibration-resolution\nprinciple\" gives a new insight into the nature of forecasting and generalizes\nthe famous sharpness principle by Gneiting et al. (2007) from probabilistic to\nall types of forecasts. It amongst others exposes the shortcomings of several\nwidely used forecast evaluation methods. The principle is based on a fully\ngeneral version of the Murphy decomposition of loss functions, which I provide.\nSpecial cases of this decomposition are well-known and widely used in\nmeteorology.\n  Besides using the decomposition in this new theoretical way, after having\nintroduced it and the underlying properties in a proper theoretical framework,\naccompanied by an illustrative example, I also employ it in its classical sense\nas a forecast evaluation method as the meteorologists do: As such, it unveils\nthe driving forces behind forecast errors and complements classical forecast\nevaluation methods. I discuss estimation of the decomposition via kernel\nregression and then apply it to popular economic forecasts. Analysis of mean\nforecasts from the US Survey of Professional Forecasters and quantile forecasts\nderived from Bank of England fan charts indeed yield interesting new insights\nand highlight the potential of the method.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:41:00 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Pohle", "Marc-Oliver", ""]]}, {"id": "2005.01860", "submitter": "Kristian Agas{\\o}ster Haaga", "authors": "Kristian Agas{\\o}ster Haaga, David Diego, Jo Brendryen, Bjarte\n  Hannisdal", "title": "A simple test for causality in complex systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP nlin.CD stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a new solution to the long-standing problem of inferring causality\nfrom observations without modeling the unknown mechanisms. We show that the\nevolution of any dynamical system is related to a predictive asymmetry that\nquantifies causal connections from limited observations. A built-in\nsignificance criterion obviates surrogate testing and drastically improves\ncomputational efficiency. We validate our test on numerous synthetic systems\nexhibiting behavior commonly occurring in nature, from linear and nonlinear\nstochastic processes to systems exhibiting nonlinear deterministic chaos, and\non real-world data with known ground truths. Applied to the controversial\nproblem of glacial-interglacial sea level and CO$_{2}$ evolving in lock-step,\nour test uncovers empirical evidence for CO$_{2}$ as a driver of sea level over\nthe last 800 thousand years. Our findings are relevant to any discipline where\ntime series are used to study natural systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:41:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Haaga", "Kristian Agas\u00f8ster", ""], ["Diego", "David", ""], ["Brendryen", "Jo", ""], ["Hannisdal", "Bjarte", ""]]}, {"id": "2005.01895", "submitter": "Ping-Shou Zhong", "authors": "Shawn Santo and Ping-Shou Zhong", "title": "Homogeneity Tests of Covariance and Change-Points Identification for\n  High-Dimensional Functional Data", "comments": "The paper has 32 pages with 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference problems for high-dimensional (HD) functional data with\na dense number (T) of repeated measurements taken for a large number of p\nvariables from a small number of n experimental units. The spatial and temporal\ndependence, high dimensionality, and the dense number of repeated measurements\nall make theoretical studies and computation challenging. This paper has two\naims; our first aim is to solve the theoretical and computational challenges in\ndetecting and identifying change points among covariance matrices from HD\nfunctional data. The second aim is to provide computationally efficient and\ntuning-free tools with a guaranteed stochastic error control. The change point\ndetection procedure is developed in the form of testing the homogeneity of\ncovariance matrices. The weak convergence of the stochastic process formed by\nthe test statistics is established under the \"large p, large T and small n\"\nsetting. Under a mild set of conditions, our change point identification\nestimator is proven to be consistent for change points in any location of a\nsequence. Its rate of convergence depends on the data dimension, sample size,\nnumber of repeated measurements, and signal-to-noise ratio. We also show that\nour proposed computation algorithms can significantly reduce the computation\ntime and are applicable to real-world data such as fMRI data with a large\nnumber of HD repeated measurements. Simulation results demonstrate both finite\nsample performance and computational effectiveness of our proposed procedures.\nWe observe that the empirical size of the test is well controlled at the\nnominal level, and the locations of multiple change points can accurately be\nidentified. An application to fMRI data demonstrates that our proposed methods\ncan identify event boundaries in the preface of the movie Sherlock. Our\nproposed procedures are implemented in an R package TechPhD.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:04:58 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Santo", "Shawn", ""], ["Zhong", "Ping-Shou", ""]]}, {"id": "2005.01899", "submitter": "Zhou Zhou", "authors": "Zhou Zhou, Yang-Guan-Jian Guo and Hau-Tieng Wu", "title": "Frequency Detection and Change Point Estimation for Time Series of\n  Complex Oscillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider detecting the evolutionary oscillatory pattern of a signal when\nit is contaminated by non-stationary noises with complexly time-varying data\ngenerating mechanism. A high-dimensional dense progressive periodogram test is\nproposed to accurately detect all oscillatory frequencies. A further\nphase-adjusted local change point detection algorithm is applied in the\nfrequency domain to detect the locations at which the oscillatory pattern\nchanges. Our method is shown to be able to detect all oscillatory frequencies\nand the corresponding change points within an accurate range with a prescribed\nprobability asymptotically. This study is motivated by oscillatory frequency\nestimation and change point detection problems encountered in physiological\ntime series analysis. An application to spindle detection and estimation in\nsleep EEG data is used to illustrate the usefulness of the proposed\nmethodology. A Gaussian approximation scheme and an overlapping-block\nmultiplier bootstrap methodology for sums of complex-valued high dimensional\nnon-stationary time series without variance lower bounds are established, which\ncould be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:20:04 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhou", "Zhou", ""], ["Guo", "Yang-Guan-Jian", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "2005.01962", "submitter": "Mikko Kuronen", "authors": "Mikko Kuronen, Aila S\\\"arkk\\\"a, Matti Vihola, Mari Myllym\\\"aki", "title": "Hierarchical log Gaussian Cox process for regeneration in uneven-aged\n  forests", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchical log Gaussian Cox process (LGCP) for point patterns,\nwhere a set of points x affects another set of points y but not vice versa. We\nuse the model to investigate the effect of large trees to the locations of\nseedlings. In the model, every point in x has a parametric influence kernel or\nsignal, which together form an influence field. Conditionally on the\nparameters, the influence field acts as a spatial covariate in the intensity of\nthe model, and the intensity itself is a non-linear function of the parameters.\nPoints outside the observation window may affect the influence field inside the\nwindow. We propose an edge correction to account for this missing data. The\nparameters of the model are estimated in a Bayesian framework using Markov\nchain Monte Carlo (MCMC) where a Laplace approximation is used for the Gaussian\nfield of the LGCP model. The proposed model is used to analyze the effect of\nlarge trees on the success of regeneration in uneven-aged forest stands in\nFinland.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 06:18:15 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 08:15:49 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 11:41:16 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kuronen", "Mikko", ""], ["S\u00e4rkk\u00e4", "Aila", ""], ["Vihola", "Matti", ""], ["Myllym\u00e4ki", "Mari", ""]]}, {"id": "2005.02125", "submitter": "Nicholas James", "authors": "Nick James and Max Menzies", "title": "Cluster-based dual evolution for multivariate time series: analyzing\n  COVID-19", "comments": "Accepted manuscript. Major expression edits and minor new inferences\n  relative to v2. Equal contribution", "journal-ref": "Chaos 30, 061108 (2020)", "doi": "10.1063/5.0013156", "report-no": null, "categories": "stat.ME math.DS physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a cluster-based method to analyze the evolution of\nmultivariate time series and applies this to the COVID-19 pandemic. On each\nday, we partition countries into clusters according to both their case and\ndeath counts. The total number of clusters and individual countries' cluster\nmemberships are algorithmically determined. We study the change in both\nquantities over time, demonstrating a close similarity in the evolution of\ncases and deaths. The changing number of clusters of the case counts precedes\nthat of the death counts by 32 days. On the other hand, there is an optimal\noffset of 16 days with respect to the greatest consistency between cluster\ngroupings, determined by a new method of comparing affinity matrices. With this\noffset in mind, we identify anomalous countries in the progression from\nCOVID-19 cases to deaths. This analysis can aid in highlighting the most and\nleast significant public policies in minimizing a country's COVID-19 mortality\nrate.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 13:16:54 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 16:17:28 GMT"}, {"version": "v3", "created": "Sun, 14 Jun 2020 08:38:58 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""]]}, {"id": "2005.02245", "submitter": "Rom\\'an Salmer\\'on", "authors": "Rom\\'an Salmer\\'on and Catalina Garc\\'ia and Jos\\'e Garc\\'ia", "title": "Overcoming the inconsistences of the variance inflation factor: a\n  redefined VIF and a test to detect statistical troubling multicollinearity", "comments": "23 pages, working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicollinearity is relevant to many different fields where linear\nregression models are applied, and its existence may affect the analysis of\nordinary least squares (OLS) estimators from both the numerical and statistical\npoints of views. Thus, multicollinearity can lead to incoherence in the\nstatistical significance of the independent variables and the global\nsignificance of the model. The variance inflation factor (VIF) is traditionally\napplied to diagnose the possible existence of multicollinearity, but it is not\nalways the case that detection by VIF of a troubling degree of\nmulticollinearity corresponds to negative effects on the statistical analysis.\nThe reason for the lack of specificity of VIF is that there are other factors,\nsuch as the size of the sample and the variance of the random disturbance, that\ncan lead to high values of the VIF but not to problematic variance in the OLS\nestimators (see O'Brien 2007). This paper presents a new variance inflation\nfactor (TVIF) that consider all these additional factors. Thresholds for this\nnew measure and from the index provided by Stewart (1987) are also provided.\nThese thresholds are reinterpreted and presented as a new statistical test to\ndiagnose the existence of statistical troubling multicollinearity. The\ncontributions of this paper are illustrated with two real data examples\npreviously applied in the scientific literature.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:44:50 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Salmer\u00f3n", "Rom\u00e1n", ""], ["Garc\u00eda", "Catalina", ""], ["Garc\u00eda", "Jos\u00e9", ""]]}, {"id": "2005.02302", "submitter": "Mahdi Teimouri Yanesari", "authors": "Mahdi Teimouri", "title": "Bayesian Inference for Johnson's SB and Weibull distributions", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The four-parameter Johnson's SB (JSB) and three-parameter Weibull\ndistributions have received much attention in the field of forestry for\ncharacterizing diameters at breast height (DBH). In this work, we suggest the\nBayesian method for estimating parameters of the JBS distribution. The maximum\nlikelihood approach uses iterative methods such as Newton-Raphson (NR)\nalgorithm for maximizing the logarithm of the likelihood function. But there is\nno guarantee that the NR method converges. This fact that the NR method for\nestimating the parameters of the JSB distribution sometimes fails to converge\nwas verified through simulation in this study. Further, it was shown that the\nBayesian estimators presented in this work were robust with respect to the\ninitial values and estimate the parameters of the JSB distribution efficiently.\nThe performance of the JSB and three-parameter Weibull distributions was\ncompared in a Bayesian paradigm when these models were fitted to DBH data of\nthree plots that randomly selected from a study established in 107 plots of\nmixed-age ponderosa pine (Pinus ponderosa Dougl. ex Laws.) with scattered\nwestern junipers at the Malheur National Forest in south end of the Blue\nMountains near Burns, Oregon, USA. Bayesian paradigm demonstrated that JBS was\nsuperior model than the three-parameter Weibull for characterizing the DBH\ndistribution when these models were fitted to the DBH data of the three plots.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 23:59:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Teimouri", "Mahdi", ""]]}, {"id": "2005.02321", "submitter": "Shayan Hundrieser", "authors": "Shayan Hundrieser, Benjamin Eltzner, Stephan F. Huckemann", "title": "Finite Sample Smeariness of Fr\\'echet Means and Application to Climate", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fr\\'echet means on non-Euclidean spaces may exhibit nonstandard asymptotic\nrates rendering quantile-based asymptotic inference inapplicable. We show here\nthat this affects, among others, all circular distributions whose support\nexceeds a half circle. We exhaustively describe this phenomenon and introduce a\nnew concept which we call finite samples smeariness (FSS). In the presence of\nFSS, it turns out that quantile-based tests for equality of Fr\\'echet means\nsystematically feature effective levels higher than their nominal level which\nperseveres asymptotically in case of Type I FSS. In contrast, suitable\nbootstrap-based tests correct for FSS and asymptotically attain the correct\nlevel. For illustration of the relevance of FSS in real data, we apply our\nmethod to directional wind data from two European cities. It turns out that\nquantile based tests, not correcting for FSS, find a multitude of significant\nwind changes. This multitude condenses to a few years featuring significant\nwind changes, when our bootstrap tests are applied, correcting for FSS.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:28:35 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 07:04:16 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 20:51:35 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Hundrieser", "Shayan", ""], ["Eltzner", "Benjamin", ""], ["Huckemann", "Stephan F.", ""]]}, {"id": "2005.02458", "submitter": "Guzin Bayraksan", "authors": "Jangho Park and Rebecca Stockbridge and G\\\"uzin Bayraksan", "title": "Variance Reduction for Sequential Sampling in Stochastic Programming", "comments": null, "journal-ref": "Annals of Operations Research 300 (2021) 171-204", "doi": "10.1007/s10479-020-03908-x", "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the variance reduction techniques Antithetic Variates\n(AV) and Latin Hypercube Sampling (LHS) when used for sequential sampling in\nstochastic programming and presents a comparative computational study. It shows\nconditions under which the sequential sampling with AV and LHS satisfy finite\nstopping guarantees and are asymptotically valid, discussing LHS in detail. It\ncomputationally compares their use in both the sequential and non-sequential\nsettings through a collection of two-stage stochastic linear programs with\ndifferent characteristics. The numerical results show that while both AV and\nLHS can be preferable to random sampling in either setting, LHS typically\ndominates in the non-sequential setting while performing well sequentially and\nAV gains some advantages in the sequential setting. These results imply that,\ngiven the ease of implementation of these variance reduction techniques, armed\nwith the same theoretical properties and improved empirical performance\nrelative to random sampling, AV and LHS sequential procedures present\nattractive alternatives in practice for a class of stochastic programs.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:04:42 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 23:37:49 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Park", "Jangho", ""], ["Stockbridge", "Rebecca", ""], ["Bayraksan", "G\u00fczin", ""]]}, {"id": "2005.02490", "submitter": "Antonio Linero", "authors": "Yinpu Li, Antonio R. Linero, and Jared S. Murray", "title": "Adaptive Conditional Distribution Estimation with Bayesian Decision Tree\n  Ensembles", "comments": "5 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian nonparametric model for conditional distribution\nestimation using Bayesian additive regression trees (BART). The generative\nmodel we use is based on rejection sampling from a base model. Typical of BART\nmodels, our model is flexible, has a default prior specification, and is\ncomputationally convenient. To address the distinguished role of the response\nin the BART model we propose, we further introduce an approach to targeted\nsmoothing which is possibly of independent interest for BART models. We study\nthe proposed model theoretically and provide sufficient conditions for the\nposterior distribution to concentrate at close to the minimax optimal rate\nadaptively over smoothness classes in the high-dimensional regime in which many\npredictors are irrelevant. To fit our model we propose a data augmentation\nalgorithm which allows for existing BART samplers to be extended with minimal\neffort. We illustrate the performance of our methodology on simulated data and\nuse it to study the relationship between education and body mass index using\ndata from the medical expenditure panel survey (MEPS).\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:56:17 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Yinpu", ""], ["Linero", "Antonio R.", ""], ["Murray", "Jared S.", ""]]}, {"id": "2005.02502", "submitter": "Peter Schochet", "authors": "Peter Z. Schochet", "title": "A Lasso-OLS Hybrid Approach to Covariate Selection and Average Treatment\n  Effect Estimation for Clustered RCTs Using Design-Based Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical power is often a concern for clustered RCTs due to variance\ninflation from design effects and the high cost of adding study clusters (such\nas hospitals, schools, or communities). While covariate pre-specification is\nthe preferred approach for improving power to estimate regression-adjusted\naverage treatment effects (ATEs), further precision gains can be achieved\nthrough covariate selection once primary outcomes have been collected. This\narticle uses design-based methods underlying clustered RCTs to develop a\nLasso-OLS hybrid procedure for the post-hoc selection of covariates and ATE\nestimation that avoids model overfitting and lack of transparency. In the first\nstage, lasso estimation is conducted using cluster-level averages, where\nasymptotic normality is proved using a new central limit theorem for finite\npopulation regression estimators. In the second stage, ATEs and design-based\nstandard errors are estimated using weighted least squares with the first stage\nlasso covariates. This nonparametric approach applies to continuous, binary,\nand discrete outcomes. Simulation results indicate that Type 1 errors of the\nsecond stage ATE estimates are near nominal values and standard errors are near\ntrue ones, although somewhat conservative with small samples. The method is\ndemonstrated using data from a large, federally funded clustered RCT testing\nthe effects of school-based programs promoting behavioral health.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:15:43 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Schochet", "Peter Z.", ""]]}, {"id": "2005.02773", "submitter": "Topi Paananen", "authors": "Topi Paananen, Alejandro Catalina, Paul-Christian B\\\"urkner, Aki\n  Vehtari", "title": "Group Heterogeneity Assessment for Multilevel Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sets contain an inherent multilevel structure, for example, because\nof repeated measurements of the same observational units. Taking this structure\ninto account is critical for the accuracy and calibration of any statistical\nanalysis performed on such data. However, the large number of possible model\nconfigurations hinders the use of multilevel models in practice. In this work,\nwe propose a flexible framework for efficiently assessing differences between\nthe levels of given grouping variables in the data. The assessed group\nheterogeneity is valuable in choosing the relevant group coefficients to\nconsider in a multilevel model. Our empirical evaluations demonstrate that the\nframework can reliably identify relevant multilevel components in both\nsimulated and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 12:42:04 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Paananen", "Topi", ""], ["Catalina", "Alejandro", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "2005.02793", "submitter": "Dr. Wolfgang A. Rolke", "authors": "Wolfgang Rolke and Cristian Gutierrez Gongora", "title": "A Chi-square Goodness-of-Fit Test for Continuous Distributions against a\n  known Alternative", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": "10.1007/s00180-020-00997-x", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chi square goodness-of-fit test is among the oldest known statistical\ntests, first proposed by Pearson in 1900 for the multinomial distribution. It\nhas been in use in many fields ever since. However, various studies have shown\nthat when applied to data from a continuous distribution it is generally\ninferior to other methods such as the Kolmogorov-Smirnov or Anderson-Darling\ntests. However, the performance, that is the power, of the chi square test\ndepends crucially on the way the data is binned. In this paper we describe a\nmethod that automatically finds a binning that is very good against a specific\nalternative. We show that then the chi square test is generally competitive and\nsometimes even superior to other standard tests.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:08:59 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Rolke", "Wolfgang", ""], ["Gongora", "Cristian Gutierrez", ""]]}, {"id": "2005.02800", "submitter": "Shonosuke Sugasawa", "authors": "Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa", "title": "Log-Regularly Varying Scale Mixture of Normals for Robust Regression", "comments": "62 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression with the classical normality assumption for the error\ndistribution may lead to an undesirable posterior inference of regression\ncoefficients due to the potential outliers. This paper considers the finite\nmixture of two components with thin and heavy tails as the error distribution,\nwhich has been routinely employed in applied statistics. For the heavily-tailed\ncomponent, we introduce the novel class of distributions; their densities are\nlog-regularly varying and have heavier tails than those of Cauchy distribution,\nyet they are expressed as a scale mixture of normal distributions and enable\nthe efficient posterior inference by Gibbs sampler. We prove the robustness to\noutliers of the posterior distributions under the proposed models with a\nminimal set of assumptions, which justifies the use of shrinkage priors with\nunbounded densities for the coefficient vector in the presence of outliers. The\nextensive comparison with the existing methods via simulation study shows the\nimproved performance of our model in point and interval estimation, as well as\nits computational efficiency. Further, we confirm the posterior robustness of\nour method in the empirical study with the shrinkage priors for regression\ncoefficients.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:25:35 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 13:46:18 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Hamura", "Yasuyuki", ""], ["Irie", "Kaoru", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "2005.02908", "submitter": "Louisa Smith", "authors": "Louisa H. Smith, Maya B. Mathur, Tyler J. VanderWeele", "title": "Multiple-bias sensitivity analysis using bounds", "comments": "23 pages (main text); 19 pages (appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured confounding, selection bias, and measurement error are well-known\nsources of bias in epidemiologic research. Methods for assessing these biases\nhave their own limitations. Many quantitative sensitivity analysis approaches\nconsider each type of bias individually, while more complex approaches are\nharder to implement or require numerous assumptions. By failing to consider\nmultiple biases at once, researchers can underestimate -- or overestimate --\ntheir joint impact. We show that it is possible to bound the total composite\nbias due to these three sources, and to use that bound to assess the\nsensitivity of a risk ratio to any combination of these biases. We derive\nbounds for the total composite bias under a variety of scenarios, providing\nresearchers with tools to assess their total potential impact. We apply this\ntechnique to a study where unmeasured confounding and selection bias are both\nconcerns, and to another study in which possible differential exposure\nmisclassification and unmeasured confounding are concerns. We also show that a\n\"multi-bias E-value\" can describe the minimal strength of joint bias-parameter\nassociation necessary for an observed risk ratio to be compatible with a null\ncausal effect (or with other pre-specified effect sizes). This may provide\nintuition about the relative impacts of each type of bias. The approach we\ndescribe is easy to implement with minimal assumptions, and we provide R\nfunctions to do so.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:30:38 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Smith", "Louisa H.", ""], ["Mathur", "Maya B.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "2005.02921", "submitter": "Tom Michoel", "authors": "Muhammad Ammar Malik and Tom Michoel", "title": "Restricted maximum-likelihood method for learning latent variance\n  components in gene expression data with known and unknown confounders", "comments": "13 pages, 4 figures, plus 16 pages supplementary information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.GN q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed modelling is a popular approach for detecting and correcting\nspurious sample correlations due to hidden confounders in genome-wide gene\nexpression data. In applications where some confounding factors are known,\nestimating simultaneously the contribution of known and latent variance\ncomponents in linear mixed models is a challenge that has so far relied on\nnumerical gradient-based optimizers to maximize the likelihood function. This\nis unsatisfactory because the resulting solution is poorly characterized and\nthe efficiency of the method may be suboptimal. Here we prove analytically that\nmaximum-likelihood latent variables can always be chosen orthogonal to the\nknown confounding factors, in other words, that maximum-likelihood latent\nvariables explain sample covariances not already explained by known factors.\nBased on this result we propose a restricted maximum-likelihood method which\nestimates the latent variables by maximizing the likelihood on the restricted\nsubspace orthogonal to the known confounding factors, and show that this\nreduces to probabilistic PCA on that subspace. The method then estimates the\nvariance-covariance parameters by maximizing the remaining terms in the\nlikelihood function given the latent variables, using a newly derived analytic\nsolution for this problem. Compared to gradient-based optimizers, our method\nattains equal or higher likelihood values, can be computed using standard\nmatrix operations, results in latent factors that don't overlap with any known\nfactors, and has a runtime reduced by several orders of magnitude. We\nanticipate that the restricted maximum-likelihood method will facilitate the\napplication of linear mixed modelling strategies for learning latent variance\ncomponents to much larger gene expression datasets than currently possible.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:53:17 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Malik", "Muhammad Ammar", ""], ["Michoel", "Tom", ""]]}, {"id": "2005.02930", "submitter": "Ray Bai", "authors": "Ray Bai, Lifeng Lin, Mary R. Boland, Yong Chen", "title": "A Robust Bayesian Copas Selection Model for Quantifying and Correcting\n  Publication Bias", "comments": "25 pages, 4 figures, 2 tables. The model has been revised to consider\n  several different distributions for the study-specific random effects", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The validity of conclusions from meta-analysis is potentially threatened by\npublication bias. Most existing procedures for correcting publication bias\nassume normality of the study-specific effects that account for between-study\nheterogeneity. However, this assumption may not be valid, and the performance\nof these bias correction procedures can be highly sensitive to departures from\nnormality. Further, there exist few measures to quantify the magnitude of\npublication bias based on selection models. In this paper, we address both of\nthese issues. First, we explore the use of heavy-tailed distributions for the\nstudy-specific effects within a Bayesian hierarchical framework. The deviance\ninformation criterion (DIC) is used to determine the appropriate distribution\nto use for conducting the final analysis. Second, we develop a new measure to\nquantify the magnitude of publication bias based on Hellinger distance. Our\nmeasure is easy to interpret and takes advantage of the estimation uncertainty\nafforded naturally by the posterior distribution. We illustrate our proposed\napproach through simulation studies and meta-analyses on lung cancer and\nantidepressants. To assess the prevalence of publication bias, we apply our\nmethod to 1500 meta-analyses of dichotomous outcomes in the Cochrane Database\nof Systematic Reviews. Our methods are implemented in the publicly available R\npackage RobustBayesianCopas.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:09:36 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:07:48 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 16:50:52 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bai", "Ray", ""], ["Lin", "Lifeng", ""], ["Boland", "Mary R.", ""], ["Chen", "Yong", ""]]}, {"id": "2005.02952", "submitter": "Mickael Albertus", "authors": "Mickael Albertus", "title": "Exponential increase of the power of the independence and homogeneity\n  chi-square tests with auxiliary information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an extension of the work about the exponential increase of the\npower of two non-parametric tests: the $ Z $-test and the chi-square\ngoodness-of-fit test. Subject to having auxiliary information, it is possible\nto improve exponentially relative to the size of the sample the power of the\nfamous chi-square tests of independence and homogeneity. Improving the power of\nthese statistical tests by using auxiliary information makes it possible either\nto reduce the probability of accepting the null hypothesis under the\nalternative hypothesis, or to reduce the size of the sample necessary to reach\na predefined power. The suggested method is computational and some simple\nstatistical applications are presented to illustrate these results. The\nframework of this work is non-parametric, so it can be applied to any kind of\ndata and any area using statistics.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:47:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Albertus", "Mickael", ""]]}, {"id": "2005.03051", "submitter": "Y Cooper", "authors": "J. Batson, N. Bottman, Y. Cooper, and F. Janda", "title": "A comparison of group testing architectures for COVID-19 testing", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important component of every country's COVID-19 response is fast and\nefficient testing - to identify and isolate cases, as well as for early\ndetection of local hotspots. For many countries, producing a sufficient number\nof tests has been a serious limiting factor in their efforts to control\nCOVID-19 infections. Group testing is a well-established mathematical tool,\nwhich can provide a substantial and inexpensive expansion of testing capacity.\nIn this note, we compare several popular group testing schemes in the context\nof qPCR testing for COVID-19. We find that in practical settings, for\nidentification of individuals with COVID-19, Dorfman testing is the best choice\nat prevalences up to 30%, while for estimation of COVID-19 prevalence rates in\nthe total population, Gibbs-Gower testing is the best choice at prevalences up\nto 30% given a fixed and relatively small number of tests. For instance, at a\nprevalence of up to 2%, Dorfman testing gives an efficiency gain of 3.5--8; at\n1% prevalence, Gibbs-Gower testing gives an efficiency gain of 18, even when\ncapping the pool size at a feasible number .\n  This note is intended as a helpful handbook for labs implementing group\ntesting methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 18:01:31 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 21:18:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Batson", "J.", ""], ["Bottman", "N.", ""], ["Cooper", "Y.", ""], ["Janda", "F.", ""]]}, {"id": "2005.03151", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "On the Optimality of Randomization in Experimental Design: How to\n  Randomize for Minimax Variance and Design-Based Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I study the minimax-optimal design for a two-arm controlled experiment where\nconditional mean outcomes may vary in a given set. When this set is permutation\nsymmetric, the optimal design is complete randomization, and using a single\npartition (i.e., the design that only randomizes the treatment labels for each\nside of the partition) has minimax risk larger by a factor of $n-1$. More\ngenerally, the optimal design is shown to be the mixed-strategy optimal design\n(MSOD) of Kallus (2018). Notably, even when the set of conditional mean\noutcomes has structure (i.e., is not permutation symmetric), being\nminimax-optimal for variance still requires randomization beyond a single\npartition. Nonetheless, since this targets precision, it may still not ensure\nsufficient uniformity in randomization to enable randomization (i.e.,\ndesign-based) inference by Fisher's exact test to appropriately detect\nviolations of null. I therefore propose the inference-constrained MSOD, which\nis minimax-optimal among all designs subject to such uniformity constraints. On\nthe way, I discuss Johansson et al. (2020) who recently compared\nrerandomization of Morgan and Rubin (2012) and the pure-strategy optimal design\n(PSOD) of Kallus (2018). I point out some errors therein and set straight that\nrandomization is minimax-optimal and that the \"no free lunch\" theorem and\nexample in Kallus (2018) are correct.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 21:43:50 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "2005.03220", "submitter": "Ariel Rokem", "authors": "Ariel Rokem, Kendrick Kay", "title": "Fractional ridge regression: a fast, interpretable reparameterization of\n  ridge regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge regression (RR) is a regularization technique that penalizes the\nL2-norm of the coefficients in linear regression. One of the challenges of\nusing RR is the need to set a hyperparameter ($\\alpha$) that controls the\namount of regularization. Cross-validation is typically used to select the best\n$\\alpha$ from a set of candidates. However, efficient and appropriate selection\nof $\\alpha$ can be challenging, particularly where large amounts of data are\nanalyzed. Because the selected $\\alpha$ depends on the scale of the data and\npredictors, it is not straightforwardly interpretable. Here, we propose to\nreparameterize RR in terms of the ratio $\\gamma$ between the L2-norms of the\nregularized and unregularized coefficients. This approach, called fractional RR\n(FRR), has several benefits: the solutions obtained for different $\\gamma$ are\nguaranteed to vary, guarding against wasted calculations, and automatically\nspan the relevant range of regularization, avoiding the need for arduous manual\nexploration. We provide an algorithm to solve FRR, as well as open-source\nsoftware implementations in Python and MATLAB\n(https://github.com/nrdg/fracridge). We show that the proposed method is fast\nand scalable for large-scale data problems, and delivers results that are\nstraightforward to interpret and compare across models and datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:12:23 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Rokem", "Ariel", ""], ["Kay", "Kendrick", ""]]}, {"id": "2005.03226", "submitter": "Yichong Zhang", "authors": "Shujie Ma, Liangjun Su, Yichong Zhang", "title": "Detecting Latent Communities in Network Formation Models", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a logistic undirected network formation model which\nallows for assortative matching on observed individual characteristics and the\npresence of edge-wise fixed effects. We model the coefficients of observed\ncharacteristics to have a latent community structure and the edge-wise fixed\neffects to be of low rank. We propose a multi-step estimation procedure\ninvolving nuclear norm regularization, sample splitting, iterative logistic\nregression and spectral clustering to detect the latent communities. We show\nthat the latent communities can be exactly recovered when the expected degree\nof the network is of order log n or higher, where n is the number of nodes in\nthe network. The finite sample performance of the new estimation and inference\nmethods is illustrated through both simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:34:29 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 03:20:45 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 12:07:34 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ma", "Shujie", ""], ["Su", "Liangjun", ""], ["Zhang", "Yichong", ""]]}, {"id": "2005.03447", "submitter": "Zhenyu Zhao", "authors": "Zhenyu Zhao, Yumin Zhang, Totte Harinen, Mike Yung", "title": "Feature Selection Methods for Uplift Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uplift modeling is a predictive modeling technique that estimates the\nuser-level incremental effect of a treatment using machine learning models. It\nis often used for targeting promotions and advertisements, as well as for the\npersonalization of product offerings. In these applications, there are often\nhundreds of features available to build such models. Keeping all the features\nin a model can be costly and inefficient. Feature selection is an essential\nstep in the modeling process for multiple reasons: improving the estimation\naccuracy by eliminating irrelevant features, accelerating model training and\nprediction speed, reducing the monitoring and maintenance workload for feature\ndata pipeline, and providing better model interpretation and diagnostics\ncapability. However, feature selection methods for uplift modeling have been\nrarely discussed in the literature. Although there are various feature\nselection methods for standard machine learning models, we will demonstrate\nthat those methods are sub-optimal for solving the feature selection problem\nfor uplift modeling. To address this problem, we introduce a set of feature\nselection methods designed specifically for uplift modeling, including both\nfilter methods and embedded methods. To evaluate the effectiveness of the\nproposed feature selection methods, we use different uplift models and measure\nthe accuracy of each model with a different number of selected features. We use\nboth synthetic and real data to conduct these experiments. We also implemented\nthe proposed filter methods in an open source Python package (CausalML).\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 00:28:18 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhao", "Zhenyu", ""], ["Zhang", "Yumin", ""], ["Harinen", "Totte", ""], ["Yung", "Mike", ""]]}, {"id": "2005.03496", "submitter": "Zhaoxing Gao", "authors": "Zhaoxing Gao, Ruey S. Tsay", "title": "Modeling High-Dimensional Unit-Root Time Series", "comments": "45 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:1808.07932", "journal-ref": "International Journal of Forecasting 2020", "doi": "10.1016/j.ijforecast.2020.09.008", "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new procedure to build factor models for\nhigh-dimensional unit-root time series by postulating that a $p$-dimensional\nunit-root process is a nonsingular linear transformation of a set of unit-root\nprocesses, a set of stationary common factors, which are dynamically dependent,\nand some idiosyncratic white noise components. For the stationary components,\nwe assume that the factor process captures the temporal-dependence and the\nidiosyncratic white noise series explains, jointly with the factors, the\ncross-sectional dependence. The estimation of nonsingular linear loading spaces\nis carried out in two steps. First, we use an eigenanalysis of a nonnegative\ndefinite matrix of the data to separate the unit-root processes from the\nstationary ones and a modified method to specify the number of unit roots. We\nthen employ another eigenanalysis and a projected principal component analysis\nto identify the stationary common factors and the white noise series. We\npropose a new procedure to specify the number of white noise series and, hence,\nthe number of stationary common factors, establish asymptotic properties of the\nproposed method for both fixed and diverging $p$ as the sample size $n$\nincreases, and use simulation and a real example to demonstrate the performance\nof the proposed method in finite samples. We also compare our method with some\ncommonly used ones in the literature regarding the forecast ability of the\nextracted factors and find that the proposed method performs well in\nout-of-sample forecasting of a 508-dimensional PM$_{2.5}$ series in Taiwan.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:40:23 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 18:57:55 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Gao", "Zhaoxing", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "2005.03511", "submitter": "Graciela Boente Prof.", "authors": "Ana M. Bianco, Graciela Boente, Wenceslao Gonz\\'alez-Manteiga and Ana\n  P\\'erez-Gonz\\'alez", "title": "Robust location estimators in regression models with covariates and\n  responses missing at random", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with robust marginal estimation under a general regression\nmodel when missing data occur in the response and also in some of covariates.\nThe target is a marginal location parameter which is given through an\n$M-$functional. To obtain robust Fisher--consistent estimators, properly\ndefined marginal distribution function estimators are considered. These\nestimators avoid the bias due to missing values by assuming a missing at random\ncondition. Three methods are considered to estimate the marginal distribution\nfunction which allows to obtain the $M-$location of interest: the well-known\ninverse probability weighting, a convolution--based method that makes use of\nthe regression model and an augmented inverse probability weighting procedure\nthat prevents against misspecification. The robust proposed estimators and the\nclassical ones are compared through a numerical study under different missing\nmodels including clean and contaminated samples. We illustrate the estimators\nbehaviour under a nonlinear model. A real data set is also analysed.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:16:54 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Bianco", "Ana M.", ""], ["Boente", "Graciela", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["P\u00e9rez-Gonz\u00e1lez", "Ana", ""]]}, {"id": "2005.03513", "submitter": "Dennis Kristensen", "authors": "Ruijun Bu, Kaddour Hadri, Dennis Kristensen", "title": "Diffusion Copulas: Identification and Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new semiparametric approach for modelling nonlinear univariate\ndiffusions, where the observed process is a nonparametric transformation of an\nunderlying parametric diffusion (UPD). This modelling strategy yields a general\nclass of semiparametric Markov diffusion models with parametric dynamic copulas\nand nonparametric marginal distributions. We provide primitive conditions for\nthe identification of the UPD parameters together with the unknown\ntransformations from discrete samples. Likelihood-based estimators of both\nparametric and nonparametric components are developed and we analyze the\nasymptotic properties of these. Kernel-based drift and diffusion estimators are\nalso proposed and shown to be normally distributed in large samples. A\nsimulation study investigates the finite sample performance of our estimators\nin the context of modelling US short-term interest rates. We also present a\nsimple application of the proposed method for modelling the CBOE volatility\nindex data.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:20:00 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Bu", "Ruijun", ""], ["Hadri", "Kaddour", ""], ["Kristensen", "Dennis", ""]]}, {"id": "2005.03540", "submitter": "Micha\\\"el Zamo", "authors": "Micha\\\"el Zamo, Liliane Bel, Olivier Mestre", "title": "Sequential Aggregation of Probabilistic Forecasts -- Applicaton to Wind\n  Speed Ensemble Forecasts", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the field of numerical weather prediction (NWP), the probabilistic\ndistribution of the future state of the atmosphere is sampled with\nMonte-Carlo-like simulations, called ensembles. These ensembles have\ndeficiencies (such as conditional biases) that can be corrected thanks to\nstatistical post-processing methods. Several ensembles exist and may be\ncorrected with different statistiscal methods. A further step is to combine\nthese raw or post-processed ensembles. The theory of prediction with expert\nadvice allows us to build combination algorithms with theoretical guarantees on\nthe forecast performance. This article adapts this theory to the case of\nprobabilistic forecasts issued as step-wise cumulative distribution functions\n(CDF). The theory is applied to wind speed forecasting, by combining several\nraw or post-processed ensembles, considered as CDFs. The second goal of this\nstudy is to explore the use of two forecast performance criteria: the Continous\nranked probability score (CRPS) and the Jolliffe-Primo test. Comparing the\nresults obtained with both criteria leads to reconsidering the usual way to\nbuild skillful probabilistic forecasts, based on the minimization of the CRPS.\nMinimizing the CRPS does not necessarily produce reliable forecasts according\nto the Jolliffe-Primo test. The Jolliffe-Primo test generally selects reliable\nforecasts, but could lead to issuing suboptimal forecasts in terms of CRPS. It\nis proposed to use both criterion to achieve reliable and skillful\nprobabilistic forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:07:43 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zamo", "Micha\u00ebl", ""], ["Bel", "Liliane", ""], ["Mestre", "Olivier", ""]]}, {"id": "2005.03658", "submitter": "Mark Risser", "authors": "Mark Risser", "title": "Nonstationary Bayesian modeling for a large data set of derived surface\n  temperature return values", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.14101", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heat waves resulting from prolonged extreme temperatures pose a significant\nrisk to human health globally. Given the limitations of observations of extreme\ntemperature, climate models are often used to characterize extreme temperature\nglobally, from which one can derive quantities like return values to summarize\nthe magnitude of a low probability event for an arbitrary geographic location.\nHowever, while these derived quantities are useful on their own, it is also\noften important to apply a spatial statistical model to such data in order to,\ne.g., understand how the spatial dependence properties of the return values\nvary over space and emulate the climate model for generating additional spatial\nfields with corresponding statistical properties. For these objectives, when\nmodeling global data it is critical to use a nonstationary covariance function.\nFurthermore, given that the output of modern global climate models can be on\nthe order of $\\mathcal{O}(10^4)$, it is important to utilize approximate\nGaussian process methods to enable inference. In this paper, we demonstrate the\napplication of methodology introduced in Risser and Turek (2020) to conduct a\nnonstationary and fully Bayesian analysis of a large data set of 20-year return\nvalues derived from an ensemble of global climate model runs with over 50,000\nspatial locations. This analysis uses the freely available BayesNSGP software\npackage for R.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 21:34:33 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Risser", "Mark", ""]]}, {"id": "2005.03694", "submitter": "Karl Gregory", "authors": "Xiangyang Cao, Karl Gregory, Dewei Wang", "title": "High-Dimensional Inference Based on the Leave-One-Covariate-Out LASSO\n  Path", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new measure of variable importance in high-dimensional\nregression based on the change in the LASSO solution path when one covariate is\nleft out. The proposed procedure provides a novel way to calculate variable\nimportance and conduct variable screening. In addition, our procedure allows\nfor the construction of P-values for testing whether each coefficient is equal\nto zero as well as for testing hypotheses involving multiple regression\ncoefficients simultaneously; bootstrap techniques are used to construct the\nnull distribution. For low-dimensional linear models, our method can achieve\nhigher power than the $t$-test. Extensive simulations are provided to show the\neffectiveness of our method. In the high-dimensional setting, our proposed\nsolution path based test achieves greater power than some other recently\ndeveloped high-dimensional inference methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:42:51 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Cao", "Xiangyang", ""], ["Gregory", "Karl", ""], ["Wang", "Dewei", ""]]}, {"id": "2005.03861", "submitter": "Michael Gallaugher", "authors": "Salvatore D. Tomarchio, Michael P.B. Gallaugher, Antonio Punzo, and\n  Paul D. McNicholas", "title": "Mixtures of Contaminated Matrix Variate Normal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of three-way data is becoming ever more prevalent in the literature,\nespecially in the area of clustering and classification. Real data, including\nreal three-way data, are often contaminated by potential outlying observations.\nTheir detection, as well as the development of robust models insensitive to\ntheir presence, is particularly important for this type of data because of the\npractical issues concerning their effective visualization. Herein, the\ncontaminated matrix variate normal distribution is discussed and then utilized\nin the mixture model paradigm for clustering. One key advantage of the proposed\nmodel is the ability to automatically detect potential outlying matrices by\ncomputing their \\textit{a posteriori} probability to be a \"good\" or \"bad\"\npoint. Such detection is currently unavailable using existing matrix variate\nmethods. An expectation conditional maximization algorithm is used for\nparameter estimation, and both simulated and real data are used for\nillustration.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 05:32:18 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Tomarchio", "Salvatore D.", ""], ["Gallaugher", "Michael P. B.", ""], ["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "2005.03906", "submitter": "Niko Hauzenberger", "authors": "Niko Hauzenberger and Florian Huber and Gary Koop", "title": "Dynamic Shrinkage Priors for Large Time-varying Parameter Regressions\n  using Scalable Markov Chain Monte Carlo Methods", "comments": "Keywords: Time-varying parameter regression, dynamic shrinkage prior,\n  global-local shrinkage prior, Bayesian variable selection, scalable Markov\n  Chain Monte Carlo JEL Codes: C11, C30, E3, D31", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying parameter (TVP) regression models can involve a huge number of\ncoefficients. Careful prior elicitation is required to yield sensible posterior\nand predictive inferences. In addition, the computational demands of Markov\nChain Monte Carlo (MCMC) methods mean their use is limited to the case where\nthe number of predictors is not too large. In light of these two concerns, this\npaper proposes a new dynamic shrinkage prior which reflects the empirical\nregularity that TVPs are typically sparse (i.e. time variation may occur only\nepisodically and only for some of the coefficients). A scalable MCMC algorithm\nis developed which is capable of handling very high dimensional TVP regressions\nor TVP Vector Autoregressions. In an exercise using artificial data we\ndemonstrate the accuracy and computational efficiency of our methods. In an\napplication involving the term structure of interest rates in the eurozone, we\nfind our dynamic shrinkage prior to effectively pick out small amounts of\nparameter change and our methods to forecast well.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 08:40:09 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Hauzenberger", "Niko", ""], ["Huber", "Florian", ""], ["Koop", "Gary", ""]]}, {"id": "2005.03952", "submitter": "Chris Oates", "authors": "Marina Riabiz, Wilson Chen, Jon Cockayne, Pawel Swietach, Steven A.\n  Niederer, Lester Mackey, Chris. J. Oates", "title": "Optimal Thinning of MCMC Output", "comments": "To appear in the Journal of the Royal Statistical Society, Series B,\n  2021+", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of heuristics to assess the convergence and compress the output of\nMarkov chain Monte Carlo can be sub-optimal in terms of the empirical\napproximations that are produced. Typically a number of the initial states are\nattributed to \"burn in\" and removed, whilst the remainder of the chain is\n\"thinned\" if compression is also required. In this paper we consider the\nproblem of retrospectively selecting a subset of states, of fixed cardinality,\nfrom the sample path such that the approximation provided by their empirical\ndistribution is close to optimal. A novel method is proposed, based on greedy\nminimisation of a kernel Stein discrepancy, that is suitable for problems where\nheavy compression is required. Theoretical results guarantee consistency of the\nmethod and its effectiveness is demonstrated in the challenging context of\nparameter inference for ordinary differential equations. Software is available\nin the Stein Thinning package in Python, R and MATLAB.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 10:54:25 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:48:04 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 14:58:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Riabiz", "Marina", ""], ["Chen", "Wilson", ""], ["Cockayne", "Jon", ""], ["Swietach", "Pawel", ""], ["Niederer", "Steven A.", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2005.04010", "submitter": "Mirrelijn Van Nee", "authors": "Mirrelijn M. van Nee, Lodewyk F.A. Wessels and Mark A. van de Wiel", "title": "Flexible co-data learning for high-dimensional prediction", "comments": "Document consists of main content (20 pages, 10 figures) and\n  supplementary material (14 pages, 13 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical research often focuses on complex traits in which many variables\nplay a role in mechanisms driving, or curing, diseases. Clinical prediction is\nhard when data is high-dimensional, but additional information, like domain\nknowledge and previously published studies, may be helpful to improve\npredictions. Such complementary data, or co-data, provide information on the\ncovariates, such as genomic location or p-values from external studies. Our\nmethod enables exploiting multiple and various co-data sources to improve\npredictions. We use discrete or continuous co-data to define possibly\noverlapping or hierarchically structured groups of covariates. These are then\nused to estimate adaptive multi-group ridge penalties for generalised linear\nand Cox models. We combine empirical Bayes estimation of group penalty\nhyperparameters with an extra level of shrinkage. This renders a uniquely\nflexible framework as any type of shrinkage can be used on the group level. The\nhyperparameter shrinkage learns how relevant a specific co-data source is,\ncounters overfitting of hyperparameters for many groups, and accounts for\nstructured co-data. We describe various types of co-data and propose suitable\nforms of hypershrinkage. The method is very versatile, as it allows for\nintegration and weighting of multiple co-data sets, inclusion of unpenalised\ncovariates and posterior variable selection. We demonstrate it on two cancer\ngenomics applications and show that it may improve the performance of other\ndense and parsimonious prognostic models substantially, and stabilises variable\nselection.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:04:31 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["van Nee", "Mirrelijn M.", ""], ["Wessels", "Lodewyk F. A.", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "2005.04034", "submitter": "Michel Besserve", "authors": "Shervin Safavi, Nikos K. Logothetis and Michel Besserve", "title": "From univariate to multivariate coupling between continuous signals and\n  point processes: a mathematical framework", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series datasets often contain heterogeneous signals, composed of both\ncontinuously changing quantities and discretely occurring events. The coupling\nbetween these measurements may provide insights into key underlying mechanisms\nof the systems under study. To better extract this information, we investigate\nthe asymptotic statistical properties of coupling measures between continuous\nsignals and point processes. We first introduce martingale stochastic\nintegration theory as a mathematical model for a family of statistical\nquantities that include the Phase Locking Value, a classical coupling measure\nto characterize complex dynamics. Based on the martingale Central Limit\nTheorem, we can then derive the asymptotic Gaussian distribution of estimates\nof such coupling measure, that can be exploited for statistical testing.\nSecond, based on multivariate extensions of this result and Random Matrix\nTheory, we establish a principled way to analyze the low rank coupling between\na large number of point processes and continuous signals. For a null hypothesis\nof no coupling, we establish sufficient conditions for the empirical\ndistribution of squared singular values of the matrix to converge, as the\nnumber of measured signals increases, to the well-known Marchenko-Pastur (MP)\nlaw, and the largest squared singular value converges to the upper end of the\nMPs support. This justifies a simple thresholding approach to assess the\nsignificance of multivariate coupling. Finally, we illustrate with simulations\nthe relevance of our univariate and multivariate results in the context of\nneural time series, addressing how to reliably quantify the interplay between\nmulti channel Local Field Potential signals and the spiking activity of a large\npopulation of neurons.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:31:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Safavi", "Shervin", ""], ["Logothetis", "Nikos K.", ""], ["Besserve", "Michel", ""]]}, {"id": "2005.04051", "submitter": "Arkadius Kalka", "authors": "Arkadius Kalka and Sonja Kuhnt", "title": "The numerical statistical fan for noisy experimental designs", "comments": "24 pages, 2 figures. Some proofs and more Literature were added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.AC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifiability of polynomial models is a key requirement for multiple\nregression. We consider an analogue of the so-called statistical fan, the set\nof all maximal identifiable hierarchical models, for cases of noisy\nexperimental designs or measured covariate vectors with a given tolerance\nvector. This gives rise to the definition of the numerical statistical fan. It\nincludes all maximal hierarchical models that avoid approximate linear\ndependence of the model terms. We develop an algorithm to compute the numerical\nstatistical fan using recent results on the computation of all border bases of\na design ideal from the field of algebra. The ideas are applied to data from a\nthermal spraying process. It turns out that the numerical statistical fan is\neffectively computable and much smaller than the respective statistical fan.\nThe gained enhanced knowledge of the space of all stable identifiable\nhierarchical models enables improved model selection procedures.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:06:07 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 17:29:53 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Kalka", "Arkadius", ""], ["Kuhnt", "Sonja", ""]]}, {"id": "2005.04089", "submitter": "David Preinerstorfer", "authors": "Benedikt M. P\\\"otscher and David Preinerstorfer", "title": "How Reliable are Bootstrap-based Heteroskedasticity Robust Tests?", "comments": "54 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop theoretical finite-sample results concerning the size of wild\nbootstrap-based heteroskedasticity robust tests in linear regression models. In\nparticular, these results provide an efficient diagnostic check, which can be\nused to weed out tests that are unreliable for a given testing problem in the\nsense that they overreject substantially. This allows us to assess the\nreliability of a large variety of wild bootstrap-based tests in an extensive\nnumerical study.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:06:36 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Preinerstorfer", "David", ""]]}, {"id": "2005.04141", "submitter": "Pascal Michaillat", "authors": "Adam McCloskey, Pascal Michaillat", "title": "Incentive-Compatible Critical Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical hypothesis tests are a cornerstone of scientific research. The\ntests are informative when their size is properly controlled, so the frequency\nof rejecting true null hypotheses (type I error) stays below a prespecified\nnominal level. Publication bias exaggerates test sizes, however. Since\nscientists can typically only publish results that reject the null hypothesis,\nthey have the incentive to continue conducting studies until attaining\nrejection. Such $p$-hacking takes many forms: from collecting additional data\nto examining multiple regression specifications, all in the search of\nstatistical significance. The process inflates test sizes above their nominal\nlevels because the critical values used to determine rejection assume that test\nstatistics are constructed from a single study---abstracting from $p$-hacking.\nThis paper addresses the problem by constructing critical values that are\ncompatible with scientists' behavior given their incentives. We assume that\nresearchers conduct studies until finding a test statistic that exceeds the\ncritical value, or until the benefit from conducting an extra study falls below\nthe cost. We then solve for the incentive-compatible critical value (ICCV).\nWhen the ICCV is used to determine rejection, readers can be confident that\nsize is controlled at the desired significance level, and that the researcher's\nresponse to the incentives delineated by the critical value is accounted for.\nSince they allow researchers to search for significance among multiple studies,\nICCVs are larger than classical critical values. Yet, for a broad range of\nresearcher behaviors and beliefs, ICCVs lie in a fairly narrow range.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 16:37:11 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["McCloskey", "Adam", ""], ["Michaillat", "Pascal", ""]]}, {"id": "2005.04362", "submitter": "Pengfei Li", "authors": "Meng Yuan, Pengfei Li and Changbao Wu", "title": "Semiparametric Inference of the Youden Index and the Optimal Cutoff\n  Point under Density Ratio Models", "comments": "37 pages, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Youden index is a popular summary statistic for receiver operating\ncharacteristic curve. It gives the optimal cutoff point of a biomarker to\ndistinguish the diseased and healthy individuals. In this paper, we propose to\nmodel the distributions of a biomarker for individuals in the healthy and\ndiseased groups via a semiparametric density ratio model. Based on this model,\nwe use the maximum empirical likelihood method to estimate the Youden index and\nthe optimal cutoff point. We further establish the asymptotic normality of the\nproposed estimators and construct valid confidence intervals for the Youden\nindex and the corresponding optimal cutoff point. The proposed method\nautomatically covers both cases when there is no lower limit of detection\n(LLOD) and when there is a fixed and finite LLOD for the biomarker. Extensive\nsimulation studies and a real data example are used to illustrate the\neffectiveness of the proposed method and its advantages over the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 03:42:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yuan", "Meng", ""], ["Li", "Pengfei", ""], ["Wu", "Changbao", ""]]}, {"id": "2005.04500", "submitter": "Joann Jasiak", "authors": "Christian Gourieroux and Joann Jasiak", "title": "Time Varying Markov Process with Partially Observed Aggregate Data; An\n  Application to Coronavirus", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major difficulty in the analysis of propagation of the coronavirus is that\nmany infected individuals show no symptoms of Covid-19. This implies a lack of\ninformation on the total counts of infected individuals and of recovered and\nimmunized individuals. In this paper, we consider parametric time varying\nMarkov processes of Coronavirus propagation and show how to estimate the model\nparameters and approximate the unobserved counts from daily numbers of infected\nand detected individuals and total daily death counts. This model-based\napproach is illustrated in an application to French data.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 19:34:21 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gourieroux", "Christian", ""], ["Jasiak", "Joann", ""]]}, {"id": "2005.04549", "submitter": "Huiqin Xin", "authors": "Huiqin Xin, Sihai Dave Zhao", "title": "A nonparametric empirical Bayes approach to covariance matrix estimation", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an empirical Bayes method to estimate high-dimensional covariance\nmatrices. Our procedure centers on vectorizing the covariance matrix and\ntreating matrix estimation as a vector estimation problem. Drawing from the\ncompound decision theory literature, we introduce a new class of decision rules\nthat generalizes several existing procedures. We then use a nonparametric\nempirical Bayes g-modeling approach to estimate the oracle optimal rule in that\nclass. This allows us to let the data itself determine how best to shrink the\nestimator, rather than shrinking in a pre-determined direction such as toward a\ndiagonal matrix. Simulation results and a gene expression network analysis\nshows that our approach can outperform a number of state-of-the-art proposals\nin a wide range of settings, sometimes substantially.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 02:05:11 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 23:00:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xin", "Huiqin", ""], ["Zhao", "Sihai Dave", ""]]}, {"id": "2005.04584", "submitter": "Chengchun Shi", "authors": "Chengchun Shi and Lexin Li", "title": "Testing Mediation Effects Using Logic of Boolean Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis is becoming an increasingly important tool in scientific\nstudies. A central question in high-dimensional mediation analysis is to infer\nthe significance of individual mediators. The main challenge is the sheer\nnumber of possible paths that go through all combinations of mediators. Most\nexisting mediation inference solutions either explicitly impose that the\nmediators are conditionally independent given the exposure, or ignore any\npotential directed paths among the mediators. In this article, we propose a\nnovel hypothesis testing procedure to evaluate individual mediation effects,\nwhile taking into account potential interactions among the mediators. Our\nproposal thus fills a crucial gap, and greatly extends the scope of existing\nmediation tests. Our key idea is to construct the test statistic using the\nlogic of Boolean matrices, which enables us to establish the proper limiting\ndistribution under the null hypothesis. We further employ screening, data\nsplitting, and decorrelated estimation to reduce the bias and increase the\npower of the test. We show our test can control both the size and false\ndiscovery rate asymptotically, and the power of the test approaches one,\nmeanwhile allowing the number of mediators to diverge to infinity with the\nsample size. We demonstrate the efficacy of our method through both simulations\nand a neuroimaging study of Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 06:01:50 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 13:29:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Shi", "Chengchun", ""], ["Li", "Lexin", ""]]}, {"id": "2005.04683", "submitter": "Emilie Lebarbier", "authors": "Annarosa Quarello and Olivier Bock and Emilie Lebarbier", "title": "A new segmentation method for the homogenisation of GNSS-derived IWV\n  time-series", "comments": "25 pages, 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homogenization is an important and crucial step to improve the usage of\nobservational data for climate analysis. This work is motivated by the analysis\nof long series of GNSS Integrated Water Vapour (IWV) data which have not yet\nbeen used in this context. This paper proposes a novel segmentation method that\nintegrates a periodic bias and a heterogeneous, monthly varying, variance. The\nmethod consists in estimating first the variance using a robust estimator and\nthen estimating the segmentation and periodic bias iteratively. This strategy\nallows for the use of the dynamic programming algorithm that remains the most\nefficient exact algorithm to estimate the change-point positions. The\nstatistical performance of the method is assessed through numerical\nexperiments. An application to a real data set of 120 global GNSS stations is\npresented. The method is implemented in the R package GNSSseg that will be\navailable on the CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 14:59:28 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Quarello", "Annarosa", ""], ["Bock", "Olivier", ""], ["Lebarbier", "Emilie", ""]]}, {"id": "2005.04721", "submitter": "Geoffrey Johnson", "authors": "Geoffrey S Johnson", "title": "Decision Making in Drug Development via Confidence Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical drug development a typical phase three power calculation for a\nGo/No-Go decision is performed by replacing unknown population-level quantities\nin the power function with what is observed from a literature review or what is\nobserved in phase two. Many authors and practitioners view this as an assumed\nvalue of power and offer the Bayesian quantity probability of success or\nassurance as an alternative. The claim is by averaging over a prior or\nposterior distribution, probability of success transcends power by capturing\nthe uncertainty around the unknown true treatment effect and any other\npopulation-level parameters. We use confidence distributions to frame both the\nprobability of success calculation and the typical power calculation as merely\nproducing two different point estimates of power. We demonstrate that Go/No-Go\ndecisions based on either point estimate of power do not adequately quantify\nand control the risk involved, and instead we argue for Go/No-Go decisions that\nutilize inference on power for better risk management and decision making. This\ninference on power can be derived and displayed using confidence distributions.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 17:25:05 GMT"}, {"version": "v10", "created": "Wed, 10 Mar 2021 19:46:14 GMT"}, {"version": "v11", "created": "Mon, 7 Jun 2021 18:22:09 GMT"}, {"version": "v12", "created": "Thu, 15 Jul 2021 17:03:43 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 03:50:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 12:33:07 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 19:06:31 GMT"}, {"version": "v5", "created": "Thu, 6 Aug 2020 13:12:07 GMT"}, {"version": "v6", "created": "Mon, 10 Aug 2020 22:35:41 GMT"}, {"version": "v7", "created": "Tue, 13 Oct 2020 14:25:49 GMT"}, {"version": "v8", "created": "Sat, 2 Jan 2021 00:05:34 GMT"}, {"version": "v9", "created": "Fri, 19 Feb 2021 01:13:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Johnson", "Geoffrey S", ""]]}, {"id": "2005.04798", "submitter": "Zhiyang Zhou", "authors": "Zhiyang Zhou", "title": "Fast implementation of partial least squares for function-on-function\n  regression", "comments": "25 pages, 2 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People employ the function-on-function regression to model the relationship\nbetween two random curves. Fitting this model, widely used strategies include\nalgorithms falling into the framework of functional partial least squares\n(typically requiring iterative eigen-decomposition). Here we introduce a route\nof functional partial least squares based upon Krylov subspaces. It can be\nexpressed in two forms equivalent to each other (in exact arithmetic): one is\nnon-iterative with explicit forms of estimators and predictions, facilitating\nthe theoretical derivation and potential extensions (to more complex models);\nthe other one stabilizes numerical outputs. The consistence of estimators and\npredictions is established under regularity conditions. Our proposal is\nhighlighted as it is less computationally involved. Meanwhile, it is\ncompetitive in terms of both estimation and prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 22:13:30 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 04:52:42 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Zhou", "Zhiyang", ""]]}, {"id": "2005.04834", "submitter": "Jean Feng", "authors": "Jean Feng and Noah Simon", "title": "Ensembled sparse-input hierarchical networks for high-dimensional\n  datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have seen limited use in prediction for high-dimensional data\nwith small sample sizes, because they tend to overfit and require tuning many\nmore hyperparameters than existing off-the-shelf machine learning methods. With\nsmall modifications to the network architecture and training procedure, we show\nthat dense neural networks can be a practical data analysis tool in these\nsettings. The proposed method, Ensemble by Averaging Sparse-Input Hierarchical\nnetworks (EASIER-net), appropriately prunes the network structure by tuning\nonly two L1-penalty parameters, one that controls the input sparsity and\nanother that controls the number of hidden layers and nodes. The method selects\nvariables from the true support if the irrelevant covariates are only weakly\ncorrelated with the response; otherwise, it exhibits a grouping effect, where\nstrongly correlated covariates are selected at similar rates. On a collection\nof real-world datasets with different sizes, EASIER-net selected network\narchitectures in a data-adaptive manner and achieved higher prediction accuracy\nthan off-the-shelf methods on average.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 02:08:53 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Feng", "Jean", ""], ["Simon", "Noah", ""]]}, {"id": "2005.04837", "submitter": "Lin Qiu", "authors": "Lin Qiu and Vernon M. Chinchilli", "title": "Probabilistic Canonical Correlation Analysis for Sparse Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a classical and important\nmultivariate technique for exploring the relationship between two sets of\ncontinuous variables. CCA has applications in many fields, such as genomics and\nneuroimaging. It can extract meaningful features as well as use these features\nfor subsequent analysis. Although some sparse CCA methods have been developed\nto deal with high-dimensional problems, they are designed specifically for\ncontinuous data and do not consider the integer-valued data from\nnext-generation sequencing platforms that exhibit very low counts for some\nimportant features. We propose a model-based probabilistic approach for\ncorrelation and canonical correlation estimation for two sparse count data sets\n(PSCCA). PSCCA demonstrates that correlations and canonical correlations\nestimated at the natural parameter level are more appropriate than traditional\nestimation methods applied to the raw data. We demonstrate through simulation\nstudies that PSCCA outperforms other standard correlation approaches and sparse\nCCA approaches in estimating the true correlations and canonical correlations\nat the natural parameter level. We further apply the PSCCA method to study the\nassociation of miRNA and mRNA expression data sets from a squamous cell lung\ncancer study, finding that PSCCA can uncover a large number of strongly\ncorrelated pairs than standard correlation and other sparse CCA approaches.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 02:19:57 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Qiu", "Lin", ""], ["Chinchilli", "Vernon M.", ""]]}, {"id": "2005.04914", "submitter": "Jie Wu", "authors": "J. Wu, Z. Zheng, Y. Li, Y. Zhang", "title": "Scalable Interpretable Learning for Multi-Response Error-in-Variables\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corrupted data sets containing noisy or missing observations are prevalent in\nvarious contemporary applications such as economics, finance and\nbioinformatics. Despite the recent methodological and algorithmic advances in\nhigh-dimensional multi-response regression, how to achieve scalable and\ninterpretable estimation under contaminated covariates is unclear. In this\npaper, we develop a new methodology called convex conditioned sequential sparse\nlearning (COSS) for error-in-variables multi-response regression under both\nadditive measurement errors and random missing data. It combines the strengths\nof the recently developed sequential sparse factor regression and the nearest\npositive semi-definite matrix projection, thus enjoying stepwise convexity and\nscalability in large-scale association analyses. Comprehensive theoretical\nguarantees are provided and we demonstrate the effectiveness of the proposed\nmethodology through numerical studies.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:17:45 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wu", "J.", ""], ["Zheng", "Z.", ""], ["Li", "Y.", ""], ["Zhang", "Y.", ""]]}, {"id": "2005.04930", "submitter": "Jelle Goeman", "authors": "Jelle Goeman and Aldo Solari", "title": "Comparing three groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit simple and powerful methods for multiple pairwise comparisons that\ncan be used in designs with three groups. We argue that the proper choice of\nmethod should be determined by the assessment which of the comparisons are\nconsidered primary and which are secondary, as determined by subject-matter\nconsiderations. We review four different methods that are simple to use with\nany standard software, but are substantially more powerful than frequently-used\nmethods such as an ANOVA test followed by Tukey's method.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:53:42 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Goeman", "Jelle", ""], ["Solari", "Aldo", ""]]}, {"id": "2005.05101", "submitter": "Palash Sharma", "authors": "Palash Sharma", "title": "Generalized Univariate Distributions and a New Asymmetric Laplace Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work provides a survey of the general class of distributions generated\nfrom the mixture of the beta random variables. We provide an extensive review\nof the literature, concerning generating new distributions via the inverse CDF\ntransformation. In particular, we accounted for beta generated and Kumaraswamy\ngenerated families of distributions. We provide a brief summary of each of\ntheir families of distributions. We also propose a new asymmetric mixture\ndistribution, which is an alternative to beta generated distributions. We\nprovide basic properties of this new class of distributions generated from the\nLaplace model. We also address the issue of parameter estimation of this new\nskew generalized Laplace model.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 13:37:31 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Sharma", "Palash", ""]]}, {"id": "2005.05113", "submitter": "Jasper Velthoen", "authors": "Jasper Velthoen, Juan-Juan Cai, Geurt Jongbloed", "title": "Interpretable random forest models through forward variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest is a popular prediction approach for handling high dimensional\ncovariates. However, it often becomes infeasible to interpret the obtained high\ndimensional and non-parametric model. Aiming for obtaining an interpretable\npredictive model, we develop a forward variable selection method using the\ncontinuous ranked probability score (CRPS) as the loss function. Our stepwise\nprocedure leads to a smallest set of variables that optimizes the CRPS risk by\nperforming at each step a hypothesis test on a significant decrease in CRPS\nrisk. We provide mathematical motivation for our method by proving that in\npopulation sense the method attains the optimal set. Additionally, we show that\nthe test is consistent provided that the random forest estimator of a quantile\nfunction is consistent.\n  In a simulation study, we compare the performance of our method with an\nexisting variable selection method, for different sample sizes and different\ncorrelation strength of covariates. Our method is observed to have a much lower\nfalse positive rate. We also demonstrate an application of our method to\nstatistical post-processing of daily maximum temperature forecasts in the\nNetherlands. Our method selects about 10% covariates while retaining the same\npredictive power.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 13:56:49 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Velthoen", "Jasper", ""], ["Cai", "Juan-Juan", ""], ["Jongbloed", "Geurt", ""]]}, {"id": "2005.05156", "submitter": "Philippe Lambert", "authors": "Philippe Lambert", "title": "Fast Bayesian Inference in Nonparametric Double Additive Location-Scale\n  Models With Right- and Interval-Censored Data", "comments": "38 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized B-splines are routinely used in additive models to describe smooth\nchanges in a response with quantitative covariates. It is typically done\nthrough the conditional mean in the exponential family using generalized\nadditive models with an indirect impact on other conditional moments. Another\ncommon strategy consists in focussing on several low-order conditional moments,\nleaving the complete conditional distribution unspecified. Alternatively, a\nmulti-parameter distribution could be assumed for the response with several of\nits parameters jointly regressed on covariates using additive expressions.\n  Our work can be connected to the latter proposal for a right- or\ninterval-censored continuous response with a highly flexible and smooth\nnonparametric density. We focus on location-scale models with additive terms in\nthe conditional mean and standard deviation. Starting from recent results in\nthe Bayesian framework, we propose a quickly converging algorithm to select\npenalty parameters from their marginal posteriors. It relies on Laplace\napproximations to the conditional posterior of the spline parameters.\nSimulations suggest that the so-obtained estimators own excellent frequentist\nproperties and increase efficiency as compared to approaches with a working\nGaussian hypothesis. We illustrate the methodology with the analysis of\nimprecisely measured income data.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 14:46:11 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Lambert", "Philippe", ""]]}, {"id": "2005.05286", "submitter": "Benjamin Guedj", "authors": "Florent Dewez and Benjamin Guedj and Vincent Vandewalle", "title": "From industry-wide parameters to aircraft-centric on-flight inference:\n  improving aeronautics performance prediction with machine learning", "comments": "Published in Data-Centric Engineering", "journal-ref": "Data-Centric Engineering 2020", "doi": "10.1017/dce.2020.12", "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Aircraft performance models play a key role in airline operations, especially\nin planning a fuel-efficient flight. In practice, manufacturers provide\nguidelines which are slightly modified throughout the aircraft life cycle via\nthe tuning of a single factor, enabling better fuel predictions. However this\nhas limitations, in particular they do not reflect the evolution of each\nfeature impacting the aircraft performance. Our goal here is to overcome this\nlimitation. The key contribution of the present article is to foster the use of\nmachine learning to leverage the massive amounts of data continuously recorded\nduring flights performed by an aircraft and provide models reflecting its\nactual and individual performance. We illustrate our approach by focusing on\nthe estimation of the drag and lift coefficients from recorded flight data. As\nthese coefficients are not directly recorded, we resort to aerodynamics\napproximations. As a safety check, we provide bounds to assess the accuracy of\nboth the aerodynamics approximation and the statistical performance of our\napproach. We provide numerical results on a collection of machine learning\nalgorithms. We report excellent accuracy on real-life data and exhibit\nempirical evidence to support our modelling, in coherence with aerodynamics\nprinciples.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:40:17 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 16:42:36 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 10:22:43 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Dewez", "Florent", ""], ["Guedj", "Benjamin", ""], ["Vandewalle", "Vincent", ""]]}, {"id": "2005.05324", "submitter": "Sanjeena Subedi", "authors": "Yuan Fang, Dimitris Karlis, Sanjeena Subedi", "title": "Infinite mixtures of multivariate normal-inverse Gaussian distributions\n  for clustering of skewed data", "comments": "61 pages. arXiv admin note: text overlap with arXiv:2005.02585", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of multivariate normal inverse Gaussian (MNIG) distributions can be\nused to cluster data that exhibit features such as skewness and heavy tails.\nHowever, for cluster analysis, using a traditional finite mixture model\nframework, either the number of components needs to be known $a$-$priori$ or\nneeds to be estimated $a$-$posteriori$ using some model selection criterion\nafter deriving results for a range of possible number of components. However,\ndifferent model selection criteria can sometimes result in different number of\ncomponents yielding uncertainty. Here, an infinite mixture model framework,\nalso known as Dirichlet process mixture model, is proposed for the mixtures of\nMNIG distributions. This Dirichlet process mixture model approach allows the\nnumber of components to grow or decay freely from 1 to $\\infty$ (in practice\nfrom 1 to $N$) and the number of components is inferred along with the\nparameter estimates in a Bayesian framework thus alleviating the need for model\nselection criteria. We provide real data applications with benchmark datasets\nas well as a small simulation experiment to compare with other existing models.\nThe proposed method provides competitive clustering results to other clustering\napproaches for both simulation and real data and parameter recovery are\nillustrated using simulation studies.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:08:27 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Fang", "Yuan", ""], ["Karlis", "Dimitris", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "2005.05452", "submitter": "Paul Smith", "authors": "Peter G M van der Heijden and Paul A. Smith", "title": "On estimating the size of overcoverage with the latent class model. A\n  critique of the paper \"Population Size Estimation Using Multiple Incomplete\n  Lists with Overcoverage\" by di Cecco, di Zio, Filipponi and Rocchetti (2018,\n  JOS 34 557-572)", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We read with interest the article by di Cecco et al. (2018), but have\nreservations about the usefulness of the latent class model specifically for\nestimating overcoverage. In particular, we question the interpretation of the\nparameters of the fitted latent class model.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:40:59 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["van der Heijden", "Peter G M", ""], ["Smith", "Paul A.", ""]]}, {"id": "2005.05464", "submitter": "Marcos Prates O", "authors": "Douglas R. M. Azevedo and Marcos O. Prates and Michael R. Willig", "title": "Non-Separable Spatio-temporal Models via Transformed Gaussian Markov\n  Random Fields", "comments": "15 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that capture the spatial and temporal dynamics are applicable in many\nscience fields. Non-separable spatio-temporal models were introduced in the\nliterature to capture these features. However, these models are generally\ncomplicated in construction and interpretation. We introduce a class of\nnon-separable Transformed Gaussian Markov Random Fields (TGMRF) in which the\ndependence structure is flexible and facilitates simple interpretations\nconcerning spatial, temporal and spatio-temporal parameters. Moreover, TGMRF\nmodels have the advantage of allowing specialists to define any desired\nmarginal distribution in model construction without suffering from\nspatio-temporal confounding. Consequently, the use of spatio-temporal models\nunder the TGMRF framework leads to a new class of general models, such as\nspatio-temporal Gamma random fields, that can be directly used to model Poisson\nintensity for space-time data. The proposed model was applied to identify\nimportant environmental characteristics that affect variation in the abundance\nof Nenia tridens, a dominant species of snail in a well-studied tropical\necosystem, and to characterize its spatial and temporal trends, which are\nparticularly critical during the Anthropocene, an epoch of time characterized\nby human-induced environmental change associated with climate and land use.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 22:07:35 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Azevedo", "Douglas R. M.", ""], ["Prates", "Marcos O.", ""], ["Willig", "Michael R.", ""]]}, {"id": "2005.05506", "submitter": "Eugene Katsevich", "authors": "Eugene Katsevich and Aaditya Ramdas", "title": "On the power of conditional independence testing under model-X", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For testing conditional independence (CI) of a response Y and a predictor X\ngiven covariates Z, the recently introduced model-X (MX) framework has been the\nsubject of active methodological research, especially in the context of MX\nknockoffs and their successful application to genome-wide association studies.\nIn this paper, we study the power of MX CI tests, yielding quantitative\nexplanations for empirically observed phenomena and novel insights to guide the\ndesign of MX methodology. We show that any valid MX CI test must also be valid\nconditionally on Y and Z; this conditioning allows us to reformulate the\nproblem as testing a point null hypothesis involving the conditional\ndistribution of X. The Neyman-Pearson lemma then implies that the conditional\nrandomization test (CRT) based on a likelihood statistic is the most powerful\nMX CI test against a point alternative. We also obtain a related optimality\nresult for MX knockoffs. Switching to an asymptotic framework with arbitrarily\ngrowing covariate dimension, we derive an expression for the limiting power of\nthe CRT against local semiparametric alternatives in terms of the prediction\nerror of the machine learning algorithm on which its test statistic is based.\nFinally, we exhibit a resampling-free test with uniform asymptotic Type-I error\ncontrol under the assumption that only the first two moments of X given Z are\nknown, a significant relaxation of the MX assumption.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:24:25 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 16:06:32 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 20:08:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Katsevich", "Eugene", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2005.05511", "submitter": "Kyunghee Han", "authors": "Kyunghee Han, Thomas Lumley, Bryan E. Shepherd, Pamela A. Shaw", "title": "Two-phase analysis and study design for survival models with error-prone\n  exposures", "comments": "22 pages, 2 figures, 3 tables, supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly, medical research is dependent on data collected for\nnon-research purposes, such as electronic health records data (EHR). EHR data\nand other large databases can be prone to measurement error in key exposures,\nand unadjusted analyses of error-prone data can bias study results. Validating\na subset of records is a cost-effective way of gaining information on the error\nstructure, which in turn can be used to adjust analyses for this error and\nimprove inference. We extend the mean score method for the two-phase analysis\nof discrete-time survival models, which uses the unvalidated covariates as\nauxiliary variables that act as surrogates for the unobserved true exposures.\nThis method relies on a two-phase sampling design and an estimation approach\nthat preserves the consistency of complete case regression parameter estimates\nin the validated subset, with increased precision leveraged from the auxiliary\ndata. Furthermore, we develop optimal sampling strategies which minimize the\nvariance of the mean score estimator for a target exposure under a fixed cost\nconstraint. We consider the setting where an internal pilot is necessary for\nthe optimal design so that the phase two sample is split into a pilot and an\nadaptive optimal sample. Through simulations and data example, we evaluate\nefficiency gains of the mean score estimator using the derived optimal\nvalidation design compared to balanced and simple random sampling for the phase\ntwo sample. We also empirically explore efficiency gains that the proposed\ndiscrete optimal design can provide for the Cox proportional hazards model in\nthe setting of a continuous-time survival outcome.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:47:43 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Han", "Kyunghee", ""], ["Lumley", "Thomas", ""], ["Shepherd", "Bryan E.", ""], ["Shaw", "Pamela A.", ""]]}, {"id": "2005.05628", "submitter": "Claire Boyer", "authors": "Pascaline Descloux (UNIGE), Claire Boyer (LPSM UMR 8001), Julie Josse\n  (CMAP), Aude Sportisse (LPSM (UMR\\_8001)), Sylvain Sardy", "title": "Robust Lasso-Zero for sparse corruption and model selection with missing\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Robust Lasso-Zero, an extension of the Lasso-Zero methodology\n[Descloux and Sardy, 2018], initially introduced for sparse linear models, to\nthe sparse corruptions problem. We give theoretical guarantees on the sign\nrecovery of the parameters for a slightly simplified version of the estimator,\ncalled Thresholded Justice Pursuit. The use of Robust Lasso-Zero is showcased\nfor variable selection with missing values in the covariates. In addition to\nnot requiring the specification of a model for the covariates, nor estimating\ntheir covariance matrix or the noise variance, the method has the great\nadvantage of handling missing not-at random values without specifying a\nparametric model. Numerical experiments and a medical application underline the\nrelevance of Robust Lasso-Zero in such a context with few available\ncompetitors. The method is easy to use and implemented in the R library lass0.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 09:05:46 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Descloux", "Pascaline", "", "UNIGE"], ["Boyer", "Claire", "", "LPSM UMR 8001"], ["Josse", "Julie", "", "CMAP"], ["Sportisse", "Aude", "", "LPSM"], ["Sardy", "Sylvain", ""]]}, {"id": "2005.05808", "submitter": "Eva Cantoni", "authors": "Setareh Ranjbar, Eva Cantoni, Val\\'erie Chavez-Demoulin, Giampiero\n  Marra, Rosalba Radice, Katia Jaton-Ogay", "title": "Modelling the Extremes of Seasonal Viruses and Hospital Congestion: The\n  Example of Flu in a Swiss Hospital", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viruses causing flu or milder coronavirus colds are often referred to as\n\"seasonal viruses\" as they tend to subside in warmer months. In other words,\nmeteorological conditions tend to impact the activity of viruses, and this\ninformation can be exploited for the operational management of hospitals. In\nthis study, we use three years of daily data from one of the biggest hospitals\nin Switzerland and focus on modelling the extremes of hospital visits from\npatients showing flu-like symptoms and the number of positive cases of flu. We\npropose employing a discrete Generalized Pareto distribution for the number of\npositive and negative cases, and a Generalized Pareto distribution for the odds\nof positive cases. Our modelling framework allows for the parameters of these\ndistributions to be linked to covariate effects, and for outlying observations\nto be dealt with via a robust estimation approach. Because meteorological\nconditions may vary over time, we use meteorological and not calendar\nvariations to explain hospital charge extremes, and our empirical findings\nhighlight their significance. We propose a measure of hospital congestion and a\nrelated tool to estimate the resulting CaRe (Charge-at-Risk-estimation) under\ndifferent meteorological conditions. The relevant numerical computations can be\neasily carried out using the freely available GJRM R package. The introduced\napproach could be applied to several types of seasonal disease data such as\nthose derived from the new virus SARS-CoV-2 and its COVID-19 disease which is\nat the moment wreaking havoc worldwide. The empirical effectiveness of the\nproposed method is assessed through a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:21:02 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Ranjbar", "Setareh", ""], ["Cantoni", "Eva", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""], ["Marra", "Giampiero", ""], ["Radice", "Rosalba", ""], ["Jaton-Ogay", "Katia", ""]]}, {"id": "2005.05880", "submitter": "Susan Murphy A", "authors": "Ashley E. Walton, Linda M. Collins, Predrag Klasnja, Inbal\n  Nahum-Shani, Mashfiqui Rabbi, Maureen A. Walton, Susan A. Murphy", "title": "The Micro-Randomized Trial for Developing Digital Interventions:\n  Experimental Design Considerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just-in-time adaptive interventions (JITAIs) are time-varying adaptive\ninterventions that use frequent opportunities for the intervention to be\nadapted such as weekly, daily, or even many times a day. This high intensity of\nadaptation is facilitated by the ability of digital technology to continuously\ncollect information about an individual's current context and deliver\ntreatments adapted to this information. The micro-randomized trial (MRT) has\nemerged for use in informing the construction of JITAIs. MRTs operate in, and\ntake advantage of, the rapidly time-varying digital intervention environment.\nMRTs can be used to address research questions about whether and under what\ncircumstances particular components of a JITAI are effective, with the ultimate\nobjective of developing effective and efficient components. The purpose of this\narticle is to clarify why, when, and how to use MRTs; to highlight elements\nthat must be considered when designing and implementing an MRT; and to discuss\nthe possibilities this emerging optimization trial design offers for future\nresearch in the behavioral sciences, education, and other fields. We briefly\nreview key elements of JITAIs, and then describe three case studies of MRTs,\neach of which highlights research questions that can be addressed using the MRT\nand experimental design considerations that might arise. We also discuss a\nvariety of considerations that go into planning and designing an MRT, using the\ncase studies as examples.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:41:00 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Walton", "Ashley E.", ""], ["Collins", "Linda M.", ""], ["Klasnja", "Predrag", ""], ["Nahum-Shani", "Inbal", ""], ["Rabbi", "Mashfiqui", ""], ["Walton", "Maureen A.", ""], ["Murphy", "Susan A.", ""]]}, {"id": "2005.06006", "submitter": "Jeffrey N\\\"af", "authors": "Loris Michel and Jeffrey N\\\"af", "title": "High Probability Lower Bounds for the Total Variation Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistics and machine learning communities have recently seen a growing\ninterest in classification-based approaches to two-sample testing. The outcome\nof a classification-based two-sample test remains a rejection decision, which\nis not always informative since the null hypothesis is seldom strictly true.\nTherefore, when a test rejects, it would be beneficial to provide an additional\nquantity serving as a refined measure of distributional difference. In this\nwork, we introduce a framework for the construction of high-probability lower\nbounds on the total variation distance. These bounds are based on a\none-dimensional projection, such as a classification or regression method, and\ncan be interpreted as the minimal fraction of samples pointing towards a\ndistributional difference. We further derive asymptotic power and detection\nrates of two proposed estimators and discuss potential uses through an\napplication to a reanalysis climate dataset\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 18:55:05 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 10:38:44 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 17:04:46 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2021 17:19:29 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Michel", "Loris", ""], ["N\u00e4f", "Jeffrey", ""]]}, {"id": "2005.06095", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla", "title": "Exchangeability, Conformal Prediction, and Rank Tests", "comments": "Added reference to Shi et al. (2020, JASA). Corrected Theorem 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction has been a very popular method of distribution-free\npredictive inference in recent years in machine learning and statistics. Its\npopularity stems from the fact that it works as a wrapper around any prediction\nalgorithm such as neural networks or random forests. Exchangeability is at the\ncore of the validity of conformal prediction. The concept of exchangeability is\nalso at the core of rank tests widely known in nonparametric statistics. In\nthis paper, we review the concept of exchangeability and discuss the\nimplications for conformal prediction and rank tests. We provide a low-level\nintroduction to these topics, and discuss the similarities between conformal\nprediction and rank tests.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 00:39:45 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 16:12:48 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 01:35:40 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "2005.06171", "submitter": "Steven Pav", "authors": "Steven E. Pav", "title": "Inference on Achieved Signal Noise Ratio", "comments": "v2 adds analysis for hedged portfolios", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a procedure to perform approximate inference on the achieved\nsignal-noise ratio of the Markowitz Portfolio under Gaussian i.i.d. returns.\nThe procedure relies on a statistic similar to the Sharpe Ratio Information\nCriterion. Testing indicates the procedure is somewhat conservative, but\notherwise works well for reasonable values of sample and asset universe sizes.\nWe adapt the procedure to deal with generalizations of the portfolio\noptimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 06:24:17 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 18:36:05 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Pav", "Steven E.", ""]]}, {"id": "2005.06413", "submitter": "Louis Abraham", "authors": "Louis Abraham, Gary B\\'ecigneul, Bernhard Sch\\\"olkopf", "title": "Crackovid: Optimizing Group Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem usually referred to as group testing in the context of\nCOVID-19. Given $n$ samples taken from patients, how should we select mixtures\nof samples to be tested, so as to maximize information and minimize the number\nof tests? We consider both adaptive and non-adaptive strategies, and take a\nBayesian approach with a prior both for infection of patients and test errors.\nWe start by proposing a mathematically principled objective, grounded in\ninformation theory. We then optimize non-adaptive optimization strategies using\ngenetic algorithms, and leverage the mathematical framework of adaptive\nsub-modularity to obtain theoretical guarantees for the greedy-adaptive method.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:40:09 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Abraham", "Louis", ""], ["B\u00e9cigneul", "Gary", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2005.06633", "submitter": "Beste Hamiye Beyaztas", "authors": "Beste Hamiye Beyaztas and Soutir Bandyopadhyay", "title": "Robust Estimation for Linear Panel Data Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In different fields of applications including, but not limited to,\nbehavioral, environmental, medical sciences and econometrics, the use of panel\ndata regression models has become increasingly popular as a general framework\nfor making meaningful statistical inferences. However, when the ordinary least\nsquares (OLS) method is used to estimate the model parameters, presence of\noutliers may significantly alter the adequacy of such models by producing\nbiased and inefficient estimates. In this work we propose a new, weighted\nlikelihood based robust estimation procedure for linear panel data models with\nfixed and random effects. The finite sample performances of the proposed\nestimators have been illustrated through an extensive simulation study as well\nas with an application to blood pressure data set. Our thorough study\ndemonstrates that the proposed estimators show significantly better\nperformances over the traditional methods in the presence of outliers and\nproduce competitive results to the OLS based estimates when no outliers are\npresent in the data set.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 21:45:32 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Beyaztas", "Beste Hamiye", ""], ["Bandyopadhyay", "Soutir", ""]]}, {"id": "2005.06709", "submitter": "Dylan Small", "authors": "Bikram Karmakar and Dylan Small", "title": "Inference for a test-negative case-control study with added controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Test-negative designs with added controls have recently been proposed to\nstudy COVID-19. An individual is test-positive or test-negative accordingly if\nthey took a test for a disease but tested positive or tested negative. Adding a\ncontrol group to a comparison of test-positives vs test-negatives is useful\nsince additional comparison of test-positives vs controls can have potential\nbiases different from the first comparison. Bonferroni correction ensures\nnecessary type-I error control for these two comparisons done simultaneously.\nWe propose two new methods for inference which have better interpretability and\nhigher statistical power for these designs. These methods add a third\ncomparison that is essentially independent of the first comparison, but our\nproposed second method often pays much less for these three comparisons than\nwhat a Bonferroni correction would pay for the two comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 03:42:37 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Karmakar", "Bikram", ""], ["Small", "Dylan", ""]]}, {"id": "2005.06776", "submitter": "Bastien Mallein", "authors": "Vincent Brault, Bastien Mallein and Jean-Francois Rupprecht", "title": "Group testing as a strategy for the epidemiologic monitoring of COVID-19", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008726", "report-no": null, "categories": "q-bio.QM physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample pooling consists in combining samples from multiple individuals into a\nsingle pool that is then tested using a unique test-kit. A positive test means\nthat at least one individual within the pool is infected. Here, we propose an\nanalysis and applications of sample pooling to the epidemiologic monitoring of\nCOVID-19. We first introduce a model of the RT-qPCR process used to test for\nthe presence of virus in a sample and construct a statistical model for the\nviral load in a typical infected individual inspired by the clinical data from\nJones et. al. (2020). We then propose a method for the measure of the\nprevalence in a population, based on group testing, taking into account the\nincreased number of false negatives associated with this method. Finally, we\npresent an application of sample pooling for the prevention of epidemic\noutbreak in closed connected communities (e.g. nursing homes).\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 07:47:35 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 14:09:44 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 14:16:14 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Brault", "Vincent", ""], ["Mallein", "Bastien", ""], ["Rupprecht", "Jean-Francois", ""]]}, {"id": "2005.06819", "submitter": "Willem van den Boom", "authors": "Willem van den Boom, Marta Tallarita and Maria De Iorio", "title": "Bayesian Joint Modelling of Recurrence and Survival: a Conditional\n  Approach", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent event processes describe the stochastic repetition of an event over\ntime. Recurrent event times are often censored with dependence between the\ncensoring time and recurrence process. For instance, recurrent disease events\nare censored by a terminal event such as death, while frailty might affect both\ndisease recurrence and survival. As such, it is important to model the\nrecurrent event process and the event time process jointly to better capture\nthe dependency between them and improve interpretability of the results. We\npropose a model in which the number of gap times, i.e. the time between two\nconsecutive recurrent events, before the terminal event occurs is a random\nvariable of interest. Then, conditionally on the number of recurrent events, we\nspecify a joint distribution for the gap times and the survival time.\nDependence between the the recurrence and survival processes is introduced by\nspecifying a joint distribution on their respective frailty terms. Moreover, an\nautoregressive model of order one is assumed to model the evolution of gap\ntimes over time. A non-parametric random effects distribution for the frailty\nterms accommodates population heterogeneity and allows for data-driven\nclustering of the subjects. Posterior inference is performed through a a Gibbs\nsampler strategy involving a reversible jump step and slice sampling. We\nillustrate our model on atrial fibrillation data and compare the performance of\nour model with existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 08:59:01 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Boom", "Willem van den", ""], ["Tallarita", "Marta", ""], ["De Iorio", "Maria", ""]]}, {"id": "2005.06889", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Arthur Pewsey, Eduardo Garc\\'ia-Portugu\\'es", "title": "Recent advances in directional statistics", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mainstream statistical methodology is generally applicable to data observed\nin Euclidean space. There are, however, numerous contexts of considerable\nscientific interest in which the natural supports for the data under\nconsideration are Riemannian manifolds like the unit circle, torus, sphere and\ntheir extensions. Typically, such data can be represented using one or more\ndirections, and directional statistics is the branch of statistics that deals\nwith their analysis. In this paper we provide a review of the many recent\ndevelopments in the field since the publication of Mardia and Jupp (1999),\nstill the most comprehensive text on directional statistics. Many of those\ndevelopments have been stimulated by interesting applications in fields as\ndiverse as astronomy, medicine, genetics, neurology, aeronautics, acoustics,\nimage analysis, text mining, environmetrics, and machine learning. We begin by\nconsidering developments for the exploratory analysis of directional data\nbefore progressing to distributional models, general approaches to inference,\nhypothesis testing, regression, nonparametric curve estimation, methods for\ndimension reduction, classification and clustering, and the modelling of time\nseries, spatial and spatio-temporal data. An overview of currently available\nsoftware for analysing directional data is also provided, and potential future\ndevelopments discussed.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 11:50:11 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:31:45 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 07:49:02 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Pewsey", "Arthur", ""], ["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""]]}, {"id": "2005.07142", "submitter": "Michail Katsoulis", "authors": "Michail Katsoulis, Christina Bamia", "title": "Moving from two- to multi-way interactions among binary risk factors on\n  the additive scale", "comments": "14 pages (main text), 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have focused on investigating deviations from additive\ninteraction of two dichotomous risk factors on a binary outcome. There is,\nhowever, a gap in the literature with respect to interactions on the additive\nscale of >2 risk factors. In this paper, we present an approach for examining\ndeviations from additive interaction among three on more binary exposures. The\nrelative excess risk due to interaction (RERI) is used as measure of additive\ninteraction. First, we concentrate on three risk factors - we propose to\ndecompose the total RERI to: the RERI owned to the joint presence of all 3 risk\nfactors and the RERI of any two risk factors, given that the third is absent.\nWe then extend this approach, to >3 binary risk factors. For illustration, we\nuse a sample from data from the Greek EPIC cohort and we investigate the\nassociation with overall mortality of Mediterranean diet, body mass index\n(BMI), and, smoking. Our formulae enable better interpretability of any\nevidence for deviations from additivity owned to more than two risk factors and\nprovide simple ways of communicating such results from a public health\nperspective by attributing any excess relative risk to specific combinations of\nthese factors.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:01:37 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Katsoulis", "Michail", ""], ["Bamia", "Christina", ""]]}, {"id": "2005.07180", "submitter": "Julius von K\\\"ugelgen", "authors": "Julius von K\\\"ugelgen, Luigi Gresele, Bernhard Sch\\\"olkopf", "title": "Simpson's paradox in Covid-19 case fatality rates: a mediation analysis\n  of age-related causal effects", "comments": "Journal version with full Appendix. The first two authors contributed\n  equally to this work", "journal-ref": "IEEE Transactions on Artificial Intelligence, vol. 2, no. 1, pp.\n  18-27, Feb. 2021", "doi": "10.1109/TAI.2021.3073088", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We point out an instantiation of Simpson's paradox in Covid-19 case fatality\nrates (CFRs): comparing a large-scale study from China (17 Feb) with early\nreports from Italy (9 Mar), we find that CFRs are lower in Italy for every age\ngroup, but higher overall. This phenomenon is explained by a stark difference\nin case demographic between the two countries. Using this as a motivating\nexample, we introduce basic concepts from mediation analysis and show how these\ncan be used to quantify different direct and indirect effects when assuming a\ncoarse-grained causal graph involving country, age, and case fatality. We\ncurate an age-stratified CFR dataset with >750k cases and conduct a case study,\ninvestigating total, direct, and indirect (age-mediated) causal effects between\ndifferent countries and at different points in time. This allows us to separate\nage-related effects from others unrelated to age and facilitates a more\ntransparent comparison of CFRs across countries at different stages of the\nCovid-19 pandemic. Using longitudinal data from Italy, we discover a sign\nreversal of the direct causal effect in mid-March which temporally aligns with\nthe reported collapse of the healthcare system in parts of the country.\nMoreover, we find that direct and indirect effects across 132 pairs of\ncountries are only weakly correlated, suggesting that a country's policy and\ncase demographic may be largely unrelated. We point out limitations and\nextensions for future work, and, finally, discuss the role of causal reasoning\nin the broader context of using AI to combat the Covid-19 pandemic.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:52:10 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 18:26:14 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 11:40:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["von K\u00fcgelgen", "Julius", ""], ["Gresele", "Luigi", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2005.07314", "submitter": "Olli Saarela", "authors": "Bo Chen, Olli Saarela", "title": "Hierarchical causal variance decomposition for institution and provider\n  comparisons in healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease-specific quality indicators (QIs) are used to compare institutions\nand health care providers in terms processes or outcomes relevant to treatment\nof a particular condition. In the context of surgical cancer treatments, the\nperformance variations can be due to hospital and/or surgeon level differences,\ncreating a hierarchical clustering. We consider how the observed variation in\ncare received at patient level can be decomposed into that causally explained\nby the hospital performance, surgeon performance within hospital, patient\ncase-mix, and unexplained (residual) variation. For this purpose, we derive a\nfour-way variance decomposition, with particular attention to the causal\ninterpretation of the components. For estimation, we use inputs from a\nmixed-effect model with nested random hospital/surgeon-specific effects, and a\nmultinomial logistic model for the hospital/surgeon-specific patient\npopulations. We investigate the performance of our methods in a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 01:19:40 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 01:13:36 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Chen", "Bo", ""], ["Saarela", "Olli", ""]]}, {"id": "2005.07322", "submitter": "Olli Saarela", "authors": "Sudipta Saha, Zhihui Liu, Olli Saarela", "title": "Instrumental variable estimation of early treatment effect in randomized\n  screening trials", "comments": "Lifetime Data Anal (2021)", "journal-ref": null, "doi": "10.1007/s10985-021-09527-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary analysis of randomized screening trials for cancer typically\nadheres to the intention-to-screen principle, measuring cancer-specific\nmortality reductions between screening and control arms. These mortality\nreductions result from a combination of the screening regimen, screening\ntechnology and the effect of the early, screening-induced, treatment. This\nmotivates addressing these different aspects separately. Here we are interested\nin the causal effect of early versus delayed treatments on cancer mortality\namong the screening-detectable subgroup, which under certain assumptions is\nestimable from conventional randomized screening trial using instrumental\nvariable type methods. To define the causal effect of interest, we formulate a\nsimplified structural multi-state model for screening trials, based on a\nhypothetical intervention trial where screening detected individuals would be\nrandomized into early versus delayed treatments. The cancer-specific mortality\nreductions after screening detection are quantified by a cause-specific hazard\nratio. For this, we propose two estimators, based on an estimating equation and\na likelihood expression. The methods extend existing instrumental variable\nmethods for time-to-event and competing risks outcomes to time-dependent\nintermediate variables. Using the multi-state model as the basis of a data\ngenerating mechanism, we investigate the performance of the new estimators\nthrough simulation studies. In addition, we illustrate the proposed method in\nthe context of CT screening for lung cancer using the US National Lung\nScreening Trial (NLST) data.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 01:52:08 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:56:52 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 03:08:57 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 17:45:59 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Saha", "Sudipta", ""], ["Liu", "Zhihui", ""], ["Saarela", "Olli", ""]]}, {"id": "2005.07342", "submitter": "Kean Ming Tan", "authors": "Jiaying Zhou, Jie Ding, Kean Ming Tan, Vahid Tarokh", "title": "Model Linkage Selection for Cooperative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid developments in data collecting devices and computation platforms\nproduce an emerging number of learners and data modalities in many scientific\ndomains. We consider the setting in which each learner holds a pair of\nparametric statistical model and a specific data source, with the goal of\nintegrating information across a set of learners to enhance the prediction\naccuracy of a specific learner. One natural way to integrate information is to\nbuild a joint model across a set of learners that shares common parameters of\ninterest. However, the parameter sharing patterns across a set of learners are\nnot known a priori. Misspecifying the parameter sharing patterns and the\nparametric statistical model for each learner yields a biased estimator and\ndegrades the prediction accuracy of the joint model. In this paper, we propose\na novel framework for integrating information across a set of learners that is\nrobust against model misspecification and misspecified parameter sharing\npatterns. The main crux is to sequentially incorporates additional learners\nthat can enhance the prediction accuracy of an existing joint model based on a\nuser-specified parameter sharing patterns across a set of learners, starting\nfrom a model with one learner. Theoretically, we show that the proposed method\ncan data-adaptively select the correct parameter sharing patterns based on a\nuser-specified parameter sharing patterns, and thus enhances the prediction\naccuracy of a learner. Extensive numerical studies are performed to evaluate\nthe performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 03:44:01 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 19:09:54 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhou", "Jiaying", ""], ["Ding", "Jie", ""], ["Tan", "Kean Ming", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2005.07380", "submitter": "Bruno Sudret", "authors": "P.-R. Wagner, S. Marelli and B. Sudret", "title": "Bayesian model inversion using stochastic spectral embedding", "comments": null, "journal-ref": "Journal of Computational Physics, Vol. 436, 110141 (2021)", "doi": "10.1016/j.jcp.2021.110141", "report-no": "RSUQ-2020-005B", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new sampling-free approach to solve Bayesian model\ninversion problems that is an extension of the previously proposed spectral\nlikelihood expansions (SLE) method. Our approach, called stochastic spectral\nlikelihood embedding (SSLE), uses the recently presented stochastic spectral\nembedding (SSE) method for local spectral expansion refinement to approximate\nthe likelihood function at the core of Bayesian inversion problems. We show\nthat, similar to SLE, this approach results in analytical expressions for key\nstatistics of the Bayesian posterior distribution, such as evidence, posterior\nmoments and posterior marginals, by direct post-processing of the expansion\ncoefficients. Because SSLE and SSE rely on the direct approximation of the\nlikelihood function, they are in a way independent of the\ncomputational/mathematical complexity of the forward model. We further enhance\nthe efficiency of SSLE by introducing a likelihood specific adaptive sample\nenrichment scheme. To showcase the performance of the proposed SSLE, we solve\nthree problems that exhibit different kinds of complexity in the likelihood\nfunction: multimodality, high posterior concentration and high nominal\ndimensionality. We demonstrate how SSLE significantly improves on SLE, and\npresent it as a promising alternative to existing inversion frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 07:06:55 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:22:44 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wagner", "P. -R.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "2005.07430", "submitter": "Ruben Loaiza-Maya", "authors": "Rub\\'en Loaiza-Maya, Michael Stanley Smith, David J. Nott and Peter J.\n  Danaher", "title": "Fast and Accurate Variational Inference for Models with Many Latent\n  Variables", "comments": "Macroeconomic example was replaced by the bigger and more challenging\n  time varying parameter vector autoregression model with stochastic\n  volatility. Microeconomic example was extended to 20,000 individuals and\n  variational subsampling is also implemented for this example. Small\n  microeconomics example now uses 1000 individuals", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with a large number of latent variables are often used to fully\nutilize the information in big or complex data. However, they can be difficult\nto estimate using standard approaches, and variational inference methods are a\npopular alternative. Key to the success of these is the selection of an\napproximation to the target density that is accurate, tractable and fast to\ncalibrate using optimization methods. Most existing choices can be inaccurate\nor slow to calibrate when there are many latent variables. Here, we propose a\nfamily of tractable variational approximations that are more accurate and\nfaster to calibrate for this case. It combines a parsimonious parametric\napproximation for the parameter posterior, with the exact conditional posterior\nof the latent variables. We derive a simplified expression for the\nre-parameterization gradient of the variational lower bound, which is the main\ningredient of efficient optimization algorithms used to implement variational\nestimation. To do so only requires the ability to generate exactly or\napproximately from the conditional posterior of the latent variables, rather\nthan to compute its density. We illustrate using two complex contemporary\neconometric examples. The first is a nonlinear multivariate state space model\nfor U.S. macroeconomic variables. The second is a random coefficients tobit\nmodel applied to two million sales by 20,000 individuals in a large consumer\npanel from a marketing study. In both cases, we show that our approximating\nfamily is considerably more accurate than mean field or structured Gaussian\napproximations, and faster than Markov chain Monte Carlo. Last, we show how to\nimplement data sub-sampling in variational inference for our approximation,\nwhich can lead to a further reduction in computation time. MATLAB code\nimplementing the method for our examples is included in supplementary material.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:24:28 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 06:36:59 GMT"}, {"version": "v3", "created": "Sun, 18 Apr 2021 22:13:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Loaiza-Maya", "Rub\u00e9n", ""], ["Smith", "Michael Stanley", ""], ["Nott", "David J.", ""], ["Danaher", "Peter J.", ""]]}, {"id": "2005.07484", "submitter": "Michael Kammer", "authors": "Michael Kammer, Daniela Dunkler, Stefan Michiels, Georg Heinze", "title": "Evaluating methods for Lasso selective inference in biomedical research\n  by a comparative simulation study", "comments": "42 pages, 19 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for regression models plays a key role in the analysis of\nbiomedical data. However, inference after selection is not covered by classical\nstatistical frequentist theory which assumes a fixed set of covariates in the\nmodel. We review two interpretations of inference after selection: the full\nmodel view, in which the parameters of interest are those of the full model on\nall predictors, and then focus on the submodel view, in which the parameters of\ninterest are those of the selected model only. In the context of L1-penalized\nregression we compare proposals for submodel inference (selective inference)\nvia confidence intervals available to applied researchers via software packages\nusing a simulation study inspired by real data commonly seen in biomedical\nstudies. Furthermore, we present an exemplary application of these methods to a\npublicly available dataset to discuss their practical usability. Our findings\nindicate that the frequentist properties of selective confidence intervals are\ngenerally acceptable, but desired coverage levels are not guaranteed in all\nscenarios except for the most conservative methods. The choice of inference\nmethod potentially has a large impact on the resulting interval estimates,\nthereby necessitating that the user is acutely aware of the goal of inference\nin order to interpret and communicate the results. Currently available software\npackages are not yet very user friendly or robust which might affect their use\nin practice. In summary, we find submodel inference after selection useful for\nexperienced statisticians to assess the importance of individual selected\npredictors in future applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 12:09:34 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 10:43:42 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Kammer", "Michael", ""], ["Dunkler", "Daniela", ""], ["Michiels", "Stefan", ""], ["Heinze", "Georg", ""]]}, {"id": "2005.07573", "submitter": "Meagan Carney", "authors": "Meagan Carney, Holger Kantz, Matthew Nicol", "title": "Analysis and Simulation of Extremes and Rare Events in Complex Systems", "comments": "32 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rare weather and climate events, such as heat waves and floods, can bring\ntremendous social costs. Climate data is often limited in duration and spatial\ncoverage, and climate forecasting has often turned to simulations of climate\nmodels to make better predictions of rare weather events. However very long\nsimulations of complex models, in order to obtain accurate probability\nestimates, may be prohibitively slow. It is an important scientific problem to\ndevelop probabilistic and dynamical techniques to estimate the probabilities of\nrare events accurately from limited data. In this paper we compare four modern\nmethods of estimating the probability of rare events: the generalized extreme\nvalue (GEV) method from classical extreme value theory; two importance sampling\ntechniques, genealogical particle analysis (GPA) and the\nGiardina-Kurchan-Lecomte-Tailleur (GKLT) algorithm; as well as brute force\nMonte Carlo (MC). With these techniques we estimate the probabilities of rare\nevents in three dynamical models: the Ornstein-Uhlenbeck process, the Lorenz\n'96 system and PlaSim (a climate model). We keep the computational effort\nconstant and see how well the rare event probability estimation of each\ntechnique compares to a gold standard afforded by a very long run control.\nSomewhat surprisingly we find that classical extreme value theory methods\noutperform GPA, GKLT and MC at estimating rare events.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:44:53 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Carney", "Meagan", ""], ["Kantz", "Holger", ""], ["Nicol", "Matthew", ""]]}, {"id": "2005.07587", "submitter": "Yufei Yi", "authors": "Yufei Yi, Matey Neykov", "title": "Non-Sparse PCA in High Dimensions via Cone Projected Power Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a cone projected power iteration algorithm to\nrecover the first principal eigenvector from a noisy positive semidefinite\nmatrix. When the true principal eigenvector is assumed to belong to a convex\ncone, the proposed algorithm is fast and has a tractable error. Specifically,\nthe method achieves polynomial time complexity for certain convex cones\nequipped with fast projection such as the monotone cone. It attains a small\nerror when the noisy matrix has a small cone-restricted operator norm. We\nsupplement the above results with a minimax lower bound of the error under the\nspiked covariance model. Our numerical experiments on simulated and real data,\nshow that our method achieves shorter run time and smaller error in comparison\nto the ordinary power iteration and some sparse principal component analysis\nalgorithms if the principal eigenvector is in a convex cone.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 15:02:24 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 17:22:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yi", "Yufei", ""], ["Neykov", "Matey", ""]]}, {"id": "2005.07742", "submitter": "Daniel Eck", "authors": "Charles Young, David Dalpiaz, Daniel J. Eck", "title": "SEAM methodology for context-rich player matchup evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the SEAM (synthetic estimated average matchup) method for\ndescribing batter versus pitcher matchups in baseball, both numerically and\nvisually. We first estimate the distribution of balls put into play by a batter\nfacing a pitcher, called the spray chart distribution. This distribution is\nconditional on batter and pitcher characteristics. These characteristics are a\nbetter expression of talent than any conventional statistics. Many individual\nmatchups have a sample size that is too small to be reliable. Synthetic\nversions of the batter and pitcher under consideration are constructed in order\nto alleviate these concerns. Weights governing how much influence these\nsynthetic players have on the overall spray chart distribution are constructed\nto minimize expected mean square error. We then provide novel performance\nmetrics that are calculated as expectations taken with respect to the spray\nchart distribution. These performance metrics provide a context rich approach\nto player evaluation. Our main contribution is a Shiny app that allows users to\nevaluate any batter-pitcher matchup that has occurred or could have occurred in\nthe last five years. One can access this app at\n\\url{https://seam.stat.illinois.edu/app/}. This interactive tool has utility\nfor anyone interested in baseball as well as team executives and players.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 18:58:46 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Young", "Charles", ""], ["Dalpiaz", "David", ""], ["Eck", "Daniel J.", ""]]}, {"id": "2005.07972", "submitter": "Matteo Fontana", "authors": "Gianluca Zeni and Matteo Fontana and Simone Vantini", "title": "Conformal Prediction: a Unified Review of Theory and New Challenges", "comments": "arXiv admin note: text overlap with arXiv:0706.3188,\n  arXiv:1604.04173, arXiv:1709.06233, arXiv:1203.5422 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we provide a review of basic ideas and novel developments about\nConformal Prediction -- an innovative distribution-free, non-parametric\nforecasting method, based on minimal assumptions -- that is able to yield in a\nvery straightforward way predictions sets that are valid in a statistical sense\nalso in in the finite sample case. The in-depth discussion provided in the\npaper covers the theoretical underpinnings of Conformal Prediction, and then\nproceeds to list the more advanced developments and adaptations of the original\nidea.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 12:38:19 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zeni", "Gianluca", ""], ["Fontana", "Matteo", ""], ["Vantini", "Simone", ""]]}, {"id": "2005.08049", "submitter": "Hyungsuk Tak", "authors": "Zhirui Hu and Hyungsuk Tak", "title": "Modeling Stochastic Variability in Multi-Band Time Series Data", "comments": null, "journal-ref": null, "doi": "10.3847/1538-3881/abc1e2", "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In preparation for the era of the time-domain astronomy with upcoming\nlarge-scale surveys, we propose a state-space representation of a multivariate\ndamped random walk process as a tool to analyze irregularly-spaced multi-filter\nlight curves with heteroscedastic measurement errors. We adopt a\ncomputationally efficient and scalable Kalman-filtering approach to evaluate\nthe likelihood function, leading to maximum $O(k^3n)$ complexity, where $k$ is\nthe number of available bands and $n$ is the number of unique observation times\nacross the $k$ bands. This is a significant computational advantage over a\ncommonly used univariate Gaussian process that can stack up all multi-band\nlight curves in one vector with maximum $O(k^3n^3)$ complexity. Using such\nefficient likelihood computation, we provide both maximum likelihood estimates\nand Bayesian posterior samples of the model parameters. Three numerical\nillustrations are presented; (i) analyzing simulated five-band light curves for\na comparison with independent single-band fits; (ii) analyzing five-band light\ncurves of a quasar obtained from the Sloan Digital Sky Survey (SDSS) Stripe~82\nto estimate the short-term variability and timescale; (iii) analyzing\ngravitationally lensed $g$- and $r$-band light curves of Q0957+561 to infer the\ntime delay. Two R packages, Rdrw and timedelay, are publicly available to fit\nthe proposed models.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 17:25:10 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 04:08:21 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Hu", "Zhirui", ""], ["Tak", "Hyungsuk", ""]]}, {"id": "2005.08057", "submitter": "Yang Feng", "authors": "Yang Feng and Qingfeng Liu", "title": "Nested Model Averaging on Solution Path for High-dimensional Linear\n  Regression", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the nested model averaging method on the solution path for a\nhigh-dimensional linear regression problem. In particular, we propose to\ncombine model averaging with regularized estimators (e.g., lasso and SLOPE) on\nthe solution path for high-dimensional linear regression. In simulation\nstudies, we first conduct a systematic investigation on the impact of predictor\nordering on the behavior of nested model averaging, then show that nested model\naveraging with lasso and SLOPE compares favorably with other competing methods,\nincluding the infeasible lasso and SLOPE with the tuning parameter optimally\nselected. A real data analysis on predicting the per capita violent crime in\nthe United States shows an outstanding performance of the nested model\naveraging with lasso.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 18:09:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Feng", "Yang", ""], ["Liu", "Qingfeng", ""]]}, {"id": "2005.08155", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan and Xinwei Zhang", "title": "On loss functions and regret bounds for multi-category classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new approaches in multi-class settings for constructing proper\nscoring rules and hinge-like losses and establishing corresponding regret\nbounds with respect to the zero-one or cost-weighted classification loss. Our\nconstruction of losses involves deriving new inverse mappings from a concave\ngeneralized entropy to a loss through the use of a convex dissimilarity\nfunction related to the multi-distribution $f$-divergence. Moreover, we\nidentify new classes of multi-class proper scoring rules, which also recover\nand reveal interesting relationships between various composite losses currently\nin use. We establish new classification regret bounds in general for\nmulti-class proper scoring rules by exploiting the Bregman divergences of the\nassociated generalized entropies, and, as applications, provide simple\nmeaningful regret bounds for two specific classes of proper scoring rules.\nFinally, we derive new hinge-like convex losses, which are tighter convex\nextensions than related hinge-like losses and geometrically simpler with fewer\nnon-differentiable edges, while achieving similar regret bounds. We also\nestablish a general classification regret bound for all losses which induce the\nsame generalized entropy as the zero-one loss.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 03:12:52 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 19:14:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tan", "Zhiqiang", ""], ["Zhang", "Xinwei", ""]]}, {"id": "2005.08217", "submitter": "Ryan Thompson", "authors": "Ryan Thompson", "title": "Robust subset selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best subset selection (or \"best subsets\") estimator is a classic tool for\nsparse regression, and developments in mathematical optimization over the past\ndecade have made it more computationally tractable than ever. Notwithstanding\nits desirable statistical properties, the best subsets estimator is susceptible\nto outliers and can break down in the presence of a single contaminated data\npoint. To address this issue, we propose a robust adaption of best subsets that\nis highly resistant to contamination in both the response and the predictors.\nOur estimator generalizes the notion of subset selection to both predictors and\nobservations, thereby achieving robustness in addition to sparsity. This\nprocedure, which we call \"robust subset selection\" (or \"robust subsets\"), is\ndefined by a combinatorial optimization problem for which we apply modern\ndiscrete optimization methods. We formally establish the robustness of our\nestimator in terms of the finite-sample breakdown point of its objective value.\nIn support of this result, we report experiments on both synthetic and real\ndata that demonstrate the superiority of robust subsets over best subsets in\nthe presence of contamination. Importantly, robust subsets fares competitively\nacross several metrics compared with popular robust adaptions of the Lasso.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 10:56:33 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 09:00:24 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Thompson", "Ryan", ""]]}, {"id": "2005.08261", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell and Yuguo Chen", "title": "Latent Space Models for Dynamic Networks with Weighted Edges", "comments": null, "journal-ref": "Social Networks, 44:105-116, 2016", "doi": "10.1016/j.socnet.2015.07.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal binary relational data can be better understood by implementing\na latent space model for dynamic networks. This approach can be broadly\nextended to many types of weighted edges by using a link function to model the\nmean of the dyads, or by employing a similar strategy via data augmentation. To\ndemonstrate this, we propose models for count dyads and for non-negative real\ndyads, analyzing simulated data and also both mobile phone data and world\nexport/import data. The model parameters and latent actors' trajectories,\nestimated by Markov chain Monte Carlo algorithms, provide insight into the\nnetwork dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 14:23:21 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sewell", "Daniel K.", ""], ["Chen", "Yuguo", ""]]}, {"id": "2005.08267", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell, Yuguo Chen, William Bernhard and Tracy Sulkin", "title": "Model-Based Longitudinal Clustering with Varying Cluster Assignments", "comments": null, "journal-ref": "Statistica Sinica. Vol. 26, No. 1 (January 2016), pp. 205-233", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often of interest to perform clustering on longitudinal data, yet it is\ndifficult to formulate an intuitive model for which estimation is\ncomputationally feasible. We propose a model-based clustering method for\nclustering objects that are observed over time. The proposed model can be\nviewed as an extension of the normal mixture model for clustering to\nlongitudinal data. While existing models only account for clustering effects,\nwe propose modeling the distribution of the observed values of each object as a\nblending of a cluster effect and an individual effect, hence also giving an\nestimate of how much the behavior of an object is determined by the cluster to\nwhich it belongs. Further, it is important to detect how explanatory variables\naffect the clustering. An advantage of our method is that it can handle\nmultiple explanatory variables of any type through a linear modeling of the\ncluster transition probabilities. We implement the generalized EM algorithm\nusing several recursive relationships to greatly decrease the computational\ncost. The accuracy of our estimation method is illustrated in a simulation\nstudy, and U.S. Congressional data is analyzed.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 14:38:30 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sewell", "Daniel K.", ""], ["Chen", "Yuguo", ""], ["Bernhard", "William", ""], ["Sulkin", "Tracy", ""]]}, {"id": "2005.08269", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell and Yuguo Chen", "title": "Analysis of the Formation of the Structure of Social Networks using\n  Latent Space Models for Ranked Dynamic Networks", "comments": null, "journal-ref": "J. R. Stat. Soc. C, 64: 611-633 (2015)", "doi": "10.1111/rssc.12093", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The formation of social networks and the evolution of their structures have\nbeen of interest to researchers for many decades. We wish to answer questions\nabout network stability, group formation and popularity effects. We propose a\nlatent space model for ranked dynamic networks that can be used to intuitively\nframe and answer these questions. The well known data collected by Newcomb in\nthe 1950's is very well suited to analyze the formation of a social network. We\napplied our model to this data in order to investigate the network stability,\nwhat groupings emerge and when they emerge, and how individual popularity is\nassociated with individual stability.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 14:49:45 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sewell", "Daniel K.", ""], ["Chen", "Yuguo", ""]]}, {"id": "2005.08276", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell and Yuguo Chen", "title": "Latent Space Approaches to Community Detection in Dynamic Networks", "comments": null, "journal-ref": "Bayesian Anal. 12 (2017), no. 2, 351--377.\n  https://projecteuclid.org/euclid.ba/1461603847", "doi": "10.1214/16-BA1000", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding dyadic data into a latent space has long been a popular approach to\nmodeling networks of all kinds. While clustering has been done using this\napproach for static networks, this paper gives two methods of community\ndetection within dynamic network data, building upon the distance and\nprojection models previously proposed in the literature. Our proposed\napproaches capture the time-varying aspect of the data, can model directed or\nundirected edges, inherently incorporate transitivity and account for each\nactor's individual propensity to form edges. We provide Bayesian estimation\nalgorithms, and apply these methods to a ranked dynamic friendship network and\nworld export/import data.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 15:14:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sewell", "Daniel K.", ""], ["Chen", "Yuguo", ""]]}, {"id": "2005.08361", "submitter": "Fangting Zhou", "authors": "Fangting Zhou, Kejun He, Qiwei Li, Robert S. Chapkin, Yang Ni", "title": "Bayesian biclustering for microbial metagenomic sequencing data via\n  multinomial matrix factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput sequencing technology provides unprecedented opportunities to\nquantitatively explore human gut microbiome and its relation to diseases.\nMicrobiome data are compositional, sparse, noisy, and heterogeneous, which pose\nserious challenges for statistical modeling. We propose an identifiable\nBayesian multinomial matrix factorization model to infer overlapping clusters\non both microbes and hosts. The proposed method represents the observed\nover-dispersed zero-inflated count matrix as Dirichlet-multinomial mixtures on\nwhich latent cluster structures are built hierarchically. Under the Bayesian\nframework, the number of clusters is automatically determined and available\ninformation from a taxonomic rank tree of microbes is naturally incorporated,\nwhich greatly improves the interpretability of our findings. We demonstrate the\nutility of the proposed approach by comparing to alternative methods in\nsimulations. An application to a human gut microbiome dataset involving\npatients with inflammatory bowel disease reveals interesting clusters, which\ncontain bacteria families Bacteroidaceae, Bifidobacteriaceae,\nEnterobacteriaceae, Fusobacteriaceae, Lachnospiraceae, Ruminococcaceae,\nPasteurellaceae, and Porphyromonadaceae that are known to be related to the\ninflammatory bowel disease and its subtypes according to biological literature.\nOur findings can help generate potential hypotheses for future investigation of\nthe heterogeneity of the human gut microbiome.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 20:11:48 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 23:35:57 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhou", "Fangting", ""], ["He", "Kejun", ""], ["Li", "Qiwei", ""], ["Chapkin", "Robert S.", ""], ["Ni", "Yang", ""]]}, {"id": "2005.08373", "submitter": "Kevin Smith", "authors": "Kevin D. Smith", "title": "A Tutorial on Multivariate $k$-Statistics and their Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document aims to provide an accessible tutorial on the unbiased\nestimation of multivariate cumulants, using $k$-statistics. We offer an\nexplicit and general formula for multivariate $k$-statistics of arbitrary\norder. We also prove that the $k$-statistics are unbiased, using M\\\"obius\ninversion and rudimentary combinatorics. Many detailed examples are considered\nthroughout the paper. We conclude with a discussion of $k$-statistics\ncomputation, including the challenge of time complexity, and we examine a\ncouple of possible avenues to improve the efficiency of this computation. The\npurpose of this document is threefold: to provide a clear introduction to\n$k$-statistics without relying on specialized tools like the umbral calculus;\nto construct an explicit formula for $k$-statistics that might facilitate\nfuture approximations and faster algorithms; and to serve as a companion paper\nto our Python library PyMoments, which implements this formula.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 21:20:40 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Smith", "Kevin D.", ""]]}, {"id": "2005.08457", "submitter": "Yong He", "authors": "Chen Hao, Guo Ying, He Yong, Ji Jiadong, Liu Lei, Shi Yufeng, Wang\n  Yikai, Yu Long, Zhang Xinsheng (for the Alzheimer's Disease Neuroimaging\n  Initiative)", "title": "Simultaneous Differential Network Analysis and Classification for\n  High-dimensional Matrix-variate Data, with application to Brain Connectivity\n  Alteration Detection and fMRI-guided Medical Diagnoses of Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is the most common form of dementia, which causes\nproblems with memory, thinking and behavior. Growing evidence has shown that\nthe brain connectivity network experiences alterations for such a complex\ndisease. Network comparison, also known as differential network analysis, is\nthus particularly powerful to reveal the disease pathologies and identify\nclinical biomarkers for medical diagnoses (classification). Data from\nneurophysiological measurements are multi-dimensional and in matrix-form, which\nposes major challenges in brain connectivity analysis and medical diagnoses.\nNaive vectorization method is not sufficient as it ignores the structural\ninformation within the matrix. In the article, we adopt the Kronecker product\ncovariance matrix framework to capture both spatial and temporal correlations\nof the matrix-variate data while the temporal covariance matrix is treated as a\nnuisance parameter. By recognizing that the strengths of network connections\nmay vary across subjects, we develop an ensemble-learning procedure, which\nidentifies the differential interaction patterns of brain regions between the\nAD group and the control group and conducts medical diagnosis (classification)\nof AD simultaneously. We applied the proposed procedure to functional\nconnectivity analysis of fMRI dataset related with Alzheimer's disease. The hub\nnodes and differential interaction patterns identified are consistent with\nexisting experimental studies, and satisfactory out-of-sample classification\nperformance is achieved for medical diagnosis of Alzheimer's disease. An R\npackage \\SDNCMV\" for implementation is available at\nhttps://github.com/heyongstat/SDNCMV.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 04:40:57 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 01:55:17 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hao", "Chen", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Ying", "Guo", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Yong", "He", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Jiadong", "Ji", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Lei", "Liu", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Yufeng", "Shi", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Yikai", "Wang", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Long", "Yu", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Xinsheng", "Zhang", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"]]}, {"id": "2005.08459", "submitter": "Harlan Campbell", "authors": "Harlan Campbell, Perry de Valpine, Lauren Maxwell, Valentijn MT de\n  Jong, Thomas Debray, Thomas J\\\"anisch, Paul Gustafson", "title": "Bayesian adjustment for preferential testing in estimating the COVID-19\n  infection fatality rate", "comments": "53 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in estimating the infection fatality rate (IFR) -- and its\nrelation with various factors of interest -- is determining the total number of\ncases. The total number of cases is not known because not everyone is tested,\nbut also, more importantly, because tested individuals are not representative\nof the population at large. We refer to the phenomenon whereby infected\nindividuals are more likely to be tested than non-infected individuals, as\n\"preferential testing.\" An open question is whether or not it is possible to\nreliably estimate the IFR without any specific knowledge about the degree to\nwhich the data are biased by preferential testing. In this paper we take a\npartial identifiability approach, formulating clearly where deliberate prior\nassumptions can be made and presenting a Bayesian model which pools information\nfrom different samples. When the model is fit to European data obtained from\nseroprevalence studies and national official COVID-19 statistics, we estimate\nthe overall COVID-19 IFR for Europe to be 0.53%, 95% C.I. = [0.39%, 0.69%].\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 05:00:46 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 20:52:44 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 19:36:18 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 18:56:36 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Campbell", "Harlan", ""], ["de Valpine", "Perry", ""], ["Maxwell", "Lauren", ""], ["de Jong", "Valentijn MT", ""], ["Debray", "Thomas", ""], ["J\u00e4nisch", "Thomas", ""], ["Gustafson", "Paul", ""]]}, {"id": "2005.08543", "submitter": "Atalanti A. Mastakouri", "authors": "Atalanti A. Mastakouri, Bernhard Sch\\\"olkopf, Dominik Janzing", "title": "Necessary and sufficient conditions for causal feature selection in time\n  series with latent common causes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the identification of direct and indirect causes on time series and\nprovide conditions in the presence of latent variables, which we prove to be\nnecessary and sufficient under some graph constraints. Our theoretical results\nand estimation algorithms require two conditional independence tests for each\nobserved candidate time series to determine whether or not it is a cause of an\nobserved target time series. We provide experimental results in simulations, as\nwell as real data. Our results show that our method leads to very low false\npositives and relatively low false negative rates, outperforming the widely\nused Granger causality.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 09:14:34 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 16:57:47 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 08:16:30 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mastakouri", "Atalanti A.", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Janzing", "Dominik", ""]]}, {"id": "2005.08549", "submitter": "Mingyang Shan", "authors": "Mingyang Shan, Kali Thomas, Roee Gutman", "title": "A Bayesian Multi-Layered Record Linkage Procedure to Analyze Functional\n  Status of Medicare Patients with Traumatic Brain Injury", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the association between injury severity and patients' potential\nfor recovery is crucial to providing better care for patients with traumatic\nbrain injury (TBI). Estimation of this relationship requires clinical\ninformation on injury severity, patient demographics, and healthcare\nutilization, which are often obtained from separate data sources. Because of\nprivacy and confidentiality regulations, these data sources do not include\nunique identifiers to link records across data sources. Record linkage is a\nprocess to identify records that represent the same entity across data sources\nin the absence of unique identifiers. These processes commonly rely on\nagreement between variables that appear in both data sources to link records.\nHowever, when the number of records in each file is large, this task is\ncomputationally intensive and may result in false links. Blocking is a data\npartitioning technique that reduces the number of possible links that should be\nconsidered. Healthcare providers can be used as blocks in applications of\nrecord linkage with healthcare datasets. However, providers may not be uniquely\nidentified across files. We propose a Bayesian record linkage procedure that\nsimultaneously performs block-level and record-level linkage. This iterative\napproach incorporates the record-level linkage within block pairs to improve\nthe accuracy of the block-level linkage. Subsequently, the algorithm improves\nrecord-level linkage using the accurate partitioning of the linkage space\nthrough blocking. We demonstrate that our proposed method provides improved\nperformance compared to existing Bayesian record linkage methods that do not\nincorporate blocking. The proposed procedure is then used to merge registry\ndata from the National Trauma Data Bank with Medicare claims data to estimate\nthe relationship between injury severity and TBI patients' recovery.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 09:32:24 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Shan", "Mingyang", ""], ["Thomas", "Kali", ""], ["Gutman", "Roee", ""]]}, {"id": "2005.08608", "submitter": "Norman Fenton Prof", "authors": "Norman Fenton", "title": "A note on 'Collider bias undermines our understanding of COVID-19\n  disease risk and severity' and how causal Bayesian networks both expose and\n  resolve the problem", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important recent preprint by Griffith et al highlights how 'collider bias'\nin studies of COVID19 undermines our understanding of the disease risk and\nseverity. This is typically caused by the data being restricted to people who\nhave undergone COVID19 testing, among whom healthcare workers are\noverrepresented. For example, collider bias caused by smokers being\nunderrepresented in the dataset may (at least partly) explain empirical results\nthat suggest smoking reduces the risk of COVID19. We extend the work of\nGriffith et al making more explicit use of graphical causal models to interpret\nobserved data. We show that their smoking example can be clarified and improved\nusing Bayesian network models with realistic data and assumptions. We show that\nthere is an even more fundamental problem for risk factors like 'stress' which,\nunlike smoking, is more rather than less prevalent among healthcare workers; in\nthis case, because of a combination of collider bias from the biased dataset\nand the fact that 'healthcare worker' is a confounding variable, it is likely\nthat studies will wrongly conclude that stress reduces rather than increases\nthe risk of COVID19. Indeed, \"being in close contact with COVID19 people\"\nreduces the risk of COVID19. To avoid such potentially erroneous conclusions,\nany analysis of observational data must take account of the underlying causal\nstructure including colliders and confounders. If analysts fail to do this\nexplicitly then any conclusions they make about the effect of specific risk\nfactors on COVID19 are likely to be flawed.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:43:25 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 08:05:40 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Fenton", "Norman", ""]]}, {"id": "2005.08639", "submitter": "Rune Christiansen", "authors": "Rune Christiansen, Matthias Baumann, Tobias Kuemmerle, Miguel D.\n  Mahecha, Jonas Peters", "title": "Towards Causal Inference for Spatio-Temporal Data: Conflict and Forest\n  Loss in Colombia", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data scientific problems, we are interested not only in modeling the\nbehaviour of a system that is passively observed, but also in inferring how the\nsystem reacts to changes in the data generating mechanism. Given knowledge of\nthe underlying causal structure, such behaviour can be estimated from purely\nobservational data. To do so, one typically assumes that the causal structure\nof the data generating mechanism can be fully specified. Furthermore, many\nmethods assume that data are generated as independent replications from that\nmechanism. Both of these assumptions are usually hard to justify in practice:\ndatasets often have complex dependence structures, as is the case for\nspatio-temporal data, and the full causal structure between all involved\nvariables is hardly known. Here, we present causal models that are adapted to\nthe characteristics of spatio-temporal data, and which allow us to define and\nquantify causal effects despite incomplete causal background knowledge. We\nfurther introduce a simple approach for estimating causal effects, and a\nnon-parametric hypothesis test for these effects being zero. The proposed\nmethods do not rely on any distributional assumptions on the data, and allow\nfor arbitrarily many latent confounders, given that these confounders do not\nvary across time (or, alternatively, they do not vary across space). Our\ntheoretical findings are supported by simulations and code is available online.\nThis work has been motivated by the following real-world question: how has the\nColombian conflict influenced tropical forest loss? There is evidence for both\nenhancing and reducing impacts, but most literature analyzing this problem is\nnot using formal causal methodology. When applying our method to data from 2000\nto 2018, we find a reducing but insignificant causal effect of conflict on\nforest loss. Regionally, both enhancing and reducing effects can be identified.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:24:37 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 12:19:58 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Christiansen", "Rune", ""], ["Baumann", "Matthias", ""], ["Kuemmerle", "Tobias", ""], ["Mahecha", "Miguel D.", ""], ["Peters", "Jonas", ""]]}, {"id": "2005.08703", "submitter": "Christian Bongiorno", "authors": "Christian Bongiorno and Damien Challet", "title": "Reactive Global Minimum Variance Portfolios with $k-$BAHC covariance\n  cleaning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a $k$-fold boosted version of our Boostrapped Average\nHierarchical Clustering cleaning procedure for correlation and covariance\nmatrices. We then apply this method to global minimum variance portfolios for\nvarious values of $k$ and compare their performance with other state-of-the-art\nmethods. Generally, we find that our method yields better Sharpe ratios after\ntransaction costs than competing filtering methods, despite requiring a larger\nturnover.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 13:34:16 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bongiorno", "Christian", ""], ["Challet", "Damien", ""]]}, {"id": "2005.08711", "submitter": "Charles  Horvath", "authors": "Charles Horvath", "title": "Remarks on a data-driven model for predicting the course of COVID-19\n  epidemic", "comments": "now irrelevant and obsolete", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Norden E. Huang, Fangli Qiao and Ka Kit Tung presented a data-driven model\nfor the COVID-19 epidemic in which the relevant functions depend on a set of\nseven parameters obtained from a statistical analysis of the available data.\nThese parameters are not independent, they are linked through a set of\nrelations the authors call Main Results which are validated by a statistical\nanalysis of the data. The parameters in questions and the relations between\nthem are not always explicitated by the authors. By given them here their\n(simple) mathematical formulations all the relevant functions describing the\ndynamic can be explicitely written down. All the explicit formulas follow from\nthe fact that the log of the number of infected, is a quadratic function of\ntime. The formulas presented here are not themselves approximations - but the\nparameters they involve are of course statistical quantities derived from the\ndata. These formulas could maybe be of some use either to validate the data,\nthe model itself, to update the model or to find approximations to the relevant\nquantities.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:18:26 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 10:40:17 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Horvath", "Charles", ""]]}, {"id": "2005.08749", "submitter": "Sofia Triantafillou", "authors": "Sofia Triantafillou and Gregory Cooper", "title": "Learning Adjustment Sets from Observational and Limited Experimental\n  Data", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal effects from observational data is not always possible due\nto confounding. Identifying a set of appropriate covariates (adjustment set)\nand adjusting for their influence can remove confounding bias; however, such a\nset is typically not identifiable from observational data alone. Experimental\ndata do not have confounding bias, but are typically limited in sample size and\ncan therefore yield imprecise estimates. Furthermore, experimental data often\ninclude a limited set of covariates, and therefore provide limited insight into\nthe causal structure of the underlying system. In this work we introduce a\nmethod that combines large observational and limited experimental data to\nidentify adjustment sets and improve the estimation of causal effects. The\nmethod identifies an adjustment set (if possible) by calculating the marginal\nlikelihood for the experimental data given observationally-derived prior\nprobabilities of potential adjustmen sets. In this way, the method can make\ninferences that are not possible using only the conditional dependencies and\nindependencies in all the observational and experimental data. We show that the\nmethod successfully identifies adjustment sets and improves causal effect\nestimation in simulated data, and it can sometimes make additional inferences\nwhen compared to state-of-the-art methods for combining experimental and\nobservational data.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:23:32 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 20:35:49 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Triantafillou", "Sofia", ""], ["Cooper", "Gregory", ""]]}, {"id": "2005.08808", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell and Yuguo Chen", "title": "Latent Space Models for Dynamic Networks", "comments": null, "journal-ref": "Journal of the American Statistical Association, 110:512,\n  1646-1657 (2015)", "doi": "10.1080/01621459.2014.988214", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks are used in a variety of fields to represent the structure\nand evolution of the relationships between entities. We present a model which\nembeds longitudinal network data as trajectories in a latent Euclidean space. A\nMarkov chain Monte Carlo algorithm is proposed to estimate the model parameters\nand latent positions of the actors in the network. The model yields meaningful\nvisualization of dynamic networks, giving the researcher insight into the\nevolution and the structure, both local and global, of the network. The model\nhandles directed or undirected edges, easily handles missing edges, and lends\nitself well to predicting future edges. Further, a novel approach is given to\ndetect and visualize an attracting influence between actors using only the edge\ninformation. We use the case-control likelihood approximation to speed up the\nestimation algorithm, modifying it slightly to account for missing data. We\napply the latent space model to data collected from a Dutch classroom, and a\ncosponsorship network collected on members of the U.S. House of\nRepresentatives, illustrating the usefulness of the model by making insights\ninto the networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:33:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sewell", "Daniel K.", ""], ["Chen", "Yuguo", ""]]}, {"id": "2005.08827", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell and Aaron Miller", "title": "Simulation-free estimation of an individual-based SEIR model for\n  evaluating nonpharmaceutical interventions with an application to COVID-19 in\n  Iowa", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0241949", "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing COVID-19 pandemic has overwhelmingly demonstrated the need to\naccurately evaluate the effects of implementing new or altering existing\nnonpharmaceutical interventions. Since these interventions applied at the\nsocietal level cannot be evaluated through traditional experimental means,\npublic health officials and other decision makers must rely on statistical and\nmathematical epidemiological models. Nonpharmaceutical interventions are\ntypically focused on contacts between members of a population, and yet most\nepidemiological models rely on homogeneous mixing which has repeatedly been\nshown to be an unrealistic representation of contact patterns. An alternative\napproach is individual based models (IBMs), but these are often time intensive\nand computationally expensive to implement, requiring a high degree of\nexpertise and computational resources. More often, decision makers need to know\nthe effects of potential public policy decisions in a very short time window\nusing limited resources. This paper presents an estimation algorithm for an IBM\ndesigned to evaluate nonpharmaceutical interventions. By utilizing recursive\nrelationships, our method can quickly compute the expected epidemiological\noutcomes even for large populations based on any arbitrary contact network. We\nutilize our methods to evaluate the effects of relaxing current social\ndistancing measures in Iowa, USA, at various times and to various degrees.\n\\verb!R! code for our method is provided in the supplementary material, thereby\nallowing others to utilize our approach for other regions.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:58:32 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 20:06:27 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 20:54:31 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sewell", "Daniel K.", ""], ["Miller", "Aaron", ""]]}, {"id": "2005.08923", "submitter": "Paula Navarro-Esteban", "authors": "P. Navarro-Esteban, J. A. Cuesta-Albertos", "title": "High-dimensional outlier detection using random projections", "comments": "43 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There exist multiple methods to detect outliers in multivariate data in the\nliterature, but most of them require to estimate the covariance matrix. The\nhigher the dimension, the more complex the estimation of the matrix becoming\nimpossible in high dimensions. In order to avoid estimating this matrix, we\npropose a novel random projections-based procedure to detect outliers in\nGaussian multivariate data. It consists in projecting the data in several\none-dimensional subspaces where an appropriate univariate outlier detection\nmethod, similar to Tukey's method but with a threshold depending on the initial\ndimension and the sample size, is applied. The required number of projections\nis determined using sequential analysis. Simulated and real datasets illustrate\nthe performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:50:51 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 22:26:40 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Navarro-Esteban", "P.", ""], ["Cuesta-Albertos", "J. A.", ""]]}, {"id": "2005.09017", "submitter": "Peyman Jalali", "authors": "Peyman Jalali, Kshitij Khare and George Michailidis", "title": "B-CONCORD -- A scalable Bayesian high-dimensional precision matrix\n  estimation procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse estimation of the precision matrix under high-dimensional scaling\nconstitutes a canonical problem in statistics and machine learning. Numerous\nregression and likelihood based approaches, many frequentist and some Bayesian\nin nature have been developed. Bayesian methods provide direct uncertainty\nquantification of the model parameters through the posterior distribution and\nthus do not require a second round of computations for obtaining debiased\nestimates of the model parameters and their confidence intervals. However, they\nare computationally expensive for settings involving more than 500 variables.\nTo that end, we develop B-CONCORD for the problem at hand, a Bayesian analogue\nof the CONvex CORrelation selection methoD (CONCORD) introduced by Khare et al.\n(2015). B-CONCORD leverages the CONCORD generalized likelihood function\ntogether with a spike-and-slab prior distribution to induce sparsity in the\nprecision matrix parameters. We establish model selection and estimation\nconsistency under high-dimensional scaling; further, we develop a procedure\nthat refits only the non-zero parameters of the precision matrix, leading to\nsignificant improvements in the estimates in finite samples. Extensive\nnumerical work illustrates the computational scalability of the proposed\napproach vis-a-vis competing Bayesian methods, as well as its accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:25:52 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Jalali", "Peyman", ""], ["Khare", "Kshitij", ""], ["Michailidis", "George", ""]]}, {"id": "2005.09065", "submitter": "Shane Barratt", "authors": "Shane Barratt, Guillermo Angeris, Stephen Boyd", "title": "Optimal Representative Sample Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of assigning weights to a set of samples or data\nrecords, with the goal of achieving a representative weighting, which happens\nwhen certain sample averages of the data are close to prescribed values. We\nframe the problem of finding representative sample weights as an optimization\nproblem, which in many cases is convex and can be efficiently solved. Our\nformulation includes as a special case the selection of a fixed number of the\nsamples, with equal weights, i.e., the problem of selecting a smaller\nrepresentative subset of the samples. While this problem is combinatorial and\nnot convex, heuristic methods based on convex optimization seem to perform very\nwell. We describe rsw, an open-source implementation of the ideas described in\nthis paper, and apply it to a skewed sample of the CDC BRFSS dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 20:29:00 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Barratt", "Shane", ""], ["Angeris", "Guillermo", ""], ["Boyd", "Stephen", ""]]}, {"id": "2005.09166", "submitter": "Samuel Gingras", "authors": "Samuel Gingras and William J. McCausland", "title": "A Flexible Stochastic Conditional Duration Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new stochastic duration model for transaction times in asset\nmarkets. We argue that widely accepted rules for aggregating seemingly related\ntrades mislead inference pertaining to durations between unrelated trades:\nwhile any two trades executed in the same second are probably related, it is\nextremely unlikely that all such pairs of trades are, in a typical sample. By\nplacing uncertainty about which trades are related within our model, we improve\ninference for the distribution of durations between unrelated trades,\nespecially near zero. We introduce a normalized conditional distribution for\ndurations between unrelated trades that is both flexible and amenable to\nshrinkage towards an exponential distribution, which we argue is an appropriate\nfirst-order model. Thanks to highly efficient draws of state variables,\nnumerical efficiency of posterior simulation is much higher than in previous\nstudies. In an empirical application, we find that the conditional hazard\nfunction for durations between unrelated trades varies much less than what most\nstudies find. We claim that this is because we avoid statistical artifacts that\narise from deterministic trade-aggregation rules and unsuitable parametric\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 02:01:45 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Gingras", "Samuel", ""], ["McCausland", "William J.", ""]]}, {"id": "2005.09213", "submitter": "Jose Jimenez", "authors": "Jos\\'e L. Jim\\'enez, Julia Niewczas, Alexander Bore, Carl-Fredrik\n  Burman", "title": "A modified weighted log-rank test for confirmatory trials with a high\n  proportion of treatment switching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In confirmatory cancer clinical trials, overall survival (OS) is normally a\nprimary endpoint in the intention-to-treat (ITT) analysis under regulatory\nstandards. After the tumor progresses, it is common that patients allocated to\nthe control group switch to the experimental treatment, or another drug in the\nsame class. Such treatment switching may dilute the relative efficacy of the\nnew drug compared to the control group, leading to lower statistical power. It\nwould be possible to decrease the estimation bias by shortening the follow-up\nperiod but this may lead to a loss of information and power. Instead we propose\na modified weighted log-rank test (mWLR) that aims at balancing these factors\nby down-weighting events occurring when many patients have switched treatment.\n  As the weighting should be pre-specified and the impact of treatment\nswitching is unknown, we predict the hazard ratio function and use it to\ncompute the weights of the mWLR. The method may incorporate information from\nprevious trials regarding the potential hazard ratio function over time.\n  We are motivated by the RECORD-1 trial of everolimus against placebo in\npatients with metastatic renal-cell carcinoma where almost 80\\% of the patients\nin the placebo group received everolimus after disease progression. Extensive\nsimulations show that the new test gives considerably higher efficiency than\nthe standard log-rank test in realistic scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 04:56:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Jim\u00e9nez", "Jos\u00e9 L.", ""], ["Niewczas", "Julia", ""], ["Bore", "Alexander", ""], ["Burman", "Carl-Fredrik", ""]]}, {"id": "2005.09301", "submitter": "Mark van de Wiel", "authors": "Mark A. van de Wiel, Mirrelijn M. van Nee, Armin Rauschenberger", "title": "Fast cross-validation for multi-penalty ridge regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional prediction with multiple data types needs to account for\npotentially strong differences in predictive signal. Ridge regression is a\nsimple model for high-dimensional data that has challenged the predictive\nperformance of many more complex models and learners, and that allows inclusion\nof data type specific penalties. The largest challenge for multi-penalty ridge\nis to optimize these penalties efficiently in a cross-validation (CV) setting,\nin particular for GLM and Cox ridge regression, which require an additional\nestimation loop by iterative weighted least squares (IWLS). Our main\ncontribution is a computationally very efficient formula for the multi-penalty,\nsample-weighted hat-matrix, as used in the IWLS algorithm. As a result, nearly\nall computations are in low-dimensional space, rendering a speed-up of several\norders of magnitude. We developed a flexible framework that facilitates\nmultiple types of response, unpenalized covariates, several performance\ncriteria and repeated CV. Extensions to paired and preferential data types are\nincluded and illustrated on several cancer genomics survival prediction\nproblems. Moreover, we present similar computational shortcuts for maximum\nmarginal likelihood and Bayesian probit regression. The corresponding\nR-package, multiridge, serves as a versatile standalone tool, but also as a\nfast benchmark for other more complex models and multi-view learners.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 09:13:43 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 07:52:28 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["van de Wiel", "Mark A.", ""], ["van Nee", "Mirrelijn M.", ""], ["Rauschenberger", "Armin", ""]]}, {"id": "2005.09365", "submitter": "Peter Green", "authors": "Peter J. Green, Julia Mortera", "title": "Inference about complex relationships using peak height data from DNA\n  mixtures", "comments": "29 pages, 12 figures, 20 tables; V2 has different casework examples,\n  and general minor edits; V3 has general edits following review, including\n  lengthier exposition; V4 has further explanation, and a supplementary\n  appendix on related software; V5 corrects typo, updates references and has\n  new version of Fig 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both criminal cases and civil cases there is an increasing demand for the\nanalysis of DNA mixtures involving relationships. The goal might be, for\nexample, to identify the contributors to a DNA mixture where the donors may be\nrelated, or to infer the relationship between individuals based on a mixture.\n  This paper introduces an approach to modelling and computation for DNA\nmixtures involving contributors with arbitrarily complex relationships. It\nbuilds on an extension of Jacquard's condensed coefficients of identity, to\nspecify and compute with joint relationships, not only pairwise ones, including\nthe possibility of inbreeding. The methodology developed is applied to two\ncasework examples involving a missing person, and simulation studies of\nperformance, in which the ability of the methodology to recover complex\nrelationship information from synthetic data with known `true' family structure\nis examined.\n  The methods used to analyse the examples are implemented in the new KinMix R\npackage, that extends the DNAmixtures package to allow for modelling DNA\nmixtures with related contributors. KinMix inherits from DNAmixtures the\ncapacity to deal with mixtures with many contributors, in a time- and\nspace-efficient way.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 11:17:13 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 18:29:59 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 17:58:04 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2021 17:13:01 GMT"}, {"version": "v5", "created": "Wed, 21 Apr 2021 14:52:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Green", "Peter J.", ""], ["Mortera", "Julia", ""]]}, {"id": "2005.09396", "submitter": "Giacomo De Nicola", "authors": "Giacomo De Nicola, Benjamin Sischka and G\\\"oran Kauermann", "title": "Mixture Models and Networks -- Overview of Stochastic Blockmodelling", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are probabilistic models aimed at uncovering and representing\nlatent subgroups within a population. In the realm of network data analysis,\nthe latent subgroups of nodes are typically identified by their connectivity\nbehaviour, with nodes behaving similarly belonging to the same community. In\nthis context, mixture modelling is pursued through stochastic blockmodelling.\nWe consider stochastic blockmodels and some of their variants and extensions\nfrom a mixture modelling perspective. We also survey some of the main classes\nof estimation methods available, and propose an alternative approach. In\naddition to the discussion of inferential properties and estimating procedures,\nwe focus on the application of the models to several real-world network\ndatasets, showcasing the advantages and pitfalls of different approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:42:21 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 17:19:28 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["De Nicola", "Giacomo", ""], ["Sischka", "Benjamin", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "2005.09421", "submitter": "Gregor Pasemann", "authors": "Gregor Pasemann, Sven Flemming, Sergio Alonso, Carsten Beta, Wilhelm\n  Stannat", "title": "Diffusivity Estimation for Activator-Inhibitor Models: Theory and\n  Application to Intracellular Dynamics of the Actin Cytoskeleton", "comments": null, "journal-ref": "J Nonlinear Sci 31, 59 (2021)", "doi": "10.1007/s00332-021-09714-4", "report-no": null, "categories": "q-bio.QM cs.NA math.DS math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theory for diffusivity estimation for spatially extended\nactivator-inhibitor dynamics modelling the evolution of intracellular signaling\nnetworks is developed in the mathematical framework of stochastic\nreaction-diffusion systems. In order to account for model uncertainties, we\nextend the results for parameter estimation for semilinear stochastic partial\ndifferential equations, as developed in [PS20], to the problem of joint\nestimation of diffusivity and parametrized reaction terms. Our theoretical\nfindings are applied to the estimation of effective diffusivity of signaling\ncomponents contributing to intracellular dynamics of the actin cytoskeleton in\nthe model organism Dictyostelium discoideum.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 15:59:12 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 10:10:28 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 15:09:45 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Pasemann", "Gregor", ""], ["Flemming", "Sven", ""], ["Alonso", "Sergio", ""], ["Beta", "Carsten", ""], ["Stannat", "Wilhelm", ""]]}, {"id": "2005.09440", "submitter": "Jonathan Embleton", "authors": "Jonathan Embleton, Marina I. Knight and Hernando Ombao", "title": "Multiscale modelling of replicated nonstationary time series", "comments": "24 pages and 13 figures (main paper), supplementary material included", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the neurosciences, to observe variability across time in the dynamics\nof an underlying brain process is neither new nor unexpected. Wavelets are\nessential in analyzing brain signals because, even within a single trial, brain\nsignals exhibit nonstationary behaviour. However, neurological signals\ngenerated within an experiment may also potentially exhibit evolution across\ntrials (replicates). As neurologists consider localised spectra of brain\nsignals to be most informative, here we develop a novel wavelet-based tool\ncapable to formally represent process nonstationarities across both time and\nreplicate dimensions. Specifically, we propose the Replicate Locally Stationary\nWavelet (RLSW) process, that captures the potential nonstationary behaviour\nwithin and across trials. Estimation using wavelets gives a natural desired\ntime- and replicate-localisation of the process dynamics. We develop the\nassociated spectral estimation framework and establish its asymptotic\nproperties. By means of thorough simulation studies, we demonstrate the\ntheoretical estimator properties hold in practice. A real data investigation\ninto the evolutionary dynamics of the hippocampus and nucleus accumbens during\nan associative learning experiment, demonstrate the applicability of our\nproposed methodology, as well as the new insights it provides.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 13:37:44 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Embleton", "Jonathan", ""], ["Knight", "Marina I.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2005.09487", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell", "title": "Network Autocorrelation Models with Egocentric Data", "comments": null, "journal-ref": "Social Networks, 49:113-123, 2017", "doi": "10.1016/j.socnet.2017.01.001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network autocorrelation models have been widely used for decades to model the\njoint distribution of the attributes of a network's actors. This class of\nmodels can estimate both the effect of individual characteristics as well as\nthe network effect, or social influence, on some actor attribute of interest.\nCollecting data on the entire network, however, is very often infeasible or\nimpossible if the network boundary is unknown or difficult to define. Obtaining\negocentric network data overcomes these obstacles, but as of yet there has been\nno clear way to model this type of data and still appropriately capture the\nnetwork effect on the actor attributes in a way that is compatible with a joint\ndistribution on the full network data. This paper adapts the class of network\nautocorrelation models to handle egocentric data. The proposed methods thus\nincorporate the complex dependence structure of the data induced by the network\nrather than simply using ad hoc measures of the egos' networks to model the\nmean structure, and can estimate the network effect on the actor attribute of\ninterest. The vast quantities of unknown information about the network can be\nsuccinctly represented in such a way that only depends on the number of alters\nin the egocentric network data and not on the total number of actors in the\nnetwork. Estimation is done within a Bayesian framework. A simulation study is\nperformed to evaluate the estimation performance, and an egocentric data set is\nanalyzed where the aim is to determine if there is a network effect on\nenvironmental mastery, an important aspect of psychological well-being.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 15:26:47 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Sewell", "Daniel K.", ""]]}, {"id": "2005.09488", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell", "title": "Simultaneous and Temporal Autoregressive Network Models", "comments": null, "journal-ref": "Network Science, 6(2):204-231, 2018", "doi": "10.1017/nws.2017.36", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While logistic regression models are easily accessible to researchers, when\napplied to network data there are unrealistic assumptions made about the\ndependence structure of the data. For temporal networks measured in discrete\ntime, recent work has made good advances \\citep{almquist2014logistic}, but\nthere is still the assumption that the dyads are conditionally independent\ngiven the edge histories. This assumption can be quite strong and is sometimes\ndifficult to justify. If time steps are rather large, one would typically\nexpect not only the existence of temporal dependencies among the dyads across\nobserved time points but also the existence of simultaneous dependencies\naffecting how the dyads of the network co-evolve. We propose a general\nobservation driven model for dynamic networks which overcomes this problem by\nmodeling both the mean and the covariance structures as functions of the edge\nhistories using a flexible autoregressive approach. This approach can be shown\nto fit into a generalized linear mixed model framework. We propose a\nvisualization method which provides evidence concerning the existence of\nsimultaneous dependence. We describe a simulation study to determine the\nmethod's performance in the presence and absence of simultaneous dependence,\nand we analyze both a proximity network from conference attendees and a world\ntrade network. We also use this last data set to illustrate how simultaneous\ndependencies become more prominent as the time intervals become coarser.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 15:43:02 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Sewell", "Daniel K.", ""]]}, {"id": "2005.09498", "submitter": "Lola Etievant", "authors": "Lola Etievant and Vivian Viallon", "title": "On some limitations of probabilistic models for dimension-reduction:\n  illustration in the case of one particular probabilistic formulation of PLS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial Least Squares (PLS) refer to a class of dimension-reduction\ntechniques aiming at the identification of two sets of components with maximal\ncovariance, in order to model the relationship between two sets of observed\nvariables $x\\in\\mathbb{R}^p$ and $y\\in\\mathbb{R}^q$, with $p\\geq 1, q\\geq 1$.\nEl Bouhaddani et al. (2017) have recently proposed a probabilistic formulation\nof PLS. Under the constraints they consider for the parameters of their model,\nthis latter can be seen as a probabilistic formulation of one version of PLS,\nnamely the PLS-SVD. However, we establish that these constraints are too\nrestrictive as they define a very particular subset of distributions for\n$(x,y)$ under which, roughly speaking, components with maximal covariance\n(solutions of PLS-SVD), are also necessarily of respective maximal variances\n(solutions of the principal components analyses of $x$ and $y$, respectively).\nThen, we propose a simple extension of el Bouhaddani et al.'s model, which\ncorresponds to a more general probabilistic formulation of PLS-SVD, and which\nis no longer restricted to these particular distributions. We present numerical\nexamples to illustrate the limitations of the original model of el Bouhaddani\net al. (2017).\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 14:53:01 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Etievant", "Lola", ""], ["Viallon", "Vivian", ""]]}, {"id": "2005.09583", "submitter": "Elan Segarra", "authors": "Felix Elwert and Elan Segarra", "title": "Instrumental Variables with Treatment-Induced Selection: Exact Bias\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables (IV) estimation suffers selection bias when the\nanalysis conditions on the treatment. Judea Pearl's early graphical definition\nof instrumental variables explicitly prohibited conditioning on the treatment.\nNonetheless, the practice remains common. In this paper, we derive exact\nanalytic expressions for IV selection bias across a range of data-generating\nmodels, and for various selection-inducing procedures. We present four sets of\nresults for linear models. First, IV selection bias depends on the conditioning\nprocedure (covariate adjustment vs. sample truncation). Second, IV selection\nbias due to covariate adjustment is the limiting case of IV selection bias due\nto sample truncation. Third, in certain models, the IV and OLS estimators under\nselection bound the true causal effect in large samples. Fourth, we\ncharacterize situations where IV remains preferred to OLS despite selection on\nthe treatment. These results broaden the notion of IV selection bias beyond\nsample truncation, replace prior simulation findings with exact analytic\nformulas, and enable formal sensitivity analyses.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 16:53:19 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Elwert", "Felix", ""], ["Segarra", "Elan", ""]]}, {"id": "2005.09596", "submitter": "Mohammad Arashi", "authors": "Mohammad Arashi, Andriette Bekker, Mahdi Salehi, Sollie Millard,\n  Barend Erasmus, Tanita Cronje, and Mohammad Golpaygani", "title": "Spatial analysis and prediction of COVID-19 spread in South Africa after\n  lockdown", "comments": "16 pages, 9 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the impact of COVID-19 on South Africa? This paper envisages\nassisting researchers and decision-makers in battling the COVID-19 pandemic\nfocusing on South Africa. This paper focuses on the spread of the disease by\napplying heatmap retrieval of hotspot areas and spatial analysis is carried out\nusing the Moran index. For capturing spatial autocorrelation between the\nprovinces of South Africa, the adjacent, as well as the geographical distance\nmeasures, are used as a weight matrix for both absolute and relative counts.\nFurthermore, generalized logistic growth curve modeling is used for the\nprediction of the COVID-19 spread. We expect this data-driven modeling to\nprovide some insights into hotspot identification and timeous action\ncontrolling the spread of the virus.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:18:30 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Arashi", "Mohammad", ""], ["Bekker", "Andriette", ""], ["Salehi", "Mahdi", ""], ["Millard", "Sollie", ""], ["Erasmus", "Barend", ""], ["Cronje", "Tanita", ""], ["Golpaygani", "Mohammad", ""]]}, {"id": "2005.09600", "submitter": "Li-Chun Zhang", "authors": "Li-Chun Zhang", "title": "Generalised regression estimation given imperfectly matched auxiliary\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised regression estimation allows one to make use of available\nauxiliary information in survey sampling. We develop three types of generalised\nregression estimator when the auxiliary data cannot be matched perfectly to the\nsample units, so that the standard estimator is inapplicable. The inference\nremains design-based. Consistency of the proposed estimators is either given by\nconstruction or else can be tested given the observed sample and links. Mean\nsquare errors can be estimated. A simulation study is used to explore the\npotentials of the proposed estimators.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:23:02 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zhang", "Li-Chun", ""]]}, {"id": "2005.09644", "submitter": "Jianfeng Zhou", "authors": "Jianfeng Zhou", "title": "A Statistical Model for Imaging Systems", "comments": "17 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV physics.data-an physics.optics stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavior of photons is controlled by quantum mechanics, not as\ndeterministic as classical optics shows. To this end, we defined a new\nstatistic $Z$, which is equal to the variance minus the expectation or mean.\nThen, we established a statistical model for imaging systems and obtained three\nfundamental imaging formulas. Among them, the first formula is entirely\nconsistent with the classic convolution equation. The second and third ones\nlink the $Z$ quantities of the object and noise elegantly with the $Z$ image\nand covariance image, revealing new laws. Consequently, besides the flux\ndensity, the $Z$ quantity of an object is also imageable, which opens a new\nrealm for imaging systems to explore the physical world.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 07:14:34 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zhou", "Jianfeng", ""]]}, {"id": "2005.09696", "submitter": "Hany Farid", "authors": "Emily A. Cooper, Hany Farid", "title": "A Toolbox for the Radial and Angular Marginalization of Bivariate Normal\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bivariate normal distributions are often used to describe the joint\nprobability density of a pair of random variables. These distributions arise\nacross many domains, from telecommunications, to meteorology, ballistics, and\ncomputational neuroscience. In these applications, it is often useful to\nradially and angularly marginalize (i.e.,~under a polar transformation) the\njoint probability distribution relative to the coordinate system's origin. This\nmarginalization is trivial for a zero-mean, isotropic distribution, but is\nnon-trivial for the most general case of a non-zero-mean, anisotropic\ndistribution with a non-diagonal covariance matrix. Across domains, a range of\nsolutions with varying degrees of generality have been derived. Here, we\nprovide a concise summary of analytic solutions for the polar marginalization\nof bivariate normal distributions. This report accompanies a Matlab (Mathworks,\nInc.) and R toolbox that provides closed-form and numeric implementations for\nthe marginalizations described herein.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 18:30:23 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Cooper", "Emily A.", ""], ["Farid", "Hany", ""]]}, {"id": "2005.09711", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Hongjin Zhang, Konstantinos Tsampourakis and George\n  Michailidis", "title": "Inference on the Change Point for High Dimensional Dynamic Graphical\n  Models", "comments": "Software available upon request (built in R)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an estimator for the change point parameter for a dynamically\nevolving graphical model, and also obtain its asymptotic distribution under\nhigh dimensional scaling. To procure the latter result, we establish that the\nproposed estimator exhibits an $O_p(\\psi^{-2})$ rate of convergence, wherein\n$\\psi$ represents the jump size between the graphical model parameters before\nand after the change point. Further, it retains sufficient adaptivity against\nplug-in estimates of the graphical model parameters. We characterize the forms\nof the asymptotic distribution under the both a vanishing and a non-vanishing\nregime of the magnitude of the jump size. Specifically, in the former case it\ncorresponds to the argmax of a negative drift asymmetric two sided Brownian\nmotion, while in the latter case to the argmax of a negative drift asymmetric\ntwo sided random walk, whose increments depend on the distribution of the\ngraphical model. Easy to implement algorithms are provided for estimating the\nchange point and their performance assessed on synthetic data. The proposed\nmethodology is further illustrated on RNA-sequenced microbiome data and their\nchanges between young and older individuals.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 19:15:32 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 20:51:16 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 05:30:44 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kaul", "Abhishek", ""], ["Zhang", "Hongjin", ""], ["Tsampourakis", "Konstantinos", ""], ["Michailidis", "George", ""]]}, {"id": "2005.09738", "submitter": "Yun Li", "authors": "Yun Li, Douglas E. Schaubel, Kevin He", "title": "Matching methods for obtaining survival functions to estimate the effect\n  of a time-dependent treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies of survival time featuring a binary time-dependent\ntreatment, the hazard ratio (an instantaneous measure) is often used to\nrepresent the treatment effect. However, investigators are often more\ninterested in the difference in survival functions. We propose semiparametric\nmethods to estimate the causal effect of treatment among the treated with\nrespect to survival probability. The objective is to compare post-treatment\nsurvival with the survival function that would have been observed in the\nabsence of treatment. For each patient, we compute a prognostic score (based on\nthe pre-treatment death hazard) and a propensity score (based on the treatment\nhazard). Each treated patient is then matched with an alive, uncensored and\nnot-yet-treated patient with similar prognostic and/or propensity scores. The\nexperience of each treated and matched patient is weighted using a variant of\nInverse Probability of Censoring Weighting to account for the impact of\ncensoring. We propose estimators of the treatment-specific survival functions\n(and their difference), computed through weighted Nelson-Aalen estimators.\nClosed-form variance estimators are proposed which take into consideration the\npotential replication of subjects across matched sets. The proposed methods are\nevaluated through simulation, then applied to estimate the effect of kidney\ntransplantation on survival among end-stage renal disease patients using data\nfrom a national organ failure registry.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:22:54 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Li", "Yun", ""], ["Schaubel", "Douglas E.", ""], ["He", "Kevin", ""]]}, {"id": "2005.09744", "submitter": "Roberto Vila Gabriel", "authors": "Helton Saulo, Roberto Vila, Leonardo Paiva, Narayanaswamy Balakrishnan", "title": "On a family of discrete log-symmetric distributions", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of continuous probability distributions has been widespread in\nproblems with purely discrete nature. In general, such distributions are not\nappropriate in this scenario. In this paper, we introduce a class of discrete\nand asymmetric distributions based on the family of continuous log-symmetric\ndistributions. Some properties are discussed as well as estimation by the\nmaximum likelihood method. A Monte Carlo simulation study is carried out to\nevaluate the performance of the estimators, and censored and uncensored data\nsets are used to illustrate the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:34:15 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Saulo", "Helton", ""], ["Vila", "Roberto", ""], ["Paiva", "Leonardo", ""], ["Balakrishnan", "Narayanaswamy", ""]]}, {"id": "2005.09780", "submitter": "Yun Li", "authors": "Yun Li, Yoonseok Lee, Friedrich K Port and Bruce M Robinson", "title": "The Impact of Unmeasured Within- and Between-Cluster Confounding on the\n  Bias of Effect Estimators from Fixed Effect, Mixed effect and Instrumental\n  Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods are popular choices in combating unmeasured\nconfounding to obtain less biased effect estimates. However, we demonstrate\nthat alternative methods may give less biased estimates depending on the nature\nof unmeasured confounding. Treatment preferences of clusters (e.g., physician\npractices) are the most f6requently used instruments in instrumental variable\nanalyses (IVA). These preference-based IVAs are usually conducted on data\nclustered by region, hospital/facility, or physician, where unmeasured\nconfounding often occurs within or between clusters. We aim to quantify the\nimpact of unmeasured confounding on the bias of effect estimators in IVA, as\nwell as alternative methods including ordinary least squares regression, linear\nmixed models (LMM) and fixed effect models (FE) to study the effect of a\ncontinuous exposure (e.g., treatment dose). We derive bias formulae of\nestimators from these four methods in the presence of unmeasured within- and/or\nbetween-cluster confounders. We show that IVAs can provide consistent estimates\nwhen unmeasured within-cluster confounding exists, but not when between-cluster\nconfounding exists. On the other hand, FEs and LMMs can provide consistent\nestimates when unmeasured between-cluster confounding exits, but not for\nwithin-cluster confounding. Whether IVAs are advantageous in reducing bias over\nFEs and LMMs depends on the extent of unmeasured within-cluster confounding\nrelative to between-cluster confounding. Furthermore, the impact of unmeasured\nbetween-cluster confounding on IVA estimates is larger than the impact of\nunmeasured within-cluster confounding on FE and LMM estimates. We illustrate\nthese methods through data applications. Our findings provide guidance for\nchoosing appropriate methods to combat the dominant types of unmeasured\nconfounders and help interpret statistical results in the context of unmeasured\nconfounding\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 22:45:28 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Li", "Yun", ""], ["Lee", "Yoonseok", ""], ["Port", "Friedrich K", ""], ["Robinson", "Bruce M", ""]]}, {"id": "2005.09827", "submitter": "Jeremy Koster", "authors": "Jeremy Koster", "title": "Dyadic Reciprocity as a Function of Covariates", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reciprocity in dyadic interactions is common and a topic of interest across\ndisciplines. In some cases, reciprocity may be expected to be more or less\nprevalent among certain kinds of dyads. In response to interest among\nresearchers in estimating dyadic reciprocity as a function of covariates, this\npaper proposes an extension to the multilevel Social Relations Model. The\noutcome variable is assumed to be a binomial proportion, as is commonly\nencountered in observational and archival research. The approach draws on\nprinciples of multilevel modeling to implement random intercepts and slopes\nthat vary among dyads. The corresponding variance function permits the\ncomputation of a dyadic reciprocity correlation. The modeling approach can\npotentially be integrated with other statistical models in the field of social\nnetwork analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 02:22:07 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Koster", "Jeremy", ""]]}, {"id": "2005.09918", "submitter": "Gertraud Malsiner-Walli", "authors": "Sylvia Fr\\\"uhwirth-Schnatter, Gertraud Malsiner-Walli, Bettina Gr\\\"un", "title": "Generalized mixtures of finite mixtures and telescoping sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a Bayesian framework, a comprehensive investigation of the model class\nof mixtures of finite mixtures (MFMs) where a prior on the number of components\nis specified is performed. This model class has applications in model-based\nclustering as well as for semi-parametric density approximation, but requires\nsuitable prior specifications and inference methods to exploit its full\npotential.\n  We contribute to the Bayesian analysis of MFMs by considering a generalized\nclass of MFMs containing static and dynamic MFMs where the Dirichlet parameter\nof the component weights either is fixed or depends on the number of\ncomponents. We emphasize the distinction between the number of components $K$\nof a mixture and the number of clusters $K_+$, i.e., the number of filled\ncomponents given the data. In the MFM model, $K_+$ is a random variable and its\nprior depends on the prior on the number of components $K$ and the mixture\nweights. We characterize the prior on the number of clusters $K_+$ for\ngeneralized MFMs and derive computationally feasible formulas to calculate this\nimplicit prior. In addition we propose a flexible prior distribution class for\nthe number of components $K$ and link MFMs to Bayesian non-parametric mixtures.\n  For posterior inference of a generalized MFM, we propose the novel\ntelescoping sampler which allows Bayesian inference for mixtures with arbitrary\ncomponent distributions without the need to resort to RJMCMC methods. The\ntelescoping sampler explicitly samples the number of components, but otherwise\nrequires only the usual MCMC steps for estimating a finite mixture model. The\nease of its application using different component distributions is demonstrated\non several data sets.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 09:02:40 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 07:57:32 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Malsiner-Walli", "Gertraud", ""], ["Gr\u00fcn", "Bettina", ""]]}, {"id": "2005.09960", "submitter": "Michel Petitjean", "authors": "Michel Petitjean", "title": "Tables of Quantiles of the Distribution of the Empirical Chiral Index in\n  the Case of the Uniform Law and in the Case of the Normal Law", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The empirical distribution of the chiral index is simulated for various\nsample sizes for the uniform law and and for the normal law. The estimated\nquantiles $K_{0.90}$, $K_{0.95}$, $K_{0.98}$, and $K_{0.99}$, are tabulated for\nuse in symmetry testing in the uniform case and in the normal case.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:04:36 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Petitjean", "Michel", ""]]}, {"id": "2005.09994", "submitter": "Ariane Hanebeck", "authors": "Ariane Hanebeck and Bernhard Klar", "title": "Smooth Distribution Function Estimation for Lifetime Distributions using\n  Szasz-Mirakyan Operators", "comments": "Small typo in Theorem 10: Now -1/12 instead of +1/12 in the term of\n  order $m^{-1}$", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new smooth estimator for continuous\ndistribution functions on the positive real half-line using Szasz-Mirakyan\noperators, similar to Bernstein's approximation theorem. We show that the\nproposed estimator outperforms the empirical distribution function in terms of\nasymptotic (integrated) mean-squared error, and generally compares favourably\nwith other competitors in theoretical comparisons. Also, we conduct the\nsimulations to demonstrate the finite sample performance of the proposed\nestimator.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 12:34:02 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 19:51:22 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 17:34:31 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2020 17:00:29 GMT"}, {"version": "v5", "created": "Wed, 27 Jan 2021 19:38:07 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Hanebeck", "Ariane", ""], ["Klar", "Bernhard", ""]]}, {"id": "2005.09996", "submitter": "Daniel Sewell", "authors": "Daniel K. Sewell", "title": "Heterogeneous Susceptibilities in Social Influence Models", "comments": null, "journal-ref": "Social Networks, 52:135-144, 2017", "doi": "10.1016/j.socnet.2017.06.004", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network autocorrelation models are widely used to evaluate the impact of\nsocial influence on some variable of interest. This is a large class of models\nthat parsimoniously accounts for how one's neighbors influence one's own\nbehaviors or opinions by incorporating the network adjacency matrix into the\njoint distribution of the data. These models assume homogeneous susceptibility\nto social influence, however, which may be a strong assumption in many\ncontexts. This paper proposes a hierarchical model that allows the influence\nparameter to be a function of individual attributes and/or of local network\ntopological features. We derive an approximation of the posterior distribution\nin a general framework that is applicable to the Durbin, network effects,\nnetwork disturbances, or network moving average autocorrelation models. The\nproposed approach can also be applied to investigating determinants of social\ninfluence in the context of egocentric network data. We apply our method to a\ndata set collected via mobile phones in which we determine the effect of social\ninfluence on physical activity levels, as well as classroom data in which we\ninvestigate peer influence on student defiance. With this last data set, we\nalso investigate the performance of the proposed egocentric network model.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 15:33:04 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Sewell", "Daniel K.", ""]]}, {"id": "2005.10059", "submitter": "Frank Bretz", "authors": "Jianan Peng, Wei Liu, Frank Bretz, Anthony Hayter", "title": "Simultaneous Confidence Tubes for Comparison of Several Multivariate\n  Linear Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the research on multiple comparison and simultaneous inference in the\npast sixty years or so has been for the comparisons of several population\nmeans. Spurrier (1999) seems to be the first to study the multiple comparison\nof several simple linear regression lines by using simultaneous confidence\nbands. In this paper, the work of Liu et al. (2004) for finite comparisons of\nseveral univariate linear regression models by using simultaneous confidence\nbands has been extended to finite comparison of several multivariate linear\nregression models by using simultaneous confidence tubes. We show how\nsimultaneous confidence tubes can be constructed to allow more informative\ninferences for the comparison of several multivariate linear regression models\nthan the current approach of hypotheses testing. The methodologies are\nillustrated with examples.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:09:02 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Peng", "Jianan", ""], ["Liu", "Wei", ""], ["Bretz", "Frank", ""], ["Hayter", "Anthony", ""]]}, {"id": "2005.10092", "submitter": "Matteo Fasiolo", "authors": "Christian Capezza, Biagio Palumbo, Yannig Goude, Simon N. Wood and\n  Matteo Fasiolo", "title": "Additive stacking for disaggregate electricity demand forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future grid management systems will coordinate distributed production and\nstorage resources to manage, in a cost effective fashion, the increased load\nand variability brought by the electrification of transportation and by a\nhigher share of weather dependent production. Electricity demand forecasts at a\nlow level of aggregation will be key inputs for such systems. We focus on\nforecasting demand at the individual household level, which is more challenging\nthan forecasting aggregate demand, due to the lower signal-to-noise ratio and\nto the heterogeneity of consumption patterns across households. We propose a\nnew ensemble method for probabilistic forecasting, which borrows strength\nacross the households while accommodating their individual idiosyncrasies. In\nparticular, we develop a set of models or 'experts' which capture different\ndemand dynamics and we fit each of them to the data from each household. Then\nwe construct an aggregation of experts where the ensemble weights are estimated\non the whole data set, the main innovation being that we let the weights vary\nwith the covariates by adopting an additive model structure. In particular, the\nproposed aggregation method is an extension of regression stacking (Breiman,\n1996) where the mixture weights are modelled using linear combinations of\nparametric, smooth or random effects. The methods for building and fitting\nadditive stacking models are implemented by the gamFactory R package, available\nat https://github.com/mfasiolo/gamFactory.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:54:22 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Capezza", "Christian", ""], ["Palumbo", "Biagio", ""], ["Goude", "Yannig", ""], ["Wood", "Simon N.", ""], ["Fasiolo", "Matteo", ""]]}, {"id": "2005.10119", "submitter": "Vivian Viallon", "authors": "Ballout Nadim and Etievant Lola and Viallon Vivian", "title": "On the use of cross-validation for the calibration of the adaptive lasso", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive lasso refers to a class of methods that use weighted versions of\nthe $L_1$-norm penalty, with weights derived from an initial estimate of the\nparameter vector to be estimated. Irrespective of the method chosen to compute\nthis initial estimate, the performance of the adaptive lasso critically depends\non the value of a hyperparameter, which controls the magnitude of the weighted\n$L_1$-norm in the penalized criterion. As for other machine learning methods,\ncross-validation is very popular for the calibration of the adaptive lasso,\nthat this for the selection of a data-driven optimal value of this\nhyperparameter. However, the most simple cross-validation scheme is not valid\nin this context, and a more elaborate one has to be employed to guarantee an\noptimal calibration. The discrepancy of the simple cross-validation scheme has\nbeen well documented in other contexts, but less so when it comes to the\ncalibration of the adaptive lasso, and, therefore, many statistical analysts\nstill overlook it. In this work, we recall appropriate cross-validation schemes\nfor the calibration of the adaptive lasso, and illustrate the discrepancy of\nthe simple scheme, using both synthetic and real-world examples. Our results\nclearly establish the suboptimality of the simple scheme, in terms of support\nrecovery and prediction error, for several versions of the adaptive lasso,\nincluding the popular one-step lasso.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:19:07 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 10:01:59 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 12:47:45 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Nadim", "Ballout", ""], ["Lola", "Etievant", ""], ["Vivian", "Viallon", ""]]}, {"id": "2005.10125", "submitter": "Mariflor Vega-Carrasco", "authors": "Mariflor Vega-Carrasco, Jason O'sullivan, Rosie Prior, Ioanna\n  Manolopoulou, Mirco Musolesi", "title": "Modelling Grocery Retail Topic Distributions: Evaluation,\n  Interpretability and Stability", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the shopping motivations behind market baskets has high\ncommercial value in the grocery retail industry. Analyzing shopping\ntransactions demands techniques that can cope with the volume and\ndimensionality of grocery transactional data while keeping interpretable\noutcomes. Latent Dirichlet Allocation (LDA) provides a suitable framework to\nprocess grocery transactions and to discover a broad representation of\ncustomers' shopping motivations. However, summarizing the posterior\ndistribution of an LDA model is challenging, while individual LDA draws may not\nbe coherent and cannot capture topic uncertainty. Moreover, the evaluation of\nLDA models is dominated by model-fit measures which may not adequately capture\nthe qualitative aspects such as interpretability and stability of topics.\n  In this paper, we introduce clustering methodology that post-processes\nposterior LDA draws to summarise the entire posterior distribution and identify\nsemantic modes represented as recurrent topics. Our approach is an alternative\nto standard label-switching techniques and provides a single posterior summary\nset of topics, as well as associated measures of uncertainty. Furthermore, we\nestablish a more holistic definition for model evaluation, which assesses topic\nmodels based not only on their likelihood but also on their coherence,\ndistinctiveness and stability. By means of a survey, we set thresholds for the\ninterpretation of topic coherence and topic similarity in the domain of grocery\nretail data. We demonstrate that the selection of recurrent topics through our\nclustering methodology not only improves model likelihood but also outperforms\nthe qualitative aspects of LDA such as interpretability and stability. We\nillustrate our methods on an example from a large UK supermarket chain.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:23:36 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 15:29:50 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Vega-Carrasco", "Mariflor", ""], ["O'sullivan", "Jason", ""], ["Prior", "Rosie", ""], ["Manolopoulou", "Ioanna", ""], ["Musolesi", "Mirco", ""]]}, {"id": "2005.10173", "submitter": "Yolanda Larriba", "authors": "Cristina Rueda, Yolanda Larriba and Adri\\'an Lamela", "title": "The hidden waves in the ECG uncovered: a sound automated interpretation\n  method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for analysing cardiac rhythm data is presented in this\npaper. Heartbeats are decomposed into the five fundamental $P$, $Q$, $R$, $S$\nand $T$ waves plus an error term to account for artefacts in the data which\nprovides a meaningful, physical interpretation of the heart's electric system.\nThe morphology of each wave is concisely described using four parameters that\nallow to all the different patterns in heartbeats be characterized and thus\ndifferentiated\n  This multi-purpose approach solves such questions as the extraction of\ninterpretable features, the detection of the fiducial marks of the fundamental\nwaves, or the generation of synthetic data and the denoising of signals. Yet,\nthe greatest benefit from this new discovery will be the automatic diagnosis of\nheart anomalies as well as other clinical uses with great advantages compared\nto the rigid, vulnerable and black box machine learning procedures, widely used\nin medical devices.\n  The paper shows the enormous potential of the method in practice;\nspecifically, the capability to discriminate subjects, characterize\nmorphologies and detect the fiducial marks (reference points) are validated\nnumerically using simulated and real data, thus proving that it outperforms its\ncompetitors.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 16:30:33 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 12:25:20 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Rueda", "Cristina", ""], ["Larriba", "Yolanda", ""], ["Lamela", "Adri\u00e1n", ""]]}, {"id": "2005.10285", "submitter": "Tran Vi-Vi \\'Elodie Perrin", "authors": "T.V.E. Perrin, O. Roustant, J. Rohmer, O. Alata, J.P. Naulin, D.\n  Idier, R. Pedreros, D. Moncoulon, P. Tinard", "title": "Functional principal component analysis for global sensitivity analysis\n  of model with spatial output", "comments": "26 pages, 12 figures, 1 algoritm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by risk assessment of coastal flooding, we consider time-consuming\nsimulators with a spatial output. The aim is to perform sensitivity analysis\n(SA), quantifying the influence of input parameters on the output. There are\nthree main issues. First, due to computational time, standard SA techniques\ncannot be directly applied on the simulator. Second, the output is infinite\ndimensional, or at least high dimensional if the output is discretized. Third,\nthe spatial output is non-stationary and exhibits strong local variations. We\nshow that all these issues can be addressed all together by using functional\nPCA (FPCA). We first specify a functional basis, such as wavelets or B-splines,\ndesigned to handle local variations. Secondly, we select the most influential\nbasis terms, either with an energy criterion after basis orthonormalization, or\ndirectly on the original basis with a penalized regression approach. Then FPCA\nfurther reduces dimension by doing PCA on the most influential basis\ncoefficients, with an ad-hoc metric. Finally, fast-to-evaluate metamodels are\nbuilt on the few selected principal components. They provide a proxy on which\nSA can be done. As a by-product, we obtain analytical formulas for\nvariance-based sensitivity indices, generalizing known formula assuming\northonormality of basis functions.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 18:07:27 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 13:38:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Perrin", "T. V. E.", ""], ["Roustant", "O.", ""], ["Rohmer", "J.", ""], ["Alata", "O.", ""], ["Naulin", "J. P.", ""], ["Idier", "D.", ""], ["Pedreros", "R.", ""], ["Moncoulon", "D.", ""], ["Tinard", "P.", ""]]}, {"id": "2005.10287", "submitter": "Mario Beraha", "authors": "Mario Beraha and Alessandra Guglielmi and Fernando A. Quintana", "title": "The semi-hierarchical Dirichlet Process and its application to\n  clustering homogeneous distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing homogeneity of distributions is an old problem that has received\nconsiderable attention, especially in the nonparametric Bayesian literature. To\nthis effect, we propose the semi-hierarchical Dirichlet process, a novel\nhierarchical prior that extends the hierarchical Dirichlet process of Teh et\nal. (2006) and that avoids the degeneracy issues of nested processes recently\ndescribed by Camerlenghi et al. (2019a). We go beyond the simple yes/no answer\nto the homogeneity question and embed the proposed prior in a random partition\nmodel; this procedure allows us to give a more comprehensive response to the\nabove question and in fact find groups of populations that are internally\nhomogeneous when I greater or equal than 2 such populations are considered. We\nstudy theoretical properties of the semi-hierarchical Dirichlet process and of\nthe Bayes factor for the homogeneity test when I = 2. Extensive simulation\nstudies and applications to educational data are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 18:10:13 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 17:20:07 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 08:15:54 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 15:42:52 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Beraha", "Mario", ""], ["Guglielmi", "Alessandra", ""], ["Quintana", "Fernando A.", ""]]}, {"id": "2005.10306", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Luis E. Nieto-Barajas", "title": "Dependence on a collection of Poisson random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two novel ways of introducing dependence among Poisson counts\nthrough the use of latent variables in a three levels hierarchical model.\nMarginal distributions of the random variables of interest are Poisson with\nstrict stationarity as special case. Order--$p$ dependence is described in\ndetail for a temporal sequence of random variables, however spatial or\nspatio-temporal dependencies are also possible. A full Bayesian inference of\nthe models is described and performance of the models is illustrated with a\nnumerical analysis of maternal mortality in Mexico. Extensions to cope with\noverdispersion are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 18:35:11 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 00:12:51 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Nieto-Barajas", "Luis E.", ""]]}, {"id": "2005.10307", "submitter": "William Artman", "authors": "William J. Artman, Ashkan Ertefaie, Kevin G. Lynch, James R. McKay,\n  Brent A. Johnson", "title": "Adjusting for Partial Compliance in SMARTs: a Bayesian Semiparametric\n  Approach", "comments": "31 pages, 8 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cyclical and heterogeneous nature of many substance use disorders\nhighlights the need to adapt the type or the dose of treatment to accommodate\nthe specific and changing needs of individuals. The Adaptive Treatment for\nAlcohol and Cocaine Dependence study (ENGAGE) is a multi-stage randomized trial\nthat aimed to provide longitudinal data for constructing treatment strategies\nto improve patients' engagement in therapy. However, the high rate of\nnoncompliance and lack of analytic tools to account for noncompliance have\nimpeded researchers from using the data to achieve the main goal of the trial.\nWe overcome this issue by defining our target parameter as the mean outcome\nunder different treatment strategies for given potential compliance strata and\npropose a Bayesian semiparametric model to estimate this quantity. While it\nadds substantial complexities to the analysis, one important feature of our\nwork is that we consider partial rather than binary compliance classes which is\nmore relevant in longitudinal studies. We assess the performance of our method\nthrough comprehensive simulation studies. We illustrate its application on the\nENGAGE study and demonstrate that the optimal treatment strategy depends on\ncompliance strata.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 18:38:44 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Artman", "William J.", ""], ["Ertefaie", "Ashkan", ""], ["Lynch", "Kevin G.", ""], ["McKay", "James R.", ""], ["Johnson", "Brent A.", ""]]}, {"id": "2005.10425", "submitter": "Walter Dempsey", "authors": "Walter Dempsey", "title": "The Hypothesis of Testing: Paradoxes arising out of reported coronavirus\n  case-counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statisticians, epidemiologists, economists and data scientists have\nregistered serious reservations regarding the reported coronavirus case-counts.\nLimited testing capacity across the country has been widely identified as a key\ndriver of suppressed coronavirus case-counts. The calls to increase testing\ncapacity are well-justified as they become a more frequent point of discussion\nin the public sphere. While expanded testing is a laudable goal, selection bias\nwill impact estimates of disease prevalence and the effective reproduction\nnumber until the entire population is sampled. Moreover, tests are imperfect as\nfalse positive/negative rates interact in complex ways with selection bias. In\nthis paper, we attempt to clarify this interaction. Through simple\ncalculations, we demonstrate pitfalls and paradoxes that can arise when\nconsidering case-count data in the presence of selection bias and measurement\nerror. The discussion guides several suggestions on how to improve current\ncase-count reporting.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:12:10 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Dempsey", "Walter", ""]]}, {"id": "2005.10435", "submitter": "HaiYing Wang", "authors": "Jun Yu, HaiYing Wang, Mingyao Ai and Huiming Zhang", "title": "Optimal Distributed Subsampling for Maximum Quasi-Likelihood Estimators\n  with Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonuniform subsampling methods are effective to reduce computational burden\nand maintain estimation efficiency for massive data. Existing methods mostly\nfocus on subsampling with replacement due to its high computational efficiency.\nIf the data volume is so large that nonuniform subsampling probabilities cannot\nbe calculated all at once, then subsampling with replacement is infeasible to\nimplement. This paper solves this problem using Poisson subsampling. We first\nderive optimal Poisson subsampling probabilities in the context of\nquasi-likelihood estimation under the A- and L-optimality criteria. For a\npractically implementable algorithm with approximated optimal subsampling\nprobabilities, we establish the consistency and asymptotic normality of the\nresultant estimators. To deal with the situation that the full data are stored\nin different blocks or at multiple locations, we develop a distributed\nsubsampling framework, in which statistics are computed simultaneously on\nsmaller partitions of the full data. Asymptotic properties of the resultant\naggregated estimator are investigated. We illustrate and evaluate the proposed\nstrategies through numerical experiments on simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:46:56 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 13:45:32 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 15:32:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yu", "Jun", ""], ["Wang", "HaiYing", ""], ["Ai", "Mingyao", ""], ["Zhang", "Huiming", ""]]}, {"id": "2005.10483", "submitter": "Gherardo Varando", "authors": "Gherardo Varando and Niels Richard Hansen", "title": "Graphical continuous Lyapunov models", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear Lyapunov equation of a covariance matrix parametrizes the\nequilibrium covariance matrix of a stochastic process. This parametrization can\nbe interpreted as a new graphical model class, and we show how the model class\nbehaves under marginalization and introduce a method for structure learning via\n$\\ell_1$-penalized loss minimization. Our proposed method is demonstrated to\noutperform alternative structure learning algorithms in a simulation study, and\nwe illustrate its application for protein phosphorylation network\nreconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 06:50:27 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Varando", "Gherardo", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "2005.10494", "submitter": "Xuekui Zhang", "authors": "Yitao Lu, Julie Zhou, Li Xing, Xuekui Zhang", "title": "The Optimal Design of Clinical Trials with Potential Biomarker Effects,\n  A Novel Computational Approach", "comments": "18 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a future trend of healthcare, personalized medicine tailors medical\ntreatments to individual patients. It requires to identify a subset of patients\nwith the best response to treatment. The subset can be defined by a biomarker\n(e.g. expression of a gene) and its cutoff value. Topics on subset\nidentification have received massive attention. There are over 2 million hits\nby keyword searches on Google Scholar. However, how to properly incorporate the\nidentified subsets/biomarkers to design clinical trials is not trivial and\nrarely discussed in the literature, which leads to a gap between research\nresults and real-world drug development.\n  To fill in this gap, we formulate the problem of clinical trial design into\nan optimization problem involving high-dimensional integration, and propose a\nnovel computational solution based on Monte-Carlo and smoothing methods. Our\nmethod utilizes the modern techniques of General-Purpose computing on Graphics\nProcessing Units for large-scale parallel computing. Compared to the standard\nmethod in three-dimensional problems, our approach is more accurate and 133\ntimes faster. This advantage increases when dimensionality increases. Our\nmethod is scalable to higher-dimensional problems since the precision bound is\na finite number not affected by dimensionality.\n  Our software will be available on GitHub and CRAN, which can be applied to\nguide the design of clinical trials to incorporate the biomarker better.\nAlthough our research is motivated by the design of clinical trials, the method\ncan be used widely to solve other optimization problems involving\nhigh-dimensional integration.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 07:19:07 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Lu", "Yitao", ""], ["Zhou", "Julie", ""], ["Xing", "Li", ""], ["Zhang", "Xuekui", ""]]}, {"id": "2005.10579", "submitter": "Shu Yang", "authors": "Shu Yang, Donglin Zeng, Xiaofei Wang", "title": "Elastic Integrative Analysis of Randomized Trial and Real-World Data for\n  Treatment Heterogeneity Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel randomized trial (RT) and real-world (RW) data are becoming\nincreasingly available for treatment evaluation. Given the complementary\nfeatures of the RT and RW data, we propose a test-based elastic integrative\nanalysis of the RT and RW data for accurate and robust estimation of the\nheterogeneity of treatment effect (HTE), which lies at the heart of precision\nmedicine. When the RW data are not subject to bias, e.g., due to unmeasured\nconfounding, our approach combines the RT and RW data for optimal estimation by\nexploiting semiparametric efficiency theory. Utilizing the design advantage of\nRTs, we construct a built-in test procedure to gauge the reliability of the RW\ndata and decide whether or not to use RW data in an integrative analysis. We\ncharacterize the asymptotic distribution of the test-based elastic integrative\nestimator under local alternatives, which provides a better approximation of\nthe finite-sample behaviors of the test and estimator when the idealistic\nassumption required for the RW data is weakly violated. We provide a\ndata-adaptive procedure to select the threshold of the test statistic that\npromises the smallest mean square error of the proposed estimator of the HTE.\nLastly, we construct an elastic confidence interval that has a good\nfinite-sample coverage property. We apply the proposed method to characterize\nwho can benefit from adjuvant chemotherapy in patients with stage IB non-small\ncell lung cancer.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 11:42:14 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 10:08:43 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Yang", "Shu", ""], ["Zeng", "Donglin", ""], ["Wang", "Xiaofei", ""]]}, {"id": "2005.10743", "submitter": "Anru R. Zhang", "authors": "Yuetian Luo and Anru R. Zhang", "title": "Tensor Clustering with Planted Structures: Statistical Optimality and\n  Computational Limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies the statistical and computational limits of high-order\nclustering with planted structures. We focus on two clustering models, constant\nhigh-order clustering (CHC) and rank-one higher-order clustering (ROHC), and\nstudy the methods and theory for testing whether a cluster exists (detection)\nand identifying the support of cluster (recovery).\n  Specifically, we identify the sharp boundaries of signal-to-noise ratio for\nwhich CHC and ROHC detection/recovery are statistically possible. We also\ndevelop the tight computational thresholds: when the signal-to-noise ratio is\nbelow these thresholds, we prove that polynomial-time algorithms cannot solve\nthese problems under the computational hardness conjectures of hypergraphic\nplanted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS)\nrecovery. We also propose polynomial-time tensor algorithms that achieve\nreliable detection and recovery when the signal-to-noise ratio is above these\nthresholds. Both sparsity and tensor structures yield the computational\nbarriers in high-order tensor clustering. The interplay between them results in\nsignificant differences between high-order tensor clustering and matrix\nclustering in literature in aspects of statistical and computational phase\ntransition diagrams, algorithmic approaches, hardness conjecture, and proof\ntechniques. To our best knowledge, we are the first to give a thorough\ncharacterization of the statistical and computational trade-off for such a\ndouble computational-barrier problem. Finally, we provide evidence for the\ncomputational hardness conjectures of HPC detection and HPDS recovery.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 15:53:44 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 01:57:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Luo", "Yuetian", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2005.10779", "submitter": "Saptarshi Chakraborty", "authors": "Saptarshi Chakraborty, Colin B. Begg, and Ronglai Shen", "title": "Using the \"Hidden\" Genome to Improve Classification of Cancer Types", "comments": "24 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly common clinically for cancer specimens to be examined\nusing techniques that identify somatic mutations. In principle these mutational\nprofiles can be used to diagnose the tissue of origin, a critical task for the\n3-5% of tumors that have an unknown primary site. Diagnosis of primary site is\nalso critical for screening tests that employ circulating DNA. However, most\nmutations observed in any new tumor are very rarely occurring mutations, and\nindeed the preponderance of these may never have been observed in any previous\nrecorded tumor. To create a viable diagnostic tool we need to harness the\ninformation content in this \"hidden genome\" of variants for which no direct\ninformation is available. To accomplish this we propose a multi-level\nmeta-feature regression to extract the critical information from rare variants\nin the training data in a way that permits us to also extract diagnostic\ninformation from any previously unobserved variants in the new tumor sample. A\nscalable implementation of the model is obtained by combining a\nhigh-dimensional feature screening approach with a group-lasso penalized\nmaximum likelihood approach based on an equivalent mixed-effect representation\nof the multilevel model. We apply the method to the Cancer Genome Atlas\nwhole-exome sequencing data set including 3702 tumor samples across 7 common\ncancer sites. Results show that our multi-level approach can harness\nsubstantial diagnostic information from the hidden genome.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:58:11 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 15:30:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chakraborty", "Saptarshi", ""], ["Begg", "Colin B.", ""], ["Shen", "Ronglai", ""]]}, {"id": "2005.10892", "submitter": "Martin F\\'elix Medina", "authors": "Mart\\'in Humberto F\\'elix Medina", "title": "Combining Cluster Sampling and Link-Tracing Sampling to Estimate Totals\n  and Means of Hidden Populations in Presence of Heterogeneous Probabilities of\n  Links", "comments": "34 pages, 2 figures, technical report", "journal-ref": null, "doi": null, "report-no": "FCFM-UAS-2020-01", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Horvitz-Thompson-like and Hajek-like estimators of the total and\nmean of the values of a variable of interest associated with the elements of a\nhard-to-reach population sampled by the variant of link-tracing sampling\nproposed by Felix-Medina and Thompson (2004). As examples of this type of\npopulation are drug users, homeless people and sex workers. In this sampling\nvariant, a frame of venues or places where the members of the population tend\nto gather, such as parks and bars, is constructed. The frame is not assumed to\ncover the whole population. An initial cluster sample of elements is selected\nfrom the frame, where the clusters are the venues, and the elements in the\ninitial sample are asked to name their contacts who are also members of the\npopulation. The sample size is increased by including in the sample the named\nelements who are not in the initial sample. The proposed estimators do not use\ndesign-based inclusion probabilities, but model-based inclusion probabilities\nwhich are derived from a model proposed by Felix-Medina et al. (2015) and are\nestimated by maximum likelihood estimators. The inclusion probabilities are\nassumed to be heterogeneous, that is, that they depend on the sampled people.\nEstimates of the variances of the proposed estimators are obtained by bootstrap\nand they are used to construct confidence intervals of the totals and means.\nThe performance of the proposed estimators and confidence intervals is\nevaluated by two numerical studies, one of them based on real data, and the\nresults show that their performance is acceptable.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:33:56 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Medina", "Mart\u00edn Humberto F\u00e9lix", ""]]}, {"id": "2005.10998", "submitter": "Hiroto Katsumata", "authors": "Hiroto Katsumata", "title": "Navigated Weighting to Improve Inverse Probability Weighting for Missing\n  Data Problems and Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse probability weighting (IPW) is broadly utilized to address\nmissing data problems including causal inference but may suffer from large\nvariances and biases due to propensity score model misspecification. To solve\nthese problems, I propose an estimation method called the navigated weighting\n(NAWT), which utilizes estimating equations suitable for a specific\npre-specified parameter of interest (e.g., the average treatment effects on the\ntreated). Since these pre-specified parameters determine the relative\nimportance of each unit as a function of propensity scores, the NAWT\nprioritizes important units in the propensity score estimation to improve\nefficiency and robustness to model misspecification. I investigate its\nlarge-sample properties and demonstrate its finite sample improvements through\nsimulation studies and an empirical example. An R package nawtilus which\nimplements the NAWT is developed and available from the Comprehensive R Archive\nNetwork (http://cran.r-project.org/package=nawtilus).\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 04:25:49 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 02:14:43 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 11:48:22 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Katsumata", "Hiroto", ""]]}, {"id": "2005.11303", "submitter": "Nima Hejazi", "authors": "Ashkan Ertefaie, Nima S. Hejazi, Mark J. van der Laan", "title": "Nonparametric inverse probability weighted estimators based on the\n  highly adaptive lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse probability weighted estimators are the oldest and potentially most\ncommonly used class of procedures for the estimation of causal effects. By\nadjusting for selection biases via a weighting mechanism, these procedures\nestimate an effect of interest by constructing a pseudo-population in which\nselection biases are eliminated. Despite their ease of use, these estimators\nrequire the correct specification of a model for the weighting mechanism, are\nknown to be inefficient, and suffer from the curse of dimensionality. We\npropose a class of nonparametric inverse probability weighted estimators in\nwhich the weighting mechanism is estimated via undersmoothing of the highly\nadaptive lasso, a nonparametric regression function proven to converge at\n$n^{-1/3}$-rate to the true weighting mechanism. We demonstrate that our\nestimators are asymptotically linear with variance converging to the\nnonparametric efficiency bound. Unlike doubly robust estimators, our procedures\nrequire neither derivation of the efficient influence function nor\nspecification of the conditional outcome model. Our theoretical developments\nhave broad implications for the construction of efficient inverse probability\nweighted estimators in large statistical models and a variety of problem\nsettings. We assess the practical performance of our estimators in simulation\nstudies and demonstrate use of our proposed methodology with data from a\nlarge-scale epidemiologic study.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 17:49:46 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 07:35:38 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ertefaie", "Ashkan", ""], ["Hejazi", "Nima S.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2005.11461", "submitter": "HaiYing Wang", "authors": "Guanyu Hu and HaiYing Wang", "title": "Most Likely Optimal Subsampled Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) requires to evaluate the full data likelihood\nat different parameter values iteratively and is often computationally\ninfeasible for large data sets. In this paper, we propose to approximate the\nlog-likelihood with subsamples taken according to nonuniform subsampling\nprobabilities, and derive the most likely optimal (MLO) subsampling\nprobabilities for better approximation. Compared with existing subsampled MCMC\nalgorithm with equal subsampling probabilities, our MLO subsampled MCMC has a\nhigher estimation efficiency with the same subsampling ratio. We also derive a\nformula using the asymptotic distribution of the subsampled log-likelihood to\ndetermine the required subsample size in each MCMC iteration for a given level\nof precision. This formula is used to develop an adaptive version of the MLO\nsubsampled MCMC algorithm. Numerical experiments demonstrate that the proposed\nmethod outperforms the uniform subsampled MCMC.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 04:13:12 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Hu", "Guanyu", ""], ["Wang", "HaiYing", ""]]}, {"id": "2005.11499", "submitter": "Shinji Kakinaka SK", "authors": "Shinji Kakinaka, Ken Umeno", "title": "Flexible Two-point Selection Approach for Characteristic Function-based\n  Parameter Estimation of Stable Laws", "comments": "15 pages, 7 figures", "journal-ref": "Chaos 30 (2020) 073128", "doi": "10.1063/5.0013148", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable distribution is one of the attractive models that well describes\nfat-tail behaviors and scaling phenomena in various scientific fields. The\napproach based upon the method of moments yields a simple procedure for\nestimating stable law parameters with the requirement of using momental points\nfor the characteristic function, but the selection of points is only poorly\nexplained and has not been elaborated. We propose a new characteristic\nfunction-based approach by introducing a technique of selecting plausible\npoints, which could bring the method of moments available for practical use.\nOur method outperforms other state-of-art methods that exhibit a closed-form\nexpression of all four parameters of stable laws. Finally, the applicability of\nthe method is illustrated by using several data of financial assets. Numerical\nresults reveal that our approach is advantageous when modeling empirical data\nwith stable distributions.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 09:19:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kakinaka", "Shinji", ""], ["Umeno", "Ken", ""]]}, {"id": "2005.11510", "submitter": "Ionas Erb", "authors": "Ionas Erb and Nihat Ay", "title": "The Information-Geometric Perspective of Compositional Data Analysis", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information geometry uses the formal tools of differential geometry to\ndescribe the space of probability distributions as a Riemannian manifold with\nan additional dual structure. The formal equivalence of compositional data with\ndiscrete probability distributions makes it possible to apply the same\ndescription to the sample space of Compositional Data Analysis (CoDA). The\nlatter has been formally described as a Euclidean space with an orthonormal\nbasis featuring components that are suitable combinations of the original\nparts. In contrast to the Euclidean metric, the information-geometric\ndescription singles out the Fisher information metric as the only one keeping\nthe manifold's geometric structure invariant under equivalent representations\nof the underlying random variables. Well-known concepts that are valid in\nEuclidean coordinates, e.g., the Pythogorean theorem, are generalized by\ninformation geometry to corresponding notions that hold for more general\ncoordinates. In briefly reviewing Euclidean CoDA and, in more detail, the\ninformation-geometric approach, we show how the latter justifies the use of\ndistance measures and divergences that so far have received little attention in\nCoDA as they do not fit the Euclidean geometry favored by current thinking. We\nalso show how entropy and relative entropy can describe amalgamations in a\nsimple way, while Aitchison distance requires the use of geometric means to\nobtain more succinct relationships. We proceed to prove the information\nmonotonicity property for Aitchison distance. We close with some thoughts about\nnew directions in CoDA where the rich structure that is provided by information\ngeometry could be exploited.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 10:36:50 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 22:40:32 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 09:14:11 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Erb", "Ionas", ""], ["Ay", "Nihat", ""]]}, {"id": "2005.11528", "submitter": "Sorawit Saengkyongam", "authors": "Sorawit Saengkyongam and Ricardo Silva", "title": "Learning Joint Nonlinear Effects from Single-variable Interventions in\n  the Presence of Hidden Confounders", "comments": "Accepted to The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to estimate the effect of multiple simultaneous\ninterventions in the presence of hidden confounders. To overcome the problem of\nhidden confounding, we consider the setting where we have access to not only\nthe observational data but also sets of single-variable interventions in which\neach of the treatment variables is intervened on separately. We prove\nidentifiability under the assumption that the data is generated from a\nnonlinear continuous structural causal model with additive Gaussian noise. In\naddition, we propose a simple parameter estimation method by pooling all the\ndata from different regimes and jointly maximizing the combined likelihood. We\nalso conduct comprehensive experiments to verify the identifiability result as\nwell as to compare the performance of our approach against a baseline on both\nsynthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 12:52:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 06:20:09 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Saengkyongam", "Sorawit", ""], ["Silva", "Ricardo", ""]]}, {"id": "2005.11586", "submitter": "Sandra E. Safo", "authors": "Thierry Chekouo and Sandra E. Safo", "title": "Bayesian Integrative Analysis and Prediction with Application to\n  Atherosclerosis Cardiovascular Disease", "comments": "48 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular diseases (CVD), including atherosclerosis CVD (ASCVD), are\nmultifactorial diseases that present a major economic and social burden\nworldwide. Tremendous efforts have been made to understand traditional risk\nfactors for ASCVD, but these risk factors account for only about half of all\ncases of ASCVD. It remains a critical need to identify nontraditional risk\nfactors (e.g., genetic variants, genes) contributing to ASCVD. Further,\nincorporating functional knowledge in prediction models have the potential to\nreveal pathways associated with disease risk. We propose Bayesian hierarchical\nfactor analysis models that associate multiple omics data, predict a clinical\noutcome, allow for prior functional information, and can accommodate clinical\ncovariates. The models, motivated by available data and the need for other risk\nfactors of ASCVD, are used for the integrative analysis of clinical,\ndemographic, and multi-omics data to identify genetic variants, genes, and gene\npathways potentially contributing to 10-year ASCVD risk in healthy adults. Our\nfindings revealed several genetic variants, genes and gene pathways that were\nhighly associated with ASCVD risk. Interestingly, some of these have been\nimplicated in CVD risk. The others could be explored for their potential roles\nin CVD. Our findings underscore the merit in joint association and prediction\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 18:50:44 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Chekouo", "Thierry", ""], ["Safo", "Sandra E.", ""]]}, {"id": "2005.11599", "submitter": "Wennan Chang", "authors": "Wennan Chang, Xinyu Zhou, Yong Zang, Chi Zhang, Sha Cao", "title": "Component-wise Adaptive Trimming For Robust Mixture Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Parameter estimation of mixture regression model using the expectation\nmaximization (EM) algorithm is highly sensitive to outliers. Here we propose a\nfast and efficient robust mixture regression algorithm, called Component-wise\nAdaptive Trimming (CAT) method. We consider simultaneous outlier detection and\nrobust parameter estimation to minimize the effect of outlier contamination.\nRobust mixture regression has many important applications including in human\ncancer genomics data, where the population often displays strong heterogeneity\nadded by unwanted technological perturbations. Existing robust mixture\nregression methods suffer from outliers as they either conduct parameter\nestimation in the presence of outliers, or rely on prior knowledge of the level\nof outlier contamination. CAT was implemented in the framework of\nclassification expectation maximization, under which a natural definition of\noutliers could be derived. It implements a least trimmed squares (LTS) approach\nwithin each exclusive mixing component, where the robustness issue could be\ntransformed from the mixture case to simple linear regression case. The high\nbreakdown point of the LTS approach allows us to avoid the pre-specification of\ntrimming parameter. Compared with multiple existing algorithms, CAT is the most\ncompetitive one that can handle and adaptively trim off outliers as well as\nheavy tailed noise, in different scenarios of simulated data and real genomic\ndata. CAT has been implemented in an R package `RobMixReg' available in CRAN.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 20:59:16 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 05:02:57 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 15:54:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chang", "Wennan", ""], ["Zhou", "Xinyu", ""], ["Zang", "Yong", ""], ["Zhang", "Chi", ""], ["Cao", "Sha", ""]]}, {"id": "2005.11641", "submitter": "Tudor Manole", "authors": "Tudor Manole, Abbas Khalili", "title": "Estimating the Number of Components in Finite Mixture Models via the\n  Group-Sort-Fuse Procedure", "comments": "79 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the number of components (or order) of a finite mixture model\nis a long standing and challenging problem in statistics. We propose the\nGroup-Sort-Fuse (GSF) procedure---a new penalized likelihood approach for\nsimultaneous estimation of the order and mixing measure in multidimensional\nfinite mixture models. Unlike methods which fit and compare mixtures with\nvarying orders using criteria involving model complexity, our approach directly\npenalizes a continuous function of the model parameters. More specifically,\ngiven a conservative upper bound on the order, the GSF groups and sorts mixture\ncomponent parameters to fuse those which are redundant. For a wide range of\nfinite mixture models, we show that the GSF is consistent in estimating the\ntrue mixture order and achieves the $n^{-1/2}$ convergence rate for parameter\nestimation up to polylogarithmic factors. The GSF is implemented for several\nunivariate and multivariate mixture models in the R package GroupSortFuse. Its\nfinite sample performance is supported by a thorough simulation study, and its\napplication is illustrated on two real data examples.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 02:38:12 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Manole", "Tudor", ""], ["Khalili", "Abbas", ""]]}, {"id": "2005.11785", "submitter": "Saverio Ranciati", "authors": "Saverio Ranciati, Alberto Roverato, Alessandra Luati", "title": "Fused graphical lasso for brain networks with symmetries", "comments": "27 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging is the growing area of neuroscience devoted to produce data with\nthe goal of capturing processes and dynamics of the human brain. We consider\nthe problem of inferring the brain connectivity network from time dependent\nfunctional magnetic resonance imaging (fMRI) scans. To this aim we propose the\nsymmetric graphical lasso, a penalized likelihood method with a fused type\npenalty function that takes into explicit account the natural symmetrical\nstructure of the brain. Symmetric graphical lasso allows one to learn\nsimultaneously both the network structure and a set of symmetries across the\ntwo hemispheres. We implement an alternating directions method of multipliers\nalgorithm to solve the corresponding convex optimization problem. Furthermore,\nwe apply our methods to estimate the brain networks of two subjects, one\nhealthy and the other affected by a mental disorder, and to compare them with\nrespect to their symmetric structure. The method applies once the temporal\ndependence characterising fMRI data has been accounted for and we compare the\nimpact on the analysis of different detrending techniques on the estimated\nbrain networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:45:52 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 14:26:46 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ranciati", "Saverio", ""], ["Roverato", "Alberto", ""], ["Luati", "Alessandra", ""]]}, {"id": "2005.11805", "submitter": "John Paige", "authors": "John Paige, Geir-Arne Fuglstad, Andrea Riebler, and Jon Wakefield", "title": "Bayesian Multiresolution Modeling Of Georeferenced Data", "comments": "main manuscript: 33 pages, 7 figures, 2 tables; supplemental\n  materials: 9 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current implementations of multiresolution methods are limited in terms of\npossible types of responses and approaches to inference. We provide a\nmultiresolution approach for spatial analysis of non-Gaussian responses using\nlatent Gaussian models and Bayesian inference via integrated nested Laplace\napproximation (INLA). The approach builds on `LatticeKrig', but uses a\nreparameterization of the model parameters that is intuitive and interpretable\nso that modeling and prior selection can be guided by expert knowledge about\nthe different spatial scales at which dependence acts. The priors can be used\nto make inference robust and integration over model parameters allows for more\naccurate posterior estimates of uncertainty. The extended LatticeKrig (ELK)\nmodel is compared to a standard implementation of LatticeKrig (LK), and a\nstandard Mat\\'ern model, and we find modest improvement in spatial\noversmoothing and prediction for the ELK model for counts of secondary\neducation completion for women in Kenya collected in the 2014 Kenya demographic\nhealth survey. Through a simulation study with Gaussian responses and a\nrealistic mix of short and long scale dependencies, we demonstrate that the\ndifferences between the three approaches for prediction increases with distance\nto nearest observation.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 17:22:49 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 01:39:43 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Paige", "John", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""], ["Wakefield", "Jon", ""]]}, {"id": "2005.11975", "submitter": "Antonello Maruotti", "authors": "Alessio Farcomeni, Antonello Maruotti, Fabio Divino, Giovanna Jona\n  Lasinio, Gianfranco Lovison", "title": "An ensemble approach to short-term forecast of COVID-19 intensive care\n  occupancy in Italian Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of intensive care beds during the Covid-19 epidemic is\ncrucial to guarantee the best possible treatment to severely affected patients.\nIn this work we show a simple strategy for short-term prediction of Covid-19\nICU beds, that has proved very effective during the Italian outbreak in\nFebruary to May 2020. Our approach is based on an optimal ensemble of two\nsimple methods: a generalized linear mixed regression model which pools\ninformation over different areas, and an area-specific non-stationary integer\nautoregressive methodology. Optimal weights are estimated using a\nleave-last-out rationale. The approach has been set up and validated during the\nepidemic in Italy. A report of its performance for predicting ICU occupancy at\nRegional level is included.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 08:37:33 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Farcomeni", "Alessio", ""], ["Maruotti", "Antonello", ""], ["Divino", "Fabio", ""], ["Lasinio", "Giovanna Jona", ""], ["Lovison", "Gianfranco", ""]]}, {"id": "2005.12017", "submitter": "Shu Yang", "authors": "Lili Wu, Shu Yang, Brian J. Reich, Ana G. Rappold", "title": "Estimating spatially varying health effects in app-based citizen science\n  research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildland fire smoke exposures present an increasingly severe threat to public\nhealth, and thus there is a growing need for studying the effects of protective\nbehaviors on improving health. Emerging smartphone applications provide\nunprecedented opportunities to study this important problem, but also pose\nnovel challenges. Smoke Sense, a citizen science project, provides an\ninteractive platform for participants to engage with a smartphone app that\nrecords air quality, health symptoms, and behaviors taken to reduce smoke\nexposures. We propose a new, doubly robust estimator of the structural nested\nmean model that accounts for spatially- and time-varying effects via a local\nestimating equation approach with geographical kernel weighting. Moreover, our\nanalytical framework is flexible enough to handle informative missingness by\ninverse probability weighting of estimating functions. We evaluate the new\nmethod using extensive simulation studies and apply it to Smoke Sense survey\ndata collected from smartphones for a better understanding of the relationship\nbetween smoke preventive measures and health effects. Our results estimate how\nthe protective behaviors' effects vary over space and time and find that\nprotective behaviors have more significant effects on reducing health symptoms\nin the Southwest than the Northwest region of the USA.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 10:30:17 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 20:14:30 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Wu", "Lili", ""], ["Yang", "Shu", ""], ["Reich", "Brian J.", ""], ["Rappold", "Ana G.", ""]]}, {"id": "2005.12068", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Erica Ponzi, Torkjel Sandanger, Magne Thoresen", "title": "Robust Sure Independence Screening for Non-polynomial dimensional\n  Generalized Linear Models", "comments": "Pre-print; Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable screening in ultra-high dimensional\ngeneralized linear models (GLMs) of non-polynomial orders. Since the popular\nSIS approach is extremely unstable in the presence of contamination and noise,\nwe discuss a new robust screening procedure based on the minimum density power\ndivergence estimator (MDPDE) of the marginal regression coefficients. Our\nproposed screening procedure performs well under pure and contaminated data\nscenarios. We provide a theoretical motivation for the use of marginal MDPDEs\nfor variable screening from both population as well as sample aspects; in\nparticular, we prove that the marginal MDPDEs are uniformly consistent leading\nto the sure screening property of our proposed algorithm. Finally, we propose\nan appropriate MDPDE based extension for robust conditional screening in GLMs\nalong with the derivation of its sure screening property. Our proposed methods\nare illustrated through extensive numerical studies along with an interesting\nreal data application.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 12:18:10 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 12:48:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Ponzi", "Erica", ""], ["Sandanger", "Torkjel", ""], ["Thoresen", "Magne", ""]]}, {"id": "2005.12101", "submitter": "Enrico Zorzetto", "authors": "Enrico Zorzetto, Antonio Canale, Marco Marani", "title": "Bayesian non-asymptotic extreme value models for environmental data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the analysis of extreme rainfall data, we introduce a general\nBayesian hierarchical model for estimating the probability distribution of\nextreme values of intermittent random sequences, a common problem in\ngeophysical and environmental science settings. The approach presented here\nrelaxes the asymptotic assumption typical of the traditional extreme value (EV)\ntheory, and accounts for the possible underlying variability in the\ndistribution of event magnitudes and occurrences, which are described through a\nlatent temporal process. Focusing on daily rainfall extremes, the structure of\nthe proposed model lends itself to incorporating prior geo-physical\nunderstanding of the rainfall process. By means of an extensive simulation\nstudy, we show that this methodology can significantly reduce estimation\nuncertainty with respect to Bayesian formulations of traditional asymptotic EV\nmethods, particularly in the case of relatively small samples. The benefits of\nthe approach are further illustrated with an application to a large data set of\n479 long daily rainfall historical records from across the continental United\nStates. By comparing measures of in-sample and out-of-sample predictive\naccuracy, we find that the model structure developed here, combined with the\nuse of all available observations for inference, significantly improves\nrobustness with respect to overfitting to the specific sample.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:35:50 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zorzetto", "Enrico", ""], ["Canale", "Antonio", ""], ["Marani", "Marco", ""]]}, {"id": "2005.12168", "submitter": "Blanka Szeitl", "authors": "Blanka Szeitl, Tam\\'as Rudas", "title": "Reducing Variance with Sample Allocation Based on Expected Response\n  Rates", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several techniques exist to assess and reduce nonresponse bias, including\npropensity models, calibration methods, or post-stratification. These\napproaches can only be applied after the data collection, and assume reliable\ninformation regarding unit nonresponse patterns for the entire population. In\nthis paper, we demonstrate that sample allocation taking into account the\nexpected response rates (ERR) have advantages in this context. The performance\nof ERR allocation is assessed by comparing the variances of estimates obtained\nthose arising from a classical allocation proportional to size (PS) and then\napplying post-stratification. The main theoretical tool is asymptotic\ncalculations using the delta-method, and these are complemented with extensive\nsimulations. The main finding is that the ERR allocation leads to lower\nvariances than the PS allocation, when the response rates are correctly\nspecified, and also under a wide range of conditions when the response rates\ncan not be correctly specified in advance.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 15:31:41 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Szeitl", "Blanka", ""], ["Rudas", "Tam\u00e1s", ""]]}, {"id": "2005.12172", "submitter": "Changbao Wu", "authors": "Puying Zhao, J.N.K. Rao, Changbao Wu", "title": "Empirical Likelihood Inference With Public-Use Survey Data", "comments": "50 pages, including 11 pages of tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public-use survey data are an important source of information for researchers\nin social science and health studies to build statistical models and make\ninferences on the target finite population. This paper presents two general\ninferential tools through the pseudo empirical likelihood and the sample\nempirical likelihood methods. Theoretical results on point estimation and\nlinear or nonlinear hypothesis tests involving parameters defined through\nestimating equations are established, and practical issues with the\nimplementation of the proposed methods are discussed. Results from simulation\nstudies and an application to the 2016 General Social Survey dataset of\nStatistics Canada show that the proposed methods work well under different\nscenarios. The inferential procedures and theoretical results presented in the\npaper make the empirical likelihood a practically useful tool for users of\ncomplex survey data.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 15:38:42 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhao", "Puying", ""], ["Rao", "J. N. K.", ""], ["Wu", "Changbao", ""]]}, {"id": "2005.12198", "submitter": "Minjie Wang", "authors": "Minjie Wang, Tianyi Yao, Genevera I. Allen", "title": "Supervised Convex Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering has long been a popular unsupervised learning approach to identify\ngroups of similar objects and discover patterns from unlabeled data in many\napplications. Yet, coming up with meaningful interpretations of the estimated\nclusters has often been challenging precisely due to its unsupervised nature.\nMeanwhile, in many real-world scenarios, there are some noisy supervising\nauxiliary variables, for instance, subjective diagnostic opinions, that are\nrelated to the observed heterogeneity of the unlabeled data. By leveraging\ninformation from both supervising auxiliary variables and unlabeled data, we\nseek to uncover more scientifically interpretable group structures that may be\nhidden by completely unsupervised analyses. In this work, we propose and\ndevelop a new statistical pattern discovery method named Supervised Convex\nClustering (SCC) that borrows strength from both information sources and guides\ntowards finding more interpretable patterns via a joint convex fusion penalty.\nWe develop several extensions of SCC to integrate different types of\nsupervising auxiliary variables, to adjust for additional covariates, and to\nfind biclusters. We demonstrate the practical advantages of SCC through\nsimulations and a case study on Alzheimer's Disease genomics. Specifically, we\ndiscover new candidate genes as well as new subtypes of Alzheimer's Disease\nthat can potentially lead to better understanding of the underlying genetic\nmechanisms responsible for the observed heterogeneity of cognitive decline in\nolder adults.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 16:12:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wang", "Minjie", ""], ["Yao", "Tianyi", ""], ["Allen", "Genevera I.", ""]]}, {"id": "2005.12225", "submitter": "Xavier D'Haultfoeuille", "authors": "Marianne Bl\\'ehaut, Xavier D'Haultfoeuille, J\\'er\\'emy L'Hour,\n  Alexandre B. Tsybakov", "title": "An alternative to synthetic control for models with many covariates\n  under sparsity", "comments": "39 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synthetic control method is a an econometric tool to evaluate causal\neffects when only one unit is treated. While initially aimed at evaluating the\neffect of large-scale macroeconomic changes with very few available control\nunits, it has increasingly been used in place of more well-known\nmicroeconometric tools in a broad range of applications, but its properties in\nthis context are unknown. This paper introduces an alternative to the synthetic\ncontrol method, which is developed both in the usual asymptotic framework and\nin the high-dimensional scenario. We propose an estimator of average treatment\neffect that is doubly robust, consistent and asymptotically normal. It is also\nimmunized against first-step selection mistakes. We illustrate these properties\nusing Monte Carlo simulations and applications to both standard and potentially\nhigh-dimensional settings, and offer a comparison with the synthetic control\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 16:56:45 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 13:34:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bl\u00e9haut", "Marianne", ""], ["D'Haultfoeuille", "Xavier", ""], ["L'Hour", "J\u00e9r\u00e9my", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "2005.12395", "submitter": "Davide Viviano Mr.", "authors": "Davide Viviano, Jelena Bradic", "title": "Fair Policy Targeting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major concerns of targeting interventions on individuals in social\nwelfare programs is discrimination: individualized treatments may induce\ndisparities on sensitive attributes such as age, gender, or race. This paper\naddresses the question of the design of fair and efficient treatment allocation\nrules. We adopt the non-maleficence perspective of \"first do no harm\": we\npropose to select the fairest allocation within the Pareto frontier. We provide\nenvy-freeness justifications to novel counterfactual notions of fairness. We\ndiscuss easy-to-implement estimators of the policy function, by casting the\noptimization into a mixed-integer linear program formulation. We derive regret\nbounds on the unfairness of the estimated policy function, and small sample\nguarantees on the Pareto frontier. Finally, we illustrate our method using an\napplication from education economics.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 20:45:25 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 17:53:09 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Viviano", "Davide", ""], ["Bradic", "Jelena", ""]]}, {"id": "2005.12427", "submitter": "Jonas B\\'eal", "authors": "Jonas B\\'eal, Aur\\'elien Latouche", "title": "Causal inference with multiple versions of treatment and application to\n  personalized medicine", "comments": "Around 12 pages, with 5 figures Associated GitHub repository:\n  https://github.com/JonasBeal/Causal_Precision_Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of high-throughput sequencing and targeted therapies has led\nto the emergence of personalized medicine: a patient's molecular profile or the\npresence of a specific biomarker of drug response will correspond to a\ntreatment recommendation made either by a physician or by a treatment\nassignment algorithm. The growing number of such algorithms raises the question\nof how to quantify their clinical impact knowing that a personalized medicine\nstrategy will inherently include different versions of treatment.\n  We thus specify an appropriate causal framework with multiple versions of\ntreatment to define the causal effects of interest for precision medicine\nstrategies and estimate them emulating clinical trials with observational data.\nTherefore, we determine whether the treatment assignment algorithm is more\nefficient than different control arms: gold standard treatment, observed\ntreatments or random assignment of targeted treatments.\n  Causal estimates of the precision medicine effects are first evaluated on\nsimulated data and they demonstrate a lower biases and variances compared with\nnaive estimation of the difference in expected outcome between treatment arms.\nThe various simulations scenarios also point out the different bias sources\ndepending on the clinical situation (heterogeneity of response, assignment of\nobserved treatments etc.). A RShiny interactive application is also provided to\nfurther explore other user-defined scenarios. The method is then applied to\ndata from patient-derived xenografts (PDX): each patient tumour is implanted in\nseveral immunodeficient cloned mice later treated with different drugs, thus\nproviding access to all corresponding drug sensitivities for all patients.\nAccess to these unique pre-clinical data emulating counterfactual outcomes\nallows to validate the reliability of causal estimates obtained with the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 22:08:10 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["B\u00e9al", "Jonas", ""], ["Latouche", "Aur\u00e9lien", ""]]}, {"id": "2005.12556", "submitter": "Dominik Wied", "authors": "Rafael Wei{\\ss}bach and Dominik Wied", "title": "Truncating the Exponential with a Uniform Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a sample of Exponentially distributed durations we aim at point\nestimation and a confidence interval for its parameter. A duration is only\nobserved if it has ended within a certain time interval, determined by a\nUniform distribution. Hence, the data is a truncated empirical process that we\ncan approximate by a Poisson process when only a small portion of the sample is\nobserved, as is the case for our applications. We derive the likelihood from\nstandard arguments for point processes, acknowledging the size of the latent\nsample as the second parameter, and derive the maximum likelihood estimator for\nboth. Consistency and asymptotic normality of the estimator for the Exponential\nparameter are derived from standard results on M-estimation. We compare the\ndesign with a simple random sample assumption for the observed durations. In\napplications from the social and economic sciences and in simulations, we find\na moderately increased standard error, apparently due to the stochastic\ndependence of units in the truncated sample.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 07:50:24 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wei\u00dfbach", "Rafael", ""], ["Wied", "Dominik", ""]]}, {"id": "2005.12601", "submitter": "Thomas Berrett", "authors": "Thomas B. Berrett and Cristina Butucea", "title": "Locally private non-asymptotic testing of discrete distributions is\n  faster using interactive mechanisms", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find separation rates for testing multinomial or more general discrete\ndistributions under the constraint of local differential privacy. We construct\nefficient randomized algorithms and test procedures, in both the case where\nonly non-interactive privacy mechanisms are allowed and also in the case where\nall sequentially interactive privacy mechanisms are allowed. The separation\nrates are faster in the latter case. We prove general information theoretical\nbounds that allow us to establish the optimality of our algorithms among all\npairs of privacy mechanisms and test procedures, in most usual cases.\nConsidered examples include testing uniform, polynomially and exponentially\ndecreasing distributions.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 09:43:08 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Butucea", "Cristina", ""]]}, {"id": "2005.12620", "submitter": "Masahiro Tanaka", "authors": "Masahiro Tanaka", "title": "On the Likelihood of Local Projection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A local projection model is defined by a set of linear regressions that\naccount for the associations between exogenous variables and an endogenous\nvariable observed at different time points. While it is standard practice to\nseparately estimate individual regressions using the ordinary least squares\nestimator, some recent studies treat a local projection model as a multivariate\nregression with correlated errors, i.e., seemingly unrelated regressions, and\npropose Bayesian and non-Bayesian methods to improve the estimation accuracy.\nHowever, it is not clear how and when this way of treatment of local projection\nmodels is justified. The primary purpose of this paper is to fill this gap by\nshowing that the likelihood of local projection models can be analytically\nderived from a stationary vector moving average process. By means of numerical\nexperiments, we confirm that this treatment of local projections is tenable for\nfinite samples.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 10:42:14 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 07:42:51 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tanaka", "Masahiro", ""]]}, {"id": "2005.12641", "submitter": "Hidetoshi Matsui", "authors": "Hidetoshi Matsui", "title": "Varying-coefficient functional additive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the varying coefficient functional linear model to the nonlinear\nmodel and propose a varying coefficient functional additive model. The proposed\nmethod can represent the relationship between functional predictors and a\nscalar response where the response depends on an exogenous variable.It captures\nthe nonlinear structure between variables and also provides interpretable\nrelationship of them. The model is estimated through basis expansions and\npenalized likelihood method, and then the tuning parameters included at the\nestimation procedure are selected by a model selection criterion. Simulation\nstudies are provided to show the effectiveness of the proposed method. We also\napply it to the analysis of crop yield data and then investigate how and when\nthe environmental factor relates to the amount of the crop yield.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 11:44:38 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Matsui", "Hidetoshi", ""]]}, {"id": "2005.12647", "submitter": "Christian Truden", "authors": "Konstantin Posch, Christian Truden, Philipp Hungerl\\\"ander, J\\\"urgen\n  Pilz", "title": "A Bayesian Approach for Predicting Food and Beverage Sales in Staff\n  Canteens and Restaurants", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijforecast.2021.06.001", "report-no": null, "categories": "stat.AP cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate demand forecasting is one of the key aspects for successfully\nmanaging restaurants and staff canteens. In particular, properly predicting\nfuture sales of menu items allows a precise ordering of food stock. From an\nenvironmental point of view, this ensures maintaining a low level of\npre-consumer food waste, while from the managerial point of view, this is\ncritical to guarantee the profitability of the restaurant. Hence, we are\ninterested in predicting future values of the daily sold quantities of given\nmenu items. The corresponding time series show multiple strong seasonalities,\ntrend changes, data gaps, and outliers. We propose a forecasting approach that\nis solely based on the data retrieved from Point of Sales systems and allows\nfor a straightforward human interpretation. Therefore, we propose two\ngeneralized additive models for predicting the future sales. In an extensive\nevaluation, we consider two data sets collected at a casual restaurant and a\nlarge staff canteen consisting of multiple time series, that cover a period of\n20 months, respectively. We show that the proposed models fit the features of\nthe considered restaurant data. Moreover, we compare the predictive performance\nof our method against the performance of other well-established forecasting\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 12:05:06 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 16:07:44 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 13:06:55 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Posch", "Konstantin", ""], ["Truden", "Christian", ""], ["Hungerl\u00e4nder", "Philipp", ""], ["Pilz", "J\u00fcrgen", ""]]}, {"id": "2005.12784", "submitter": "Tenglong Li", "authors": "Tenglong Li and Kenneth A. Frank", "title": "The probability of a robust inference for internal validity and its\n  applications in regression models", "comments": "arXiv admin note: substantial text overlap with arXiv:1906.08726", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal validity of observational study is often subject to debate. In\nthis study, we define the unobserved sample based on the counterfactuals and\nformalize its relationship with the null hypothesis statistical testing (NHST)\nfor regression models. The probability of a robust inference for internal\nvalidity, i.e., the PIV, is the probability of rejecting the null hypothesis\nagain based on the ideal sample which is defined as the combination of the\nobserved and unobserved samples, provided the same null hypothesis has already\nbeen rejected for the observed sample. When the unconfoundedness assumption is\ndubious, one can bound the PIV of an inference based on bounded belief about\nthe mean counterfactual outcomes, which is often needed in this case.\nEssentially, the PIV is statistical power of the NHST that is thought to be\nbuilt on the ideal sample. We summarize the process of evaluating internal\nvalidity with the PIV into a six-step procedure and illustrate it with an\nempirical example (i.e., Hong and Raudenbush (2005)).\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 03:29:19 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Li", "Tenglong", ""], ["Frank", "Kenneth A.", ""]]}, {"id": "2005.12822", "submitter": "Carmen Armero", "authors": "Carmen Armero", "title": "Bayesian joint models for longitudinal and survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes a quick look at Bayesian joint models (BJM) for longitudinal\nand survival data. A general formulation for BJM is examined in terms of the\nsampling distribution of the longitudinal and survival processes, the\nconditional distribution of the random effects and the prior distribution. Next\na basic BJM defined in terms of a mixed linear model and a Cox survival\nregression models is discussed and some extensions and other Bayesian topics\nare briefly outlined.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:56:45 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Armero", "Carmen", ""]]}, {"id": "2005.12993", "submitter": "Donghui Yan", "authors": "Donghui Yan, Ying Xu, Pei Wang", "title": "Estimating the Number of Infected Cases in COVID-19 Pandemic", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The COVID-19 pandemic has caused major disturbance to human life. An\nimportant reason behind the widespread social anxiety is the huge uncertainty\nabout the pandemic. A fundamental uncertainty is how many or what percentage of\npeople have been infected. There are published and frequently updated data on\nvarious statistics of the pandemic, at local, country or global level. However,\ndue to various reasons, many cases were not included in those reported numbers.\nWe propose a structured approach for the estimation of the number of unreported\ncases, where we distinguish cases that arrive late in the reported numbers and\nthose who had mild or no symptoms and thus were not captured by any medical\nsystem at all. We use post-report data for the estimation of the former and\npopulation matching to the latter. We estimate that the reported number of\ninfected cases in the US should be corrected by multiplying a factor of 220.54%\nas of Apr 20, 2020, while the infection ratio out of the US population is\nestimated to be 0.53%, implying a case mortality rate at 2.85% which is close\nto the 3.4% suggested by the WHO in Mar 2020. Towards the end of the summer of\n2020, the overall infection ratio of the US rises to 2.49% while the case\nmortality decreases to 2.09%, and the ratio of asymptomatic cases out of all\ninfected cases reduces from the pre-summer 35-40% to around 20-25%.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 22:19:43 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 13:53:05 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yan", "Donghui", ""], ["Xu", "Ying", ""], ["Wang", "Pei", ""]]}, {"id": "2005.13001", "submitter": "Zekun Xu", "authors": "Zekun Xu, Eric Laber, Ana-Maria Staicu, Emanuel Severus", "title": "Latent-state models for precision medicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational longitudinal studies are a common means to study treatment\nefficacy and safety in chronic mental illness. In many such studies, treatment\nchanges may be initiated by either the patient or by their clinician and can\nthus vary widely across patients in their timing, number, and type. Indeed, in\nthe observational longitudinal pathway of the STEP-BD study of bipolar\ndepression, one of the motivations for this work, no two patients have the same\ntreatment history even after coarsening clinic visits to a weekly time-scale.\nEstimation of an optimal treatment regime using such data is challenging as one\ncannot naively pool together patients with the same treatment history, as is\nrequired by methods based on inverse probability weighting, nor is it possible\nto apply backwards induction over the decision points, as is done in Q-learning\nand its variants. Thus, additional structure is needed to effectively pool\ninformation across patients and within a patient over time. Current scientific\ntheory for many chronic mental illnesses maintains that a patient's disease\nstatus can be conceptualized as transitioning among a small number of discrete\nstates. We use this theory to inform the construction of a partially observable\nMarkov decision process model of patient health trajectories wherein observed\nhealth outcomes are dictated by a patient's latent health state. Using this\nmodel, we derive and evaluate estimators of an optimal treatment regime under\ntwo common paradigms for quantifying long-term patient health. The finite\nsample performance of the proposed estimator is demonstrated through a series\nof simulation experiments and application to the observational pathway of the\nSTEP-BD study. We find that the proposed method provides high-quality estimates\nof an optimal treatment strategy in settings where existing approaches cannot\nbe applied without ad hoc modifications.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 19:52:10 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 16:00:18 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Xu", "Zekun", ""], ["Laber", "Eric", ""], ["Staicu", "Ana-Maria", ""], ["Severus", "Emanuel", ""]]}, {"id": "2005.13011", "submitter": "Sara Algeri", "authors": "Sara Algeri and Xiangyu Zhang", "title": "Exhaustive goodness-of-fit via smoothed inference and graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical tests of goodness-of-fit aim to validate the conformity of a\npostulated model to the data under study. Given their inferential nature, they\ncan be considered a crucial step in confirmatory data analysis. In their\nstandard formulation, however, they do not allow exploring how the hypothesized\nmodel deviates from the truth nor do they provide any insight into how the\nrejected model could be improved to better fit the data. The main goal of this\nwork is to establish a comprehensive framework for goodness-of-fit which\nnaturally integrates modeling, estimation, inference, and graphics. Modeling\nand estimation focus on a novel formulation of smooth tests that easily extends\nto arbitrary distributions, either continuous or discrete. Inference and\nadequate post-selection adjustments are performed via a specially designed\nsmoothed bootstrap and the results are summarized via an exhaustive graphical\ntool called CD-plot.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:14:17 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 18:21:35 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Algeri", "Sara", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2005.13181", "submitter": "Riko Kelter", "authors": "Riko Kelter", "title": "How to choose between different Bayesian posterior indices for\n  hypothesis testing in practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing is an essential statistical method in psychology and the\ncognitive sciences. The problems of traditional null hypothesis significance\ntesting (NHST) have been discussed widely, and among the proposed solutions to\nthe replication problems caused by the inappropriate use of significance tests\nand $p$-values is a shift towards Bayesian data analysis. However, Bayesian\nhypothesis testing is concerned with various posterior indices for significance\nand the size of an effect. This complicates Bayesian hypothesis testing in\npractice, as the availability of multiple Bayesian alternatives to the\ntraditional $p$-value causes confusion which one to select and why. In this\npaper, we compare various Bayesian posterior indices which have been proposed\nin the literature and discuss their benefits and limitations. Our comparison\nshows that conceptually not all proposed Bayesian alternatives to NHST and\n$p$-values are beneficial, and the usefulness of some indices strongly depends\non the study design and research goal. However, our comparison also reveals\nthat there exist at least two candidates among the available Bayesian posterior\nindices which have appealing theoretical properties and are, to our best\nknowledge, widely underused among psychologists.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 06:03:04 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kelter", "Riko", ""]]}, {"id": "2005.13244", "submitter": "Euloge Clovis Kenne Pagui", "authors": "Vincenzo Gioia and Euloge Clovis Kenne Pagui and Alessandra Salvan", "title": "Median bias reduction in cumulative link models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel estimation approach for cumulative link models,\nbased on median bias reduction as developed in Kenne Pagui et al. (2017). The\nmedian bias reduced estimator is obtained as solution of an estimating equation\nbased on an adjustment of the score. It allows to obtain higher-order median\ncentering of maximum likelihood estimates without requiring their finiteness.\nMoreover, the estimator is equivariant under componentwise monotone\nreparameterizations and the method is effective in preventing boundary\nestimates. We evaluate the properties of the median bias reduced estimator\nthrough simulation studies and compare it with the two main competitors, the\nmaximum likelihood and the mean bias reduced (Firth, 1993) estimators. Finally,\nwe show an application where the proposed estimator is able to solve the\nboundary estimates problem.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 08:56:47 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 07:40:17 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Gioia", "Vincenzo", ""], ["Pagui", "Euloge Clovis Kenne", ""], ["Salvan", "Alessandra", ""]]}, {"id": "2005.13245", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "On the Monotonicity of a Nondifferentially Mismeasured Binary Confounder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are interested in the average causal effect of a binary\ntreatment on an outcome when this relationship is confounded by a binary\nconfounder. Suppose that the confounder is unobserved but a nondifferential\nproxy of it is observed. We show that, under certain monotonicity assumption\nthat is empirically verifiable, adjusting for the proxy produces a measure of\nthe effect that is between the unadjusted and the true measures.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 09:07:07 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 07:35:40 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 12:07:23 GMT"}, {"version": "v4", "created": "Fri, 26 Jun 2020 08:48:17 GMT"}, {"version": "v5", "created": "Fri, 3 Jul 2020 09:39:26 GMT"}, {"version": "v6", "created": "Sun, 2 Aug 2020 20:24:29 GMT"}, {"version": "v7", "created": "Tue, 4 Aug 2020 14:07:42 GMT"}, {"version": "v8", "created": "Fri, 14 Aug 2020 18:38:40 GMT"}, {"version": "v9", "created": "Sun, 23 Aug 2020 07:45:49 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2005.13261", "submitter": "Mia Tackney", "authors": "Mia S.Tackney and David C. Woods and Ilya Shpitser", "title": "Nonmyopic and pseudo-nonmyopic approaches to optimal sequential design\n  in the presence of covariates", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sequential experiments, subjects become available for the study over a\nperiod of time, and covariates are often measured at the time of arrival. We\nconsider the setting where the sample size is fixed but covariate values are\nunknown until subjects enrol. Given a model for the outcome, a sequential\noptimal design approach can be used to allocate treatments to minimize the\nvariance of the treatment effect. We extend existing optimal design methodology\nso it can be used within a nonmyopic framework, where treatment allocation for\nthe current subject depends not only on the treatments and covariates of the\nsubjects already enrolled in the study, but also the impact of possible future\ntreatment assignments. The nonmyopic approach is computationally expensive as\nit requires recursive formulae. We propose a pseudo-nonmyopic approach which\nhas a similar aim to the nonmyopic approach, but does not involve recursion and\ninstead relies on simulations of future possible decisions. Our simulation\nstudies show that the myopic approach is the most efficient for the logistic\nmodel case with a single binary covariate and binary treatment.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 09:57:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Tackney", "Mia S.", ""], ["Woods", "David C.", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2005.13271", "submitter": "Maja Pohar Perme", "authors": "Per Kragh Andersen, Maja Pohar Perme, Hans C van Houwelingen, Richard\n  J Cook, Pierre Joly, Torben Martinussen, Jeremy MG Taylor, Michal\n  Abrahamowicz, Terry M Therneau (for the STRATOS TG8 topic group)", "title": "Analysis of time-to-event for observational studies: Guidance to the use\n  of intensity models", "comments": "28 pages, 12 figures. For associated Supplementary material, see\n  http://publicifsv.sund.ku.dk/~pka/STRATOSTG8/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides guidance for researchers with some mathematical\nbackground on the conduct of time-to-event analysis in observational studies\nbased on intensity (hazard) models. Discussions of basic concepts like time\naxis, event definition and censoring are given. Hazard models are introduced,\nwith special emphasis on the Cox proportional hazards regression model. We\nprovide check lists that may be useful both when fitting the model and\nassessing its goodness of fit and when interpreting the results. Special\nattention is paid to how to avoid problems with immortal time bias by\nintroducing time-dependent covariates. We discuss prediction based on hazard\nmodels and difficulties when attempting to draw proper causal conclusions from\nsuch models. Finally, we present a series of examples where the methods and\ncheck lists are exemplified. Computational details and implementation using the\nfreely available R software are documented in Supplementary Material. The paper\nwas prepared as part of the STRATOS initiative.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 10:39:57 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 12:00:54 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Andersen", "Per Kragh", "", "for the STRATOS TG8 topic group"], ["Perme", "Maja Pohar", "", "for the STRATOS TG8 topic group"], ["van Houwelingen", "Hans C", "", "for the STRATOS TG8 topic group"], ["Cook", "Richard J", "", "for the STRATOS TG8 topic group"], ["Joly", "Pierre", "", "for the STRATOS TG8 topic group"], ["Martinussen", "Torben", "", "for the STRATOS TG8 topic group"], ["Taylor", "Jeremy MG", "", "for the STRATOS TG8 topic group"], ["Abrahamowicz", "Michal", "", "for the STRATOS TG8 topic group"], ["Therneau", "Terry M", "", "for the STRATOS TG8 topic group"]]}, {"id": "2005.13375", "submitter": "Adam Edwards", "authors": "Adam M. Edwards, Robert B. Gramacy", "title": "Precision Aggregated Local Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale Gaussian process (GP) regression is infeasible for larger data\nsets due to cubic scaling of flops and quadratic storage involved in working\nwith covariance matrices. Remedies in recent literature focus on\ndivide-and-conquer, e.g., partitioning into sub-problems and inducing\nfunctional (and thus computational) independence. Such approximations can be\nspeedy, accurate, and sometimes even more flexible than an ordinary GPs.\nHowever, a big downside is loss of continuity at partition boundaries. Modern\nmethods like local approximate GPs (LAGPs) imply effectively infinite\npartitioning and are thus pathologically good and bad in this regard. Model\naveraging, an alternative to divide-and-conquer, can maintain absolute\ncontinuity but often over-smooths, diminishing accuracy. Here we propose\nputting LAGP-like methods into a local experts-like framework, blending\npartition-based speed with model-averaging continuity, as a flagship example of\nwhat we call precision aggregated local models (PALM). Using $K$ LAGPs, each\nselecting $n$ from $N$ total data pairs, we illustrate a scheme that is at most\ncubic in $n$, quadratic in $K$, and linear in $N$, drastically reducing\ncomputational and storage demands. Extensive empirical illustration shows how\nPALM is at least as accurate as LAGP, can be much faster in terms of speed, and\nfurnishes continuous predictive surfaces. Finally, we propose sequential\nupdating scheme which greedily refines a PALM predictor up to a computational\nbudget.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 14:12:13 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Edwards", "Adam M.", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "2005.13388", "submitter": "Amanda Mejia", "authors": "Amanda F. Mejia, David Bolin, Yu Ryan Yue, Jiongran Wang, Brian S.\n  Caffo and Mary Beth Nebel", "title": "A spatial template independent component analysis model for\n  subject-level brain network estimation and inference", "comments": "32 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis is commonly applied to functional magnetic\nresonance imaging (fMRI) data to extract independent components (ICs)\nrepresenting functional brain networks. While ICA produces reliable group-level\nestimates, single-subject ICA often produces noisy results. Template ICA (tICA)\nis a hierarchical ICA model using empirical population priors to produce\nreliable subject-level IC estimates. However, this and other hierarchical ICA\nmodels assume unrealistically that subject effects are spatially independent.\nHere, we propose spatial template ICA (stICA), which incorporates spatial\nprocess priors into tICA. This results in greater estimation efficiency of ICs\nand subject effects. Additionally, the joint posterior distribution can be used\nto identify engaged areas using an excursions set approach. By leveraging\nspatial dependencies and avoiding massive multiple comparisons, stICA has high\npower to detect true effects. We derive an efficient expectation-maximization\nalgorithm to obtain maximum likelihood estimates of the model parameters and\nposterior moments of the latent fields. Based on analysis of simulated data and\nfMRI data from the Human Connectome Project, we find that stICA produces\nestimates that are more accurate and reliable than benchmark approaches, and\nidentifies larger and more reliable areas of engagement. The algorithm is quite\ntractable, achieving convergence within 7 hours in our fMRI analysis.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 14:41:08 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 16:24:38 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Mejia", "Amanda F.", ""], ["Bolin", "David", ""], ["Yue", "Yu Ryan", ""], ["Wang", "Jiongran", ""], ["Caffo", "Brian S.", ""], ["Nebel", "Mary Beth", ""]]}, {"id": "2005.13396", "submitter": "Davide Ravagli", "authors": "Davide Ravagli, Georgi N. Boshnakov", "title": "Portfolio optimization with mixture vector autoregressive models", "comments": "19 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining reliable estimates of conditional covariance matrices is an\nimportant task of heteroskedastic multivariate time series. In portfolio\noptimization and financial risk management, it is crucial to provide measures\nof uncertainty and risk as accurately as possible. We propose using mixture\nvector autoregressive (MVAR) models for portfolio optimization. Combining a\nmixture of distributions that depend on the recent history of the process, MVAR\nmodels can accommodate asymmetry, multimodality, heteroskedasticity and\ncross-correlation in multivariate time series data. For mixtures of Normal\ncomponents, we exploit a property of the multivariate Normal distribution to\nobtain explicit formulas of conditional predictive distributions of returns on\na portfolio of assets. After showing how the method works, we perform a\ncomparison with other relevant multivariate time series models on real stock\nreturn data.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 14:50:29 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ravagli", "Davide", ""], ["Boshnakov", "Georgi N.", ""]]}, {"id": "2005.13596", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep (DEEP) Mukhopadhyay and Kaijun Wang", "title": "Breiman's \"Two Cultures\" Revisited and Reconciled", "comments": "This paper celebrates the 70th anniversary of Statistical Machine\n  Learning--- how far we've come, and how far we have to go. Keywords:\n  Integrated statistical learning theory, Exploratory machine learning,\n  Uncertainty prediction machine, ML-powered modern applied statistics,\n  Information theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a landmark paper published in 2001, Leo Breiman described the tense\nstandoff between two cultures of data modeling: parametric statistical and\nalgorithmic machine learning. The cultural division between these two\nstatistical learning frameworks has been growing at a steady pace in recent\nyears. What is the way forward? It has become blatantly obvious that this\nwidening gap between \"the two cultures\" cannot be averted unless we find a way\nto blend them into a coherent whole. This article presents a solution by\nestablishing a link between the two cultures. Through examples, we describe the\nchallenges and potential gains of this new integrated statistical thinking.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 19:02:56 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Subhadeep", "", "", "DEEP"], ["Mukhopadhyay", "", ""], ["Wang", "Kaijun", ""]]}, {"id": "2005.13622", "submitter": "Akira Horiguchi", "authors": "Akira Horiguchi (1), Matthew T. Pratola (1), Thomas J. Santner (1)\n  ((1) Department of Statistics, The Ohio State University)", "title": "Assessing variable activity for Bayesian regression trees", "comments": "46 pages, 8 figures, submitted to the special issue \"Recent Advances\n  in Sensitivity Analysis of Model Outputs\" in the Reliability Engineering and\n  Safety System journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Additive Regression Trees (BART) are non-parametric models that can\ncapture complex exogenous variable effects. In any regression problem, it is\noften of interest to learn which variables are most active. Variable activity\nin BART is usually measured by counting the number of times a tree splits for\neach variable. Such one-way counts have the advantage of fast computations.\nDespite their convenience, one-way counts have several issues. They are\nstatistically unjustified, cannot distinguish between main effects and\ninteraction effects, and become inflated when measuring interaction effects. An\nalternative method well-established in the literature is Sobol' indices, a\nvariance-based global sensitivity analysis technique. However, these indices\noften require Monte Carlo integration, which can be computationally expensive.\nThis paper provides analytic expressions for Sobol' indices for BART posterior\nsamples. These expressions are easy to interpret and are computationally\nfeasible. Furthermore, we will show a fascinating connection between\nfirst-order (main-effects) Sobol' indices and one-way counts. We also introduce\na novel ranking method, and use this to demonstrate that the proposed indices\npreserve the Sobol'-based rank order of variable importance. Finally, we\ncompare these methods using analytic test functions and the En-ROADS climate\nimpacts simulator.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:12:10 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 02:47:05 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Horiguchi", "Akira", "", "Department of Statistics, The Ohio State University"], ["Pratola", "Matthew T.", "", "Department of Statistics, The Ohio State University"], ["Santner", "Thomas J.", "", "Department of Statistics, The Ohio State University"]]}, {"id": "2005.13631", "submitter": "Francois Vernotte", "authors": "Fran\\c{c}ois Vernotte, Siyuan Chen, Enrico Rubiola", "title": "Response and Uncertainty of the Parabolic Variance PVAR to Non-Integer\n  Exponents of the Power Law", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oscillator fluctuations are described as the phase or frequency noise\nspectrum, or in terms of a wavelet variance as a function of the measurement\ntime. The spectrum is generally approximated by the `power law,' i.e., a\nLaurent polynomial with integer exponents of the frequency. This article\nextends the domain of application of PVAR, a wavelet variance which uses the\nlinear regression on phase data to estimate the frequency, and called\n`parabolic' because such regression is equivalent to a parabolic-shaped weight\nfunction applied to frequency fluctuations. In turn, PVAR is relevant in that\nit improves on the widely-used Modified Allan variance (MVAR) enabling the\ndetection of the same noise processes at the same confidence level in a shorter\nmeasurement time. More specifically, we provide (i) the analytical expression\nof the response of the PVAR to the frequency-noise spectrum in the general case\nof non-integer exponents of the frequency, and (ii) a useful approximate\nexpression of the statistical uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:26:10 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 18:22:51 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 18:50:08 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Vernotte", "Fran\u00e7ois", ""], ["Chen", "Siyuan", ""], ["Rubiola", "Enrico", ""]]}, {"id": "2005.13719", "submitter": "Gyuhyeong Goh", "authors": "Gyuhyeong Goh and Jisang Yu", "title": "Synthetic control method with convex hull restrictions: A Bayesian\n  maximum a posteriori approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic control methods have gained popularity among causal studies with\nobservational data, particularly when estimating the impacts of the\ninterventions that are implemented to a small number of large units.\nImplementing the synthetic control methods faces two major challenges: a)\nestimating weights for each control unit to create a synthetic control and b)\nproviding statistical inferences. To overcome these challenges, we propose a\nBayesian framework that implements the synthetic control method with the\nparallelly shiftable convex hull and provides a useful Bayesian inference,\nwhich is drawn from the duality between a penalized least squares method and a\nBayesian Maximum A Posteriori (MAP) approach. Simulation results indicate that\nthe proposed method leads to smaller biases compared to alternatives. We apply\nour Bayesian method to the real data example of Abadie and Gardeazabal (2003)\nand find that the treatment effects are statistically significant during the\nsubset of the post-treatment period.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 00:49:59 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Goh", "Gyuhyeong", ""], ["Yu", "Jisang", ""]]}, {"id": "2005.13739", "submitter": "Tong Chen", "authors": "Tong Chen, Thomas Lumley", "title": "Optimal multi-wave sampling for regression modelling in two-phase\n  designs", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8760", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-phase designs involve measuring extra variables on a subset of the cohort\nwhere some variables are already measured. The goal of two-phase designs is to\nchoose a subsample of individuals from the cohort and analyse that subsample\nefficiently. It is of interest to obtain an optimal design that gives the most\nefficient estimates of regression parameters. In this paper, we propose a\nmulti-wave sampling design to approximate the optimal design for design-based\nestimators. Influences functions are used to compute the optimal sampling\nallocations. We propose to use informative priors on regression parameters to\nderive the wave-1 sampling probabilities because any pre-specified sampling\nprobabilities may be far from optimal and decrease efficiency. Generalised\nraking is used in statistical analysis. We show that a two-wave sampling with\nreasonable informative priors will end up with higher precision for the\nparameter of interest and be close to the underlying optimal design.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 02:09:05 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 01:39:45 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chen", "Tong", ""], ["Lumley", "Thomas", ""]]}, {"id": "2005.13794", "submitter": "Rizky Reza Fauzi", "authors": "Rizky Reza Fauzi, Yoshihiko Maesono", "title": "Boundary-free Kernel-smoothed Goodness-of-fit Tests for Data on General\n  Interval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose kernel-type smoothed Kolmogorov-Smirnov and Cram\\'{e}r-von Mises\ntests for data on general interval, using bijective transformations. Though not\nas severe as in the kernel density estimation, utilizing naive kernel method\ndirectly to those particular tests will result in boundary problem as well.\nThis happens mostly because the value of the naive kernel distribution function\nestimator is still larger than $0$ (or less than $1$) when it is evaluated at\nthe boundary points. This situation can increase the errors of the tests\nespecially the second-type error. In this article, we use bijective\ntransformations to eliminate the boundary problem. Some simulation results\nillustrating the estimator and the tests' performances will be presented in the\nlast part of this article.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:23:23 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Fauzi", "Rizky Reza", ""], ["Maesono", "Yoshihiko", ""]]}, {"id": "2005.13805", "submitter": "Rizky Reza Fauzi", "authors": "Rizky Reza Fauzi, Yoshihiko Maesono", "title": "Boundary-free Estimators of the Mean Residual Life Function by\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new kernel-type estimators of the mean residual life function\n$m_X(t)$ of bounded or half-bounded interval supported distributions. Though\nnot as severe as the boundary problems in the kernel density estimation,\neliminating the boundary bias problems that occur in the naive kernel estimator\nof the mean residual life function is needed. In this article, we utilize the\nproperty of bijective transformation. Furthermore, our proposed methods\npreserve the mean value property, which cannot be done by the naive kernel\nestimator. Some simulation results showing the estimators' performances and a\nreal data analysis will be presented in the last part of this article.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:54:37 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Fauzi", "Rizky Reza", ""], ["Maesono", "Yoshihiko", ""]]}, {"id": "2005.13912", "submitter": "Rem-Sophia Mouradi", "authors": "Rem-Sophia Mouradi, C\\'edric Goeury, Olivier Thual, Fabrice Zaoui and\n  Pablo Tassi", "title": "Physically interpretable machine learning algorithm on multidimensional\n  non-linear fields", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2020.110074", "report-no": null, "categories": "physics.comp-ph cs.LG physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an ever-increasing interest for Machine Learning (ML) and a favorable data\ndevelopment context, we here propose an original methodology for data-based\nprediction of two-dimensional physical fields. Polynomial Chaos Expansion\n(PCE), widely used in the Uncertainty Quantification community (UQ), has long\nbeen employed as a robust representation for probabilistic input-to-output\nmapping. It has been recently tested in a pure ML context, and shown to be as\npowerful as classical ML techniques for point-wise prediction. Some advantages\nare inherent to the method, such as its explicitness and adaptability to small\ntraining sets, in addition to the associated probabilistic framework.\nSimultaneously, Dimensionality Reduction (DR) techniques are increasingly used\nfor pattern recognition and data compression and have gained interest due to\nimproved data quality. In this study, the interest of Proper Orthogonal\nDecomposition (POD) for the construction of a statistical predictive model is\ndemonstrated. Both POD and PCE have amply proved their worth in their\nrespective frameworks. The goal of the present paper was to combine them for a\nfield-measurement-based forecasting. The described steps are also useful to\nanalyze the data. Some challenging issues encountered when using\nmultidimensional field measurements are addressed, for example when dealing\nwith few data. The POD-PCE coupling methodology is presented, with particular\nfocus on input data characteristics and training-set choice. A simple\nmethodology for evaluating the importance of each physical parameter is\nproposed for the PCE model and extended to the POD-PCE coupling.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 11:26:06 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:54:49 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Mouradi", "Rem-Sophia", ""], ["Goeury", "C\u00e9dric", ""], ["Thual", "Olivier", ""], ["Zaoui", "Fabrice", ""], ["Tassi", "Pablo", ""]]}, {"id": "2005.13988", "submitter": "Chong Gu", "authors": "Chong Gu", "title": "Composition Estimation via Shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we explore a simple approach to composition estimation, using\npenalized likelihood density estimation on a nominal discrete domain. Practical\nissues such as smoothing parameter selection and the use of prior information\nare investigated in simulations, and a theoretical analysis is attempted. The\nmethod has been implemented in a pair of R functions for use by practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:41:45 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Gu", "Chong", ""]]}, {"id": "2005.14025", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "copent: Estimating Copula Entropy and Transfer Entropy in R", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT cs.LG cs.MS math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical independence and conditional independence are two fundamental\nconcepts in statistics and machine learning. Copula Entropy is a mathematical\nconcept defined by Ma and Sun for multivariate statistical independence\nmeasuring and testing, and also proved to be closely related to conditional\nindependence (or transfer entropy). As the unified framework for measuring both\nindependence and causality, CE has been applied to solve several related\nstatistical or machine learning problems, including association discovery,\nstructure learning, variable selection, and causal discovery. The nonparametric\nmethods for estimating copula entropy and transfer entropy were also proposed\npreviously. This paper introduces copent, the R package which implements these\nproposed methods for estimating copula entropy and transfer entropy. The\nimplementation detail of the package is introduced. Three examples with\nsimulated data and real-world data on variable selection and causal discovery\nare also presented to demonstrate the usage of this package. The examples on\nvariable selection and causal discovery show the strong ability of copent on\ntesting (conditional) independence compared with the related packages. The\ncopent package is available on the Comprehensive R Archive Network (CRAN) and\nalso on GitHub at https://github.com/majianthu/copent.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 10:01:12 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 00:14:25 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 00:41:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "2005.14057", "submitter": "Andrii Babii", "authors": "Andrii Babii and Eric Ghysels and Jonas Striaukas", "title": "Machine Learning Time Series Regressions with an Application to\n  Nowcasting", "comments": "Portions of this work previously appeared as arXiv:1912.06307v1 which\n  has been split into two articles", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces structured machine learning regressions for\nhigh-dimensional time series data potentially sampled at different frequencies.\nThe sparse-group LASSO estimator can take advantage of such time series data\nstructures and outperforms the unstructured LASSO. We establish oracle\ninequalities for the sparse-group LASSO estimator within a framework that\nallows for the mixing processes and recognizes that the financial and the\nmacroeconomic data may have heavier than exponential tails. An empirical\napplication to nowcasting US GDP growth indicates that the estimator performs\nfavorably compared to other alternatives and that text data can be a useful\naddition to more traditional numerical data.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:42:58 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 00:50:12 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 19:53:54 GMT"}, {"version": "v4", "created": "Sat, 12 Dec 2020 18:30:09 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Babii", "Andrii", ""], ["Ghysels", "Eric", ""], ["Striaukas", "Jonas", ""]]}, {"id": "2005.14291", "submitter": "Robert L Obenchain PhD", "authors": "Robert L. Obenchain", "title": "Ridge TRACE Diagnostics", "comments": "Considerable text overlap with arXiv:2103.05161 plus use of a\n  less-efficient and more complicated shrinkage path", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new p-parameter generalized ridge-regression shrinkage-pattern\nrecently implemented in the RXshrink CRAN R-package. The five distinct types of\nridge TRACE displays discussed and illustrated here provide invaluable\ndata-analytic insights and improved self-confidence to researchers and data\nscientists fitting linear models to ill-conditioned datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 21:02:50 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 13:12:40 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 15:31:25 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Obenchain", "Robert L.", ""]]}, {"id": "2005.14372", "submitter": "James Tucker", "authors": "J. Derek Tucker, Lyndsay Shand, and Kenny Chowdhary", "title": "Multimodal Bayesian Registration of Noisy Functions using Hamiltonian\n  Monte Carlo", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2021.107298", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data registration is a necessary processing step for many\napplications. The observed data can be inherently noisy, often due to\nmeasurement error or natural process uncertainty, which most functional\nalignment methods cannot handle. A pair of functions can also have multiple\noptimal alignment solutions, which is not addressed in current literature. In\nthis paper, a flexible Bayesian approach to functional alignment is presented,\nwhich appropriately accounts for noise in the data without any pre-smoothing\nrequired. Additionally, by running parallel MCMC chains, the method can account\nfor multiple optimal alignments via the multi-modal posterior distribution of\nthe warping functions. To most efficiently sample the warping functions, the\napproach relies on a modification of the standard Hamiltonian Monte Carlo to be\nwell-defined on the infinite-dimensional Hilbert space. This flexible Bayesian\nalignment method is applied to both simulated data and real data sets to show\nits efficiency in handling noisy functions and successfully accounting for\nmultiple optimal alignments in the posterior; characterizing the uncertainty\nsurrounding the warping functions.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 03:12:09 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 04:16:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Tucker", "J. Derek", ""], ["Shand", "Lyndsay", ""], ["Chowdhary", "Kenny", ""]]}, {"id": "2005.14396", "submitter": "Satoshi Hattori", "authors": "Ao Huang, Sho Komukai, Tim Friede, Satoshi Hattori", "title": "Using clinical trial registries to inform Copas selection model for\n  publication bias in meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prospective registration of study protocols in clinical trial registries is a\nuseful way to minimize the risk of publication bias in meta-analysis, and\nseveral clinical trial registries are available nowadays. However, they are\nmainly used as a tool for searching studies and information submitted to the\nregistries has not been utilized as efficiently as it could. In addressing\npublication bias in meta-analyses, sensitivity analysis with the Copas\nselection model is a more objective alternative to widely-used graphical\nmethods such as the funnel-plot and the trim-and-fill method. Despite its\nability to quantify the potential impact of publication bias, a drawback of the\nmodel is that some parameters not to be specified. This may result in some\ndifficulty in interpreting the results of the sensitivity analysis. In this\npaper, we propose an alternative inference procedure for the Copas selection\nmodel by utilizing information from clinical trial registries. Our method\nprovides a simple and accurate way to estimate all unknown parameters in the\nCopas selection model. A simulation study revealed that our proposed method\nresulted in smaller biases and more accurate confidence intervals than existing\nmethods. Furthermore, two published meta-analyses had been re-analysed to\ndemonstrate how to implement the proposed method in practice.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 05:35:37 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Huang", "Ao", ""], ["Komukai", "Sho", ""], ["Friede", "Tim", ""], ["Hattori", "Satoshi", ""]]}, {"id": "2005.14458", "submitter": "Domagoj \\'Cevid MMath", "authors": "Domagoj \\'Cevid, Loris Michel, Jeffrey N\\\"af, Nicolai Meinshausen,\n  Peter B\\\"uhlmann", "title": "Distributional Random Forests: Heterogeneity Adjustment and Multivariate\n  Distributional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forests (Breiman, 2001) is a successful and widely used regression and\nclassification algorithm. Part of its appeal and reason for its versatility is\nits (implicit) construction of a kernel-type weighting function on training\ndata, which can also be used for targets other than the original mean\nestimation. We propose a novel forest construction for multivariate responses\nbased on their joint conditional distribution, independent of the estimation\ntarget and the data model. It uses a new splitting criterion based on the MMD\ndistributional metric, which is suitable for detecting heterogeneity in\nmultivariate distributions. The induced weights define an estimate of the full\nconditional distribution, which in turn can be used for arbitrary and\npotentially complicated targets of interest. The method is very versatile and\nconvenient to use, as we illustrate on a wide range of examples. The code is\navailable as Python and R packages drf.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 09:05:00 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:21:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["\u0106evid", "Domagoj", ""], ["Michel", "Loris", ""], ["N\u00e4f", "Jeffrey", ""], ["Meinshausen", "Nicolai", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2005.14462", "submitter": "Azam Asanjarani", "authors": "Azam Asanjarani, Benoit Liquet, Yoni Nazarathy", "title": "Estimation of Semi-Markov Multi-state Models: A Comparison of the\n  Sojourn Times and Transition Intensities Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Markov models are widely used for survival analysis and reliability\nanalysis. In general, there are two competing parameterizations and each\nentails its own interpretation and inference properties. On the one hand, a\nsemi-Markov process can be defined based on the distribution of sojourn times,\noften via hazard rates, together with transition probabilities of an embedded\nMarkov chain. On the other hand, intensity transition functions may be used,\noften referred to as the hazard rates of the semi-Markov process. We summarize\nand contrast these two parameterizations both from a probabilistic and an\ninference perspective, and we highlight relationships between the two\napproaches. In general, the intensity transition based approach allows the\nlikelihood to be split into likelihoods of two-state models having fewer\nparameters, allowing efficient computation and usage of many survival analysis\ntools. {Nevertheless, in certain cases the sojourn time based approach is\nnatural and has been exploited extensively in applications.} In contrasting the\ntwo approaches and contemporary relevant R packages used for inference, we use\ntwo real datasets highlighting the probabilistic and inference properties of\neach approach. This analysis is accompanied by an R vignette.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 09:18:05 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 04:00:49 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Asanjarani", "Azam", ""], ["Liquet", "Benoit", ""], ["Nazarathy", "Yoni", ""]]}, {"id": "2005.14473", "submitter": "Roman Flury", "authors": "Roman Flury and Reinhard Furrer", "title": "Multiresolution Decomposition of Areal Count Data", "comments": "4 pages, 3 figures, GRASPA 2019 conference proceeding", "journal-ref": "Proceedings of the GRASPA 2019 Conference, Pescara, 15-16 July\n  2019", "doi": "10.6092/GRASPA19_pp86-89", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution decomposition is commonly understood as a procedure to\ncapture scale-dependent features in random signals. Such methods were first\nestablished for image processing and typically rely on raster or regularly\ngridded data. In this article, we extend a particular multiresolution\ndecomposition procedure to areal count data, i.e.~discrete irregularly gridded\ndata. More specifically, we incorporate in a new model concept and\ndistributions from the so-called Besag--York--Molli\\'{e} model to include a\npriori demographical knowledge. These adaptions and subsequent changes in the\ncomputation schemes are carefully outlined below, whereas the main idea of the\noriginal multiresolution decomposition remains. Finally, we show the\nextension's feasibility by applying it to oral cavity cancer counts in Germany.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 09:44:51 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Flury", "Roman", ""], ["Furrer", "Reinhard", ""]]}, {"id": "2005.14524", "submitter": "R\\'emy Mari\\'etan", "authors": "R\\'emy Mari\\'etan, Stephan Morgenthaler", "title": "Statistical applications of Random matrix theory: comparison of two\n  populations III", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a statistical procedure for testing the equality of\ntwo independently estimated covariance matrices when the number of potentially\ndependent data vectors is large and proportional to the size of the vectors,\nthat is, the number of variables. Inspired by the spike models used in random\nmatrix theory, we concentrate on the largest eigenvalues of the matrices in\norder to determine significant differences. To avoid false rejections we must\nguard against residual spikes and need a sufficiently precise description of\nthe properties of the largest eigenvalues under the null hypothesis. In this\npaper, we extend arXiv:2002.12741 for perturbation of order $1$ and\narXiv:2002.12703 studying simpler statistic. The residual spike introduce in\nthe first paper is investigated and leads to a statistic that results in a good\ntest of equality of two populations. Simulations show that this new test does\nnot rely on some hypotheses that were necessary for the proofs and in the\nsecond paper.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 12:22:20 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 08:26:54 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Mari\u00e9tan", "R\u00e9my", ""], ["Morgenthaler", "Stephan", ""]]}]