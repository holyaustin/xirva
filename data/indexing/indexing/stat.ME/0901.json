[{"id": "0901.0017", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Alfred Hero and Herv\\'e Perdry", "title": "Space Alternating Penalized Kullback Proximal Point Algorithms for\n  Maximizing Likelihood with Nondifferentiable Penalty", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM algorithm is a widely used methodology for penalized likelihood\nestimation. Provable monotonicity and convergence are the hallmarks of the EM\nalgorithm and these properties are well established for smooth likelihood and\nsmooth penalty functions. However, many relaxed versions of variable selection\npenalties are not smooth. The goal of this paper is to introduce a new class of\nSpace Alternating Penalized Kullback Proximal extensions of the EM algorithm\nfor nonsmooth likelihood inference. We show that the cluster points of the new\nmethod are stationary points even when on the boundary of the parameter set.\nSpecial attention has been paid to the construction of component-wise version\nof the method in order to ease the implementation for complicated models.\nIllustration for the problems of model selection for finite mixtures of\nregression and to sparse image reconstruction is presented.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2008 22:41:05 GMT"}, {"version": "v2", "created": "Tue, 2 Nov 2010 21:53:08 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2011 13:22:45 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Hero", "Alfred", ""], ["Perdry", "Herv\u00e9", ""]]}, {"id": "0901.0135", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Wenjie Fu, Le Song", "title": "A state-space mixed membership blockmodel for dynamic network tomography", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS311 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 535-566", "doi": "10.1214/09-AOAS311", "report-no": "IMS-AOAS-AOAS311", "categories": "stat.ML q-bio.MN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a dynamic social or biological environment, the interactions between the\nactors can undergo large and systematic changes. In this paper we propose a\nmodel-based approach to analyze what we will refer to as the dynamic tomography\nof such time-evolving networks. Our approach offers an intuitive but powerful\ntool to infer the semantic underpinnings of each actor, such as its social\nroles or biological functions, underlying the observed network topologies. Our\nmodel builds on earlier work on a mixed membership stochastic blockmodel for\nstatic networks, and the state-space model for tracking object trajectory. It\novercomes a major limitation of many current network inference techniques,\nwhich assume that each actor plays a unique and invariant role that accounts\nfor all its interactions with other actors; instead, our method models the role\nof each actor as a time-evolving mixed membership vector that allows actors to\nbehave differently over time and carry out different roles/functions when\ninteracting with different peers, which is closer to reality. We present an\nefficient algorithm for approximate inference and learning using our model; and\nwe applied our model to analyze a social network between monks (i.e., the\nSampson's network), a dynamic email communication network between the Enron\nemployees, and a rewiring gene interaction network of fruit fly collected\nduring its full life cycle. In all cases, our model reveals interesting\npatterns of the dynamic roles of the actors.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2008 21:27:33 GMT"}, {"version": "v2", "created": "Mon, 8 Nov 2010 09:05:23 GMT"}], "update_date": "2010-11-09", "authors_parsed": [["Xing", "Eric P.", ""], ["Fu", "Wenjie", ""], ["Song", "Le", ""]]}, {"id": "0901.0225", "submitter": "Robert Kohn", "authors": "Paolo Giordani, Xiuyan Mun and Robert Kohn", "title": "Flexible Multivariate Density Estimation with Marginal Adaptation", "comments": "23 pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article addresses the problem of flexibly estimating a multivariate\ndensity while also attempting to estimate its marginals correctly. We do so by\nproposing two new estimators that try to capture the best features of mixture\nof normals and copula estimators while avoiding some of their weaknesses. The\nfirst estimator we propose is a mixture of normals copula model that is a\nflexible alternative to parametric copula models such as the normal and t\ncopula. The second is a marginally adapted mixture of normals estimator that\nimproves on the standard mixture of normals by using information contained in\nunivariate estimates of the marginal densities. We show empirically that copula\nbased approaches can behave much better or much worse than estimators based on\nmixture of normals depending on the properties of the data. We provide fast and\nreliable implementations of the estimators and illustrate the methodology on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2009 09:27:50 GMT"}], "update_date": "2009-01-05", "authors_parsed": [["Giordani", "Paolo", ""], ["Mun", "Xiuyan", ""], ["Kohn", "Robert", ""]]}, {"id": "0901.0335", "submitter": "Jeb Willenbring", "authors": "Jay H. Beder and Jeb F. Willenbring", "title": "Invariance of generalized wordlength patterns", "comments": "To appear in: Journal of Statistical Planning and Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized wordlength pattern (GWLP) introduced by Xu and Wu (2001) for\nan arbitrary fractional factorial design allows one to extend the use of the\nminimum aberration criterion to such designs. Ai and Zhang (2004) defined the\n$J$-characteristics of a design and showed that they uniquely determine the\ndesign. While both the GWLP and the $J$-characteristics require indexing the\nlevels of each factor by a cyclic group, we see that the definitions carry over\nwith appropriate changes if instead one uses an arbitrary abelian group. This\nmeans that the original definitions rest on an arbitrary choice of group\nstructure. We show that the GWLP of a design is independent of this choice, but\nthat the $J$-characteristics are not. We briefly discuss some implications of\nthese results.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2009 20:15:43 GMT"}], "update_date": "2009-01-06", "authors_parsed": [["Beder", "Jay H.", ""], ["Willenbring", "Jeb F.", ""]]}, {"id": "0901.0401", "submitter": "Adom Giffin", "authors": "Adom Giffin", "title": "From Physics to Economics: An Econometric Example Using Maximum Relative\n  Entropy", "comments": "This paper has been accepted in Physica A. 19 Pages, 3 Figures", "journal-ref": "Physica A 388 (2009), pp. 1610-1620", "doi": "10.1016/j.physa.2008.12.066", "report-no": null, "categories": "q-fin.ST cs.IT math.IT physics.data-an physics.pop-ph stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Econophysics, is based on the premise that some ideas and methods from\nphysics can be applied to economic situations. We intend to show in this paper\nhow a physics concept such as entropy can be applied to an economic problem. In\nso doing, we demonstrate how information in the form of observable data and\nmoment constraints are introduced into the method of Maximum relative Entropy\n(MrE). A general example of updating with data and moments is shown. Two\nspecific econometric examples are solved in detail which can then be used as\ntemplates for real world problems. A numerical example is compared to a large\ndeviation solution which illustrates some of the advantages of the MrE method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2009 21:37:04 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Giffin", "Adom", ""]]}, {"id": "0901.0762", "submitter": "William Rea", "authors": "William Rea, Les Oxley, Marco Reale, Jennifer Brown", "title": "Estimators for Long Range Dependence: An Empirical Study", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2009_353", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of a simulation study into the properties of 12\ndifferent estimators of the Hurst parameter, $H$, or the fractional integration\nparameter, $d$, in long memory time series. We compare and contrast their\nperformance on simulated Fractional Gaussian Noises and fractionally integrated\nseries with lengths between 100 and 10,000 data points and $H$ values between\n0.55 and 0.90 or $d$ values between 0.05 and 0.40. We apply all 12 estimators\nto the Campito Mountain data and estimate the accuracy of their estimates using\nthe Beran goodness of fit test for long memory time series. MCS code: 37M10\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2009 06:57:19 GMT"}], "update_date": "2009-01-08", "authors_parsed": [["Rea", "William", ""], ["Oxley", "Les", ""], ["Reale", "Marco", ""], ["Brown", "Jennifer", ""]]}, {"id": "0901.1378", "submitter": "Yves F. Atchad\\'{e}", "authors": "Yves F. Atchad\\'e", "title": "A cautionary tale on the efficiency of some adaptive Monte Carlo schemes", "comments": "Published in at http://dx.doi.org/10.1214/09-AAP636 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2010, Vol. 20, No. 3, 841-868", "doi": "10.1214/09-AAP636", "report-no": "IMS-AAP-AAP636", "categories": "stat.CO math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in the literature for adaptive Markov chain Monte\nCarlo methods based on sequences of random transition kernels $\\{P_n\\}$ where\nthe kernel $P_n$ is allowed to have an invariant distribution $\\pi_n$ not\nnecessarily equal to the distribution of interest $\\pi$ (target distribution).\nThese algorithms are designed such that as $n\\to\\infty$, $P_n$ converges to\n$P$, a kernel that has the correct invariant distribution $\\pi$. Typically, $P$\nis a kernel with good convergence properties, but one that cannot be directly\nimplemented. It is then expected that the algorithm will inherit the good\nconvergence properties of $P$. The equi-energy sampler of [Ann. Statist. 34\n(2006) 1581--1619] is an example of this type of adaptive MCMC. We show in this\npaper that the asymptotic variance of this type of adaptive MCMC is always at\nleast as large as the asymptotic variance of the Markov chain with transition\nkernel $P$. We also show by simulation that the difference can be substantial.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2009 14:16:39 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2009 01:00:29 GMT"}, {"version": "v3", "created": "Fri, 15 Oct 2010 07:07:56 GMT"}], "update_date": "2010-10-18", "authors_parsed": [["Atchad\u00e9", "Yves F.", ""]]}, {"id": "0901.1504", "submitter": "Bharath Sriperumbudur", "authors": "Bharath Sriperumbudur, David Torres and Gert Lanckriet", "title": "A D.C. Programming Approach to the Sparse Generalized Eigenvalue Problem", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the sparse eigenvalue problem wherein the goal is\nto obtain a sparse solution to the generalized eigenvalue problem. We achieve\nthis by constraining the cardinality of the solution to the generalized\neigenvalue problem and obtain sparse principal component analysis (PCA), sparse\ncanonical correlation analysis (CCA) and sparse Fisher discriminant analysis\n(FDA) as special cases. Unlike the $\\ell_1$-norm approximation to the\ncardinality constraint, which previous methods have used in the context of\nsparse PCA, we propose a tighter approximation that is related to the negative\nlog-likelihood of a Student's t-distribution. The problem is then framed as a\nd.c. (difference of convex functions) program and is solved as a sequence of\nconvex programs by invoking the majorization-minimization method. The resulting\nalgorithm is proved to exhibit \\emph{global convergence} behavior, i.e., for\nany random initialization, the sequence (subsequence) of iterates generated by\nthe algorithm converges to a stationary point of the d.c. program. The\nperformance of the algorithm is empirically demonstrated on both sparse PCA\n(finding few relevant genes that explain as much variance as possible in a\nhigh-dimensional gene dataset) and sparse CCA (cross-language document\nretrieval and vocabulary selection for music retrieval) applications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2009 05:02:18 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2009 03:01:23 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Sriperumbudur", "Bharath", ""], ["Torres", "David", ""], ["Lanckriet", "Gert", ""]]}, {"id": "0901.1925", "submitter": "Tina Toni", "authors": "Tina Toni, David Welch, Natalja Strelkowa, Andreas Ipsen, Michael P.H.\n  Stumpf", "title": "Approximate Bayesian computation scheme for parameter inference and\n  model selection in dynamical systems", "comments": "26 pages, 9 figures", "journal-ref": "Journal of the Royal Society Interface, Volume 6, Number 31, 2009,\n  pages 187-202", "doi": "10.1098/rsif.2008.0172", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation methods can be used to evaluate posterior\ndistributions without having to calculate likelihoods. In this paper we discuss\nand apply an approximate Bayesian computation (ABC) method based on sequential\nMonte Carlo (SMC) to estimate parameters of dynamical models. We show that ABC\nSMC gives information about the inferability of parameters and model\nsensitivity to changes in parameters, and tends to perform better than other\nABC approaches. The algorithm is applied to several well known biological\nsystems, for which parameters and their credible intervals are inferred.\nMoreover, we develop ABC SMC as a tool for model selection; given a range of\ndifferent mathematical descriptions, ABC SMC is able to choose the best model\nusing the standard Bayesian model selection apparatus.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2009 03:45:46 GMT"}], "update_date": "2009-01-15", "authors_parsed": [["Toni", "Tina", ""], ["Welch", "David", ""], ["Strelkowa", "Natalja", ""], ["Ipsen", "Andreas", ""], ["Stumpf", "Michael P. H.", ""]]}, {"id": "0901.2231", "submitter": "Christoph Leuenberger", "authors": "Christoph Leuenberger Daniel Wegmann Laurent Excoffier", "title": "Bayesian Computation and Model Selection in Population Genetics", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, the use of Bayesian inference in population genetics was\nlimited to a few cases because for many realistic population genetic models the\nlikelihood function cannot be calculated analytically . The situation changed\nwith the advent of likelihood-free inference algorithms, often subsumed under\nthe term Approximate Bayesian Computation (ABC). A key innovation was the use\nof a post-sampling regression adjustment, allowing larger tolerance values and\nas such shifting computation time to realistic orders of magnitude (see\nBeaumont et al., 2002). Here we propose a reformulation of the regression\nadjustment in terms of a General Linear Model (GLM). This allows the\nintegration into the framework of Bayesian statistics and the use of its\nmethods, including model selection via Bayes factors. We then apply the\nproposed methodology to the question of population subdivision among western\nchimpanzees Pan troglodytes verus.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2009 11:15:27 GMT"}], "update_date": "2009-01-16", "authors_parsed": [["Excoffier", "Christoph Leuenberger Daniel Wegmann Laurent", ""]]}, {"id": "0901.2234", "submitter": "Nicole Kraemer", "authors": "Stefan Haufe, Guido Nolte, Klaus-Robert Mueller, Nicole Kraemer", "title": "Sparse Causal Discovery in Multivariate Time Series", "comments": "to appear in Journal of Machine Learning Research, Proceedings of the\n  NIPS'08 workshop on Causality", "journal-ref": "JMLR Workshop and Conference Proceedings 6: Causality: Objectives\n  and Assessment (NIPS 2008), 97 - 106", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to estimate causal interactions in multivariate time series.\nUsing vector autoregressive (VAR) models, these can be defined based on\nnon-vanishing coefficients belonging to respective time-lagged instances. As in\nmost cases a parsimonious causality structure is assumed, a promising approach\nto causal discovery consists in fitting VAR models with an additional\nsparsity-promoting regularization. Along this line we here propose that\nsparsity should be enforced for the subgroups of coefficients that belong to\neach pair of time series, as the absence of a causal relation requires the\ncoefficients for all time-lags to become jointly zero. Such behavior can be\nachieved by means of l1-l2-norm regularized regression, for which an efficient\nactive set solver has been proposed recently. Our method is shown to outperform\nstandard methods in recovering simulated causality graphs. The results are on\npar with a second novel approach which uses multiple statistical testing.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2009 11:21:33 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Haufe", "Stefan", ""], ["Nolte", "Guido", ""], ["Mueller", "Klaus-Robert", ""], ["Kraemer", "Nicole", ""]]}, {"id": "0901.2730", "submitter": "Jun Zhu", "authors": "Jun Zhu, and Eric P. Xing", "title": "Maximum Entropy Discrimination Markov Networks", "comments": "39 pages", "journal-ref": "Journal of Machine Learning Research, 10(Nov):2531-2569, 2009", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel and general framework called {\\it Maximum\nEntropy Discrimination Markov Networks} (MaxEnDNet), which integrates the\nmax-margin structured learning and Bayesian-style estimation and combines and\nextends their merits. Major innovations of this model include: 1) It\ngeneralizes the extant Markov network prediction rule based on a point\nestimator of weights to a Bayesian-style estimator that integrates over a\nlearned distribution of the weights. 2) It extends the conventional max-entropy\ndiscrimination learning of classification rule to a new structural max-entropy\ndiscrimination paradigm of learning the distribution of Markov networks. 3) It\nsubsumes the well-known and powerful Maximum Margin Markov network (M$^3$N) as\na special case, and leads to a model similar to an $L_1$-regularized M$^3$N\nthat is simultaneously primal and dual sparse, or other types of Markov network\nby plugging in different prior distributions of the weights. 4) It offers a\nsimple inference algorithm that combines existing variational inference and\nconvex-optimization based M$^3$N solvers as subroutines. 5) It offers a\nPAC-Bayesian style generalization bound. This work represents the first\nsuccessful attempt to combine Bayesian-style learning (based on generative\nmodels) with structured maximum margin learning (based on a discriminative\nmodel), and outperforms a wide array of competing methods for structured\ninput/output learning on both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2009 20:07:17 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Zhu", "Jun", ""], ["Xing", "Eric P.", ""]]}, {"id": "0901.2808", "submitter": "Pierre, R. Bertrand", "authors": "Antoine Ayache (LPP), Pierre R. Bertrand (INRIA Saclay - Ile de\n  France)", "title": "A process very similar to multifractional Brownian motion", "comments": "18 pages", "journal-ref": "Recent Developments in Fractals and Related Fields (2010) 311--326", "doi": "10.1007/978-0-8176-4888-6", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Ayache and Taqqu (2005), the multifractional Brownian (mBm) motion is\nobtained by replacing the constant parameter $H$ of the fractional Brownian\nmotion (fBm) by a smooth enough functional parameter $H(.)$ depending on the\ntime $t$. Here, we consider the process $Z$ obtained by replacing in the\nwavelet expansion of the fBm the index $H$ by a function $H(.)$ depending on\nthe dyadic point $k/2^j$. This process was introduced in Benassi et al (2000)\nto model fBm with piece-wise constant Hurst index and continuous paths. In this\nwork, we investigate the case where the functional parameter satisfies an\nuniform H\\\"older condition of order $\\beta>\\sup_{t\\in \\rit} H(t)$ and ones\nshows that, in this case, the process $Z$ is very similar to the mBm in the\nfollowing senses: i) the difference between $Z$ and a mBm satisfies an uniform\nH\\\"older condition of order $d>\\sup_{t\\in \\R} H(t)$; ii) as a by product, one\ndeduces that at each point $t\\in \\R$ the pointwise H\\\"older exponent of $Z$ is\n$H(t)$ and that $Z$ is tangent to a fBm with Hurst parameter $H(t)$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2009 10:46:15 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2009 16:01:11 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Ayache", "Antoine", "", "LPP"], ["Bertrand", "Pierre R.", "", "INRIA Saclay - Ile de\n  France"]]}, {"id": "0901.2996", "submitter": "Pierre, R. Bertrand", "authors": "Pierre R. Bertrand (INRIA Saclay - Ile de France), Gilles Teyssi\\`ere,\n  Gil Boudet, Alain Chamoux", "title": "Detection of Change--Points in the Spectral Density. With Applications\n  to ECG Data", "comments": "proceeding of the workshop 'Fouille de donn\\'ees temporelles et\n  analyse de flux de donn\\'ees' EGC'2009, january 27, Strasbourg, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for estimating the change-points of heart rate in the\northosympathetic and parasympathetic bands, based on the wavelet transform in\nthe complex domain and the study of the change-points in the moments of the\nmodulus of these wavelet transforms. We observe change-points in the\ndistribution for both bands.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2009 08:04:38 GMT"}], "update_date": "2009-01-21", "authors_parsed": [["Bertrand", "Pierre R.", "", "INRIA Saclay - Ile de France"], ["Teyssi\u00e8re", "Gilles", ""], ["Boudet", "Gil", ""], ["Chamoux", "Alain", ""]]}, {"id": "0901.3326", "submitter": "Jean-Fran\\c{c}ois Giovannelli", "authors": "Jean-Francois Giovannelli", "title": "Unsupervised bayesian convex deconvolution based on a field with an\n  explicit partition function", "comments": null, "journal-ref": "IEEE Trans. Image Processing, vol. 17, no. 1, pp. 16-23, January\n  2008", "doi": "10.1109/TIP.2007.911819", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a non-Gaussian Markov field with a special feature: an\nexplicit partition function. To the best of our knowledge, this is an original\ncontribution. Moreover, the explicit expression of the partition function\nenables the development of an unsupervised edge-preserving convex deconvolution\nmethod. The method is fully Bayesian, and produces an estimate in the sense of\nthe posterior mean, numerically calculated by means of a Monte-Carlo Markov\nChain technique. The approach is particularly effective and the computational\npracticability of the method is shown on a simple simulated example.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2009 17:51:31 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2009 14:54:49 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Giovannelli", "Jean-Francois", ""]]}, {"id": "0901.3379", "submitter": "Yifeng Xue", "authors": "Fei Li and Yifeng Xue", "title": "Zonal polynomials and hypergeometric functions of quaternion matrix\n  argument", "comments": "22 pages. Communications in Statistics - Theory and Methods (appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define zonal polynomials of quaternion matrix argument and deduce some\nimportant formulae of zonal polynomials and hypergeometric functions of\nquaternion matrix argument. As an application, we give the distributions of the\nlargest and smallest eigenvalues of a quaternion central Wishart matrix\n$W\\sim\\mathbb{Q}W(n,\\Sigma)$, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2009 23:22:03 GMT"}], "update_date": "2009-01-23", "authors_parsed": [["Li", "Fei", ""], ["Xue", "Yifeng", ""]]}, {"id": "0901.3531", "submitter": "Matthias Kohl", "authors": "Matthias Kohl, Peter Ruckdeschel, Helmut Rieder", "title": "Infinitesimally Robust Estimation in General Smoothly Parametrized\n  Models", "comments": null, "journal-ref": "Statistical Methods and Application 2010", "doi": "10.1007/s10260-010-0133-0", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the shrinking neighborhood approach of Robust Statistics, which\napplies to general smoothly parametrized models, especially, exponential\nfamilies. Equal generality is achieved by object oriented implementation of the\noptimally robust estimators. We evaluate the estimates on real datasets from\nliterature by means of our R packages ROptEst and RobLox.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2009 18:11:02 GMT"}], "update_date": "2010-08-04", "authors_parsed": [["Kohl", "Matthias", ""], ["Ruckdeschel", "Peter", ""], ["Rieder", "Helmut", ""]]}, {"id": "0901.4220", "submitter": "Matti Lassas j", "authors": "Matti Lassas. Eero Saksman, Samuli Siltanen", "title": "Discretization-invariant Bayesian inversion and Besov space priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian solution of an inverse problem for indirect measurement $M = AU +\n{\\mathcal{E}}$ is considered, where $U$ is a function on a domain of $R^d$.\nHere $A$ is a smoothing linear operator and $ {\\mathcal{E}}$ is Gaussian white\nnoise. The data is a realization $m_k$ of the random variable $M_k = P_kA U+P_k\n{\\mathcal{E}}$, where $P_k$ is a linear, finite dimensional operator related to\nmeasurement device. To allow computerized inversion, the unknown is discretized\nas $U_n=T_nU$, where $T_n$ is a finite dimensional projection, leading to the\ncomputational measurement model $M_{kn}=P_k A U_n + P_k {\\mathcal{E}}$. Bayes\nformula gives then the posterior distribution $\\pi_{kn}(u_n |\nm_{kn})\\sim\\pi_n(u_n) \\exp(-{1/2}\\|m_{kn} - P_kA u_n\\|_2^2)$ in $R^d$, and the\nmean $U^{CM}_{kn}:=\\int u_n \\pi_{kn}(u_n | m_k) du_n$ is considered as the\nreconstruction of $U$. We discuss a systematic way of choosing prior\ndistributions $\\prior_n$ for all $n\\geq n_0>0$ by achieving them as projections\nof a distribution in a infinite-dimensional limit case. Such choice of prior\ndistributions is {\\em discretization-invariant} in the sense that $\\prior_n$\nrepresent the same {\\em a priori} information for all $n$ and that the mean\n$U^{CM}_{kn}$ converges to a limit estimate as $k,n\\to\\infty$. Gaussian\nsmoothness priors and wavelet-based Besov space priors are shown to be\ndiscretization invariant. In particular, Bayesian inversion in dimension two\nwith $B^1_{11}$ prior is related to penalizing the $\\ell^1$ norm of the wavelet\ncoefficients of $U$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 11:11:03 GMT"}], "update_date": "2009-01-28", "authors_parsed": [["Saksman", "Matti Lassas. Eero", ""], ["Siltanen", "Samuli", ""]]}, {"id": "0901.4605", "submitter": "David  Nott", "authors": "David Nott and Chenlei Leng", "title": "Bayesian projection approaches to variable selection and exploring model\n  uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian approach to variable selection which is based on the expected\nKullback-Leibler divergence between the full model and its projection onto a\nsubmodel has recently been suggested in the literature. Here we extend this\nidea by considering projections onto subspaces defined via some form of $L_1$\nconstraint on the parameter in the full model. This leads to Bayesian model\nselection approaches related to the lasso. In the posterior distribution of the\nprojection there is positive probability that some components are exactly zero\nand the posterior distribution on the model space induced by the projection\nallows exploration of model uncertainty. We also consider use of the approach\nin structured variable selection problems such as ANOVA models where it is\ndesired to incorporate main effects in the presence of interactions. Here we\nmake use of projections related to the non-negative garotte which are able to\nrespect the hierarchical constraints. We also prove a consistency result\nconcerning the posterior distribution on the model induced by the projection,\nand show that for some projections related to the adaptive lasso and\nnon-negative garotte the posterior distribution concentrates on the true model\nasymptotically.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2009 05:46:06 GMT"}], "update_date": "2009-01-30", "authors_parsed": [["Nott", "David", ""], ["Leng", "Chenlei", ""]]}, {"id": "0901.4715", "submitter": "Tomonari Sei", "authors": "Tomonari Sei", "title": "A structural model on a hypercube represented by optimal transport", "comments": "28pages, 6figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible statistical model for high-dimensional quantitative\ndata on a hypercube. Our model, called the structural gradient model (SGM), is\nbased on a one-to-one map on the hypercube that is a solution for an optimal\ntransport problem. As we show with many examples, SGM can describe various\ndependence structures including correlation and heteroscedasticity. The maximum\nlikelihood estimation of SGM is effectively solved by the\ndeterminant-maximization programming. In particular, a lasso-type estimation is\navailable by adding constraints. SGM is compared with graphical Gaussian models\nand mixture models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2009 16:25:19 GMT"}], "update_date": "2009-01-30", "authors_parsed": [["Sei", "Tomonari", ""]]}, {"id": "0901.4752", "submitter": "Stephane Chretien", "authors": "Stephane Chretien", "title": "Estimation of Gaussian mixtures in small sample studies using $l_1$\n  penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many experiments in medicine and ecology can be conveniently modeled by\nfinite Gaussian mixtures but face the problem of dealing with small data sets.\nWe propose a robust version of the estimator based on self-regression and\nsparsity promoting penalization in order to estimate the components of Gaussian\nmixtures in such contexts. A space alternating version of the penalized EM\nalgorithm is obtained and we prove that its cluster points satisfy the\nKarush-Kuhn-Tucker conditions. Monte Carlo experiments are presented in order\nto compare the results obtained by our method and by standard maximum\nlikelihood estimation. In particular, our estimator is seen to perform better\nthan the maximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2009 19:21:33 GMT"}, {"version": "v2", "created": "Wed, 8 Oct 2014 16:55:20 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Chretien", "Stephane", ""]]}, {"id": "0901.4881", "submitter": "Artur Lemonte", "authors": "Artur J. Lemonte, Gauss M. Cordeiro", "title": "Birnbaum-Saunders nonlinear regression models", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2009.06.015", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce, for the first time, a new class of Birnbaum-Saunders nonlinear\nregression models potentially useful in lifetime data analysis. The class\ngeneralizes the regression model described by Rieck and Nedelman [1991, A\nlog-linear model for the Birnbaum-Saunders distribution, Technometrics, 33,\n51-60]. We discuss maximum likelihood estimation for the parameters of the\nmodel, and derive closed-form expressions for the second-order biases of these\nestimates. Our formulae are easily computed as ordinary linear regressions and\nare then used to define bias corrected maximum likelihood estimates. Some\nsimulation results show that the bias correction scheme yields nearly unbiased\nestimates without increasing the mean squared errors. We also give an\napplication to a real fatigue data set.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2009 13:43:14 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2009 18:02:50 GMT"}], "update_date": "2009-07-03", "authors_parsed": [["Lemonte", "Artur J.", ""], ["Cordeiro", "Gauss M.", ""]]}]