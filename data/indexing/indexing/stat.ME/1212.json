[{"id": "1212.0122", "submitter": "Luca Martino", "authors": "David Luengo and Luca Martino", "title": "Fully Adaptive Gaussian Mixture Metropolis-Hastings Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2013.6638846", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo methods are widely used in signal processing and\ncommunications for statistical inference and stochastic optimization. In this\nwork, we introduce an efficient adaptive Metropolis-Hastings algorithm to draw\nsamples from generic multi-modal and multi-dimensional target distributions.\nThe proposal density is a mixture of Gaussian densities with all parameters\n(weights, mean vectors and covariance matrices) updated using all the\npreviously generated samples applying simple recursive rules. Numerical results\nfor the one and two-dimensional cases are provided.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 15:05:59 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2013 11:33:56 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 13:28:20 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Luengo", "David", ""], ["Martino", "Luca", ""]]}, {"id": "1212.0178", "submitter": "Alexander Blocker", "authors": "Edoardo M. Airoldi and Alexander W. Blocker", "title": "Estimating latent processes on a network from indirect measurements", "comments": "39 pages, 6 figures, 4 tables. Journal of the American Statistical\n  Association. To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a communication network, point-to-point traffic volumes over time are\ncritical for designing protocols that route information efficiently and for\nmaintaining security, whether at the scale of an internet service provider or\nwithin a corporation. While technically feasible, the direct measurement of\npoint-to-point traffic imposes a heavy burden on network performance and is\ntypically not implemented. Instead, indirect aggregate traffic volumes are\nroutinely collected. We consider the problem of estimating point-to-point\ntraffic volumes, x_t, from aggregate traffic volumes, y_t, given information\nabout the network routing protocol encoded in a matrix A. This estimation task\ncan be reformulated as finding the solutions to a sequence of ill-posed linear\ninverse problems, y_t = A x_t, since the number of origin-destination routes of\ninterest is higher than the number of aggregate measurements available.\n  Here, we introduce a novel multilevel state-space model of aggregate traffic\nvolumes with realistic features. We implement a naive strategy for estimating\nunobserved point-to-point traffic volumes from indirect measurements of\naggregate traffic, based on particle filtering. We then develop a more\nefficient two-stage inference strategy that relies on model-based\nregularization: a simple model is used to calibrate regularization parameters\nthat lead to efficient and scalable inference in the multilevel state-space\nmodel. We apply our methods to corporate and academic networks, where we show\nthat the proposed inference strategy outperforms existing approaches and scales\nto larger networks. We also design a simulation study to explore the factors\nthat influence the performance. Our results suggest that model-based\nregularization may be an efficient strategy for inference in other complex\nmultilevel models.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 03:06:09 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Airoldi", "Edoardo M.", ""], ["Blocker", "Alexander W.", ""]]}, {"id": "1212.0270", "submitter": "Rong Zhu", "authors": "Rong Zhu and Guohua Zou and Chun Wang and Yi Hu", "title": "Semiparametric Small Area Estimation of Crop Acreage under Partially\n  Linear Model", "comments": "It has been submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area estimation under linear mixed models often assumes that the small\narea effect is random effect in almost all previous studies. However, in this\npaper a new approach is proposed explaining small area effect as the unknown\nfunction of an area-indicative variable, which is a kind of semiparametric\nmodels. The nonparametric part for area effect is represented by using\npenalized splines, from which the estimation and inference are done in the\nlinear mixed model framework. The mean-squared error of empirical estimators is\nshown, and the testing for small area effect also considered. Additionally,\nsome numerical simulations are demonstrated to express the good performance of\nthis method.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 02:20:27 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2013 21:08:20 GMT"}, {"version": "v3", "created": "Tue, 15 Apr 2014 02:39:34 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Zhu", "Rong", ""], ["Zou", "Guohua", ""], ["Wang", "Chun", ""], ["Hu", "Yi", ""]]}, {"id": "1212.0352", "submitter": "Silvia Bacci Dr", "authors": "Silvia Bacci, Silvia Pandolfi, Fulvia Pennoni", "title": "A comparison of some criteria for states selection in the latent Markov\n  model for longitudinal data", "comments": "32 pages, 1 figure, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare different selection criteria to choose the number of latent states\nof a multivariate latent Markov model for longitudinal data. This model is\nbased on an underlying Markov chain to represent the evolution of a latent\ncharacteristic of a group of individuals over time. Then, the response\nvariables observed at the different occasions are assumed to be conditionally\nindependent given this chain. Maximum likelihood of the model is carried out\nthrough an Expectation-Maximization algorithm based on forward-backward\nrecursions which are well known in the hidden Markov literature for time\nseries. The selection criteria we consider in our comparison are based on\npenalized versions of the maximum log-likelihood or on the posterior\nprobabilities of belonging to each latent state, that is the conditional\nprobability of the latent state given the observed data. A Monte Carlo\nsimulation study shows that the indices referred to the log-likelihood based\ninformation criteria perform in general better with respect to those referred\nto the classification based criteria. This is due to the fact that the latter\ntend to underestimate the true number of latent states, especially in the\nunivariate case.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 11:22:50 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Bacci", "Silvia", ""], ["Pandolfi", "Silvia", ""], ["Pennoni", "Fulvia", ""]]}, {"id": "1212.0372", "submitter": "Silvia Bacci Dr", "authors": "Silvia Bacci, Francesco Bartolucci, Luca Pieroni", "title": "A causal analysis of mother's education on birth inequalities", "comments": "37 pages, 10 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a causal analysis of the mother's educational level on the health\nstatus of the newborn, in terms of gestational weeks and weight. The analysis\nis based on a finite mixture structural equation model, the parameters of which\nhave a causal interpretation. The model is applied to a dataset of almost ten\nthousand deliveries collected in an Italian region. The analysis confirms that\nstandard regression overestimates the impact of education on the child health.\nWith respect to the current economic literature, our findings indicate that\nonly high education has positive consequences on child health, implying that\npolicy efforts in education should have benefits for welfare.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 13:04:11 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Bacci", "Silvia", ""], ["Bartolucci", "Francesco", ""], ["Pieroni", "Luca", ""]]}, {"id": "1212.0440", "submitter": "Hern\\'an Larralde", "authors": "Hern\\'an Larralde", "title": "Maximum Entropy distributions of correlated variables with prespecified\n  marginals", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.86.061117", "report-no": null, "categories": "cond-mat.stat-mech q-fin.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of determining the joint probability distributions for correlated\nrandom variables with pre-specified marginals is considered. When the joint\ndistribution satisfying all the required conditions is not unique, the \"most\nunbiased\" choice corresponds to the distribution of maximum entropy. The\ncalculation of the maximum entropy distribution requires the solution of rather\ncomplicated nonlinear coupled integral equations, exact solutions to which are\nobtained for the case of Gaussian marginals; otherwise, the solution can be\nexpressed as a perturbation around the product of the marginals if the marginal\nmoments exist.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 16:37:47 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Larralde", "Hern\u00e1n", ""]]}, {"id": "1212.0442", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Victor Chernozhukov, Denis Chetverikov and Kengo\n  Kato", "title": "Some New Asymptotic Theory for Least Squares Series: Pointwise and\n  Uniform Results", "comments": null, "journal-ref": "Journal of Econometrics 186 (2015) 345-366", "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications it is common that the exact form of a conditional expectation\nis unknown and having flexible functional forms can lead to improvements.\nSeries method offers that by approximating the unknown function based on $k$\nbasis functions, where $k$ is allowed to grow with the sample size $n$. We\nconsider series estimators for the conditional mean in light of: (i) sharp LLNs\nfor matrices derived from the noncommutative Khinchin inequalities, (ii) bounds\non the Lebesgue factor that controls the ratio between the $L^\\infty$ and\n$L_2$-norms of approximation errors, (iii) maximal inequalities for processes\nwhose entropy integrals diverge, and (iv) strong approximations to series-type\nprocesses.\n  These technical tools allow us to contribute to the series literature,\nspecifically the seminal work of Newey (1997), as follows. First, we weaken the\ncondition on the number $k$ of approximating functions used in series\nestimation from the typical $k^2/n \\to 0$ to $k/n \\to 0$, up to log factors,\nwhich was available only for spline series before. Second, we derive $L_2$\nrates and pointwise central limit theorems results when the approximation error\nvanishes. Under an incorrectly specified model, i.e. when the approximation\nerror does not vanish, analogous results are also shown. Third, under stronger\nconditions we derive uniform rates and functional central limit theorems that\nhold if the approximation error vanishes or not. That is, we derive the strong\napproximation for the entire estimate of the nonparametric function.\n  We derive uniform rates, Gaussian approximations, and uniform confidence\nbands for a wide collection of linear functionals of the conditional\nexpectation function.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 16:43:08 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 05:24:24 GMT"}, {"version": "v3", "created": "Fri, 25 Jul 2014 19:43:23 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 17:11:01 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Kato", "Kengo", ""]]}, {"id": "1212.0614", "submitter": "Lei Hua Dr.", "authors": "Lei Hua, Harry Joe", "title": "Intermediate Tail Dependence: A Review and Some New Results", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of intermediate tail dependence is useful if one wants to\nquantify the degree of positive dependence in the tails when there is no strong\nevidence of presence of the usual tail dependence. We first review existing\nstudies on intermediate tail dependence, and then we report new results to\nsupplement the review. Intermediate tail dependence for elliptical, extreme\nvalue and Archimedean copulas are reviewed and further studied, respectively.\nFor Archimedean copulas, we not only consider the frailty model but also the\nrecently studied scale mixture model; for the latter, conditions leading to\nupper intermediate tail dependence are presented, and it provides a useful way\nto simulate copulas with desirable intermediate tail dependence structures.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 05:07:06 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Hua", "Lei", ""], ["Joe", "Harry", ""]]}, {"id": "1212.0634", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "Better subset regression", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To find efficient screening methods for high dimensional linear regression\nmodels, this paper studies the relationship between model fitting and screening\nperformance. Under a sparsity assumption, we show that a subset that includes\nthe true submodel always yields smaller residual sum of squares (i.e., has\nbetter model fitting) than all that do not in a general asymptotic setting.\nThis indicates that, for screening important variables, we could follow a\n\"better fitting, better screening\" rule, i.e., pick a \"better\" subset that has\nbetter model fitting. To seek such a better subset, we consider the\noptimization problem associated with best subset regression. An EM algorithm,\ncalled orthogonalizing subset screening, and its accelerating version are\nproposed for searching for the best subset. Although the two algorithms cannot\nguarantee that a subset they yield is the best, their monotonicity property\nmakes the subset have better model fitting than initial subsets generated by\npopular screening methods, and thus the subset can have better screening\nperformance asymptotically. Simulation results show that our methods are very\ncompetitive in high dimensional variable screening even for finite sample\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 07:49:48 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 02:58:03 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1212.0637", "submitter": "Alessandro Baldi Antognini", "authors": "Alessandro Baldi Antognini, Maroussa Zagoraiou", "title": "On the almost sure convergence of adaptive allocation procedures", "comments": "Published at http://dx.doi.org/10.3150/13-BEJ591 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 2, 881-908", "doi": "10.3150/13-BEJ591", "report-no": "IMS-BEJ-BEJ591", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide some general convergence results for adaptive\ndesigns for treatment comparison, both in the absence and presence of\ncovariates. In particular, we demonstrate the almost sure convergence of the\ntreatment allocation proportion for a vast class of adaptive procedures, also\nincluding designs that have not been formally investigated but mainly explored\nthrough simulations, such as Atkinson's optimum biased coin design, Pocock and\nSimon's minimization method and some of its generalizations. Even if the large\nmajority of the proposals in the literature rely on continuous allocation\nrules, our results allow to prove via a unique mathematical framework the\nconvergence of adaptive allocation methods based on both continuous and\ndiscontinuous randomization functions. Although several examples of earlier\nworks are included in order to enhance the applicability, our approach provides\nsubstantial insight for future suggestions, especially in the absence of a\nprefixed target and for designs characterized by sequences of allocation rules.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 08:06:02 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2013 15:06:54 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2013 14:09:32 GMT"}, {"version": "v4", "created": "Fri, 29 May 2015 10:35:45 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Antognini", "Alessandro Baldi", ""], ["Zagoraiou", "Maroussa", ""]]}, {"id": "1212.0707", "submitter": "Rodrigo Silva", "authors": "Marcelo Bourguignon, Rodrigo B. Silva and Gauss M. Cordeiro", "title": "A new class of fatigue life distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the Birnbaum-Saunders power series class of\ndistributions which is obtained by compounding Birnbaum-Saunders and power\nseries distributions. The new class of distributions has as a particular case\nthe two-parameter Birnbaum-Saunders distribution. The hazard rate function of\nthe proposed class can be increasing and upside-down bathtub shaped. We provide\nimportant mathematical properties such as moments, order statistics, estimation\nof the parameters and inference for large sample. Three special cases of the\nnew class are investigated with some details. We illustrate the usefulness of\nthe new distributions by means of two applications to real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 12:58:50 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2012 17:26:06 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Bourguignon", "Marcelo", ""], ["Silva", "Rodrigo B.", ""], ["Cordeiro", "Gauss M.", ""]]}, {"id": "1212.0733", "submitter": "Tony Sit", "authors": "Mark Brown, Victor de la Pena and Tony Sit", "title": "From Boundary Crossing of Non-Random Functions to Boundary Crossing of\n  Stochastic Processes", "comments": null, "journal-ref": "Prob. Eng. Inf. Sci. 29 (2015) 345-359", "doi": "10.1017/S0269964815000030", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One problem of wide interest involves estimating expected crossing-times.\nSeveral tools have been developed to solve this problem beginning with the\nworks of Wald and the theory of sequential analysis. An extension of his\napproach is provided by the optional sampling theorem in conjunction with\nmartingale inequalities. Deriving the explicit close form solution for the\nexpected crossing times may be difficult. In this paper, we provide a framework\nthat can be used to estimate expected crossing times of arbitrary stochastic\nprocesses. Our key assumption is the knowledge of the average behavior of the\nsupremum of the process. Our results include a universal sharp lower bound on\nthe expected crossing times.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 14:22:22 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2012 16:29:53 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Brown", "Mark", ""], ["de la Pena", "Victor", ""], ["Sit", "Tony", ""]]}, {"id": "1212.0764", "submitter": "Aaron Sim", "authors": "Aaron Sim, Sarah Filippi, Michael P. H. Stumpf", "title": "Information Geometry and Sequential Monte Carlo", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the application of methods from information geometry to\nthe sequential Monte Carlo (SMC) sampler. In particular the Riemannian manifold\nMetropolis-adjusted Langevin algorithm (mMALA) is adapted for the transition\nkernels in SMC. Similar to its function in Markov chain Monte Carlo methods,\nthe mMALA is a fully adaptable kernel which allows for efficient sampling of\nhigh-dimensional and highly correlated parameter spaces. We set up the\ntheoretical framework for its use in SMC with a focus on the application to the\nproblem of sequential Bayesian inference for dynamical systems as modelled by\nsets of ordinary differential equations. In addition, we argue that defining\nthe sequence of distributions on geodesics optimises the effective sample sizes\nin the SMC run. We illustrate the application of the methodology by inferring\nthe parameters of simulated Lotka-Volterra and Fitzhugh-Nagumo models. In\nparticular we demonstrate that compared to employing a standard adaptive random\nwalk kernel, the SMC sampler with an information geometric kernel design\nattains a higher level of statistical robustness in the inferred parameters of\nthe dynamical systems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 15:31:50 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Sim", "Aaron", ""], ["Filippi", "Sarah", ""], ["Stumpf", "Michael P. H.", ""]]}, {"id": "1212.0849", "submitter": "Sinan Yildirim", "authors": "Sinan Yildirim, Lan Jiang, Sumeetpal S. Singh, Tom Dean", "title": "Estimating the Static Parameters in Linear Gaussian Multiple Target\n  Tracking Models", "comments": "17 double column pages, 9 figures", "journal-ref": null, "doi": null, "report-no": "CUED/F-INFENG/TR.681", "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present both offline and online maximum likelihood estimation (MLE)\ntechniques for inferring the static parameters of a multiple target tracking\n(MTT) model with linear Gaussian dynamics. We present the batch and online\nversions of the expectation-maximisation (EM) algorithm for short and long data\nsets respectively, and we show how Monte Carlo approximations of these methods\ncan be implemented. Performance is assessed in numerical examples using\nsimulated data for various scenarios and a comparison with a Bayesian\nestimation procedure is also provided.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 20:51:07 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Yildirim", "Sinan", ""], ["Jiang", "Lan", ""], ["Singh", "Sumeetpal S.", ""], ["Dean", "Tom", ""]]}, {"id": "1212.0870", "submitter": "Jalmar Manuel  Farfan Carrasco", "authors": "Jalmar M. F. Carrasco, Silvia L. P. Ferrari and Reinaldo B.\n  Arellano-Valle", "title": "Errors-in-variables beta regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta regression models provide an adequate approach for modeling continuous\noutcomes limited to the interval (0,1). This paper deals with an extension of\nbeta regression models that allow for explanatory variables to be measured with\nerror. The structural approach, in which the covariates measured with error are\nassumed to be random variables, is employed. Three estimation methods are\npresented, namely maximum likelihood, maximum pseudo-likelihood and regression\ncalibration. Monte Carlo simulations are used to evaluate the performance of\nthe proposed estimators and the na\\\"ive estimator. Also, a residual analysis\nfor beta regression models with measurement errors is proposed. The results are\nillustrated in a real data set.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 21:06:19 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2013 14:42:57 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Carrasco", "Jalmar M. F.", ""], ["Ferrari", "Silvia L. P.", ""], ["Arellano-Valle", "Reinaldo B.", ""]]}, {"id": "1212.1004", "submitter": "Saralees Nadarajah", "authors": "Xin Liao, Zuoxiang Peng, Saralees Nadarajah, Xiaoqian Wang", "title": "Rates of convergence of extremes from skew normal samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a skew normal random sequence, convergence rates of the distribution of\nits partial maximum to the Gumbel extreme value distribution are derived. The\nasymptotic expansion of the distribution of the normalized maximum is given\nunder an optimal choice of norming constants. We find that the optimal\nconvergence rate of the normalized maximum to the Gumbel extreme value\ndistribution is proportional to $1/\\log n$.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 12:16:18 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Liao", "Xin", ""], ["Peng", "Zuoxiang", ""], ["Nadarajah", "Saralees", ""], ["Wang", "Xiaoqian", ""]]}, {"id": "1212.1440", "submitter": "Richard Warr", "authors": "Richard L. Warr and David H. Collins", "title": "A Comprehensive Method for Solving Finite-State Semi-Markov Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Markov processes (SMPs) provide a rich framework for many real-world\nproblems. However, due to difficulty implementing practical solutions they are\nrarely used with their full capability. The theory of SMPs is quite mature but\nwas mainly developed at a time when computational resources were not widely\navailable. With the exception of some of the simplest cases, solutions to SMPs\nare inherently numerical, and SMPs have been underutilized by practitioners\nbecause of difficulty implementing the theory in applications. This paper\ndemonstrates the theory and computational methods needed to implement SMP\nmodels in practical settings. Methods are illustrated with an application\nmodeling the movement of coronary patients in a hospital. Our aim is to allow\npractitioners to use richer SMP models without being burdened with the rigorous\nmathematical theory.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 20:29:13 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 14:44:31 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 16:16:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Warr", "Richard L.", ""], ["Collins", "David H.", ""]]}, {"id": "1212.1479", "submitter": "Scott Sisson", "authors": "Y. Fan and D. J. Nott and S. A. Sisson", "title": "Approximate Bayesian Computation via Regression Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods, which are applicable when the\nlikelihood is difficult or impossible to calculate, are an active topic of\ncurrent research. Most current ABC algorithms directly approximate the\nposterior distribution, but an alternative, less common strategy is to\napproximate the likelihood function. This has several advantages. First, in\nsome problems, it is easier to approximate the likelihood than to approximate\nthe posterior. Second, an approximation to the likelihood allows reference\nanalyses to be constructed based solely on the likelihood. Third, it is\nstraightforward to perform sensitivity analyses for several different choices\nof prior once an approximation to the likelihood is constructed, which needs to\nbe done only once. The contribution of the present paper is to consider\nregression density estimation techniques to approximate the likelihood in the\nABC setting. Our likelihood approximations build on recently developed marginal\nadaptation density estimators by extending them for conditional density\nestimation. Our approach facilitates reference Bayesian inference, as well as\nfrequentist inference. The method is demonstrated via a challenging problem of\ninference for stereological extremes, where we perform both frequentist and\nBayesian inference.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 21:31:56 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Fan", "Y.", ""], ["Nott", "D. J.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1212.1642", "submitter": "Steven Ellis", "authors": "Steven P. Ellis and Arno Klein", "title": "Describing High-Order Statistical Dependence Using \"Concurrence\n  Topology\", with Application to Functional MRI Brain Data", "comments": "20 pages, 3 figures", "journal-ref": "Homology, Homotopy and Applications, 16, 245--264", "doi": null, "report-no": null, "categories": "stat.ME math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multivariate data, dependence beyond pair-wise can be important. This is\ntrue, for example, in using functional MRI (fMRI) data to investigate brain\nfunctional connectivity. When one has more than a few variables, however, the\nnumber of simple summaries of even third-order dependence can be unmanageably\nlarge.\n  \"Concurrence topology\" is an apparently new nonparametric method for\ndescribing high-order dependence among up to dozens of dichotomous variables\n(e.g., seventh-order dependence in 32 variables). This method generally\nproduces summaries of $p^{th}$-order dependence of manageable size no matter\nhow big $p$ is. (But computing time can be lengthy.) For time series, this\nmethod can be applied in both the time and Fourier domains.\n  Write each observation as a vector of 0's and 1's. A \"concurrence\" is a group\nof variables all \"1\" in the same observation. The collection of concurrences\ncan be represented as a sequence of shapes (\"filtration\").\n  Holes in the filtration indicate weak or negative association among the\nvariables. The pattern of the holes in the filtration can be analyzed using\ncomputational topology.\n  This method is demonstrated on dichotomized fMRI data. The dataset includes\nsubjects diagnosed with ADHD and healthy controls. In an exploratory analysis\nnumerous group differences in the topology of the filtrations are found.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 16:14:10 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 16:13:27 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2013 15:19:12 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Ellis", "Steven P.", ""], ["Klein", "Arno", ""]]}, {"id": "1212.1947", "submitter": "Kyle Vincent Ph. D", "authors": "Kyle Vincent", "title": "A Contribution to the Theory Behind the M0 Capture-Recapture Model: An\n  Improved Estimator", "comments": "These two papers have been merged into paper arXiv:1401.6849", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of a sufficient statistic based on the identified members\nthat are obtained for samples that are selected under the $M_0$\ncapture-recapture closed population model (Schwarz and Seber, 1999). A\nRao-Blackwellized version of the estimator based on a sufficient statistic is\nthen presented. We explore the efficiency of the improved estimator via a\nsimulation study. The R code for the simulation is provided in the appendix.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 01:23:38 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 22:03:26 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 19:28:21 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Vincent", "Kyle", ""]]}, {"id": "1212.1949", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene", "title": "A Semiparametric Bayesian Approach for Extreme Values Using Dirichlet\n  Process Mixture of Gamma and Generalized Pareto Densities", "comments": "New version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For extreme value estimation we propose to use a model with a Dirichlet\nprocess mixture of gamma densities in the center and generalized Pareto\ndensities for the tails. Due to the randomness in the center and a heavy tailed\ndensity in the tails density estimation and posterior inference for high\nquantiles are possible. The approach can be used in a \"default\" manner on the\npositive reals because it works when prior information is unavailable. The\nproposed model can be easy to implement and a sensitivity analysis is provided.\nWe applied the proposed model for simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 01:38:19 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2012 11:49:50 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2013 10:54:46 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2013 22:36:29 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Fuquene", "Jairo", ""]]}, {"id": "1212.2228", "submitter": "Xun Huan", "authors": "Xun Huan, Youssef M. Marzouk", "title": "Gradient-based stochastic optimization methods in Bayesian experimental\n  design", "comments": "Preprint 40 pages, 10 figures (121 small figures). v1 submitted to\n  the International Journal for Uncertainty Quantification on December 10,\n  2012; v2 submitted on September 10, 2013. v2 changes: (a) clarified algorithm\n  stopping criteria and other parameters; (b) emphasized paper contributions,\n  plus other minor edits; v3 submitted on December 26, 2014. v3 changes: minor\n  edits", "journal-ref": "International Journal for Uncertainty Quantification 4 (2014)\n  479-510", "doi": "10.1615/Int.J.UncertaintyQuantification.2014006730", "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal experimental design (OED) seeks experiments expected to yield the\nmost useful data for some purpose. In practical circumstances where experiments\nare time-consuming or resource-intensive, OED can yield enormous savings. We\npursue OED for nonlinear systems from a Bayesian perspective, with the goal of\nchoosing experiments that are optimal for parameter inference. Our objective in\nthis context is the expected information gain in model parameters, which in\ngeneral can only be estimated using Monte Carlo methods. Maximizing this\nobjective thus becomes a stochastic optimization problem.\n  This paper develops gradient-based stochastic optimization methods for the\ndesign of experiments on a continuous parameter space. Given a Monte Carlo\nestimator of expected information gain, we use infinitesimal perturbation\nanalysis to derive gradients of this estimator. We are then able to formulate\ntwo gradient-based stochastic optimization approaches: (i) Robbins-Monro\nstochastic approximation, and (ii) sample average approximation combined with a\ndeterministic quasi-Newton method. A polynomial chaos approximation of the\nforward model accelerates objective and gradient evaluations in both cases. We\ndiscuss the implementation of these optimization methods, then conduct an\nempirical comparison of their performance. To demonstrate design in a nonlinear\nsetting with partial differential equation forward models, we use the problem\nof sensor placement for source inversion. Numerical results yield useful\nguidelines on the choice of algorithm and sample sizes, assess the impact of\nestimator bias, and quantify tradeoffs of computational cost versus solution\nquality and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 21:47:11 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 21:36:57 GMT"}, {"version": "v3", "created": "Fri, 26 Dec 2014 22:54:32 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Huan", "Xun", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1212.2462", "submitter": "Mathias Drton", "authors": "Mathias Drton, Thomas S. Richardson", "title": "A New Algorithm for Maximum Likelihood Estimation in Gaussian Graphical\n  Models for Marginal Independence", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-184-191", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models with bi-directed edges (<->) represent marginal\nindependence: the absence of an edge between two vertices indicates that the\ncorresponding variables are marginally independent. In this paper, we consider\nmaximum likelihood estimation in the case of continuous variables with a\nGaussian joint distribution, sometimes termed a covariance graph model. We\npresent a new fitting algorithm which exploits standard regression techniques\nand establish its convergence properties. Moreover, we contrast our procedure\nto existing estimation methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:52 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Drton", "Mathias", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1212.2506", "submitter": "Jiji Zhang", "authors": "Jiji Zhang, Peter L. Spirtes", "title": "Strong Faithfulness and Uniform Consistency in Causal Inference", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-632-639", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question in causal inference is whether it is possible to\nreliably infer manipulation effects from observational data. There are a\nvariety of senses of asymptotic reliability in the statistical literature,\namong which the most commonly discussed frequentist notions are pointwise\nconsistency and uniform consistency. Uniform consistency is in general\npreferred to pointwise consistency because the former allows us to control the\nworst case error bounds with a finite sample size. In the sense of pointwise\nconsistency, several reliable causal inference algorithms have been established\nunder the Markov and Faithfulness assumptions [Pearl 2000, Spirtes et al.\n2001]. In the sense of uniform consistency, however, reliable causal inference\nis impossible under the two assumptions when time order is unknown and/or\nlatent confounders are present [Robins et al. 2000]. In this paper we present\ntwo natural generalizations of the Faithfulness assumption in the context of\nstructural equation models, under which we show that the typical algorithms in\nthe literature (in some cases with modifications) are uniformly consistent even\nwhen the time order is unknown. We also discuss the situation where latent\nconfounders may be present and the sense in which the Faithfulness assumption\nis a limiting case of the stronger assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:09:00 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Zhang", "Jiji", ""], ["Spirtes", "Peter L.", ""]]}, {"id": "1212.2652", "submitter": "Hamdi Raissi", "authors": "Valentin Patilea and Hamdi Ra\\\"issi", "title": "Testing second order dynamics for autoregressive processes in presence\n  of time-varying variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volatility modeling for autoregressive univariate time series is\nconsidered. A benchmark approach is the stationary ARCH model of Engle (1982).\nMotivated by real data evidence, processes with non constant unconditional\nvariance and ARCH effects have been recently introduced. We take into account\nsuch possible non stationarity and propose simple testing procedures for ARCH\neffects. Adaptive McLeod and Li's portmanteau and ARCH-LM tests for checking\nfor second order dynamics are provided. The standard versions of these tests,\ncommonly used by practitioners, suppose constant unconditional variance. We\nprove the failure of these standard tests with time-varying unconditional\nvariance. The theoretical results are illustrated by mean of simulated and real\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 21:28:46 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Patilea", "Valentin", ""], ["Ra\u00efssi", "Hamdi", ""]]}, {"id": "1212.2784", "submitter": "Elvira Romano Dr", "authors": "Elvira Romano, Antonio Balzanella", "title": "Clustering of functional boxplots for multiple streaming time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a micro-clustering strategy for Functional\nBoxplots. The aim is to summarize a set of streaming time series splitted in\nnon overlapping windows. It is a two step strategy which performs at first, an\non-line summarization by means of functional data structures, named Functional\nBoxplot micro-clusters; then it reveals the final summarization by processing,\noff-line, the functional data structures. Our main contribute consists in\nproviding a new definition of micro-cluster based on Functional Boxplots and,\nin defining a proximity measure which allows to compare and update them. This\nallows to get a finer graphical summarization of the streaming time series by\nfive functional basic statistics of data. The obtained synthesis will be able\nto keep track of the dynamic evolution of the multiple streams.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 11:54:11 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Romano", "Elvira", ""], ["Balzanella", "Antonio", ""]]}, {"id": "1212.2812", "submitter": "Ronaldo Dias", "authors": "Adriano Zanin Zambom and Ronaldo Dias", "title": "A Review of Kernel Density Estimation with Applications to Econometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric density estimation is of great importance when econometricians\nwant to model the probabilistic or stochastic structure of a data set. This\ncomprehensive review summarizes the most important theoretical aspects of\nkernel density estimation and provides an extensive description of classical\nand modern data analytic methods to compute the smoothing parameter. Throughout\nthe text, several references can be found to the most up-to-date and cut point\nresearch approaches in this area, while econometric data sets are analyzed as\nexamples. Lastly, we present SIZer, a new approach introduced by Chaudhuri and\nMarron (2000), whose objective is to analyze the visible features representing\nimportant underlying structures for different bandwidths.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 13:30:23 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Zambom", "Adriano Zanin", ""], ["Dias", "Ronaldo", ""]]}, {"id": "1212.2882", "submitter": "Harrison Zhou", "authors": "T. Tony Cai, Weidong Liu and Harrison H. Zhou", "title": "Estimating Sparse Precision Matrix: Optimal Rates of Convergence and\n  Adaptive Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision matrix is of significant importance in a wide range of applications\nin multivariate analysis. This paper considers adaptive minimax estimation of\nsparse precision matrices in the high dimensional setting. Optimal rates of\nconvergence are established for a range of matrix norm losses. A fully data\ndriven estimator based on adaptive constrained $\\ell_1$ minimization is\nproposed and its rate of convergence is obtained over a collection of parameter\nspaces. The estimator, called ACLIME, is easy to implement and performs well\nnumerically.\n  A major step in establishing the minimax rate of convergence is the\nderivation of a rate-sharp lower bound. A \"two-directional\" lower bound\ntechnique is applied to obtain the minimax lower bound. The upper and lower\nbounds together yield the optimal rates ofconvergence for sparse precision\nmatrix estimation and show that the ACLIME estimator is adaptively minimax rate\noptimal for a collection of parameter spaces and a range of matrix norm losses\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 16:56:25 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Cai", "T. Tony", ""], ["Liu", "Weidong", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1212.2995", "submitter": "Lu Tian", "authors": "Lu Tian, Ash Alizadeh, Andrew Gentles, Robert Tibshirani", "title": "A Simple Method for Detecting Interactions between a Treatment and a\n  Large Number of Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a setting in which we have a treatment and a large number of\ncovariates for a set of observations, and wish to model their relationship with\nan outcome of interest. We propose a simple method for modeling interactions\nbetween the treatment and covariates. The idea is to modify the covariate in a\nsimple way, and then fit a standard model using the modified covariates and no\nmain effects. We show that coupled with an efficiency augmentation procedure,\nthis method produces valid inferences in a variety of settings. It can be\nuseful for personalized medicine: determining from a large set of biomarkers\nthe subset of patients that can potentially benefit from a treatment. We apply\nthe method to both simulated datasets and gene expression studies of cancer.\nThe modified data can be used for other purposes, for example large scale\nhypothesis testing for determining which of a set of covariates interact with a\ntreatment variable.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 21:57:03 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Tian", "Lu", ""], ["Alizadeh", "Ash", ""], ["Gentles", "Andrew", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1212.3267", "submitter": "Yuan Liao", "authors": "Yuan Liao, Anna Simoni", "title": "Semi-parametric Bayesian Partially Identified Models based on Support\n  Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a comprehensive semi-parametric study of Bayesian partially\nidentified econometric models. While the existing literature on Bayesian\npartial identification has mostly focused on the structural parameter, our\nprimary focus is on Bayesian credible sets (BCS's) of the unknown identified\nset and the posterior distribution of its support function. We construct a\n(two-sided) BCS based on the support function of the identified set. We prove\nthe Bernstein-von Mises theorem for the posterior distribution of the support\nfunction. This powerful result in turn infers that, while the BCS and the\nfrequentist confidence set for the partially identified parameter are\nasymptotically different, our constructed BCS for the identified set has an\nasymptotically correct frequentist coverage probability. Importantly, we\nillustrate that the constructed BCS for the identified set does not require a\nprior on the structural parameter. It can be computed efficiently for subset\ninference, especially when the target of interest is a sub-vector of the\npartially identified parameter, where projecting to a low-dimensional subset is\noften required. Hence, the proposed methods are useful in many applications.\n  The Bayesian partial identification literature has been assuming a known\nparametric likelihood function. However, econometric models usually only\nidentify a set of moment inequalities, and therefore using an incorrect\nlikelihood function may result in misleading inferences. In contrast, with a\nnonparametric prior on the unknown likelihood function, our proposed Bayesian\nprocedure only requires a set of moment conditions, and can efficiently make\ninference about both the partially identified parameter and its identified set.\nThis makes it widely applicable in general moment inequality models. Finally,\nthe proposed method is illustrated in a financial asset pricing problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 18:58:43 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 16:30:34 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Liao", "Yuan", ""], ["Simoni", "Anna", ""]]}, {"id": "1212.3556", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti (1), Caterina May (2), Chiara Tommasi (1) ((1)\n  Universit\\`a degli Studi di Milano, (2) Universit\\`a del Piemonte Orientale)", "title": "KL-optimum designs: theoretical properties and practical computation", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s11222-014-9515-8", "journal-ref": "Stat Comput (2016) 26: 107-117", "doi": "10.1007/s11222-014-9515-8", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper some new properties and computational tools for finding\nKL-optimum designs are provided. KL-optimality is a general criterion useful to\nselect the best experimental conditions to discriminate between statistical\nmodels. A KL-optimum design is obtained from a minimax optimization problem,\nwhich is defined on a infinite-dimensional space. In particular, continuity of\nthe KL-optimality criterion is proved under mild conditions; as a consequence,\nthe first-order algorithm converges to the set of KL-optimum designs for a\nlarge class of models. It is also shown that KL-optimum designs are invariant\nto any scale-position transformation. Some examples are given and discussed,\ntogether with some practical implications for numerical computation purposes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 18:17:15 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2013 12:03:08 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2014 17:02:30 GMT"}, {"version": "v4", "created": "Mon, 29 Sep 2014 06:31:29 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Aletti", "Giacomo", ""], ["May", "Caterina", ""], ["Tommasi", "Chiara", ""]]}, {"id": "1212.3587", "submitter": "Lucy Robinson", "authors": "Lucy F. Robinson and Carey E. Priebe", "title": "Detecting Time-dependent Structure in Network Data via a New Class of\n  Latent Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of latent process models for dynamic relational\nnetwork data with the goal of detecting time-dependent structure. Network data\nare often observed over time, and static network models for such data may fail\nto capture relevant dynamic features. We present a new technique for\nidentifying the emergence or disappearance of distinct subpopulations of\nvertices. In this formulation, a network is observed over time, with attributed\nedges appearing at random times. At unknown time points, subgroups of vertices\nmay exhibit a change in behavior. Such changes may take the form of a change in\nthe overall probability of connection within or between subgroups, or a change\nin the distribution of edge attributes. A mixture distribution for latent\nvertex positions is used to detect heterogeneities in connectivity behavior\nover time and over vertices. The probability of edges with various attributes\nat a given time is modeled using a latent-space stochastic process associated\nwith each vertex. A random dot product model is used to describe the dependency\nstructure of the graph. As an application we analyze the Enron email corpus.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 20:28:06 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 16:15:10 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Robinson", "Lucy F.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1212.3712", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos and Stephen G. Walker", "title": "A Latent-Variable Bayesian Nonparametric Regression Model", "comments": "1 table, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a random partition model for Bayesian nonparametric regression.\nThe model is based on infinitely-many disjoint regions of the range of a latent\ncovariate-dependent Gaussian process. Given a realization of the process, the\ncluster of dependent variable responses that share a common region are assumed\nto arise from the same distribution. Also, the latent Gaussian process prior\nallows for the random partitions (i.e., clusters of the observations) to\nexhibit dependencies among one another. The model is illustrated through the\nanalysis of a real data set arising from education, and through the analysis of\nsimulated data that were generated from complex data-generating models.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2012 18:02:17 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2013 23:29:19 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Karabatsos", "George", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1212.3730", "submitter": "Pantelis - Zenon Hadjipantelis", "authors": "Pantelis Z. Hadjipantelis, Nick S. Jones, John Moriarty, David A.\n  Springate, Christopher G. Knight", "title": "Function-Valued Traits in Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological characteristics of evolutionary interest are not scalar\nvariables but continuous functions. Given a dataset of function-valued traits\ngenerated by evolution, we develop a practical statistical approach to infer\nancestral function-valued traits, and estimate the generative evolutionary\nprocess. We do this by combining dimension reduction and phylogenetic Gaussian\nprocess regression, a nonparametric procedure which explicitly accounts for\nknown phylogenetic relationships. We test the methods' performance on simulated\nfunction-valued data generated from a stochastic evolutionary model. The\nmethods are applied assuming that only the phylogeny and the function-valued\ntraits of taxa at its tips are known. Our method is robust and applicable to a\nwide range of function-valued data, and also offers a phylogenetically aware\nmethod for estimating the autocorrelation of function-valued traits.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2012 21:11:43 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Hadjipantelis", "Pantelis Z.", ""], ["Jones", "Nick S.", ""], ["Moriarty", "John", ""], ["Springate", "David A.", ""], ["Knight", "Christopher G.", ""]]}, {"id": "1212.3953", "submitter": "Klaus Nordhausen", "authors": "Pauliina Ilmonen, Klaus Nordhausen, Hannu Oja and Esa Ollila", "title": "On asymptotics of ICA estimators and their performance indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) has become a popular multivariate\nanalysis and signal processing technique with diverse applications. This paper\nis targeted at discussing theoretical large sample properties of ICA unmixing\nmatrix functionals. We provide a formal definition of unmixing matrix\nfunctional and consider two popular estimators in detail: the family based on\ntwo scatter matrices with the independence property (e.g., FOBI estimator) and\nthe family of deflation-based fastICA estimators. The limiting behavior of the\ncorresponding estimates is discussed and the asymptotic normality of the\ndeflation-based fastICA estimate is proven under general assumptions.\nFurthermore, properties of several performance indices commonly used for\ncomparison of different unmixing matrix estimates are discussed and a new\nperformance index is proposed. The proposed index fullfills three desirable\nfeatures which promote its use in practice and distinguish it from others.\nNamely, the index possesses an easy interpretation, is fast to compute and its\nasymptotic properties can be inferred from asymptotics of the unmixing matrix\nestimate. We illustrate the derived asymptotical results and the use of the\nproposed index with a small simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 10:28:55 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Ilmonen", "Pauliina", ""], ["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""], ["Ollila", "Esa", ""]]}, {"id": "1212.4307", "submitter": "Francois Vernotte", "authors": "Eric Lantz and Francois Vernotte", "title": "Can we define a best estimator in simple 1-D cases ?", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a small number of measurements, the three most well known estimators of\na single parameter give very different results when estimating a scale\nparameter. A transformation of this scale parameter to a location parameter by\nusing logarithms of the data rends the three estimators equivalent in the\nabsence of any a priori information.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 11:09:46 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 19:32:39 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Lantz", "Eric", ""], ["Vernotte", "Francois", ""]]}, {"id": "1212.4693", "submitter": "Michael Betancourt", "authors": "M. J. Betancourt", "title": "A General Metric for Riemannian Manifold Hamiltonian Monte Carlo", "comments": "13 pages, 10 images", "journal-ref": "First International Conference, GSI 2013, Paris, France, August\n  28-30, 2013. Proceedings", "doi": "10.1007/978-3-642-40020-9_35", "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) is an invaluable means of inference with\ncomplicated models, and Hamiltonian Monte Carlo, in particular Riemannian\nManifold Hamiltonian Monte Carlo (RMHMC), has demonstrated impressive success\nin many challenging problems. Current RMHMC implementations, however, rely on a\nRiemannian metric that limits their application to analytically-convenient\nmodels. In this paper I propose a new metric for RMHMC without these\nlimitations and verify its success on a distribution that emulates many\nhierarchical and latent models.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 15:04:37 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2013 21:20:59 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Betancourt", "M. J.", ""]]}, {"id": "1212.4767", "submitter": "Kody Law", "authors": "Kody J. H. Law", "title": "Proposals which speed-up function-space MCMC", "comments": null, "journal-ref": null, "doi": "10.1016/j.cam.2013.07.026", "report-no": null, "categories": "stat.ME math.DS physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems lend themselves naturally to a Bayesian formulation, in\nwhich the quantity of interest is a posterior distribution of state and/or\nparameters given some uncertain observations. For the common case in which the\nforward operator is smoothing, then the inverse problem is ill-posed.\nWell-posedness is imposed via regularisation in the form of a prior, which is\noften Gaussian. Under quite general conditions, it can be shown that the\nposterior is absolutely continuous with respect to the prior and it may be\nwell-defined on function space in terms of its density with respect to the\nprior. In this case, by constructing a proposal for which the prior is\ninvariant, one can define Metropolis-Hastings schemes for MCMC which are\nwell-defined on function space, and hence do not degenerate as the dimension of\nthe underlying quantity of interest increases to infinity, e.g. under mesh\nrefinement when approximating PDE in finite dimensions. However, in practice,\ndespite the attractive theoretical properties of the currently available\nschemes, they may still suffer from long correlation times, particularly if the\ndata is very informative about some of the unknown parameters. In fact, in this\ncase it may be the directions of the posterior which coincide with the (already\nknown) prior which decorrelate the slowest. The information incorporated into\nthe posterior through the data is often contained within some\nfinite-dimensional subspace, in an appropriate basis, perhaps even one defined\nby eigenfunctions of the prior. We aim to exploit this fact and improve the\nmixing time of function-space MCMC by careful rescaling of the proposal. To\nthis end, we introduce two new basic methods of increasing complexity,\ninvolving (i) characteristic function truncation of high frequencies and (ii)\nhessian information to interpolate between low and high frequencies.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 18:03:43 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2013 15:15:12 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2013 23:14:46 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Law", "Kody J. H.", ""]]}, {"id": "1212.5049", "submitter": "Gabriele Cantaluppi", "authors": "Gabriele Cantaluppi", "title": "A Partial Least Squares Algorithm Handling Ordinal Variables also in\n  Presence of a Small Number of Categories", "comments": "33 pages, 13 figures, 9 tables, submitted", "journal-ref": null, "doi": null, "report-no": "Universit\\`a Cattolica del Sacro Cuore, Milano, Dipartimento di\n  Scienze statistiche, Quaderno di Dipartimento N. 14, Serie E.P. N. 144", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial least squares (PLS) is a popular modeling technique commonly used\nin social sciences. The traditional PLS algorithm deals with variables measured\non interval scales while data are often collected on ordinal scales: a\nreformulation of the algorithm, named ordinal PLS (OPLS), is introduced, which\nproperly deals with ordinal variables. An application to customer satisfaction\ndata and some simulations are also presented. The technique seems to perform\nbetter than the traditional PLS when the number of categories of the items in\nthe questionnaire is small (4 or 5) which is typical in the most common\npractical situations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 14:20:43 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Cantaluppi", "Gabriele", ""]]}, {"id": "1212.5090", "submitter": "Jouchi Nakajima", "authors": "Jouchi Nakajima", "title": "Bayesian analysis of multivariate stochastic volatility with skew\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate stochastic volatility models with skew distributions are\nproposed. Exploiting Cholesky stochastic volatility modeling, univariate\nstochastic volatility processes with leverage effect and generalized hyperbolic\nskew t-distributions are embedded to multivariate analysis with time-varying\ncorrelations. Bayesian prior works allow this approach to provide parsimonious\nskew structure and to easily scale up for high-dimensional problem. Analyses of\ndaily stock returns are illustrated. Empirical results show that the\ntime-varying correlations and the sparse skew structure contribute to improved\nprediction performance and VaR forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 15:40:54 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Nakajima", "Jouchi", ""]]}, {"id": "1212.5203", "submitter": "Michael Larsen", "authors": "Michael D. Larsen", "title": "An Experiment with Hierarchical Bayesian Record Linkage", "comments": "14 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In record linkage (RL), or exact file matching, the goal is to identify the\nlinks between entities with information on two or more files. RL is an\nimportant activity in areas including counting the population, enhancing survey\nframes and data, and conducting epidemiological and follow-up studies. RL is\nchallenging when files are very large, no accurate personal identification (ID)\nnumber is present on all files for all units, and some information is recorded\nwith error. Without an unique ID number one must rely on comparisons of names,\naddresses, dates, and other information to find the links. Latent class models\ncan be used to automatically score the value of information for determining\nmatch status. Data for fitting models come from comparisons made within groups\nof units that pass initial file blocking requirements. Data distributions can\nvary across blocks. This article examines the use of prior information and\nhierarchical latent class models in the context of RL.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 19:37:25 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Larsen", "Michael D.", ""]]}, {"id": "1212.5301", "submitter": "Ken Fujimoto", "authors": "Ken Akira Fujimoto and George Karabatsos", "title": "Dependent Dirichlet Process Rating Model (DDP-RM)", "comments": "2 tables and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical IRT rating-scale models assume that the rating category threshold\nparameters are the same over examinees. However, it can be argued that many\nrating data sets violate this assumption. To address this practical\npsychometric problem, we introduce a novel, Bayesian nonparametric IRT model\nfor rating scale items. The model is an infinite-mixture of Rasch partial\ncredit models, based on a localized Dependent Dirichlet process (DDP). The\nmodel treats the rating thresholds as the random parameters that are subject to\nthe mixture, and has (stick-breaking) mixture weights that are\ncovariate-dependent. Thus, the novel model allows the rating category\nthresholds to vary flexibly across items and examinees, and allows the\ndistribution of the category thresholds to vary flexibly as a function of\ncovariates. We illustrate the new model through the analysis of a simulated\ndata set, and through the analysis of a real rating data set that is well-known\nin the psychometric literature. The model is shown to have better\npredictive-fit performance, compared to other commonly used IRT rating models.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 23:54:07 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2013 05:09:47 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2013 01:39:47 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Fujimoto", "Ken Akira", ""], ["Karabatsos", "George", ""]]}, {"id": "1212.5321", "submitter": "Florentina Bunea", "authors": "Florentina Bunea, Luo Xiao", "title": "On the sample covariance matrix estimator of reduced effective rank\n  population matrices, with applications to fPCA", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ602 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 2, 1200-1230", "doi": "10.3150/14-BEJ602", "report-no": "IMS-BEJ-BEJ602", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a unified analysis of the properties of the sample\ncovariance matrix $\\Sigma_n$ over the class of $p\\times p$ population\ncovariance matrices $\\Sigma$ of reduced effective rank $r_e(\\Sigma)$. This\nclass includes scaled factor models and covariance matrices with decaying\nspectrum. We consider $r_e(\\Sigma)$ as a measure of matrix complexity, and\nobtain sharp minimax rates on the operator and Frobenius norm of\n$\\Sigma_n-\\Sigma$, as a function of $r_e(\\Sigma)$ and $\\|\\Sigma\\|_2$, the\noperator norm of $\\Sigma$. With guidelines offered by the optimal rates, we\ndefine classes of matrices of reduced effective rank over which $\\Sigma_n$ is\nan accurate estimator. Within the framework of these classes, we perform a\ndetailed finite sample theoretical analysis of the merits and limitations of\nthe empirical scree plot procedure routinely used in PCA. We show that\nidentifying jumps in the empirical spectrum that consistently estimate jumps in\nthe spectrum of $\\Sigma$ is not necessarily informative for other goals, for\ninstance for the selection of those sample eigenvalues and eigenvectors that\nare consistent estimates of their population counterparts. The scree plot\nmethod can still be used for selecting consistent eigenvalues, for appropriate\nthreshold levels. We provide a threshold construction and also give a rule for\nchecking the consistency of the corresponding sample eigenvectors. We\nspecialize these results and analysis to population covariance matrices with\npolynomially decaying spectra, and extend it to covariance operators with\npolynomially decaying spectra. An application to fPCA illustrates how our\nresults can be used in functional data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 03:13:35 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 19:19:01 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2014 01:36:51 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 13:01:33 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Bunea", "Florentina", ""], ["Xiao", "Luo", ""]]}, {"id": "1212.5405", "submitter": "Adriano Polpo", "authors": "Carlos A. de B. Pereira and Cassio P. de Campos and Adriano Polpo", "title": "Confidence Statements for Ordering Quantiles", "comments": null, "journal-ref": "Entropy 2016, 18, 357", "doi": "10.3390/e18100357", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes Quor, a simple yet effective nonparametric method to\ncompare independent samples with respect to corresponding quantiles of their\npopulations. The method is solely based on the order statistics of the samples,\nand independence is its only requirement. All computations are performed using\nexact distributions with no need for any asymptotic considerations, and yet can\nbe run using a fast quadratic-time dynamic programming idea. Computational\nperformance is essential in high-dimensional domains, such as gene expression\ndata. We describe the approach and discuss on the most important assumptions,\nbuilding a parallel with assumptions and properties of widely used techniques\nfor the same problem. Experiments using real data from biomedical studies are\nperformed to empirically compare Quor and other methods in a classification\ntask over a selection of high-dimensional data sets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 11:47:37 GMT"}, {"version": "v2", "created": "Thu, 17 Jul 2014 18:58:46 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Pereira", "Carlos A. de B.", ""], ["de Campos", "Cassio P.", ""], ["Polpo", "Adriano", ""]]}, {"id": "1212.5449", "submitter": "Shohei Hidaka", "authors": "Shohei Hidaka", "title": "Characterizing Multivariate Information Flows", "comments": "This manuscript is submitted to Proceedings of the National Academy\n  of Sciences of the United States of America", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.DS math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the crucial steps in scientific studies is to specify dependent\nrelationships among factors in a system of interest. Given little knowledge of\na system, can we characterize the underlying dependent relationships through\nobservation of its temporal behaviors? In multivariate systems, there are\npotentially many possible dependent structures confusable with each other, and\nit may cause false detection of illusory dependency between unrelated factors.\nThe present study proposes a new information-theoretic measure with\nconsideration to such potential multivariate relationships. The proposed\nmeasure, called multivariate transfer entropy, is an extension of transfer\nentropy, a measure of temporal predictability. In the simulations and empirical\nstudies, we demonstrated that the proposed measure characterized the latent\ndependent relationships in unknown dynamical systems more accurately than its\nalternative measure.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 14:07:40 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Hidaka", "Shohei", ""]]}, {"id": "1212.5497", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Pierre Latouche, Charles Bouveyron, Patrick Rivera,\n  Laurent Jegou, St\\'ephane Lamass\\'e", "title": "The random subgraph model for the analysis of an ecclesiastical network\n  in Merovingian Gaul", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS691 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 377-405", "doi": "10.1214/13-AOAS691", "report-no": "IMS-AOAS-AOAS691", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades many random graph models have been proposed to\nextract knowledge from networks. Most of them look for communities or, more\ngenerally, clusters of vertices with homogeneous connection profiles. While the\nfirst models focused on networks with binary edges only, extensions now allow\nto deal with valued networks. Recently, new models were also introduced in\norder to characterize connection patterns in networks through mixed\nmemberships. This work was motivated by the need of analyzing a historical\nnetwork where a partition of the vertices is given and where edges are typed. A\nknown partition is seen as a decomposition of a network into subgraphs that we\npropose to model using a stochastic model with unknown latent clusters. Each\nsubgraph has its own mixing vector and sees its vertices associated to the\nclusters. The vertices then connect with a probability depending on the\nsubgraphs only, while the types of edges are assumed to be sampled from the\nlatent clusters. A variational Bayes expectation-maximization algorithm is\nproposed for inference as well as a model selection criterion for the\nestimation of the cluster number. Experiments are carried out on simulated data\nto assess the approach. The proposed methodology is then applied to an\necclesiastical network in Merovingian Gaul. An R code, called Rambo,\nimplementing the inference algorithm is available from the authors upon\nrequest.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 15:48:45 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 12:47:47 GMT"}, {"version": "v3", "created": "Mon, 5 May 2014 06:38:51 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Jernite", "Yacine", ""], ["Latouche", "Pierre", ""], ["Bouveyron", "Charles", ""], ["Rivera", "Patrick", ""], ["Jegou", "Laurent", ""], ["Lamass\u00e9", "St\u00e9phane", ""]]}, {"id": "1212.5517", "submitter": "Benjamin Jourdain", "authors": "Benjamin Jourdain, Tony Leli\\`evre, B{\\l}a\\.zej Miasojedow", "title": "Optimal scaling for the transient phase of Metropolis Hastings\n  algorithms: The longtime behavior", "comments": "Published in at http://dx.doi.org/10.3150/13-BEJ546 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli, Vol. 20, No. 4, 1930-1978 (2014)", "doi": "10.3150/13-BEJ546", "report-no": "IMS-BEJ-BEJ546", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Random Walk Metropolis algorithm on $\\mathbb{R}^n$ with\nGaussian proposals, and when the target probability measure is the $n$-fold\nproduct of a one-dimensional law. It is well known (see Roberts et al. (Ann.\nAppl. Probab. 7 (1997) 110-120)) that, in the limit $n\\to\\infty$, starting at\nequilibrium and for an appropriate scaling of the variance and of the timescale\nas a function of the dimension $n$, a diffusive limit is obtained for each\ncomponent of the Markov chain. In Jourdain et al. (Optimal scaling for the\ntransient phase of the random walk Metropolis algorithm: The mean-field limit\n(2012) Preprint), we generalize this result when the initial distribution is\nnot the target probability measure. The obtained diffusive limit is the\nsolution to a stochastic differential equation nonlinear in the sense of\nMcKean. In the present paper, we prove convergence to equilibrium for this\nequation. We discuss practical counterparts in order to optimize the variance\nof the proposal distribution to accelerate convergence to equilibrium. Our\nanalysis confirms the interest of the constant acceptance rate strategy (with\nacceptance rate between $1/4$ and $1/3$) first suggested in Roberts et al.\n(Ann. Appl. Probab. 7 (1997) 110-120). We also address scaling of the\nMetropolis-Adjusted Langevin Algorithm. When starting at equilibrium, a\ndiffusive limit for an optimal scaling of the variance is obtained in Roberts\nand Rosenthal (J. R. Stat. Soc. Ser. B. Stat. Methodol. 60 (1998) 255-268). In\nthe transient case, we obtain formally that the optimal variance scales very\ndifferently in $n$ depending on the sign of a moment of the distribution, which\nvanishes at equilibrium. This suggest that it is difficult to derive practical\nrecommendations for MALA from such asymptotic results.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 16:42:13 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 09:19:08 GMT"}, {"version": "v3", "created": "Tue, 21 Oct 2014 13:24:03 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Jourdain", "Benjamin", ""], ["Leli\u00e8vre", "Tony", ""], ["Miasojedow", "B\u0142a\u017cej", ""]]}, {"id": "1212.5586", "submitter": "Eisa Mahmoudi", "authors": "Eisa Mahmoudi and Afsaneh Sepahdar", "title": "Exponentiated Weibull-Poisson distribution: model, properties and\n  applications", "comments": "arXiv admin note: text overlap with arXiv:1206.4008", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new four-parameters distribution with increasing,\ndecreasing, bathtub-shaped and unimodal failure rate, called as the\nexponentiated Weibull-Poisson (EWP) distribution. The new distribution arises\non a latent complementary risk problem base and is obtained by compounding\nexponentiated Weibull (EW) and Poisson distributions. This distribution\ncontains several lifetime sub-models such as: generalized exponential-Poisson\n(GEP), complementary Weibull-Poisson (CWP), complementary exponential-Poisson\n(CEP), exponentiated Rayleigh-Poisson (ERP) and Rayleigh-Poisson (RP)\ndistributions.\n  We obtain several properties of the new distribution such as its probability\ndensity function, its reliability and failure rate functions, quantiles and\nmoments. The maximum likelihood estimation procedure via a EM-algorithm is\npresented in this paper. Sub-models of the EWP distribution are studied in\ndetails. In the end, Applications to two real data sets are given to show the\nflexibility and potentiality of the new distribution.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 20:51:05 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Mahmoudi", "Eisa", ""], ["Sepahdar", "Afsaneh", ""]]}, {"id": "1212.5613", "submitter": "Eisa Mahmoudi", "authors": "Eisa Mahmoudi and Mitra Shiran", "title": "Exponentiated Weibull Power Series Distributions and its Applications", "comments": "arXiv admin note: substantial text overlap with arXiv:1206.4008,\n  arXiv:1204.4248, arXiv:1212.5586", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the exponentiated Weibull power series (EWPS)\nclass of distributions which is obtained by compounding exponentiated Weibull\nand power series distributions, where the compounding procedure follows same\nway that was previously carried out by Roman et al. (2010) and Cancho et al.\n(2011) in introducing the complementary exponential-geometric (CEG) and the\ntwo-parameter Poisson-exponential (PE) lifetime distributions, respectively.\nThis distribution contains several lifetime models such as: exponentiated\nweibull-geometric (EWG), exponentiated weibull-binomial (EWB), exponentiated\nweibull-poisson (EWP), exponentiated weibull-logarithmic (EWL) distributions as\na special case.\n  The hazard rate function of the EWPS distribution can be increasing,\ndecreasing, bathtub-shaped and unimodal failure rate among others. We obtain\nseveral properties of the EWPS distribution such as its probability density\nfunction, its reliability and failure rate functions, quantiles and moments.\nThe maximum likelihood estimation procedure via a EM-algorithm is presented in\nthis paper. Sub-models of the EWPS distribution are studied in details. In the\nend, Applications to two real data sets are given to show the flexibility and\npotentiality of the EWPS distribution.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 21:21:48 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Mahmoudi", "Eisa", ""], ["Shiran", "Mitra", ""]]}, {"id": "1212.5615", "submitter": "Eisa Mahmoudi", "authors": "Ali Akbar Jafari and Eisa Mahmoudi", "title": "Beta-Linear Failure Rate Distribution and its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper a new four-parameter generalized version of the\nlinear failure rate (LFR) distribution which is called Beta-linear failure rate\n(BLFR) distribution. The new distribution is quite flexible and can be used\neffectively in modeling survival data and reliability problems. It can have a\nconstant, decreasing, increasing, upside-down bathtub (unimodal) and\nbathtub-shaped failure rate function depending on its parameters. It includes\nsome well-known lifetime distributions as special submodels. We provide a\ncomprehensive account of the mathematical properties of the new distributions.\nIn particular, A closed-form expressions for the density, cumulative\ndistribution and hazard rate function of the BLFR is given. Also, the $r$th\norder moment of this distribution is derived. We discuss maximum likelihood\nestimation of the unknown parameters of the new model for complete sample and\nobtain an expression for Fishers information matrix. In the end, to show the\nflexibility of this distribution and illustrative purposes, an application\nusing a real data set is presented.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 21:41:52 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Jafari", "Ali Akbar", ""], ["Mahmoudi", "Eisa", ""]]}, {"id": "1212.5669", "submitter": "Viktor Witkovsky", "authors": "Viktor Witkovsk\\'y", "title": "Estimation, Testing, and Prediction Regions of the Fixed and Random\n  Effects by Solving the Henderson's Mixed Model Equations", "comments": null, "journal-ref": "Measurement Science Review, Vol. 12, No. 6, 2012, 234-248", "doi": "10.2478/v10048-012-0033-6", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present a brief overview of the methods for making statistical inference\n(testing statistical hypotheses, construction of confidence and/or prediction\nintervals and regions) about linear functions of the fixed effects and/or about\nthe fixed and random effects simultaneously, in conventional simple linear\nmixed model. The presented approach is based on solutions from the Henderson's\nmixed model equations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 09:07:35 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 08:59:48 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Witkovsk\u00fd", "Viktor", ""]]}, {"id": "1212.5715", "submitter": "Nakahiro Yoshida", "authors": "Masayuki Uchida and Nakahiro Yoshida", "title": "Nondegeneracy of Random Field and Estimation of Diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a quasi likelihood analysis for diffusions under the\nhigh-frequency sampling over a finite time interval. For this, we prove a\npolynomial type large deviation inequality for the quasi likelihood random\nfield. Then it becomes crucial to prove nondegeneracy of a key index chi_0. By\nnature of the sampling setting, chi_0 is random. This makes it difficult to\napply a naive sufficient condition, and requires a new machinery. In order to\nestablish a quasi likelihood analysis, we need quantitative estimate of the\nnondegeneracy of chi_0. The existence of a nondegenerate local section of a\ncertain tensor bundle associated with the statistical random field solves this\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 19:16:45 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Uchida", "Masayuki", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "1212.5760", "submitter": "Paul McNicholas", "authors": "Yuhong Wei and Paul D. McNicholas", "title": "Mixture Model Averaging for Clustering", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-014-0182-6", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mixture model-based clustering applications, it is common to fit several\nmodels from a family and report clustering results from only the `best' one. In\nsuch circumstances, selection of this best model is achieved using a model\nselection criterion, most often the Bayesian information criterion. Rather than\nthrow away all but the best model, we average multiple models that are in some\nsense close to the best one, thereby producing a weighted average of clustering\nresults. Two (weighted) averaging approaches are considered: averaging the\ncomponent membership probabilities and averaging models. In both cases, Occam's\nwindow is used to determine closeness to the best model and weights are\ncomputed within a Bayesian model averaging paradigm. In some cases, we need to\nmerge components before averaging; we introduce a method for merging mixture\ncomponents based on the adjusted Rand index. The effectiveness of our\nmodel-based clustering averaging approaches is illustrated using a family of\nGaussian mixture models on real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2012 04:29:13 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2013 14:26:16 GMT"}, {"version": "v3", "created": "Sat, 26 Jul 2014 20:36:39 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Wei", "Yuhong", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1212.6006", "submitter": "Tomokazu Konishi", "authors": "Tomokazu Konishi", "title": "Principal Component Analysis for Experiments", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Although principal component analysis is frequently applied to\nreduce the dimensionality of matrix data, the method is sensitive to noise and\nbias and has difficulty with comparability and interpretation. These issues are\naddressed by improving the fidelity to the study design. Principal axes and the\ncomponents for variables are found through the arrangement of the training data\nset, and the centers of data are found according to the design. By using both\nthe axes and the center, components for an observation that belong to various\nstudies can be separately estimated. Both of the components for variables and\nobservations are scaled to a unit length, which enables relationships to be\nseen between them.\n  Results: Analyses in transcriptome studies showed an improvement in the\nseparation of experimental groups and in robustness to bias and noise. Unknown\nsamples were appropriately classified on predetermined axes. These axes well\nreflected the study design, and this facilitated the interpretation. Together,\nthe introduced concepts resulted in improved generality and objectivity in the\nanalytical results, with the ability to locate hidden structures in the data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 09:33:06 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Konishi", "Tomokazu", ""]]}, {"id": "1212.6020", "submitter": "Gordon J Ross", "authors": "Gordon J. Ross, Dimitris K. Tasoulis, Niall M. Adams", "title": "Sequential Monitoring of a Bernoulli Sequence when the Pre-change\n  Parameter is Unknown", "comments": "Computational Statistics, March 2012", "journal-ref": "Computational Statistics (2012), 28:463-479", "doi": "10.1007/s00180-012-0311-7", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of monitoring for a change in the mean of a sequence of Bernoulli\nrandom variables has been widely studied. However most existing approaches make\nat least one of the following assumptions, which may be violated in many\nreal-world situations: 1) the pre-change value of the Bernoulli parameter is\nknown in advance, 2) computational efficiency is not paramount, and 3) enough\nobservations occur between change points to allow asymptotic approximations to\nbe used. We develop a novel change detection method based on Fisher's Exact\nTest which does not make any of these assumptions. We show that our method can\nbe implemented in a computationally efficient manner, and is hence suited to\nsequential monitoring where new observations are constantly being received over\ntime. We assess our method's performance empirically via using simulated data,\nand find that it is comparable to the optimal CUSUM scheme which assumes both\npre- and post-change values of the parameter to be known.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 11:12:21 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ross", "Gordon J.", ""], ["Tasoulis", "Dimitris K.", ""], ["Adams", "Niall M.", ""]]}, {"id": "1212.6232", "submitter": "Wei Lin", "authors": "Wei Lin and Jinchi Lv", "title": "High-Dimensional Sparse Additive Hazards Regression", "comments": "41 pages, 3 figures, to appear in Journal of the American Statistical\n  Association (http://www.tandfonline.com/r/JASA)", "journal-ref": "Journal of the American Statistical Association (2013), 108,\n  247-264", "doi": "10.1080/01621459.2012.746068", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional sparse modeling with censored survival data is of great\npractical importance, as exemplified by modern applications in high-throughput\ngenomic data analysis and credit risk analysis. In this article, we propose a\nclass of regularization methods for simultaneous variable selection and\nestimation in the additive hazards model, by combining the nonconcave penalized\nlikelihood approach and the pseudoscore method. In a high-dimensional setting\nwhere the dimensionality can grow fast, polynomially or nonpolynomially, with\nthe sample size, we establish the weak oracle property and oracle property\nunder mild, interpretable conditions, thus providing strong performance\nguarantees for the proposed methodology. Moreover, we show that the regularity\nconditions required by the $L_1$ method are substantially relaxed by a certain\nclass of sparsity-inducing concave penalties. As a result, concave penalties\nsuch as the smoothly clipped absolute deviation (SCAD), minimax concave penalty\n(MCP), and smooth integration of counting and absolute deviation (SICA) can\nsignificantly improve on the $L_1$ method and yield sparser models with better\nprediction performance. We present a coordinate descent algorithm for efficient\nimplementation and rigorously investigate its convergence properties. The\npractical utility and effectiveness of the proposed methods are demonstrated by\nsimulation studies and a real data example.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 19:31:41 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Lin", "Wei", ""], ["Lv", "Jinchi", ""]]}, {"id": "1212.6234", "submitter": "Peter Hoff", "authors": "Peter Hoff, Bailey Fosdick, Alex Volfovsky, Katherine Stovel", "title": "Likelihoods for fixed rank nomination networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies that gather social network data use survey methods that lead to\ncensored, missing or otherwise incomplete information. For example, the popular\nfixed rank nomination (FRN) scheme, often used in studies of schools and\nbusinesses, asks study participants to nominate and rank at most a small number\nof contacts or friends, leaving the existence other relations uncertain.\nHowever, most statistical models are formulated in terms of completely observed\nbinary networks. Statistical analyses of FRN data with such models ignore the\ncensored and ranked nature of the data and could potentially result in\nmisleading statistical inference. To investigate this possibility, we compare\nparameter estimates obtained from a likelihood for complete binary networks to\nthose from a likelihood that is derived from the FRN scheme, and therefore\nrecognizes the ranked and censored nature of the data. We show analytically and\nvia simulation that the binary likelihood can provide misleading inference, at\nleast for certain model parameters that relate network ties to characteristics\nof individuals and pairs of individuals. We also compare these different\nlikelihoods in a data analysis of several adolescent social networks. For some\nof these networks, the parameter estimates from the binary and FRN likelihoods\nlead to different conclusions, indicating the importance of analyzing FRN data\nwith a method that accounts for the FRN survey design.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 19:35:55 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Hoff", "Peter", ""], ["Fosdick", "Bailey", ""], ["Volfovsky", "Alex", ""], ["Stovel", "Katherine", ""]]}, {"id": "1212.6546", "submitter": "Richard Warr", "authors": "Richard L. Warr", "title": "Numerical Approximation of Probability Mass Functions Via the Inverse\n  Discrete Fourier Transform", "comments": null, "journal-ref": null, "doi": "10.1007/s11009-013-9366-3", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  First passage distributions of semi-Markov processes are of interest in\nfields such as reliability, survival analysis, and many others. The problem of\nfinding or computing first passage distributions is, in general, quite\nchallenging. We take the approach of using characteristic functions (or Fourier\ntransforms) and inverting them, to numerically calculate the first passage\ndistribution. Numerical inversion of characteristic functions can be\nnumerically unstable for a general probability measure, however, we show for\nlattice distributions they can be quickly calculated using the inverse discrete\nFourier transform. Using the fast Fourier transform algorithm these\ncomputations can be extremely fast. In addition to the speed of this approach,\nwe are able to prove a few useful bounds for the numerical inversion error of\nthe characteristic functions. These error bounds rely on the existence of a\nfirst or second moment of the distribution, or on an eventual monotonicity\ncondition. We demonstrate these techniques in an example and include R-code.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2012 18:01:26 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Warr", "Richard L.", ""]]}, {"id": "1212.6596", "submitter": "Toshihiro Hirano", "authors": "Toshihiro Hirano", "title": "Pseudo best estimator by a separable approximation of spatial covariance\n  structures", "comments": "28 pages, 3 figures, 7 tables; adding some details", "journal-ref": "Journal of the Japan Statistical Society (2014), Vol.44, 43-71", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a linear regression model with a spatially correlated error term\non a lattice. When estimating coefficients in the linear regression model, the\ngeneralized least squares estimator (GLSE) is used if the covariance structures\nare known. However, the GLSE for large spatial data sets is computationally\nexpensive, because it involves inverting the covariance matrix of error terms\nfrom each observations. To reduce the computational complexity, we propose a\npseudo best estimator (PBE) using spatial covariance structures approximated by\nseparable covariance functions. We derive the asymptotic covariance matrix of\nthe PBE and compare it with those of the least squares estimator (LSE) and the\nGLSE through some simulations. Monte Carlo simulations demonstrate that the PBE\nusing separable covariance functions has superior accuracy to that of the LSE,\nwhich does not contain the information of the spatial covariance structure,\neven if the true process has an isotropic Mat\\'ern covariance function.\nAdditionally, our proposed PBE is computationally efficient relative to the\nGLSE for large spatial data sets.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2012 08:09:37 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2013 05:57:25 GMT"}, {"version": "v3", "created": "Sun, 15 Jun 2014 16:28:30 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Hirano", "Toshihiro", ""]]}]