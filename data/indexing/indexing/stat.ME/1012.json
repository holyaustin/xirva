[{"id": "1012.0073", "submitter": "Richard Barker", "authors": "Richard J. Barker and William A. Link", "title": "Posterior model probabilities computed from model-specific Gibbs output", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible jump Markov chain Monte Carlo (RJMCMC) extends ordinary MCMC\nmethods for use in Bayesian multimodel inference. We show that RJMCMC can be\nimplemented as Gibbs sampling with alternating updates of a model indicator and\na vector-valued \"palette\" of parameters denoted $\\bm \\psi$. Like an artist uses\nthe palette to mix dabs of color for specific needs, we create model-specific\nparameters from the set available in $\\bm \\psi$. This description not only\nremoves some of the mystery of RJMCMC, but also provides a basis for fitting\nmodels one at a time using ordinary MCMC and computing model weights or Bayes\nfactors by post-processing the Monte Carlo output. We illustrate our procedure\nusing several examples.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 00:44:09 GMT"}, {"version": "v2", "created": "Thu, 26 May 2011 01:50:19 GMT"}], "update_date": "2011-05-27", "authors_parsed": [["Barker", "Richard J.", ""], ["Link", "William A.", ""]]}, {"id": "1012.0866", "submitter": "Michele Guindani", "authors": "Edoardo M. Airoldi, Thiago Costa, Federico Bassetti, Fabrizio Leisen\n  and Michele Guindani", "title": "Generalized Species Sampling Priors with Latent Beta reinforcements", "comments": "For correspondence purposes, Edoardo M. Airoldi's email is\n  airoldi@fas.harvard.edu; Federico Bassetti's email is\n  federico.bassetti@unipv.it; Michele Guindani's email is\n  mguindani@mdanderson.org ; Fabrizo Leisen's email is\n  fabrizio.leisen@gmail.com. To appear in the Journal of the American\n  Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular Bayesian nonparametric priors can be characterized in terms of\nexchangeable species sampling sequences. However, in some applications,\nexchangeability may not be appropriate. We introduce a {novel and\nprobabilistically coherent family of non-exchangeable species sampling\nsequences characterized by a tractable predictive probability function with\nweights driven by a sequence of independent Beta random variables. We compare\ntheir theoretical clustering properties with those of the Dirichlet Process and\nthe two parameters Poisson-Dirichlet process. The proposed construction\nprovides a complete characterization of the joint process, differently from\nexisting work. We then propose the use of such process as prior distribution in\na hierarchical Bayes modeling framework, and we describe a Markov Chain Monte\nCarlo sampler for posterior inference. We evaluate the performance of the prior\nand the robustness of the resulting inference in a simulation study, providing\na comparison with popular Dirichlet Processes mixtures and Hidden Markov\nModels. Finally, we develop an application to the detection of chromosomal\naberrations in breast cancer by leveraging array CGH data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Dec 2010 00:03:16 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2012 22:40:09 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2013 12:43:35 GMT"}, {"version": "v4", "created": "Fri, 1 Aug 2014 20:20:34 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Airoldi", "Edoardo M.", ""], ["Costa", "Thiago", ""], ["Bassetti", "Federico", ""], ["Leisen", "Fabrizio", ""], ["Guindani", "Michele", ""]]}, {"id": "1012.0996", "submitter": "Kengo Kamatani", "authors": "Kengo Kamatani", "title": "Local Consistency of Markov Chain Monte Carlo Methods", "comments": "12 pages", "journal-ref": "Ann. Inst. Statist. Math. 66(1) (2014) 63-74", "doi": "10.1007/s10463-013-0403-3", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the notion of efficiency (consistency) and\nexamine some asymptotic properties of Markov chain Monte Carlo methods. We\napply these results to the data augmentation (DA) procedure for independent and\nidentically distributed observations. More precisely, we show that if both the\nsample size and the running time of the DA procedure tend to infinity the\nempirical distribution of the DA procedure tends to the posterior distribution.\nThis is a local property of the DA procedure, which may be, in some cases, more\nhelpful than the global properties to describe its behavior. The advantages of\nusing the local properties are the simplicity and the generality of the\nresults. The local properties provide useful insight into the problem of how to\nconstruct efficient algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 5 Dec 2010 13:37:32 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2011 06:30:35 GMT"}, {"version": "v3", "created": "Sat, 6 Aug 2011 08:03:09 GMT"}, {"version": "v4", "created": "Wed, 25 Sep 2013 06:51:52 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Kamatani", "Kengo", ""]]}, {"id": "1012.1042", "submitter": "Nicolas Bousquet", "authors": "Nicolas Bousquet", "title": "Accelerated Monte Carlo estimation of failure probabilities in output of\n  monotone computer codes", "comments": "Accepted (under another title) in Annales de la Facult\\'e des\n  Sciences de Toulouse - Special Issue on Mathematical Methods for Design and\n  Analysis of Numerical Experiments (in press, 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the probability p=P(g(X<0) is considered when X\nrepresents a multivariate stochastic input of a monotone function g. First, a\nheuristic method to bound p is formally described, involving a specialized\ndesign of numerical experiments. Then a statistical estimation of p is\nconsidered based on a sequential stochastic exploration of the input space. A\nmaximum likelihood estimator of p based on successive dependent Bernoulli data\nis defined and its theoretical convergence properties are studied. Under\nintuitive or mild conditions, the estimation is faster and more robust than the\ntraditional Monte Carlo approach, therefore adapted to time-consuming computer\ncodes g. The main result of the paper is related to the variance of the\nestimator. It appears as a new baseline measure of efficiency under monotone\nconstraints, which could play a similar role to the usual Monte Carlo estimator\nvariance in unconstrained frameworks. Furthermore the bias of the estimator is\nshown to be corrigible via bootstrap heuristics. The behavior of the method is\nillustrated by numerical tests led on a class of toy examples and a more\nrealistic hydraulic case-study.\n  Keywords : monotone function, deterministic computer codes, Monte Carlo\nacceleration, failure probability\n", "versions": [{"version": "v1", "created": "Sun, 5 Dec 2010 21:57:08 GMT"}, {"version": "v2", "created": "Tue, 7 Dec 2010 13:31:52 GMT"}, {"version": "v3", "created": "Wed, 8 Dec 2010 11:54:20 GMT"}, {"version": "v4", "created": "Mon, 13 Dec 2010 08:59:41 GMT"}, {"version": "v5", "created": "Thu, 16 Dec 2010 10:24:41 GMT"}, {"version": "v6", "created": "Tue, 21 Dec 2010 07:35:10 GMT"}, {"version": "v7", "created": "Sat, 28 Apr 2012 12:22:49 GMT"}, {"version": "v8", "created": "Fri, 18 May 2012 07:04:46 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bousquet", "Nicolas", ""]]}, {"id": "1012.1047", "submitter": "Luis Carvalho", "authors": "Luis Carvalho", "title": "A Bayesian Statistical Approach for Inference on Static\n  Origin-Destination Matrices", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of static OD matrix estimation from a formal\nstatistical viewpoint. We adopt a novel Bayesian framework to develop a class\nof models that explicitly cast trip configurations in the study region as\nrandom variables. As a consequence, classical solutions from growth factor,\ngravity, and maximum entropy models are identified to specific estimators under\nthe proposed models. We show that each of these solutions usually account for\nonly a small fraction of the posterior probability mass in the ensemble and we\nthen contend that the uncertainty in the inference should be propagated to\nlater analyses or next-stage models. We also propose alternative, more robust\nestimators and devise Markov chain Monte Carlo sampling schemes to obtain them\nand perform other types of inference. We present several examples showcasing\nthe proposed models and approach, and highlight how other sources of data can\nbe incorporated in the model and inference in a principled, non-heuristic way.\n", "versions": [{"version": "v1", "created": "Sun, 5 Dec 2010 23:00:55 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2011 00:28:23 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Carvalho", "Luis", ""]]}, {"id": "1012.1161", "submitter": "Bradley Efron", "authors": "Bradley Efron", "title": "The Future of Indirect Evidence", "comments": "This paper commented in: [arXiv:1012.1414], [arXiv:1012.1423],\n  [arXiv:1012.1479]. Rejoinder in [arXiv:1012.1489]. Published in at\n  http://dx.doi.org/10.1214/09-STS308 the Statistical Science\n  (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 145-157", "doi": "10.1214/09-STS308", "report-no": "IMS-STS-STS308", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Familiar statistical tests and estimates are obtained by the direct\nobservation of cases of interest: a clinical trial of a new drug, for instance,\nwill compare the drug's effects on a relevant set of patients and controls.\nSometimes, though, indirect evidence may be temptingly available, perhaps the\nresults of previous trials on closely related drugs. Very roughly speaking, the\ndifference between direct and indirect statistical evidence marks the boundary\nbetween frequentist and Bayesian thinking. Twentieth-century statistical\npractice focused heavily on direct evidence, on the grounds of superior\nobjectivity. Now, however, new scientific devices such as microarrays routinely\nproduce enormous data sets involving thousands of related situations, where\nindirect evidence seems too important to ignore. Empirical Bayes methodology\noffers an attractive direct/indirect compromise. There is already some evidence\nof a shift toward a less rigid standard of statistical objectivity that allows\nbetter use of indirect evidence. This article is basically the text of a recent\ntalk featuring some examples from current practice, with a little bit of\nfuturistic speculation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Dec 2010 13:12:53 GMT"}, {"version": "v2", "created": "Wed, 8 Dec 2010 08:33:48 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Efron", "Bradley", ""]]}, {"id": "1012.1297", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Christian Hansen", "title": "LASSO Methods for Gaussian Instrumental Variables Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO,\nsqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate\noptimal instruments in linear instrumental variables (IV) models with many\ninstruments in the canonical Gaussian case. The methods apply even when the\nnumber of instruments is much larger than the sample size. We derive asymptotic\ndistributions for the resulting IV estimators and provide conditions under\nwhich these sparsity-based IV estimators are asymptotically oracle-efficient.\nIn simulation experiments, a sparsity-based IV estimator with a data-driven\npenalty performs well compared to recently advocated many-instrument-robust\nprocedures. We illustrate the procedure in an empirical example using the\nAngrist and Krueger (1991) schooling data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Dec 2010 20:04:51 GMT"}, {"version": "v2", "created": "Wed, 23 Feb 2011 23:39:28 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""]]}, {"id": "1012.1414", "submitter": "Sander Greenland", "authors": "Sander Greenland", "title": "Comment: The Need for Syncretism in Applied Statistics", "comments": "Published in at http://dx.doi.org/10.1214/10-STS308A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 158-161", "doi": "10.1214/10-STS308A", "report-no": "IMS-STS-STS308A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on \"The Need for Syncretism in Applied Statistics\" [arXiv:1012.1161]\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 07:15:06 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Greenland", "Sander", ""]]}, {"id": "1012.1423", "submitter": "Andrew Gelman", "authors": "Andrew Gelman", "title": "Bayesian Statistics Then and Now", "comments": "Published in at http://dx.doi.org/10.1214/10-STS308B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 162-165", "doi": "10.1214/10-STS308B", "report-no": "IMS-STS-STS308B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"The Future of Indirect Evidence\" by Bradley Efron\n[arXiv:1012.1161]\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 07:37:26 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Gelman", "Andrew", ""]]}, {"id": "1012.1479", "submitter": "Robert E. Kass", "authors": "Robert E. Kass", "title": "Comment: How Should Indirect Evidence Be Used?", "comments": "Published in at http://dx.doi.org/10.1214/10-STS308C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 166-169", "doi": "10.1214/10-STS308C", "report-no": "IMS-STS-STS308C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect evidence is crucial for successful statistical practice. Sometimes,\nhowever, it is better used informally. Future efforts should be directed toward\nunderstanding better the connection between statistical methods and scientific\nproblems. [arXiv:1012.1161]\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 11:51:03 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Kass", "Robert E.", ""]]}, {"id": "1012.1489", "submitter": "Bradley Efron", "authors": "Bradley Efron", "title": "Rejoinder: The Future of Indirect Evidence", "comments": "Published in at http://dx.doi.org/10.1214/10-STS308REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 170-171", "doi": "10.1214/10-STS308REJ", "report-no": "IMS-STS-STS308REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"The Future of Indirect Evidence\" [arXiv:1012.1161]\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 12:39:43 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Efron", "Bradley", ""]]}, {"id": "1012.1563", "submitter": "Ya'acov Ritov", "authors": "N. Cohen, E. Greenshtein, and Y. Ritov", "title": "Compound decision in the presence of proxies with an application to\n  spatio-temporal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of incorporating covariates in a compound decision\nsetup. It is desired to estimate the means of $n$ response variables, which are\nindependent and normally distributed, and each is accompanied by a vector of\ncovariates. We suggest a method that involves non-parametric empirical Bayes\ntechniques and may be viewed as a generalization of the celebrated Fay-Herriot\n(1979) method.\n  Some optimality properties of our method are proved. We also compare it\nnumerically with Fay-Herriot and other methods, using a `semi-real' data set\nthat involves spatio-temporal covariates, where the goal is to estimate certain\nproportions in many small areas (Statistical-Areas)\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 17:43:03 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Cohen", "N.", ""], ["Greenshtein", "E.", ""], ["Ritov", "Y.", ""]]}, {"id": "1012.2098", "submitter": "Matt Taddy", "authors": "Matt Taddy", "title": "Multinomial Inverse Regression for Text Analysis", "comments": "Published in the Journal of the American Statistical Association 108,\n  2013, with discussion (rejoinder is here: http://arxiv.org/abs/1304.4200).\n  Software is available in the textir package for R", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text data, including speeches, stories, and other document forms, are often\nconnected to sentiment variables that are of interest for research in\nmarketing, economics, and elsewhere. It is also very high dimensional and\ndifficult to incorporate into statistical analyses. This article introduces a\nstraightforward framework of sentiment-preserving dimension reduction for text\ndata. Multinomial inverse regression is introduced as a general tool for\nsimplifying predictor sets that can be represented as draws from a multinomial\ndistribution, and we show that logistic regression of phrase counts onto\ndocument annotations can be used to obtain low dimension document\nrepresentations that are rich in sentiment information. To facilitate this\nmodeling, a novel estimation technique is developed for multinomial logistic\nregression with very high-dimension response. In particular, independent\nLaplace priors with unknown variance are assigned to each regression\ncoefficient, and we detail an efficient routine for maximization of the joint\nposterior over coefficients and their prior scale. This \"gamma-lasso\" scheme\nyields stable and effective estimation for general high-dimension logistic\nregression, and we argue that it will be superior to current methods in many\nsettings. Guidelines for prior specification are provided, algorithm\nconvergence is detailed, and estimator properties are outlined from the\nperspective of the literature on non-concave likelihood penalization. Related\nwork on sentiment analysis from statistics, econometrics, and machine learning\nis surveyed and connected. Finally, the methods are applied in two detailed\nexamples and we provide out-of-sample prediction studies to illustrate their\neffectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 9 Dec 2010 19:54:27 GMT"}, {"version": "v2", "created": "Wed, 19 Jan 2011 18:34:15 GMT"}, {"version": "v3", "created": "Sun, 24 Jul 2011 23:59:25 GMT"}, {"version": "v4", "created": "Mon, 28 May 2012 16:53:15 GMT"}, {"version": "v5", "created": "Mon, 27 Aug 2012 14:29:08 GMT"}, {"version": "v6", "created": "Mon, 5 Nov 2012 22:16:18 GMT"}, {"version": "v7", "created": "Thu, 8 Aug 2013 14:36:15 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Taddy", "Matt", ""]]}, {"id": "1012.2105", "submitter": "Matt Taddy", "authors": "Matthew A. Taddy and Athanasios Kottas", "title": "Mixture Modeling for Marked Poisson Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general modeling framework for marked Poisson processes observed\nover time or space. The modeling approach exploits the connection of the\nnonhomogeneous Poisson process intensity with a density function. Nonparametric\nDirichlet process mixtures for this density, combined with nonparametric or\nsemiparametric modeling for the mark distribution, yield flexible prior models\nfor the marked Poisson process. In particular, we focus on fully nonparametric\nmodel formulations that build the mark density and intensity function from a\njoint nonparametric mixture, and provide guidelines for straightforward\napplication of these techniques. A key feature of such models is that they can\nyield flexible inference about the conditional distribution for multivariate\nmarks without requiring specification of a complicated dependence scheme. We\naddress issues relating to choice of the Dirichlet process mixture kernels, and\ndevelop methods for prior specification and posterior simulation for full\ninference about functionals of the marked Poisson process. Moreover, we discuss\na method for model checking that can be used to assess and compare goodness of\nfit of different model specifications under the proposed framework. The\nmethodology is illustrated with simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Dec 2010 20:29:53 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2011 16:29:43 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Taddy", "Matthew A.", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1012.2184", "submitter": "Christian P. Robert", "authors": "Andrew Gelman (Columbia University), Christian P. Robert (Universite\n  Paris-Dauphine, IUF, and CREST), and Judith Rousseau (CREST-ENSAE, and\n  Universite Paris-Dauphine)", "title": "Inherent Difficulties of Non-Bayesian Likelihood-based Inference, as\n  Revealed by an Examination of a Recent Book by Aitkin", "comments": "17 pages, 3 figures, revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many decades, statisticians have made attempts to prepare the Bayesian\nomelette without breaking the Bayesian eggs; that is, to obtain probabilistic\nlikelihood-based inferences without relying on informative prior distributions.\nA recent example is Murray Aitkin's recent book, {\\em Statistical Inference},\nwhich presents an approach to statistical hypothesis testing based on\ncomparisons of posterior distributions of likelihoods under competing models.\nAitkin develops and illustrates his method using some simple examples of\ninference from iid data and two-way tests of independence. We analyze in this\nnote some consequences of the inferential paradigm adopted therein, discussing\nwhy the approach is incompatible with a Bayesian perspective and why we do not\nfind it relevant for applied work.\n", "versions": [{"version": "v1", "created": "Fri, 10 Dec 2010 05:29:18 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2011 16:42:19 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Gelman", "Andrew", "", "Columbia University"], ["Robert", "Christian P.", "", "Universite\n  Paris-Dauphine, IUF, and CREST"], ["Rousseau", "Judith", "", "CREST-ENSAE, and\n  Universite Paris-Dauphine"]]}, {"id": "1012.2340", "submitter": "Carlo Berzuini", "authors": "Carlo Berzuini and A. Philip Dawid", "title": "Deep determinism and the assessment of mechanistic interaction between\n  categorical and continuous variables", "comments": "20 pages including the four figures, plus two tables. Submitted to\n  \"Biostatistics\" on November 24, 2010", "journal-ref": "Biostatistics 14 (2013), 502-513", "doi": "10.1093/biostatistics/kxs049", "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim is to detect mechanistic interaction between the effects of two\ncausal factors on a binary response, as an aid to identifying situations where\nthe effects are mediated by a common mechanism. We propose a formalization of\nmechanistic interaction which acknowledges asymmetries of the kind \"factor A\ninterferes with factor B, but not viceversa\". A class of tests for mechanistic\ninteraction is proposed, which works on discrete or continuous causal\nvariables, in any combination. Conditions under which these tests can be\napplied under a generic regime of data collection, be it interventional or\nobservational, are discussed in terms of conditional independence assumptions\nwithin the framework of Augmented Directed Graphs. The scientific relevance of\nthe method and the practicality of the graphical framework are illustrated with\nthe aid of two studies in coronary artery disease. Our analysis relies on the\n\"deep determinism\" assumption that there exists some relevant set V - possibly\nunobserved - of \"context variables\", such that the response Y is a\ndeterministic function of the values of V and of the causal factors of\ninterest. Caveats regarding this assumption in real studies are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 10 Dec 2010 18:45:11 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Berzuini", "Carlo", ""], ["Dawid", "A. Philip", ""]]}, {"id": "1012.3754", "submitter": "Rene Andrae", "authors": "Rene Andrae and Tim Schulze-Hartung and Peter Melchior", "title": "Dos and don'ts of reduced chi-squared", "comments": "12 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduced chi-squared is a very popular method for model assessment, model\ncomparison, convergence diagnostic, and error estimation in astronomy. In this\nmanuscript, we discuss the pitfalls involved in using reduced chi-squared.\nThere are two independent problems: (a) The number of degrees of freedom can\nonly be estimated for linear models. Concerning nonlinear models, the number of\ndegrees of freedom is unknown, i.e., it is not possible to compute the value of\nreduced chi-squared. (b) Due to random noise in the data, also the value of\nreduced chi-squared itself is subject to noise, i.e., the value is uncertain.\nThis uncertainty impairs the usefulness of reduced chi-squared for\ndifferentiating between models or assessing convergence of a minimisation\nprocedure. The impact of noise on the value of reduced chi-squared is\nsurprisingly large, in particular for small data sets, which are very common in\nastrophysical problems. We conclude that reduced chi-squared can only be used\nwith due caution for linear models, whereas it must not be used for nonlinear\nmodels at all. Finally, we recommend more sophisticated and reliable methods,\nwhich are also applicable to nonlinear models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 21:00:50 GMT"}], "update_date": "2010-12-20", "authors_parsed": [["Andrae", "Rene", ""], ["Schulze-Hartung", "Tim", ""], ["Melchior", "Peter", ""]]}, {"id": "1012.3851", "submitter": "Benedikt M. P\\\"otscher", "authors": "Florian Gach and Benedikt M. P\\\"otscher", "title": "Non-Parametric Maximum Likelihood Density Estimation and\n  Simulation-Based Minimum Distance Estimators", "comments": "minor corrections, some discussion added, some material removed", "journal-ref": "Mathematical Methods of Statistics 20, 2011, 288-326", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect inference estimators (i.e., simulation-based minimum distance\nestimators) in a parametric model that are based on auxiliary non-parametric\nmaximum likelihood density estimators are shown to be asymptotically normal. If\nthe parametric model is correctly specified, it is furthermore shown that the\nasymptotic variance-covariance matrix equals the inverse of the\nFisher-information matrix. These results are based on uniform-in-parameters\nconvergence rates and a uniform-in-parameters Donsker-type theorem for\nnon-parametric maximum likelihood density estimators.\n", "versions": [{"version": "v1", "created": "Fri, 17 Dec 2010 11:16:58 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2011 08:34:50 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Gach", "Florian", ""], ["P\u00f6tscher", "Benedikt M.", ""]]}, {"id": "1012.4078", "submitter": "Etienne Roquain", "authors": "Etienne Roquain (PMA)", "title": "Type I error rate control for testing many hypotheses: a survey with\n  proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a survey on some recent advances for the type I error\nrate control in multiple testing methodology. We consider the problem of\ncontrolling the $k$-family-wise error rate (kFWER, probability to make $k$\nfalse discoveries or more) and the false discovery proportion (FDP, proportion\nof false discoveries among the discoveries). The FDP is controlled either via\nits expectation, which is the so-called false discovery rate (FDR), or via its\nupper-tail distribution function. We aim at deriving general and unified\nresults together with concise and simple mathematical proofs. Furthermore,\nwhile this paper is mainly meant to be a survey paper, some new contributions\nfor controlling the kFWER and the upper-tail distribution function of the FDP\nare provided. In particular, we derive a new procedure based on the quantiles\nof the binomial distribution that controls the FDP under independence.\n", "versions": [{"version": "v1", "created": "Sat, 18 Dec 2010 10:14:35 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2011 10:09:33 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Roquain", "Etienne", "", "PMA"]]}, {"id": "1012.4185", "submitter": "Andrew C. Thomas", "authors": "Andrew C. Thomas, Stephen E. Fienberg", "title": "Exploring the Consequences of IED Deployment with a Generalized Linear\n  Model Implementation of the Canadian Traveller Problem", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of improvised explosive devices (IEDs) along major roadways\nhas been a favoured strategy of insurgents in recent war zones, both for the\nability to cause damage to targets along roadways at minimal cost, but also as\na means of controlling the flow of traffic and causing additional expense to\nopposing forces. Among other related approaches (which we discuss), the\nadversarial problem has an analogue in the Canadian Traveller Problem, wherein\na stretch of road is blocked with some independent probability, and the state\nof the road is only discovered once the traveller reaches one of the\nintersections that bound this stretch of road. We discuss the implementation of\nideas from social network analysis, namely the notion of \"betweenness\ncentrality\", and how this can be adapted to the notion of deployment of IEDs\nwith the aid of Generalized Linear Models (GLMs): namely, how we can model the\nprobability of an IED deployment in terms of the increased effort due to\nCanadian betweenness, how we can include expert judgement on the probability of\na deployment, and how we can extend the approach to estimation and updating\nover several time steps.\n", "versions": [{"version": "v1", "created": "Sun, 19 Dec 2010 16:53:07 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Thomas", "Andrew C.", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1012.4255", "submitter": "Gaorong Li", "authors": "Gaorong Li, Heng Peng, Jun Zhang, Lixing Zhu", "title": "Robust rank correlation based screening", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1024 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). arXiv admin note: text overlap with\n  arXiv:0903.5255", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 3, 1846-1877", "doi": "10.1214/12-AOS1024", "report-no": "IMS-AOS-AOS1024", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independence screening is a variable selection method that uses a ranking\ncriterion to select significant variables, particularly for statistical models\nwith nonpolynomial dimensionality or \"large p, small n\" paradigms when p can be\nas large as an exponential of the sample size n. In this paper we propose a\nrobust rank correlation screening (RRCS) method to deal with ultra-high\ndimensional data. The new procedure is based on the Kendall \\tau correlation\ncoefficient between response and predictor variables rather than the Pearson\ncorrelation of existing methods. The new method has four desirable features\ncompared with existing independence screening methods. First, the sure\nindependence screening property can hold only under the existence of a second\norder moment of predictor variables, rather than exponential tails or\nalikeness, even when the number of predictor variables grows as fast as\nexponentially of the sample size. Second, it can be used to deal with\nsemiparametric models such as transformation regression models and single-index\nmodels under monotonic constraint to the link function without involving\nnonparametric estimation even when there are nonparametric functions in the\nmodels. Third, the procedure can be largely used against outliers and influence\npoints in the observations. Last, the use of indicator functions in rank\ncorrelation screening greatly simplifies the theoretical derivation due to the\nboundedness of the resulting statistics, compared with previous studies on\nvariable screening. Simulations are carried out for comparisons with existing\nmethods and a real data example is analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 20 Dec 2010 08:05:32 GMT"}, {"version": "v2", "created": "Thu, 27 Jan 2011 12:45:27 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2012 05:44:09 GMT"}, {"version": "v4", "created": "Wed, 17 Oct 2012 12:24:49 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Li", "Gaorong", ""], ["Peng", "Heng", ""], ["Zhang", "Jun", ""], ["Zhu", "Lixing", ""]]}, {"id": "1012.4397", "submitter": "Xu Han", "authors": "Xu Han, Weijie Gu and Jianqing Fan", "title": "Control of the False Discovery Rate Under Arbitrary Covariance\n  Dependence", "comments": "44 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": "1010.6056v2", "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Multiple hypothesis testing is a fundamental problem in high dimensional\ninference, with wide applications in many scientific fields. In genome-wide\nassociation studies, tens of thousands of tests are performed simultaneously to\nfind if any genes are associated with some traits and those tests are\ncorrelated. When test statistics are correlated, false discovery control\nbecomes very challenging under arbitrary dependence. In the current paper, we\npropose a new methodology based on principal factor approximation, which\nsuccessfully substracts the common dependence and weakens significantly the\ncorrelation structure, to deal with an arbitrary dependence structure. We\nderive the theoretical distribution for false discovery proportion (FDP) in\nlarge scale multiple testing when a common threshold is used and provide a\nconsistent FDP. This result has important applications in controlling FDR and\nFDP. Our estimate of FDP compares favorably with Efron (2007)'s approach, as\ndemonstrated by in the simulated examples. Our approach is further illustrated\nby some real data applications.\n", "versions": [{"version": "v1", "created": "Mon, 20 Dec 2010 16:48:05 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Han", "Xu", ""], ["Gu", "Weijie", ""], ["Fan", "Jianqing", ""]]}, {"id": "1012.4769", "submitter": "Michael Braun", "authors": "Michael Braun and Andr\\'e Bonfrer", "title": "Scalable Inference of Customer Similarities from Interactions Data using\n  Dirichlet Processes", "comments": null, "journal-ref": "Marketing Science 30:3 513-531 2011", "doi": "10.1287/mksc.1110.0640", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the sociological theory of homophily, people who are similar to one\nanother are more likely to interact with one another. Marketers often have\naccess to data on interactions among customers from which, with homophily as a\nguiding principle, inferences could be made about the underlying similarities.\nHowever, larger networks face a quadratic explosion in the number of potential\ninteractions that need to be modeled. This scalability problem renders\nprobability models of social interactions computationally infeasible for all\nbut the smallest networks. In this paper we develop a probabilistic framework\nfor modeling customer interactions that is both grounded in the theory of\nhomophily, and is flexible enough to account for random variation in who\ninteracts with whom. In particular, we present a novel Bayesian nonparametric\napproach, using Dirichlet processes, to moderate the scalability problems that\nmarketing researchers encounter when working with networked data. We find that\nthis framework is a powerful way to draw insights into latent similarities of\ncustomers, and we discuss how marketers can apply these insights to\nsegmentation and targeting activities.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 19:18:49 GMT"}], "update_date": "2011-07-06", "authors_parsed": [["Braun", "Michael", ""], ["Bonfrer", "Andr\u00e9", ""]]}, {"id": "1012.4921", "submitter": "Satoshi Kuriki", "authors": "Satoshi Kuriki, Yoshiaki Harushima, Hironori Fujisawa, Nori Kurata", "title": "Approximate tail probabilities of the maximum of a chi-square field on\n  multi-dimensional lattice points and their applications to detection of loci\n  interactions", "comments": "33 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Define a chi-square random field on a multi-dimensional lattice points index\nset with a direct-product covariance structure, and consider the distribution\nof the maximum of this random field. We provide two approximate formulas for\nthe upper tail probability of the distribution based on nonlinear renewal\ntheory and an integral-geometric approach called the volume-of-tube method.\nThis study is motivated by the detection problem of the interactive loci pairs\nwhich play an important role in forming biological species. The joint\ndistribution of scan statistics for detecting the pairs is regarded as the\nchi-square random field above, and hence the multiplicity-adjusted $p$-value\ncan be calculated by using the proposed approximate formulas. By using these\nformulas, we examine the data of Mizuta, et al. (2010) who reported a new\ninteractive loci pair of rice inter-subspecies.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 10:03:48 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2012 09:50:35 GMT"}, {"version": "v3", "created": "Sat, 30 Mar 2013 05:01:27 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Kuriki", "Satoshi", ""], ["Harushima", "Yoshiaki", ""], ["Fujisawa", "Hironori", ""], ["Kurata", "Nori", ""]]}, {"id": "1012.5066", "submitter": "Yilun Chen", "authors": "Yilun Chen, Yuantao Gu, Alfred O. Hero", "title": "Regularized Least-Mean-Square Algorithms", "comments": "9 pages, double column, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider adaptive system identification problems with convex constraints\nand propose a family of regularized Least-Mean-Square (LMS) algorithms. We show\nthat with a properly selected regularization parameter the regularized LMS\nprovably dominates its conventional counterpart in terms of mean square\ndeviations. We establish simple and closed-form expressions for choosing this\nregularization parameter. For identifying an unknown sparse system we propose\nsparse and group-sparse LMS algorithms, which are special examples of the\nregularized LMS family. Simulation results demonstrate the advantages of the\nproposed filters in both convergence rate and steady-state error under sparsity\nassumptions on the true coefficient vector.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 18:34:08 GMT"}, {"version": "v2", "created": "Thu, 23 Dec 2010 16:50:42 GMT"}], "update_date": "2010-12-24", "authors_parsed": [["Chen", "Yilun", ""], ["Gu", "Yuantao", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1012.5390", "submitter": "Sumeetpal S Singh", "authors": "Pierre Del Moral, Arnaud Doucet, Sumeetpal Singh", "title": "Forward Smoothing using Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": "CUED/F-INFENG/TR638 (Cambridge University Engineering Department)", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods are a widely used set of computational\ntools for inference in non-linear non-Gaussian state-space models. We propose a\nnew SMC algorithm to compute the expectation of additive functionals\nrecursively. Essentially, it is an online or forward-only implementation of a\nforward filtering backward smoothing SMC algorithm proposed in Doucet .et .al\n(2000). Compared to the standard path space SMC estimator whose asymptotic\nvariance increases quadratically with time even under favourable mixing\nassumptions, the asymptotic variance of the proposed SMC estimator only\nincreases linearly with time. This forward smoothing procedure allows us to\nimplement on-line maximum likelihood parameter estimation algorithms which do\nnot suffer from the particle path degeneracy problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 Dec 2010 12:01:25 GMT"}], "update_date": "2010-12-27", "authors_parsed": [["Del Moral", "Pierre", ""], ["Doucet", "Arnaud", ""], ["Singh", "Sumeetpal", ""]]}, {"id": "1012.6033", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Large-scale interval and point estimates from an empirical Bayes\n  extension of confidence posteriors", "comments": null, "journal-ref": "Bickel, D. R. (2012). Empirical Bayes Interval Estimates that are\n  Conditionally Equal to Unadjusted Confidence Intervals or to Default Prior\n  Credibility Intervals. Statistical Applications in Genetics and Molecular\n  Biology, 11 (article 3)", "doi": "10.1515/1544-6115.1765", "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed approach extends the confidence posterior distribution to the\nsemi-parametric empirical Bayes setting. Whereas the Bayesian posterior is\ndefined in terms of a prior distribution conditional on the observed data, the\nconfidence posterior is defined such that the probability that the parameter\nvalue lies in any fixed subset of parameter space, given the observed data, is\nequal to the coverage rate of the corresponding confidence interval. A\nconfidence posterior that has correct frequentist coverage at each fixed\nparameter value is combined with the estimated local false discovery rate to\nyield a parameter distribution from which interval and point estimates are\nderived within the framework of minimizing expected loss. The point estimates\nexhibit suitable shrinkage toward the null hypothesis value, making them\npractical for automatically ranking features in order of priority. The\ncorresponding confidence intervals are also shrunken and tend to be much\nshorter than their fixed-parameter counterparts, as illustrated with gene\nexpression data. Further, simulations confirm a theoretical argument that the\nshrunken confidence intervals cover the parameter at a higher-than-nominal\nfrequency.\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 20:47:55 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Bickel", "David R.", ""]]}]