[{"id": "1509.00051", "submitter": "Laura Tupper", "authors": "Laura L. Tupper, David S. Matteson, C. Lindsay Anderson", "title": "Band Depth Clustering for Nonstationary Time Series and Wind Speed\n  Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the behavior of wind speed over time, using the Eastern Wind\nDataset published by the National Renewable Energy Laboratory. This dataset\ngives wind speeds over three years at hundreds of potential wind farm sites.\nWind speed analysis is necessary to the integration of wind energy into the\npower grid; short-term variability in wind speed affects decisions about usage\nof other power sources, so that the shape of the wind speed curve becomes as\nimportant as the overall level. To assess differences in intra-day time series,\nwe propose a functional distance measure, the band distance, which extends the\nband depth of Lopez-Pintado and Romo (2009). This measure emphasizes the shape\nof time series or functional observations relative to other members of a\ndataset, and allows clustering of observations without reliance on pointwise\nEuclidean distance. To emphasize short-term variability, we examine the\nshort-time Fourier transform of the nonstationary speed time series; we can\nalso adjust for seasonal effects, and use these standardizations as input for\nthe band distance. We show that these approaches to characterizing the data go\nbeyond mean-dependent standard clustering methods, such as k-means, to provide\nmore shape-influenced cluster representatives useful for power grid decisions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 20:28:25 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Tupper", "Laura L.", ""], ["Matteson", "David S.", ""], ["Anderson", "C. Lindsay", ""]]}, {"id": "1509.00103", "submitter": "Rand Wilcox", "authors": "Rand Wilcox", "title": "ANCOVA: A heteroscedastic global test when there is curvature and two\n  covariates", "comments": "19 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two independent groups, let $M_j(\\mathbf{X})$ be some conditional measure\nof location for the $j$th group associated with some random variable $Y$ given\n$\\mathbf {X}=(X_1, X_2)$. Let $\\Omega=\\{\\mathbf{X}_1, \\ldots, \\mathbf{X}_K\\}$\nbe a set of $K$ points to be determined. An extant technique can be used to\ntest $H_0$:\n  $M_1(\\mathbf{X})=M_2(\\mathbf{X})$ for each $\\mathbf{X} \\in \\Omega$ without\nmaking any parametric assumption about $M_j(\\mathbf{X})$. But there are two\ngeneral reasons to suspect that the method can have relatively low power.\n  The paper reports simulation results on an alternative approach that is\ndesigned to test the global hypothesis $H_0$: $M_1(\\mathbf{X})=M_2(\\mathbf{X})$\nfor all $\\mathbf{X} \\in \\Omega$. The main result is that the new method offers\na distinct power advantage.\n  Using data from the Well Elderly 2 study, it is illustrated that the\nalternative method can make a practical difference in terms of detecting a\ndifference between two groups.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 00:54:56 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Wilcox", "Rand", ""]]}, {"id": "1509.00172", "submitter": "Andrew Golightly", "authors": "Chris Sherlock, Andrew Golightly, Daniel A. Henderson", "title": "Adaptive, delayed-acceptance MCMC for targets with expensive likelihoods", "comments": "50 pages (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When conducting Bayesian inference, delayed acceptance (DA)\nMetropolis-Hastings (MH) algorithms and DA pseudo-marginal MH algorithms can be\napplied when it is computationally expensive to calculate the true posterior or\nan unbiased estimate thereof, but a computationally cheap approximation is\navailable. A first accept-reject stage is applied, with the cheap approximation\nsubstituted for the true posterior in the MH acceptance ratio. Only for those\nproposals which pass through the first stage is the computationally expensive\ntrue posterior (or unbiased estimate thereof) evaluated, with a second\naccept-reject stage ensuring that detailed balance is satisfied with respect to\nthe intended true posterior. In some scenarios there is no obvious\ncomputationally cheap approximation. A weighted average of previous evaluations\nof the computationally expensive posterior provides a generic approximation to\nthe posterior. If only the $k$-nearest neighbours have non-zero weights then\nevaluation of the approximate posterior can be made computationally cheap\nprovided that the points at which the posterior has been evaluated are stored\nin a multi-dimensional binary tree, known as a KD-tree. The contents of the\nKD-tree are potentially updated after every computationally intensive\nevaluation. The resulting adaptive, delayed-acceptance [pseudo-marginal]\nMetropolis-Hastings algorithm is justified both theoretically and empirically.\nGuidance on tuning parameters is provided and the methodology is applied to a\ndiscretely observed Markov jump process characterising predator-prey\ninteractions and an ODE system describing the dynamics of an autoregulatory\ngene network.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 08:17:27 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 14:28:49 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Sherlock", "Chris", ""], ["Golightly", "Andrew", ""], ["Henderson", "Daniel A.", ""]]}, {"id": "1509.00331", "submitter": "Fl\\'avio Gon\\c{c}alves", "authors": "Fl\\'avio B Gon\\c{c}alves, Marcos O. Prates, Victor H. Lachos", "title": "Robust Bayesian model selection for heavy-tailed linear regression using\n  finite mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel methodology to perform Bayesian model\nselection in linear models with heavy-tailed distributions. We consider a\nfinite mixture of distributions to model a latent variable where each component\nof the mixture corresponds to one possible model within the symmetrical class\nof normal independent distributions. Naturally, the Gaussian model is one of\nthe possibilities. This allows for a simultaneous analysis based on the\nposterior probability of each model. Inference is performed via Markov chain\nMonte Carlo - a Gibbs sampler with Metropolis-Hastings steps for a class of\nparameters. Simulated examples highlight the advantages of this approach\ncompared to a segregated analysis based on arbitrarily chosen model selection\ncriteria. Examples with real data are presented and an extension to censored\nlinear regression is introduced and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 15:02:26 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 00:31:04 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Gon\u00e7alves", "Fl\u00e1vio B", ""], ["Prates", "Marcos O.", ""], ["Lachos", "Victor H.", ""]]}, {"id": "1509.00351", "submitter": "Judith Lok", "authors": "Judith J. Lok", "title": "Defining and estimating causal direct and indirect effects when setting\n  the mediator to specific values is not feasible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural direct and indirect effects decompose the effect of a treatment into\nthe part that is mediated by a covariate (the mediator) and the part that is\nnot. Their definitions rely on the concept of outcomes under treatment with the\nmediator \"set\" to its value without treatment. Typically, the mechanism through\nwhich the mediator is set to this value is left unspecified, and in many\napplications it may be challenging to fix the mediator to particular values for\neach unit or individual. Moreover, how one sets the mediator may affect the\ndistribution of the outcome. This article introduces \"organic\" direct and\nindirect effects, which can be defined and estimated without relying on setting\nthe mediator to specific values. Organic direct and indirect effects can be\napplied for example to estimate how much of the effect of some treatments for\nHIV/AIDS on mother-to-child transmission of HIV-infection is mediated by the\neffect of the treatment on the HIV viral load in the blood of the mother.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 15:26:15 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Lok", "Judith J.", ""]]}, {"id": "1509.00368", "submitter": "Toby Hocking", "authors": "Toby Dylan Hocking", "title": "A breakpoint detection error function for segmentation model selection\n  and evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider the multiple breakpoint detection problem, which is concerned\nwith detecting the locations of several distinct changes in a one-dimensional\nnoisy data series. We propose the breakpointError, a function that can be used\nto evaluate estimated breakpoint locations, given the known locations of true\nbreakpoints. We discuss an application of the breakpointError for finding\noptimal penalties for breakpoint detection in simulated data. Finally, we show\nhow to relax the breakpointError to obtain an annotation error function which\ncan be used more readily in practice on real data. A fast C implementation of\nan algorithm that computes the breakpointError is available in an R package on\nR-Forge.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 15:55:42 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Hocking", "Toby Dylan", ""]]}, {"id": "1509.00415", "submitter": "Marco Scutari", "authors": "Marco Scutari, Ian Mackay, David Balding", "title": "Using Genetic Distance to Infer the Accuracy of Genomic Prediction", "comments": "36 pages, 9 figures", "journal-ref": "PLoS Genetics 2016, 12(9):e1006288, 1-19", "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of phenotypic traits using high-density genomic data has many\napplications such as the selection of plants and animals of commercial\ninterest; and it is expected to play an increasing role in medical diagnostics.\nStatistical models used for this task are usually tested using\ncross-validation, which implicitly assumes that new individuals (whose\nphenotypes we would like to predict) originate from the same population the\ngenomic prediction model is trained on. In this paper we propose an approach\nbased on clustering and resampling to investigate the effect of increasing\ngenetic distance between training and target populations when predicting\nquantitative traits. This is important for plant and animal genetics, where\ngenomic selection programs rely on the precision of predictions in future\nrounds of breeding. Therefore, estimating how quickly predictive accuracy\ndecays is important in deciding which training population to use and how often\nthe model has to be recalibrated. We find that the correlation between true and\npredicted values decays approximately linearly with respect to either $\\F$ or\nmean kinship between the training and the target populations. We illustrate\nthis relationship using simulations and a collection of data sets from mice,\nwheat and human genetics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 17:43:17 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 21:42:54 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2016 22:14:56 GMT"}, {"version": "v4", "created": "Mon, 11 Jul 2016 13:22:15 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Scutari", "Marco", ""], ["Mackay", "Ian", ""], ["Balding", "David", ""]]}, {"id": "1509.00500", "submitter": "Marianna Pensky", "authors": "Daniela De Canditiis and Marianna Pensky", "title": "Estimation of delta-contaminated density of the random intensity of\n  Poisson data", "comments": "23 pages, 6 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we constructed an estimator of a delta contaminated\nmixing density function $g(\\lambda)$ of the intensity $\\lambda$ of the Poisson\ndistribution. The estimator is based on an expansion of the continuous portion\n$g_0(\\lambda)$ of the unknown pdf over an overcomplete dictionary with the\nrecovery of the coefficients obtained as solution of an optimization problem\nwith Lasso penalty. In order to apply Lasso technique in the, so called,\nprediction setting where it requires virtually no assumptions on dictionary\nand, moreover, to ensure fast convergence of Lasso estimator, we use a novel\nformulation of the optimization problem based on inversion of the dictionary\nelements. The total estimator of the delta contaminated mixing pdf is obtained\nusing a two-stage iterative procedure. We formulate conditions on the\ndictionary and the unknown mixing density that yield a sharp oracle inequality\nfor the norm of the difference between $g_0 (\\lambda)$ and its estimator and,\nthus, obtain a smaller error than in a minimax setting. Numerical simulations\nand comparisons with the Laguerre functions based estimator recently\nconstructed by Comte and Genon-Catalot (2015) also show advantages of our\nprocedure. At last, we apply the technique developed in the paper to estimation\nof a delta contaminated mixing density of the Poisson intensity of the Saturn's\nrings data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 20:47:48 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["De Canditiis", "Daniela", ""], ["Pensky", "Marianna", ""]]}, {"id": "1509.00503", "submitter": "Aaron King", "authors": "Aaron A. King, Dao Nguyen, Edward L. Ionides", "title": "Statistical Inference for Partially Observed Markov Processes via the R\n  Package pomp", "comments": "In press at the Journal of Statistical Software. A version of this\n  paper is provided at the pomp package website: http://kingaa.github.io/pomp", "journal-ref": null, "doi": "10.18637/jss.v069.i12", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Partially observed Markov process (POMP) models, also known as hidden Markov\nmodels or state space models, are ubiquitous tools for time series analysis.\nThe R package pomp provides a very flexible framework for Monte Carlo\nstatistical investigations using nonlinear, non-Gaussian POMP models. A range\nof modern statistical methods for POMP models have been implemented in this\nframework including sequential Monte Carlo, iterated filtering, particle Markov\nchain Monte Carlo, approximate Bayesian computation, maximum synthetic\nlikelihood estimation, nonlinear forecasting, and trajectory matching. In this\npaper, we demonstrate the application of these methodologies using some simple\ntoy problems. We also illustrate the specification of more complex POMP models,\nusing a nonlinear epidemiological model with a discrete population,\nseasonality, and extra-demographic stochasticity. We discuss the specification\nof user-defined models and the development of additional methods within the\nprogramming environment provided by pomp.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 20:59:15 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 21:27:31 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["King", "Aaron A.", ""], ["Nguyen", "Dao", ""], ["Ionides", "Edward L.", ""]]}, {"id": "1509.00650", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis and Annamaria Guolo and Cristiano Varin", "title": "Improving the accuracy of likelihood-based inference in meta-analysis\n  and meta-regression", "comments": null, "journal-ref": "Biometrika 104 (2017) 489-496,", "doi": "10.1093/biomet/asx001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects models are frequently used to synthesise information from\ndifferent studies in meta-analysis. While likelihood-based inference is\nattractive both in terms of limiting properties and of implementation, its\napplication in random-effects meta-analysis may result in misleading\nconclusions, especially when the number of studies is small to moderate. The\ncurrent paper shows how methodology that reduces the asymptotic bias of the\nmaximum likelihood estimator of the variance component can also substantially\nimprove inference about the mean effect size. The results are derived for the\nmore general framework of random-effects meta-regression, which allows the mean\neffect size to vary with study-specific covariates.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 11:50:58 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 11:50:30 GMT"}, {"version": "v3", "created": "Sun, 8 Jan 2017 20:45:00 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 22:39:14 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Kosmidis", "Ioannis", ""], ["Guolo", "Annamaria", ""], ["Varin", "Cristiano", ""]]}, {"id": "1509.00676", "submitter": "Yoichi Arai", "authors": "Yoichi Arai and Hidehiko Ichimura", "title": "Optimal Bandwidth Selection for the Fuzzy Regression Discontinuity\n  Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new bandwidth selection method for the fuzzy regression discontinuity\nestimator is proposed. The method chooses two bandwidths simultaneously, one\nfor each side of the cut-off point by using a criterion based on the estimated\nasymptotic mean square error taking into account a second-order bias term. A\nsimulation study demonstrates the usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 13:04:51 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Arai", "Yoichi", ""], ["Ichimura", "Hidehiko", ""]]}, {"id": "1509.00729", "submitter": "Antonio D'Ambrosio Dr.", "authors": "Carmela Iorio, Gianluca Frasso, Antonio D'Ambrosio and Roberta\n  Siciliano", "title": "Parsimonious Time Series Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": "STAD_Report_02_2015", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a parsimonious model-based framework for clustering time course\ndata. In these applications the computational burden becomes often an issue due\nto the number of available observations. The measured time series can also be\nvery noisy and sparse and a suitable model describing them can be hard to\ndefine. We propose to model the observed measurements by using P-spline\nsmoothers and to cluster the functional objects as summarized by the optimal\nspline coefficients. In principle, this idea can be adopted within all the most\ncommon clustering frameworks. In this work we discuss applications based on a\nk-means algorithm. We evaluate the accuracy and the efficiency of our proposal\nby simulations and by dealing with drosophila melanogaster gene expression\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 15:00:36 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Iorio", "Carmela", ""], ["Frasso", "Gianluca", ""], ["D'Ambrosio", "Antonio", ""], ["Siciliano", "Roberta", ""]]}, {"id": "1509.00803", "submitter": "Antonio D'Ambrosio Dr.", "authors": "Sonia Amodio, Antonio D'Ambrosio, Carmela Iorio and Roberta Siciliano", "title": "Adjusted Concordance Index, an extension of the Adjusted Rand index to\n  fuzzy partitions", "comments": "Work presented at the International Federation of Classification\n  Society (IFCS), Bologna (Italy), 2015", "journal-ref": null, "doi": null, "report-no": "STAD_Report_03_2015", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In comparing clustering partitions, Rand index (RI) and Adjusted Rand index\n(ARI) are commonly used for measuring the agreement between the partitions.\nBoth these external validation indexes aim to analyze how close is a cluster to\na reference (or to prior knowledge about the data) by counting corrected\nclassified pairs of elements. When the aim is to evaluate the solution of a\nfuzzy clustering algorithm, the computation of these measures require\nconverting the soft partitions into hard ones. It is known that different fuzzy\npartitions describing very different structures in the data can lead to the\nsame crisp partition and consequently to the same values of these measures. We\ncompare the existing approaches to evaluate the external validation criteria in\nfuzzy clustering and we propose an extension of the ARI for fuzzy partitions\nbased on the normalized degree of concordance. Through use of real and\nsimulated data, we analyze and evaluate the performance of our proposal.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 18:02:42 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 11:16:28 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Amodio", "Sonia", ""], ["D'Ambrosio", "Antonio", ""], ["Iorio", "Carmela", ""], ["Siciliano", "Roberta", ""]]}, {"id": "1509.00817", "submitter": "Xiaofei Wang", "authors": "Xiaofei Wang and John W. Emerson", "title": "Bayesian Change Point Analysis of Linear Models on Graphs", "comments": "30 pages, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider observations $y_1,\\dots,y_n$ on nodes of a connected graph, where\nthe $y_i$ independently come from $N(\\theta_i, \\sigma^2)$ distributions and an\nunknown partition divides the $n$ observations into blocks. One well-studied\nclass of change point problems assumes the means $\\theta_i$ are equal for all\nnodes within contiguous blocks of a simple graph of sequential observations;\nboth frequentist and Bayesian approaches have been used to estimate the\n$\\theta_i$ and the change points of the underlying partition. This paper\nexamines a broad class of change point problems on general connected graphs in\nwhich a regression model is assumed to apply within each block of the partition\nof the graph. This general class also supports multivariate change point\nproblems. We use Bayesian methods to estimate change points or block boundaries\nof the underlying partition. This paper presents the methodology for the\ngeneral class of change point problems and develops new algorithms for\nimplementation via Markov Chain Monte Carlo. The paper concludes with\nsimulations and real data examples to demonstrate application of the\nmethodology on a wide range of problems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 18:51:02 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Wang", "Xiaofei", ""], ["Emerson", "John W.", ""]]}, {"id": "1509.00882", "submitter": "Amanda Mejia", "authors": "Amanda F. Mejia, Mary Beth Nebel, Ani Eloyan, Brian Caffo and Martin\n  A. Lindquist", "title": "PCA leverage: outlier detection for high-dimensional functional magnetic\n  resonance imaging data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection for high-dimensional (HD) data is a popular topic in modern\nstatistical research. However, one source of HD data that has received\nrelatively little attention is functional magnetic resonance images (fMRI),\nwhich consists of hundreds of thousands of measurements sampled at hundreds of\ntime points. At a time when the availability of fMRI data is rapidly\ngrowing---primarily through large, publicly available grassroots\ndatasets---automated quality control and outlier detection methods are greatly\nneeded. We propose PCA leverage and demonstrate how it can be used to identify\noutlying time points in an fMRI run. Furthermore, PCA leverage is a measure of\nthe influence of each observation on the estimation of principal components,\nwhich are often of interest in fMRI data. We also propose an alternative\nmeasure, PCA robust distance, which is less sensitive to outliers and has\ncontrollable statistical properties. The proposed methods are validated through\nsimulation studies and are shown to be highly accurate. We also conduct a\nreliability study using resting-state fMRI data from the Autism Brain Imaging\nData Exchange and find that removal of outliers using the proposed methods\nresults in more reliable estimation of subject-level resting-state networks\nusing ICA.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 21:37:41 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 20:19:31 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Mejia", "Amanda F.", ""], ["Nebel", "Mary Beth", ""], ["Eloyan", "Ani", ""], ["Caffo", "Brian", ""], ["Lindquist", "Martin A.", ""]]}, {"id": "1509.00899", "submitter": "Souhil Chakar", "authors": "Souhil Chakar", "title": "A robust approach for estimating change-points in the mean of an AR(p)\n  process", "comments": "33 pages, 7 tables, 14 figures. arXiv admin note: text overlap with\n  arXiv:1403.1958", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of change-points estimation in the mean of an AR(p)\nprocess. Taking into account the dependence structure does not allow us to use\nthe approach of the independent case. Especially, the dynamic programming\nalgorithm giving the optimal solution in the independent case cannot be used\nanymore. We propose a two-step method, based on the preliminary robust (to the\nchange-points) estimation of the autoregression parameters. Then, we propose to\nfollow the classical approach, by plugging this estimator in the criterion used\nfor change-point estimation, which is equivalent to decorrelate the series\nusing the estimated autoregression parameters. We show that the asymptotic\nproperties of these change-point location and mean estimators are the same as\nthose of the classical estimators in the independent framework. The same\nplug-in approach is then used to approximate the modified BIC and choose the\nnumber of segments, and to derive a heuristic BIC criterion to select both the\nnumber of changes and the order of the autoregression. Finally, we show, in the\nsimulation section, that for finite sample size taking into account the\ndependence structure improves the statistical performance of the change-point\nestimators and of the selection criterion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 23:22:47 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Chakar", "Souhil", ""]]}, {"id": "1509.00908", "submitter": "Adam Mantz", "authors": "Adam B. Mantz (KIPAC/Stanford)", "title": "A Gibbs Sampler for Multivariate Linear Regression", "comments": "11 pages, 5 figures, 2 tables. Code is available on GitHub at\n  https://github.com/abmantz/lrgs and from CRAN", "journal-ref": "Mon. Not. Roy. Astron. Soc. 457:1279-1288, 2016", "doi": "10.1093/mnras/stv3008", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kelly (2007, hereafter K07) described an efficient algorithm, using Gibbs\nsampling, for performing linear regression in the fairly general case where\nnon-zero measurement errors exist for both the covariates and response\nvariables, where these measurements may be correlated (for the same data\npoint), where the response variable is affected by intrinsic scatter in\naddition to measurement error, and where the prior distribution of covariates\nis modeled by a flexible mixture of Gaussians rather than assumed to be\nuniform. Here I extend the K07 algorithm in two ways. First, the procedure is\ngeneralized to the case of multiple response variables. Second, I describe how\nto model the prior distribution of covariates using a Dirichlet process, which\ncan be thought of as a Gaussian mixture where the number of mixture components\nis learned from the data. I present an example of multivariate regression using\nthe extended algorithm, namely fitting scaling relations of the gas mass,\ntemperature, and luminosity of dynamically relaxed galaxy clusters as a\nfunction of their mass and redshift. An implementation of the Gibbs sampler in\nthe R language, called LRGS, is provided.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 00:51:04 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 19:34:32 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Mantz", "Adam B.", "", "KIPAC/Stanford"]]}, {"id": "1509.00922", "submitter": "Ryan Martin", "authors": "Nicholas Syring and Ryan Martin", "title": "Calibrating general posterior credible regions", "comments": "15 pages, including supplementary material; 2 figures, 3 tables", "journal-ref": "Biometrika, 2019, volume 106, number 2, pages 479--486", "doi": "10.1093/biomet/asy054", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An advantage of methods that base inference on a posterior distribution is\nthat credible regions are readily obtained. Except in well-specified\nsituations, however, there is no guarantee that such regions will achieve the\nnominal frequentist coverage probability, even approximately. To overcome this\ndifficulty, we propose a general strategy that introduces an additional scalar\ntuning parameter to control the posterior spread, and we develop an algorithm\nthat chooses this parameter so that the corresponding credible region achieves\nthe nominal coverage probability.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 02:41:51 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 02:12:54 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 21:19:46 GMT"}, {"version": "v4", "created": "Sun, 22 Apr 2018 13:47:43 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Syring", "Nicholas", ""], ["Martin", "Ryan", ""]]}, {"id": "1509.01010", "submitter": "Sara Algeri", "authors": "Sara Algeri, Jan Conrad and David A. van Dyk", "title": "A method for comparing non-nested models with application to\n  astrophysical searches for new physics", "comments": "We welcome examples of non-nested models testing problems", "journal-ref": "MNRAS Letters, 2016", "doi": "10.1093/mnrasl/slw025", "report-no": null, "categories": "physics.data-an astro-ph.HE hep-ex hep-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searches for unknown physics and decisions between competing astrophysical\nmodels to explain data both rely on statistical hypothesis testing. The usual\napproach in searches for new physical phenomena is based on the statistical\nLikelihood Ratio Test (LRT) and its asymptotic properties. In the common\nsituation, when neither of the two models under comparison is a special case of\nthe other i.e., when the hypotheses are non-nested, this test is not\napplicable. In astrophysics, this problem occurs when two models that reside in\ndifferent parameter spaces are to be compared. An important example is the\nrecently reported excess emission in astrophysical $\\gamma$-rays and the\nquestion whether its origin is known astrophysics or dark matter. We develop\nand study a new, simple, generally applicable, frequentist method and validate\nits statistical properties using a suite of simulations studies. We exemplify\nit on realistic simulated data of the Fermi-LAT $\\gamma$-ray satellite, where\nnon-nested hypotheses testing appears in the search for particle dark matter.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 09:54:14 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 18:28:07 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 10:12:32 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Algeri", "Sara", ""], ["Conrad", "Jan", ""], ["van Dyk", "David A.", ""]]}, {"id": "1509.01058", "submitter": "James Barrett", "authors": "James E. Barrett", "title": "Information-adaptive clinical trials with selective recruitment and\n  binary outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective recruitment designs preferentially recruit individuals that are\nestimated to be statistically informative onto a clinical trial. Individuals\nthat are expected to contribute less information have a lower probability of\nrecruitment. Furthermore, in an information-adaptive design recruits are\nallocated to treatment arms in a manner that maximises information gain. The\ninformativeness of an individual depends on their covariate (or biomarker)\nvalues and how information is defined is a critical element of\ninformation-adaptive designs. In this paper we define and evaluate four\ndifferent methods for quantifying statistical information. Using both\nexperimental data and numerical simulations we show that selective recruitment\ndesigns can offer a substantial increase in statistical power compared to\nrandomised designs. In trials without selective recruitment we find that\nallocating individuals to treatment arms according to information-adaptive\nprotocols also leads to an increase in statistical power. Consequently,\nselective recruitment designs can potentially achieve successful trials using\nfewer recruits thereby offering economic and ethical advantages.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 12:41:28 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 16:10:56 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 15:06:24 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Barrett", "James E.", ""]]}, {"id": "1509.01171", "submitter": "Roman Jandarov", "authors": "Roman A. Jandarov, Lianne A. Sheppard, Paul D. Sampson, Adam A. Szpiro", "title": "A novel principal component analysis for spatially-misaligned\n  multivariate air pollution data", "comments": "43 pages, 5 figures, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel methods for predictive (sparse) PCA with spatially\nmisaligned data. These methods identify principal component loading vectors\nthat explain as much variability in the observed data as possible, while also\nensuring the corresponding principal component scores can be predicted\naccurately by means of spatial statistics at locations where air pollution\nmeasurements are not available. This will make it possible to identify\nimportant mixtures of air pollutants and to quantify their health effects in\ncohort studies, where currently available methods cannot be used. We\ndemonstrate the utility of predictive (sparse) PCA in simulated data and apply\nthe approach to annual averages of particulate matter speciation data from\nnational Environmental Protection Agency (EPA) regulatory monitors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 17:28:45 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Jandarov", "Roman A.", ""], ["Sheppard", "Lianne A.", ""], ["Sampson", "Paul D.", ""], ["Szpiro", "Adam A.", ""]]}, {"id": "1509.01365", "submitter": "Tsagris Michail", "authors": "Konstantinos C. Fragkos, Michail Tsagris, and Christos C. Frangos", "title": "Publication Bias in Meta-Analysis: Confidence Intervals for Rosenthal's\n  Fail-Safe Number", "comments": "Published in the International Scholarly Research Notices in December\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of the present paper is to assess the efficacy of confidence\nintervals for Rosenthal's fail-safe number. Although Rosenthal's estimator is\nhighly used by researchers, its statistical properties are largely unexplored.\nFirst of all, we developed statistical theory which allowed us to produce\nconfidence intervals for Rosenthal's fail-safe number.This was produced by\ndiscerning whether the number of studies analysed in a meta-analysis is fixed\nor random. Each case produces different variance estimators. For a given number\nof studies and a given distribution, we provided five variance estimators.\nConfidence intervals are examined with a normal approximation and a\nnonparametric bootstrap. The accuracy of the different confidence interval\nestimates was then tested by methods of simulation under different\ndistributional assumptions. The half normal distribution variance estimator has\nthe best probability coverage. Finally, we provide a table of lower confidence\nintervals for Rosenthal's estimator.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 08:24:43 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Fragkos", "Konstantinos C.", ""], ["Tsagris", "Michail", ""], ["Frangos", "Christos C.", ""]]}, {"id": "1509.01384", "submitter": "Robert Stelzer", "authors": "Robert Stelzer and \\.Zywilla fechner", "title": "Limit behaviour of the truncated pathwise Fourier-transformation of\n  L\\'evy-driven CARMA processes for non-equidistant discrete time observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a continuous time analogue of the classical\nautoregressive moving average processes, L\\'evy-driven CARMA processes. First\nwe describe limiting properties of the periodogram by means of the so-called\ntruncated Fourier transform if observations are available continuously. The\nobtained results are in accordance with their counterparts from the\ndiscrete-time case. Then we discuss the numerical approximation of the\ntruncated Fourier transform based on non-equidistant high frequency data. In\norder to ensure convergence of the numerical approximation to the true value of\nthe truncated Fourier transform a certain control on the maximal distance\nbetween observations and the number of observations is needed. We obtain both\nconvergence to the continuous time quantity and asymptotic normality under a\nhigh-frequency infinite time horizon limit.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 09:49:26 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 11:04:18 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Stelzer", "Robert", ""], ["fechner", "\u017bywilla", ""]]}, {"id": "1509.01405", "submitter": "Maria Francesca  Marino", "authors": "Maria Francesca Marino, Marco Alf\\'o", "title": "Latent drop-out transitions in quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal data are characterized by the dependence between observations\ncoming from the same individual. In a regression perspective, such a dependence\ncan be usefully ascribed to unobserved features (covariates) specific to each\nindividual. On these grounds, random parameter models with time-constant or\ntime-varying structure are well established in the generalized linear model\ncontext. In the quantile regression framework, specifications based on random\nparameters have only recently known a flowering interest. We start from the\nrecent proposal by Farcomeni (2012) on longitudinal quantile hidden Markov\nmodels, and extend it to handle potentially informative missing data mechanism.\nIn particular, we focus on monotone missingness which may lead to selection\nbias and, therefore, to unreliable inferences on model parameters. We detail\nthe proposed approach by re-analyzing a well known dataset on the dynamics of\nCD4 cell counts in HIV seroconverters and by means of a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 11:20:47 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Marino", "Maria Francesca", ""], ["Alf\u00f3", "Marco", ""]]}, {"id": "1509.01541", "submitter": "S\\'ev\\'erien Nkurunziza", "authors": "Severien Nkurunziza", "title": "On Combining Estimation Problems Under Quadratic Loss: A Generalization", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main theorem in Judge and Mittelhammer [Judge, G. G., and Mittelhammer,\nR. (2004), A Semiparametric Basis for Combining Estimation Problems under\nQuadratic Loss; JASA, 99, 466, 479--487] stipulates that, in the context of\nnonzero correlation, a sufficient condition for the Stein rule (SR)-type\nestimator to dominate the base estimator is that the dimension $k$ should be at\nleast 5. Thanks to some refined inequalities, this dominance result is proved\nin its full generality; for a class of estimators which includes the SR\nestimator as a special case. Namely, we prove that, for any member of the\nderived class, $k\\geqslant 3$ is a sufficient condition regardless of the\ncorrelation factor. We also relax the Gaussian condition of the distribution of\nthe base estimator, as we consider the family of elliptically contoured\nvariates. Finally, we waive the condition on the invertibility of the\nvariance-covariance matrix of the base and the competing estimators. Our\ntheoretical findings are corroborated by some simulation studies, and the\nproposed method is applied to the Cigarette dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 17:52:14 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Nkurunziza", "Severien", ""]]}, {"id": "1509.01570", "submitter": "Aleksey Polunchenko", "authors": "Andrey Pepelyshev and Aleksey S. Polunchenko", "title": "Real-time financial surveillance via quickest change-point detection\n  methods", "comments": "14 pages, 10 figures; to appear in Statistic and Its Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficient financial surveillance aimed at\n\"on-the-go\" detection of structural breaks (anomalies) in \"live\"-monitored\nfinancial time series. With the problem approached statistically, viz. as that\nof multi-cyclic sequential (quickest) change-point detection, we propose a\nsemi-parametric multi-cyclic change-point detection procedure to promptly spot\nanomalies as they occur in the time series under surveillance. The proposed\nprocedure is a derivative of the likelihood ratio-based Shiryaev-Roberts (SR)\nprocedure; the latter is a quasi-Bayesian surveillance method known to deliver\nthe fastest (in the multi-cyclic sense) speed of detection, whatever be the\nfalse alarm frequency. We offer a case study where we first carry out, step by\nstep, statistical analysis of a set of real-world financial data, and then set\nup and devise (a) the proposed SR-based anomaly-detection procedure and (b) the\ncelebrated Cumulative Sum (CUSUM) chart to detect structural breaks in the\ndata. While both procedures performed well, the proposed SR-derivative,\nconforming to the intuition, seemed slightly better.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 19:28:50 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 18:09:58 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Pepelyshev", "Andrey", ""], ["Polunchenko", "Aleksey S.", ""]]}, {"id": "1509.01652", "submitter": "Caleb Miles", "authors": "Caleb H. Miles, Phyllis Kanki, Seema Meloni, and Eric J. Tchetgen\n  Tchetgen", "title": "On Partial Identification of the Pure Direct Effect", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal mediation analysis, nonparametric identification of the pure\n(natural) direct effect typically relies on, in addition to no unobserved\npre-exposure confounding, fundamental assumptions of (i) so-called\n\"cross-world-counterfactuals\" independence and (ii) no exposure- induced\nconfounding. When the mediator is binary, bounds for partial identification\nhave been given when neither assumption is made, or alternatively when assuming\nonly (ii). We extend existing bounds to the case of a polytomous mediator, and\nprovide bounds for the case assuming only (i). We apply these bounds to data\nfrom the Harvard PEPFAR program in Nigeria, where we evaluate the extent to\nwhich the effects of antiretroviral therapy on virological failure are mediated\nby a patient's adherence, and show that inference on this effect is somewhat\nsensitive to model assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 01:07:24 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Miles", "Caleb H.", ""], ["Kanki", "Phyllis", ""], ["Meloni", "Seema", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1509.01688", "submitter": "Yuta Umezu", "authors": "Yuta Umezu, Yusuke Shimizu, Hiroki Masuda, and Yoshiyuki Ninomiya", "title": "AIC for Non-concave Penalized Likelihood Method", "comments": "30 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-concave penalized maximum likelihood methods, such as the Bridge, the\nSCAD, and the MCP, are widely used because they not only do parameter\nestimation and variable selection simultaneously but also have a high\nefficiency as compared to the Lasso. They include a tuning parameter which\ncontrols a penalty level, and several information criteria have been developed\nfor selecting it. While these criteria assure the model selection consistency\nand so have a high value, it is a severe problem that there are no appropriate\nrules to choose the one from a class of information criteria satisfying such a\npreferred asymptotic property. In this paper, we derive an information\ncriterion based on the original definition of the AIC by considering the\nminimization of the prediction error rather than the model selection\nconsistency. Concretely speaking, we derive a function of the score statistic\nwhich is asymptotically equivalent to the non-concave penalized maximum\nlikelihood estimator, and then we provide an asymptotically unbiased estimator\nof the Kullback-Leibler divergence between the true distribution and the\nestimated distribution based on the function. Furthermore, through simulation\nstudies, we check that the performance of the proposed information criterion\ngives almost the same as or better than that of the cross-validation.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 10:30:13 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 09:01:20 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Umezu", "Yuta", ""], ["Shimizu", "Yusuke", ""], ["Masuda", "Hiroki", ""], ["Ninomiya", "Yoshiyuki", ""]]}, {"id": "1509.02029", "submitter": "Clara Happ", "authors": "C. Happ and S. Greven", "title": "Multivariate Functional Principal Component Analysis for Data Observed\n  on Different (Dimensional) Domains", "comments": "Revised Version. R-Code for the online appendix is available in the\n  .zip file associated with this article in subdirectory \"/Software\". The\n  software associated with this article is available on CRAN (packages funData\n  and MFPCA)", "journal-ref": null, "doi": "10.1080/01621459.2016.1273115", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for multivariate functional principal component analysis\nare restricted to data on the same one-dimensional interval. The presented\napproach focuses on multivariate functional data on different domains that may\ndiffer in dimension, e.g. functions and images. The theoretical basis for\nmultivariate functional principal component analysis is given in terms of a\nKarhunen-Lo\\`eve Theorem. For the practically relevant case of a finite\nKarhunen-Lo\\`eve representation, a relationship between univariate and\nmultivariate functional principal component analysis is established. This\noffers an estimation strategy to calculate multivariate functional principal\ncomponents and scores based on their univariate counterparts. For the resulting\nestimators, asymptotic results are derived. The approach can be extended to\nfinite univariate expansions in general, not necessarily orthonormal bases. It\nis also applicable for sparse functional data or data with measurement error. A\nflexible R-implementation is available on CRAN. The new method is shown to be\ncompetitive to existing approaches for data observed on a common\none-dimensional domain. The motivating application is a neuroimaging study,\nwhere the goal is to explore how longitudinal trajectories of a\nneuropsychological test score covary with FDG-PET brain scans at baseline.\nSupplementary material, including detailed proofs, additional simulation\nresults and software is available online.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 13:20:29 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 17:08:56 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 13:54:39 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Happ", "C.", ""], ["Greven", "S.", ""]]}, {"id": "1509.02069", "submitter": "Sharon Lee", "authors": "Sharon X. Lee, Geoffrey J. McLachlan", "title": "EMMIXcskew: an R Package for the Fitting of a Mixture of Canonical\n  Fundamental Skew t-Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an R package EMMIXcskew for the fitting of the canonical\nfundamental skew t-distribution (CFUST) and finite mixtures of this\ndistribution (FM-CFUST) via maximum likelihood (ML). The CFUST distribution\nprovides a flexible family of models to handle non-normal data, with parameters\nfor capturing skewness and heavy-tails in the data. It formally encompasses the\nnormal, t, and skew-normal distributions as special and/or limiting cases. A\nfew other versions of the skew t-distributions are also nested within the CFUST\ndistribution. In this paper, an Expectation-Maximization (EM) algorithm is\ndescribed for computing the ML estimates of the parameters of the FM-CFUST\nmodel, and different strategies for initializing the algorithm are discussed\nand illustrated. The methodology is implemented in the EMMIXcskew package, and\nexamples are presented using two real datasets. The EMMIXcskew package contains\nfunctions to fit the FM-CFUST model, including procedures for generating\ndifferent initial values. Additional features include random sample generation\nand contour visualization in 2D and 3D.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 15:00:55 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 14:25:13 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1509.02124", "submitter": "Yajuan  Si", "authors": "Yajuan Si, Jerome P. Reiter and D. Sunshine Hillygus", "title": "Bayesian Latent Pattern Mixture Models for Handling Attrition in Panel\n  Studies With Refreshment Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many panel studies collect refreshment samples---new, randomly sampled\nrespondents who complete the questionnaire at the same time as a subsequent\nwave of the panel. With appropriate modeling, these samples can be leveraged to\ncorrect inferences for biases caused by non-ignorable attrition. We present\nsuch a model when the panel includes many categorical survey variables. The\nmodel relies on a Bayesian latent pattern mixture model, in which an indicator\nfor attrition and the survey variables are modeled jointly via a latent class\nmodel. We allow the multinomial probabilities within classes to depend on the\nattrition indicator, which offers additional flexibility over standard\napplications of latent class models. We present results of simulation studies\nthat illustrate the benefits of this flexibility. We apply the model to correct\nattrition bias in an analysis of data from the 2007-2008 Associated Press/Yahoo\nNews election panel study.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 17:06:06 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Si", "Yajuan", ""], ["Reiter", "Jerome P.", ""], ["Hillygus", "D. Sunshine", ""]]}, {"id": "1509.02179", "submitter": "Mike Ludkovski", "authors": "Michael Ludkovski", "title": "Kriging Metamodels and Experimental Design for Bermudan Option Pricing", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two new strategies for the numerical solution of optimal\nstopping problems within the Regression Monte Carlo (RMC) framework of\nLongstaff and Schwartz. First, we propose the use of stochastic kriging\n(Gaussian process) meta-models for fitting the continuation value. Kriging\noffers a flexible, nonparametric regression approach that quantifies\napproximation quality. Second, we connect the choice of stochastic grids used\nin RMC to the Design of Experiments paradigm. We examine space-filling and\nadaptive experimental designs; we also investigate the use of batching with\nreplicated simulations at design sites to improve the signal-to-noise ratio.\nNumerical case studies for valuing Bermudan Puts and Max-Calls under a variety\nof asset dynamics illustrate that our methods offer significant reduction in\nsimulation budgets over existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 20:22:38 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 22:24:52 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 18:06:40 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Ludkovski", "Michael", ""]]}, {"id": "1509.02193", "submitter": "Renliang Gu", "authors": "Renliang Gu and Aleksandar Dogand\\v{z}i\\'c", "title": "Polychromatic X-ray CT Image Reconstruction and Mass-Attenuation\n  Spectrum Estimation", "comments": null, "journal-ref": "IEEE Trans. Comput. Imag., vol, 2, no. 2, (2016) 150-165", "doi": "10.1109/TCI.2016.2523431", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for sparse image reconstruction from polychromatic\ncomputed tomography (CT) measurements under the blind scenario where the\nmaterial of the inspected object and the incident-energy spectrum are unknown.\nWe obtain a parsimonious measurement-model parameterization by changing the\nintegral variable from photon energy to mass attenuation, which allows us to\ncombine the variations brought by the unknown incident spectrum and mass\nattenuation into a single unknown mass-attenuation spectrum function; the\nresulting measurement equation has the Laplace integral form. The\nmass-attenuation spectrum is then expanded into first order B-spline basis\nfunctions. We derive a block coordinate-descent algorithm for constrained\nminimization of a penalized negative log-likelihood (NLL) cost function, where\npenalty terms ensure nonnegativity of the spline coefficients and nonnegativity\nand sparsity of the density map. The image sparsity is imposed using\ntotal-variation (TV) and $\\ell_1$ norms, applied to the density-map image and\nits discrete wavelet transform (DWT) coefficients, respectively. This algorithm\nalternates between Nesterov's proximal-gradient (NPG) and limited-memory\nBroyden-Fletcher-Goldfarb-Shanno with box constraints (L-BFGS-B) steps for\nupdating the image and mass-attenuation spectrum parameters. To accelerate\nconvergence of the density-map NPG step, we apply a step-size selection scheme\nthat accounts for varying local Lipschitz constant of the NLL. We consider\nlognormal and Poisson noise models and establish conditions for biconvexity of\nthe corresponding NLLs. We also prove the Kurdyka-{\\L}ojasiewicz property of\nthe objective function, which is important for establishing local convergence\nof the algorithm. Numerical experiments with simulated and real X-ray CT data\ndemonstrate the performance of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 21:10:25 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Gu", "Renliang", ""], ["Dogand\u017ei\u0107", "Aleksandar", ""]]}, {"id": "1509.02319", "submitter": "Enrico Bibbona", "authors": "Enrico Bibbona, Laura Sacerdote, Emiliano Torre", "title": "A copula-based method to build diffusion models with prescribed marginal\n  and serial dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the probabilistic properties that determine the\nexistence of space-time transformations between diffusion processes. We prove\nthat two diffusions are related by a monotone space-time transformation if and\nonly if they share the same serial dependence. The serial dependence of a\ndiffusion process is studied by means of its copula density and the effect of\nmonotone and non-monotone space-time transformations on the copula density is\ndiscussed. This provides us a methodology to build diffusion models by freely\ncombining prescribed marginal behaviors and temporal dependence structures.\nExplicit expressions of copula densities are provided for tractable models. A\npossible application in neuroscience is sketched as a proof of concept.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 10:48:05 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 10:44:30 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Bibbona", "Enrico", ""], ["Sacerdote", "Laura", ""], ["Torre", "Emiliano", ""]]}, {"id": "1509.02380", "submitter": "Marco Compagnoni", "authors": "Marco Compagnoni, Antonio Canclini, Paolo Bestagini, Fabio Antonacci,\n  Augusto Sarti and Stefano Tubaro", "title": "Source localization and denoising: a perspective from the TDOA space", "comments": "25 pages, 9 figures", "journal-ref": "Multidimensional Systems and Signal Processing 2016", "doi": "10.1007/s11045-016-0400-9", "report-no": null, "categories": "cs.SD cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we formulate the problem of denoising Time Differences of\nArrival (TDOAs) in the TDOA space, i.e. the Euclidean space spanned by TDOA\nmeasurements. The method consists of pre-processing the TDOAs with the purpose\nof reducing the measurement noise. The complete set of TDOAs (i.e., TDOAs\ncomputed at all microphone pairs) is known to form a redundant set, which lies\non a linear subspace in the TDOA space. Noise, however, prevents TDOAs from\nlying exactly on this subspace. We therefore show that TDOA denoising can be\nseen as a projection operation that suppresses the component of the noise that\nis orthogonal to that linear subspace. We then generalize the projection\noperator also to the cases where the set of TDOAs is incomplete. We\nanalytically show that this operator improves the localization accuracy, and we\nfurther confirm that via simulation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 14:19:22 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 15:03:41 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Compagnoni", "Marco", ""], ["Canclini", "Antonio", ""], ["Bestagini", "Paolo", ""], ["Antonacci", "Fabio", ""], ["Sarti", "Augusto", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1509.02556", "submitter": "Wang Miao", "authors": "Wang Miao, Lan Liu, Eric Tchetgen Tchetgen, and Zhi Geng", "title": "Identification, Doubly Robust Estimation, and Semiparametric Efficiency\n  Theory of Nonignorable Missing Data With a Shadow Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider identification and estimation with an outcome missing not at\nrandom (MNAR). We study an identification strategy based on a so-called shadow\nvariable. A shadow variable is assumed to be correlated with the outcome, but\nindependent of the missingness process conditional on the outcome and fully\nobserved covariates. We describe a general condition for nonparametric\nidentification of the full data law under MNAR using a valid shadow variable.\nOur condition is satisfied by many commonly-used models; moreover, it is\nimposed on the complete cases, and therefore has testable implications with\nobserved data only. We describe semiparametric estimation methods and evaluate\ntheir performance on both simulation data and a real data example. We\ncharacterize the semiparametric efficiency bound for the class of regular and\nasymptotically linear estimators, and derive a closed form for the efficient\ninfluence function.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 20:59:05 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 16:27:20 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 11:06:19 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Miao", "Wang", ""], ["Liu", "Lan", ""], ["Tchetgen", "Eric Tchetgen", ""], ["Geng", "Zhi", ""]]}, {"id": "1509.02568", "submitter": "Min Wang", "authors": "Min Wang and Guangying Liu", "title": "A simple two-sample Bayesian t-test for hypothesis testing", "comments": "To appear in The American Statistician", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an explicit closed-form Bayes factor for the\nproblem of two-sample hypothesis testing. The proposed approach can be regarded\nas a Bayesian version of the pooled-variance t-statistic and has various\nappealing properties in practical applications. It relies on data only through\nthe t-statistic and can thus be calculated by using an Excel spreadsheet or a\npocket calculator. It avoids several undesirable paradoxes, which may be\nencountered by the previous Bayesian approach of Gonen et al. (2005).\nSpecifically, the proposed approach can be easily taught in an introductory\nstatistics course with an emphasis on Bayesian thinking. Simulated and real\ndata examples are provided for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 21:57:45 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Wang", "Min", ""], ["Liu", "Guangying", ""]]}, {"id": "1509.02574", "submitter": "Elizabeth Roberto", "authors": "Elizabeth Roberto and Jackelyn Hwang", "title": "Barriers to Integration: Physical Boundaries and the Spatial Structure\n  of Residential Segregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite modest declines in residential segregation levels since the Civil\nRights Era, segregation remains a defining feature of the U.S. landscape. This\nstudy highlights the importance of considering physical barriers--features of\nthe urban environment that disconnect locations--when measuring segregation. We\nuse population and geographic data for 20 U.S. Rustbelt cities from the 2010\ndecennial census and a novel approach for measuring and analyzing segregation\nthat incorporates the connectivity of roads and the excess distance imposed by\nphysical barriers, such as highways, railroad tracks, and dead-end streets. We\nfind that physical barriers divide urban space in ways that reinforce or\nexacerbate segregation, but there is substantial variation in the extent to\nwhich they increase segregation both within and across these cities and for\ndifferent ethnoracial groups. By uncovering a new source of variation in the\nsegregation experienced by city residents, the findings have implications for\nunderstanding the mechanisms that contribute to the persistence of segregation\nand the consequences of segregation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 22:47:40 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 22:15:54 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 21:14:40 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Roberto", "Elizabeth", ""], ["Hwang", "Jackelyn", ""]]}, {"id": "1509.02805", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Hierarchical Nearest Neighbor Descent (H-NND)", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previously in 2014, we proposed the Nearest Descent (ND) method, capable of\ngenerating an efficient Graph, called the in-tree (IT). Due to some beautiful\nand effective features, this IT structure proves well suited for data\nclustering. Although there exist some redundant edges in IT, they usually have\nsalient features and thus it is not hard to remove them.\n  Subsequently, in order to prevent the seemingly redundant edges from\noccurring, we proposed the Nearest Neighbor Descent (NND) by adding the\n\"Neighborhood\" constraint on ND. Consequently, clusters automatically emerged,\nwithout the additional requirement of removing the redundant edges. However,\nNND proved still not perfect, since it brought in a new yet worse problem, the\n\"over-partitioning\" problem.\n  Now, in this paper, we propose a method, called the Hierarchical Nearest\nNeighbor Descent (H-NND), which overcomes the over-partitioning problem of NND\nvia using the hierarchical strategy. Specifically, H-NND uses ND to effectively\nmerge the over-segmented sub-graphs or clusters that NND produces. Like ND,\nH-NND also generates the IT structure, in which the redundant edges once again\nappear. This seemingly comes back to the situation that ND faces. However,\ncompared with ND, the redundant edges in the IT structure generated by H-NND\ngenerally become more salient, thus being much easier and more reliable to be\nidentified even by the simplest edge-removing method which takes the edge\nlength as the only measure. In other words, the IT structure constructed by\nH-NND becomes more fitted for data clustering. We prove this on several\nclustering datasets of varying shapes, dimensions and attributes. Besides,\ncompared with ND, H-NND generally takes less computation time to construct the\nIT data structure for the input data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 15:15:44 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 15:43:25 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 15:50:58 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1509.02870", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira and Haruyoshi Maeda", "title": "An information criterion for model selection with missing data via\n  complete-data divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an information criterion to select a parametric model of\ncomplete-data distribution when only incomplete or partially observed data is\navailable. Compared with AIC, our new criterion has an additional penalty term\nfor missing data, which is expressed by the Fisher information matrices of\ncomplete data and incomplete data. We prove that our criterion is an\nasymptotically unbiased estimator of complete-data divergence, namely, the\nexpected Kullback-Leibler divergence between the true distribution and the\nestimated distribution for complete data, whereas AIC is that for the\nincomplete data. Information criteria PDIO (Shimodaira 1994) and AICcd\n(Cavanaugh and Shumway 1998) have been previously proposed to estimate\ncomplete-data divergence, and they have the same penalty term. The additional\npenalty term of our criterion for missing data turns out to be only half the\nvalue of that in PDIO and AICcd. The difference in the penalty term is\nattributed to the fact that our criterion is derived under a weaker assumption.\nA simulation study with the weaker assumption shows that our criterion is\nunbiased while the other two criteria are biased. In addition, we review the\ngeometrical view of alternating minimizations of the EM algorithm. This\ngeometrical view plays an important role in deriving our new criterion.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 17:51:14 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 04:44:13 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 02:13:33 GMT"}, {"version": "v4", "created": "Fri, 4 Nov 2016 03:27:04 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Shimodaira", "Hidetoshi", ""], ["Maeda", "Haruyoshi", ""]]}, {"id": "1509.03108", "submitter": "Joseph B. Lang", "authors": "Joseph B. Lang", "title": "A Closer Look at Testing the \"No-Treatment-Effect\" Hypothesis in a\n  Comparative Experiment", "comments": "Published at http://dx.doi.org/10.1214/15-STS513 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 3, 352-371", "doi": "10.1214/15-STS513", "report-no": "IMS-STS-STS513", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard tests of the \"no-treatment-effect\" hypothesis for a comparative\nexperiment include permutation tests, the Wilcoxon rank sum test, two-sample\n$t$ tests, and Fisher-type randomization tests. Practitioners are aware that\nthese procedures test different no-effect hypotheses and are based on different\nmodeling assumptions. However, this awareness is not always, or even usually,\naccompanied by a clear understanding or appreciation of these differences.\nBorrowing from the rich literatures on causality and finite-population sampling\ntheory, this paper develops a modeling framework that affords answers to\nseveral important questions, including: exactly what hypothesis is being\ntested, what model assumptions are being made, and are there other, perhaps\nbetter, approaches to testing a no-effect hypothesis? The framework lends\nitself to clear descriptions of three main inference approaches: process-based,\nrandomization-based, and selection-based. It also promotes careful\nconsideration of model assumptions and targets of inference, and highlights the\nimportance of randomization. Along the way, Fisher-type randomization tests are\ncompared to permutation tests and a less well-known Neyman-type randomization\ntest. A simulation study compares the operating characteristics of the\nNeyman-type randomization test to those of the other more familiar tests.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 11:35:47 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Lang", "Joseph B.", ""]]}, {"id": "1509.03261", "submitter": "Martin Lysy", "authors": "John W.R. Mellnik, Martin Lysy, Paula A. Vasquez, Natesh S. Pillai,\n  David B. Hill, Jeremy Crib, Scott A. McKinley, M. Gregory Forest", "title": "Maximum Likelihood Estimation for Single Particle, Passive Microrheology\n  Data with Drift", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": "10.1122/1.4943988", "report-no": null, "categories": "cond-mat.soft stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volume limitations and low yield thresholds of biological fluids have led to\nwidespread use of passive microparticle rheology. The mean-squared-displacement\n(MSD) statistics of bead position time series (bead paths) are either applied\ndirectly to determine the creep compliance [Xu et al (1998)] or transformed to\ndetermine dynamic storage and loss moduli [Mason & Weitz (1995)]. A prevalent\nhurdle arises when there is a non-diffusive experimental drift in the data.\nCommensurate with the magnitude of drift relative to diffusive mobility,\nquantified by a P\\'eclet number, the MSD statistics are distorted, and thus the\npath data must be \"corrected\" for drift. The standard approach is to estimate\nand subtract the drift from particle paths, and then calculate MSD statistics.\nWe present an alternative, parametric approach using maximum likelihood\nestimation that simultaneously fits drift and diffusive model parameters from\nthe path data; the MSD statistics (and consequently the compliance and dynamic\nmoduli) then follow directly from the best-fit model. We illustrate and compare\nboth methods on simulated path data over a range of P\\'eclet numbers, where\nexact answers are known. We choose fractional Brownian motion as the numerical\nmodel because it affords tunable, sub-diffusive MSD statistics consistent with\ntypical 30 second long, experimental observations of microbeads in several\nbiological fluids. Finally, we apply and compare both methods on data from\nhuman bronchial epithelial cell culture mucus.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 18:39:56 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 00:31:34 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Mellnik", "John W. R.", ""], ["Lysy", "Martin", ""], ["Vasquez", "Paula A.", ""], ["Pillai", "Natesh S.", ""], ["Hill", "David B.", ""], ["Crib", "Jeremy", ""], ["McKinley", "Scott A.", ""], ["Forest", "M. Gregory", ""]]}, {"id": "1509.03271", "submitter": "Anna Smith", "authors": "Anna Smith, Catherine A. Calder, and Christopher R. Browning", "title": "Empirical Reference Distributions for Networks of Different Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network analysis has become an increasingly prevalent research tool across a\nvast range of scientific fields. Here, we focus on the particular issue of\ncomparing network statistics, i.e. graph-level measures of network structural\nfeatures, across multiple networks that differ in size. Although \"normalized\"\nversions of some network statistics exist, we demonstrate via simulation why\ndirect comparison of raw and normalized statistics is often inappropriate. We\nexamine a recent suggestion to normalize network statistics relative to\nErdos-Renyi random graphs and demonstrate via simulation how this is an\nimprovement over direct comparison, but still sometimes problematic. We propose\na new adjustment method based on a reference distribution constructed as a\nmixture model of random graphs which reflect the dependence structure exhibited\nin the observed networks. We show that using simple Bernoulli models as mixture\ncomponents in this reference distribution can provide adjusted network\nstatistics that are relatively comparable across different network sizes but\nstill describe interesting features of networks, and that this can be\naccomplished at relatively low computational expense. Finally, we apply this\nmethodology to a collection of co-location networks derived from the Los\nAngeles Family and Neighborhood Survey activity location data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 18:51:58 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 17:01:41 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 16:40:00 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Smith", "Anna", ""], ["Calder", "Catherine A.", ""], ["Browning", "Christopher R.", ""]]}, {"id": "1509.03459", "submitter": "Chao Zheng", "authors": "Wen-Xin Zhou, Chao Zheng and Zhen Zhang", "title": "Two-Sample Smooth Tests for the Equality of Distributions", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of testing the equality of two unspecified\ndistributions. The classical omnibus tests such as the Kolmogorov-Smirnov and\nCram\\`er-von Mises are known to suffer from low power against essentially all\nbut location-scale alternatives. We propose a new two-sample test that modifies\nthe Neyman's smooth test and extend it to the multivariate case based on the\nidea of projection pursue. The asymptotic null property of the test and its\npower against local alternatives are studied. The multiplier bootstrap method\nis employed to compute the critical value of the multivariate test. We\nestablish validity of the bootstrap approximation in the case where the\ndimension is allowed to grow with the sample size. Numerical studies show that\nthe new testing procedures perform well even for small sample sizes and are\npowerful in detecting local features or high-frequency components.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 11:10:56 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 10:25:27 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Zhou", "Wen-Xin", ""], ["Zheng", "Chao", ""], ["Zhang", "Zhen", ""]]}, {"id": "1509.03595", "submitter": "Ali Akbar Jafari", "authors": "Ali Akbar Jafari and Saeid Tahmasebi", "title": "Gompertz - Power Series Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the Gompertz power series class of distributions\nwhich is obtained by compounding Gompertz and power series distributions. This\ndistribution contains several lifetime models such as Gompertz-geometric,\nGompertz-Poisson, Gompertz-binomial, and Gompertz-logarithmic distributions as\nspecial cases. Sub-models of the GPS distribution are studied in details. The\nhazard rate function of the GPS distribution can be increasing, decreasing, and\nbathtub-shaped. We obtain several properties of the GPS distribution such as\nits probability density function, and failure rate function, Shannon entropy,\nmean residual life function, quantiles and moments. The maximum likelihood\nestimation procedure via a EM-algorithm is presented, and simulation studies\nare performed for evaluation of this estimation for complete data, and the MLE\nof parameters for censored data. At the end, a real example is given.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 18:02:57 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Jafari", "Ali Akbar", ""], ["Tahmasebi", "Saeid", ""]]}, {"id": "1509.03678", "submitter": "Elizabeth Roberto", "authors": "Elizabeth Roberto", "title": "The Spatial Proximity and Connectivity (SPC) Method for Measuring and\n  Analyzing Residential Segregation", "comments": "32 pages, 12 figures, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been increasing attention to the spatial\ndimensions of residential segregation, such as the spatial arrangement of\nsegregated neighborhoods and the geographic scale or relative size of\nsegregated areas. However, the methods used to measure segregation do not\nincorporate features of the built environment, such as the road connectivity\nbetween locations or the physical barriers that divide groups. This article\nintroduces the Spatial Proximity and Connectivity (SPC) method for measuring\nand analyzing segregation. The SPC method addresses the limitations of current\napproaches by taking into account how the physical structure of the built\nenvironment affects the proximity and connectivity of locations. In this\narticle, I describe the method and its application for studying segregation and\nspatial inequality more broadly. I demonstrate one such application-analyzing\nthe impact of physical barriers on residential segregation-with a stylized\nexample and an empirical analysis of racial segregation in Pittsburgh, PA. The\nSPC method contributes to scholarship on residential segregation by capturing\nthe effect of an important yet understudied mechanism of segregation-the\nconnectivity, or physical barriers, between locations-on the level and spatial\npattern of segregation, and enables further consideration of the role of the\nbuilt environment in segregation processes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 22:09:04 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 21:04:22 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 21:02:51 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Roberto", "Elizabeth", ""]]}, {"id": "1509.03697", "submitter": "Kshitij Khare", "authors": "Bala Rajaratnam and Doug Sparks and Kshitij Khare and Liyuan Zhang", "title": "Scalable Bayesian shrinkage and uncertainty quantification for\n  high-dimensional regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian shrinkage methods have generated a lot of recent interest as tools\nfor high-dimensional regression and model selection. These methods naturally\nfacilitate tractable uncertainty quantification and incorporation of prior\ninformation. This benefit has led to extensive use of the Bayesian shrinkage\nmethods across diverse applications. A common feature of these models is that\nthe corresponding priors on the regression coefficients can be expressed as\nscale mixture of normals. While the three-step Gibbs sampler used to sample\nfrom the often intractable associated posterior density has been shown to be\ngeometrically ergodic for several of these models, it has been demonstrated\nrecently that convergence of this sampler can still be quite slow in modern\nhigh-dimensional settings despite this apparent theoretical safeguard. We\npropose a new method to draw from the same posterior via a tractable two-step\nblocked Gibbs sampler. We demonstrate that our proposed two-step blocked\nsampler exhibits vastly superior convergence behavior compared to the original\nthree-step sampler in high-dimensional regimes on both real and simulated data.\nWe also provide a detailed theoretical underpinning to the new method in the\ncontext of the Bayesian lasso. First, we prove that the proposed two-step\nsampler is geometrically ergodic, and derive explicit upper bounds for the\n(geometric) rate of convergence. Furthermore, we demonstrate theoretically that\nwhile the original Bayesian lasso chain is not Hilbert-Schmidt, the proposed\nchain is trace class (and hence Hilbert-Schmidt). The trace class property\nimplies that the corresponding Markov operator is compact, and its (countably\nmany) eigenvalues are summable. It also facilitates a rigorous comparison of\nthe two-step blocked chain with \"sandwich\" algorithms which aim to improve\nperformance of the two-step chain by inserting an inexpensive extra step.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 02:27:32 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 21:10:38 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Rajaratnam", "Bala", ""], ["Sparks", "Doug", ""], ["Khare", "Kshitij", ""], ["Zhang", "Liyuan", ""]]}, {"id": "1509.03730", "submitter": "Yi Yu", "authors": "Ivor Cribben and Yi Yu", "title": "Estimating whole brain dynamics using spectral clustering", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of time-varying networks for functional Magnetic Resonance\nImaging (fMRI) data sets is of increasing importance and interest. In this\nwork, we formulate the problem in a high-dimensional time series framework and\nintroduce a data-driven method, namely Network Change Points Detection (NCPD),\nwhich detects change points in the network structure of a multivariate time\nseries, with each component of the time series represented by a node in the\nnetwork. NCPD is applied to various simulated data and a resting-state fMRI\ndata set. This new methodology also allows us to identify common functional\nstates within and across subjects. Finally, NCPD promises to offer a deep\ninsight into the large-scale characterisations and dynamics of the brain\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 11:35:04 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 16:55:51 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Cribben", "Ivor", ""], ["Yu", "Yi", ""]]}, {"id": "1509.03737", "submitter": "Laurent Younes", "authors": "Kamel Lahouel, Donald Geman and Laurent Younes", "title": "Coarse-to-fine Multiple Testing Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze control of the familywise error rate (FWER) in a multiple testing\nscenario with a great many null hypotheses about the distribution of a\nhigh-dimensional random variable among which only a very small fraction are\nfalse, or \"active\". In order to improve power relative to conservative\nBonferroni bounds, we explore a coarse-to-fine procedure adapted to a situation\nin which tests are partitioned into subsets, or \"cells\", and active hypotheses\ntend to cluster within cells. We develop procedures for a standard linear model\nwith Gaussian data and a non-parametric case based on generalized permutation\ntesting, and demonstrate considerably higher power than Bonferroni estimates at\nthe same FWER when the active hypotheses do cluster. The main technical\ndifficulty arises from the correlation between the test statistics at the\nindividual and cell levels, which increases the likelihood of a hypothesis\nbeing falsely discovered when the cell that contains it is falsely discovered\n(survivorship bias). This requires sharp estimates of certain quadrant\nprobabilities when a cell is inactive.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 12:43:23 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Lahouel", "Kamel", ""], ["Geman", "Donald", ""], ["Younes", "Laurent", ""]]}, {"id": "1509.03767", "submitter": "Zhihua Su", "authors": "Dennis Cook, Liliana Forzani, Zhihua Su", "title": "Algorithms for Envelope Estimation II", "comments": "38 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for envelope estimation, along with a new root n\nconsistent method for computing starting values. The new algorithm, which does\nnot require optimization over a Grassmannian, is shown by simulation to be much\nfaster and typically more accurate that the best existing algorithm proposed by\nCook and Zhang (2015c).\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 19:17:01 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Cook", "Dennis", ""], ["Forzani", "Liliana", ""], ["Su", "Zhihua", ""]]}, {"id": "1509.03813", "submitter": "Alexander Aue", "authors": "Alexander Aue, Lajos Horvath, Daniel Pellatt", "title": "Functional generalized autoregressive conditional heteroskedasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heteroskedasticity is a common feature of financial time series and is\ncommonly addressed in the model building process through the use of ARCH and\nGARCH processes. More recently multivariate variants of these processes have\nbeen in the focus of research with attention given to methods seeking an\nefficient and economic estimation of a large number of model parameters. Due to\nthe need for estimation of many parameters, however, these models may not be\nsuitable for modeling now prevalent high-frequency volatility data. One\npotentially useful way to bypass these issues is to take a functional approach.\nIn this paper, theory is developed for a new functional version of the\ngeneralized autoregressive conditionally heteroskedastic process, termed\nfGARCH. The main results are concerned with the structure of the fGARCH(1,1)\nprocess, providing criteria for the existence of a strictly stationary\nsolutions both in the space of square-integrable and continuous functions. An\nestimation procedure is introduced and its consistency verified. A small\nempirical study highlights potential applications to intraday volatility\nestimation.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 06:19:26 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 05:40:46 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Aue", "Alexander", ""], ["Horvath", "Lajos", ""], ["Pellatt", "Daniel", ""]]}, {"id": "1509.03927", "submitter": "Shaojie Chen", "authors": "Shaojie Chen, Kai Liu, Yuguang Yang, Yuting Xu, Seonjoo Lee, Martin\n  Lindquist, Brian S. Caffo, and Joshua T. Vogelstein", "title": "An M-Estimator for Reduced-Rank High-Dimensional Linear Dynamical System\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional time-series data are becoming increasingly abundant across a\nwide variety of domains, spanning economics, neuroscience, particle physics,\nand cosmology. Fitting statistical models to such data, to enable parameter\nestimation and time-series prediction, is an important computational primitive.\nExisting methods, however, are unable to cope with the high-dimensional nature\nof these problems, due to both computational and statistical reasons. We\nmitigate both kinds of issues via proposing an M-estimator for Reduced-rank\nSystem IDentification (MR. SID). A combination of low-rank approximations, L-1\nand L-2 penalties, and some numerical linear algebra tricks, yields an\nestimator that is computationally efficient and numerically stable. Simulations\nand real data examples demonstrate the utility of this approach in a variety of\nproblems. In particular, we demonstrate that MR. SID can estimate spatial\nfilters, connectivity graphs, and time-courses from native resolution\nfunctional magnetic resonance imaging data. Other applications and extensions\nare immediately available, as our approach is a generalization of the classical\nKalman Filter-Smoother Expectation-Maximization algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 02:23:31 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Chen", "Shaojie", ""], ["Liu", "Kai", ""], ["Yang", "Yuguang", ""], ["Xu", "Yuting", ""], ["Lee", "Seonjoo", ""], ["Lindquist", "Martin", ""], ["Caffo", "Brian S.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1509.03935", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran", "title": "Markov Boundary Discovery with Ridge Regularized Linear Models", "comments": "submitted to the Journal of Causal Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge regularized linear models (RRLMs), such as ridge regression and the\nSVM, are a popular group of methods that are used in conjunction with\ncoefficient hypothesis testing to discover explanatory variables with a\nsignificant multivariate association to a response. However, many investigators\nare reluctant to draw causal interpretations of the selected variables due to\nthe incomplete knowledge of the capabilities of RRLMs in causal inference.\nUnder reasonable assumptions, we show that a modified form of RRLMs can get\nvery close to identifying a subset of the Markov boundary by providing a\nworst-case bound on the space of possible solutions. The results hold for any\nconvex loss, even when the underlying functional relationship is nonlinear, and\nthe solution is not unique. Our approach combines ideas in Markov boundary and\nsufficient dimension reduction theory. Experimental results show that the\nmodified RRLMs are competitive against state-of-the-art algorithms in\ndiscovering part of the Markov boundary from gene expression data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 02:58:58 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1509.03938", "submitter": "Yiyuan She", "authors": "Yiyuan She and Kun Chen", "title": "Robust Reduced Rank Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional multivariate regression problems, enforcing low rank in\nthe coefficient matrix offers effective dimension reduction, which greatly\nfacilitates parameter estimation and model interpretation. However,\ncommonly-used reduced-rank methods are sensitive to data corruption, as the\nlow-rank dependence structure between response variables and predictors is\neasily distorted by outliers. We propose a robust reduced-rank regression\napproach for joint modeling and outlier detection. The problem is formulated as\na regularized multivariate regression with a sparse mean-shift parametrization,\nwhich generalizes and unifies some popular robust multivariate methods. An\nefficient thresholding-based iterative procedure is developed for optimization.\nWe show that the algorithm is guaranteed to converge, and the coordinatewise\nminimum point produced is statistically accurate under regularity conditions.\nOur theoretical investigations focus on nonasymptotic robust analysis, which\ndemonstrates that joint rank reduction and outlier detection leads to improved\nprediction accuracy. In particular, we show that redescending $\\psi$-functions\ncan essentially attain the minimax optimal error rate, and in some less\nchallenging problems convex regularization guarantees the same low error rate.\nThe performance of the proposed method is examined by simulation studies and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 03:09:04 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 04:35:58 GMT"}, {"version": "v3", "created": "Sat, 1 Apr 2017 20:06:39 GMT"}, {"version": "v4", "created": "Thu, 13 Apr 2017 16:54:31 GMT"}, {"version": "v5", "created": "Sat, 15 Jul 2017 09:52:38 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["She", "Yiyuan", ""], ["Chen", "Kun", ""]]}, {"id": "1509.03951", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Tatsuya Kubokawa", "title": "Transforming response values in small area prediction", "comments": "24 pages; To appear in Computational Statistics & Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real applications of small area estimation, one often encounters data with\npositive response values. The use of a parametric transformation for positive\nresponse values in the Fay-Herriot model is proposed for such a case. An\nasymptotically unbiased small area predictor is derived and a second-order\nunbiased estimator of the mean squared error is established using the\nparametric bootstrap. Through simulation studies, a finite sample performance\nof the proposed predictor and the MSE estimator is investigated. The\nmethodology is also successfully applied to Japanese survey data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 04:51:59 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 07:17:45 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 05:45:32 GMT"}, {"version": "v4", "created": "Tue, 21 Feb 2017 04:27:01 GMT"}, {"version": "v5", "created": "Thu, 30 Mar 2017 01:59:53 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1509.04099", "submitter": "Antony Overstall", "authors": "Antony Overstall, David Woods, Ben Parker", "title": "Bayesian optimal design for ordinary differential equation models with\n  application in biological science", "comments": "Update involves only updating affiliation for author 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimal design is considered for experiments where the response\ndistribution depends on the solution to a system of non-linear ordinary\ndifferential equations. The motivation is an experiment to estimate parameters\nin the equations governing the transport of amino acids through cell membranes\nin human placentas. Decision-theoretic Bayesian design of experiments for such\nnonlinear models is conceptually very attractive, allowing the formal\nincorporation of prior knowledge to overcome the parameter dependence of\nfrequentist design and being less reliant on asymptotic approximations.\nHowever, the necessary approximation and maximization of the, typically\nanalytically intractable, expected utility results in a computationally\nchallenging problem. These issues are further exacerbated if the solution to\nthe differential equations is not available in closed-form. This paper proposes\na new combination of a probabilistic solution to the equations embedded within\na Monte Carlo approximation to the expected utility with cyclic descent of a\nsmooth approximation to find the optimal design. A novel precomputation\nalgorithm reduces the computational burden, making the search for an optimal\ndesign feasible for bigger problems. The methods are demonstrated by finding\nnew designs for a number of common models derived from differential equations,\nand by providing optimal designs for the placenta experiment.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 14:00:13 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 13:00:45 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 15:23:44 GMT"}, {"version": "v4", "created": "Wed, 2 Jan 2019 09:36:16 GMT"}, {"version": "v5", "created": "Wed, 1 May 2019 15:55:40 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Overstall", "Antony", ""], ["Woods", "David", ""], ["Parker", "Ben", ""]]}, {"id": "1509.04334", "submitter": "Graciela Boente Prof.", "authors": "Graciela Boente and Alejandra Martinez", "title": "Marginal integration $M-$estimators for additive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive regression models have a long history in multivariate nonparametric\nregression. They provide a model in which each regression function depends only\non a single explanatory variable allowing to obtain estimators at the optimal\nunivariate rate. Beyond backfitting, marginal integration is a common procedure\nto estimate each component. In this paper, we propose a robust estimator of the\nadditive components which combines local polynomials on the component to be\nestimated and marginal integration. The proposed estimators are consistent and\nasymptotically normally distributed. A simulation study allows to show the\nadvantage of the proposal over the classical one when outliers are present in\nthe responses, leading to estimators with good robustness and efficiency\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 21:26:21 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Boente", "Graciela", ""], ["Martinez", "Alejandra", ""]]}, {"id": "1509.04348", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla and James G. Scott", "title": "Nonparametric density estimation by histogram trend filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for density estimation called histogram trend\nfiltering. Our estimator arises from looking at surrogate Poisson model for\ncounts of observations in a partition of the support of the data. We begin by\nshowing consistency for a variational estimator for this density estimation\nproblem. We then study a discrete estimator that can be efficiently found via\nconvex optimization. We show that the estimator enjoys strong statistical\nguarantees, yet is much more practical and computationally efficient than other\nestimators that enjoy similar guarantees. Finally, in our simulation study the\nproposed method showed smaller averaged mean square error than competing\nmethods. This favorable blend of properties makes histogram trend filtering an\nideal candidate for use in routine data-analysis applications that call for a\nquick, efficient, accurate density estimate.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 22:45:30 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2016 21:32:05 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Scott", "James G.", ""]]}, {"id": "1509.04350", "submitter": "Alona Kryshchenko", "authors": "Alona Kryshchenko, Alan Schumitzky, Mike van Guilder, Michael Neely", "title": "Nonparametric estimation of a mixing distribution for a family of linear\n  stochastic dynamical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a nonparametric maximum likelihood estimate of the\nmixing distribution of the parameters of a linear stochastic dynamical system.\nThis includes, for example, pharmacokinetic population models with process and\nmeasurement noise that are linear in the state vector, input vector and the\nprocess and measurement noise vectors. Most research in mixing distributions\nonly considers measurement noise. The advantages of the models with process\nnoise are that, in addition to the measurements errors, the uncertainties in\nthe model itself are taken into the account. For example, for deterministic\npharmacokinetic models, errors in dose amounts, administration times, and\ntiming of blood samples are typically not included. For linear stochastic\nmodels, we use linear Kalman-Bucy filtering to calculate the likelihood of the\nobservations and then employ a nonparametric adaptive grid algorithm to find\nthe nonparametric maximum likelihood estimate of the mixing distribution. We\nthen use the directional derivatives of the estimated mixing distribution to\nshow that the result found attains a global maximum. A simple example using a\none compartment pharmacokinetic linear stochastic model is given. In addition\nto population pharmacokinetics, this research also applies to empirical Bayes\nestimation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 23:28:23 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Kryshchenko", "Alona", ""], ["Schumitzky", "Alan", ""], ["van Guilder", "Mike", ""], ["Neely", "Michael", ""]]}, {"id": "1509.04413", "submitter": "Fran\\c{c}ois Portier", "authors": "Fran\\c{c}ois Portier", "title": "Efficiency of Z-estimators indexed by the objective functions", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence of $Z$-estimators $\\widehat \\theta(\\eta)\\in \\mathbb\nR^p$ for which the objective function depends on a parameter $\\eta$ that\nbelongs to a Banach space $\\mathcal H$. Our results include the uniform\nconsistency over $\\mathcal H$ and the weak convergence in the space of bounded\n$\\mathbb R^p$-valued functions defined on $\\mathcal H$. Furthermore when $\\eta$\nis a tuning parameter optimally selected at $\\eta_0$, we provide conditions\nunder which an estimated $\\widehat \\eta$ can be replaced by $\\eta_0$ without\naffecting the asymptotic variance. Interestingly, these conditions are free\nfrom any rate of convergence of $\\widehat \\eta$ to $\\eta_0$ but they require\nthe space described by $\\widehat \\eta$ to be not too large. We highlight\nseveral applications of our results and we study in detail the case where\n$\\eta$ is the weight function in weighted regression.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 06:17:07 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Portier", "Fran\u00e7ois", ""]]}, {"id": "1509.04448", "submitter": "Michael Chipeta Mr", "authors": "Michael G. Chipeta and Dianne J. Terlouw and Kamija Phiri and Peter J.\n  Diggle", "title": "Adaptive Geostatistical Design and Analysis for Sequential Prevalence\n  Surveys", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-adaptive geostatistical designs (NAGD) offer standard ways of collecting\nand analysing geostatistical data in which sampling locations are fixed in\nadvance of any data collection. In contrast, adaptive geostatistical designs\n(AGD) allow collection of exposure and outcome data over time to depend on\ninformation obtained from previous information to optimise data collection\ntowards the analysis objective. AGDs are becoming more important in spatial\nmapping, particularly in poor resource settings where uniformly precise mapping\nmay be unrealistically costly and priority is often to identify critical areas\nwhere interventions can have the most health impact. Two constructions are:\n$singleton$ and $batch$ adaptive sampling. In singleton sampling, locations\n$x_i$ are chosen sequentially and at each stage, $x_{k+1}$ depends on data\nobtained at locations $x_1,\\ldots , x_k$. In batch sampling, locations are\nchosen in batches of size $b > 1$, allowing new batch, $\\{x_{(k+1)},\\ldots\n,x_{(k+b)}\\}$, to depend on data obtained at locations $x_1,\\ldots, x_{kb}$. In\nmost settings, batch sampling is more realistic than singleton sampling. We\npropose specific batch AGDs and assess their efficiency relative to their\nsingleton adaptive and non-adaptive counterparts by using simulations. We show\nhow we apply these findings to inform an AGD of a rolling Malaria Indicator\nSurvey, part of a large-scale, five-year malaria transmission reduction project\nin Malawi.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 09:00:17 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Chipeta", "Michael G.", ""], ["Terlouw", "Dianne J.", ""], ["Phiri", "Kamija", ""], ["Diggle", "Peter J.", ""]]}, {"id": "1509.04576", "submitter": "Facundo Costa", "authors": "Facundo Costa, Hadj Batatia, Thomas Oberlin, and Jean-Yves Tourneret", "title": "Bayesian Structured Sparsity Priors for EEG Source Localization\n  Technical Report", "comments": "38 pages, extended version of a paper that will be submitted for\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report introduces a new hierarchical Bayesian model for the EEG source\nlocalization problem. This model promotes structured sparsity to search for\nfocal brain activity. This sparsity is obtained via a multivariate Bernoulli\nLaplacian prior assigned to the brain activity approximating an $\\ell_{20}$\npseudo norm regularization in a Bayesian framework. A partially collapsed Gibbs\nsampler is used to draw samples asymptotically distributed according to the\nposterior associated with the proposed Bayesian model. The generated samples\nare used to estimate the brain activity and the model hyperparameters jointly\nin an unsupervised framework. Two different kinds of Metropolis-Hastings moves\nare introduced to accelerate the convergence of the Gibbs sampler. The first\nmove is based on multiple dipole shifts within each MCMC chain whereas the\nsecond one exploits proposals associated with different MCMC chains. We use\nboth synthetic and real data to compare the performance of the proposed method\nwith the weighted $\\ell_{21}$ mixed norm regularization and a method based on a\nmultiple sparse prior, showing that our algorithm presents advantages in\nseveral scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 14:22:44 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Costa", "Facundo", ""], ["Batatia", "Hadj", ""], ["Oberlin", "Thomas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1509.04695", "submitter": "Yolanda Hagar", "authors": "Yolanda Hagar, Danielle Harvey, Laurel Beckett", "title": "A Multivariate Cure Model for Left- and Right-Censored Data with\n  Application to Colorectal Cancer Screening Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a multivariate cure survival model to estimate lifetime patterns\nof colorectal cancer screening. Screening data cover long periods of time, with\nsparse observations for each person. Some events may occur before the study\nbegins or after the study ends, so the data are both left- and right-censored,\nand some individuals are never screened (the \"cured\" population). We propose a\nmultivariate parametric cure model that can be used with left- and\nright-censored data. Our model allows for the estimation of the time to\nscreening as well as the average number of times individuals will be screened.\nWe calculate likelihood functions based on the observations for each subject\nusing a distribution that accounts for within-subject correlation, and estimate\nparameters using Markov Chain Monte Carlo methods. We apply our methods to the\nestimation of lifetime colorectal cancer screening behavior in the\nSEER-Medicare data set.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 19:38:40 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Hagar", "Yolanda", ""], ["Harvey", "Danielle", ""], ["Beckett", "Laurel", ""]]}, {"id": "1509.04704", "submitter": "Xiao Li", "authors": "Xiao Li and Karl Rohe", "title": "Central limit theorems for network driven sampling", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-Driven Sampling is a popular technique for sampling hidden\npopulations. This paper models Respondent-Driven Sampling as a Markov process\nindexed by a tree. Our main results show that the Volz-Heckathorn estimator is\nasymptotically normal below a critical threshold. The key technical\ndifficulties stem from (i) the dependence between samples and (ii) the tree\nstructure which characterizes the dependence. The theorems allow the growth\nrate of the tree to exceed one and suggest that this growth rate should not be\ntoo large. To illustrate the usefulness of these results beyond their obvious\nuse, an example shows that in certain cases the sample average is preferable to\ninverse probability weighting. We provide a test statistic to distinguish\nbetween these two cases.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 11:22:56 GMT"}, {"version": "v2", "created": "Sun, 28 Aug 2016 00:56:50 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Li", "Xiao", ""], ["Rohe", "Karl", ""]]}, {"id": "1509.04752", "submitter": "Michael Riis Andersen", "authors": "Michael Riis Andersen, Aki Vehtari, Ole Winther, Lars Kai Hansen", "title": "Bayesian inference for spatio-temporal spike-and-slab priors", "comments": "58 pages, 17 figures", "journal-ref": "Journal of Machine Learning Research, 18(139):1-58, 2017", "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of solving a series of underdetermined\nlinear inverse problems subject to a sparsity constraint. We generalize the\nspike-and-slab prior distribution to encode a priori correlation of the support\nof the solution in both space and time by imposing a transformed Gaussian\nprocess on the spike-and-slab probabilities. An expectation propagation (EP)\nalgorithm for posterior inference under the proposed model is derived. For\nlarge scale problems, the standard EP algorithm can be prohibitively slow. We\ntherefore introduce three different approximation schemes to reduce the\ncomputational complexity. Finally, we demonstrate the proposed model using\nnumerical experiments based on both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 21:58:12 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 18:10:07 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 13:38:56 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Andersen", "Michael Riis", ""], ["Vehtari", "Aki", ""], ["Winther", "Ole", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1509.04774", "submitter": "Bal\\'azs Csan\\'ad Cs\\'aji", "authors": "Valerio Volpe, Bal\\'azs Cs. Cs\\'aji, Algo Car\\`e, Erik Weyer, Marco C.\n  Campi", "title": "Sign-Perturbed Sums (SPS) with Instrumental Variables for the\n  Identification of ARX Systems - Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of the recently developed system identification\nmethod called Sign-Perturbed Sums (SPS). The proposed construction is based on\nthe instrumental variables estimate and, unlike the original SPS, it can\nconstruct non-asymptotic confidence regions for linear regression models where\nthe regressors contain past values of the output. Hence, it is applicable to\nARX systems, as well as systems with feedback. We show that this approach\nprovides regions with exact confidence under weak assumptions, i.e., the true\nparameter is included in the regions with a (user-chosen) exact probability for\nany finite sample. The paper also proves the strong consistency of the method\nand proposes a computationally efficient generalization of the previously\nproposed ellipsoidal outer-approximation. Finally, the new method is\ndemonstrated through numerical experiments, using both real-world and simulated\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 23:57:50 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Volpe", "Valerio", ""], ["Cs\u00e1ji", "Bal\u00e1zs Cs.", ""], ["Car\u00e8", "Algo", ""], ["Weyer", "Erik", ""], ["Campi", "Marco C.", ""]]}, {"id": "1509.04992", "submitter": "Norm Matloff PhD", "authors": "Xiao Gu and Norman Matloff", "title": "A Different Approach to the Problem of Missing Data", "comments": "Software at https://github.com/matloff/regtools", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a long history of devleopment of methodology dealing with missing\ndata in statistical analysis. Today, the most popular methods fall into two\nclasses, Complete Cases (CC) and Multiple Imputation (MI). Another approach,\nAvailable Cases (AC), has occasionally been mentioned in the research\nliterature, in the context of linear regression analysis, but has generally\nbeen ignored. In this paper, we revisit the AC method, showing that it can\nperform better than CC and MI, and we extend its breadth of application.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 18:23:20 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Gu", "Xiao", ""], ["Matloff", "Norman", ""]]}, {"id": "1509.05111", "submitter": "Rong Zhu", "authors": "Rong Zhu, Ping Ma, Michael W. Mahoney, Bin Yu", "title": "Optimal Subsampling Approaches for Large Sample Linear Regression", "comments": "This paper has been withdrawn by the author due to the incompleteness\n  of this draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant hurdle for analyzing large sample data is the lack of effective\nstatistical computing and inference methods. An emerging powerful approach for\nanalyzing large sample data is subsampling, by which one takes a random\nsubsample from the original full sample and uses it as a surrogate for\nsubsequent computation and estimation. In this paper, we study subsampling\nmethods under two scenarios: approximating the full sample ordinary\nleast-square (OLS) estimator and estimating the coefficients in linear\nregression. We present two algorithms, weighted estimation algorithm and\nunweighted estimation algorithm, and analyze asymptotic behaviors of their\nresulting subsample estimators under general conditions. For the weighted\nestimation algorithm, we propose a criterion for selecting the optimal sampling\nprobability by making use of the asymptotic results. On the basis of the\ncriterion, we provide two novel subsampling methods, the optimal subsampling\nand the predictor- length subsampling methods. The predictor-length subsampling\nmethod is based on the L2 norm of predictors rather than leverage scores. Its\ncomputational cost is scalable. For unweighted estimation algorithm, we show\nthat its resulting subsample estimator is not consistent to the full sample OLS\nestimator. However, it has better performance than the weighted estimation\nalgorithm for estimating the coefficients. Simulation studies and a real data\nexample are used to demonstrate the effectiveness of our proposed subsampling\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 03:25:21 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 00:17:26 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zhu", "Rong", ""], ["Ma", "Ping", ""], ["Mahoney", "Michael W.", ""], ["Yu", "Bin", ""]]}, {"id": "1509.05244", "submitter": "Mauro Ribeiro de Oliveira JR", "authors": "Francisco Louzada, Mauro R. de Oliveira Jr, Fernando F. Moreira", "title": "The zero-inflated cure rate regression model: Applications to fraud\n  detection in bank loan portfolios", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a methodology based on the zero-inflated cure\nrate model to detect fraudsters in bank loan applications. In fact, our\napproach enables us to accommodate three different types of loan applicants,\ni.e., fraudsters, those who are susceptible to default and finally, those who\nare not susceptible to default. An advantage of our approach is to accommodate\nzero-inflated times, which is not possible in the standard cure rate model. To\nillustrate the proposed method, a real dataset of loan survival times is fitted\nby the zero-inflated Weibull cure rate model. The parameter estimation is\nreached by maximum likelihood estimation procedure and Monte Carlo simulations\nare carried out to check its finite sample performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 13:15:00 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2015 22:35:25 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Louzada", "Francisco", ""], ["Oliveira", "Mauro R. de", "Jr"], ["Moreira", "Fernando F.", ""]]}, {"id": "1509.05438", "submitter": "Qiyi Lu", "authors": "Qiyi Lu and Xingye Qiao", "title": "Sparse Fisher's Linear Discriminant Analysis for Partially Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is an important tool with many useful applications. Among the\nmany classification methods, Fisher's Linear Discriminant Analysis (LDA) is a\ntraditional model-based approach which makes use of the covariance information.\nHowever, in the high-dimensional, low-sample size setting, LDA cannot be\ndirectly deployed because the sample covariance is not invertible. While there\nare modern methods designed to deal with high-dimensional data, they may not\nfully use the covariance information as LDA does. Hence in some situations, it\nis still desirable to use a model-based method such as LDA for classification.\nThis article exploits the potential of LDA in more complicated data settings.\nIn many real applications, it is costly to manually place labels on\nobservations; hence it is often that only a small portion of labeled data is\navailable while a large number of observations are left without a label. It is\na great challenge to obtain good classification performance through the labeled\ndata alone, especially when the dimension is greater than the size of the\nlabeled data. In order to overcome this issue, we propose a semi-supervised\nsparse LDA classifier to take advantage of the seemingly useless unlabeled\ndata. They provide additional information which helps to boost the\nclassification performance in some situations. A direct estimation method is\nused to reconstruct LDA and achieve the sparsity; meanwhile we employ the\ndifference-convex algorithm to handle the non-convex loss function associated\nwith the unlabeled data. Theoretical properties of the proposed classifier are\nstudied. Our simulated examples help to understand when and how the information\nextracted from the unlabeled data can be useful. A real data example further\nillustrates the usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 20:42:42 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Lu", "Qiyi", ""], ["Qiao", "Xingye", ""]]}, {"id": "1509.05453", "submitter": "Xi Chen", "authors": "Xi Chen and Weidong Liu", "title": "Graph Estimation for Matrix-variate Gaussian Data", "comments": "50 pages; 8 figures", "journal-ref": "Statistica Sinica (2017)", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-variate Gaussian graphical models (GGM) have been widely used for\nmodeling matrix-variate data. Since the support of sparse precision matrix\nrepresents the conditional independence graph among matrix entries, conducting\nsupport recovery yields valuable information. A commonly used approach is the\npenalized log-likelihood method. However, due to the complicated structure of\nprecision matrices in the form of Kronecker product, the log-likelihood is\nnon-convex, which presents challenges for both computation and theoretical\nanalysis. In this paper, we propose an alternative approach by formulating the\nsupport recovery problem into a multiple testing problem. A new test statistic\nis developed and based on that, we use the popular Benjamini and Hochberg's\nprocedure to control false discovery rate (FDR) asymptotically. Our method\ninvolves only convex optimization, making it computationally attractive.\nTheoretically, our method allows very weak conditions, i.e., even when the\nsample size is finite and the dimensions go to infinity, the asymptotic\nnormality of the test statistics and FDR control can still be guaranteed. We\nfurther provide the power analysis result. The finite sample performance of the\nproposed method is illustrated by both simulated and real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 21:52:26 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 02:40:24 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Chen", "Xi", ""], ["Liu", "Weidong", ""]]}, {"id": "1509.05555", "submitter": "Adeshina Bello M.SC", "authors": "Oyedele Adeshina Bello, Timothy Adebayo Bamiduro, Unna Angela Chuwkwu,\n  and Oyedeji Isola Osowole", "title": "Bootstrap Nonlinear Regression Application in a Design of an Experiment\n  Data for Fewer Sample Size", "comments": null, "journal-ref": "International Journal of Research (IJR) Vol 2, Issue 02, February\n  2015", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on application of bootstrap nonlinear regression method to\na design of an experiment dataset with fewer experimental runs. Design with\ndesired properties was augmented and verified using graphical techniques. The\naugmented design with the desired properties benefited the accuracy of the\napproximated function used. The computation power of R-language and SAS for\ncomputing nonlinear function and bootstrap was also compared.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 09:15:16 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Bello", "Oyedele Adeshina", ""], ["Bamiduro", "Timothy Adebayo", ""], ["Chuwkwu", "Unna Angela", ""], ["Osowole", "Oyedeji Isola", ""]]}, {"id": "1509.05570", "submitter": "Sarah Friedrich", "authors": "Sarah Friedrich and Edgar Brunner and Markus Pauly", "title": "Permuting longitudinal data despite all the dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For general repeated measures designs the Wald-type statistic (WTS) is an\nasymptotically valid procedure allowing for unequal covariance matrices and\npossibly non-normal multivariate observations. The drawback of this procedure\nis the poor performance for small to moderate samples, i.e. decisions based on\nthe WTS may become quite liberal. It is the aim of the present paper to improve\nits small sample behavior by means of a novel permutation procedure. In\nparticular, it is shown that a permutation version of the WTS inherits its good\nlarge sample properties while yielding a very accurate finite sample control of\nthe type-I error as shown in extensive simulations. Moreover, the new\npermutation method is motivated by a practical data set of a split plot design\nwith a factorial structure on the repeated measures.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 10:13:49 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 08:02:57 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Friedrich", "Sarah", ""], ["Brunner", "Edgar", ""], ["Pauly", "Markus", ""]]}, {"id": "1509.05810", "submitter": "James Long", "authors": "James P. Long", "title": "A Note on Parameter Estimation for Misspecified Regression Models with\n  Heteroskedastic Errors", "comments": "28 pages, 4 figures, 2 tables", "journal-ref": "Electronic Journal of Statistics Vol. 11 (2017) 1464-1490", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misspecified models often provide useful information about the true data\ngenerating distribution. For example, if $y$ is a non-linear function of $x$\nthe least squares estimator $\\hat{\\beta}$ is an estimate of $\\beta$, the slope\nof the best linear approximation to the non-linear function. Motivated by\nproblems in astronomy, we study how to incorporate observation measurement\nerror variances into fitting parameters of misspecified models. Our asymptotic\ntheory focuses on the particular case of linear regression where often weighted\nleast squares procedures are used to account for heteroskedasticity. We find\nthat when the response is a non-linear function of the independent variable,\nthe standard procedure of weighting by the inverse of the observation variances\ncan be counter-productive. In particular, ordinary least squares may have lower\nasymptotic variance. We construct an adaptive estimator which has lower\nasymptotic variance than either OLS or standard WLS. We demonstrate our theory\nin a small simulation and apply these ideas to the problem of estimating the\nperiod of a periodic function using a sinusoidal model.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 21:55:18 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 04:29:21 GMT"}, {"version": "v3", "created": "Mon, 15 May 2017 19:06:11 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Long", "James P.", ""]]}, {"id": "1509.05861", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana, Fabio Rapallo and Maria-Piera Rogantin", "title": "Aberration in qualitative multilevel designs", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Word Length Pattern (GWLP) is an important and widely-used tool\nfor comparing fractional factorial designs. We consider qualitative factors,\nand we code their levels using the roots of the unity. We write the GWLP of a\nfraction ${\\mathcal F}$ using the polynomial indicator function, whose\ncoefficients encode many properties of the fraction. We show that the\ncoefficient of a simple or interaction term can be written using the counts of\nits levels. This apparently simple remark leads to major consequence, including\na convolution formula for the counts. We also show that the mean aberration of\na term over the permutation of its levels provides a connection with the\nvariance of the level counts. Moreover, using mean aberrations for symmetric\n$s^m$ designs with $s$ prime, we derive a new formula for computing the GWLP of\n${\\mathcal F}$. It is computationally easy, does not use complex numbers and\nalso provides a clear way to interpret the GWLP. As case studies, we consider\nnon-isomorphic orthogonal arrays that have the same GWLP. The different\ndistributions of the mean aberrations suggest that they could be used as a\nfurther tool to discriminate between fractions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 08:58:22 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""], ["Rogantin", "Maria-Piera", ""]]}, {"id": "1509.06088", "submitter": "Qiyi Lu", "authors": "Qiyi Lu, Xingye Qiao", "title": "Significance Analysis of High-Dimensional, Low-Sample Size Partially\n  Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and clustering are both important topics in statistical\nlearning. A natural question herein is whether predefined classes are really\ndifferent from one another, or whether clusters are really there. Specifically,\nwe may be interested in knowing whether the two classes defined by some class\nlabels (when they are provided), or the two clusters tagged by a clustering\nalgorithm (where class labels are not provided), are from the same underlying\ndistribution. Although both are challenging questions for the high-dimensional,\nlow-sample size data, there has been some recent development for both. However,\nwhen it is costly to manually place labels on observations, it is often that\nonly a small portion of the class labels is available. In this article, we\npropose a significance analysis approach for such type of data, namely\npartially labeled data. Our method makes use of the whole data and tries to\ntest the class difference as if all the labels were observed. Compared to a\ntesting method that ignores the label information, our method provides a\ngreater power, meanwhile, maintaining the size, illustrated by a comprehensive\nsimulation study. Theoretical properties of the proposed method are studied\nwith emphasis on the high-dimensional, low-sample size setting. Our simulated\nexamples help to understand when and how the information extracted from the\nlabeled data can be effective. A real data example further illustrates the\nusefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 01:23:45 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Lu", "Qiyi", ""], ["Qiao", "Xingye", ""]]}, {"id": "1509.06123", "submitter": "Ricardo Oliveros-Ramos", "authors": "Ricardo Oliveros-Ramos, Philippe Verley and Yunne-Jai Shin", "title": "A sequential approach to calibrate ecosystem models with multiple time\n  series data", "comments": "33 pages, 4 tables, 13 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecosystem approach to fisheries requires a thorough understanding of fishing\nimpacts on ecosystem status and processes as well as predictive tools such as\necosystem models to provide useful information for management. The credibility\nof such models is essential when used as decision making tools, and model\nfitting to observed data is one major criterion to assess such credibility.\nHowever, more attention has been given to the exploration of model behavior\nthan to a rigorous confrontation to observations, as the calibration of\necosystem models is challenging in many ways. First, ecosystem models can only\nbe simulated numerically and are generally too complex for mathematical\nanalysis and explicit parameter estimation; secondly, the complex dynamics\nrepresented in ecosystem models allow species-specific parameters to impact\nother species parameters through ecological interactions; thirdly, critical\ndata about non-commercial species are often poor; lastly, technical aspects can\nbe impediments to the calibration with regard to the high computational cost\npotentially involved and the scarce documentation published on fitting complex\necosystem models to data. This work highlights some issues related to the\nconfrontation of complex ecosystem models to data and proposes a methodology\nfor a sequential multi-phases calibration of ecosystem models. We propose\ncriteria to classify the parameters of a model: model dependency and time\nvariability of the parameters. These criteria and the availability of\napproximate initial estimates are used as decision rules to determine which\nparameters need to be estimated, and their precedence order in the sequential\ncalibration process. The end-to-end ecosystem model ROMS-PISCES-OSMOSE applied\nto the Northern Humboldt Current Ecosystem is used as an illustrative case\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 06:59:23 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Oliveros-Ramos", "Ricardo", ""], ["Verley", "Philippe", ""], ["Shin", "Yunne-Jai", ""]]}, {"id": "1509.06132", "submitter": "Marianne Jonker", "authors": "M.A. Jonker, M.W.T. Tanck", "title": "A powerful allele based test for case-control association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a case-control study aimed at localizing disease variants, association\nbetween a marker and the disease status is often tested by comparing the marker\nallele frequencies among cases and controls. These marker allele frequencies\nare expected to be different if the marker is associated with the disease. The\npower of the commonly used allele based test is based on the marker allele\nfrequency; markers with a low minor allele frequency have less power to be\ndetected (if they are associated with the disease), than markers with high\nminor allele frequency. Therefore the strategy of selecting markers for\nfollow-up study based on their p-values, favors markers with a high minor\nallele frequency.\n  We propose an allele based test that does not have this (unwanted) property\nand is therefore more powerful for markers with a low minor allele frequency.\nThis test may, therefore, be more effective when searching for rare causal\nvariants. The asymptotic power function of the test is derived and simulation\nstudies are performed for finite sample properties of the test. Next, the\nexisting and the proposed tests are applied to data; this is not included yet.\n  In the light of the current interest in detecting association between complex\nphenotypes and causal variants with a low minor allele frequencies, this test\nis expected to be of relevance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 07:59:39 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Jonker", "M. A.", ""], ["Tanck", "M. W. T.", ""]]}, {"id": "1509.06310", "submitter": "Aixin Tan", "authors": "Vivekananda Roy, Aixin Tan, and James M. Flegal", "title": "Estimating standard errors for importance sampling estimators with\n  multiple Markov chains", "comments": "49 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The naive importance sampling estimator, based on samples from a single\nimportance density, can be numerically unstable. Instead, we consider\ngeneralized importance sampling estimators where samples from more than one\nprobability distribution are combined. We study this problem in the Markov\nchain Monte Carlo context, where independent samples are replaced with Markov\nchain samples. If the chains converge to their respective target distributions\nat a polynomial rate, then under two finite moment conditions, we show a\ncentral limit theorem holds for the generalized estimators. Further, we develop\nan easy to implement method to calculate valid asymptotic standard errors based\non batch means. We also provide a batch means estimator for calculating\nasymptotically valid standard errors of Geyer(1994) reverse logistic estimator.\nWe illustrate the method using a Bayesian variable selection procedure in\nlinear regression. In particular, the generalized importance sampling estimator\nis used to perform empirical Bayes variable selection and the batch means\nestimator is used to obtain standard errors in a high-dimensional setting where\ncurrent methods are not applicable.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 17:18:34 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 15:10:30 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Roy", "Vivekananda", ""], ["Tan", "Aixin", ""], ["Flegal", "James M.", ""]]}, {"id": "1509.06358", "submitter": "Robert Krafty", "authors": "Robert T. Krafty", "title": "Discriminant Analysis of Time Series in the Presence of Within-Group\n  Spectral Variability", "comments": null, "journal-ref": "Journal of Time Series Analysis, 37: 435-450 (2016)", "doi": "10.1111/jtsa.12166", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies record replicated time series epochs from different groups with\nthe goal of using frequency domain properties to discriminate between the\ngroups. In many applications, there exists variation in cyclical patterns from\ntime series in the same group. Although a number of frequency domain methods\nfor the discriminant analysis of time series have been explored, there is a\ndearth of models and methods that account for within-group spectral\nvariability. This article proposes a model for groups of time series in which\ntransfer functions are modeled as stochastic variables that can account for\nboth between-group and within-group differences in spectra that are identified\nfrom individual replicates. An ensuing discriminant analysis of stochastic\ncepstra under this model is developed to obtain parsimonious measures of\nrelative power that optimally separate groups in the presence of within-group\nspectral variability. The approach possess favorable properties in classifying\nnew observations and can be consistently estimated through a simple\ndiscriminant analysis of a finite number of estimated cepstral coefficients.\nBenefits in accounting for within-group spectral variability are empirically\nillustrated in a simulation study and through an analysis of gait variability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 19:36:46 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Krafty", "Robert T.", ""]]}, {"id": "1509.06403", "submitter": "Amadou Diadie Ba", "authors": "Gane Samb Lo, Diadie Ba, Elhadji Deme and Cheikh Seck", "title": "Consistency bands for the mean excess function and application to\n  graphical goodness of fit test for financial data", "comments": "34 pages, 23 figures, Conference: Galaye Dia- Scientific days\n  28-29-30 july 2015 , Gaston Berger University. Saint louis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use the modern setting of functional empirical processes\nand recent techniques on uniform estimation for non parametric objects to\nderive consistency bands for the mean excess function in the i.i.d. case. We\napply our results for modelling financial data, in particular Dow Jones data\nbasis to see how good the Generalized hyperbolic distribution models fit\nmonthly data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 21:09:13 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 18:08:50 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Lo", "Gane Samb", ""], ["Ba", "Diadie", ""], ["Deme", "Elhadji", ""], ["Seck", "Cheikh", ""]]}, {"id": "1509.06428", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Large-Scale Mode Identification and Data-Driven Sciences", "comments": "I would like to express my sincere thanks to the Editor and the\n  anonymous reviewers for their in-depth comments, which have greatly improved\n  the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bump-hunting or mode identification is a fundamental problem that arises in\nalmost every scientific field of data-driven discovery. Surprisingly, very few\ndata modeling tools are available for automatic (not requiring manual\ncase-by-base investigation), objective (not subjective), and nonparametric (not\nbased on restrictive parametric model assumptions) mode discovery, which can\nscale to large data sets. This article introduces LPMode--an algorithm based on\na new theory for detecting multimodality of a probability density. We apply\nLPMode to answer important research questions arising in various fields from\nenvironmental science, ecology, econometrics, analytical chemistry to astronomy\nand cancer genomics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 23:44:36 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 12:57:25 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2015 23:51:32 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 22:19:37 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1509.06459", "submitter": "Dustin Tran", "authors": "Dustin Tran, Panos Toulis, Edoardo M. Airoldi", "title": "Stochastic gradient descent methods for estimation with large data sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for parameter estimation in settings with large-scale data\nsets, where traditional methods are no longer tenable. Our methods rely on\nstochastic approximations, which are computationally efficient as they maintain\none iterate as a parameter estimate, and successively update that iterate based\non a single data point. When the update is based on a noisy gradient, the\nstochastic approximation is known as standard stochastic gradient descent,\nwhich has been fundamental in modern applications with large data sets.\nAdditionally, our methods are numerically stable because they employ implicit\nupdates of the iterates. Intuitively, an implicit update is a shrinked version\nof a standard one, where the shrinkage factor depends on the observed Fisher\ninformation at the corresponding data point. This shrinkage prevents numerical\ndivergence of the iterates, which can be caused either by excess noise or\noutliers. Our sgd package in R offers the most extensive and robust\nimplementation of stochastic gradient descent methods. We demonstrate that sgd\ndominates alternative software in runtime for several estimation problems with\nmassive data sets. Our applications include the wide class of generalized\nlinear models as well as M-estimation for robust regression.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 04:25:54 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Tran", "Dustin", ""], ["Toulis", "Panos", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1509.06490", "submitter": "Shaan Qamar", "authors": "Rajarshi Guhaniyogi, Shaan Qamar, David B. Dunson", "title": "Bayesian Tensor Regression", "comments": "32 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a Bayesian approach to regression with a scalar\nresponse against vector and tensor covariates. Tensor covariates are commonly\nvectorized prior to analysis, failing to exploit the structure of the tensor,\nand resulting in poor estimation and predictive performance. We develop a novel\nclass of multiway shrinkage priors for the coefficients in tensor regression\nmodels. Properties are described, including posterior consistency under mild\nconditions, and an efficient Markov chain Monte Carlo algorithm is developed\nfor posterior computation. Simulation studies illustrate substantial gains over\nvectorizing or using existing tensor regression methods in terms of estimation\nand parameter inference. The approach is further illustrated in a neuroimaging\napplication.\n  Keywords: Dimension reduction; multiway shrinkage prior; magnetic resonance\nimaging (MRI); parafac decomposition; posterior consistency; tensor regression\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 08:07:15 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Qamar", "Shaan", ""], ["Dunson", "David B.", ""]]}, {"id": "1509.06492", "submitter": "Tiep Mai", "authors": "Tiep Mai and Simon Wilson", "title": "Modifying iterated Laplace approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, several modifications are introduced to the functional\napproximation method iterLap to reduce the approximation error, including\nstopping rule adjustment, proposal of new residual function, starting point\nselection for numerical optimisation, scaling of Hessian matrix. Illustrative\nexamples are also provided to show the trade-off between running time and\naccuracy of the original and modified methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 08:09:21 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Mai", "Tiep", ""], ["Wilson", "Simon", ""]]}, {"id": "1509.06513", "submitter": "Geert Verdoolaege", "authors": "Geert Verdoolaege", "title": "A New Robust Regression Method Based on Minimization of Geodesic\n  Distances on a Probabilistic Manifold: Application to Power Laws", "comments": "Published in Entropy. This is an extended version of our paper at the\n  34th International Workshop on Bayesian Inference and Maximum Entropy Methods\n  in Science and Engineering (MaxEnt 2014), 21-26 September 2014, Amboise,\n  France", "journal-ref": "Entropy, vol 17, pp 4602-4626, 2015", "doi": "10.3390/e17074602", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In regression analysis for deriving scaling laws that occur in various\nscientific disciplines, usually standard regression methods have been applied,\nof which ordinary least squares (OLS) is the most popular. In many situations,\nthe assumptions underlying OLS are not fulfilled, and several other approaches\nhave been proposed. However, most techniques address only part of the\nshortcomings of OLS. We here discuss a new and more general regression method,\nwhich we call geodesic least squares regression (GLS). The method is based on\nminimization of the Rao geodesic distance on a probabilistic manifold. For the\ncase of a power law, we demonstrate the robustness of the method on synthetic\ndata in the presence of significant uncertainty on both the data and the\nregression model. We then show good performance of the method in an application\nto a scaling law in magnetic confinement fusion.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 09:05:56 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Verdoolaege", "Geert", ""]]}, {"id": "1509.06718", "submitter": "Milan Stehlik", "authors": "Pavlina Jordanova, Milan Stehlik", "title": "Flexible Extreme Value Inference And Hill Plots For A Small, Mid And\n  Large Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic normality of extreme value tail estimators received much attention\nin the literature, giving rise to increasingly complicated 2nd order regularity\nconditions. However, such conditions are really difficult to be checked for\nreal data. Especially it is difficult or impossible to check such conditions\nusing small samples. Beside that most of those conditions suffer from the\ndrawback of a potentially singular integral representations. However, we can\nhave various orders of approximation by normal distributions, e.g. Berry-Essen\nTypes and Edgeworth types. In this paper we indicate that for Berry-Essen Types\nof normal approximation and related asymptotic normality of generalized Hill\nestimators, we do not necessarily need 2nd order regularity conditions and we\ncan apply only Karamata's representation for regularly varying tails. 2nd order\nregularity conditions however better relates to Edgeworth types of normal\napproximations, albeit requiring larger data samples for their proper check.\nFinally both expansions are prone for bootstrap and other subsampling\ntechniques. All existing results indicate that proper representation of tail\nbehavior play a special and somewhat intriguing role in that context. We dispel\nthat widespread opinion by providing a full characterization and\nrepresentation, in a general regular variation context, of the integral\nsingularity phenomenon, highlighting its relation to an asymptotical normality\nof the Generalized Hill estimator without the 2nd order condition. Thus\napplication of this new methodology is simple and much more flexible, optimal\nfor real data sets. Alternative and powerful versions of the Hill plot are also\nintroduced and illustrated on ecological data of snow extremes from Slovakia.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 18:37:18 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Jordanova", "Pavlina", ""], ["Stehlik", "Milan", ""]]}, {"id": "1509.06721", "submitter": "Liwen Ouyang", "authors": "Liwen Ouyang, Daniel W. Apley, Sanjay Mehrotra", "title": "Designed Sampling from Large Databases for Controlled Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing prevalence of rich sources of data and the availability of\nelectronic medical record databases and electronic registries opens tremendous\nopportunities for enhancing medical research. For example, controlled trials\nare ubiquitously used to investigate the effect of a medical treatment, perhaps\ndependent on a set of patient covariates, and traditional approaches have\nrelied primarily on randomized patient sampling and allocation to treatment and\ncontrol group. However, when covariate data for a large cohort group of\npatients have already been collected and are available in a database, one can\npotentially design a treatment/control sample and allocation that provides far\nbetter estimates of the covariate-dependent effects of the treatment. In this\npaper, we develop a new approach that uses optimal design of experiments (DOE)\nconcepts to accomplish this objective. The approach selects the patients for\nthe treatment and control samples upfront, based on their covariate values, in\na manner that optimizes the information content in the data. For the optimal\nsample selection, we develop simple guidelines and an optimization algorithm\nthat provides solutions that are substantially better than random sampling.\nMoreover, our approach causes no sampling bias in the estimated effects, for\nthe same reason that DOE principles do not bias estimated effects. We test our\nmethod with a simulation study based on a testbed data set containing\ninformation on the effect of statins on low-density lipoprotein (LDL)\ncholesterol.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 18:41:43 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Ouyang", "Liwen", ""], ["Apley", "Daniel W.", ""], ["Mehrotra", "Sanjay", ""]]}, {"id": "1509.06866", "submitter": "Bernhard Klar", "authors": "Hajo Holzmann and Bernhard Klar", "title": "Expectile Asymptotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss in detail the asymptotic distribution of sample expectiles. First,\nwe show uniform consistency under the assumption of a finite mean. In case of a\nfinite second moment, we show that for expectiles other then the mean, only the\nadditional assumption of continuity of the distribution function at the\nexpectile implies asymptotic normality, otherwise, the limit is non-normal. For\na continuous distribution function we show the uniform central limit theorem\nfor the expectile process. If, in contrast, the distribution is heavy-tailed,\nand contained in the domain of attraction of a stable law with $1 < \\alpha <\n2$, then we show that the expectile is also asymptotically stable distributed.\nOur findings are illustrated in a simulation section.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 07:37:19 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 14:53:46 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Holzmann", "Hajo", ""], ["Klar", "Bernhard", ""]]}, {"id": "1509.07017", "submitter": "Matthew Reimherr", "authors": "Panayiotis Constantinou and Piotr Kokoszka and Matthew Reimherr", "title": "Testing separability of space--time functional processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new methodology and accompanying theory to test for separability\nof spatio-temporal functional data. In spatio-temporal statistics, separability\nis a common simplifying assumption concerning the covariance structure which,\nif true, can greatly increase estimation accuracy and inferential power. While\nour focus is on testing for the separation of space and time in spatio-temporal\ndata, our methods can be applied to any area where separability is useful,\nincluding biomedical imaging. We present three tests, one being a functional\nextension of the Monte Carlo likelihood method of Mitchell et. al. (2005),\nwhile the other two are based on quadratic forms. Our tests are based on\nasymptotic distributions of maximum likelihood estimators, and do not require\nMonte Carlo or bootstrap replications. The specification of the joint\nasymptotic distribution of these estimators is the main theoretical\ncontribution of this paper. It can be used to derive many other tests. The main\nmethodological finding is that one of the quadratic form methods, which we call\na norm approach, emerges as a clear winner in terms of finite sample\nperformance in nearly every setting we considered. The norm approach focuses\ndirectly on the Frobenius distance between the spatio-temporal covariance\nfunction and its separable approximation. We demonstrate the efficacy of our\nmethods via simulations and an application to Irish wind data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 15:02:31 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Constantinou", "Panayiotis", ""], ["Kokoszka", "Piotr", ""], ["Reimherr", "Matthew", ""]]}, {"id": "1509.07158", "submitter": "Fang Han", "authors": "Fang Han, Hongkai Ji, Zhicheng Ji, and Honglang Wang", "title": "A Provable Smoothing Approach for High Dimensional Generalized\n  Regression with Applications in Genomics", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, linear models fit the data poorly. This article studies\nan appealing alternative, the generalized regression model. This model only\nassumes that there exists an unknown monotonically increasing link function\nconnecting the response $Y$ to a single index $X^T\\beta^*$ of explanatory\nvariables $X\\in\\mathbb{R}^d$. The generalized regression model is flexible and\ncovers many widely used statistical models. It fits the data generating\nmechanisms well in many real problems, which makes it useful in a variety of\napplications where regression models are regularly employed. In low dimensions,\nrank-based M-estimators are recommended to deal with the generalized regression\nmodel, giving root-$n$ consistent estimators of $\\beta^*$. Applications of\nthese estimators to high dimensional data, however, are questionable. This\narticle studies, both theoretically and practically, a simple yet powerful\nsmoothing approach to handle the high dimensional generalized regression model.\nTheoretically, a family of smoothing functions is provided, and the amount of\nsmoothing necessary for efficient inference is carefully calculated.\nPractically, our study is motivated by an important and challenging scientific\nproblem: decoding gene regulation by predicting transcription factors that bind\nto cis-regulatory elements. Applying our proposed method to this problem shows\nsubstantial improvement over the state-of-the-art alternative in real data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 20:56:00 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 19:49:20 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 01:01:08 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Han", "Fang", ""], ["Ji", "Hongkai", ""], ["Ji", "Zhicheng", ""], ["Wang", "Honglang", ""]]}, {"id": "1509.07426", "submitter": "Hua Zhou", "authors": "Hua Zhou and Liuyi Hu and Jin Zhou and Kenneth Lange", "title": "MM Algorithms for Variance Components Models", "comments": "36 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variance components estimation and mixed model analysis are central themes in\nstatistics with applications in numerous scientific disciplines. Despite the\nbest efforts of generations of statisticians and numerical analysts, maximum\nlikelihood estimation and restricted maximum likelihood estimation of variance\ncomponent models remain numerically challenging. Building on the\nminorization-maximization (MM) principle, this paper presents a novel iterative\nalgorithm for variance components estimation. MM algorithm is trivial to\nimplement and competitive on large data problems. The algorithm readily extends\nto more complicated problems such as linear mixed models, multivariate response\nmodels possibly with missing data, maximum a posteriori estimation, penalized\nestimation, and generalized estimating equations (GEE). We establish the global\nconvergence of the MM algorithm to a KKT point and demonstrate, both\nnumerically and theoretically, that it converges faster than the classical EM\nalgorithm when the number of variance components is greater than two and all\ncovariance matrices are positive definite.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 16:38:16 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Zhou", "Hua", ""], ["Hu", "Liuyi", ""], ["Zhou", "Jin", ""], ["Lange", "Kenneth", ""]]}, {"id": "1509.07510", "submitter": "James M. Flegal", "authors": "Lei Gong, James M. Flegal, Stephen R. Spindler, and Patricia L. Mote", "title": "Bayesian model selection on linear mixed-effects models for comparisons\n  between multiple treatments and a control", "comments": "22 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bayesian model selection technique on linear mixed-effects\nmodels to compare multiple treatments with a control. A fully Bayesian approach\nis implemented to estimate the marginal inclusion probabilities that provide a\ndirect measure of the difference between treatments and the control, along with\nthe model-averaged posterior distributions. Default priors are proposed for\nmodel selection incorporating domain knowledge and a component-wise Gibbs\nsampler is developed for efficient posterior computation. We demonstrate the\nproposed method based on simulated data and an experimental dataset from a\nlongitudinal study of mouse lifespan and weight trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 20:07:51 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Gong", "Lei", ""], ["Flegal", "James M.", ""], ["Spindler", "Stephen R.", ""], ["Mote", "Patricia L.", ""]]}, {"id": "1509.07535", "submitter": "Sayantan Banerjee", "authors": "Sayantan Banerjee, Rehan Akbani and Veerabhadran Baladandayuthapani", "title": "Bayesian Nonparametric Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present clustering methods for multivariate data exploiting the underlying\ngeometry of the graphical structure between variables. As opposed to standard\napproaches that assume known graph structures, we first estimate the edge\nstructure of the unknown graph using Bayesian neighborhood selection\napproaches, wherein we account for the uncertainty of graphical structure\nlearning through model-averaged estimates of the suitable parameters.\nSubsequently, we develop a nonparametric graph clustering model on the lower\ndimensional projections of the graph based on Laplacian embeddings using\nDirichlet process mixture models. In contrast to standard algorithmic\napproaches, this fully probabilistic approach allows incorporation of\nuncertainty in estimation and inference for both graph structure learning and\nclustering. More importantly, we formalize the arguments for Laplacian\nembeddings as suitable projections for graph clustering by providing\ntheoretical support for the consistency of the eigenspace of the estimated\ngraph Laplacians. We develop fast computational algorithms that allow our\nmethods to scale to large number of nodes. Through extensive simulations we\ncompare our clustering performance with standard clustering methods. We apply\nour methods to a novel pan-cancer proteomic data set, and evaluate protein\nnetworks and clusters across multiple different cancer types.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 20:52:05 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Banerjee", "Sayantan", ""], ["Akbani", "Rehan", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1509.07583", "submitter": "Garth Tarr", "authors": "Garth Tarr, Samuel M\\\"uller, Alan Welsh", "title": "mplot: An R Package for Graphical Model Stability and Variable Selection\n  Procedures", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": "10.18637/jss.v083.i09", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mplot package provides an easy to use implementation of model stability\nand variable inclusion plots (M\\\"uller and Welsh 2010; Murray, Heritier, and\nM\\\"uller 2013) as well as the adaptive fence (Jiang, Rao, Gu, and Nguyen 2008;\nJiang, Nguyen, and Rao 2009) for linear and generalised linear models. We\nprovide a number of innovations on the standard procedures and address many\npractical implementation issues including the addition of redundant variables,\ninteractive visualisations and approximating logistic models with linear\nmodels. An option is provided that combines our bootstrap approach with glmnet\nfor higher dimensional models. The plots and graphical user interface leverage\nstate of the art web technologies to facilitate interaction with the results.\nThe speed of implementation comes from the leaps package and cross-platform\nmulticore support.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 04:23:33 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 22:59:52 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Tarr", "Garth", ""], ["M\u00fcller", "Samuel", ""], ["Welsh", "Alan", ""]]}, {"id": "1509.07900", "submitter": "Tiep Mai", "authors": "Tiep Mai and Simon Wilson", "title": "Bayesian sequential parameter estimation with a Laplace type\n  approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for sequential inference of the fixed parameters of a dynamic latent\nGaussian models is proposed and evaluated that is based on the iterated Laplace\napproximation. The method provides a useful trade-off between computational\nperformance and the accuracy of the approximation to the true posterior\ndistribution. Approximation corrections are shown to improve the accuracy of\nthe approximation in simulation studies. A population-based approach is also\nshown to provide a more robust inference method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 21:23:07 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Mai", "Tiep", ""], ["Wilson", "Simon", ""]]}, {"id": "1509.07913", "submitter": "Charles Manski", "authors": "Charles F. Manski and Aleksey Tetenov", "title": "Clinical trial design enabling {\\epsilon}-optimal treatment rules", "comments": null, "journal-ref": "Proc. Natl. Acad. Sci. U.S.A. 113 (2016) 10518-10523", "doi": "10.1073/pnas.1612174113", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical research has evolved conventions for choosing sample size in\nrandomized clinical trials that rest on the theory of hypothesis testing.\nBayesians have argued that trials should be designed to maximize subjective\nexpected utility in settings of clinical interest. This perspective is\ncompelling given a credible prior distribution on treatment response, but\nBayesians have struggled to provide guidance on specification of priors. We use\nthe frequentist statistical decision theory of Wald (1950) to study design of\ntrials under ambiguity. We show that {\\epsilon}-optimal rules exist when trials\nhave large enough sample size. An {\\epsilon}-optimal rule has expected welfare\nwithin {\\epsilon} of the welfare of the best treatment in every state of\nnature. Equivalently, it has maximum regret no larger than {\\epsilon}. We\nconsider trials that draw predetermined numbers of subjects at random within\ngroups stratified by covariates and treatments. The principal analytical\nfindings are simple sufficient conditions on sample sizes that ensure existence\nof {\\epsilon}-optimal treatment rules when outcomes are bounded. These\nconditions are obtained by application of Hoeffding (1963) large deviations\ninequalities to evaluate the performance of empirical success rules.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 22:36:22 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Manski", "Charles F.", ""], ["Tetenov", "Aleksey", ""]]}, {"id": "1509.07942", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Tatsuya Kubokawa", "title": "Bayesian Estimators in Uncertain Nested Error Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested error regression models are useful tools for analysis of grouped data,\nespecially in the case of small area estimation. This paper suggests a nested\nerror regression model using uncertain random effects in which the random\neffect in each area is expressed as a mixture of a normal distribution and a\npositive mass at $0$. For estimation of the model parameters and prediction of\nthe random effects, an objective Bayesian inference is proposed by setting\nnon-informative prior distributions on the model parameters. Under mild\nsufficient conditions, it is shown that the posterior distribution is proper\nand the posterior variances are finite, confirming the validity of posterior\ninference. To generate samples from the posterior distribution, we provide the\nGibbs sampling method with familiar forms for all the full conditional\ndistributions. This paper also addresses the problem of predicting finite\npopulation means, and a sampling-based method is suggested to tackle this\nissue. Finally, the proposed model is compared with the conventional nested\nerror regression model through simulation and empirical studies.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 03:39:49 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 04:46:21 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1509.07982", "submitter": "Carel F.W. Peeters", "authors": "Anders Ellern Bilgrau, Carel F.W. Peeters, Poul Svante Eriksen, Martin\n  B{\\o}gsted, Wessel N. van Wieringen", "title": "Targeted Fused Ridge Estimation of Inverse Covariance Matrices from\n  Multiple High-Dimensional Data Classes", "comments": "52 pages, 11 figures", "journal-ref": "Journal of Machine Learning Research, 21(26):1--52, 2020", "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of jointly estimating multiple inverse covariance\nmatrices from high-dimensional data consisting of distinct classes. An\n$\\ell_2$-penalized maximum likelihood approach is employed. The suggested\napproach is flexible and generic, incorporating several other\n$\\ell_2$-penalized estimators as special cases. In addition, the approach\nallows specification of target matrices through which prior knowledge may be\nincorporated and which can stabilize the estimation procedure in\nhigh-dimensional settings. The result is a targeted fused ridge estimator that\nis of use when the precision matrices of the constituent classes are believed\nto chiefly share the same structure while potentially differing in a number of\nlocations of interest. It has many applications in (multi)factorial study\ndesigns. We focus on the graphical interpretation of precision matrices with\nthe proposed estimator then serving as a basis for integrative or meta-analytic\nGaussian graphical modeling. Situations are considered in which the classes are\ndefined by data sets and subtypes of diseases. The performance of the proposed\nestimator in the graphical modeling setting is assessed through extensive\nsimulation experiments. Its practical usability is illustrated by the\ndifferential network modeling of 12 large-scale gene expression data sets of\ndiffuse large B-cell lymphoma subtypes. The estimator and its related\nprocedures are incorporated into the R-package rags2ridges.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 14:08:14 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 14:06:45 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Bilgrau", "Anders Ellern", ""], ["Peeters", "Carel F. W.", ""], ["Eriksen", "Poul Svante", ""], ["B\u00f8gsted", "Martin", ""], ["van Wieringen", "Wessel N.", ""]]}, {"id": "1509.08036", "submitter": "Arturo Erdely", "authors": "Arturo Erdely", "title": "Backtesting forecast accuracy", "comments": "10 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical test based on the geometric mean is proposed to determine if a\npredictive model should be rejected or not, when the quantity of interest is a\nstrictly positive continuous random variable. A simulation study is performed\nto compare test power performance against an alternative procedure, and an\napplication to insurance claims reserving is illustrated.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 23:46:28 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 12:59:47 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Erdely", "Arturo", ""]]}, {"id": "1509.08056", "submitter": "Kun Zhang", "authors": "Kun Zhang, Biwei Huang, Jiji Zhang, Bernhard Sch\\\"olkopf, Clark\n  Glymour", "title": "Discovery and Visualization of Nonstationary Causal Models", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonplace to encounter nonstationary data, of which the underlying\ngenerating process may change over time or across domains. The nonstationarity\npresents both challenges and opportunities for causal discovery. In this paper\nwe propose a principled framework to handle nonstationarity, and develop some\nmethods to address three important questions. First, we propose an enhanced\nconstraint-based method to detect variables whose local mechanisms are\nnonstationary and recover the skeleton of the causal structure over observed\nvariables. Second, we present a way to determine some causal directions by\ntaking advantage of information carried by changing distributions. Third, we\ndevelop a method for visualizing the nonstationarity of causal modules.\nExperimental results on various synthetic and real-world data sets are\npresented to demonstrate the efficacy of our methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 06:22:01 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 17:54:26 GMT"}, {"version": "v3", "created": "Sat, 18 Jun 2016 09:36:50 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zhang", "Kun", ""], ["Huang", "Biwei", ""], ["Zhang", "Jiji", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Glymour", "Clark", ""]]}, {"id": "1509.08076", "submitter": "Robert Kohn", "authors": "Minh Ngoc Tran and Robert Kohn", "title": "Exact ABC using Importance Sampling", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a powerful method for carrying out\nBayesian inference when the likelihood is computationally intractable. However,\na drawback of ABC is that it is an approximate method that induces a systematic\nerror because it is necessary to set a tolerance level to make the computation\ntractable. The issue of how to optimally set this tolerance level has been the\nsubject of extensive research. This paper proposes an ABC algorithm based on\nimportance sampling that estimates expectations with respect to the \"exact\"\nposterior distribution given the observed summary statistics. This overcomes\nthe need to select the tolerance level. By \"exact\" we mean that there is no\nsystematic error and the Monte Carlo error can be made arbitrarily small by\nincreasing the number of importance samples. We provide a formal justification\nfor the method and study its convergence properties. The method is illustrated\nin two applications and the empirical results suggest that the proposed ABC\nbased estimators consistently converge to the true values as the number of\nimportance samples increases. Our proposed approach can be applied more\ngenerally to any importance sampling problem where an unbiased estimate of the\nlikelihood is required.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 10:13:07 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Tran", "Minh Ngoc", ""], ["Kohn", "Robert", ""]]}, {"id": "1509.08124", "submitter": "Kelly Bodwin", "authors": "Kelly Bodwin, Kai Zhang, and Andrew Nobel", "title": "A testing-based approach to the discovery of differentially correlated\n  variable sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data obtained under two sampling conditions, it is often of interest to\nidentify variables that behave differently in one condition than in the other.\nWe introduce a method for differential analysis of second-order behavior called\nDifferential Correlation Mining (DCM). The DCM method identifies differentially\ncorrelated sets of variables, with the property that the average pairwise\ncorrelation between variables in a set is higher under one sample condition\nthan the other. DCM is based on an iterative search procedure that adaptively\nupdates the size and elements of a candidate variable set. Updates are\nperformed via hypothesis testing of individual variables, based on the\nasymptotic distribution of their average differential correlation. We\ninvestigate the performance of DCM by applying it to simulated data as well as\nrecent experimental datasets in genomics and brain imaging.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 19:10:39 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 08:59:45 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Bodwin", "Kelly", ""], ["Zhang", "Kai", ""], ["Nobel", "Andrew", ""]]}, {"id": "1509.08165", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder, Arkopal Choudhury, Garud Iyengar, Bodhisattva Sen", "title": "A Computational Framework for Multivariate Convex Regression and its\n  Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the nonparametric least squares estimator (LSE) of a multivariate\nconvex regression function. The LSE, given as the solution to a quadratic\nprogram with $O(n^2)$ linear constraints ($n$ being the sample size), is\ndifficult to compute for large problems. Exploiting problem specific structure,\nwe propose a scalable algorithmic framework based on the augmented Lagrangian\nmethod to compute the LSE. We develop a novel approach to obtain smooth convex\napproximations to the fitted (piecewise affine) convex LSE and provide formal\nbounds on the quality of approximation. When the number of samples is not too\nlarge compared to the dimension of the predictor, we propose a regularization\nscheme --- Lipschitz convex regression --- where we constrain the norm of the\nsubgradients, and study the rates of convergence of the obtained LSE. Our\nalgorithmic framework is simple and flexible and can be easily adapted to\nhandle variants: estimation of a non-decreasing/non-increasing convex/concave\n(with or without a Lipschitz bound) function. We perform numerical studies\nillustrating the scalability of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 00:34:02 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Mazumder", "Rahul", ""], ["Choudhury", "Arkopal", ""], ["Iyengar", "Garud", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1509.08184", "submitter": "Harry Crane", "authors": "Harry Crane and Walter Dempsey", "title": "Atypical scaling behavior persists in real world interaction networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale-free power law structure describes complex networks derived from a wide\nrange of real world processes. The extensive literature focuses almost\nexclusively on networks with power law exponent strictly larger than 2, which\ncan be explained by constant vertex growth and preferential attachment. The\ncomplementary scale-free behavior in the range between 1 and 2 has been mostly\nneglected as atypical because there is no known generating mechanism to explain\nhow networks with this property form. However, empirical observations reveal\nthat scaling in this range is an inherent feature of real world networks\nobtained from repeated interactions within a population, as in social,\ncommunication, and collaboration networks. A generative model explains the\nobserved phenomenon through the realistic dynamics of constant edge growth and\na positive feedback mechanism. Our investigation, therefore, yields a novel\nempirical observation grounded in a strong theoretical basis for its\noccurrence.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 03:36:43 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Crane", "Harry", ""], ["Dempsey", "Walter", ""]]}, {"id": "1509.08398", "submitter": "Bartolomeo Stellato", "authors": "Bartolomeo Stellato, Bart Van Parys and Paul J. Goulart", "title": "Multivariate Chebyshev Inequality with Estimated Mean and Variance", "comments": null, "journal-ref": null, "doi": "10.1080/00031305.2016.1186559", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variant of the well-known Chebyshev inequality for scalar random variables\ncan be formulated in the case where the mean and variance are estimated from\nsamples. In this paper we present a generalization of this result to multiple\ndimensions where the only requirement is that the samples are independent and\nidentically distributed. Furthermore, we show that as the number of samples\ntends to infinity our inequality converges to the theoretical multi-dimensional\nChebyshev bound.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 17:16:26 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 16:56:09 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Stellato", "Bartolomeo", ""], ["Van Parys", "Bart", ""], ["Goulart", "Paul J.", ""]]}, {"id": "1509.08442", "submitter": "Melissa Turcotte", "authors": "Melissa J. M. Turcotte, Nicholas A. Heard", "title": "Adaptive sequential Monte Carlo for multiple changepoint analysis", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process monitoring and control requires detection of structural changes in a\ndata stream in real time. This article introduces an efficient sequential Monte\nCarlo algorithm designed for learning unknown changepoints in continuous time.\nThe method is intuitively simple: new changepoints for the latest window of\ndata are proposed by conditioning only on data observed since the most recent\nestimated changepoint, as these carry most of the information about the state\nof the process prior to the update. The proposed method shows improved\nperformance over the current state of the art. Another advantage of the\nproposed algorithm is that it can be made adaptive, varying the number of\nparticles according to the apparent local complexity of the target changepoint\nprobability distribution. This saves valuable computing time when changes in\nthe change- point distribution are negligible, and enables re-balancing of the\nimportance weights of ex- isting particles when a significant change in the\ntarget distribution is encountered. The plain and adaptive versions of the\nmethod are illustrated using the canonical con- tinuous time changepoint\nproblem of inferring the intensity of an inhomogeneous Poisson process.\nPerformance is demonstrated using both conjugate and non-conjugate Bayesian\nmodels for the intensity.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 19:29:06 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Turcotte", "Melissa J. M.", ""], ["Heard", "Nicholas A.", ""]]}, {"id": "1509.08444", "submitter": "Xianyang Zhang", "authors": "Xianyang Zhang", "title": "Testing High Dimensional Mean Under Sparsity", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the likelihood ratio test under the Gaussian assumption, we\ndevelop a maximum sum-of-squares test for conducting hypothesis testing on high\ndimensional mean vector. The proposed test which incorporates the dependence\namong the variables is designed to ease the computational burden and to\nmaximize the asymptotic power in the likelihood ratio test. A simulation-based\napproach is developed to approximate the sampling distribution of the test\nstatistic. The validity of the testing procedure is justified under both the\nnull and alternative hypotheses. We further extend the main results to the two\nsample problem without the equal covariance assumption. Numerical results\nsuggest that the proposed test can be more powerful than some existing\nalternatives.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 19:35:00 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 23:39:38 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Zhang", "Xianyang", ""]]}, {"id": "1509.08638", "submitter": "Antonello Maruotti", "authors": "Gianluca Mastrantonio, Giovanna Jona Lasinio, Antonello Maruotti,\n  Gianfranco Calise", "title": "On initial direction, orientation and discreteness in the analysis of\n  circular variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a discrete circular distribution obtained by\nextending the wrapped Poisson distribution. This new distribution, the\nInvariant Wrapped Poisson (IWP), enjoys numerous advantages: simple tractable\ndensity, parameter-parsimony and interpretability, good circular dependence\nstructure and easy random number generation thanks to known\nmarginal/conditional distributions. Existing discrete circular distributions\nstrongly depend on the initial direction and orientation, i.e. a change of the\nreference system on the circle may lead to misleading inferential results. We\ninvestigate the invariance properties, i.e. invariance under change of initial\ndirection and of the reference system orientation, for several continuous and\ndiscrete distributions. We prove that the introduced IWP distribution satisfies\nthese two crucial properties. We estimate parameters in a Bayesian framework\nand provide all computational details to implement the algorithm. Inferential\nissues related to the invariance properties are discussed through numerical\nexamples on artificial and real data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:44:13 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Lasinio", "Giovanna Jona", ""], ["Maruotti", "Antonello", ""], ["Calise", "Gianfranco", ""]]}, {"id": "1509.08775", "submitter": "Daniel Paulin", "authors": "Daniel Paulin, Ajay Jasra, Alexandre Thiery", "title": "Error Bounds for Sequential Monte Carlo Samplers for Multimodal\n  Distributions", "comments": "42 pages, 6 figures. Some minor corrections in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide bounds on the asymptotic variance for a class of\nsequential Monte Carlo (SMC) samplers designed for approximating multimodal\ndistributions. Such methods combine standard SMC methods and Markov chain Monte\nCarlo (MCMC) kernels. Our bounds improve upon previous results, and unlike some\nearlier work, they also apply in the case when the MCMC kernels can move\nbetween the modes. We apply our results to the Potts model from statistical\nphysics. In this case, the problem of sharp peaks is encountered. Earlier\nmethods, such as parallel tempering, are only able to sample from it at an\nexponential (in an important parameter of the model) cost. We propose a\nsequence of interpolating distributions called interpolation to independence,\nand show that the SMC sampler based on it is able to sample from this target\ndistribution at a polynomial cost. We believe that our method is generally\napplicable to many other distributions as well.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 14:33:59 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 12:09:35 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 15:52:04 GMT"}, {"version": "v4", "created": "Wed, 24 Jan 2018 06:53:21 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Paulin", "Daniel", ""], ["Jasra", "Ajay", ""], ["Thiery", "Alexandre", ""]]}, {"id": "1509.08864", "submitter": "Tiago Fragoso", "authors": "Tiago M. Fragoso and Francisco Louzada Neto", "title": "Bayesian model averaging: A systematic review and conceptual\n  classification", "comments": null, "journal-ref": null, "doi": "10.1111/insr.12243", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Model Averaging (BMA) is an application of Bayesian inference to the\nproblems of model selection, combined estimation and prediction that produces a\nstraightforward model choice criteria and less risky predictions. However, the\napplication of BMA is not always straightforward, leading to diverse\nassumptions and situational choices on its different aspects. Despite the\nwidespread application of BMA in the literature, there were not many accounts\nof these differences and trends besides a few landmark revisions in the late\n1990s and early 2000s, therefore not taking into account any advancements made\nin the last 15 years. In this work, we present an account of these developments\nthrough a careful content analysis of 587 articles in BMA published between\n1996 and 2014. We also develop a conceptual classification scheme to better\ndescribe this vast literature, understand its trends and future directions and\nprovide guidance for the researcher interested in both the application and\ndevelopment of the methodology. The results of the classification scheme and\ncontent review are then used to discuss the present and future of the BMA\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 17:34:48 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Fragoso", "Tiago M.", ""], ["Neto", "Francisco Louzada", ""]]}, {"id": "1509.09169", "submitter": "Wessel van Wieringen", "authors": "Wessel N. van Wieringen", "title": "Lecture notes on ridge regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The linear regression model cannot be fitted to high-dimensional data, as the\nhigh-dimensionality brings about empirical non-identifiability. Penalized\nregression overcomes this non-identifiability by augmentation of the loss\nfunction by a penalty (i.e. a function of regression coefficients). The ridge\npenalty is the sum of squared regression coefficients, giving rise to ridge\nregression. Here many aspect of ridge regression are reviewed e.g. moments,\nmean squared error, its equivalence to constrained estimation, and its relation\nto Bayesian regression. Finally, its behaviour and use are illustrated in\nsimulation and on omics data. Subsequently, ridge regression is generalized to\nallow for a more general penalty. The ridge penalization framework is then\ntranslated to logistic regression and its properties are shown to carry over.\nTo contrast ridge penalized estimation, the final chapter introduces its lasso\ncounterpart.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 13:38:31 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 19:36:19 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 16:52:27 GMT"}, {"version": "v4", "created": "Mon, 22 Jul 2019 11:05:13 GMT"}, {"version": "v5", "created": "Sat, 18 Jan 2020 21:41:11 GMT"}, {"version": "v6", "created": "Sun, 2 Aug 2020 12:11:13 GMT"}, {"version": "v7", "created": "Mon, 31 May 2021 07:26:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["van Wieringen", "Wessel N.", ""]]}, {"id": "1509.09254", "submitter": "Harry Crane", "authors": "Harry Crane and Walter Dempsey", "title": "Community detection for interaction networks", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, it is common practice to obtain a network from\ninteraction counts by thresholding each pairwise count at a prescribed value.\nOur analysis calls attention to the dependence of certain methods, notably\nNewman--Girvan modularity, on the choice of threshold. Essentially, the\nthreshold either separates the network into clusters automatically, making the\nalgorithm's job trivial, or erases all structure in the data, rendering\nclustering impossible. By fitting the original interaction counts as given, we\nshow that minor modifications to classical statistical methods outperform the\nprevailing approaches for community detection from interaction datasets. We\nalso introduce a new hidden Markov model for inferring community structures\nthat vary over time. We demonstrate each of these features on three real\ndatasets: the karate club dataset, voting data from the U.S.\\ Senate\n(2001--2003), and temporal voting data for the U.S. Supreme Court (1990--2004).\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 16:56:05 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Crane", "Harry", ""], ["Dempsey", "Walter", ""]]}]