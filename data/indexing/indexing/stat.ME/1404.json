[{"id": "1404.0221", "submitter": "Arthur White Mr.", "authors": "Arthur White and Thomas Brendan Murphy", "title": "Mixed-Membership of Experts Stochastic Blockmodel", "comments": "32 pages, 8 figures", "journal-ref": "Network Science, 4, pp 48-80 (2016)", "doi": "10.1017/nws.2015.29", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis is the study of how links between a set of actors are\nformed. Typically, it is believed that links are formed in a structured manner,\nwhich may be due to, for example, political or material incentives, and which\noften may not be directly observable. The stochastic blockmodel represents this\nstructure using latent groups which exhibit different connective properties, so\nthat conditional on the group membership of two actors, the probability of a\nlink being formed between them is represented by a connectivity matrix. The\nmixed membership stochastic blockmodel (MMSBM) extends this model to allow\nactors membership to different groups, depending on the interaction in\nquestion, providing further flexibility.\n  Attribute information can also play an important role in explaining network\nformation. Network models which do not explicitly incorporate covariate\ninformation require the analyst to compare fitted network models to additional\nattributes in a post-hoc manner. We introduce the mixed membership of experts\nstochastic blockmodel, an extension to the MMSBM which incorporates covariate\nactor information into the existing model. The method is illustrated with\napplication to the Lazega Lawyers dataset. Model and variable selection methods\nare also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 12:51:23 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["White", "Arthur", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1404.0541", "submitter": "Johannes Lederer", "authors": "Johannes Lederer and Christian M\\\"uller", "title": "Don't Fall for Tuning Parameters: Tuning-Free Variable Selection in High\n  Dimensions With the TREX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lasso is a seminal contribution to high-dimensional statistics, but it hinges\non a tuning parameter that is difficult to calibrate in practice. A partial\nremedy for this problem is Square-Root Lasso, because it inherently calibrates\nto the noise variance. However, Square-Root Lasso still requires the\ncalibration of a tuning parameter to all other aspects of the model. In this\nstudy, we introduce TREX, an alternative to Lasso with an inherent calibration\nto all aspects of the model. This adaptation to the entire model renders TREX\nan estimator that does not require any calibration of tuning parameters. We\nshow that TREX can outperform cross-validated Lasso in terms of variable\nselection and computational efficiency. We also introduce a bootstrapped\nversion of TREX that can further improve variable selection. We illustrate the\npromising performance of TREX both on synthetic data and on a recent\nhigh-dimensional biological data set that considers riboflavin production in B.\nsubtilis.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 13:11:36 GMT"}, {"version": "v2", "created": "Sun, 13 Apr 2014 23:32:15 GMT"}, {"version": "v3", "created": "Sun, 24 May 2015 09:04:21 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Lederer", "Johannes", ""], ["M\u00fcller", "Christian", ""]]}, {"id": "1404.0860", "submitter": "Klaus Nordhausen", "authors": "Klaus Nordhausen and David E. Tyler", "title": "A cautionary note on robust covariance plug-in methods", "comments": "24 pages, 7 figures", "journal-ref": "Biometrika, 102, 573-588, 2015", "doi": "10.1093/biomet/asv022", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many multivariate statistical methods rely heavily on the sample covariance\nmatrix. It is well known though that the sample covariance matrix is highly\nnon-robust. One popular alternative approach for \"robustifying\" the\nmultivariate method is to simply replace the role of the covariance matrix with\nsome robust scatter matrix. The aim of this paper is to point out that in some\nsituations certain properties of the covariance matrix are needed for the\ncorresponding robust \"plug-in\" method to be a valid approach, and that not all\nscatter matrices necessarily possess these important properties. In particular,\nthe following three multivariate methods are discussed in this paper:\nindependent components analysis, observational regression and graphical\nmodeling. For each case, it is shown that using a symmetrized robust scatter\nmatrix in place of the covariance matrix results in a proper robust\nmultivariate method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 11:19:35 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Nordhausen", "Klaus", ""], ["Tyler", "David E.", ""]]}, {"id": "1404.1175", "submitter": "Alessio Farcomeni", "authors": "Alessio Farcomeni, Sara Viviani", "title": "Longitudinal quantile regression in presence of informative drop-out\n  through longitudinal-survival joint modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a joint model for a time-to-event outcome and a quantile of a\ncontinuous response repeatedly measured over time. The quantile and survival\nprocesses are associated via shared latent and manifest variables. Our joint\nmodel provides a flexible approach to handle informative drop-out in quantile\nregression. A general Monte Carlo Expectation Maximization strategy based on\nimportance sampling is proposed, which is directly applicable under any\ndistributional assumption for the longitudinal outcome and random effects, and\nparametric and non-parametric assumptions for the baseline hazard. Model\nproperties are illustrated through a simulation study and an application to an\noriginal data set about dilated cardiomyopathies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 08:00:11 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Farcomeni", "Alessio", ""], ["Viviani", "Sara", ""]]}, {"id": "1404.1239", "submitter": "Chris Oates", "authors": "Chris J. Oates, Lilia Carneiro da Costa, Tom Nichols", "title": "Towards a Multi-Subject Analysis of Neural Connectivity", "comments": "to appear in Neural Computation 27:1-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs (DAGs) and associated probability models are widely\nused to model neural connectivity and communication channels. In many\nexperiments, data are collected from multiple subjects whose connectivities may\ndiffer but are likely to share many features. In such circumstances it is\nnatural to leverage similarity between subjects to improve statistical\nefficiency. The first exact algorithm for estimation of multiple related DAGs\nwas recently proposed by Oates et al. 2014; in this letter we present examples\nand discuss implications of the methodology as applied to the analysis of fMRI\ndata from a multi-subject experiment. Elicitation of tuning parameters requires\ncare and we illustrate how this may proceed retrospectively based on technical\nreplicate data. In addition to joint learning of subject-specific connectivity,\nwe allow for heterogeneous collections of subjects and simultaneously estimate\nrelationships between the subjects themselves. This letter aims to highlight\nthe potential for exact estimation in the multi-subject setting.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 13:01:46 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 11:16:13 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Oates", "Chris J.", ""], ["da Costa", "Lilia Carneiro", ""], ["Nichols", "Tom", ""]]}, {"id": "1404.1240", "submitter": "Andrea Pagnani", "authors": "Carlo Baldassi, Marco Zamparo, Christoph Feinauer, Andrea Procaccini,\n  Riccardo Zecchina, Martin Weigt, Andrea Pagnani", "title": "Fast and accurate multivariate Gaussian modeling of protein families:\n  Predicting residue contacts and protein-interaction partners", "comments": "24 pages, 7 pdf figures, 2 tables, plus supporting informations.\n  Published on PLOS ONE", "journal-ref": "PLoS ONE 9(3): e92721", "doi": "10.1371/journal.pone.0092721", "report-no": null, "categories": "q-bio.QM cond-mat.dis-nn stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the course of evolution, proteins show a remarkable conservation of their\nthree-dimensional structure and their biological function, leading to strong\nevolutionary constraints on the sequence variability between homologous\nproteins. Our method aims at extracting such constraints from rapidly\naccumulating sequence data, and thereby at inferring protein structure and\nfunction from sequence information alone. Recently, global statistical\ninference methods (e.g. direct-coupling analysis, sparse inverse covariance\nestimation) have achieved a breakthrough towards this aim, and their\npredictions have been successfully implemented into tertiary and quaternary\nprotein structure prediction methods. However, due to the discrete nature of\nthe underlying variable (amino-acids), exact inference requires exponential\ntime in the protein length, and efficient approximations are needed for\npractical applicability. Here we propose a very efficient multivariate Gaussian\nmodeling approach as a variant of direct-coupling analysis: the discrete\namino-acid variables are replaced by continuous Gaussian random variables. The\nresulting statistical inference problem is efficiently and exactly solvable. We\nshow that the quality of inference is comparable or superior to the one\nachieved by mean-field approximations to inference with discrete variables, as\ndone by direct-coupling analysis. This is true for (i) the prediction of\nresidue-residue contacts in proteins, and (ii) the identification of\nprotein-protein interaction partner in bacterial signal transduction. An\nimplementation of our multivariate Gaussian approach is available at the\nwebsite http://areeweb.polito.it/ricerca/cmp/code\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 13:04:28 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Baldassi", "Carlo", ""], ["Zamparo", "Marco", ""], ["Feinauer", "Christoph", ""], ["Procaccini", "Andrea", ""], ["Zecchina", "Riccardo", ""], ["Weigt", "Martin", ""], ["Pagnani", "Andrea", ""]]}, {"id": "1404.1310", "submitter": "Benedikt M. P\\\"otscher", "authors": "David Preinerstorfer and Benedikt M. P\\\"otscher", "title": "On the Power of Invariant Tests for Hypotheses on a Covariance Matrix", "comments": null, "journal-ref": "Econom. Theory 33 (2017) 1-68", "doi": "10.1017/S026646661500033X", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavior of the power function of autocorrelation tests such as the\nDurbin-Watson test in time series regressions or the Cliff-Ord test in spatial\nregression models has been intensively studied in the literature. When the\ncorrelation becomes strong, Kr\\\"amer (1985) (for the Durbin-Watson test) and\nKr\\\"amer (2005) (for the Cliff-Ord test) have shown that the power can be very\nlow, in fact can converge to zero, under certain circumstances. Motivated by\nthese results, Martellosio (2010) set out to build a general theory that would\nexplain these findings. Unfortunately, Martellosio (2010) does not achieve this\ngoal, as a substantial portion of his results and proofs suffer from serious\nflaws. The present paper now builds a theory as envisioned in Martellosio\n(2010) in a fairly general framework, covering general invariant tests of a\nhypothesis on the disturbance covariance matrix in a linear regression model.\nThe general results are then specialized to testing for spatial correlation and\nto autocorrelation testing in time series regression models. We also\ncharacterize the situation where the null and the alternative hypothesis are\nindistinguishable by invariant tests.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 16:47:26 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 14:33:14 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Preinerstorfer", "David", ""], ["P\u00f6tscher", "Benedikt M.", ""]]}, {"id": "1404.1340", "submitter": "Emmanuel Busato", "authors": "Emmanuel Busato", "title": "Equivalence between hybrid CLs and bayesian methods for limit setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between hybrid CLs and bayesian methods used for limit setting\nis discussed. It is shown that the two methods are equivalent in the single\nchannel case even when the background yield is not perfectly known. Only\ncounting experiments are considered in this document.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 19:02:04 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Busato", "Emmanuel", ""]]}, {"id": "1404.1429", "submitter": "Tsuyoshi Kunihama", "authors": "Tsuyoshi Kunihama, David B. Dunson", "title": "Nonparametric Bayes inference on conditional independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In broad applications, it is routinely of interest to assess whether there is\nevidence in the data to refute the assumption of conditional independence of\n$Y$ and $X$ conditionally on $Z$. Such tests are well developed in parametric\nmodels but are not straightforward in the nonparametric case. We propose a\ngeneral Bayesian approach, which relies on an encompassing nonparametric Bayes\nmodel for the joint distribution of $Y$, $X$ and $Z$. The framework allows $Y$,\n$X$ and $Z$ to be random variables on arbitrary spaces, and can accommodate\ndifferent dimensional vectors having a mixture of discrete and continuous\nmeasurement scales. Using conditional mutual information as a scalar summary of\nthe strength of the conditional dependence relationship, we construct null and\nalternative hypotheses. We provide conditions under which the correct\nhypothesis will be consistently selected. Computational methods are developed,\nwhich can be incorporated within MCMC algorithms for the encompassing model.\nThe methods are applied to variable selection and assessed through simulations\nand criminology applications.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 03:59:29 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 13:48:16 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2015 18:36:51 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Kunihama", "Tsuyoshi", ""], ["Dunson", "David B.", ""]]}, {"id": "1404.1473", "submitter": "Dan Ben-Moshe", "authors": "Dan Ben-Moshe", "title": "Identification of Linear Regressions with Errors in all Variables", "comments": "To be published in Econometric Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the classical linear regression model with measurement\nerrors in all the variables. First, we provide necessary and sufficient\nconditions for identification of the coefficients. We show that the\ncoefficients are not identified if and only if an independent normally\ndistributed linear combination of regressors can be transferred from the\nregressors to the errors. Second, we introduce a new estimator for the\ncoefficients using a continuum of moments that are based on second derivatives\nof the log characteristic function of the observables. In Monte Carlo\nsimulations, the estimator performs well and is robust to the amount of\nmeasurement error and number of mismeasured regressors. In an application to\nfirm investment decisions, the estimates are similar to those produced by a\ngeneralized method of moments estimator based on third to fifth moments.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 12:21:19 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 11:27:31 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 20:10:28 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ben-Moshe", "Dan", ""]]}, {"id": "1404.1556", "submitter": "Christopher Fallaize", "authors": "Christopher Fallaize, Peter Green, Kanti Mardia, Stuart Barber", "title": "Bayesian Protein Sequence and Structure Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of a protein is crucial in determining its functionality, and\nis much more conserved than sequence during evolution. A key task in structural\nbiology is to compare protein structures in order to determine evolutionary\nrelationships, estimate the function of newly-discovered structures, and\npredict unknown structures. We propose a Bayesian method for protein structure\nalignment, with the prior on alignments based on functions which penalise\n``gaps'' in the aligned sequences. We show how a broad class of penalty\nfunctions fits into this framework, and how the resulting posterior\ndistribution can be efficiently sampled. A commonly-used gap penalty function\nis shown to be a special case, and we propose a new penalty function which\nalleviates an undesirable feature of the commonly-used penalty. We illustrate\nour method on benchmark data sets, and find it competes well with popular tools\nfrom computational biology. Our method has the benefit of being able to\npotentially explore multiple competing alignments and quantify their merits\nprobabilistically. The framework naturally allows for further information such\nas amino acid sequence to be included, and could be adapted to other situations\nsuch as flexible proteins or domain swaps.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 08:56:15 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 11:30:46 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 18:22:33 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Fallaize", "Christopher", ""], ["Green", "Peter", ""], ["Mardia", "Kanti", ""], ["Barber", "Stuart", ""]]}, {"id": "1404.1578", "submitter": "Andreas Buja", "authors": "Andreas Buja, Richard Berk, Lawrence Brown, Edward George, Emil\n  Pitkin, Mikhail Traskin, Linda Zhao, and Kai Zhang", "title": "Models as Approximations I: Consequences Illustrated with Linear\n  Regression", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the early 1980s Halbert White inaugurated a \"model-robust'' form of\nstatistical inference based on the \"sandwich estimator'' of standard error.\nThis estimator is known to be \"heteroskedasticity-consistent\", but it is less\nwell-known to be \"nonlinearity-consistent'' as well. Nonlinearity, however,\nraises fundamental issues because in its presence regressors are not ancillary,\nhence can't be treated as fixed.\n  The consequences are deep: (1)~population slopes need to be re-interpreted as\nstatistical functionals obtained from OLS fits to largely arbitrary joint\n$\\xy$~distributions; (2)~the meaning of slope parameters needs to be rethought;\n(3)~the regressor distribution affects the slope parameters; (4)~randomness of\nthe regressors becomes a source of sampling variability in slope estimates;\n(5)~inference needs to be based on model-robust standard errors, including\nsandwich estimators or the $\\xy$~bootstrap. In theory, model-robust and\nmodel-trusting standard errors can deviate by arbitrary magnitudes either way.\nIn practice, significant deviations between them can be detected with a\ndiagnostic test.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 14:05:46 GMT"}, {"version": "v2", "created": "Sat, 16 Aug 2014 22:56:43 GMT"}, {"version": "v3", "created": "Thu, 8 Dec 2016 05:16:56 GMT"}, {"version": "v4", "created": "Sat, 6 Jul 2019 19:36:11 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Buja", "Andreas", ""], ["Berk", "Richard", ""], ["Brown", "Lawrence", ""], ["George", "Edward", ""], ["Pitkin", "Emil", ""], ["Traskin", "Mikhail", ""], ["Zhao", "Linda", ""], ["Zhang", "Kai", ""]]}, {"id": "1404.1709", "submitter": "Rajesh  Singh", "authors": "Prayas Sharma and Rajesh Singh", "title": "Method of estimation in the presence of non-response and measurement\n  errors simultaneously", "comments": "14 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper discusses the problem of estimating the finite population\nmean of study variable in simple random sampling in the presence of non\nresponse and response error together. The estimators in this article use\nauxiliary information to improve efficiency and we suppose that non response\nand measurement error are present in both the study and auxiliary variables. A\nclass of estimators has been proposed and its properties are studied in the\nsimultaneous presence of non-response and response errors. It has been shown\nthat proposed class of estimators is more efficient than the usual unbiased\nestimator, ratio and product estimators under non-response and response error\ntogether. In addition, a numerical study is carried out to compare the\nperformance of the proposed class of estimators over others.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 09:41:36 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Sharma", "Prayas", ""], ["Singh", "Rajesh", ""]]}, {"id": "1404.1733", "submitter": "Geoffrey McLachlan", "authors": "Geoffrey J. McLachlan and Sharon X. Lee", "title": "Comment on \"Comparing two formulations of skew distributions with\n  special reference to model-based clustering\" by A. Azzalini, R. Browne, M.\n  Genton, and P. McNicholas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we comment on the recent comparison in Azzalini et al. (2014)\nof two different distributions proposed in the literature for the modelling of\ndata that have asymmetric and possibly long-tailed clusters. They are referred\nto as the restricted and unrestricted skew t-distributions by Lee and McLachlan\n(2013a). Firstly, we wish to point out that in Lee and McLachlan (2014b), which\npreceded this comparison, it is shown how a distribution belonging to the\nbroader class, the canonical fundamental skew t (CFUST) class, can be fitted\nwith essentially no additional computational effort than for the unrestricted\ndistribution. The CFUST class includes the restricted and unrestricted\ndistributions as special cases. Thus the user now has the option of letting the\ndata decide as to which model is appropriate for their particular dataset.\nSecondly, we wish to identify several statements in the comparison by Azzalini\net al.(2014) that demonstrate a serious misunderstanding of the reporting of\nresults in Lee and McLachlan (2014a) on the relative performance of these two\nskew t-distributions. In particular, there is an apparent misunderstanding of\nthe nomenclature that has been adopted to distinguish between these two models.\nThirdly, we take the opportunity to report here that we have obtained improved\nfits, in some cases a marked improvement, for the unrestricted model for\nvarious cases corresponding to different combinations of the variables in the\ntwo real datasets that were used in Azzalini et al. (2014) to mount their\nclaims on the relative superiority of the restricted and unrestricted models.\nFor one case the misclassification rate of our fit under the unrestricted model\nis less than one third of their reported error rate. Our results thus reverse\ntheir claims on the ranking of the restricted and unrestricted models in such\ncases.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 10:39:16 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["McLachlan", "Geoffrey J.", ""], ["Lee", "Sharon X.", ""]]}, {"id": "1404.1785", "submitter": "Fan Li", "authors": "Fan Li, Kari Lock Morgan, Alan M. Zaslavsky", "title": "Balancing Covariates via Propensity Score Weighting", "comments": "33 pages, 5 figures, 5 tables", "journal-ref": null, "doi": "10.1080/01621459.2016.1260466", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate balance is crucial for unconfounded descriptive or causal\ncomparisons. However, lack of balance is common in observational studies. This\narticle considers weighting strategies for balancing covariates. We define a\ngeneral class of weights---the balancing weights---that balance the weighted\ndistributions of the covariates between treatment groups. These weights\nincorporate the propensity score to weight each group to an analyst-selected\ntarget population. This class unifies existing weighting methods, including\ncommonly used weights such as inverse-probability weights as special cases.\nGeneral large-sample results on nonparametric estimation based on these weights\nare derived. We further propose a new weighting scheme, the overlap weights, in\nwhich each unit's weight is proportional to the probability of that unit being\nassigned to the opposite group. The overlap weights are bounded, and minimize\nthe asymptotic variance of the weighted average treatment effect among the\nclass of balancing weights. The overlap weights also possess a desirable\nsmall-sample exact balance property, based on which we propose a new method\nthat achieves exact balance for means of any selected set of covariates. Two\napplications illustrate these methods and compare them with other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 13:40:53 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 02:53:03 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 21:15:54 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Li", "Fan", ""], ["Morgan", "Kari Lock", ""], ["Zaslavsky", "Alan M.", ""]]}, {"id": "1404.1808", "submitter": "Kees Mulder", "authors": "Kees T. Mulder", "title": "Ensuring anonymity in survey panel research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survey panel research, anonymity of the participants is of great\nimportance, as it must be ensured to prevent negative effects of participation\nas well as to maintain trust that the sensitive data that respondents provide\nis handled with care. Measures have been developed in the disclosure control\nliterature to provide estimates for re-identification risk in a dataset.\nApplying them to survey research is not straightforward as most methods that\nhave been developed do handle missing data properly. Here, a new method is\napplied that closely matches a realistic scenario of an attempt of\nre-identification for survey research. This method can be applied to assess\nre-identification risk in survey research.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 15:04:02 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Mulder", "Kees T.", ""]]}, {"id": "1404.1856", "submitter": "Cmu Statistics", "authors": "Joseph B. Kadane", "title": "Sums of Possibly Associated Bernoulli Variables: The\n  Conway-Maxwell-Binomial Distribution", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of sums of possibly associated Bernoulli random variables has been\nhampered by an asymmetry between positive correlation and negative correlation.\nThe Conway-Maxwell Binomial (COMB) distribution and its multivariate extension,\nthe Conway-Maxwell Multinomial (COMM) distribution, gracefully model both\npositive and negative association. Sufficient statistics and a family of proper\nconjugate distributions are found. The relationship of this distribution to the\nexchangeable special case is explored, and two applications are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 17:36:18 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Kadane", "Joseph B.", ""]]}, {"id": "1404.2050", "submitter": "Maciej Kostrzewski", "authors": "Maciej Kostrzewski", "title": "Bayesian DEJD model and detection of asymmetric jumps", "comments": "20 pages, 5 figures, working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News might trigger jump arrivals in financial time series. The \"bad\" and\n\"good\" news seems to have distinct impact. In the research, a double\nexponential jump distribution is applied to model downward and upward jumps.\nBayesian double exponential jump-diffusion model is proposed. Theorems stated\nin the paper enable estimation of the model's parameters, detection of jumps\nand analysis of jump frequency. The methodology, founded upon the idea of\nlatent variables, is illustrated with two empirical studies, employing both\nsimulated and real-world data (the KGHM index). News might trigger jump\narrivals in financial time series. The \"bad\" and \"good\" news seems to have\ndistinct impact. In the research, a double exponential jump distribution is\napplied to model downward and upward jumps. Bayesian double exponential\njump-diffusion model is proposed. Theorems stated in the paper enable\nestimation of the model's parameters, detection of jumps and analysis of jump\nfrequency. The methodology, founded upon the idea of latent variables, is\nillustrated with two empirical studies, employing both simulated and real-world\ndata (the KGHM index).\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 09:11:26 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Kostrzewski", "Maciej", ""]]}, {"id": "1404.2764", "submitter": "Matthew Moores", "authors": "Matthew T. Moores, Catriona E. Hargrave, Fiona Harden and Kerrie\n  Mengersen", "title": "An external field prior for the hidden Potts model, with application to\n  cone-beam computed tomography", "comments": null, "journal-ref": "Computational Statistics & Data Analysis 86 (2015) 27-41", "doi": "10.1016/j.csda.2014.12.001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In images with low contrast-to-noise ratio (CNR), the information gain from\nthe observed pixel values can be insufficient to distinguish foreground\nobjects. A Bayesian approach to this problem is to incorporate prior\ninformation about the objects into a statistical model. This paper introduces a\nmethod for representing spatial prior information as an external field in a\nhidden Potts model of the image lattice. The prior distribution of the latent\npixel labels is a mixture of Gaussian fields, centred on the positions of the\nobjects at a previous point in time. This model is particularly applicable in\nlongitudinal imaging studies, where the manual segmentation of one image can be\nused as a prior for automatic segmentation of subsequent images. The model is\ndemonstrated by application to cone-beam computed tomography (CT), an imaging\nmodality that exhibits distortions in pixel values due to X-ray scatter. The\nexternal field prior results in a substantial improvement in segmentation\naccuracy, reducing the mean pixel misclassification rate on our test images\nfrom 87% to 6%.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 10:38:25 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Moores", "Matthew T.", ""], ["Hargrave", "Catriona E.", ""], ["Harden", "Fiona", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1404.2854", "submitter": "Alan Heavens", "authors": "A.F. Heavens, M. Seikel, B.D. Nord, M. Aich, Y. Bouffanais, B.A.\n  Bassett, M.P. Hobson", "title": "Generalised Fisher Matrices", "comments": "Analysis generalised; now more powerful. Accepted by MNRAS", "journal-ref": "MNRAS, (2014) Vol. 445 : 1687-1693", "doi": "10.1093/mnras/stu1866", "report-no": null, "categories": "astro-ph.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher Information Matrix formalism is extended to cases where the data\nis divided into two parts (X,Y), where the expectation value of Y depends on X\naccording to some theoretical model, and X and Y both have errors with\narbitrary covariance. In the simplest case, (X,Y) represent data pairs of\nabscissa and ordinate, in which case the analysis deals with the case of data\npairs with errors in both coordinates, but X can be any measured quantities on\nwhich Y depends. The analysis applies for arbitrary covariance, provided all\nerrors are gaussian, and provided the errors in X are small, both in comparison\nwith the scale over which the expected signal Y changes, and with the width of\nthe prior distribution. This generalises the Fisher Matrix approach, which\nnormally only considers errors in the `ordinate' Y. In this work, we include\nerrors in X by marginalising over latent variables, effectively employing a\nBayesian hierarchical model, and deriving the Fisher Matrix for this more\ngeneral case. The methods here also extend to likelihood surfaces which are not\ngaussian in the parameter space, and so techniques such as DALI (Derivative\nApproximation for Likelihoods) can be generalised straightforwardly to include\narbitrary gaussian data error covariances. For simple mock data and theoretical\nmodels, we compare to Markov Chain Monte Carlo experiments, illustrating the\nmethod with cosmological supernova data. We also include the new method in the\nFisher4Cast software.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 15:50:46 GMT"}, {"version": "v2", "created": "Sun, 14 Sep 2014 13:05:38 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Heavens", "A. F.", ""], ["Seikel", "M.", ""], ["Nord", "B. D.", ""], ["Aich", "M.", ""], ["Bouffanais", "Y.", ""], ["Bassett", "B. A.", ""], ["Hobson", "M. P.", ""]]}, {"id": "1404.2910", "submitter": "Karthik Bharath", "authors": "Karthik Bharath, Prabhanjan Kambadur, Dipak. K. Dey, Arvind Rao and\n  Veerabhadran Baladandayuthapani", "title": "Statistical Tests for Large Tree-structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general statistical framework for the analysis and inference of\nlarge tree-structured data, with a focus on developing asymptotic\ngoodness-of-fit tests. We first propose a consistent statistical model for\nbinary trees, from which we develop a class of invariant tests. Using the model\nfor binary trees, we then construct tests for general trees by using the\ndistributional properties of the Continuum Random Tree, which arises as the\ninvariant limit for a broad class of models for tree-structured data based on\nconditioned Galton--Watson processes. The test statistics for the\ngoodness-of-fit tests are simple to compute and are asymptotically distributed\nas $\\chi^2$ and $F$ random variables. We illustrate our methods on an important\napplication of detecting tumour heterogeneity in brain cancer. We use a novel\napproach with tree-based representations of magnetic resonance images and\nemploy the developed tests to ascertain tumor heterogeneity between two groups\nof patients.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 19:19:21 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 16:43:11 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 21:43:13 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Bharath", "Karthik", ""], ["Kambadur", "Prabhanjan", ""], ["Dey", "Dipak. K.", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1404.2918", "submitter": "Longhai Li", "authors": "Longhai Li, Shi Qiu, Bei Zhang, and Cindy X. Feng", "title": "Approximating Cross-validatory Predictive Evaluation in Bayesian Latent\n  Variables Models with Integrated IS and WAIC", "comments": "38 pages", "journal-ref": "Statistics and Computing, 2016, Volume 26, Issue 4, pp 881-897", "doi": "10.1007/s11222-015-9577-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural method for approximating out-of-sample predictive evaluation is\nleave-one-out cross-validation (LOOCV) --- we alternately hold out each case\nfrom a full data set and then train a Bayesian model using Markov chain Monte\nCarlo (MCMC) without the held-out; at last we evaluate the posterior predictive\ndistribution of all cases with their actual observations. However, actual LOOCV\nis time-consuming. This paper introduces two methods, namely iIS and iWAIC, for\napproximating LOOCV with only Markov chain samples simulated from a posterior\nbased on a \\textit{full} data set. iIS and iWAIC aim at improving the\napproximations given by importance sampling (IS) and WAIC in Bayesian models\nwith possibly correlated latent variables. In iIS and iWAIC, we first integrate\nthe predictive density over the distribution of the latent variables associated\nwith the held-out without reference to its observation, then apply IS and WAIC\napproximations to the integrated predictive density. We compare iIS and iWAIC\nwith other approximation methods in three real data examples that respectively\nuse mixture models, models with correlated spatial effects, and a random effect\nlogistic model. Our empirical results show that iIS and iWAIC give\nsubstantially better approximates than non-integrated IS and WAIC and other\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 19:43:11 GMT"}, {"version": "v2", "created": "Sun, 4 May 2014 15:30:51 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 21:35:19 GMT"}, {"version": "v4", "created": "Tue, 10 Jun 2014 15:57:07 GMT"}, {"version": "v5", "created": "Thu, 15 Jan 2015 17:03:44 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Li", "Longhai", ""], ["Qiu", "Shi", ""], ["Zhang", "Bei", ""], ["Feng", "Cindy X.", ""]]}, {"id": "1404.2961", "submitter": "Zhigen Zhao", "authors": "Pengsheng Ji and Zhigen Zhao", "title": "Rate optimal multiple testing procedure in high-dimensional regression", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple testing and variable selection have gained much attention in\nstatistical theory and methodology research. They are dealing with the same\nproblem of identifying the important variables among many (Jin, 2012). However,\nthere is little overlap in the literature. Research on variable selection has\nbeen focusing on selection consistency, i.e., both type I and type II errors\nconverging to zero. This is only possible when the signals are sufficiently\nstrong, contrary to many modern applications. For the regime where the signals\nare both rare and weak, it is inevitable that a certain amount of false\ndiscoveries will be allowed, as long as some error rate can be controlled. In\nthis paper, motivated by the research by Ji and Jin (2012) and Jin (2012) in\nthe rare/weak regime, we extend their UPS procedure for variable selection to\nmultiple testing. Under certain conditions, the new UPT procedure achieves the\nfastest convergence rate of marginal false non-discovery rates, while\ncontrolling the marginal false discovery rate at any designated level $\\alpha$\nasymptotically. Numerical results are provided to demonstrate the advantage of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 22:15:04 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 21:14:58 GMT"}, {"version": "v3", "created": "Sat, 17 May 2014 04:48:27 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Ji", "Pengsheng", ""], ["Zhao", "Zhigen", ""]]}, {"id": "1404.2971", "submitter": "Guang Cheng", "authors": "Stanislav Minsker, Ying-Qi Zhao and Guang Cheng", "title": "Active Clinical Trials for Personalized Medicine", "comments": "48 Page, 9 Figures. To Appear in JASA--T&M", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individualized treatment rules (ITRs) tailor treatments according to\nindividual patient characteristics. They can significantly improve patient care\nand are thus becoming increasingly popular. The data collected during\nrandomized clinical trials are often used to estimate the optimal ITRs.\nHowever, these trials are generally expensive to run, and, moreover, they are\nnot designed to efficiently estimate ITRs. In this paper, we propose a\ncost-effective estimation method from an active learning perspective. In\nparticular, our method recruits only the \"most informative\" patients (in terms\nof learning the optimal ITRs) from an ongoing clinical trial. Simulation\nstudies and real-data examples show that our active clinical trial method\nsignificantly improves on competing methods. We derive risk bounds and show\nthat they support these observed empirical advantages.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 00:24:22 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 00:35:30 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Minsker", "Stanislav", ""], ["Zhao", "Ying-Qi", ""], ["Cheng", "Guang", ""]]}, {"id": "1404.3168", "submitter": "Mattia Ciollaro", "authors": "Mattia Ciollaro, Jessi Cisewski, Peter Freeman, Christopher Genovese,\n  Jing Lei, Ross O'Connell, Larry Wasserman", "title": "Functional Regression for Quasar Spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lyman-alpha forest is a portion of the observed light spectrum of distant\ngalactic nuclei which allows us to probe remote regions of the Universe that\nare otherwise inaccessible. The observed Lyman-alpha forest of a quasar light\nspectrum can be modeled as a noisy realization of a smooth curve that is\naffected by a `damping effect' which occurs whenever the light emitted by the\nquasar travels through regions of the Universe with higher matter\nconcentration. To decode the information conveyed by the Lyman-alpha forest\nabout the matter distribution, we must be able to separate the smooth\n`continuum' from the noise and the contribution of the damping effect in the\nquasar light spectra. To predict the continuum in the Lyman-alpha forest, we\nuse a nonparametric functional regression model in which both the response and\nthe predictor variable (the smooth part of the damping-free portion of the\nspectrum) are function-valued random variables. We demonstrate that the\nproposed method accurately predicts the unobservable continuum in the\nLyman-alpha forest both on simulated spectra and real spectra. Also, we\nintroduce distribution-free prediction bands for the nonparametric functional\nregression model that have finite sample guarantees. These prediction bands,\ntogether with bootstrap-based confidence bands for the projection of the mean\ncontinuum on a fixed number of principal components, allow us to assess the\ndegree of uncertainty in the model predictions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 17:52:34 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Ciollaro", "Mattia", ""], ["Cisewski", "Jessi", ""], ["Freeman", "Peter", ""], ["Genovese", "Christopher", ""], ["Lei", "Jing", ""], ["O'Connell", "Ross", ""], ["Wasserman", "Larry", ""]]}, {"id": "1404.3174", "submitter": "Yang Tang", "authors": "Yang Tang, Ryan P. Browne and Paul D. McNicholas", "title": "Model Based Clustering of High-Dimensional Binary Data", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2014.12.009", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mixture of latent trait models with common slope parameters\n(MCLT) for model-based clustering of high-dimensional binary data, a data type\nfor which few established methods exist. Recent work on clustering of binary\ndata, based on a $d$-dimensional Gaussian latent variable, is extended by\nincorporating common factor analyzers. Accordingly, our approach facilitates a\nlow-dimensional visual representation of the clusters. We extend the model\nfurther by the incorporation of random block effects. The dependencies in each\nblock are taken into account through block-specific parameters that are\nconsidered to be random variables. A variational approximation to the\nlikelihood is exploited to derive a fast algorithm for determining the model\nparameters. Our approach is demonstrated on real and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 18:15:30 GMT"}, {"version": "v2", "created": "Sun, 27 Apr 2014 02:56:17 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Tang", "Yang", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1404.3287", "submitter": "Francisco Louzada", "authors": "Paulo H. Ferreira and Francisco Louzada", "title": "A modified version of the inference function for margins and interval\n  estimation for the bivariate Clayton copula SUR Tobit model: An simulation\n  approach", "comments": "25 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the analysis of bivariate seemingly unrelated regression\n(SUR) Tobit model by modeling its nonlinear dependence structure through the\nClayton copula. The ability in capturing/modeling the lower tail dependence of\nthe SUR Tobit model where some data are censored (generally, at zero point) is\nan additionally useful feature of the Clayton copula. We propose a modified\nversion of the inference function for margins (IFM) method (Joe and Xu, 1996),\nwhich we refer to as MIFM method, to obtain the estimates of the marginal\nparameters and a better (satisfactory) estimate of the copula association\nparameter. More specifically, we employ the data augmentation technique in the\nsecond stage of the IFM method to generate the censored observations (i.e. to\nobtain continuous marginal distributions, which ensures the uniqueness of the\ncopula) and then estimate the dependence parameter. Resampling procedures\n(bootstrap methods) are also proposed for obtaining confidence intervals for\nthe model parameters. A simulation study is performed in order to verify the\nbehavior of the MIFM estimates (we focus on the copula parameter estimation)\nand the coverage probability of different confidence intervals in datasets with\ndifferent percentages of censoring and degrees of dependence. The satisfactory\nresults from the simulation (under certain conditions) and empirical study\nindicate the good performance of our proposed model and methods where they are\napplied to model the U.S. ready-to-eat breakfast cereals and fluid milk\nconsumption data.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 12:52:41 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Ferreira", "Paulo H.", ""], ["Louzada", "Francisco", ""]]}, {"id": "1404.3300", "submitter": "Stephan Huckemann", "authors": "Stephan Huckemann, Kwang-Rae Kim, Axel Munk, Florian Rehfeldt, Max\n  Sommerfeld, Joachim Weickert, Carina Wollnik", "title": "The circular SiZer, inferred persistence of shape parameters and\n  application to early stem cell differentiation", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ722 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 4, 2113-2142", "doi": "10.3150/15-BEJ722", "report-no": "IMS-BEJ-BEJ722", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the SiZer of Chaudhuri and Marron (J. Amer. Statist. Assoc. 94\n(1999) 807-823, Ann. Statist. 28 (2000) 408-428) for the detection of shape\nparameters of densities on the real line to the case of circular data. It turns\nout that only the wrapped Gaussian kernel gives a symmetric, strongly Lipschitz\nsemi-group satisfying \"circular\" causality, that is, not introducing possibly\nartificial modes with increasing levels of smoothing. Some notable differences\nbetween Euclidean and circular scale space theory are highlighted. Based on\nthis, we provide an asymptotic theory to make inference about the persistence\nof shape features. The resulting circular mode persistence diagram is applied\nto the analysis of early mechanically-induced differentiation in adult human\nstem cells from their actin-myosin filament structure. As a consequence, the\ncircular SiZer based on the wrapped Gaussian kernel (WiZer) allows the\nverification at a controlled error level of the observation reported by Zemel\net al. (Nat. Phys. 6 (2010) 468-473): Within early stem cell differentiation,\npolarizations of stem cells exhibit preferred directions in three different\nmicro-environments.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 16:19:15 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 12:12:52 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Huckemann", "Stephan", ""], ["Kim", "Kwang-Rae", ""], ["Munk", "Axel", ""], ["Rehfeldt", "Florian", ""], ["Sommerfeld", "Max", ""], ["Weickert", "Joachim", ""], ["Wollnik", "Carina", ""]]}, {"id": "1404.3303", "submitter": "Enkelejd Hashorva", "authors": "Enkelejd Hashorva and Lanpeng Ji", "title": "Random Shifting and Scaling of Insurance Risks", "comments": "11 pages", "journal-ref": "Risks, 2, 277-288 (2014)", "doi": "10.3390/risks2030277", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random shifting typically appears in credibility models whereas random\nscaling is often encountered in stochastic models for claim sizes reflecting\nthe time-value property of money. In this article we discuss some aspects of\nrandom shifting and random scaling in insurance focusing in particular on\ncredibility models, dependence structure of claim sizes in collective risk\nmodels, and extreme value models for the joint dependence of large losses. We\nshow that specifying certain actuarial models using random shifting or scaling\nhas some advantages for both theoretical treatments and practical applications.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 17:07:37 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Hashorva", "Enkelejd", ""], ["Ji", "Lanpeng", ""]]}, {"id": "1404.3331", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Oscar Hernan Madrid Padilla, James G. Scott", "title": "Priors for Random Count Matrices Derived from a Family of Negative\n  Binomial Processes", "comments": "To appear in Journal of the American Statistical Association (Theory\n  and Methods). 31 pages + 11 page supplement, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a family of probability distributions for random count matrices\nwith a potentially unbounded number of rows and columns. The three\ndistributions we consider are derived from the gamma-Poisson, gamma-negative\nbinomial, and beta-negative binomial processes. Because the models lead to\nclosed-form Gibbs sampling update equations, they are natural candidates for\nnonparametric Bayesian priors over count matrices. A key aspect of our analysis\nis the recognition that, although the random count matrices within the family\nare defined by a row-wise construction, their columns can be shown to be i.i.d.\nThis fact is used to derive explicit formulas for drawing all the columns at\nonce. Moreover, by analyzing these matrices' combinatorial structure, we\ndescribe how to sequentially construct a column-i.i.d. random count matrix one\nrow at a time, and derive the predictive distribution of a new row count vector\nwith previously unseen features. We describe the similarities and differences\nbetween the three priors, and argue that the greater flexibility of the gamma-\nand beta- negative binomial processes, especially their ability to model\nover-dispersed, heavy-tailed count data, makes these well suited to a wide\nvariety of real-world applications. As an example of our framework, we\nconstruct a naive-Bayes text classifier to categorize a count vector to one of\nseveral existing random count matrices of different categories. The classifier\nsupports an unbounded number of features, and unlike most existing methods, it\ndoes not require a predefined finite vocabulary to be shared by all the\ncategories, and needs neither feature selection nor parameter tuning. Both the\ngamma- and beta- negative binomial processes are shown to significantly\noutperform the gamma-Poisson process for document categorization, with\ncomparable performance to other state-of-the-art supervised text classification\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 23:59:09 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2014 16:58:58 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 15:54:40 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Padilla", "Oscar Hernan Madrid", ""], ["Scott", "James G.", ""]]}, {"id": "1404.3441", "submitter": "Annalisa Cerquetti", "authors": "Annalisa Cerquetti", "title": "Bayesian nonparametric estimation of Tsallis diversity indices under\n  Gnedin-Pitman priors", "comments": "17 pages, new improved version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tsallis entropy is a generalized diversity index first derived in Patil and\nTaillie (1982) and then rediscovered in community ecology by Keylock (2005).\nBayesian nonparametric estimation of Shannon entropy and Simpson's diversity\nunder uniform and symmetric Dirichlet priors has been already advocated as an\nalternative to maximum likelihood estimation based on frequency counts, which\nis negatively biased in the undersampled regime. Here we present a fully\ngeneral Bayesian nonparametric estimation of the whole class of Tsallis\ndiversity indices under Gnedin-Pitman priors, a large family of random discrete\ndistributions recently deeply investigated in posterior predictive species\nrichness and discovery probability estimation. We provide both prior and\nposterior analysis. The results, illustrated through examples and an\napplication to a real dataset, show the procedure is easily implementable,\nflexible and overcomes limitations of previous frequentist and Bayesian\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 00:11:53 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 18:04:24 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Cerquetti", "Annalisa", ""]]}, {"id": "1404.3444", "submitter": "Kelly Cristina Mota Goncalves", "authors": "Kelly Cristina M. Gon\\c{c}alves and Fernando A. S. Moura", "title": "A mixture model for rare and clustered populations under adaptive\n  cluster sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare populations, such as endangered species, drug users and individuals\ninfected by rare diseases, tend to cluster in regions. Adaptive cluster designs\nare generally applied to obtain information from clustered and sparse\npopulations. The aim of this work is to propose a unit-level mixture model for\nclustered and sparse populations when the data are obtained from an adaptive\ncluster sample. Our approach considers heterogeneity among units belonging to\ndifferent clusters. The proposed model is evaluated using simulated data and a\nreal experiment in which adaptive samples were drawn from an enumeration of a\nwaterfowl species in a 5,000 km2 area of central Florida.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 00:56:47 GMT"}, {"version": "v2", "created": "Fri, 8 Aug 2014 17:35:33 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Gon\u00e7alves", "Kelly Cristina M.", ""], ["Moura", "Fernando A. S.", ""]]}, {"id": "1404.3681", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and Annette M\\\"oller", "title": "Joint probabilistic forecasting of wind speed and temperature using\n  Bayesian model averaging", "comments": "22 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1305.1184", "journal-ref": "Environmetrics 26 (2015), no. 2, 120-132", "doi": "10.1002/env.2316", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of forecasts are typically employed to account for the forecast\nuncertainties inherent in predictions of future weather states. However, biases\nand dispersion errors often present in forecast ensembles require statistical\npost-processing. Univariate post-processing models such as Bayesian model\naveraging (BMA) have been successfully applied for various weather quantities.\nNonetheless, BMA and many other standard post-processing procedures are\ndesigned for a single weather variable, thus ignoring possible dependencies\namong weather quantities. In line with recently upcoming research to develop\nmultivariate post-processing procedures, e.g., BMA for bivariate wind vectors,\nor flexible procedures applicable for multiple weather quantities of different\ntypes, a bivariate BMA model for joint calibration of wind speed and\ntemperature forecasts is proposed based on the bivariate truncated normal\ndistribution. It extends the univariate truncated normal BMA model designed for\npost-processing ensemble forecast of wind speed by adding a normally\ndistributed temperature component with a covariance structure representing the\ndependency among the two weather quantities.\n  The method is applied to wind speed and temperature forecasts of the\neight-member University of Washington mesoscale ensemble and of the\neleven-member ALADIN-HUNEPS ensemble of the Hungarian Meteorological Service\nand its predictive performance is compared to that of the general Gaussian\ncopula method. The results indicate improved calibration of probabilistic and\naccuracy of point forecasts in comparison to the raw ensemble and the overall\nperformance of this model is able to keep up with that of the Gaussian copula\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 18:24:00 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["M\u00f6ller", "Annette", ""]]}, {"id": "1404.3753", "submitter": "Jacopo Soriano", "authors": "Jacopo Soriano and Li Ma", "title": "Multi-resolution two-sample comparison through the divide-merge Markov\n  tree", "comments": "Corrected typos. Added Software section", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a probabilistic framework for two-sample comparison based on a\nnonparametric process taking the form of a Markov model that transitions\nbetween a \"divide\" and a \"merge\" state on a multi-resolution partition tree of\nthe sample space. Multi-scale two-sample comparison is achieved through\ninferring the underlying state of the process along the partition tree. The\nMarkov design allows the process to incorporate spatial clustering of\ndifferential structures, which is commonly observed in two-sample problems but\nignored by existing methods. Inference is carried out under the Bayesian\nparadigm through recursive propagation algorithms. We demonstrate the work of\nour method through simulated data and a real flow cytometry data set, and show\nthat it substantially outperforms other state-of-the-art two-sample tests in\nseveral settings.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 20:48:46 GMT"}, {"version": "v2", "created": "Thu, 29 May 2014 14:41:42 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Soriano", "Jacopo", ""], ["Ma", "Li", ""]]}, {"id": "1404.4032", "submitter": "Ping Li", "authors": "Guangcan Liu and Ping Li", "title": "Recovery of Coherent Data via Low-Rank Dictionary Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently established RPCA method provides us a convenient way to restore\nlow-rank matrices from grossly corrupted observations. While elegant in theory\nand powerful in reality, RPCA may be not an ultimate solution to the low-rank\nmatrix recovery problem. Indeed, its performance may not be perfect even when\ndata are strictly low-rank. This is because conventional RPCA ignores the\nclustering structures of the data which are ubiquitous in modern applications.\nAs the number of cluster grows, the coherence of data keeps increasing, and\naccordingly, the recovery performance of RPCA degrades. We show that the\nchallenges raised by coherent data (i.e., the data with high coherence) could\nbe alleviated by Low-Rank Representation (LRR), provided that the dictionary in\nLRR is configured appropriately. More precisely, we mathematically prove that\nif the dictionary itself is low-rank then LRR is immune to the coherence\nparameter which increases with the underlying cluster number. This provides an\nelementary principle for dealing with coherent data. Subsequently, we devise a\npractical algorithm to obtain proper dictionaries in unsupervised environments.\nOur extensive experiments on randomly generated matrices verify our claims.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 19:35:15 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 17:57:02 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Liu", "Guangcan", ""], ["Li", "Ping", ""]]}, {"id": "1404.4077", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis, Dimitris Karlis", "title": "Model-based clustering using copulas with applications", "comments": null, "journal-ref": "Stat.Comput. 26 (2016) 1079-1099", "doi": "10.1007/s11222-015-9590-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of model-based clustering techniques is based on multivariate\nNormal models and their variants. In this paper copulas are used for the\nconstruction of flexible families of models for clustering applications. The\nuse of copulas in model-based clustering offers two direct advantages over\ncurrent methods: i) the appropriate choice of copulas provides the ability to\nobtain a range of exotic shapes for the clusters, and ii) the explicit choice\nof marginal distributions for the clusters allows the modelling of multivariate\ndata of various modes (either discrete or continuous) in a natural way. This\npaper introduces and studies the framework of copula-based finite mixture\nmodels for clustering applications. Estimation in the general case can be\nperformed using standard EM, and, depending on the mode of the data, more\nefficient procedures are provided that can fully exploit the copula structure.\nThe closure properties of the mixture models under marginalization are\ndiscussed, and for continuous, real-valued data parametric rotations in the\nsample space are introduced, with a parallel discussion on parameter\nidentifiability depending on the choice of copulas for the components. The\nexposition of the methodology is accompanied and motivated by the analysis of\nreal and artificial data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 20:55:27 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 11:02:39 GMT"}, {"version": "v3", "created": "Wed, 2 Jul 2014 14:01:21 GMT"}, {"version": "v4", "created": "Thu, 14 Aug 2014 17:15:42 GMT"}, {"version": "v5", "created": "Thu, 2 Jul 2015 12:25:49 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Kosmidis", "Ioannis", ""], ["Karlis", "Dimitris", ""]]}, {"id": "1404.4178", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Robert Kohn, Mattias Villani, Minh-Ngoc Tran", "title": "Speeding Up MCMC by Efficient Data Subsampling", "comments": "Main changes: The theory has been significantly revised", "journal-ref": null, "doi": "10.1080/01621459.2018.1448827", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Subsampling MCMC, a Markov Chain Monte Carlo (MCMC) framework\nwhere the likelihood function for $n$ observations is estimated from a random\nsubset of $m$ observations. We introduce a highly efficient unbiased estimator\nof the log-likelihood based on control variates, such that the computing cost\nis much smaller than that of the full log-likelihood in standard MCMC. The\nlikelihood estimate is bias-corrected and used in two dependent pseudo-marginal\nalgorithms to sample from a perturbed posterior, for which we derive the\nasymptotic error with respect to $n$ and $m$, respectively. We propose a\npractical estimator of the error and show that the error is negligible even for\na very small $m$ in our applications. We demonstrate that Subsampling MCMC is\nsubstantially more efficient than standard MCMC in terms of sampling efficiency\nfor a given computational budget, and that it outperforms other subsampling\nmethods for MCMC proposed in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 09:33:36 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 19:45:08 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2016 07:05:04 GMT"}, {"version": "v4", "created": "Mon, 12 Dec 2016 15:39:30 GMT"}, {"version": "v5", "created": "Wed, 2 Aug 2017 00:29:59 GMT"}, {"version": "v6", "created": "Mon, 1 Jan 2018 05:19:34 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Villani", "Mattias", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "1404.4414", "submitter": "Gery Geenens", "authors": "Gery Geenens, Arthur Charpentier and Davy Paindaveine", "title": "Probit transformation for nonparametric kernel estimation of the copula\n  density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula modelling has become ubiquitous in modern statistics. Here, the\nproblem of nonparametrically estimating a copula density is addressed. Arguably\nthe most popular nonparametric density estimator, the kernel estimator is not\nsuitable for the unit-square-supported copula densities, mainly because it is\nheavily affected by boundary bias issues. In addition, most common copulas\nadmit unbounded densities, and kernel methods are not consistent in that case.\nIn this paper, a kernel-type copula density estimator is proposed. It is based\non the idea of transforming the uniform marginals of the copula density into\nnormal distributions via the probit function, estimating the density in the\ntransformed domain, which can be accomplished without boundary problems, and\nobtaining an estimate of the copula density through back-transformation.\nAlthough natural, a raw application of this procedure was, however, seen not to\nperform very well in the earlier literature. Here, it is shown that, if\ncombined with local likelihood density estimation methods, the idea yields very\ngood and easy to implement estimators, fixing boundary issues in a natural way\nand able to cope with unbounded copula densities. The asymptotic properties of\nthe suggested estimators are derived, and a practical way of selecting the\ncrucially important smoothing parameters is devised. Finally, extensive\nsimulation studies and a real data analysis evidence their excellent\nperformance compared to their main competitors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 02:21:00 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Geenens", "Gery", ""], ["Charpentier", "Arthur", ""], ["Paindaveine", "Davy", ""]]}, {"id": "1404.4452", "submitter": "M{\\aa}ns Thulin", "authors": "Maik G\\\"orgens and M{\\aa}ns Thulin", "title": "Bias-correction of the maximum likelihood estimator for the\n  $\\alpha$-Brownian bridge", "comments": null, "journal-ref": "Statistics and Probability Letters, 93, 78-86 (2014)", "doi": "10.1016/j.spl.2014.06.020", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\alpha$-Brownian bridge, or scaled Brownian bridge, is a generalization\nof the Brownian bridge with a scaling parameter that determines how strong the\nforce that pulls the process back to 0 is. The bias of the maximum likelihood\nestimator of the parameter $\\alpha$ is derived and a bias-correction that\nimproves the estimator substantially is proposed. The properties of the\nbias-corrected estimator and four Bayesian estimators based on non-informative\npriors are evaluated in a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 08:38:27 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["G\u00f6rgens", "Maik", ""], ["Thulin", "M\u00e5ns", ""]]}, {"id": "1404.4644", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "A New Space for Comparing Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a new mathematical representations for graph, which allows direct\ncomparison between different graph structures, is an open-ended research\ndirection. Having such a representation is the first prerequisite for a variety\nof machine learning algorithms like classification, clustering, etc., over\ngraph datasets. In this paper, we propose a symmetric positive semidefinite\nmatrix with the $(i,j)$-{th} entry equal to the covariance between normalized\nvectors $A^ie$ and $A^je$ ($e$ being vector of all ones) as a representation\nfor graph with adjacency matrix $A$. We show that the proposed matrix\nrepresentation encodes the spectrum of the underlying adjacency matrix and it\nalso contains information about the counts of small sub-structures present in\nthe graph such as triangles and small paths. In addition, we show that this\nmatrix is a \\emph{\"graph invariant\"}. All these properties make the proposed\nmatrix a suitable object for representing graphs.\n  The representation, being a covariance matrix in a fixed dimensional metric\nspace, gives a mathematical embedding for graphs. This naturally leads to a\nmeasure of similarity on graph objects. We define similarity between two given\ngraphs as a Bhattacharya similarity measure between their corresponding\ncovariance matrix representations. As shown in our experimental study on the\ntask of social network classification, such a similarity measure outperforms\nother widely used state-of-the-art methodologies. Our proposed method is also\ncomputationally efficient. The computation of both the matrix representation\nand the similarity value can be performed in operations linear in the number of\nedges. This makes our method scalable in practice.\n  We believe our theoretical and empirical results provide evidence for\nstudying truncated power iterations, of the adjacency matrix, to characterize\nsocial networks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 20:39:24 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1404.4646", "submitter": "Ping Li", "authors": "Guangcan Liu and Ping Li", "title": "Advancing Matrix Completion by Modeling Extra Structures beyond\n  Low-Rankness", "comments": "arXiv admin note: text overlap with arXiv:1404.4032", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known method for completing low-rank matrices based on convex\noptimization has been established by Cand{\\`e}s and Recht. Although\ntheoretically complete, the method may not entirely solve the low-rank matrix\ncompletion problem. This is because the method captures only the low-rankness\nproperty which gives merely a rough constraint that the data points locate on\nsome low-dimensional subspace, but generally ignores the extra structures which\nspecify in more detail how the data points locate on the subspace. Whenever the\ngeometric distribution of the data points is not uniform, the coherence\nparameters of data might be large and, accordingly, the method might fail even\nif the latent matrix we want to recover is fairly low-rank. To better handle\nnon-uniform data, in this paper we propose a method termed Low-Rank Factor\nDecomposition (LRFD), which imposes an additional restriction that the data\npoints must be represented as linear combinations of the bases in a dictionary\nconstructed or learnt in advance. We show that LRFD can well handle non-uniform\ndata, provided that the dictionary is configured properly: We mathematically\nprove that if the dictionary itself is low-rank then LRFD is immune to the\ncoherence parameters which might be large on non-uniform data. This provides an\nelementary principle for learning the dictionary in LRFD and, naturally, leads\nto a practical algorithm for advancing matrix completion. Extensive experiments\non randomly generated matrices and motion datasets show encouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 20:50:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 18:04:35 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Liu", "Guangcan", ""], ["Li", "Ping", ""]]}, {"id": "1404.4830", "submitter": "Stephane Robin", "authors": "C\\'ecile Durot and Sylvie Huet and Fran\\c{c}ois Koladjo and St\\'ephane\n  Robin", "title": "Nonparametric species richness estimation under convexity constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the total number $N$ of species based on the\nabundances of species that have been observed. We adopt a non parametric\napproach where the true abundance distribution $p$ is only supposed to be\nconvex. From this assumption, we propose a definition for convex abundance\ndistributions. We use a least-squares estimate of the truncated version of $p$\nunder the convexity constraint. We deduce two estimators of the total number of\nspecies, the asymptotic distribution of which are derived. We propose three\ndifferent procedures, including a bootstrap one, to obtain a confidence\ninterval for $N$. The performances of the estimators are assessed in a\nsimulation study and compared with competitors. The proposed method is\nillustrated on several examples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 16:02:21 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Durot", "C\u00e9cile", ""], ["Huet", "Sylvie", ""], ["Koladjo", "Fran\u00e7ois", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1404.4880", "submitter": "Alejandro Frery", "authors": "Abra\\~ao D. C. Nascimento and Alejandro C. Frery and Renato J. Cintra", "title": "Bias Correction and Modified Profile Likelihood under the Wishart\n  Complex Distribution", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 52, issue\n  8, August, pages 4932--4941, 2014", "doi": "10.1109/TGRS.2013.2285927", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes improved methods for the maximum likelihood (ML)\nestimation of the equivalent number of looks $L$. This parameter has a\nmeaningful interpretation in the context of polarimetric synthetic aperture\nradar (PolSAR) images. Due to the presence of coherent illumination in their\nprocessing, PolSAR systems generate images which present a granular noise\ncalled speckle. As a potential solution for reducing such interference, the\nparameter $L$ controls the signal-noise ratio. Thus, the proposal of efficient\nestimation methodologies for $L$ has been sought. To that end, we consider\nfirstly that a PolSAR image is well described by the scaled complex Wishart\ndistribution. In recent years, Anfinsen et al. derived and analyzed estimation\nmethods based on the ML and on trace statistical moments for obtaining the\nparameter $L$ of the unscaled version of such probability law. This paper\ngeneralizes that approach. We present the second-order bias expression proposed\nby Cox and Snell for the ML estimator of this parameter. Moreover, the formula\nof the profile likelihood modified by Barndorff-Nielsen in terms of $L$ is\ndiscussed. Such derivations yield two new ML estimators for the parameter $L$,\nwhich are compared to the estimators proposed by Anfinsen et al. The\nperformance of these estimators is assessed by means of Monte Carlo\nexperiments, adopting three statistical measures as comparison criterion: the\nmean square error, the bias, and the coefficient of variation. Equivalently to\nthe simulation study, an application to actual PolSAR data concludes that the\nproposed estimators outperform all the others in homogeneous scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 20:19:02 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Nascimento", "Abra\u00e3o D. C.", ""], ["Frery", "Alejandro C.", ""], ["Cintra", "Renato J.", ""]]}, {"id": "1404.5053", "submitter": "Chris Oates", "authors": "Chris J. Oates, Theodore Papamarkou, Mark Girolami", "title": "The Controlled Thermodynamic Integral for Bayesian Model Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model comparison relies upon the model evidence, yet for many models\nof interest the model evidence is unavailable in closed form and must be\napproximated. Many of the estimators for evidence that have been proposed in\nthe Monte Carlo literature suffer from high variability. This paper considers\nthe reduction of variance that can be achieved by exploiting control variates\nin this setting. Our methodology is based on thermodynamic integration and\napplies whenever the gradient of both the log-likelihood and the log-prior with\nrespect to the parameters can be efficiently evaluated. Results obtained on\nregression models and popular benchmark datasets demonstrate a significant and\nsometimes dramatic reduction in estimator variance and provide insight into the\nwider applicability of control variates to Bayesian model comparison.\n", "versions": [{"version": "v1", "created": "Sun, 20 Apr 2014 16:51:01 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 10:39:05 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Oates", "Chris J.", ""], ["Papamarkou", "Theodore", ""], ["Girolami", "Mark", ""]]}, {"id": "1404.5097", "submitter": "Maria DeYoreo", "authors": "Maria DeYoreo and Athanasios Kottas", "title": "A Fully Nonparametric Modelling Approach to Binary Regression", "comments": null, "journal-ref": "Bayesian Analysis, Volume 10, Number 4 (2015), 821-847", "doi": "10.1214/15-BA963SI", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general nonparametric Bayesian framework for binary regression,\nwhich is built from modeling for the joint response-covariate distribution. The\nobserved binary responses are assumed to arise from underlying continuous\nrandom variables through discretization, and we model the joint distribution of\nthese latent responses and the covariates using a Dirichlet process mixture of\nmultivariate normals. We show that the kernel of the induced mixture model for\nthe observed data is identifiable upon a restriction on the latent variables.\nTo allow for appropriate dependence structure while facilitating\nidentifiability, we use a square-root-free Cholesky decomposition of the\ncovariance matrix in the normal mixture kernel. In addition to allowing for the\nnecessary restriction, this modeling strategy provides substantial\nsimplifications in implementation of Markov chain Monte Carlo posterior\nsimulation. We present two data examples taken from areas for which the\nmethodology is especially well suited. In particular, the first example\ninvolves estimation of relationships between environmental variables, and the\nsecond develops inference for natural selection surfaces in evolutionary\nbiology. Finally, we discuss extensions to regression settings with\nmultivariate ordinal responses.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 03:20:23 GMT"}, {"version": "v2", "created": "Mon, 4 Aug 2014 15:50:33 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["DeYoreo", "Maria", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1404.5126", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Ayanendranath Basu, Leandro Pardo", "title": "On the Robustness of a Divergence based Test of Simple Statistical\n  Hypotheses", "comments": "Pre-print Version, 25 pages, 5 figures", "journal-ref": "Journal of Statistical Planning and Inference, 2015, 161, 91-108", "doi": "10.1016/j.jspi.2015.01.003", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular hypothesis testing procedure, the likelihood ratio test, is\nknown to be highly non-robust in many real situations. Basu et al. (2013a)\nprovided an alternative robust procedure of hypothesis testing based on the\ndensity power divergence; however, although the robustness properties of the\nlatter test were intuitively argued for by the authors together with extensive\nempirical substantiation of the same, no theoretical robustness properties were\npresented in this work. In the present paper we will consider a more general\nclass of tests which form a superfamily of the procedures described by Basu et\nal. (2013a). This superfamily derives from the class of $S$-divergences\nrecently proposed by Basu et al. (2013a). In this context we theoretically\nprove several robustness results of the new class of tests and illustrate them\nin the normal model. All the theoretical robustness properties of the Basu et\nal. (2013a) proposal follows as special cases of our results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 06:58:59 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 10:38:46 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""], ["Pardo", "Leandro", ""]]}, {"id": "1404.5218", "submitter": "Eugenia Koblents", "authors": "Eugenia Koblents and Joaqu\\'in M\\'iguez", "title": "A comparison of nonlinear population Monte Carlo and particle Markov\n  chain Monte Carlo algorithms for Bayesian inference in stochastic kinetic\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of Monte Carlo approximation of\nposterior probability distributions in stochastic kinetic models (SKMs). SKMs\nare multivariate Markov jump processes that model the interactions among\nspecies in biochemical systems according to a set of uncertain parameters.\nMarkov chain Monte Carlo (MCMC) methods have been typically preferred for this\nBayesian inference problem. Specifically, the particle MCMC (pMCMC) method has\nbeen recently shown to be an effective, while computationally demanding, method\napplicable to this problem. Within the pMCMC framework, importance sampling\n(IS) has been used only as the basis of the sequential Monte Carlo (SMC)\napproximation of the acceptance ratio in the Metropolis-Hastings kernel.\nHowever, the recently proposed nonlinear population Monte Carlo (NPMC)\nalgorithm, based on an iterative IS scheme, has also been shown to be effective\nas a Bayesian inference tool for low dimensional (predator-prey) SKMs. In this\npaper, we provide an extensive performance comparison of pMCMC versus NPMC,\nwhen applied to the challenging prokaryotic autoregulatory network. We show how\nthe NPMC method can greatly outperform the pMCMC algorithm in this scenario,\nwith an overall moderate computational effort. We complement the numerical\ncomparison of the two techniques with an asymptotic convergence analysis of the\nnonlinear IS scheme at the core of the proposed method when the importance\nweights can only be computed approximately.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 15:26:30 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Koblents", "Eugenia", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1404.5609", "submitter": "Rina Foygel Barber", "authors": "Rina Foygel Barber, Emmanuel J. Cand\\`es", "title": "Controlling the false discovery rate via knockoffs", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1337 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 5, 2055-2085", "doi": "10.1214/15-AOS1337", "report-no": "IMS-AOS-AOS1337", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields of science, we observe a response variable together with a\nlarge number of potential explanatory variables, and would like to be able to\ndiscover which variables are truly associated with the response. At the same\ntime, we need to know that the false discovery rate (FDR) - the expected\nfraction of false discoveries among all discoveries - is not too high, in order\nto assure the scientist that most of the discoveries are indeed true and\nreplicable. This paper introduces the knockoff filter, a new variable selection\nprocedure controlling the FDR in the statistical linear model whenever there\nare at least as many observations as variables. This method achieves exact FDR\ncontrol in finite sample settings no matter the design or covariates, the\nnumber of variables in the model, or the amplitudes of the unknown regression\ncoefficients, and does not require any knowledge of the noise level. As the\nname suggests, the method operates by manufacturing knockoff variables that are\ncheap - their construction does not require any new data - and are designed to\nmimic the correlation structure found within the existing variables, in a way\nthat allows for accurate FDR control, beyond what is possible with\npermutation-based methods. The method of knockoffs is very general and\nflexible, and can work with a broad class of test statistics. We test the\nmethod in combination with statistics from the Lasso for sparse regression, and\nobtain empirical results showing that the resulting method has far more power\nthan existing selection rules when the proportion of null variables is high.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 19:56:40 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 14:42:12 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 07:54:32 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1404.5671", "submitter": "Masoud  Nasari", "authors": "Miklos Csorgo, Masoud M Nasari", "title": "Inference from Small and Big Data Sets with Error Rates", "comments": "37 pages and three tables. arXiv admin note: text overlap with\n  arXiv:1307.5476", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce randomized $t$-type statistics that will be\nreferred to as randomized pivots. We show that these randomized pivots yield\ncentral limit theorems with a significantly smaller magnitude of error as\ncompared to that of their classical counterparts under the same conditions.\nThis constitutes a desirable result when a relatively small number of data is\navailable. When a data set is too big to be processed, we use our randomized\npivots to make inference about the mean based on significantly smaller\nsub-samples. The approach taken is shown to relate naturally to estimating\ndistributions of both small and big data sets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 23:32:03 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Csorgo", "Miklos", ""], ["Nasari", "Masoud M", ""]]}, {"id": "1404.5970", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Lars Holden, Geir Kjetil Sandve", "title": "Monte Carlo Null Models for Genomic Data", "comments": "Published at http://dx.doi.org/10.1214/14-STS484 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 1, 59-71", "doi": "10.1214/14-STS484", "report-no": "IMS-STS-STS484", "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As increasingly complex hypothesis-testing scenarios are considered in many\nscientific fields, analytic derivation of null distributions is often out of\nreach. To the rescue comes Monte Carlo testing, which may appear deceptively\nsimple: as long as you can sample test statistics under the null hypothesis,\nthe $p$-value is just the proportion of sampled test statistics that exceed the\nobserved test statistic. Sampling test statistics is often simple once you have\na Monte Carlo null model for your data, and defining some form of randomization\nprocedure is also, in many cases, relatively straightforward. However, there\nmay be several possible choices of a randomization null model for the data and\nno clear-cut criteria for choosing among them. Obviously, different null models\nmay lead to very different $p$-values, and a very low $p$-value may thus occur\ndue to the inadequacy of the chosen null model. It is preferable to use\nassumptions about the underlying random data generation process to guide\nselection of a null model. In many cases, we may order the null models by\nincreasing preservation of the data characteristics, and we argue in this paper\nthat this ordering in most cases gives increasing $p$-values, that is, lower\nsignificance. We denote this as the null complexity principle. The principle\ngives a better understanding of the different null models and may guide in the\nchoice between the different models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 20:25:15 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 12:34:06 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 12:32:57 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Holden", "Lars", ""], ["Sandve", "Geir Kjetil", ""]]}, {"id": "1404.6201", "submitter": "Antonello Maruotti", "authors": "Antonello Maruotti, Maurizio Vichi", "title": "Time-varying clustering of multivariate longitudinal observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical method for clustering of multivariate longitudinal\ndata into homogeneous groups. This method relies on a time-varying extension on\nthe classical K-means algorithm, where a multivariate vector autoregressive\nmodel is additionally assumed for modeling the evolution of clusters' centroids\nover time. We base the inference on a least squares specification of the model\nand coordinate descent algorithm. To illustrate our work, we consider a\nlongitudinal dataset on human development. Three variables are modeled, namely\nlife expectancy, education and gross domestic product.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 17:56:50 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Maruotti", "Antonello", ""], ["Vichi", "Maurizio", ""]]}, {"id": "1404.6216", "submitter": "Ping Li", "authors": "Ping Li", "title": "CoRE Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term \"CoRE kernel\" stands for correlation-resemblance kernel. In many\napplications (e.g., vision), the data are often high-dimensional, sparse, and\nnon-binary. We propose two types of (nonlinear) CoRE kernels for non-binary\nsparse data and demonstrate the effectiveness of the new kernels through a\nclassification experiment. CoRE kernels are simple with no tuning parameters.\nHowever, training nonlinear kernel SVM can be (very) costly in time and memory\nand may not be suitable for truly large-scale industrial applications (e.g.\nsearch). In order to make the proposed CoRE kernels more practical, we develop\nbasic probabilistic hashing algorithms which transform nonlinear kernels into\nlinear kernels.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:35:37 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1404.6274", "submitter": "Chun Yu", "authors": "Chun Yu, Weixin Yao, and Xue Bai", "title": "Robust Linear Regression: A Review and Comparison", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least-squares (OLS) estimators for a linear model are very sensitive\nto unusual values in the design space or outliers among y values. Even one\nsingle atypical value may have a large effect on the parameter estimates. This\narticle aims to review and describe some available and popular robust\ntechniques, including some recent developed ones, and compare them in terms of\nbreakdown point and efficiency. In addition, we also use a simulation study and\na real data application to compare the performance of existing robust methods\nunder different scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 21:33:47 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Yu", "Chun", ""], ["Yao", "Weixin", ""], ["Bai", "Xue", ""]]}, {"id": "1404.6289", "submitter": "Qing Zhou", "authors": "Yuliya Marchetti and Qing Zhou", "title": "Solution Path Clustering with Adaptive Concave Penalty", "comments": "36 pages", "journal-ref": "Electronic Journal of Statistics, 8 (2014): 1569-1603", "doi": "10.1214/14-EJS934", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast accumulation of large amounts of complex data has created a need for\nmore sophisticated statistical methodologies to discover interesting patterns\nand better extract information from these data. The large scale of the data\noften results in challenging high-dimensional estimation problems where only a\nminority of the data shows specific grouping patterns. To address these\nemerging challenges, we develop a new clustering methodology that introduces\nthe idea of a regularization path into unsupervised learning. A regularization\npath for a clustering problem is created by varying the degree of sparsity\nconstraint that is imposed on the differences between objects via the minimax\nconcave penalty with adaptive tuning parameters. Instead of providing a single\nsolution represented by a cluster assignment for each object, the method\nproduces a short sequence of solutions that determines not only the cluster\nassignment but also a corresponding number of clusters for each solution. The\noptimization of the penalized loss function is carried out through an MM\nalgorithm with block coordinate descent. The advantages of this clustering\nalgorithm compared to other existing methods are as follows: it does not\nrequire the input of the number of clusters; it is capable of simultaneously\nseparating irrelevant or noisy observations that show no grouping pattern,\nwhich can greatly improve data interpretation; it is a general methodology that\ncan be applied to many clustering problems. We test this method on various\nsimulated datasets and on gene expression data, where it shows better or\ncompetitive performance compared against several clustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 23:12:14 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Marchetti", "Yuliya", ""], ["Zhou", "Qing", ""]]}, {"id": "1404.6386", "submitter": "Antonello Maruotti", "authors": "Antonello Maruotti", "title": "Handling non-ignorable dropouts in longitudinal data: A conditional\n  model based on a latent Markov heterogeneity structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We illustrate a class of conditional models for the analysis of longitudinal\ndata suffering attrition in random effects models framework, where the\nsubject-specific random effects are assumed to be discrete and to follow a\ntime-dependent latent process. The latent process accounts for unobserved\nheterogeneity and correlation between individuals in a dynamic fashion, and for\ndependence between the observed process and the missing data mechanism. Of\nparticular interest is the case where the missing mechanism is non-ignorable.\nTo deal with the topic we introduce a conditional to dropout model. A shape\nchange in the random effects distribution is considered by directly modeling\nthe effect of the missing data process on the evolution of the latent\nstructure. To estimate the resulting model, we rely on the conditional maximum\nlikelihood approach and for this aim we outline an EM algorithm. The proposal\nis illustrated via simulations and then applied on a dataset concerning skin\ncancers. Comparisons with other well-established methods are provided as well.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:54:32 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Maruotti", "Antonello", ""]]}, {"id": "1404.6462", "submitter": "Abhra Sarkar", "authors": "Abhra Sarkar, Debdeep Pati, Bani K. Mallick, Raymond J. Carroll", "title": "Bayesian Semiparametric Multivariate Density Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multivariate density deconvolution when the\ninterest lies in estimating the distribution of a vector-valued random variable\nbut precise measurements of the variable of interest are not available,\nobservations being contaminated with additive measurement errors. The existing\nsparse literature on the problem assumes the density of the measurement errors\nto be completely known. We propose robust Bayesian semiparametric multivariate\ndeconvolution approaches when the measurement error density is not known but\nreplicated proxies are available for each unobserved value of the random\nvector. Additionally, we allow the variability of the measurement errors to\ndepend on the associated unobserved value of the vector of interest through\nunknown relationships which also automatically includes the case of\nmultivariate multiplicative measurement errors. Basic properties of finite\nmixture models, multivariate normal kernels and exchangeable priors are\nexploited in many novel ways to meet the modeling and computational challenges.\nTheoretical results that show the flexibility of the proposed methods are\nprovided. We illustrate the efficiency of the proposed methods in recovering\nthe true density of interest through simulation experiments. The methodology is\napplied to estimate the joint consumption pattern of different dietary\ncomponents from contaminated 24 hour recalls.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 15:43:18 GMT"}, {"version": "v2", "created": "Mon, 23 Jun 2014 04:14:45 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 16:23:00 GMT"}, {"version": "v4", "created": "Mon, 10 Oct 2016 17:48:59 GMT"}, {"version": "v5", "created": "Mon, 5 Dec 2016 13:22:38 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sarkar", "Abhra", ""], ["Pati", "Debdeep", ""], ["Mallick", "Bani K.", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1404.6473", "submitter": "Lucas Mentch", "authors": "Lucas Mentch, Giles Hooker", "title": "Quantifying Uncertainty in Random Forests via Confidence Intervals and\n  Hypothesis Tests", "comments": "To appear in The Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops formal statistical inference procedures for machine\nlearning ensemble methods. Ensemble methods based on bootstrapping, such as\nbagging and random forests, have improved the predictive accuracy of individual\ntrees, but fail to provide a framework in which distributional results can be\neasily determined. Instead of aggregating full bootstrap samples, we consider\npredicting by averaging over trees built on subsamples of the training set and\ndemonstrate that the resulting estimator takes the form of a U-statistic. As\nsuch, predictions for individual feature vectors are asymptotically normal,\nallowing for confidence intervals to accompany predictions. In practice, a\nsubset of subsamples is used for computational speed; here our estimators take\nthe form of incomplete U-statistics and equivalent results are derived. We\nfurther demonstrate that this setup provides a framework for testing the\nsignificance of features. Moreover, the internal estimation method we develop\nallows us to estimate the variance parameters and perform these inference\nprocedures at no additional computational cost. Simulations and illustrations\non a real dataset are provided.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 16:15:59 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 18:52:49 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1404.6617", "submitter": "Anna Klimova", "authors": "Anna Klimova, Caroline Uhler, Tamas Rudas", "title": "Faithfulness and learning hypergraphs from discrete distributions", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concepts of faithfulness and strong-faithfulness are important for\nstatistical learning of graphical models. Graphs are not sufficient for\ndescribing the association structure of a discrete distribution. Hypergraphs\nrepresenting hierarchical log-linear models are considered instead, and the\nconcept of parametric (strong-) faithfulness with respect to a hypergraph is\nintroduced. Strong-faithfulness ensures the existence of uniformly consistent\nparameter estimators and enables building uniformly consistent procedures for a\nhypergraph search. The strength of association in a discrete distribution can\nbe quantified with various measures, leading to different concepts of\nstrong-faithfulness. Lower and upper bounds for the proportions of\ndistributions that do not satisfy strong-faithfulness are computed for\ndifferent parameterizations and measures of association.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 07:03:00 GMT"}, {"version": "v2", "created": "Wed, 25 Jun 2014 13:04:33 GMT"}, {"version": "v3", "created": "Fri, 23 Jan 2015 11:30:39 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Klimova", "Anna", ""], ["Uhler", "Caroline", ""], ["Rudas", "Tamas", ""]]}, {"id": "1404.6633", "submitter": "Zhidong Bai", "authors": "Shurong Zheng, Z. D. Bai and Jiangfeng Yao", "title": "Substitution principle for CLT of linear spectral statistics of\n  high-dimensional sample covariance matrices with applications to hypothesis\n  testing", "comments": "36 pages, 23 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample covariance matrices are widely used in multivariate statistical\nanalysis. The central limit theorems (CLT's) for linear spectral statistics of\nhigh-dimensional non-centered sample covariance matrices have received\nconsiderable attention in random matrix theory and have been applied to many\nhigh-dimensional statistical problems. However, known population mean vectors\nare assumed for non-centered sample covariance matrices, some of which even\nassume Gaussian-like moment conditions. In fact, there are still another two\nmost frequently used sample covariance matrices: the MLE (by subtracting the\nsample mean vector from each sample vector) and the unbiased sample covariance\nmatrix (by changing the denominator $n$ as $N=n-1$ in the MLE) without\ndepending on unknown population mean vectors. In this paper, we not only\nestablish new CLT's for non-centered sample covariance matrices without\nGaussian-like moment conditions but also characterize the non-negligible\ndifferences among the CLT's for the three classes of high-dimensional sample\ncovariance matrices by establishing a {\\em substitution principle}: substitute\nthe {\\em adjusted} sample size $N=n-1$ for the actual sample size $n$ in the\nmajor centering term of the new CLT's so as to obtain the CLT of the unbiased\nsample covariance matrices. Moreover, it is found that the difference between\nthe CLT's for the MLE and unbiased sample covariance matrix is non-negligible\nin the major centering term although the two sample covariance matrices only\nhave differences $n$ and $n-1$ on the dominator. The new results are applied to\ntwo testing problems for high-dimensional data.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 10:56:58 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Zheng", "Shurong", ""], ["Bai", "Z. D.", ""], ["Yao", "Jiangfeng", ""]]}, {"id": "1404.6841", "submitter": "Brian St. Thomas", "authors": "Brian St. Thomas, Lizhen Lin, Lek-Heng Lim, Sayan Mukherjee", "title": "Learning Subspaces of Different Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian model for inferring mixtures of subspaces of\ndifferent dimensions. The key challenge in such a mixture model is\nspecification of prior distributions over subspaces of different dimensions. We\naddress this challenge by embedding subspaces or Grassmann manifolds into a\nsphere of relatively low dimension and specifying priors on the sphere. We\nprovide an efficient sampling algorithm for the posterior distribution of the\nmodel parameters. We illustrate that a simple extension of our mixture of\nsubspaces model can be applied to topic modeling. We also prove posterior\nconsistency for the mixture of subspaces model. The utility of our approach is\ndemonstrated with applications to real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 23:45:05 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 20:13:01 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 01:31:39 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Thomas", "Brian St.", ""], ["Lin", "Lizhen", ""], ["Lim", "Lek-Heng", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1404.6855", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, A.H. Welsh and Waruni Abeysekera", "title": "Fletcher-Turek Model Averaged Profile Likelihood Confidence Intervals", "comments": null, "journal-ref": "Model-averaged confidence intervals. Scandinavian Journal of\n  Statistics, 43, 35-48 (2016)", "doi": "10.1111/sjos.12163", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the model averaged profile likelihood confidence intervals\nproposed by Fletcher and Turek (2011) in a simple situation in which there are\ntwo linear regression models over which we average. We obtain exact expressions\nfor the coverage and the scaled expected length of the intervals and use these\nto compute these quantities in particular situations. We show that the\nFletcher-Turek confidence intervals can have coverage well below the nominal\ncoverage and expected length greater than that of the standard confidence\ninterval with coverage equal to the same minimum coverage. In these situations,\nthe Fletcher-Turek confidence intervals are unfortunately not better than the\nstandard confidence interval used after model selection but ignoring the model\nselection process.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 02:47:36 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Welsh", "A. H.", ""], ["Abeysekera", "Waruni", ""]]}, {"id": "1404.6865", "submitter": "Debasish Roy", "authors": "Saikat Sarkar and Debasish Roy", "title": "A global optimization paradigm based on change of measures", "comments": "38 pages; 2 tables; 4 figures. arXiv admin note: substantial text\n  overlap with arXiv:1403.1680", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A global optimization framework, acronymed COMBEO (Change OfMeasure Based\nEvolutionary Optimization), is proposed. An important aspect in the development\nis a set of derivative-free additive directional terms obtainable through a\nchange of measures en route to the imposition of any stipulated conditions\naimed at driving the realized design variables (particles) to the global\noptimum. The generalized setting offered by the new approach also enables\nseveral basic ideas, used with other global search methods such as the particle\nswarm or the differential evolution, to be rationally incorporated in the\nproposed setup via a change of measures. The global search may be further aided\nby imparting to the directional update terms additional layers of random\nperturbations such as scrambling and selection. Depending on the precise choice\nof the optimality conditions and the extent of random perturbation, the search\ncan be readily rendered either greedy or more exploratory. As numerically\ndemonstrated, the new proposal appears to provide for a more rational, more\naccurate and faster alternative to most available evolutionary optimization\nschemes, prominent amongst which are the differential evolution and the\nparticle swarm methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 04:56:35 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 17:26:29 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Sarkar", "Saikat", ""], ["Roy", "Debasish", ""]]}, {"id": "1404.7063", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Rafael Izbicki, Ann B. Lee, Chad M. Schafer", "title": "High-Dimensional Density Ratio Estimation with Extensions to Approximate\n  Likelihood Computation", "comments": "With supplementary material", "journal-ref": "JMLR W&CP 33 :420-429, 2014", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ratio between two probability density functions is an important component\nof various tasks, including selection bias correction, novelty detection and\nclassification. Recently, several estimators of this ratio have been proposed.\nMost of these methods fail if the sample space is high-dimensional, and hence\nrequire a dimension reduction step, the result of which can be a significant\nloss of information. Here we propose a simple-to-implement, fully nonparametric\ndensity ratio estimator that expands the ratio in terms of the eigenfunctions\nof a kernel-based operator; these functions reflect the underlying geometry of\nthe data (e.g., submanifold structure), often leading to better estimates\nwithout an explicit dimension reduction step. We show how our general framework\ncan be extended to address another important problem, the estimation of a\nlikelihood function in situations where that function cannot be\nwell-approximated by an analytical form. One is often faced with this situation\nwhen performing statistical inference with data from the sciences, due the\ncomplexity of the data and of the processes that generated those data. We\nemphasize applications where using existing likelihood-free methods of\ninference would be challenging due to the high dimensionality of the sample\nspace, but where our spectral series method yields a reasonable estimate of the\nlikelihood function. We provide theoretical guarantees and illustrate the\neffectiveness of our proposed method with numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 17:25:34 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 13:53:12 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Izbicki", "Rafael", ""], ["Lee", "Ann B.", ""], ["Schafer", "Chad M.", ""]]}, {"id": "1404.7084", "submitter": "Zhong Guan", "authors": "Zhong Guan", "title": "Efficient and Robust Density Estimation Using Bernstein Type Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Method of parameterizing and smoothing the unknown underling distributions\nusing Bernstein polynomials is proposed, verified and investigated. Any\ndistribution with bounded and smooth enough density can be approximated by the\nproposed model. The approximating model turns out to be a mixture of beta\ndistributions beta$(i+1, m-i+1)$, $i=0,\\ldots, m$, for some optimal degree $m$.\nA simple change-point estimating method for choosing optimal degree $m$ of the\nBernstein polynomials is also presented. The proposed methods give maximum\nlikelihood density estimate which is consistent in $L_2$ distance at an almost\nparametric rate under some conditions. Simulation study shows that one can\nbenefit from both the smoothness and the accuracy by using the proposed method.\nThe proposed model can also be used to estimate some functional of the unknown\ndistribution such as population mean. As illustration, the proposed methods are\napplied to three different type data sets including a microarray data set.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 18:36:47 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2015 02:24:33 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Guan", "Zhong", ""]]}, {"id": "1404.7197", "submitter": "Xiaoquan Wen", "authors": "Xiaoquan Wen", "title": "Bayesian Model Comparison in Genetic Association Analysis: Linear Mixed\n  Modeling and SNP Set Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of hypothesis testing and model comparison under a\nflexible Bayesian linear regression model whose formulation is closely\nconnected with the linear mixed effect model and the parametric models for SNP\nset analysis in genetic association studies. We derive a class of analytic\napproximate Bayes factors and illustrate their connections with a variety of\nfrequentist test statistics, including the Wald statistic and the variance\ncomponent score statistic. Taking advantage of Bayesian model averaging and\nhierarchical modeling, we demonstrate some distinct advantages and\nflexibilities in the approaches utilizing the derived Bayes factors in the\ncontext of genetic association studies. We demonstrate our proposed methods\nusing real or simulated numerical examples in applications of single SNP\nassociation testing, multi-locus fine-mapping and SNP set association testing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 00:26:57 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 15:36:14 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Wen", "Xiaoquan", ""]]}, {"id": "1404.7331", "submitter": "Jean Peyhardi", "authors": "Jean Peyhardi, Catherine Trottier, Yann Gu\\'edon", "title": "A new specification of generalized linear models for categorical data", "comments": "31 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models for categorical data are specified in heterogeneous ways.\nWe propose to unify the specification of such models. This allows us to define\nthe family of reference models for nominal data. We introduce the notion of\nreversible models for ordinal data that distinguishes adjacent and cumulative\nmodels from sequential ones. The combination of the proposed specification with\nthe definition of reference and reversible models and various invariance\nproperties leads to a new view of regression models for categorical data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 12:21:55 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 08:32:01 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Peyhardi", "Jean", ""], ["Trottier", "Catherine", ""], ["Gu\u00e9don", "Yann", ""]]}, {"id": "1404.7370", "submitter": "Gianluca Frasso", "authors": "Gianluca Frasso, Jonathan Jaeger and Philippe Lambert", "title": "Estimation and approximation in nonlinear dynamic systems using\n  quasilinearization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear (systems of) ordinary differential equations (ODEs) are common\ntools in the analysis of complex one-dimensional dynamic systems. In this paper\nwe propose a smoothing approach regularized by a quasilinearized ODE-based\npenalty in order to approximate the state functions and estimate the parameters\ndefining nonlinear differential systems from noisy data. Within the\nquasilinearized spline based framework, the estimation process reduces to a\nconditionally linear problem for the optimization of the spline coefficients.\nFurthermore, standard ODE compliance parameter(s) selection criteria are easily\napplicable and conditions on the state function(s) can be eventually imposed\nusing soft or hard constraints. The approach is illustrated on real and\nsimulated data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 14:14:08 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Frasso", "Gianluca", ""], ["Jaeger", "Jonathan", ""], ["Lambert", "Philippe", ""]]}, {"id": "1404.7397", "submitter": "Paula Saavedra-Nieves", "authors": "Alberto Rodr\\'iguez-Casal and Paula Saavedra-Nieves", "title": "A fully data-driven method for estimating the shape of a point cloud", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of points from some unknown distribution, we propose a\nnew data-driven method for estimating its probability support $S$. Under the\nmild assumption that $S$ is $r$-convex, the smallest $r$-convex set which\ncontains the sample points is the natural estimator. The main problem for using\nthis estimator in practice is that $r$ is an unknown geometric characteristic\nof the set $S$. A stochastic algorithm is proposed for selecting it from the\ndata under the hypothesis that the sample is uniformly generated. The new\ndata-driven reconstruction of $S$ is able to achieve the same convergence rates\nas the convex hull for estimating convex sets, but under a much more flexible\nsmoothness shape condition. The practical performance of the estimator is\nillustrated through a real data example and a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 15:34:04 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 21:30:31 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Rodr\u00edguez-Casal", "Alberto", ""], ["Saavedra-Nieves", "Paula", ""]]}, {"id": "1404.7403", "submitter": "Asaf Weinstein", "authors": "Asaf Weinstein and Daniel Yekutieli", "title": "Selective Sign-Determining Multiple Confidence Intervals with FCR\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $m$ unknown parameters with corresponding independent estimators, the\nBenjamini-Hochberg (BH) procedure can be used to classify the sign of\nparameters such that the expected proportion of erroneous directional decisions\n(directional FDR) is controlled at a preset level $q$. More ambitiously, our\ngoal is to construct sign-determining confidence intervals---instead of only\nclassifying the sign---such that the expected proportion of non-covering\nconstructed intervals (FCR) is controlled. We suggest a valid procedure which\nadjusts a marginal confidence interval in order to construct a maximum number\nof sign-determining confidence intervals. We propose a new marginal confidence\ninterval, designed specifically for our procedure, which allows to balance a\ntrade-off between power and length of the constructed intervals, and, in fact,\noften enjoy (almost) the best of both worlds. We apply our methods to detect\nthe sign of correlations in a highly publicized social neuroscience study and,\nin a second example, to detect the direction of association for SNPs with\nType-2 Diabetes in GWAS data. In both examples we compare our procedure to\nexisting methods and obtain encouraging results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 15:38:41 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 15:09:45 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 07:01:06 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 05:43:58 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Weinstein", "Asaf", ""], ["Yekutieli", "Daniel", ""]]}, {"id": "1404.7530", "submitter": "Dean Eckles", "authors": "Dean Eckles, Brian Karrer, Johan Ugander", "title": "Design and analysis of experiments in networks: Reducing bias from\n  interference", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the effects of interventions in networks is complicated when the\nunits are interacting, such that the outcomes for one unit may depend on the\ntreatment assignment and behavior of many or all other units (i.e., there is\ninterference). When most or all units are in a single connected component, it\nis impossible to directly experimentally compare outcomes under two or more\nglobal treatment assignments since the network can only be observed under a\nsingle assignment. Familiar formalism, experimental designs, and analysis\nmethods assume the absence of these interactions, and result in biased\nestimators of causal effects of interest. While some assumptions can lead to\nunbiased estimators, these assumptions are generally unrealistic, and we focus\nthis work on realistic assumptions. Thus, in this work, we evaluate methods for\ndesigning and analyzing randomized experiments that aim to reduce this bias and\nthereby reduce overall error. In design, we consider the ability to perform\nrandom assignment to treatments that is correlated in the network, such as\nthrough graph cluster randomization. In analysis, we consider incorporating\ninformation about the treatment assignment of network neighbors. We prove\nsufficient conditions for bias reduction through both design and analysis in\nthe presence of potentially global interference. Through simulations of the\nentire process of experimentation in networks, we measure the performance of\nthese methods under varied network structure and varied social behaviors,\nfinding substantial bias and error reductions. These improvements are largest\nfor networks with more clustering and data generating processes with both\nstronger direct effects of the treatment and stronger interactions between\nunits.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 20:56:23 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2014 23:03:24 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Eckles", "Dean", ""], ["Karrer", "Brian", ""], ["Ugander", "Johan", ""]]}, {"id": "1404.7547", "submitter": "Fang Han", "authors": "Fang Han, Huitong Qiu, Han Liu, Brian Caffo", "title": "On the Impact of Dimension Reduction on Graphical Structures", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians and quantitative neuroscientists have actively promoted the use\nof independence relationships for investigating brain networks, genomic\nnetworks, and other measurement technologies. Estimation of these graphs\ndepends on two steps. First is a feature extraction by summarizing measurements\nwithin a parcellation, regional or set definition to create nodes. Secondly,\nthese summaries are then used to create a graph representing relationships of\ninterest. In this manuscript we study the impact of dimension reduction on\ngraphs that describe different notions of relations among a set of random\nvariables. We are particularly interested in undirected graphs that capture the\nrandom variables' independence and conditional independence relations. A\ndimension reduction procedure can be any mapping from high dimensional spaces\nto low dimensional spaces. We exploit a general framework for modeling the raw\ndata and advocate that in estimating the undirected graphs, any acceptable\ndimension reduction procedure should be a graph-homotopic mapping, i.e., the\ngraphical structure of the data after dimension reduction should inherit the\nmain characteristics of the graphical structure of the raw data. We show that,\nin terms of inferring undirected graphs that characterize the conditional\nindependence relations among random variables, many dimension reduction\nprocedures, such as the mean, median, or principal components, cannot be\ntheoretically guaranteed to be a graph-homotopic mapping. The implications of\nthis work are broad. In the most charitable setting for researchers, where the\ncorrect node definition is known, graphical relationships can be contaminated\nmerely via the dimension reduction. The manuscript ends with a concrete\nexample, characterizing a subset of graphical structures such that the\ndimension reduction procedure using the principal components can be a\ngraph-homotopic mapping.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 22:07:00 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 21:47:56 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Han", "Fang", ""], ["Qiu", "Huitong", ""], ["Liu", "Han", ""], ["Caffo", "Brian", ""]]}, {"id": "1404.7555", "submitter": "Todd Oliver", "authors": "Todd A. Oliver, Gabriel Terejanu, Christopher S. Simmons, and Robert\n  D. Moser", "title": "Validating Predictions of Unobserved Quantities", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2014.08.023", "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate purpose of most computational models is to make predictions,\ncommonly in support of some decision-making process (e.g., for design or\noperation of some system). The quantities that need to be predicted (the\nquantities of interest or QoIs) are generally not experimentally observable\nbefore the prediction, since otherwise no prediction would be needed. Assessing\nthe validity of such extrapolative predictions, which is critical to informed\ndecision-making, is challenging. In classical approaches to validation, model\noutputs for observed quantities are compared to observations to determine if\nthey are consistent. By itself, this consistency only ensures that the model\ncan predict the observed quantities under the conditions of the observations.\nThis limitation dramatically reduces the utility of the validation effort for\ndecision making because it implies nothing about predictions of unobserved QoIs\nor for scenarios outside of the range of observations. However, there is no\nagreement in the scientific community today regarding best practices for\nvalidation of extrapolative predictions made using computational models. The\npurpose of this paper is to propose and explore a validation and predictive\nassessment process that supports extrapolative predictions for models with\nknown sources of error. The process includes stochastic modeling, calibration,\nvalidation, and predictive assessment phases where representations of known\nsources of uncertainty and error are built, informed, and tested. The proposed\nmethodology is applied to an illustrative extrapolation problem involving a\nmisspecified nonlinear oscillator.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 23:42:38 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Oliver", "Todd A.", ""], ["Terejanu", "Gabriel", ""], ["Simmons", "Christopher S.", ""], ["Moser", "Robert D.", ""]]}, {"id": "1404.7595", "submitter": "Malka Gorfine", "authors": "Malka Gorfine, Yair Goldberg, Yaacov Ritov", "title": "A Quantile Regression Model for Failure-Time Data with Time-Dependent\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since survival data occur over time, often important covariates that we wish\nto consider also change over time. Such covariates are referred as\ntime-dependent covariates. Quantile regression offers flexible modeling of\nsurvival data by allowing the covariates to vary with quantiles. This paper\nprovides a novel quantile regression model accommodating time-dependent\ncovariates, for analyzing survival data subject to right censoring. Our simple\nestimation technique assumes the existence of instrumental variables. In\naddition, we present a doubly-robust estimator in the sense of Robins and\nRotnitzky (1992). The asymptotic properties of the estimators are rigorously\nstudied. Finite-sample properties are demonstrated by a simulation study. The\nutility of the proposed methodology is demonstrated using the Stanford heart\ntransplant dataset.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 05:43:03 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Gorfine", "Malka", ""], ["Goldberg", "Yair", ""], ["Ritov", "Yaacov", ""]]}, {"id": "1404.7683", "submitter": "Anestis Touloumis", "authors": "Anestis Touloumis, Simon Tavar\\'e and John C. Marioni", "title": "Testing the Mean Matrix in High-Dimensional Transposable Data", "comments": "in Biometrics, 2015", "journal-ref": "Biometrics 71 (2015), pp. 157--166", "doi": "10.1111/biom.12257", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structural information in high-dimensional transposable data allows us to\nwrite the data recorded for each subject in a matrix such that both the rows\nand the columns correspond to variables of interest. One important problem is\nto test the null hypothesis that the mean matrix has a particular structure\nwithout ignoring the potential dependence structure among and/or between the\nrow and column variables. To address this, we develop a simple and\ncomputationally efficient nonparametric testing procedure to assess the\nhypothesis that, in each predefined subset of columns (rows), the column (row)\nmean vector remains constant. In simulation studies, the proposed testing\nprocedure seems to have good performance and unlike traditional approaches, it\nis powerful without leading to inflated nominal sizes. Finally, we illustrate\nthe use of the proposed methodology via two empirical examples from gene\nexpression microarrays.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 11:14:40 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2014 13:00:36 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 12:54:07 GMT"}, {"version": "v4", "created": "Tue, 10 Feb 2015 10:31:40 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Touloumis", "Anestis", ""], ["Tavar\u00e9", "Simon", ""], ["Marioni", "John C.", ""]]}, {"id": "1404.7684", "submitter": "Anestis Touloumis", "authors": "Anestis Touloumis and John Marioni and Simon Tavar\\'e", "title": "Hypothesis Testing for the Covariance Matrix in High-Dimensional\n  Transposable Data with Kronecker Product Dependence Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix-variate normal distribution is a popular model for\nhigh-dimensional transposable data because it decomposes the dependence\nstructure of the random matrix into the Kronecker product of two covariance\nmatrices: one for each of the row and column variables. We develop tests for\nassessing the form of the row (column) covariance matrix in high-dimensional\nsettings while treating the column (row) dependence structure as a nuisance.\nOur tests are robust to normality departures provided that the Kronecker\nproduct dependence structure holds. In simulations, we observe that the\nproposed tests maintain the nominal level and are powerful against the\nalternative hypotheses tested. We illustrate the utility of our approach by\nexamining whether genes associated with a given signalling network show\ncorrelated patterns of expression in different tissues and by studying\ncorrelation patterns within measurements of brain activity collected using\nelectroencephalography.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 11:15:22 GMT"}, {"version": "v2", "created": "Sat, 8 Nov 2014 15:18:49 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Touloumis", "Anestis", ""], ["Marioni", "John", ""], ["Tavar\u00e9", "Simon", ""]]}, {"id": "1404.7844", "submitter": "Adam Kapelner", "authors": "Adam Kapelner, Justin Bleich, Alina Levine, Zachary D. Cohen, Robert\n  J. DeRubeis and Richard Berk", "title": "Evaluating the Effectiveness of Personalized Medicine with Software", "comments": "36 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.3389/fdata.2021.572532", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methodological advances in understanding the effectiveness of\npersonalized medicine models and supply easy-to-use open-source software.\nPersonalized medicine involves the systematic use of individual patient\ncharacteristics to determine which treatment option is most likely to result in\na better outcome for the patient on average. Why is personalized medicine not\ndone more in practice? One of many reasons is because practitioners do not have\nany easy way to holistically evaluate whether their personalization procedure\ndoes better than the standard of care. Our software, \"Personalized Treatment\nEvaluator\" (the R package PTE), provides inference for improvement\nout-of-sample in many clinical scenarios. We also extend current methodology by\nallowing evaluation of improvement in the case where the endpoint is binary or\nsurvival. In the software, the practitioner inputs (1) data from a single-stage\nrandomized trial with one continuous, incidence or survival endpoint and (2) a\nfunctional form of a model for the endpoint constructed from domain knowledge.\nThe bootstrap is then employed on data unseen during model fitting to provide\nconfidence intervals for the improvement for the average future patient\n(assuming future patients are similar to the patients in the trial). One may\nalso test against a null scenario where the hypothesized personalization are\nnot more useful than a standard of care. We demonstrate our method's promise on\nsimulated data as well as on data from a randomized comparative trial\ninvestigating two treatments for depression.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 19:30:44 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 02:20:23 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 02:46:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kapelner", "Adam", ""], ["Bleich", "Justin", ""], ["Levine", "Alina", ""], ["Cohen", "Zachary D.", ""], ["DeRubeis", "Robert J.", ""], ["Berk", "Richard", ""]]}]