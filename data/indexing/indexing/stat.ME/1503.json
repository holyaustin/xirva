[{"id": "1503.00163", "submitter": "Matteo Ruggiero", "authors": "P. De Blasi, S. Favaro, A. Lijoi, R.H. Mena, I. Pruenster and M.\n  Ruggiero", "title": "Are Gibbs-type priors the most natural generalization of the Dirichlet\n  process?", "comments": "5 figures", "journal-ref": "IEEE Transactions Pattern Analysis and Machine Intelligence 2015,\n  Vol. 37, No. 2, pp. 212-229", "doi": "10.1109/TPAMI.2013.217", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete random probability measures and the exchangeable random partitions\nthey induce are key tools for addressing a variety of estimation and prediction\nproblems in Bayesian inference. Indeed, many popular nonparametric priors, such\nas the Dirichlet and the Pitman-Yor process priors, select discrete probability\ndistributions almost surely and, therefore, automatically induce exchangeable\nrandom partitions. Here we focus on the family of Gibbs-type priors, a recent\nand elegant generalization of the Dirichlet and the Pitman-Yor process priors.\nThese random probability measures share properties that are appealing both from\na theoretical and an applied point of view: (i) they admit an intuitive\ncharacterization in terms of their predictive structure justifying their use in\nterms of a precise assumption on the learning mechanism; (ii) they stand out in\nterms of mathematical tractability; (iii) they include several interesting\nspecial cases besides the Dirichlet and the Pitman-Yor processes. The goal of\nour paper is to provide a systematic and unified treatment of Gibbs-type priors\nand highlight their implications for Bayesian nonparametric inference. We will\ndeal with their distributional properties, the resulting estimators,\nfrequentist asymptotic validation and the construction of time-dependent\nversions. Applications, mainly concerning hierarchical mixture models and\nspecies sampling, will serve to convey the main ideas. The intuition inherent\nto this class of priors and the neat results that can be deduced for it lead\none to wonder whether it actually represents the most natural generalization of\nthe Dirichlet process.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 18:28:27 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["De Blasi", "P.", ""], ["Favaro", "S.", ""], ["Lijoi", "A.", ""], ["Mena", "R. H.", ""], ["Pruenster", "I.", ""], ["Ruggiero", "M.", ""]]}, {"id": "1503.00256", "submitter": "Geir-Arne Fuglstad", "authors": "Geir-Arne Fuglstad, Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Constructing Priors that Penalize the Complexity of Gaussian Random\n  Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priors are important for achieving proper posteriors with physically\nmeaningful covariance structures for Gaussian random fields (GRFs) since the\nlikelihood typically only provides limited information about the covariance\nstructure under in-fill asymptotics. We extend the recent Penalised Complexity\nprior framework and develop a principled joint prior for the range and the\nmarginal variance of one-dimensional, two-dimensional and three-dimensional\nMat\\'ern GRFs with fixed smoothness. The prior is weakly informative and\npenalises complexity by shrinking the range towards infinity and the marginal\nvariance towards zero. We propose guidelines for selecting the hyperparameters,\nand a simulation study shows that the new prior provides a principled\nalternative to reference priors that can leverage prior knowledge to achieve\nshorter credible intervals while maintaining good coverage.\n  We extend the prior to a non-stationary GRF parametrized through local ranges\nand marginal standard deviations, and introduce a scheme for selecting the\nhyperparameters based on the coverage of the parameters when fitting simulated\nstationary data. The approach is applied to a dataset of annual precipitation\nin southern Norway and the scheme for selecting the hyperparameters leads to\nconcervative estimates of non-stationarity and improved predictive performance\nover the stationary model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 11:49:36 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 21:22:10 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 11:05:01 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 18:33:34 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Fuglstad", "Geir-Arne", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1503.00269", "submitter": "Marco Loog", "authors": "Marco Loog", "title": "Contrastive Pessimistic Likelihood Estimation for Semi-Supervised\n  Classification", "comments": "32 pages, minor revision submitted to TPAMI, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvement guarantees for semi-supervised classifiers can currently only be\ngiven under restrictive conditions on the data. We propose a general way to\nperform semi-supervised parameter estimation for likelihood-based classifiers\nfor which, on the full training set, the estimates are never worse than the\nsupervised solution in terms of the log-likelihood. We argue, moreover, that we\nmay expect these solutions to really improve upon the supervised classifier in\nparticular cases. In a worked-out example for LDA, we take it one step further\nand essentially prove that its semi-supervised version is strictly better than\nits supervised counterpart. The two new concepts that form the core of our\nestimation principle are contrast and pessimism. The former refers to the fact\nthat our objective function takes the supervised estimates into account,\nenabling the semi-supervised solution to explicitly control the potential\nimprovements over this estimate. The latter refers to the fact that our\nestimates are conservative and therefore resilient to whatever form the true\nlabeling of the unlabeled data takes on. Experiments demonstrate the\nimprovements in terms of both the log-likelihood and the classification error\nrate on independent test sets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 13:16:43 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 21:36:53 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Loog", "Marco", ""]]}, {"id": "1503.00334", "submitter": "Stephen Reid", "authors": "Stephen Reid and Robert Tibshirani", "title": "Sparse regression and marginal testing using cluster prototypes", "comments": "43 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for sparse regression and marginal testing, for\ndata with correlated features. Our procedure first clusters the features, and\nthen chooses as the cluster prototype the most informative feature in that\ncluster. Then we apply either sparse regression (lasso) or marginal\nsignificance testing to these prototypes. While this kind of strategy is not\nentirely new, a key feature of our proposal is its use of the post-selection\ninference theory of Taylor et al. (2014) and Lee et al. (2014) to compute exact\np-values and confidence intervals that properly account for the selection of\nprototypes.\n  We also apply the recent \"knockoff\" idea of Barber and Cand\\`es to provide\nexact finite sample control of the FDR of our regression procedure. We\nillustrate our proposals on both real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 19:11:03 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 19:12:31 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Reid", "Stephen", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1503.00357", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster", "title": "Consistency of Importance Sampling estimates based on dependent sample\n  sets and an application to models with factorizing likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, I proof that Importance Sampling estimates based on dependent\nsample sets are consistent under certain conditions. This can be used to reduce\nvariance in Bayesian Models with factorizing likelihoods, using sample sets\nthat are much larger than the number of likelihood evaluations, a technique\ndubbed Sample Inflation. I evaluate Sample Inflation on a toy Gaussian problem\nand two Mixture Models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 21:38:49 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Schuster", "Ingmar", ""]]}, {"id": "1503.00445", "submitter": "Vincent A Traag", "authors": "V.A. Traag, R. Aldecoa, J-C. Delvenne", "title": "Detecting communities using asymptotical Surprise", "comments": null, "journal-ref": "Phys. Rev. E 92, 022816 (2015)", "doi": "10.1103/PhysRevE.92.022816", "report-no": null, "categories": "physics.soc-ph cs.DM cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nodes in real-world networks are repeatedly observed to form dense clusters,\noften referred to as communities. Methods to detect these groups of nodes\nusually maximize an objective function, which implicitly contains the\ndefinition of a community. We here analyze a recently proposed measure called\nsurprise, which assesses the quality of the partition of a network into\ncommunities. In its current form, the formulation of surprise is rather\ndifficult to analyze. We here therefore develop an accurate asymptotic\napproximation. This allows for the development of an efficient algorithm for\noptimizing surprise. Incidentally, this leads to a straightforward extension of\nsurprise to weighted graphs. Additionally, the approximation makes it possible\nto analyze surprise more closely and compare it to other methods, especially\nmodularity. We show that surprise is (nearly) unaffected by the well known\nresolution limit, a particular problem for modularity. However, surprise may\ntend to overestimate the number of communities, whereas they may be\nunderestimated by modularity. In short, surprise works well in the limit of\nmany small communities, whereas modularity works better in the limit of few\nlarge communities. In this sense, surprise is more discriminative than\nmodularity, and may find communities where modularity fails to discern any\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 09:02:53 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 09:31:32 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Traag", "V. A.", ""], ["Aldecoa", "R.", ""], ["Delvenne", "J-C.", ""]]}, {"id": "1503.00463", "submitter": "Xing He", "authors": "Xing He, Qian Ai, Robert C.qiu, Jianmo Ni, Longjian Piao, Yiting Xu,\n  Xinyi Xu", "title": "3D Power-map for Smart Grids---An Integration of High-dimensional\n  Analysis and Visualization", "comments": "5 pages, 7 figures, submitted to PESGM 2015. arXiv admin note:\n  substantial text overlap with arXiv:1502.00060", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data with features of volume, velocity, variety, and veracity are challenging\ntraditional tools to extract useful analysis for decision-making. By\nintegrating high-dimensional analysis with visualization, this paper develops a\n3D power-map animation as an effective solution to the challenge. An\narchitecture design, with detailed data processing procedure, is proposed to\nrealize the integration. Two of the most important components in the\narchitecture are presented: the Single-Ring Law for random matrices as solid\nmathematic foundation, and the proposed statistical index MSR as\nhigh-dimensional data for visualization. The whole procedure is easy in logic,\nfast in speed, objective and even robust against bad data. Moreover, it is an\nunsupervised machine learning mechanism directly oriented to the raw data\nrather than logics or models based on simplifications and assumptions. A case\nstudy validates the effectiveness and performance of the developed 3D power-map\nin analysis extraction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 10:03:38 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["He", "Xing", ""], ["Ai", "Qian", ""], ["qiu", "Robert C.", ""], ["Ni", "Jianmo", ""], ["Piao", "Longjian", ""], ["Xu", "Yiting", ""], ["Xu", "Xinyi", ""]]}, {"id": "1503.00466", "submitter": "Jakub Chorowski", "authors": "Jakub Chorowski and Mathias Trabs", "title": "Spectral estimation for diffusions with random sampling times", "comments": "30 pages, 2 figures", "journal-ref": "Stochastic Processes and their Applications, 126 (10), 2976-3008,\n  2016", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonparametric estimation of the volatility and the drift coefficient of a\nscalar diffusion is studied when the process is observed at random time points.\nThe constructed estimator generalizes the spectral method by Gobet, Hoffmann\nand Rei{\\ss} [Ann. Statist. 32 (2006), 2223-2253]. The estimation procedure is\noptimal in the minimax sense and adaptive with respect to the sampling time\ndistribution and the regularity of the coefficients. The proofs are based on\nthe eigenvalue problem for the generalized transition operator. The finite\nsample performance is illustrated in a numerical example.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 10:13:37 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 14:39:59 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2015 18:13:13 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Chorowski", "Jakub", ""], ["Trabs", "Mathias", ""]]}, {"id": "1503.00643", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "Two samples test for discrete power-law distributions", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Power-law distributions occur in wide variety of physical, biological, and\nsocial phenomena. In this paper, we propose a statistical hypothesis test based\non the log-likelihood ratio to assess whether two samples of discrete data are\ndrawn from the same power-law distribution.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 17:59:39 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1503.00890", "submitter": "C\\'ecile Proust-Lima", "authors": "C\\'ecile Proust-Lima, Viviane Philipps, Benoit Liquet", "title": "Estimation of extended mixed models using latent classes and latent\n  processes: the R package lcmm", "comments": null, "journal-ref": "Journal of Statistical Software (2017), 78(2), 1-56", "doi": "10.18637/jss.v078.i02", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package lcmm provides a series of functions to estimate statistical\nmodels based on linear mixed model theory. It includes the estimation of mixed\nmodels and latent class mixed models for Gaussian longitudinal outcomes (hlme),\ncurvilinear and ordinal univariate longitudinal outcomes (lcmm) and curvilinear\nmultivariate outcomes (multlcmm), as well as joint latent class mixed models\n(Jointlcmm) for a (Gaussian or curvilinear) longitudinal outcome and a\ntime-to-event that can be possibly left-truncated right-censored and defined in\na competing setting. Maximum likelihood esimators are obtained using a modified\nMarquardt algorithm with strict convergence criteria based on the parameters\nand likelihood stability, and on the negativity of the second derivatives. The\npackage also provides various post-fit functions including goodness-of-fit\nanalyses, classification, plots, predicted trajectories, individual dynamic\nprediction of the event and predictive accuracy assessment. This paper\nconstitutes a companion paper to the package by introducing each family of\nmodels, the estimation technique, some implementation details and giving\nexamples through a dataset on cognitive aging.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 10:47:08 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 13:01:19 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Proust-Lima", "C\u00e9cile", ""], ["Philipps", "Viviane", ""], ["Liquet", "Benoit", ""]]}, {"id": "1503.00912", "submitter": "Noel Erp van", "authors": "H.R.N. van Erp, R.O. Linger, and P.H.A.J.M. van Gelder", "title": "Exploring Beta-Like Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most well known probability distribution of probabilities is the Beta\ndistribution. If we have observed $r$ `successes', each having a probability\n$\\theta$, and $n-r$ `failures', each having a probability $1-\\theta$. In this\npaper we will derive a whole family of Beta-like distributions, which take as\ntheir data not only the number of successes and failures, but also values on\npredictor variables and time to failure or time without failure.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 12:09:23 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["van Erp", "H. R. N.", ""], ["Linger", "R. O.", ""], ["van Gelder", "P. H. A. J. M.", ""]]}, {"id": "1503.00982", "submitter": "Jonathan R. Bradley", "authors": "Jonathan R. Bradley, Scott H. Holan, Christopher K. Wikle", "title": "Multivariate spatio-temporal models for high-dimensional areal data with\n  application to Longitudinal Employer-Household Dynamics", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS862 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). arXiv admin note:\n  substantial text overlap with arXiv:1407.7479", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1761-1791", "doi": "10.1214/15-AOAS862", "report-no": "IMS-AOAS-AOAS862", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sources report related variables of interest that are also\nreferenced over geographic regions and time; however, there are relatively few\ngeneral statistical methods that one can readily use that incorporate these\nmultivariate spatio-temporal dependencies. Additionally, many multivariate\nspatio-temporal areal data sets are extremely high dimensional, which leads to\npractical issues when formulating statistical models. For example, we analyze\nQuarterly Workforce Indicators (QWI) published by the US Census Bureau's\nLongitudinal Employer-Household Dynamics (LEHD) program. QWIs are available by\ndifferent variables, regions, and time points, resulting in millions of\ntabulations. Despite their already expansive coverage, by adopting a fully\nBayesian framework, the scope of the QWIs can be extended to provide estimates\nof missing values along with associated measures of uncertainty. Motivated by\nthe LEHD, and other applications in federal statistics, we introduce the\nmultivariate spatio-temporal mixed effects model (MSTM), which can be used to\nefficiently model high-dimensional multivariate spatio-temporal areal data\nsets. The proposed MSTM extends the notion of Moran's I basis functions to the\nmultivariate spatio-temporal setting. This extension leads to several\nmethodological contributions, including extremely effective dimension\nreduction, a dynamic linear model for multivariate spatio-temporal areal\nprocesses, and the reduction of a high-dimensional parameter space using a\nnovel parameter model.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 15:44:56 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 08:53:57 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1503.00994", "submitter": "Nirian Mart\\'in", "authors": "Angel Felipe, Nirian Mart\\'in, Pedro Miranda, Leandro Pardo", "title": "Empirical phi-divergence test statistics for testing simple null\n  hypotheses based on exponentially tilted empirical likelihood estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Econometrics, imposing restrictions without assuming underlying\ndistributions to modelize complex realities is a valuable methodological tool.\nHowever, if a subset of restrictions were not correctly specified, the usual\ntest-statistics for correctly specified models tend to reject erronously a\nsimple null hypothesis. In this setting, we may say that the model suffers from\nmisspecification. We study the behavior of empirical phi-divergence\ntest-statistics, introduced in Balakrishnan et al. (2015), by using the\nexponential tilted empirical likelihood estimators of Schennach (2007), as a\ngood compromise between efficiency of the significance level for small sample\nsizes and robustness under misspecification.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 16:23:01 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 21:27:45 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Felipe", "Angel", ""], ["Mart\u00edn", "Nirian", ""], ["Miranda", "Pedro", ""], ["Pardo", "Leandro", ""]]}, {"id": "1503.01249", "submitter": "Silvia Bianconcini", "authors": "Silvia Bianconcini, Silvia Cagnone and Dimitris Rizopoulos", "title": "Approximate likelihood inference in generalized linear latent variable\n  models based on integral dimension reduction", "comments": "28 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models represent a useful tool for the analysis of complex\ndata when the constructs of interest are not observable. A problem related to\nthese models is that the integrals involved in the likelihood function cannot\nbe solved analytically. We propose a computational approach, referred to as\nDimension Reduction Method (DRM), that consists of a dimension reduction of the\nmultidimensional integral that makes the computation feasible in situations in\nwhich the quadrature based methods are not applicable. We discuss the\nadvantages of DRM compared with other existing approximation procedures in\nterms of both computational feasibility of the method and asymptotic properties\nof the resulting estimators.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 08:09:15 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Bianconcini", "Silvia", ""], ["Cagnone", "Silvia", ""], ["Rizopoulos", "Dimitris", ""]]}, {"id": "1503.01401", "submitter": "Kyle Hickmann", "authors": "Kyle S. Hickmann, James M. Hyman, Sara Y. Del Valle", "title": "Quantifying Uncertainty in Stochastic Models with Parametric Variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to quantify uncertainty in the predictions made by\nsimulations of mathematical models that can be applied to a broad class of\nstochastic, discrete, and differential equation models. Quantifying uncertainty\nis crucial for determining how accurate the model predictions are and\nidentifying which input parameters affect the outputs of interest. Most of the\nexisting methods for uncertainty quantification require many samples to\ngenerate accurate results, are unable to differentiate where the uncertainty is\ncoming from (e.g., parameters or model assumptions), or require a lot of\ncomputational resources. Our approach addresses these challenges and\nopportunities by allowing different types of uncertainty, that is, uncertainty\nin input parameters as well as uncertainty created through stochastic model\ncomponents. This is done by combining the Karhunen-Loeve decomposition,\npolynomial chaos expansion, and Bayesian Gaussian process regression to create\na statistical surrogate for the stochastic model. The surrogate separates the\nanalysis of variation arising through stochastic simulation and variation\narising through uncertainty in the model parameterization. We illustrate our\napproach by quantifying the uncertainty in a stochastic ordinary differential\nequation epidemic model. Specifically, we estimate four quantities of interest\nfor the epidemic model and show agreement between the surrogate and the actual\nmodel results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 18:03:16 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Hickmann", "Kyle S.", ""], ["Hyman", "James M.", ""], ["Del Valle", "Sara Y.", ""]]}, {"id": "1503.01577", "submitter": "Tyler J. VanderWeele", "authors": "Tyler J. VanderWeele, Eric J. Tchetgen Tchetgen, M. Elizabeth Halloran", "title": "Interference and Sensitivity Analysis", "comments": "Published in at http://dx.doi.org/10.1214/14-STS479 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 687-706", "doi": "10.1214/14-STS479", "report-no": "IMS-STS-STS479", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference with interference is a rapidly growing area. The literature\nhas begun to relax the \"no-interference\" assumption that the treatment received\nby one individual does not affect the outcomes of other individuals. In this\npaper we briefly review the literature on causal inference in the presence of\ninterference when treatments have been randomized. We then consider settings in\nwhich causal effects in the presence of interference are not identified, either\nbecause randomization alone does not suffice for identification or because\ntreatment is not randomized and there may be unmeasured confounders of the\ntreatment-outcome relationship. We develop sensitivity analysis techniques for\nthese settings. We describe several sensitivity analysis techniques for the\ninfectiousness effect which, in a vaccine trial, captures the effect of the\nvaccine of one person on protecting a second person from infection even if the\nfirst is infected. We also develop two sensitivity analysis techniques for\ncausal effects under interference in the presence of unmeasured confounding\nwhich generalize analogous techniques when interference is absent. These two\ntechniques for unmeasured confounding are compared and contrasted.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 08:53:00 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["VanderWeele", "Tyler J.", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["Halloran", "M. Elizabeth", ""]]}, {"id": "1503.01589", "submitter": "Stijn Vansteelandt", "authors": "Stijn Vansteelandt, Marshall Joffe", "title": "Structural Nested Models and G-estimation: The Partially Realized\n  Promise", "comments": "Published in at http://dx.doi.org/10.1214/14-STS493 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 707-731", "doi": "10.1214/14-STS493", "report-no": "IMS-STS-STS493", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural nested models (SNMs) and the associated method of G-estimation\nwere first proposed by James Robins over two decades ago as approaches to\nmodeling and estimating the joint effects of a sequence of treatments or\nexposures. The models and estimation methods have since been extended to\ndealing with a broader series of problems, and have considerable advantages\nover the other methods developed for estimating such joint effects. Despite\nthese advantages, the application of these methods in applied research has been\nrelatively infrequent; we view this as unfortunate. To remedy this, we provide\nan overview of the models and estimation methods as developed, primarily by\nRobins, over the years. We provide insight into their advantages over other\nmethods, and consider some possible reasons for failure of the methods to be\nmore broadly adopted, as well as possible remedies. Finally, we consider\nseveral extensions of the standard models and estimation methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 09:50:02 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Vansteelandt", "Stijn", ""], ["Joffe", "Marshall", ""]]}, {"id": "1503.01598", "submitter": "Amy Richardson", "authors": "Amy Richardson, Michael G. Hudgens, Peter B. Gilbert, Jason P. Fine", "title": "Nonparametric Bounds and Sensitivity Analysis of Treatment Effects", "comments": "Published in at http://dx.doi.org/10.1214/14-STS499 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 596-618", "doi": "10.1214/14-STS499", "report-no": "IMS-STS-STS499", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers conducting inference about the effect of a treatment (or\nexposure) on an outcome of interest. In the ideal setting where treatment is\nassigned randomly, under certain assumptions the treatment effect is\nidentifiable from the observable data and inference is straightforward.\nHowever, in other settings such as observational studies or randomized trials\nwith noncompliance, the treatment effect is no longer identifiable without\nrelying on untestable assumptions. Nonetheless, the observable data often do\nprovide some information about the effect of treatment, that is, the parameter\nof interest is partially identifiable. Two approaches are often employed in\nthis setting: (i) bounds are derived for the treatment effect under minimal\nassumptions, or (ii) additional untestable assumptions are invoked that render\nthe treatment effect identifiable and then sensitivity analysis is conducted to\nassess how inference about the treatment effect changes as the untestable\nassumptions are varied. Approaches (i) and (ii) are considered in various\nsettings, including assessing principal strata effects, direct and indirect\neffects and effects of time-varying exposures. Methods for drawing formal\ninference about partially identified parameters are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 10:23:12 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Richardson", "Amy", ""], ["Hudgens", "Michael G.", ""], ["Gilbert", "Peter B.", ""], ["Fine", "Jason P.", ""]]}, {"id": "1503.01603", "submitter": "Judea Pearl", "authors": "Judea Pearl, Elias Bareinboim", "title": "External Validity: From Do-Calculus to Transportability Across\n  Populations", "comments": "Published in at http://dx.doi.org/10.1214/14-STS486 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). arXiv admin note: text overlap with\n  arXiv:1312.7485", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 579-595", "doi": "10.1214/14-STS486", "report-no": "IMS-STS-STS486", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalizability of empirical findings to new environments, settings or\npopulations, often called \"external validity,\" is essential in most scientific\nexplorations. This paper treats a particular problem of generalizability,\ncalled \"transportability,\" defined as a license to transfer causal effects\nlearned in experimental studies to a new population, in which only\nobservational studies can be conducted. We introduce a formal representation\ncalled \"selection diagrams\" for expressing knowledge about differences and\ncommonalities between populations of interest and, using this representation,\nwe reduce questions of transportability to symbolic derivations in the\ndo-calculus. This reduction yields graph-based procedures for deciding, prior\nto observing any data, whether causal effects in the target population can be\ninferred from experimental findings in the study population. When the answer is\naffirmative, the procedures identify what experimental and observational\nfindings need be obtained from the two populations, and how they can be\ncombined to ensure bias-free transport.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 10:58:30 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Pearl", "Judea", ""], ["Bareinboim", "Elias", ""]]}, {"id": "1503.01940", "submitter": "Ivi C. Tsantili", "authors": "Dionissios T. Hristopulos, Ivi C.Tsantili", "title": "Space-Time Models based on Random Fields with Local Interactions", "comments": "29 pages, 7 figures, reprint of an article submitted for\n  consideration in International Journal of Modern Physics B \\copyright 2015,\n  copyright World Scientific Publishing Company,\n  http:/www.worldscientific.com/worldscinet/ijmpb", "journal-ref": "International Journal of Modern Physics B, 2014, id 1541007", "doi": "10.1142/S0217979215410076", "report-no": null, "categories": "stat.ME cond-mat.stat-mech math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of space-time data from complex, real-life phenomena requires\nthe use of flexible and physically motivated covariance functions. In most\ncases, it is not possible to explicitly solve the equations of motion for the\nfields or the respective covariance functions. In the statistical literature,\ncovariance functions are often based on mathematical constructions. We propose\nderiving space-time covariance functions by solving \"effective equations of\nmotion\", which can be used as statistical representations of systems with\ndiffusive behavior. In particular, we propose using the linear response theory\nto formulate space-time covariance functions based on an equilibrium effective\nHamiltonian. The effective space-time dynamics are then generated by a\nstochastic perturbation around the equilibrium point of the classical field\nHamiltonian leading to an associated Langevin equation. We employ a Hamiltonian\nwhich extends the classical Gaussian field theory by including a curvature term\nand leads to a diffusive Langevin equation. Finally, we derive new forms of\nspace-time covariance functions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 13:05:02 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Hristopulos", "Dionissios T.", ""], ["Tsantili", "Ivi C.", ""]]}, {"id": "1503.02059", "submitter": "Christian Hennig", "authors": "Christian Hennig", "title": "Clustering strategy and method selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a chapter in the forthcoming Handbook of Cluster Analysis,\nHennig et al. (2015). For definitions of basic clustering methods and some\nfurther methodology, other chapters of the Handbook are referred to. To read\nthis version of the paper without the Handbook, some knowledge of cluster\nanalysis methodology is required.\n  The aim of this chapter is to provide a framework for all the decisions that\nare required when carrying out a cluster analysis in practice. A general\nattitude to clustering is outlined, which connects these decisions closely to\nthe clustering aims in a given application. From this point of view, the\nchapter then discusses aspects of data processing such as the choice of the\nrepresentation of the objects to be clustered, dissimilarity design,\ntransformation and standardization of variables. Regarding the choice of the\nclustering method, it is explored how different methods correspond to different\nclustering aims. Then an overview of benchmarking studies comparing different\nclustering methods is given, as well as an out- line of theoretical approaches\nto characterize desiderata for clustering by axioms. Finally, aspects of\ncluster validation, i.e., the assessment of the quality of a clustering in a\ngiven dataset, are discussed, including finding an appropriate number of\nclusters, testing homogeneity, internal and external cluster validation,\nassessing clustering stability and data visualization.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 20:34:42 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Hennig", "Christian", ""]]}, {"id": "1503.02098", "submitter": "Adam Loy", "authors": "Adam Loy, Lendie Follett, Heike Hofmann", "title": "Variations of Q-Q Plots -- The Power of our Eyes!", "comments": "21 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical modeling we strive to specify models that resemble data\ncollected in studies or observed from processes. Consequently, distributional\nspecification and parameter estimation are central to parametric models.\nGraphical procedures, such as the quantile-quantile (Q-Q) plot, are arguably\nthe most widely used method of distributional assessment, though critics find\ntheir interpretation to be overly subjective. Formal goodness-of-fit tests are\navailable and are quite powerful, but only indicate whether there is a lack of\nfit, not why there is lack of fit. In this paper we explore the use of the\nlineup protocol to inject rigor to graphical distributional assessment and\ncompare its power to that of formal distributional tests. We find that lineups\nof standard Q-Q plots are more powerful than lineups of de-trended Q-Q plots\nand that lineup tests are more powerful than traditional tests of normality.\nWhile, we focus on diagnosing non-normality, our approach is general and can be\ndirectly extended to the assessment of other distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 21:55:10 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Loy", "Adam", ""], ["Follett", "Lendie", ""], ["Hofmann", "Heike", ""]]}, {"id": "1503.02220", "submitter": "Zachary Fisher", "authors": "Zachary Fisher and Elizabeth Tipton", "title": "robumeta: An R-package for robust variance estimation in meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-regression models are commonly used to synthesize and compare effect\nsizes. Unfortunately, traditional meta-regression methods are ill-equipped to\nhandle the complex and often unknown correlations among non-independent effect\nsizes. Robust variance estimation (RVE) is a recently proposed meta-analytic\nmethod for dealing with dependent effect sizes. The robumeta package provides\nfunctions for performing robust variance meta-regression using both large and\nsmall sample RVE estimators under various weighting schemes. These methods are\ndistribution free and provide valid point estimates, standard errors and\nhypothesis tests even when the degree and structure of dependence between\neffect sizes is unknown.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 22:27:19 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Fisher", "Zachary", ""], ["Tipton", "Elizabeth", ""]]}, {"id": "1503.02278", "submitter": "Ruth Heller", "authors": "Ruth Heller, Marina Bogomolov, Yoav Benjamini, and Tamar Sofer", "title": "Testing for replicability in a follow-up study when the primary study\n  hypotheses are two-sided", "comments": "arXiv admin note: text overlap with arXiv:1310.0606", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When testing for replication of results from a primary study with two-sided\nhypotheses in a follow-up study, we are usually interested in discovering the\nfeatures with discoveries in the same direction in the two studies. The\ndirection of testing in the follow-up study for each feature can therefore be\ndecided by the primary study. We prove that in this case the methods suggested\nin Heller, Bogomolov, and Benjamini (2014) for control over false replicability\nclaims are valid. Specifically, we prove that if we input into the procedures\nin Heller, Bogomolov, and Benjamini (2014) the one-sided p-values in the\ndirections favoured by the primary study, then we achieve directional control\nover the desired error measure (family-wise error rate or false discovery\nrate).\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 14:38:21 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Heller", "Ruth", ""], ["Bogomolov", "Marina", ""], ["Benjamini", "Yoav", ""], ["Sofer", "Tamar", ""]]}, {"id": "1503.02346", "submitter": "Ping Li", "authors": "Ping Li", "title": "One Scan 1-Bit Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on $\\alpha$-stable random projections with small $\\alpha$, we develop a\nsimple algorithm for compressed sensing (sparse signal recovery) by utilizing\nonly the signs (i.e., 1-bit) of the measurements. Using only 1-bit information\nof the measurements results in substantial cost reduction in collection,\nstorage, communication, and decoding for compressed sensing. The proposed\nalgorithm is efficient in that the decoding procedure requires only one scan of\nthe coordinates. Our analysis can precisely show that, for a $K$-sparse signal\nof length $N$, $12.3K\\log N/\\delta$ measurements (where $\\delta$ is the\nconfidence) would be sufficient for recovering the support and the signs of the\nsignal. While the method is very robust against typical measurement noises, we\nalso provide the analysis of the scheme under random flipping of the signs of\nthe measurements.\n  \\noindent Compared to the well-known work on 1-bit marginal regression (which\ncan also be viewed as a one-scan method), the proposed algorithm requires\norders of magnitude fewer measurements. Compared to 1-bit Iterative Hard\nThresholding (IHT) (which is not a one-scan algorithm), our method is still\nsignificantly more accurate. Furthermore, the proposed method is reasonably\nrobust against random sign flipping while IHT is known to be very sensitive to\nthis type of noise.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 23:53:04 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 17:11:29 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1503.02577", "submitter": "Helio M. de Oliveira", "authors": "G. Jer\\^onimo da Silva Jr., R.M. Campello de Souza and H.M. de\n  Oliveira", "title": "New Algorithms for Computing a Single Component of the Discrete Fourier\n  Transform", "comments": "4 pages, 3 figures, 1 table. In: 10th International Symposium on\n  Communication Theory and Applications, Ambleside, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the theory and hardware implementation of two new\nalgorithms for computing a single component of the discrete Fourier transform.\nIn terms of multiplicative complexity, both algorithms are more efficient, in\ngeneral, than the well known Goertzel Algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 17:39:44 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Silva", "G. Jer\u00f4nimo da", "Jr."], ["de Souza", "R. M. Campello", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1503.02722", "submitter": "Brian Knaeble", "authors": "Brian Knaeble and Seth Dutter", "title": "Reversals of Least-Squares Estimates and Model-Independent Estimation\n  for Directions of Unique Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a linear model is adjusted to control for additional explanatory\nvariables the sign of a fitted coefficient may reverse. Here these reversals\nare studied using coefficients of determination. The resulting theory can be\nused to determine directions of unique effects in the presence of substantial\nmodel uncertainty. This process is called model-independent estimation when the\nestimates are invariant across changes to the model structure. When a single\ncovariate is added, the reversal region can be understood geometrically as an\nelliptical cone of two nappes with an axis of symmetry relating to a\nbest-possible condition for a reversal using a single coefficient of\ndetermination. When a set of covariates are added to a model with a single\nexplanatory variable, model-independent estimation can be implemented using\nsubject matter knowledge. More general theory with partial coefficients is\napplicable to analysis of large data sets. Applications are demonstrated with\ndietary health data from the United Nations. Necessary conditions for Simpson's\nparadox are derived.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 22:52:30 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Knaeble", "Brian", ""], ["Dutter", "Seth", ""]]}, {"id": "1503.02802", "submitter": "Tomoyuki Obuchi", "authors": "Tomoyuki Obuchi, Simona Cocco, and R\\'emi Monasson", "title": "Learning probabilities from random observables in high dimensions: the\n  maximum entropy distribution and others", "comments": "30 pages, 13 figures", "journal-ref": null, "doi": "10.1007/s10955-015-1341-7", "report-no": null, "categories": "cond-mat.stat-mech stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a target probability distribution over a\nset of $N$ binary variables from the knowledge of the expectation values (with\nthis target distribution) of $M$ observables, drawn uniformly at random. The\nspace of all probability distributions compatible with these $M$ expectation\nvalues within some fixed accuracy, called version space, is studied. We\nintroduce a biased measure over the version space, which gives a boost\nincreasing exponentially with the entropy of the distributions and with an\narbitrary inverse `temperature' $\\Gamma$. The choice of $\\Gamma$ allows us to\ninterpolate smoothly between the unbiased measure over all distributions in the\nversion space ($\\Gamma=0$) and the pointwise measure concentrated at the\nmaximum entropy distribution ($\\Gamma \\to \\infty$). Using the replica method we\ncompute the volume of the version space and other quantities of interest, such\nas the distance $R$ between the target distribution and the center-of-mass\ndistribution over the version space, as functions of $\\alpha=(\\log M)/N$ and\n$\\Gamma$ for large $N$. Phase transitions at critical values of $\\alpha$ are\nfound, corresponding to qualitative improvements in the learning of the target\ndistribution and to the decrease of the distance $R$. However, for fixed\n$\\alpha$, the distance $R$ does not vary with $\\Gamma$, which means that the\nmaximum entropy distribution is not closer to the target distribution than any\nother distribution compatible with the observable values. Our results are\nconfirmed by Monte Carlo sampling of the version space for small system sizes\n($N\\le 10$).\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 08:02:06 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 09:17:34 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Obuchi", "Tomoyuki", ""], ["Cocco", "Simona", ""], ["Monasson", "R\u00e9mi", ""]]}, {"id": "1503.02834", "submitter": "Miroslav Dud\\'{i}k", "authors": "Miroslav Dud\\'ik, Dumitru Erhan, John Langford, Lihong Li", "title": "Doubly Robust Policy Evaluation and Optimization", "comments": "Published in at http://dx.doi.org/10.1214/14-STS500 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 485-511", "doi": "10.1214/14-STS500", "report-no": "IMS-STS-STS500", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sequential decision making in environments where rewards are only\npartially observed, but can be modeled as a function of observed contexts and\nthe chosen action by the decision maker. This setting, known as contextual\nbandits, encompasses a wide variety of applications such as health care,\ncontent recommendation and Internet advertising. A central task is evaluation\nof a new policy given historic data consisting of contexts, actions and\nreceived rewards. The key challenge is that the past data typically does not\nfaithfully represent proportions of actions taken by a new policy. Previous\napproaches rely either on models of rewards or models of the past policy. The\nformer are plagued by a large bias whereas the latter have a large variance. In\nthis work, we leverage the strengths and overcome the weaknesses of the two\napproaches by applying the doubly robust estimation technique to the problems\nof policy evaluation and optimization. We prove that this approach yields\naccurate value estimates when we have either a good (but not necessarily\nconsistent) model of rewards or a good (but not necessarily consistent) model\nof past policy. Extensive empirical comparison demonstrates that the doubly\nrobust estimation uniformly improves over existing techniques, achieving both\nlower variance in value estimation and better policies. As such, we expect the\ndoubly robust approach to become common practice in policy evaluation and\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 09:51:50 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Dud\u00edk", "Miroslav", ""], ["Erhan", "Dumitru", ""], ["Langford", "John", ""], ["Li", "Lihong", ""]]}, {"id": "1503.02853", "submitter": "Niels Keiding", "authors": "Niels Keiding, David Clayton", "title": "Standardization and Control for Confounding in Observational Studies: A\n  Historical Perspective", "comments": "Published in at http://dx.doi.org/10.1214/13-STS453 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 529-558", "doi": "10.1214/13-STS453", "report-no": "IMS-STS-STS453", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control for confounders in observational studies was generally handled\nthrough stratification and standardization until the 1960s. Standardization\ntypically reweights the stratum-specific rates so that exposure categories\nbecome comparable. With the development first of loglinear models, soon also of\nnonlinear regression techniques (logistic regression, failure time regression)\nthat the emerging computers could handle, regression modelling became the\npreferred approach, just as was already the case with multiple regression\nanalysis for continuous outcomes. Since the mid 1990s it has become\nincreasingly obvious that weighting methods are still often useful, sometimes\neven necessary. On this background we aim at describing the emergence of the\nmodelling approach and the refinement of the weighting approach for confounder\ncontrol.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 10:27:58 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Keiding", "Niels", ""], ["Clayton", "David", ""]]}, {"id": "1503.02894", "submitter": "Thomas S. Richardson", "authors": "Thomas S. Richardson, Andrea Rotnitzky", "title": "Causal Etiology of the Research of James M. Robins", "comments": "Published in at http://dx.doi.org/10.1214/14-STS505 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 459-484", "doi": "10.1214/14-STS505", "report-no": "IMS-STS-STS505", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This issue of Statistical Science draws its inspiration from the work of\nJames M. Robins. Jon Wellner, the Editor at the time, asked the two of us to\nedit a special issue that would highlight the research topics studied by Robins\nand the breadth and depth of Robins' contributions. Between the two of us, we\nhave collaborated closely with Jamie for nearly 40 years. We agreed to edit\nthis issue because we recognized that we were among the few in a position to\nrelate the trajectory of his research career to date.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 13:17:19 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Richardson", "Thomas S.", ""], ["Rotnitzky", "Andrea", ""]]}, {"id": "1503.02912", "submitter": "Clara Grazian", "authors": "Clara Grazian and Brunero Liseo", "title": "Approximate Bayesian inference in semiparametric copula models", "comments": "27 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple method for making inference on a functional of a\nmultivariate distribution. The method is based on a copula representation of\nthe multivariate distribution and it is based on the properties of an\nApproximate Bayesian Monte Carlo algorithm, where the proposed values of the\nfunctional of interest are weighed in terms of their empirical likelihood. This\nmethod is particularly useful when the \"true\" likelihood function associated\nwith the working model is too costly to evaluate or when the working model is\nonly partially specified.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 14:06:55 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 11:30:41 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 15:09:39 GMT"}, {"version": "v4", "created": "Sun, 16 Jul 2017 11:39:48 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Grazian", "Clara", ""], ["Liseo", "Brunero", ""]]}, {"id": "1503.03305", "submitter": "Thomas Nagler", "authors": "Thomas Nagler and Claudia Czado", "title": "Evading the curse of dimensionality in nonparametric density estimation\n  with simplified vine copulas", "comments": null, "journal-ref": "Journal of Multivariate Analysis, Volume 151, October 2016, Pages\n  69-89", "doi": "10.1016/j.jmva.2016.07.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical applications of nonparametric density estimators in more than three\ndimensions suffer a great deal from the well-known curse of dimensionality:\nconvergence slows down as dimension increases. We show that one can evade the\ncurse of dimensionality by assuming a simplified vine copula model for the\ndependence between variables. We formulate a general nonparametric estimator\nfor such a model and show under high-level assumptions that the speed of\nconvergence is independent of dimension. We further discuss a particular\nimplementation for which we validate the high-level assumptions and establish\nits asymptotic normality. Simulation experiments illustrate a large gain in\nfinite sample performance when the simplifying assumption is at least\napproximately true. But even when it is severely violated, the vine copula\nbased approach proves advantageous as soon as more than a few variables are\ninvolved. Lastly, we give an application of the estimator to a classification\nproblem from astrophysics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 12:41:25 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 21:01:23 GMT"}, {"version": "v3", "created": "Sat, 14 May 2016 17:03:50 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Nagler", "Thomas", ""], ["Czado", "Claudia", ""]]}, {"id": "1503.03381", "submitter": "Vladimir Panov", "authors": "Denis Belomestny and Vladimir Panov", "title": "Statistical inference for generalized Ornstein-Uhlenbeck processes", "comments": "32 pages. arXiv admin note: text overlap with arXiv:1312.4731", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of statistical inference for\ngeneralized Ornstein-Uhlenbeck processes of the type \\[ X_{t} = e^{-\\xi_{t}}\n\\left( X_{0} + \\int_{0}^{t} e^{\\xi_{u-}} d u \\right), \\] where \\(\\xi_s\\) is a\nL{\\'e}vy process. Our primal goal is to estimate the characteristics of the\nL\\'evy process \\(\\xi\\) from the low-frequency observations of the process\n\\(X\\). We present a novel approach towards estimating the L{\\'e}vy triplet of\n\\(\\xi,\\) which is based on the Mellin transform technique. It is shown that the\nresulting estimates attain optimal minimax convergence rates. The suggested\nalgorithms are illustrated by numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 15:42:29 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Belomestny", "Denis", ""], ["Panov", "Vladimir", ""]]}, {"id": "1503.03515", "submitter": "Art Owen", "authors": "A. B. Owen and J. Wang", "title": "Bi-cross-validation for factor analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis is over a century old, but it is still problematic to choose\nthe number of factors for a given data set. The scree test is popular but\nsubjective. The best performing objective methods are recommended on the basis\nof simulations. We introduce a method based on bi-cross-validation, using\nrandomly held-out submatrices of the data to choose the number of factors. We\nfind it performs better than the leading methods of parallel analysis (PA) and\nKaiser's rule. Our performance criterion is based on recovery of the underlying\nfactor-loading (signal) matrix rather than identifying the true number of\nfactors. Like previous comparisons, our work is simulation based. Recent\nadvances in random matrix theory provide principled choices for the number of\nfactors when the noise is homoscedastic, but not for the heteroscedastic case.\nThe simulations we choose are designed using guidance from random matrix\ntheory. In particular, we include factors too small to detect, factors large\nenough to detect but not large enough to improve the estimate, and two classes\nof factors large enough to be useful. Much of the advantage of\nbi-cross-validation comes from cases with factors large enough to detect but\ntoo small to be well estimated. We also find that a form of early stopping\nregularization improves the recovery of the signal matrix.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 21:49:29 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 18:50:55 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Owen", "A. B.", ""], ["Wang", "J.", ""]]}, {"id": "1503.03529", "submitter": "Jose Augusto Fioruci", "authors": "Jos\\'e Augusto Fioruci, Tiago Ribeiro Pellegrini, Francisco Louzada\n  and Fotios Petropoulos", "title": "The Optimised Theta Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust forecasting methods for univariate time series are very\nimportant when the objective is to produce estimates for a large number of time\nseries. In this context, the Theta method called researchers attention due its\nperformance in the largest up-to-date forecasting competition, the\nM3-Competition. Theta method proposes the decomposition of the deseasonalised\ndata into two \"theta lines\". The first theta line removes completely the\ncurvatures of the data, thus being a good estimator of the long-term trend\ncomponent. The second theta line doubles the curvatures of the series, as to\nbetter approximate the short-term behaviour. In this paper, we propose a\ngeneralisation of the Theta method by optimising the selection of the second\ntheta line, based on various validation schemes where the out-of-sample\naccuracy of the candidate variants is measured. The recomposition process of\nthe original time series builds on the asymmetry of the decomposed theta lines.\nAn empirical investigation through the M3-Competition data set shows\nimprovements on the forecasting accuracy of the proposed optimised Theta\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 23:08:06 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Fioruci", "Jos\u00e9 Augusto", ""], ["Pellegrini", "Tiago Ribeiro", ""], ["Louzada", "Francisco", ""], ["Petropoulos", "Fotios", ""]]}, {"id": "1503.03666", "submitter": "Philip Dawid", "authors": "Peter B. Imrey and A. Philip Dawid", "title": "A Commentary on Statistical Assessment of Violence Recidivism Risk", "comments": "28 pages, 4 tables, 2 figures. This is an authors' original\n  (unrevised) manuscript of an article published in the journal \"Statistics and\n  Public Policy\", available freely online at http://www.tandfonline.com/", "journal-ref": "Statistics and Public Policy 2 (2015), e1029338, 1-18", "doi": "10.1080/2330443X.2015.1029338", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing integration and availability of data on large groups of persons\nhas been accompanied by proliferation of statistical and other algorithmic\nprediction tools in banking, insurance, marketiNg, medicine, and other FIelds\n(see e.g., Steyerberg (2009a;b)). Controversy may ensue when such tools are\nintroduced to fields traditionally reliant on individual clinical evaluations.\nSuch controversy has arisen about \"actuarial\" assessments of violence\nrecidivism risk, i.e., the probability that someone found to have committed a\nviolent act will commit another during a specified period. Recently Hart et al.\n(2007a) and subsequent papers from these authors in several reputable journals\nhave claimed to demonstrate that statistical assessments of such risks are\ninherently too imprecise to be useful, using arguments that would seem to apply\nto statistical risk prediction quite broadly. This commentary examines these\narguments from a technical statistical perspective, and finds them seriously\nmistaken in many particulars. They should play no role in reasoned discussions\nof violence recidivism risk assessment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 10:54:02 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Imrey", "Peter B.", ""], ["Dawid", "A. Philip", ""]]}, {"id": "1503.03761", "submitter": "Xiaoyu Liu", "authors": "Xiaoyu Liu, Serge Guillas and Ming-Jun Lai", "title": "Efficient spatial modelling using the SPDE approach with bivariate\n  splines", "comments": "26 pages, 7 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian fields (GFs) are frequently used in spatial statistics for their\nversatility. The associated computational cost can be a bottleneck, especially\nin realistic applications. It has been shown that computational efficiency can\nbe gained by doing the computations using Gaussian Markov random fields (GMRFs)\nas the GFs can be seen as weak solutions to corresponding stochastic partial\ndifferential equations (SPDEs) using piecewise linear finite elements. We\nintroduce a new class of representations of GFs with bivariate splines instead\nof finite elements. This allows an easier implementation of piecewise\npolynomial representations of various degrees. It leads to GMRFs that can be\ninferred efficiently and can be easily extended to non-stationary fields. The\nsolutions approximated with higher order bivariate splines converge faster,\nhence the computational cost can be alleviated. Numerical simulations using\nboth real and simulated data also demonstrate that our framework increases the\nflexibility and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 15:12:03 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Liu", "Xiaoyu", ""], ["Guillas", "Serge", ""], ["Lai", "Ming-Jun", ""]]}, {"id": "1503.03879", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri", "title": "Qualitative inequalities for squared partial correlations of a Gaussian\n  random vector", "comments": "21 pages, 13 figures", "journal-ref": "Annals of the Institute of Statistical Mathematics, 66(2),\n  345-367, 2014", "doi": "10.1007/s10463-013-0417-x", "report-no": "Department of Statistics and Applied Probability, National\n  University of Singapore technical report 201301", "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe various sets of conditional independence relationships,\nsufficient for qualitatively comparing non-vanishing squared partial\ncorrelations of a Gaussian random vector. These sufficient conditions are\nsatisfied by several graphical Markov models. Rules for comparing degree of\nassociation among the vertices of such Gaussian graphical models are also\ndeveloped. We apply these rules to compare conditional dependencies on Gaussian\ntrees. In particular for trees, we show that such dependence can be completely\ncharacterized by the length of the paths joining the dependent vertices to each\nother and to the vertices conditioned on. We also apply our results to\npostulate rules for model selection for polytree models. Our rules apply to\nmutual information of Gaussian random vectors as well.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 20:15:35 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chaudhuri", "Sanjay", ""]]}, {"id": "1503.04123", "submitter": "Daniel Rudolf", "authors": "Daniel Rudolf and Nikolaus Schweizer", "title": "Perturbation theory for Markov chains via Wasserstein distance", "comments": "31 pages, accepted at Bernoulli Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perturbation theory for Markov chains addresses the question how small\ndifferences in the transitions of Markov chains are reflected in differences\nbetween their distributions. We prove powerful and flexible bounds on the\ndistance of the $n$th step distributions of two Markov chains when one of them\nsatisfies a Wasserstein ergodicity condition. Our work is motivated by the\nrecent interest in approximate Markov chain Monte Carlo (MCMC) methods in the\nanalysis of big data sets. By using an approach based on Lyapunov functions, we\nprovide estimates for geometrically ergodic Markov chains under weak\nassumptions. In an autoregressive model, our bounds cannot be improved in\ngeneral. We illustrate our theory by showing quantitative estimates for\napproximate versions of two prominent MCMC algorithms, the Metropolis-Hastings\nand stochastic Langevin algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 16:00:46 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 08:36:17 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 19:54:53 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Rudolf", "Daniel", ""], ["Schweizer", "Nikolaus", ""]]}, {"id": "1503.04178", "submitter": "Florian Maire", "authors": "Florian Maire, Nial Friel, Pierre Alquier", "title": "Light and Widely Applicable MCMC: Approximate Bayesian Inference for\n  Large Datasets", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light and Widely Applicable (LWA-) MCMC is a novel approximation of the\nMetropolis-Hastings kernel targeting a posterior distribution defined on a\nlarge number of observations. Inspired by Approximate Bayesian Computation, we\ndesign a Markov chain whose transition makes use of an unknown but fixed,\nfraction of the available data, where the random choice of sub-sample is guided\nby the fidelity of this sub-sample to the observed data, as measured by summary\n(or sufficient) statistics. LWA-MCMC is a generic and flexible approach, as\nillustrated by the diverse set of examples which we explore. In each case\nLWA-MCMC yields excellent performance and in some cases a dramatic improvement\ncompared to existing methodologies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 18:25:25 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 08:24:19 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Maire", "Florian", ""], ["Friel", "Nial", ""], ["Alquier", "Pierre", ""]]}, {"id": "1503.04303", "submitter": "Debdeep Pati", "authors": "Hanning Li, Debdeep Pati", "title": "Variable Selection Using Shrinkage Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection has received widespread attention over the last decade as\nwe routinely encounter high-throughput datasets in complex biological and\nenvironment research. Most Bayesian variable selection methods are restricted\nto mixture priors having separate components for characterizing the signal and\nthe noise. However, such priors encounter computational issues in high\ndimensions. This has motivated continuous shrinkage priors, resembling the\ntwo-component priors facilitating computation and interpretability. While such\npriors are widely used for estimating high-dimensional sparse vectors,\nselecting a subset of variables remains a daunting task. In this article, we\npropose a general approach for variable selection with shrinkage priors. The\npresence of very few tuning parameters makes our method attractive in\ncomparison to adhoc thresholding approaches. The applicability of the approach\nis not limited to continuous shrinkage priors, but can be used along with any\nshrinkage prior. Theoretical properties for near-collinear design matrices are\ninvestigated and the method is shown to have good performance in a wide range\nof synthetic data examples.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 13:17:20 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2015 21:25:02 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Li", "Hanning", ""], ["Pati", "Debdeep", ""]]}, {"id": "1503.04313", "submitter": "Shyam Mohan M", "authors": "Shyam Mohan M, Naren Naik, R.M.O. Gemson, M.R. Ananthasayanam", "title": "Introduction to the Kalman Filter and Tuning its Statistics for Near\n  Optimal Estimates and Cramer Rao Bound", "comments": "Technical Report (Dept. of Electrical Engineering, IIT Kanpur, India)", "journal-ref": null, "doi": null, "report-no": "TR/EE2015/401", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides a brief historical evolution of the concepts in the\nKalman filtering theory since ancient times to the present. A brief description\nof the filter equations its aesthetics, beauty, truth, fascinating perspectives\nand competence are described. For a Kalman filter design to provide optimal\nestimates tuning of its statistics namely initial state and covariance, unknown\nparameters, and state and measurement noise covariances is important. The\nearlier tuning approaches are reviewed. The present approach is a reference\nrecursive recipe based on multiple filter passes through the data without any\noptimization to reach a `statistical equilibrium' solution. It utilizes the a\npriori, a posteriori, and smoothed states, their corresponding predicted\nmeasurements and the actual measurements help to balance the measurement\nequation and similarly the state equation to help form a generalized likelihood\ncost function. The filter covariance at the end of each pass is heuristically\nscaled up by the number of data points is further trimmed to statistically\nmatch the exact estimates and Cramer Rao Bounds (CRBs) available with no\nprocess noise provided the initial covariance for subsequent passes. During\nsimulation studies with process noise the matching of the input and estimated\nnoise sequence over time and in real data the generalized cost functions helped\nto obtain confidence in the results. Simulation studies of a constant signal, a\nramp, a spring, mass, damper system with a weak non linear spring constant,\nlongitudinal and lateral motion of an airplane was followed by similar but more\ninvolved real airplane data was carried out in MATLAB. In all cases the present\napproach was shown to provide internally consistent and best possible estimates\nand their CRBs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 15:24:14 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["M", "Shyam Mohan", ""], ["Naik", "Naren", ""], ["Gemson", "R. M. O.", ""], ["Ananthasayanam", "M. R.", ""]]}, {"id": "1503.04407", "submitter": "Yanguang Chen", "authors": "Yanguang Chen", "title": "Spatial autocorrelation approaches to testing residuals from least\n  squares regression", "comments": "27 pages, 4 figures, 5 tables, 2 appendices", "journal-ref": "PLoS ONE, 2016, 11(1): e0146865", "doi": "10.1371/journal.pone.0146865", "report-no": null, "categories": "stat.ME physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistics, the Durbin-Watson test is always employed to detect the\npresence of serial correlation of residuals from a least squares regression\nanalysis. However, the Durbin-Watson statistic is only suitable for ordered\ntime or spatial series. If the variables comprise cross-sectional data coming\nfrom spatial random sampling, the Durbin-Watson will be ineffectual because the\nvalue of Durbin-Watson's statistic depends on the sequences of data point\narrangement. Based on the ideas from spatial autocorrelation, this paper\npresents two new statistics for testing serial correlation of residuals from\nleast squares regression based on spatial samples. By analogy with the new form\nof Moran's index, an autocorrelation coefficient is defined with a standardized\nresidual vector and a normalized spatial weight matrix. Then on the analogy of\nthe Durbin-Watson statistic, a serial correlation index is constructed. As a\ncase, the two statistics are applied to the spatial sample of 29 China's\nregions. These results show that the new spatial autocorrelation model can be\nused to test the serial correlation of residuals from regression analysis. In\npractice, the new statistics can make up for the deficiency of the\nDurbin-Watson test.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 10:07:50 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Chen", "Yanguang", ""]]}, {"id": "1503.04499", "submitter": "Mariela Fernandez", "authors": "M. Fern\\'andez and V. A. Gonz\\'alez-L\\'opez", "title": "Cumulative Conditional Expectation Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the cumulative conditional expectation function (CCEF)\nin the copula context. It is shown how to compute CCEF in terms of the\ncumulative copula function, this natural representation allows to deduce some\nuseful properties, for instance with applications to convex combination of\ncopulas. We introduce approximations of CCEF based on Bernstein polynomial\ncopulas. We introduce estimators for CCEF, which were constructed through\nBernstein polynomial estimators for copulas. The estimators are asymptotically\nnormal and biased for CCEF.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 02:01:40 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Fern\u00e1ndez", "M.", ""], ["Gonz\u00e1lez-L\u00f3pez", "V. A.", ""]]}, {"id": "1503.04662", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universite Paris-Dauphine and University of\n  Warwick) and Jean-Michel Marin (Universite de Montpellier)", "title": "Bayesian Essentials with R: The Complete Solution Manual", "comments": "117 pages, 124 exercises, 22 figures. arXiv admin note: substantial\n  text overlap with arXiv:0910.4696", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the collection of solutions for all the exercises proposed in\nBayesian Essentials with R (2014).\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 14:19:21 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Robert", "Christian P.", "", "Universite Paris-Dauphine and University of\n  Warwick"], ["Marin", "Jean-Michel", "", "Universite de Montpellier"]]}, {"id": "1503.05093", "submitter": "Yohann de Castro", "authors": "Jean-Marc Aza\\\"is, Yohann de Castro, St\\'ephane Mourareau", "title": "Power of the Spacing test for Least-Angle Regression", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Post-Selection Inference have shown that conditional\ntesting is relevant and tractable in high-dimensions. In the Gaussian linear\nmodel, further works have derived unconditional test statistics such as the\nKac-Rice Pivot for general penalized problems. In order to test the global\nnull, a prominent offspring of this breakthrough is the spacing test that\naccounts the relative separation between the first two knots of the celebrated\nleast-angle regression (LARS) algorithm. However, no results have been shown\nregarding the distribution of these test statistics under the alternative. For\nthe first time, this paper addresses this important issue for the spacing test\nand shows that it is unconditionally unbiased. Furthermore, we provide the\nfirst extension of the spacing test to the frame of unknown noise variance.\n  More precisely, we investigate the power of the spacing test for LARS and\nprove that it is unbiased: its power is always greater or equal to the\nsignificance level $\\alpha$. In particular, we describe the power of this test\nunder various scenarii: we prove that its rejection region is optimal when the\npredictors are orthogonal; as the level $\\alpha$ goes to zero, we show that the\nprobability of getting a true positive is much greater than $\\alpha$; and we\ngive a detailed description of its power in the case of two predictors.\nMoreover, we numerically investigate a comparison between the spacing test for\nLARS and the Pearson's chi-squared test (goodness of fit).\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 15:32:08 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 19:45:05 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 20:19:59 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2015 11:14:10 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Aza\u00efs", "Jean-Marc", ""], ["de Castro", "Yohann", ""], ["Mourareau", "St\u00e9phane", ""]]}, {"id": "1503.05130", "submitter": "Satyaki Mazumder", "authors": "Buddhananda Banerjee and Satyaki Mazumder", "title": "On existence of a change in mean of functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data often arise as sequential temporal observations over a\ncontinuous state-space. A set of functional data with a possible change in its\nstructure may lead to a wrong conclusion if it is not taken in to account. So,\nsometimes, it is crucial to know about the existence of change point in a given\nsequence of functional data before doing any further statistical inference. We\ndevelop a new methodology to provide a test for detecting a change in the mean\nfunction of the corresponding data. To obtain the test statistic we provide an\nalternative estimator of the covariance kernel. The proposed estimator is\nasymptotically unbiased under the null hypothesis and, at the same time, has\nsmaller amount of bias than that of the existing estimator. We show here that\nunder the null hypothesis the proposed test statistic is pivotal\nasymptotically. Moreover, it is shown that under alternative hypothesis the\ntest is consistent for large enough sample size. It is also found that the\nproposed test is more powerful than the available test procedure in the\nliterature. From the extensive simulation studies we observe that the proposed\ntest outperforms the existing one with a wide margin in power for moderate\nsample size. The developed methodology performs satisfactorily for the average\ndaily temperature of central England and monthly global average anomaly of\ntemperatures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 17:24:09 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Banerjee", "Buddhananda", ""], ["Mazumder", "Satyaki", ""]]}, {"id": "1503.05754", "submitter": "Bojana Milo\\v{s}evi\\'c", "authors": "Bojana Milo\\v{s}evi\\'c", "title": "Asymptotic Efficiency of New Exponentiality Tests Based on a\n  Characterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two new tests for exponentiality, of integral and Kolmogorov type, are\nproposed. They are based on a recent characterization and formed using\nappropriate V-statistics. Their asymptotic properties are examined and their\nlocal Bahadur efficiencies against some common alternatives are found. A class\nof locally optimal alternatives for each test is obtained. The powers of these\ntests, for some small sample sizes, are compared with different exponentiality\ntests.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 13:04:49 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Milo\u0161evi\u0107", "Bojana", ""]]}, {"id": "1503.05852", "submitter": "Tinghui Yu", "authors": "Tinghui Yu (FDA, Center for Devices and Radiological Health), Yabing\n  Mai (Merck Research Laboratories), Sherry Liu (FDA, Center for Devices and\n  Radiological Health), Xiaofei Hu (Merck Research Laboratories)", "title": "Combining Survival Trials Using Aggregate Data Based on Misspecified\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment effects of the same therapy observed from multiple clinical\ntrials can often be very different. Yet the patient characteristics accounting\nfor these differences may not be identifiable in real world practice. There\nneeds to be an unbiased way to combine the results from multiple trials and\nreport the overall treatment effect for the general population during the\ndevelopment and validation of a new therapy. The non-linear structure of the\nmaximum partial likelihood estimates for the (log) hazard ratio defined with a\nCox proportional hazard model leads to challenges in the statistical analyses\nfor combining such clinical trials. In this paper, we formulated the expected\noverall treatment effects using various modeling assumptions. Thus we are\nproposing efficient estimates and a version of Wald test for the combined\nhazard ratio using only aggregate data. Interpretation of the methods are\nprovided in the framework of robust data analyses involving misspecified\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 17:35:24 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yu", "Tinghui", "", "FDA, Center for Devices and Radiological Health"], ["Mai", "Yabing", "", "Merck Research Laboratories"], ["Liu", "Sherry", "", "FDA, Center for Devices and\n  Radiological Health"], ["Hu", "Xiaofei", "", "Merck Research Laboratories"]]}, {"id": "1503.06049", "submitter": "Sven Buhl", "authors": "Sven Buhl and Claudia Kl\\\"uppelberg", "title": "Anisotropic Brown-Resnick space-time processes: estimation and model\n  assessment", "comments": "31 pages, 6 figures. Published in Extremes. The final publication is\n  available at http://link.springer.com/article/10.1007/s10687-016-0257-1", "journal-ref": "Extremes, 2016", "doi": "10.1007/s10687-016-0257-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially isotropic max-stable processes have been used to model extreme\nspatial or space-time observations. One prominent model is the Brown-Resnick\nprocess, which has been successfully fitted to time series, spatial data and\nspace-time data. This paper extends the process to possibly anisotropic spatial\nstructures. For regular grid observations we prove strong consistency and\nasymptotic normality of pairwise maximum likelihood estimates for fixed and\nincreasing spatial domain, when the number of observations in time tends to\ninfinity. We also present a statistical test for isotropy versus anisotropy. We\napply our test to precipitation data in Florida, and present some diagnostic\ntools for model assessment. Finally, we present a method to predict conditional\nprobability fields and apply it to the data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 12:20:02 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 08:43:08 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2015 14:35:35 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 10:18:20 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Buhl", "Sven", ""], ["Kl\u00fcppelberg", "Claudia", ""]]}, {"id": "1503.06236", "submitter": "Rahul Agarwal", "authors": "Rahul Agarwal, Zhe Chen, Sridevi V. Sarma", "title": "Nonparametric Estimation of Band-limited Probability Density Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a nonparametric maximum likelihood (ML) estimator for\nband-limited (BL) probability density functions (pdfs) is proposed. The BLML\nestimator is consistent and computationally efficient. To compute the BLML\nestimator, three approximate algorithms are presented: a binary quadratic\nprogramming (BQP) algorithm for medium scale problems, a Trivial algorithm for\nlarge-scale problems that yields a consistent estimate if the underlying pdf is\nstrictly positive and BL, and a fast implementation of the Trivial algorithm\nthat exploits the band-limited assumption and the Nyquist sampling theorem\n(\"BLMLQuick\"). All three BLML estimators outperform kernel density estimation\n(KDE) algorithms (adaptive and higher order KDEs) with respect to the mean\nintegrated squared error for data generated from both BL and infinite-band\npdfs. Further, the BLMLQuick estimate is remarkably faster than the KD\nalgorithms. Finally, the BLML method is applied to estimate the conditional\nintensity function of a neuronal spike train (point process) recorded from a\nrat's entorhinal cortex grid cell, for which it outperforms state-of-the-art\nestimators used in neuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 21:51:39 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 21:39:46 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 21:08:39 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2015 01:40:31 GMT"}, {"version": "v5", "created": "Mon, 29 Jun 2015 01:32:21 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Agarwal", "Rahul", ""], ["Chen", "Zhe", ""], ["Sarma", "Sridevi V.", ""]]}, {"id": "1503.06239", "submitter": "Jinye Zhang", "authors": "Jinye Zhang, Zhijian Ou", "title": "Block-Wise MAP Inference for Determinantal Point Processes with\n  Application to Change-Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Existing MAP inference algorithms for determinantal point processes (DPPs)\nneed to calculate determinants or conduct eigenvalue decomposition generally at\nthe scale of the full kernel, which presents a great challenge for real-world\napplications. In this paper, we introduce a class of DPPs, called BwDPPs, that\nare characterized by an almost block diagonal kernel matrix and thus can allow\nefficient block-wise MAP inference. Furthermore, BwDPPs are successfully\napplied to address the difficulty of selecting change-points in the problem of\nchange-point detection (CPD), which results in a new BwDPP-based CPD method,\nnamed BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is\nfirst created based on existing well-studied metrics. Then, these change-point\ncandidates are treated as DPP items, and DPP-based subset selection is\nconducted to give the final estimate of the change-points that favours both\nquality and diversity. The effectiveness of BwDppCpd is demonstrated through\nextensive experiments on five real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 22:01:45 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zhang", "Jinye", ""], ["Ou", "Zhijian", ""]]}, {"id": "1503.06262", "submitter": "Justin Yang Mr.", "authors": "Samuel Kou and Justin J. Yang", "title": "Optimal shrinkage estimation in heteroscedastic hierarchical linear\n  models", "comments": "32 pages, 3 figures, contributed to: \"Big and Complex Data Analysis:\n  Statistical Methodologies and Applications\", Springer, New York", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimators have profound impacts in statistics and in scientific\nand engineering applications. In this article, we consider shrinkage estimation\nin the presence of linear predictors. We formulate two heteroscedastic\nhierarchical regression models and study optimal shrinkage estimators in each\nmodel. A class of shrinkage estimators, both parametric and semiparametric,\nbased on unbiased risk estimate (URE) is proposed and is shown to be\n(asymptotically) optimal under mean squared error loss in each model.\nSimulation study is conducted to compare the performance of the proposed\nmethods with existing shrinkage estimators. We also apply the method to real\ndata and obtain encouraging and interesting results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 04:41:43 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Kou", "Samuel", ""], ["Yang", "Justin J.", ""]]}, {"id": "1503.06267", "submitter": "Yong Huang", "authors": "Yong Huang and James L. Beck", "title": "Hierarchical sparse Bayesian learning: theory and application for\n  inferring structural damage from incomplete modal data", "comments": "41 pages, 8 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1408.3685", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural damage due to excessive loading or environmental degradation\ntypically occurs in localized areas in the absence of collapse. This prior\ninformation about the spatial sparseness of structural damage is exploited here\nby a hierarchical sparse Bayesian learning framework with the goal of reducing\nthe source of ill-conditioning in the stiffness loss inversion problem for\ndamage detection. Sparse Bayesian learning methodologies automatically prune\naway irrelevant or inactive features from a set of potential candidates, and so\nthey are effective probabilistic tools for producing sparse explanatory\nsubsets. We have previously proposed such an approach to establish the\nprobability of localized stiffness reductions that serve as a proxy for damage\nby using noisy incomplete modal data from before and after possible damage. The\ncore idea centers on a specific hierarchical Bayesian model that promotes\nspatial sparseness in the inferred stiffness reductions in a way that is\nconsistent with the Bayesian Ockham razor. In this paper, we improve the theory\nof our previously proposed sparse Bayesian learning approach by eliminating an\napproximation and, more importantly, incorporating a constraint on stiffness\nincreases. Our approach has many appealing features that are summarized at the\nend of the paper. We validate the approach by applying it to the Phase II\nsimulated and experimental benchmark studies sponsored by the IASC-ASCE Task\nGroup on Structural Health Monitoring. The results show that it can reliably\ndetect, locate and assess damage by inferring substructure stiffness losses\nfrom the identified modal parameters. The occurrence of missed and false damage\nalerts is effectively suppressed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 05:45:16 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""]]}, {"id": "1503.06302", "submitter": "Francesca Greselin", "authors": "L.A. Garc\\'ia-Escudero, A. Gordaliza, F. Greselin, S. Ingrassia, A.\n  Mayo-Iscar", "title": "Robust estimation for mixtures of Gaussian factor analyzers, based on\n  trimming and constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of Gaussian factors are powerful tools for modeling an unobserved\nheterogeneous population, offering - at the same time - dimension reduction and\nmodel-based clustering. Unfortunately, the high prevalence of spurious\nsolutions and the disturbing effects of outlying observations, along maximum\nlikelihood estimation, open serious issues. In this paper we consider\nrestrictions for the component covariances, to avoid spurious solutions, and\ntrimming, to provide robustness against violations of normality assumptions of\nthe underlying latent factors. A detailed AECM algorithm for this new approach\nis presented. Simulation results and an application to the AIS dataset show the\naim and effectiveness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 14:01:38 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Garc\u00eda-Escudero", "L. A.", ""], ["Gordaliza", "A.", ""], ["Greselin", "F.", ""], ["Ingrassia", "S.", ""], ["Mayo-Iscar", "A.", ""]]}, {"id": "1503.06370", "submitter": "Zichen Ma", "authors": "Zichen Ma and Ernest Fokou\\'e", "title": "Bayesian Variable Selection for Linear Regression with the $\\kappa$-$G$\n  Priors", "comments": "19 pages, 3 figures, 1 table, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new methodology for Bayesian variable selection\nin linear regression that is independent of the traditional indicator method. A\ndiagonal matrix $\\mathbf{G}$ is introduced to the prior of the coefficient\nvector $\\boldsymbol{\\beta}$, with each of the $g_j$'s, bounded between $0$ and\n$1$, on the diagonal serves as a stabilizer of the corresponding $\\beta_j$.\nMathematically, a promising variable has a $g_j$ value that is close to $0$,\nwhereas the value of $g_j$ corresponding to an unpromising variable is close to\n$1$. This property is proven in this paper under orthogonality together with\nother asymptotic properties. Computationally, the sample path of each $g_j$ is\nobtained through Metropolis-within-Gibbs sampling method. Also, in this paper\nwe give two simulations to verify the capability of this methodology in\nvariable selection.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 01:19:08 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 21:26:29 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Ma", "Zichen", ""], ["Fokou\u00e9", "Ernest", ""]]}, {"id": "1503.06426", "submitter": "Peter B\\\"uhlmann", "authors": "Peter B\\\"uhlmann, Sara van de Geer", "title": "High-dimensional inference in misspecified linear models", "comments": "24 pages, 4 figures", "journal-ref": "Electronic Journal of Statistics 2015, Vol. 9, 1449-1473", "doi": "10.1214/15-EJS1041", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional inference when the assumed linear model is\nmisspecified. We describe some correct interpretations and corresponding\nsufficient assumptions for valid asymptotic inference of the model parameters,\nwhich still have a useful meaning when the model is misspecified. We largely\nfocus on the de-sparsified Lasso procedure but we also indicate some\nimplications for (multiple) sample splitting techniques. In view of available\nmethods and software, our results contribute to robustness considerations with\nrespect to model misspecification.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 13:26:00 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["van de Geer", "Sara", ""]]}, {"id": "1503.06448", "submitter": "Dimitris Rizopoulos", "authors": "Dimitris Rizopoulos, Jeremy M.G. Taylor, Joost van Rosmalen, Ewout W.\n  Steyerberg and Johanna J.M. Takkenberg", "title": "Personalized Screening Intervals for Biomarkers using Joint Models for\n  Longitudinal and Survival Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening and surveillance are routinely used in medicine for early detection\nof disease and close monitoring of progression. Biomarkers are one of the\nprimarily tools used for these tasks, but their successful translation to\nclinical practice is closely linked to their ability to accurately predict\nclinical endpoints during follow-up. Motivated by a study of patients who\nreceived a human tissue valve in the aortic position, in this work we are\ninterested in optimizing and personalizing screening intervals for longitudinal\nbiomarker measurements. Our aim in this paper is twofold: First, to\nappropriately select the model to use at time t, the time point the patient was\nstill event-free, and second, based on this model to select the optimal time\npoint u > t to plan the next measurement. To achieve these two goals we develop\nmeasures based on information theory quantities that assess the information we\ngain for the conditional survival process given the history of the subject that\nincludes both baseline information and his/her accumulated longitudinal\nmeasurements.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 17:53:11 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Rizopoulos", "Dimitris", ""], ["Taylor", "Jeremy M. G.", ""], ["van Rosmalen", "Joost", ""], ["Steyerberg", "Ewout W.", ""], ["Takkenberg", "Johanna J. M.", ""]]}, {"id": "1503.06492", "submitter": "Kazuyoshi Yata", "authors": "Kazuyoshi Yata and Makoto Aoshima", "title": "High-dimensional inference on covariance structures via the extended\n  cross-data-matrix methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider testing the correlation coefficient matrix between\ntwo subsets of high-dimensional variables. We produce a test statistic by using\nthe extended cross-data-matrix (ECDM) methodology and show the unbiasedness of\nECDM estimator. We also show that the ECDM estimator has the consistency\nproperty and the asymptotic normality in high-dimensional settings. We propose\na test procedure by the ECDM estimator and evaluate its asymptotic size and\npower theoretically and numerically. We give several applications of the ECDM\nestimator. Finally, we demonstrate how the test procedure performs in actual\ndata analyses by using a microarray data set.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 23:02:03 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Yata", "Kazuyoshi", ""], ["Aoshima", "Makoto", ""]]}, {"id": "1503.06675", "submitter": "Pushpendra Singh", "authors": "Pushpendra Singh, Shiv Dutt Joshi, Rakesh Kumar Patney and Kaushik\n  Saha", "title": "The Fourier Decomposition Method for nonlinear and nonstationary time\n  series analysis", "comments": "14 Pages, 18 Figures", "journal-ref": "Proceedings of the Royal Society of London A; March 2017, Volume\n  473, issue 2199", "doi": "10.1098/rspa.2016.0871", "report-no": null, "categories": "stat.ME cs.IT math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since many decades, there is a general perception in literature that the\nFourier methods are not suitable for the analysis of nonlinear and\nnonstationary data. In this paper, we propose a Fourier Decomposition Method\n(FDM) and demonstrate its efficacy for the analysis of nonlinear (i.e. data\ngenerated by nonlinear systems) and nonstationary time series. The proposed FDM\ndecomposes any data into a small number of `Fourier intrinsic band functions'\n(FIBFs). The FDM presents a generalized Fourier expansion with variable\namplitudes and frequencies of a time series by the Fourier method itself. We\npropose an idea of zero-phase filter bank based multivariate FDM (MFDM)\nalgorithm, for the analysis of multivariate nonlinear and nonstationary time\nseries, from the FDM. We also present an algorithm to obtain cutoff frequencies\nfor MFDM. The MFDM algorithm is generating finite number of band limited\nmultivariate FIBFs (MFIBFs). The MFDM preserves some intrinsic physical\nproperties of the multivariate data, such as scale alignment, trend and\ninstantaneous frequency. The proposed methods produce the results in a\ntime-frequency-energy distribution that reveal the intrinsic structures of a\ndata. Simulations have been carried out and comparison is made with the\nEmpirical Mode Decomposition (EMD) methods in the analysis of various simulated\nas well as real life time series, and results show that the proposed methods\nare powerful tools for analyzing and obtaining the time-frequency-energy\nrepresentation of any data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 10:37:06 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 06:47:26 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Singh", "Pushpendra", ""], ["Joshi", "Shiv Dutt", ""], ["Patney", "Rakesh Kumar", ""], ["Saha", "Kaushik", ""]]}, {"id": "1503.06873", "submitter": "Andy  Royle", "authors": "J. Andrew Royle", "title": "Spatial Capture-recapture with Partial Identity", "comments": "Revision 4/19/2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an inference framework for spatial capture-recapture data when two\nmethods are used in which individuality cannot generally be reconciled between\nthe two methods. A special case occurs in camera trapping when left-side\n(method 1) and right-side (method 2) photos are obtained but not\nsimultaneously. We specify a spatially explicit capture-recapture model for the\nlatent \"perfect\" data set which is conditioned on known identity of individuals\nbetween methods. We regard the identity variable which associates individuals\nof the two data sets as an unknown in the model and we propose a Bayesian\nanalysis strategy for the model in which the identity variable is updated using\na Metropolis component algorithm. The work extends previous efforts to deal\nwith incomplete data by recognizing that there is information about\nindividuality in the spatial juxtaposition of captures. Thus, individual\nrecords obtained by both sampling methods that are in close proximity are more\nlikely to be the same individual than individuals that are not in close\nproximity. The model proposed here formalizes this trade-off between spatial\nproximity and probabilistic determination of individuality using spatially\nexplicit capture-recapture models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 23:14:18 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 03:27:33 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Royle", "J. Andrew", ""]]}, {"id": "1503.06876", "submitter": "Ping Li", "authors": "Ping Li", "title": "Binary and Multi-Bit Coding for Stable Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop efficient binary (i.e., 1-bit) and multi-bit coding schemes for\nestimating the scale parameter of $\\alpha$-stable distributions. The work is\nmotivated by the recent work on one scan 1-bit compressed sensing (sparse\nsignal recovery) using $\\alpha$-stable random projections, which requires\nestimating of the scale parameter at bits-level. Our technique can be naturally\napplied to data stream computations for estimating the $\\alpha$-th frequency\nmoment. In fact, the method applies to the general scale family of\ndistributions, not limited to $\\alpha$-stable distributions.\n  Due to the heavy-tailed nature of $\\alpha$-stable distributions, using\ntraditional estimators will potentially need many bits to store each\nmeasurement in order to ensure sufficient accuracy. Interestingly, our paper\ndemonstrates that, using a simple closed-form estimator with merely 1-bit\ninformation does not result in a significant loss of accuracy if the parameter\nis chosen appropriately. For example, when $\\alpha=0+$, 1, and 2, the\ncoefficients of the optimal estimation variances using full (i.e.,\ninfinite-bit) information are 1, 2, and 2, respectively. With the 1-bit scheme\nand appropriately chosen parameters, the corresponding variance coefficients\nare 1.544, $\\pi^2/4$, and 3.066, respectively. Theoretical tail bounds are also\nprovided. Using 2 or more bits per measurements reduces the estimation variance\nand importantly, stabilizes the estimate so that the variance is not sensitive\nto parameters. With look-up tables, the computational cost is minimal.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 00:05:17 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2016 15:23:36 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1503.06910", "submitter": "Enayetur Raheem", "authors": "Enayetur Raheem, A. K. Md. Ehsanes Saleh", "title": "Penalty, Shrinkage, and Preliminary Test Estimators under Full Model\n  Hypothesis", "comments": "28 pages, 4 figures, 10 tables. arXiv admin note: text overlap with\n  arXiv:1503.05160", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a multiple regression model and compares, under full\nmodel hypothesis, analytically as well as by simulation, the performance\ncharacteristics of some popular penalty estimators such as ridge regression,\nLASSO, adaptive LASSO, SCAD, and elastic net versus Least Squares Estimator,\nrestricted estimator, preliminary test estimator, and Stein-type estimators\nwhen the dimension of the parameter space is smaller than the sample space\ndimension. We find that RR uniformly dominates LSE, RE, PTE, SE and PRSE while\nLASSO, aLASSO, SCAD, and EN uniformly dominates LSE only. Further, it is\nobserved that neither penalty estimators nor Stein-type estimator dominate one\nanother.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 04:40:53 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Raheem", "Enayetur", ""], ["Saleh", "A. K. Md. Ehsanes", ""]]}, {"id": "1503.06913", "submitter": "Merlise Clyde", "authors": "Yingbo Li and Merlise A. Clyde", "title": "Mixtures of g-priors in Generalized Linear Models", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1469992", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of Zellner's g-priors have been studied extensively in linear models\nand have been shown to have numerous desirable properties for Bayesian variable\nselection and model averaging. Several extensions of g-priors to Generalized\nLinear Models (GLMs) have been proposed in the literature; however, the choice\nof prior distribution of g and resulting properties for inference have received\nconsiderably less attention. In this paper, we unify mixtures of g-priors in\nGLMs by assigning the truncated Compound Confluent Hypergeometric (tCCH)\ndistribution to 1/(1 + g), which encompasses as special cases several mixtures\nof g-priors in the literature, such as the hyper-g, Beta-prime, truncated\nGamma, incomplete inverse-Gamma, benchmark, robust, hyper-g/n, and intrinsic\npriors. Through an integrated Laplace approximation, the posterior distribution\nof 1/(1 + g) is in turn a tCCH distribution, and approximate marginal\nlikelihoods are thus available analytically, leading to \"Compound\nHypergeometric Information Criteria\" for model selection. We discuss the local\ngeometric properties of the g-prior in GLMs and show how the desiderata for\nmodel selection proposed by Bayarri et al, such as asymptotic model selection\nconsistency, intrinsic consistency, and measurement invariance may be used to\njustify the prior and specific choices of the hyper parameters. We illustrate\ninference using these priors and contrast them to other approaches via\nsimulation and real data examples. The methodology is implemented in the R\npackage BAS and freely available on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 04:52:11 GMT"}, {"version": "v2", "created": "Sun, 23 Oct 2016 21:31:48 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 19:52:29 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Li", "Yingbo", ""], ["Clyde", "Merlise A.", ""]]}, {"id": "1503.07102", "submitter": "Yuki Kawakubo", "authors": "Yuki Kawakubo, Tatsuya Kubokawa and Muni S. Srivastava", "title": "A Variant of AIC based on the Bayesian Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose information criteria that measure the prediction risk of a\npredictive density based on the Bayesian marginal likelihood from a frequentist\npoint of view. We derive criteria for selecting variables in linear regression\nmodels, assuming a prior distribution of the regression coefficients. Then, we\ndiscuss the relationship between the proposed criteria and related criteria.\nThere are three advantages of our method. First, this is a compromise between\nthe frequentist and Bayesian standpoints because it evaluates the frequentist's\nrisk of the Bayesian model. Thus, it is less influenced by a prior\nmisspecification. Second, the criteria exhibits consistency when selecting the\ntrue model. Third, when a uniform prior is assumed for the regression\ncoefficients, the resulting criterion is equivalent to the residual information\ncriterion (RIC) of Shi and Tsai (2002).\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 16:27:46 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 04:40:11 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 09:15:40 GMT"}, {"version": "v4", "created": "Tue, 21 Feb 2017 14:34:28 GMT"}, {"version": "v5", "created": "Thu, 19 Oct 2017 13:29:05 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Kawakubo", "Yuki", ""], ["Kubokawa", "Tatsuya", ""], ["Srivastava", "Muni S.", ""]]}, {"id": "1503.07307", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad and H{\\aa}vard Rue", "title": "Improving the INLA approach for approximate Bayesian inference for\n  latent Gaussian models", "comments": null, "journal-ref": "Electronic Journal of Statistics, Volume 9, Number 2 (2015),\n  2706-2731", "doi": "10.1214/15-EJS1092", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new copula-based correction for generalized linear mixed\nmodels (GLMMs) within the integrated nested Laplace approximation (INLA)\napproach for approximate Bayesian inference for latent Gaussian models. While\nINLA is usually very accurate, some (rather extreme) cases of GLMMs with e.g.\nbinomial or Poisson data have been seen to be problematic. Inaccuracies can\noccur when there is a very low degree of smoothing or \"borrowing strength\"\nwithin the model, and we have therefore developed a correction aiming to push\nthe boundaries of the applicability of INLA. Our new correction has been\nimplemented as part of the R-INLA package, and adds only negligible\ncomputational cost. Empirical evaluations on both real and simulated data\nindicate that the method works well.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 09:00:22 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 17:14:30 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 14:24:17 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2015 11:21:11 GMT"}, {"version": "v5", "created": "Tue, 24 Nov 2015 09:45:06 GMT"}, {"version": "v6", "created": "Tue, 15 Dec 2015 09:23:47 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1503.07591", "submitter": "Hau-tieng Wu", "authors": "Matthieu Kowalski, Adrien Meynard, Hau-tieng Wu", "title": "Convex Optimization approach to signals with fast varying instantaneous\n  frequency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the limitation of analyzing oscillatory signals composed of\nmultiple components with fast-varying instantaneous frequency, we approach the\ntime-frequency analysis problem by optimization. Based on the proposed adaptive\nharmonic model, the time-frequency representation of a signal is obtained by\ndirectly minimizing a functional, which involves few properties an \"ideal\ntime-frequency representation\" should satisfy, for example, the signal\nreconstruction and concentrative time frequency representation. FISTA (Fast\nIterative Shrinkage-Thresholding Algorithm) is applied to achieve an efficient\nnumerical approximation of the functional. We coin the algorithm as {\\it\nTime-frequency bY COnvex OptimizatioN} (Tycoon). The numerical results confirm\nthe potential of the Tycoon algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 01:17:21 GMT"}, {"version": "v2", "created": "Sun, 20 Mar 2016 00:40:55 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Kowalski", "Matthieu", ""], ["Meynard", "Adrien", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1503.07642", "submitter": "Trevelyan J. McKinley", "authors": "Trevelyan J. McKinley, Michelle Morters, James L. N. Wood", "title": "Bayesian Model Choice in Cumulative Link Ordinal Regression Models", "comments": "Published at http://dx.doi.org/10.1214/14-BA884 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 1, 1-30", "doi": "10.1214/14-BA884", "report-no": "VTeX-BA-BA884", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of the proportional odds (PO) model for ordinal regression is\nubiquitous in the literature. If the assumption of parallel lines does not hold\nfor the data, then an alternative is to specify a non-proportional odds (NPO)\nmodel, where the regression parameters are allowed to vary depending on the\nlevel of the response. However, it is often difficult to fit these models, and\nchallenges regarding model choice and fitting are further compounded if there\nare a large number of explanatory variables. We make two contributions towards\ntackling these issues: firstly, we develop a Bayesian method for fitting these\nmodels, that ensures the stochastic ordering conditions hold for an arbitrary\nfinite range of the explanatory variables, allowing NPO models to be fitted to\nany observed data set. Secondly, we use reversible-jump Markov chain Monte\nCarlo to allow the model to choose between PO and NPO structures for each\nexplanatory variable, and show how variable selection can be incorporated.\nThese methods can be adapted for any monotonic increasing link functions. We\nillustrate the utility of these approaches on novel data from a longitudinal\nstudy of individual-level risk factors affecting body condition score in a dog\npopulation in Zenzele, South Africa.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 08:07:41 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["McKinley", "Trevelyan J.", ""], ["Morters", "Michelle", ""], ["Wood", "James L. N.", ""]]}, {"id": "1503.07689", "submitter": "Jean-Michel Marin", "authors": "Jean-Michel Marin (U. Montpellier), Pierre Pudlo (Aix-Marseille U.),\n  Arnaud Estoup (CBGP, INRA, Montpellier) and Christian P. Robert (U.\n  Paris-Dauphine and U. Warwick)", "title": "Likelihood-free Model Choice", "comments": "21 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is an invited chapter covering the specificities of ABC model\nchoice, intended for the incoming Handbook of ABC by Sisson, Fan, and Beaumont\n(2017). Beyond exposing the potential pitfalls of ABC based posterior\nprobabilities, the review emphasizes mostly the solution proposed by Pudlo et\nal. (2016) on the use of random forests for aggregating summary statistics and\nand for estimating the posterior probability of the most likely model via a\nsecondary random fores.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 11:17:22 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 07:02:41 GMT"}, {"version": "v3", "created": "Fri, 16 Sep 2016 09:13:14 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Marin", "Jean-Michel", "", "U. Montpellier"], ["Pudlo", "Pierre", "", "Aix-Marseille U."], ["Estoup", "Arnaud", "", "CBGP, INRA, Montpellier"], ["Robert", "Christian P.", "", "U.\n  Paris-Dauphine and U. Warwick"]]}, {"id": "1503.07973", "submitter": "Itai Dattner", "authors": "Itai Dattner and Shota Gugushvili", "title": "Application of one-step method to parameter estimation in ODE models", "comments": "29 pages, 9 tables, 7 figures", "journal-ref": "Statistica Neerlandica 72 (2018), no. 2, 126-156", "doi": "10.1111/stan.12124", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study application of Le Cam's one-step method to parameter\nestimation in ordinary differential equations models. This computationally\nsimple technique can serve as an alternative to numerical evaluation of the\npopular nonlinear least squares estimator, which typically requires the use of\na multi-step iterative algorithm and repetitive numerical integration of the\nODE system. The one-step method starts from a preliminary $\\sqrt{n}$-consistent\nestimator of the parameter of interest and next turns it into an asymptotic (as\nthe sample size $n\\rightarrow\\infty$) equivalent of the least squares estimator\nthrough a numerically straightforward procedure. We demonstrate performance of\nthe one-step estimator via extensive simulations and real data examples. The\nmethod enables the researcher to obtain both point and interval estimates. The\npreliminary $\\sqrt{n}$-consistent estimator that we use depends on\nnonparametric smoothing, and we provide a data driven methodology for choosing\nits tuning parameter and support it by theory. An easy implementation scheme of\nthe one-step method for practical use is pointed out.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 06:36:07 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 13:20:48 GMT"}, {"version": "v3", "created": "Sat, 25 Nov 2017 12:11:50 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Dattner", "Itai", ""], ["Gugushvili", "Shota", ""]]}, {"id": "1503.07990", "submitter": "Anders Ellern Bilgrau", "authors": "Anders Ellern Bilgrau, Rasmus Froberg Br{\\o}ndum, Poul Svante Eriksen,\n  Karen Dybk{\\ae}r, and Martin B{\\o}gsted", "title": "Estimating a common covariance matrix for network meta-analysis of gene\n  expression datasets in diffuse large B-cell lymphoma", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of covariance matrices of gene expressions has many\napplications in cancer systems biology. Many gene expression studies, however,\nare hampered by low sample size and it has therefore become popular to increase\nsample size by collecting gene expression data across studies. Motivated by the\ntraditional meta-analysis using random effects models, we present a\nhierarchical random covariance model and use it for the meta-analysis of gene\ncorrelation networks across 11 large-scale gene expression studies of diffuse\nlarge B-cell lymphoma (DLBCL). We suggest to use a maximum likelihood estimator\nfor the underlying common covariance matrix and introduce an EM algorithm for\nestimation. By simulation experiments comparing the estimated covariance\nmatrices by cophenetic correlation and Kullback-Leibler divergence the\nsuggested estimator showed to perform better or not worse than a simple pooled\nestimator. In a posthoc analysis of the estimated common covariance matrix for\nthe DLBCL data we were able to identify novel biologically meaningful gene\ncorrelation networks with eigengenes of prognostic value. In conclusion, the\nmethod seems to provide a generally applicable framework for meta-analysis,\nwhen multiple features are measured and believed to share a common covariance\nmatrix obscured by study dependent noise.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 08:43:50 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 07:13:57 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 20:34:51 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Bilgrau", "Anders Ellern", ""], ["Br\u00f8ndum", "Rasmus Froberg", ""], ["Eriksen", "Poul Svante", ""], ["Dybk\u00e6r", "Karen", ""], ["B\u00f8gsted", "Martin", ""]]}, {"id": "1503.08148", "submitter": "Bhargab Chattopadhyay", "authors": "Shyamal Krishna De and Bhargab Chattopadhyay", "title": "Minimum Risk Point Estimation of Gini Index", "comments": null, "journal-ref": "Sankhya Series B 2017", "doi": "10.1007/s13571-017-0140-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a theory and methodology for estimation of Gini index\nsuch that both cost of sampling and estimation error are minimum. Methods in\nwhich sample size is fixed in advance, cannot minimize estimation error and\nsampling cost at the same time. In this article, a purely sequential procedure\nis proposed which provides an estimate of the sample size required to achieve a\nsufficiently smaller estimation error and lower sampling cost. Characteristics\nof the purely sequential procedure are examined and asymptotic optimality\nproperties are proved without assuming any specific distribution of the data.\nPerformance of our method is examined through extensive simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 17:05:45 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["De", "Shyamal Krishna", ""], ["Chattopadhyay", "Bhargab", ""]]}, {"id": "1503.08156", "submitter": "Bhargab Chattopadhyay", "authors": "Bhargab Chattopadhyay and Shyamal Krishna De", "title": "Estimation of Gini Index within Pre-Specied Error Bound", "comments": null, "journal-ref": "Econometrics, 2016", "doi": "10.3390/econometrics4030030", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gini index is a widely used measure of economic inequality. This article\ndevelops a general theory for constructing a confidence interval for Gini index\nwith a specified confidence coefficient and a specified width. Fixed sample\nsize methods cannot simultaneously achieve both the specified confidence\ncoefficient and specified width.\n  We develop a purely sequential procedure for interval estimation of Gini\nindex with a specified confidence coefficient and a fixed margin of error.\nOptimality properties of the proposed method, namely first order asymptotic\nefficiency and asymptotic consistency are proved. All theoretical results are\nderived without assuming any specific distribution of the data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 17:13:28 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Chattopadhyay", "Bhargab", ""], ["De", "Shyamal Krishna", ""]]}, {"id": "1503.08195", "submitter": "Tilmann Gneiting", "authors": "Werner Ehm, Tilmann Gneiting, Alexander Jordan, Fabian Kr\\\"uger", "title": "Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet\n  Representations, and Forecast Rankings", "comments": "References updated; a few minor edits in response to initial comments\n  (merely for clarity)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the practice of point prediction, it is desirable that forecasters receive\na directive in the form of a statistical functional, such as the mean or a\nquantile of the predictive distribution. When evaluating and comparing\ncompeting forecasts, it is then critical that the scoring function used for\nthese purposes be consistent for the functional at hand, in the sense that the\nexpected score is minimized when following the directive.\n  We show that any scoring function that is consistent for a quantile or an\nexpectile functional, respectively, can be represented as a mixture of extremal\nscoring functions that form a linearly parameterized family. Scoring functions\nfor the mean value and probability forecasts of binary events constitute\nimportant examples. The quantile and expectile functionals along with the\nrespective extremal scoring functions admit appealing economic interpretations\nin terms of thresholds in decision making.\n  The Choquet type mixture representations give rise to simple checks of\nwhether a forecast dominates another in the sense that it is preferable under\nany consistent scoring function. In empirical settings it suffices to compare\nthe average scores for only a finite number of extremal elements. Plots of the\naverage scores with respect to the extremal scoring functions, which we call\nMurphy diagrams, permit detailed comparisons of the relative merits of\ncompeting forecasts.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 19:35:55 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 14:08:40 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Ehm", "Werner", ""], ["Gneiting", "Tilmann", ""], ["Jordan", "Alexander", ""], ["Kr\u00fcger", "Fabian", ""]]}, {"id": "1503.08196", "submitter": "Gia-Thuy Pham", "authors": "Gia-Thuy Pham and Philippe Loubaton and Pascal Vallet", "title": "Performance analysis of spatial smoothing schemes in the context of\n  large arrays", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2480044", "report-no": null, "categories": "stat.ME cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adresses the statistical behaviour of spatial smoothing subspace\nDoA estimation schemes using a sensor array in the case where the number of\nobservations $N$ is significantly smaller than the number of sensors $M$, and\nthat the smoothing parameter $L$ is such that $M$ and $NL$ are of the same\norder of magnitude. This context is modelled by an asymptotic regime in which\n$NL$ and $M$ both converge towards $\\infty$ at the same rate. As in recent\nworks devoted to the study of (unsmoothed) subspace methods in the case where\n$M$ and $N$ are of the same order of magnitude, it is shown that it is still\npossible to derive improved DoA estimators termed as Generalized-MUSIC with\nspatial smoothing (G-MUSIC SS). The key ingredient of this work is a technical\nresult showing that the largest singular values and corresponding singular\nvectors of low rank deterministic perturbation of certain Gaussian block-Hankel\nlarge random matrices behave as if the entries of the latter random matrices\nwere independent identically distributed. This allows to conclude that when the\nnumber of sources and their DoA do not scale with $M,N,L,$ a situation\nmodelling widely spaced DoA scenarios, then both traditional and Generalized\nspatial smoothing subspace methods provide consistent DoA estimators whose\nconvergence speed is faster than $\\frac{1}{M}$. The case of DoA that are spaced\nof the order of a beamwidth, which models closely spaced sources, is also\nconsidered. It is shown that the convergence speed of G-MUSIC SS estimates is\nunchanged, but that it is no longer the case for MUSIC SS ones.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 18:48:22 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Pham", "Gia-Thuy", ""], ["Loubaton", "Philippe", ""], ["Vallet", "Pascal", ""]]}, {"id": "1503.08278", "submitter": "Stefano Favaro", "authors": "M. De Iorio, L.T. Elliott, S. Favaro, K. Adhikari, Y.W. Teh", "title": "Modeling population structure under hierarchical Dirichlet processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric model to infer population admixture,\nextending the Hierarchical Dirichlet Process to allow for correlation between\nloci due to Linkage Disequilibrium. Given multilocus genotype data from a\nsample of individuals, the model allows inferring classifying individuals as\nunadmixed or admixed, inferring the number of subpopulations ancestral to an\nadmixed population and the population of origin of chromosomal regions. Our\nmodel does not assume any specific mutation process and can be applied to most\nof the commonly used genetic markers. We present a MCMC algorithm to perform\nposterior inference from the model and discuss methods to summarise the MCMC\noutput for the analysis of population admixture. We demonstrate the performance\nof the proposed model in simulations and in a real application, using genetic\ndata from the EDAR gene, which is considered to be ancestry-informative due to\nwell-known variations in allele frequency as well as phenotypic effects across\nancestry. The structure analysis of this dataset leads to the identification of\na rare haplotype in Europeans.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 07:41:16 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["De Iorio", "M.", ""], ["Elliott", "L. T.", ""], ["Favaro", "S.", ""], ["Adhikari", "K.", ""], ["Teh", "Y. W.", ""]]}, {"id": "1503.08340", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan and Daniela Witten", "title": "Statistical Properties of Convex Clustering", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we study the statistical properties of convex clustering.\nWe establish that convex clustering is closely related to single linkage\nhierarchical clustering and $k$-means clustering. In addition, we derive the\nrange of tuning parameter for convex clustering that yields a non-trivial\nsolution. We also provide an unbiased estimate of the degrees of freedom, and\nprovide a finite sample bound for the prediction error for convex clustering.\nWe compare convex clustering to some traditional clustering methods in\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 19:47:51 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2015 01:02:49 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Tan", "Kean Ming", ""], ["Witten", "Daniela", ""]]}, {"id": "1503.08348", "submitter": "Ravi Ganti", "authors": "Ravi Ganti and Rebecca M. Willett", "title": "Sparse Linear Regression With Missing Data", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fast and accurate method for sparse regression in the\npresence of missing data. The underlying statistical model encapsulates the\nlow-dimensional structure of the incomplete data matrix and the sparsity of the\nregression coefficients, and the proposed algorithm jointly learns the\nlow-dimensional structure of the data and a linear regressor with sparse\ncoefficients. The proposed stochastic optimization method, Sparse Linear\nRegression with Missing Data (SLRM), performs an alternating minimization\nprocedure and scales well with the problem size. Large deviation inequalities\nshed light on the impact of the various problem-dependent parameters on the\nexpected squared loss of the learned regressor. Extensive simulations on both\nsynthetic and real datasets show that SLRM performs better than competing\nalgorithms in a variety of contexts.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 21:03:32 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ganti", "Ravi", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1503.08357", "submitter": "Maria Terres", "authors": "Maria A. Terres and Alan E. Gelfand", "title": "Spatial Process Gradients and Their Use in Sensitivity Analysis for\n  Environmental Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops methodology for local sensitivity analysis based on\ndirectional derivatives associated with spatial processes. Formal gradient\nanalysis for spatial processes was elaborated in previous papers, focusing on\ndistribution theory for directional derivatives associated with a response\nvariable assumed to follow a Gaussian process model. In the current work, these\nideas are extended to additionally accommodate a continuous covariate whose\ndirectional derivatives are also of interest and to relate the behavior of the\ndirectional derivatives of the response surface to those of the covariate\nsurface. It is of interest to assess whether, in some sense, the gradients of\nthe response follow those of the explanatory variable. The joint Gaussian\nstructure of all variables, including the directional derivatives, allows for\nexplicit distribution theory and, hence, kriging across the spatial region\nusing multivariate normal theory. Working within a Bayesian hierarchical\nmodeling framework, posterior samples enable all gradient analysis to occur\npost model fitting. As a proof of concept, we show how our methodology can be\napplied to a standard geostatistical modeling setting using a simulation\nexample. For a real data illustration, we work with point pattern data,\ndeferring our gradient analysis to the intensity surface, adopting a\nlog-Gaussian Cox process model. In particular, we relate elevation data to\npoint patterns associated with several tree species in Duke Forest.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 22:08:53 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Terres", "Maria A.", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1503.08399", "submitter": "Francisco Louzada Professor", "authors": "Pedro L. Ramos, Francisco Louzada, Vicente G. Cancho", "title": "Maximum Likelihood Estimation for the Weight Lindley Distribution\n  Parameters under Different Types of Censoring", "comments": "19 pgs", "journal-ref": null, "doi": null, "report-no": "ArXiv-FL-2015-02", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the maximum likelihood equations for the parameters of the\nWeight Lindley distribution are studied considering different types of\ncensoring, such as, type I, type II and random censoring mechanism. A numerical\nsimulation study is perform to evaluate the maximum likelihood estimates. The\nproposed methodology is illustrated in a real data set.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 08:40:03 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ramos", "Pedro L.", ""], ["Louzada", "Francisco", ""], ["Cancho", "Vicente G.", ""]]}, {"id": "1503.08503", "submitter": "Asaf Weinstein", "authors": "Asaf Weinstein, Zhuang Ma, Lawrence D. Brown, Cun-Hui Zhang", "title": "Group-Linear Empirical Bayes Estimates for a Heteroscedastic Normal Mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the mean of a normal vector with known but unequal\nvariances introduces substantial difficulties that impair the adequacy of\ntraditional empirical Bayes estimators. By taking a different approach, that\ntreats the known variances as part of the random observations, we restore\nsymmetry and thus the effectiveness of such methods. We suggest a group-linear\nempirical Bayes estimator, which collects observations with similar variances\nand applies a spherically symmetric estimator to each group separately. The\nproposed estimator is motivated by a new oracle rule which is stronger than the\nbest linear rule, and thus provides a more ambitious benchmark than that\nconsidered in previous literature. Our estimator asymptotically achieves the\nnew oracle risk (under appropriate conditions) and at the same time is minimax.\nThe group-linear estimator is particularly advantageous in situations where the\ntrue means and observed variances are empirically dependent. To demonstrate the\nmerits of the proposed methods in real applications, we analyze the baseball\ndata used in Brown (2008), where the group-linear methods achieved the\nprediction error of the best nonparametric estimates that have been applied to\nthe dataset, and significantly lower error than other parametric and\nsemi-parametric empirical Bayes estimators.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 22:31:14 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 00:20:51 GMT"}, {"version": "v3", "created": "Tue, 3 Jan 2017 14:04:00 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Weinstein", "Asaf", ""], ["Ma", "Zhuang", ""], ["Brown", "Lawrence D.", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1503.08610", "submitter": "Holger Dette", "authors": "Holger Dette and Weichi Wu and Zhou Zhou", "title": "Change point analysis of second order characteristics in non-stationary\n  time series", "comments": "64 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important assumption in the work on testing for structural breaks in time\nseries consists in the fact that the model is formulated such that the\nstochastic process under the null hypothesis of \"no change-point\" is\nstationary. This assumption is crucial to derive (asymptotic) critical values\nfor the corresponding testing procedures using an elegant and powerful\nmathematical theory, but it might be not very realistic from a practical point\nof view.\n  This paper develops change point analysis under less restrictive assumptions\nand deals with the problem of detecting change points in the marginal variance\nand correlation structures of a non-stationary time series. A CUSUM approach is\nproposed, which is used to test the \"classical\" hypothesis of the form $H_0:\n\\theta_1=\\theta_2$ vs. $H_1: \\theta_1 \\not =\\theta_2$, where $\\theta_1$ and\n$\\theta_2$ denote second order parameters of the process before and after a\nchange point. The asymptotic distribution of the CUSUM test statistic is\nderived under the null hypothesis. This distribution depends in a complicated\nway on the dependency structure of the nonlinear non-stationary time series and\na bootstrap approach is developed to generate critical values. The results are\nthen extended to test the hypothesis of a {\\it non relevant change point}, i.e.\n$H_0: | \\theta_1-\\theta_2 | \\leq \\delta$, which reflects the fact that\ninference should not be changed, if the difference between the parameters\nbefore and after the change-point is small.\n  In contrast to previous work, our approach does neither require the mean to\nbe constant nor - in the case of testing for lag $k$-correlation - that the\nmean, variance and fourth order joint cumulants are constant under the null\nhypothesis. In particular, we allow that the variance has a change point at a\ndifferent location than the auto-covariance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 09:28:51 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Dette", "Holger", ""], ["Wu", "Weichi", ""], ["Zhou", "Zhou", ""]]}, {"id": "1503.08621", "submitter": "Minh-Ngoc Tran", "authors": "Minh-Ngoc Tran, David J. Nott, Robert Kohn", "title": "Variational Bayes with Intractable Likelihood", "comments": "40 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is rapidly becoming a popular tool for Bayesian\ninference in statistical modeling. However, the existing VB algorithms are\nrestricted to cases where the likelihood is tractable, which precludes the use\nof VB in many interesting situations such as in state space models and in\napproximate Bayesian computation (ABC), where application of VB methods was\npreviously impossible. This paper extends the scope of application of VB to\ncases where the likelihood is intractable, but can be estimated unbiasedly. The\nproposed VB method therefore makes it possible to carry out Bayesian inference\nin many statistical applications, including state space models and ABC. The\nmethod is generic in the sense that it can be applied to almost all statistical\nmodels without requiring too much model-based derivation, which is a drawback\nof many existing VB algorithms. We also show how the proposed method can be\nused to obtain highly accurate VB approximations of marginal posterior\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 09:57:37 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 12:30:59 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Nott", "David J.", ""], ["Kohn", "Robert", ""]]}, {"id": "1503.08650", "submitter": "Juho Piironen", "authors": "Juho Piironen, Aki Vehtari", "title": "Comparison of Bayesian predictive methods for model selection", "comments": "A few minor changes; added a few sentences, corrected some\n  grammatical errors and modified Figure 7", "journal-ref": "Statistics and Computing, 2017, Volume 27, Issue 3, 711-735", "doi": "10.1007/s11222-016-9649-y", "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to compare several widely used Bayesian model\nselection methods in practical model selection problems, highlight their\ndifferences and give recommendations about the preferred approaches. We focus\non the variable subset selection for regression and classification and perform\nseveral numerical experiments using both simulated and real world data. The\nresults show that the optimization of a utility estimate such as the\ncross-validation (CV) score is liable to finding overfitted models due to\nrelatively high variance in the utility estimates when the data is scarce. This\ncan also lead to substantial selection induced bias and optimism in the\nperformance evaluation for the selected model. From a predictive viewpoint,\nbest results are obtained by accounting for model uncertainty by forming the\nfull encompassing model, such as the Bayesian model averaging solution over the\ncandidate models. If the encompassing model is too complex, it can be robustly\nsimplified by the projection method, in which the information of the full model\nis projected onto the submodels. This approach is substantially less prone to\noverfitting than selection based on CV-score. Overall, the projection method\nappears to outperform also the maximum a posteriori model and the selection of\nthe most probable variables. The study also demonstrates that the model\nselection can greatly benefit from using cross-validation outside the searching\nprocess both for guiding the model size selection and assessing the predictive\nperformance of the finally selected model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 11:58:20 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 12:34:51 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 13:27:27 GMT"}, {"version": "v4", "created": "Wed, 23 Mar 2016 11:28:15 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Piironen", "Juho", ""], ["Vehtari", "Aki", ""]]}, {"id": "1503.08658", "submitter": "Melanie Prague", "authors": "M. Prague, D. Commenges, J.M. Gran, B. Ledergerber, J. young, H.\n  Furrer and R. Thi\\'ebaut", "title": "Dynamic models for estimating the effect of HAART on CD4 in\n  observational studies: application to the Aquitaine Cohort study and the\n  Swiss HIV Cohort Study", "comments": "22 pages, 2 figures 4 tables, article submitted in a Biometrics\n  practice journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly active antiretroviral therapy (HAART) has proved efficient in\nincreasing CD4 counts in many randomized clinical trials. Because randomized\ntrials have some limitations (e.g., short duration, highly selected subjects),\nit is interesting to assess it using observational studies. This is challenging\nbecause treatment is started preferentially in subjects with severe conditions,\nin particular in subjects with low CD4 counts. This general problem had been\ntreated using Marginal Structural Models (MSM) relying on the counterfactual\nformulation. Another approach to causality is based on dynamical models. First,\nwe present three discrete-time dynamic models based on linear increments (LIM):\nthe simplest model is described by one difference equation for CD4 counts; the\nsecond has an equilibrium point; the third model is based on a system of two\ndifference equations which allows jointly modeling CD4 counts and viral load.\nThen we consider continuous time models based on ordinary differential\nequations with random effects (ODE-NLME). These mechanistic models allow\nincorporating biological knowledge when available, which leads to increased\npower for detecting treatment effect. Inference in ODE-NLME models, however, is\nchallenging from a numerical point of view, and requires specific methods and\nsoftwares. LIMs are a valuable intermediary option in terms of consistency,\nprecision and complexity. The different approaches are compared in simulation\nand applied to HIV cohorts (the ANRS CO3 Aquitaine Cohort and the Swiss HIV\nCohort Study).\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 12:52:55 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 13:22:25 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 15:41:26 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Prague", "M.", ""], ["Commenges", "D.", ""], ["Gran", "J. M.", ""], ["Ledergerber", "B.", ""], ["young", "J.", ""], ["Furrer", "H.", ""], ["Thi\u00e9baut", "R.", ""]]}, {"id": "1503.08886", "submitter": "Luis Carvalho", "authors": "Hunter Glanz, Xiaoman Huang, Minhui Zheng, and Luis E. Carvalho", "title": "A Bayesian Change Point Model for Detecting Land Cover Changes in MODIS\n  Time Series", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As both a central task in Remote Sensing and a common problem in many other\nsituations involving time series data, change point detection boasts a thorough\nand well-documented history of study. However, the treatment of missing data\nand proper exploitation of the structure in multivariate time series during\nchange point detection remains lacking. Multispectral, high temporal resolution\ntime series data from NASA's Moderate Resolution Imaging Spectroradiometer\n(MODIS) instruments provide an attractive and challenging context to contribute\nto the change point detection literature. In an effort to better monitor change\nin land cover using MODIS data, we present a novel approach to identifying\nperiods of time in which regions experience some conversion-type of land cover\nchange. That is, we propose a method for parameter estimation and change point\ndetection in the presence of missing data which capitalizes on the high\ndimensionality of MODIS data. We test the quality of our method in a simulation\nstudy alongside a contemporary change point method and apply it in a case study\nat the Xingu River Basin in the Amazon. Not only does our method maintain a\nhigh accuracy, but can provide insight into the types of changes occurring via\nland cover conversion probabilities. In this way we can better characterize the\namount and types of forest disturbance in our study area in comparison to\ntraditional change point methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 02:22:27 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Glanz", "Hunter", ""], ["Huang", "Xiaoman", ""], ["Zheng", "Minhui", ""], ["Carvalho", "Luis E.", ""]]}]