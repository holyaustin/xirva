[{"id": "1811.00153", "submitter": "Pritam Ranjan", "authors": "Ru Zhang, Chunfang Devon Lin, Pritam Ranjan", "title": "A Sequential Design Approach for Calibrating a Dynamic Population Growth\n  Model", "comments": "36 pages", "journal-ref": "SIAM/ASA J. Uncertainty Quantification, 7(4), 1245 -1274, 2019", "doi": "10.1137/18M1224544", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive understanding of the population growth of a variety of pests\nis often crucial for efficient crop management. Our motivating application\ncomes from calibrating a two-delay blowfly (TDB) model which is used to\nsimulate the population growth of Panonychus ulmi (Koch) or European red mites\nthat infest on apple leaves and diminish the yield. We focus on the inverse\nproblem, that is, to estimate the set of parameters/inputs of the TDB model\nthat produces the computer model output matching the field observation as\nclosely as possible. The time series nature of both the field observation and\nthe TDB outputs makes the inverse problem significantly more challenging than\nin the scalar valued simulator case.\n  In spirit, we follow the popular sequential design framework of computer\nexperiments. However, due to the time-series response, a singular value\ndecomposition based Gaussian process model is used for the surrogate model, and\nsubsequently, a new expected improvement criterion is developed for choosing\nthe follow-up points. We also propose a new criterion for extracting the\noptimal inverse solution from the final surrogate. Three simulated examples and\nthe real-life TDB calibration problem have been used to demonstrate higher\naccuracy of the proposed approach as compared to popular existing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:22:04 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zhang", "Ru", ""], ["Lin", "Chunfang Devon", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1811.00203", "submitter": "Stefanos Kechagias", "authors": "Yisu Jia, Stefanos Kechagias, James Livsey, Robert Lund, Vladas\n  Pipiras", "title": "Latent Gaussian Count Time Series", "comments": "Two previous versions of this paper appeared on arxiv under the\n  title, the first under the title \"Latent Gaussian Count Time Series Modeling\"\n  and the second under the title \"Count Time Series Modeling with Gaussian\n  Copulas\"", "journal-ref": null, "doi": "10.1080/01621459.2021.1944874", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops the theory and methods for modeling a stationary count\ntime series via Gaussian transformations. The techniques use a latent Gaussian\nprocess and a distributional transformation to construct stationary series with\nvery flexible correlation features that can have any pre-specified marginal\ndistribution, including the classical Poisson, generalized Poisson, negative\nbinomial, and binomial structures. Gaussian pseudo-likelihood and implied\nYule-Walker estimation paradigms, based on the autocovariance function of the\ncount series, are developed via a new Hermite expansion. Particle filtering and\nsequential Monte Carlo methods are used to conduct likelihood estimation.\nConnections to state space models are made. Our estimation approaches are\nevaluated in a simulation study and the methods are used to analyze a count\nseries of weekly retail sales.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:19:10 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 18:25:32 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 02:33:59 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Jia", "Yisu", ""], ["Kechagias", "Stefanos", ""], ["Livsey", "James", ""], ["Lund", "Robert", ""], ["Pipiras", "Vladas", ""]]}, {"id": "1811.00306", "submitter": "Haeran Cho Dr", "authors": "Matteo Barigozzi, Haeran Cho", "title": "Consistent estimation of high-dimensional factor models when the factor\n  number is over-estimated", "comments": null, "journal-ref": null, "doi": "10.1214/20-EJS1741", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high-dimensional $r$-factor model for an $n$-dimensional vector time series\nis characterised by the presence of a large eigengap (increasing with $n$)\nbetween the $r$-th and the $(r+1)$-th largest eigenvalues of the covariance\nmatrix. Consequently, Principal Component (PC) analysis is the most popular\nestimation method for factor models and its consistency, when $r$ is correctly\nestimated, is well-established in the literature. However, popular factor\nnumber estimators often suffer from the lack of an obvious eigengap in\nempirical eigenvalues and tend to over-estimate $r$ due, for example, to the\nexistence of non-pervasive factors affecting only a subset of the series. We\nshow that the errors in the PC estimators resulting from the over-estimation of\n$r$ are non-negligible, which in turn lead to the violation of the conditions\nrequired for factor-based large covariance estimation. To remedy this, we\npropose new estimators of the factor model based on scaling the entries of the\nsample eigenvectors. We show both theoretically and numerically that the\nproposed estimators successfully control for the over-estimation error, and\ninvestigate their performance when applied to risk minimisation of a portfolio\nof financial time series.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 10:36:27 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 16:41:51 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 21:07:17 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2020 17:01:55 GMT"}, {"version": "v5", "created": "Mon, 6 Jul 2020 18:32:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Cho", "Haeran", ""]]}, {"id": "1811.00439", "submitter": "Marco Doretti", "authors": "Marco Doretti, Martina Raggi and Elena Stanghellini", "title": "Exact parametric causal mediation analysis for a binary outcome with a\n  binary mediator", "comments": "24 pages, 5 figures", "journal-ref": "Statistical Methods & Applications (2021)", "doi": "10.1007/s10260-021-00562-w", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parametric expression for causal natural direct and indirect effects is\nderived for the setting of a binary outcome with a binary mediator. The\nproposed effect decomposition does not require the outcome to be rare and\ngeneralizes the existing ones, allowing for interactions between both the\nexposure and the mediator and confounding covariates. Further, it outlines the\nrelationship between the causal effects and the correspondent pathway-specific\nlogistic regression parameters, in parallel with results derived under the rare\noutcome assumption. Formulae for standard errors, obtained via the delta\nmethod, are also given. A simulation study is implemented which compares these\nestimators to a number of competing ones. An empirical application to data\ncoming from a microfinance experiment performed in Bosnia and Herzegovina is\nillustrated as an example.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:36:31 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 12:05:09 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 15:27:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Doretti", "Marco", ""], ["Raggi", "Martina", ""], ["Stanghellini", "Elena", ""]]}, {"id": "1811.00457", "submitter": "Elea McDonnell Feit", "authors": "Elea McDonnell Feit and Ron Berman", "title": "Test & Roll: Profit-Maximizing A/B Tests", "comments": null, "journal-ref": null, "doi": "10.1287/mksc.2019.1194", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marketers often use A/B testing as a tool to compare marketing treatments in\na test stage and then deploy the better-performing treatment to the remainder\nof the consumer population. While these tests have traditionally been analyzed\nusing hypothesis testing, we re-frame them as an explicit trade-off between the\nopportunity cost of the test (where some customers receive a sub-optimal\ntreatment) and the potential losses associated with deploying a sub-optimal\ntreatment to the remainder of the population.\n  We derive a closed-form expression for the profit-maximizing test size and\nshow that it is substantially smaller than typically recommended for a\nhypothesis test, particularly when the response is noisy or when the total\npopulation is small. The common practice of using small holdout groups can be\nrationalized by asymmetric priors. The proposed test design achieves nearly the\nsame expected regret as the flexible, yet harder-to-implement multi-armed\nbandit under a wide range of conditions.\n  We demonstrate the benefits of the method in three different marketing\ncontexts -- website design, display advertising and catalog tests -- in which\nwe estimate priors from past data. In all three cases, the optimal sample sizes\nare substantially smaller than for a traditional hypothesis test, resulting in\nhigher profit.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:53:28 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 22:22:41 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Feit", "Elea McDonnell", ""], ["Berman", "Ron", ""]]}, {"id": "1811.00462", "submitter": "Jie Yang", "authors": "Keren Li and Jie Yang", "title": "Score-Matching Representative Approach for Big Data Analysis with\n  Generalized Linear Models", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast and efficient strategy, called the representative approach,\nfor big data analysis with linear models and generalized linear models. With a\ngiven partition of big dataset, this approach constructs a representative data\npoint for each data block and fits the target model using the representative\ndataset. In terms of time complexity, it is as fast as the subsampling\napproaches in the literature. As for efficiency, its accuracy in estimating\nparameters is better than the divide-and-conquer method. With comprehensive\nsimulation studies and theoretical justifications, we recommend two\nrepresentative approaches. For linear models or generalized linear models with\na flat inverse link function and moderate coefficients of continuous variables,\nwe recommend mean representatives (MR). For other cases, we recommend\nscore-matching representatives (SMR). As an illustrative application to the\nAirline on-time performance data, MR and SMR are as good as the full data\nestimate when available. Furthermore, the proposed representative strategy is\nideal for analyzing massive data dispersed over a network of interconnected\ncomputers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:01:46 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Li", "Keren", ""], ["Yang", "Jie", ""]]}, {"id": "1811.00488", "submitter": "Lily Wang", "authors": "Xinyi Li and Li Wang and Dan Nettleton", "title": "Sparse Model Identification and Learning for Ultra-high-dimensional\n  Additive Partially Linear Models", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 2019", "doi": "10.1016/j.jmva.2019.02.010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The additive partially linear model (APLM) combines the flexibility of\nnonparametric regression with the parsimony of regression models, and has been\nwidely used as a popular tool in multivariate nonparametric regression to\nalleviate the \"curse of dimensionality\". A natural question raised in practice\nis the choice of structure in the nonparametric part, that is, whether the\ncontinuous covariates enter into the model in linear or nonparametric form. In\nthis paper, we present a comprehensive framework for simultaneous sparse model\nidentification and learning for ultra-high-dimensional APLMs where both the\nlinear and nonparametric components are possibly larger than the sample size.\nWe propose a fast and efficient two-stage procedure. In the first stage, we\ndecompose the nonparametric functions into a linear part and a nonlinear part.\nThe nonlinear functions are approximated by constant spline bases, and a triple\npenalization procedure is proposed to select nonzero components using adaptive\ngroup LASSO. In the second stage, we refit data with selected covariates using\nhigher order polynomial splines, and apply spline-backfitted local-linear\nsmoothing to obtain asymptotic normality for the estimators. The procedure is\nshown to be consistent for model structure identification. It can identify\nzero, linear, and nonlinear components correctly and efficiently. Inference can\nbe made on both linear coefficients and nonparametric functions. We conduct\nsimulation studies to evaluate the performance of the method and apply the\nproposed method to a dataset on the Shoot Apical Meristem (SAM) of maize\ngenotypes for illustration.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:43:51 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Xinyi", ""], ["Wang", "Li", ""], ["Nettleton", "Dan", ""]]}, {"id": "1811.00638", "submitter": "Tyler VanderWeele", "authors": "Tyler J. VanderWeele and Yige Li", "title": "Simple Sensitivity Analysis for Differential Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple sensitivity analysis results are given for differential measurement\nerror of either the exposure or the outcome. In the case of differential\nmeasurement error of the outcome it is shown that the true effect of the\nexposure on the outcome on the risk ratio scale must be at least as large as\nthe observed association between the exposure and the mis-measured outcome\ndivided by the maximum strength of differential measurement error, assessed as\nthe risk ratio of the controlled direct effect of the exposure on mis-measured\noutcome not through the true outcome. In the case of differential measurement\nerror of the exposure it is shown that the true effect on the risk ratio scale\nof the exposure on the outcome must be at least as large as the observed\nassociation between the mis-measured exposure measurement and the outcome\ndivided by the maximum strength of differential measurement error, assessed as\nthe risk ratio of the effect of the outcome on mis-measured exposure\nmeasurement conditional on the true exposure. The results can also be\nimmediately used to indicate the minimum strength of differential measurement\nerror that would be needed to explain away an observed association between an\nexposure measurement and an outcome measurement.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 21:15:39 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 23:18:59 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["VanderWeele", "Tyler J.", ""], ["Li", "Yige", ""]]}, {"id": "1811.00645", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, Victor Veitch, Haoran Zhang, Raul Rabadan, David M.\n  Blei", "title": "The Holdout Randomization Test for Feature Selection in Black Box Models", "comments": "New algorithms and simulations; accepted for publication at JCGS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the holdout randomization test (HRT), an approach to feature\nselection using black box predictive models. The HRT is a specialized version\nof the conditional randomization test (CRT; Candes et al., 2018) that uses data\nsplitting for feasible computation. The HRT works with any predictive model and\nproduces a valid $p$-value for each feature. To make the HRT more practical, we\npropose a set of extensions to maximize power and speed up computation. In\nsimulations, these extensions lead to greater power than a competing\nknockoffs-based approach, without sacrificing control of the error rate. We\napply the HRT to two case studies from the scientific literature where\nheuristics were originally used to select important features for predictive\nmodels. The results illustrate how such heuristics can be misleading relative\nto principled methods like the HRT. Code is available at\nhttps://github.com/tansey/hrt.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 21:47:43 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 21:39:34 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 12:47:15 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 15:41:50 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tansey", "Wesley", ""], ["Veitch", "Victor", ""], ["Zhang", "Haoran", ""], ["Rabadan", "Raul", ""], ["Blei", "David M.", ""]]}, {"id": "1811.00863", "submitter": "Julien Fageot", "authors": "Julien Fageot, Virginie Uhlmann, Zsuzsanna P\\\"usp\\\"oki, Benjamin Beck,\n  Michael Unser, Adrien Depeursinge", "title": "Principled Design and Implementation of Steerable Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a complete pipeline for the detection of patterns of interest in\nan image. In our approach, the patterns are assumed to be adequately modeled by\na known template, and are located at unknown positions and orientations that we\naim at retrieving. We propose a continuous-domain additive image model, where\nthe analyzed image is the sum of the patterns to localize and a background with\nself-similar isotropic power-spectrum. We are then able to compute the optimal\nfilter fulfilling the SNR criterion based on one single template and background\npair: it strongly responds to the template while being optimally decoupled from\nthe background model. In addition, we constrain our filter to be steerable,\nwhich allows for a fast template detection together with orientation\nestimation. In practice, the implementation requires to discretize a\ncontinuous-domain formulation on polar grids, which is performed using\nquadratic radial B-splines. We demonstrate the practical usefulness of our\nmethod on a variety of template approximation and pattern detection\nexperiments. We show that the detection performance drastically improves when\nwe exploit the statistics of the background via its power-spectrum decay, which\nwe refer to as spectral-shaping. The proposed scheme outperforms\nstate-of-the-art steerable methods by up to 50% of absolute detection\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:38:30 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 15:39:06 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Fageot", "Julien", ""], ["Uhlmann", "Virginie", ""], ["P\u00fcsp\u00f6ki", "Zsuzsanna", ""], ["Beck", "Benjamin", ""], ["Unser", "Michael", ""], ["Depeursinge", "Adrien", ""]]}, {"id": "1811.01076", "submitter": "Rajen Shah", "authors": "Rajen D. Shah, Benjamin Frot, Gian-Andrea Thanei and Nicolai\n  Meinshausen", "title": "RSVP-graphs: Fast High-dimensional Covariance Matrix Estimation under\n  Latent Confounding", "comments": "49 pages; to appear in JRSSB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of estimating a high-dimensional $p\n\\times p$ covariance matrix $\\Sigma$, given $n$ observations of confounded data\nwith covariance $\\Sigma + \\Gamma \\Gamma^T$, where $\\Gamma$ is an unknown $p\n\\times q$ matrix of latent factor loadings. We propose a simple and scalable\nestimator based on the projection on to the right singular vectors of the\nobserved data matrix, which we call RSVP. Our theoretical analysis of this\nmethod reveals that in contrast to PCA-based approaches, RSVP is able to cope\nwell with settings where the smallest eigenvalue of $\\Gamma^T \\Gamma$ is close\nto the largest eigenvalue of $\\Sigma$, as well as settings where the\neigenvalues of $\\Gamma^T \\Gamma$ are diverging fast. It is also able to handle\ndata that may have heavy tails and only requires that the data has an\nelliptical distribution. RSVP does not require knowledge or estimation of the\nnumber of latent factors $q$, but only recovers $\\Sigma$ up to an unknown\npositive scale factor. We argue this suffices in many applications, for example\nif an estimate of the correlation matrix is desired. We also show that by using\nsubsampling, we can further improve the performance of the method. We\ndemonstrate the favourable performance of RSVP through simulation experiments\nand an analysis of gene expression datasets collated by the GTEX consortium.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 20:23:20 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 10:24:30 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 16:00:42 GMT"}, {"version": "v4", "created": "Fri, 29 Nov 2019 20:07:48 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Shah", "Rajen D.", ""], ["Frot", "Benjamin", ""], ["Thanei", "Gian-Andrea", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1811.01117", "submitter": "Weixin Yao", "authors": "Yanyuan Ma, Shaoli Wang, Lin Xu, Weixin Yao", "title": "Semiparametric Mixture Regression with Unspecified Error Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fitting a mixture of linear regression models, normal assumption is\ntraditionally used to model the error and then regression parameters are\nestimated by the maximum likelihood estimators (MLE). This procedure is not\nvalid if the normal assumption is violated. To relax the normal assumption on\nthe error distribution hence reduce the modeling bias, we propose\nsemiparametric mixture of linear regression models with unspecified error\ndistributions. We establish a more general identifiability result under weaker\nconditions than existing results, construct a class of new estimators, and\nestablish their asymptotic properties. These asymptotic results also apply to\nmany existing semiparametric mixture regression estimators whose asymptotic\nproperties have remained unknown due to the inherent difficulties in obtaining\nthem. Using simulation studies, we demonstrate the superiority of the proposed\nestimators over the MLE when the normal error assumption is violated and the\ncomparability when the error is normal. Analysis of a newly collected Equine\nInfectious Anemia Virus data in 2017 is employed to illustrate the usefulness\nof the new estimator.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 22:55:32 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ma", "Yanyuan", ""], ["Wang", "Shaoli", ""], ["Xu", "Lin", ""], ["Yao", "Weixin", ""]]}, {"id": "1811.01261", "submitter": "Jonathan Levy", "authors": "Jonathan Levy", "title": "Canonical Least Favorable Submodels:A New TMLE Procedure for\n  Multidimensional Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a fundamental addition to the world of targeted maximum\nlikelihood estimation (TMLE) (or likewise, targeted minimum loss estimation)\nfor simultaneous estimation of multi-dimensional parameters of interest. TMLE,\nas part of the targeted learning framework, offers a crucial step in\nconstructing efficient plug-in estimators for nonparametric or semiparametric\nmodels. The so-called targeting step of targeted learning, involves fluctuating\nthe initial fit of the model in a way that maximally adjusts the plug-in\nestimate per change in the log likelihood. Previously for multidimensional\nparameters of interest, iterative TMLE's were constructed using locally least\nfavorable submodels as defined in van der Laan and Gruber, 2016, which are\nindexed by a multidimensional fluctuation parameter. In this paper we define a\ncanonical least favorable submodel in terms of a single dimensional epsilon for\na $d$-dimensional parameter of interest. One can view the clfm as the iterative\nanalog to the one-step TMLE as constructed in van der Laan and Gruber, 2016. It\nis currently implemented in several software packages we provide in the last\nsection. Using a single epsilon for the targeting step in TMLE could be useful\nfor high dimensional parameters, where using a fluctuation parameter of the\nsame dimension as the parameter of interest could suffer the consequences of\ncurse of dimensionality. The clfm also enables placing the so-called clever\ncovariate denominator as an inverse weight in an offset intercept model. It has\nbeen shown that such weighting mitigates the effect of large inverse weights\nsometimes caused by near positivity violations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 18:06:44 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 03:02:41 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 04:29:09 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Levy", "Jonathan", ""]]}, {"id": "1811.01280", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Nonparametric Spectral Methods for Multivariate Spatial and\n  Spatial-Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose computationally efficient methods for estimating stationary\nmultivariate spatial and spatial-temporal spectra from incomplete gridded data.\nThe methods are iterative and rely on successive imputation of data and\nupdating of model estimates. Imputations are done according to a periodic model\non an expanded domain. The periodicity of the imputations is a key feature that\nreduces edge effects in the periodogram and is facilitated by efficient\ncirculant embedding techniques. In addition, we describe efficient methods for\ndecomposing the estimated cross spectral density function into a linear model\nof coregionalization plus a residual process. The methods are applied to two\nstorm datasets, one of which is from Hurricane Florence, which struck the\nsouteastern United States in September 2018. The application demonstrates how\nfitted models from different datasets can be compared, and how the methods are\ncomputationally feasible on datasets with more than 200,000 total observations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 20:22:52 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1811.01301", "submitter": "Jacqueline Mauro", "authors": "Jacqueline A Mauro, Edward H Kennedy, Daniel Nagin", "title": "Instrumental Variable Methods using Dynamic Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on dynamic interventions has greatly expanded the range of causal\nquestions researchers can study while weakening identifying assumptions and\nyielding effects that are more practically relevant. However, most work in\ndynamic interventions to date has focused on settings where we directly alter\nsome unconfounded treatment of interest. In policy analysis, decision makers\nrarely have this level of control over behaviors or access to experimental\ndata. Instead, they are often faced with treatments they can affect only\nindirectly and whose effects must be learned from observational data. In this\npaper, we propose new estimands and estimators of causal effects based on\ndynamic interventions with instrumental variables. This method does not rely on\nparametric models and does not require an experiment. Instead, we estimate the\neffect of a dynamic intervention on the instrument. This robustness should\nreassure policy makers that these estimates can be used to effectively inform\npolicy. We demonstrate the usefulness of this estimation strategy in a case\nstudy examining the effect of visitation on recidivism.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 23:57:19 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 21:05:58 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Mauro", "Jacqueline A", ""], ["Kennedy", "Edward H", ""], ["Nagin", "Daniel", ""]]}, {"id": "1811.01384", "submitter": "Jong Hee Park", "authors": "Jong Hee Park and Yunkyu Sohn", "title": "Detecting Structural Changes in Longitudinal Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic modeling of longitudinal networks has been an increasingly important\ntopic in applied research. While longitudinal network data commonly exhibit\ndramatic changes in its structures, existing methods have largely focused on\nmodeling smooth topological changes over time. In this paper, we develop a\nhidden Markov multilinear tensor model (HMTM) that combines the multilinear\ntensor regression model (Hoff 2011) with a hidden Markov model using Bayesian\ninference. We model changes in network structure as shifts in discrete states\nyielding particular sets of network generating parameters. Our simulation\nresults demonstrate that the proposed method correctly detects the number,\nlocations, and types of changes in latent node characteristics. We apply the\nproposed method to international military alliance networks to find structural\nchanges in the coalition structure among nations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 15:12:03 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Park", "Jong Hee", ""], ["Sohn", "Yunkyu", ""]]}, {"id": "1811.01429", "submitter": "Cody Carroll", "authors": "Cody Carroll, Hans-Georg M\\\"uller, and Alois Kneip", "title": "Cross-Component Registration for Multivariate Functional Data, With\n  Application to Growth Curves", "comments": "29 pages, 7 figures; to be published in Biometrics. This version has\n  shorter introduction and a new Section 2 which establishes the shift-warping\n  model upfront with the Z\\\"urich growth data as a motivating example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate functional data are becoming ubiquitous with advances in modern\ntechnology and are substantially more complex than univariate functional data.\nWe propose and study a novel model for multivariate functional data where the\ncomponent processes are subject to mutual time warping. That is, the component\nprocesses exhibit a similar shape but are subject to systematic phase variation\nacross their time domains. To address this previously unconsidered mode of\nwarping, we propose new registration methodology which is based on a\nshift-warping model. Our method differs from all existing registration methods\nfor functional data in a fundamental way. Namely, instead of focusing on the\ntraditional approach to warping, where one aims to recover individual-specific\nregistration, we focus on shift registration across the components of a\nmultivariate functional data vector on a population-wide level. Our proposed\nestimates for these shifts are identifiable, enjoy parametric rates of\nconvergence and often have intuitive physical interpretations, all in contrast\nto traditional curve-specific registration approaches. We demonstrate the\nimplementation and interpretation of the proposed method by applying our\nmethodology to the Z\\\"urich Longitudinal Growth data and study its finite\nsample properties in simulations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 20:31:08 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 05:52:03 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Carroll", "Cody", ""], ["M\u00fcller", "Hans-Georg", ""], ["Kneip", "Alois", ""]]}, {"id": "1811.01520", "submitter": "Wen-Xin Zhou", "authors": "Yuan Ke, Stanislav Minsker, Zhao Ren, Qiang Sun and Wen-Xin Zhou", "title": "User-Friendly Covariance Estimation for Heavy-Tailed Distributions", "comments": "56 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a survey of recent results on covariance estimation for heavy-tailed\ndistributions. By unifying ideas scattered in the literature, we propose\nuser-friendly methods that facilitate practical implementation. Specifically,\nwe introduce element-wise and spectrum-wise truncation operators, as well as\ntheir $M$-estimator counterparts, to robustify the sample covariance matrix.\nDifferent from the classical notion of robustness that is characterized by the\nbreakdown property, we focus on the tail robustness which is evidenced by the\nconnection between nonasymptotic deviation and confidence level. The key\nobservation is that the estimators needs to adapt to the sample size,\ndimensionality of the data and the noise level to achieve optimal tradeoff\nbetween bias and robustness. Furthermore, to facilitate their practical use, we\npropose data-driven procedures that automatically calibrate the tuning\nparameters. We demonstrate their applications to a series of structured models\nin high dimensions, including the bandable and low-rank covariance matrices and\nsparse precision matrices. Numerical studies lend strong support to the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 05:53:22 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 05:26:23 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 05:19:26 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Ke", "Yuan", ""], ["Minsker", "Stanislav", ""], ["Ren", "Zhao", ""], ["Sun", "Qiang", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1811.01596", "submitter": "Mariko Takagishi", "authors": "Mariko Takagishi and Michel van de Velden", "title": "Visualizing class specific heterogeneous tendencies in categorical data", "comments": "25 pages (30 pages including appendix), 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple correspondence analysis, both individuals (observations) and\ncategories can be represented in a biplot that jointly depicts the\nrelationships across categories or individuals, as well as the associations\nbetween them. Additional information about the individuals can enhance\ninterpretation capacities, such as by including categorical variables for which\nthe interdependencies are not of immediate concern, but that facilitate the\ninterpretation of the plot with respect to relationships between individuals\nand categories. This article proposes a new method for adding such information,\naccording to a multiple-set cluster correspondence analysis approach that\nidentifies clusters specific to classes, or subsets of the data that correspond\nto the categories of the additional variables. The proposed method can\nconstruct a biplot that depicts heterogeneous tendencies of individual members,\nas well as their relationships with the original categorical variables. A\nsimulation study to investigate the performance of this proposed method and an\napplication to data regarding road accidents in the United Kingdom confirms the\nviability of this approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:39:29 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 04:05:06 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Takagishi", "Mariko", ""], ["van de Velden", "Michel", ""]]}, {"id": "1811.01619", "submitter": "Guohui Wu", "authors": "Guohui Wu, Scott H. Holan, Alexis Avril, Jonas Waldenstr\\\"om", "title": "A Bayesian Semiparametric Jolly-Seber Model with Individual\n  Heterogeneity: An Application to Migratory Mallards at Stopover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian hierarchical Jolly-Seber model that can account for\nindividual heterogeneity in departure and the dependence of arrival time on\ncovariates. Additionally, our model provides a semiparametric functional form\nfor modeling capture probabilities. The model is flexible and can be used to\nestimate the stopover duration and stopover population size, which are key to\nstopover duration analysis. From the modeling perspective, our model allows for\nindividual heterogeneity in departure due to a continuous intrinsic factor that\nvaries with time and individual. A stochastic process is considered to model\nthe change of this intrinsic factor over time. Moreover, our model links\nextrinsic factors to capture probabilities and arrival time. Consequently, our\nproposed model enables us to draw inference about the impacts of the intrinsic\nfactor on departure, and extrinsic factors on both capture outcome and arrival\ntime. Through the use of a semiparametric model for capture probabilities, we\nallow the data to suggest the functional relationship between extrinsic factors\nand capture probabilities rather than relying on an imposed parametric model.\nBy using data augmentation, we develop a well customized Markov chain Monte\nCarlo algorithm that is free of tuning. We demonstrate the effectiveness of our\nmodel through a motivating example of stopover duration analysis for mallards\n(Anas platyrhynchos) studied during fall migration in Sweden.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 11:33:29 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Wu", "Guohui", ""], ["Holan", "Scott H.", ""], ["Avril", "Alexis", ""], ["Waldenstr\u00f6m", "Jonas", ""]]}, {"id": "1811.01821", "submitter": "Lincoln Colling", "authors": "Lincoln J Colling and Denes Szucs", "title": "Statistical reform and the replication crisis", "comments": "17 Pages, 3 Figures", "journal-ref": null, "doi": "10.1007/s13164-018-0421-4", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The replication crisis has prompted many to call for statistical reform\nwithin the psychological sciences. Here we examine issues within Frequentist\nstatistics that may have led to the replication crisis, and we examine the\nalternative---Bayesian statistics---that many have suggested as a replacement.\nThe Frequentist approach and the Bayesian approach offer radically different\nperspectives on evidence and inference with the Frequentist approach\nprioritising error control and the Bayesian approach offering a formal method\nfor quantifying the relative strength of evidence for hypotheses. We suggest\nthat rather than mere statistical reform, what is needed is a better\nunderstanding of the different modes of statistical inference and a better\nunderstanding of how statistical inference relates to scientific inference.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 16:09:15 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Colling", "Lincoln J", ""], ["Szucs", "Denes", ""]]}, {"id": "1811.01992", "submitter": "Trinetri Ghosh", "authors": "Trinetri Ghosh (1), Yanyuan Ma (1), Xavier de Luna (2) ((1)\n  Pennsylvania State University, (2) Ume{\\aa} University)", "title": "Sufficient Dimension Reduction for Feasible and Robust Estimation of\n  Average Causal Effect", "comments": "47 Pages, 4 figures", "journal-ref": "Statistica Sinica 31 (2021), 1-22", "doi": "10.5705/ss.202018.0416", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When estimating the treatment effect in an observational study, we use a\nsemiparametric locally efficient dimension reduction approach to assess both\nthe treatment assignment mechanism and the average responses in both treated\nand nontreated groups. We then integrate all results through imputation,\ninverse probability weighting and doubly robust augmentation estimators. Doubly\nrobust estimators are locally efficient while imputation estimators are\nsuper-efficient when the response models are correct. To take advantage of both\nprocedures, we introduce a shrinkage estimator to automatically combine the\ntwo, which retains the double robustness property while improving on the\nvariance when the response model is correct. We demonstrate the performance of\nthese estimators through simulated experiments and a real dataset concerning\nthe effect of maternal smoking on baby birth weight.\n  Key words and phrases: Average Treatment Effect, Doubly Robust Estimator,\nEfficiency, Inverse Probability Weighting, Shrinkage Estimator.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 19:36:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ghosh", "Trinetri", ""], ["Ma", "Yanyuan", ""], ["de Luna", "Xavier", ""]]}, {"id": "1811.02316", "submitter": "Wouter van Loon", "authors": "Wouter van Loon, Marjolein Fokkema, Botond Szabo, Mark de Rooij", "title": "Stacked Penalized Logistic Regression for Selecting Views in Multi-View\n  Learning", "comments": "26 pages, 9 figures. Accepted manuscript", "journal-ref": "Information Fusion 61 (2020) 113-123", "doi": "10.1016/j.inffus.2020.03.007", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biomedical research, many different types of patient data can be\ncollected, such as various types of omics data and medical imaging modalities.\nApplying multi-view learning to these different sources of information can\nincrease the accuracy of medical classification models compared with\nsingle-view procedures. However, collecting biomedical data can be expensive\nand/or burdening for patients, so that it is important to reduce the amount of\nrequired data collection. It is therefore necessary to develop multi-view\nlearning methods which can accurately identify those views that are most\nimportant for prediction. In recent years, several biomedical studies have used\nan approach known as multi-view stacking (MVS), where a model is trained on\neach view separately and the resulting predictions are combined through\nstacking. In these studies, MVS has been shown to increase classification\naccuracy. However, the MVS framework can also be used for selecting a subset of\nimportant views. To study the view selection potential of MVS, we develop a\nspecial case called stacked penalized logistic regression (StaPLR). Compared\nwith existing view-selection methods, StaPLR can make use of faster\noptimization algorithms and is easily parallelized. We show that nonnegativity\nconstraints on the parameters of the function which combines the views play an\nimportant role in preventing unimportant views from entering the model. We\ninvestigate the performance of StaPLR through simulations, and consider two\nreal data examples. We compare the performance of StaPLR with an existing view\nselection method called the group lasso and observe that, in terms of view\nselection, StaPLR is often more conservative and has a consistently lower false\npositive rate.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:23:52 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 13:18:44 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 14:26:11 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["van Loon", "Wouter", ""], ["Fokkema", "Marjolein", ""], ["Szabo", "Botond", ""], ["de Rooij", "Mark", ""]]}, {"id": "1811.02414", "submitter": "David Woods", "authors": "W.G. Mueller, A. Rappold, D.C. Woods", "title": "Copula-based robust optimal block designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blocking is often used to reduce known variability in designed experiments by\ncollecting together homogeneous experimental units. A common modelling\nassumption for such experiments is that responses from units within a block are\ndependent. Accounting for such dependencies in both the design of the\nexperiment and the modelling of the resulting data when the response is not\nnormally distributed can be challenging, particularly in terms of the\ncomputation required to find an optimal design. The application of copulas and\nmarginal modelling provides a computationally efficient approach for estimating\npopulation-average treatment effects. Motivated by an experiment from materials\ntesting, we develop and demonstrate designs with blocks of size two using\ncopula models. Such designs are also important in applications ranging from\nmicroarray experiments to experiments on human eyes or limbs with naturally\noccurring blocks of size two. We present methodology for design selection, make\ncomparisons to existing approaches in the literature and assess the robustness\nof the designs to modelling assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 15:29:52 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Mueller", "W. G.", ""], ["Rappold", "A.", ""], ["Woods", "D. C.", ""]]}, {"id": "1811.02503", "submitter": "Vera Djordjilovi\\'c", "authors": "Vera Djordjilovi\\'c and Monica Chiogna", "title": "Searching for a source of difference in Gaussian graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we look at a two-sample problem within the framework of\nGaussian graphical models. When the global hypothesis of equality of two\ndistributions is rejected, the interest is usually in localizing the source of\ndifference. Motivated by the idea that diseases can be seen as system\nperturbations, and by the need to distinguish between the origin of\nperturbation and components affected by the perturbation, we introduce the\nconcept of a minimal seed set, and its graphical counterpart a graphical seed\nset. They intuitively consist of variables driving the difference between the\ntwo conditions. We propose a simple testing procedure, linear in the number of\nnodes, to estimate the graphical seed set from data, and study its finite\nsample behavior with a stimulation study. We illustrate our approach in the\ncontext of gene set analysis by means of a publicly available gene expression\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:15:54 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Djordjilovi\u0107", "Vera", ""], ["Chiogna", "Monica", ""]]}, {"id": "1811.02504", "submitter": "Christian R\\\"over", "authors": "T. Friede, M. Posch, S. Zohar, C. Alberti, N. Benda, E. Comets, S.\n  Day, A. Dmitrenko, A. Graf, B. K. G\\\"unhan, S. W. Hee, F. Lentz, J. Madan, F.\n  Miller, T. Ondra, M. Pearce, C. R\\\"over, A. Tournazi, S. Unkel, M. Ursino, G.\n  Wassmer, N. Stallard", "title": "Recent advances in methodology for clinical trials in small populations:\n  the InSPiRe project", "comments": "9 pages, 3 figures", "journal-ref": "Orphanet Journal of Rare Diseases, 13:136, 2018", "doi": "10.1186/s13023-018-0919-y", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where there are a limited number of patients, such as in a rare disease,\nclinical trials in these small populations present several challenges,\nincluding statistical issues. This led to an EU FP7 call for proposals in 2013.\nOne of the three projects funded was the Innovative Methodology for Small\nPopulations Research (InSPiRe) project. This paper summarizes the main results\nof the project, which was completed in 2017. The InSPiRe project has led to\ndevelopment of novel statistical methodology for clinical trials in small\npopulations in four areas. We have explored new decision-making methods for\nsmall population clinical trials using a Bayesian decision-theoretic framework\nto compare costs with potential benefits, developed approaches for targeted\ntreatment trials, enabling simultaneous identification of subgroups and\nconfirmation of treatment effect for these patients, worked on early phase\nclinical trial design and on extrapolation from adult to pediatric studies,\ndeveloping methods to enable use of pharmacokinetics and pharmacodynamics data,\nand also developed improved robust meta-analysis methods for a small number of\ntrials to support the planning, analysis and interpretation of a trial as well\nas enabling extrapolation between patient groups. In addition to scientific\npublications, we have contributed to regulatory guidance and produced free\nsoftware in order to facilitate implementation of the novel methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 08:04:44 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Friede", "T.", ""], ["Posch", "M.", ""], ["Zohar", "S.", ""], ["Alberti", "C.", ""], ["Benda", "N.", ""], ["Comets", "E.", ""], ["Day", "S.", ""], ["Dmitrenko", "A.", ""], ["Graf", "A.", ""], ["G\u00fcnhan", "B. K.", ""], ["Hee", "S. W.", ""], ["Lentz", "F.", ""], ["Madan", "J.", ""], ["Miller", "F.", ""], ["Ondra", "T.", ""], ["Pearce", "M.", ""], ["R\u00f6ver", "C.", ""], ["Tournazi", "A.", ""], ["Unkel", "S.", ""], ["Ursino", "M.", ""], ["Wassmer", "G.", ""], ["Stallard", "N.", ""]]}, {"id": "1811.02833", "submitter": "S\\\"oren R. K\\\"unzel", "authors": "S\\\"oren R. K\\\"unzel, Simon J. S. Walter, Jasjeet S. Sekhon", "title": "Causaltoolbox---Estimator Stability for Heterogeneous Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating heterogeneous treatment effects has become increasingly important\nin many fields and life and death decisions are now based on these estimates:\nfor example, selecting a personalized course of medical treatment. Recently, a\nvariety of procedures relying on different assumptions have been suggested for\nestimating heterogeneous treatment effects. Unfortunately, there are no\ncompelling approaches that allow identification of the procedure that has\nassumptions that hew closest to the process generating the data set under study\nand researchers often select one arbitrarily. This approach risks making\ninferences that rely on incorrect assumptions and gives the experimenter too\nmuch scope for $p$-hacking. A single estimator will also tend to overlook\npatterns other estimators could have picked up. We believe that the conclusion\nof many published papers might change had a different estimator been chosen and\nwe suggest that practitioners should evaluate many estimators and assess their\nsimilarity when investigating heterogeneous treatment effects. We demonstrate\nthis by applying 28 different estimation procedures to an emulated\nobservational data set; this analysis shows that different estimation\nprocedures may give starkly different estimates. We also provide an extensible\n\\texttt{R} package which makes it straightforward for practitioners to follow\nour recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 11:05:54 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 23:20:45 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["K\u00fcnzel", "S\u00f6ren R.", ""], ["Walter", "Simon J. S.", ""], ["Sekhon", "Jasjeet S.", ""]]}, {"id": "1811.02916", "submitter": "Jonas Haslbeck", "authors": "Jonas Haslbeck, Sacha Epskamp, Maarten Marsman, Lourens Waldorp", "title": "Interpreting the Ising Model: The Input Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ising model is a model for pairwise interactions between binary variables\nthat has become popular in the psychological sciences. It has been first\nintroduced as a theoretical model for the alignment between positive (+1) and\nnegative (-1) atom spins. In many psychological applications, however, the\nIsing model is defined on the domain $\\{0,1\\}$ instead of the classical domain\n$\\{-1,1\\}$. While it is possible to transform the parameters of a given Ising\nmodel in one domain to obtain a statistically equivalent model in the other\ndomain, the parameters in the two versions of the Ising model lend themselves\nto different interpretations and imply different dynamics, when studying the\nIsing model as a dynamical system. In this tutorial paper, we provide an\naccessible discussion of the interpretation of threshold and interaction\nparameters in the two domains and show how the dynamics of the Ising model\ndepends on the choice of domain. Finally, we provide a transformation that\nallows to transform the parameters in an Ising model in one domain into a\nstatistically equivalent Ising model in the other domain.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 14:49:37 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 16:20:34 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 10:25:33 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 07:47:21 GMT"}, {"version": "v5", "created": "Sat, 7 Mar 2020 11:24:57 GMT"}, {"version": "v6", "created": "Fri, 13 Mar 2020 09:42:00 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Haslbeck", "Jonas", ""], ["Epskamp", "Sacha", ""], ["Marsman", "Maarten", ""], ["Waldorp", "Lourens", ""]]}, {"id": "1811.02962", "submitter": "Britta Velten", "authors": "Britta Velten and Wolfgang Huber", "title": "Adaptive penalization in high-dimensional regression and classification\n  with external covariates using variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalization schemes like Lasso or ridge regression are routinely used to\nregress a response of interest on a high-dimensional set of potential\npredictors. Despite being decisive, the question of the relative strength of\npenalization is often glossed over and only implicitly determined by the scale\nof individual predictors. At the same time, additional information on the\npredictors is available in many applications but left unused. Here, we propose\nto make use of such external covariates to adapt the penalization in a\ndata-driven manner. We present a method that differentially penalizes feature\ngroups defined by the covariates and adapts the relative strength of\npenalization to the information content of each group. Using techniques from\nthe Bayesian tool-set our procedure combines shrinkage with feature selection\nand provides a scalable optimization scheme. We demonstrate in simulations that\nthe method accurately recovers the true effect sizes and sparsity patterns per\nfeature group. Furthermore, it leads to an improved prediction performance in\nsituations where the groups have strong differences in dynamic range. In\napplications to data from high-throughput biology, the method enables\nre-weighting the importance of feature groups from different assays. Overall,\nusing available covariates extends the range of applications of penalized\nregression, improves model interpretability and can improve prediction\nperformance. We provide an open-source implementation of the method in the R\npackage graper.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 16:23:47 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Velten", "Britta", ""], ["Huber", "Wolfgang", ""]]}, {"id": "1811.03004", "submitter": "Mike Pereira", "authors": "Mike Pereira, Nicolas Desassis", "title": "Finite element approximation of non-Markovian random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present finite element approximations of a class of\nGeneralized random fields defined over a bounded domain of R d or a smooth\nd-dimensional Riemannian manifold (d $\\ge$ 1). An explicit expression for the\ncovariance matrix of the weights of the finite element representation of these\nfields is provided and an analysis of the approximation error is carried out.\nFinally, a method to generate simulations of these weights while limiting\ncomputational and storage costs is presented.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 09:25:43 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Pereira", "Mike", ""], ["Desassis", "Nicolas", ""]]}, {"id": "1811.03745", "submitter": "Jonathan Levy", "authors": "Jonathan Levy and Mark van der Laan and Alan Hubbard and Romain\n  Pirracchio", "title": "A Fundamental Measure of Treatment Effect Heterogeneity", "comments": "Presented at JSM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a non-parametric plug-in estimator for an important measure of\ntreatment effect variability and provide minimum conditions under which the\nestimator is asymptotically efficient. The stratum specific treatment effect\nfunction or so-called blip function, is the average treatment effect for a\nrandomly drawn stratum of confounders. The mean of the blip function is the\naverage treatment effect (ATE), whereas the variance of the blip function\n(VTE), the main subject of this paper, measures overall clinical effect\nheterogeneity, perhaps providing a strong impetus to refine treatment based on\nthe confounders. VTE is also an important measure for assessing reliability of\nthe treatment for an individual. The CV-TMLE provides simultaneous plug-in\nestimates and inference for both ATE and VTE, guaranteeing asymptotic\nefficiency under one less condition than for TMLE. This condition is difficult\nto guarantee a priori, particularly when using highly adaptive machine learning\nthat we need to employ in order to eliminate bias. Even in defiance of this\ncondition, CV-TMLE sampling distributions maintain normality, not guaranteed\nfor TMLE, and have a lower mean squared error than their TMLE counterparts. In\naddition to verifying the theoretical properties of TMLE and CV-TMLE through\nsimulations, we point out some of the challenges in estimating VTE, which lacks\ndouble robustness and might be unavoidably biased if the true VTE is small and\nsample size insufficient. We will provide an application of the estimator on a\ndata set for treatment of acute trauma patients.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:39:38 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 08:07:58 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 09:58:22 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Levy", "Jonathan", ""], ["van der Laan", "Mark", ""], ["Hubbard", "Alan", ""], ["Pirracchio", "Romain", ""]]}, {"id": "1811.04150", "submitter": "Daniel Ting", "authors": "Daniel Ting", "title": "Count-Min: Optimal Estimation and Tight Error Bounds using Empirical\n  Error Distributions", "comments": "Long version of a KDD 2018 paper of the same name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Count-Min sketch is an important and well-studied data summarization\nmethod. It allows one to estimate the count of any item in a stream using a\nsmall, fixed size data sketch. However, the accuracy of the sketch depends on\ncharacteristics of the underlying data. This has led to a number of count\nestimation procedures which work well in one scenario but perform poorly in\nothers. A practitioner is faced with two basic, unanswered questions. Which\nvariant should be chosen when the data is unknown? Given an estimate, is its\nerror sufficiently small to be trustworthy?\n  We provide answers to these questions. We derive new count estimators,\nincluding a provably optimal estimator, which best or match previous estimators\nin all scenarios. We also provide practical, tight error bounds at query time\nfor both new and existing estimators. These error estimates also yield\nprocedures to choose the sketch tuning parameters optimally, as they can\nextrapolate the error to different choices of sketch width and depth.\n  The key observation is that the distribution of errors in each counter can be\nempirically estimated from the sketch itself. By first estimating this\ndistribution, count estimation becomes a statistical estimation and inference\nproblem with a known error distribution. This provides both a principled way to\nderive new and optimal estimators as well as a way to study the error and\nproperties of existing estimators.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 22:11:56 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Ting", "Daniel", ""]]}, {"id": "1811.04161", "submitter": "John Galati", "authors": "John C. Galati", "title": "When is $Y_{obs}$ missing and $Y_{mis}$ observed?", "comments": "12 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical modelling of incomplete data, missingness is encoded as a\nrelation between datasets Y and response patterns R. The partitioning of Y into\nobserved and missing components is often denoted Yobs and Ymis. We point out a\nmathematical defect in this notation which results from two different\nmathematical relationships between Y and R not being distinguished, (Yobs,\nYmis, R) in which Yobs values are always observed, and Ymis values are always\nmissing, and the overlaying of a missingness pattern onto the marginal\ndistribution for Y, denoted (Yobs, Ymis). With the latter, Yobs and Ymis each\ndenote mixtures of observable and unobservable data. This overlaying of the\nmissingness pattern onto Y creates a link between the mathematics and the\nmeta-mathematics which violates the stochastic relationship encoded in (Y, R).\nAdditionally, in the theory there is a need to compare partitions of Y\naccording to different missingness patterns simultaneously. A simple remedy for\nthese problems is to use four symbols instead of two, and to make the\ndependence on the missingness pattern explicit. We explain these and related\nissues.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 23:55:55 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 08:51:31 GMT"}, {"version": "v3", "created": "Sat, 30 Mar 2019 00:16:56 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 02:53:01 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Galati", "John C.", ""]]}, {"id": "1811.04170", "submitter": "Eli Ben-Michael", "authors": "Eli Ben-Michael, Avi Feller, Jesse Rothstein", "title": "The Augmented Synthetic Control Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synthetic control method (SCM) is a popular approach for estimating the\nimpact of a treatment on a single unit in panel data settings. The \"synthetic\ncontrol\" is a weighted average of control units that balances the treated\nunit's pre-treatment outcomes as closely as possible. A critical feature of the\noriginal proposal is to use SCM only when the fit on pre-treatment outcomes is\nexcellent. We propose Augmented SCM as an extension of SCM to settings where\nsuch pre-treatment fit is infeasible. Analogous to bias correction for inexact\nmatching, Augmented SCM uses an outcome model to estimate the bias due to\nimperfect pre-treatment fit and then de-biases the original SCM estimate. Our\nmain proposal, which uses ridge regression as the outcome model, directly\ncontrols pre-treatment fit while minimizing extrapolation from the convex hull.\nThis estimator can also be expressed as a solution to a modified synthetic\ncontrols problem that allows negative weights on some donor units. We bound the\nestimation error of this approach under different data generating processes,\nincluding a linear factor model, and show how regularization helps to avoid\nover-fitting to noise. We demonstrate gains from Augmented SCM with extensive\nsimulation studies and apply this framework to estimate the impact of the 2012\nKansas tax cuts on economic growth. We implement the proposed method in the new\naugsynth R package.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 01:18:52 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 17:26:04 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 15:19:18 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ben-Michael", "Eli", ""], ["Feller", "Avi", ""], ["Rothstein", "Jesse", ""]]}, {"id": "1811.04274", "submitter": "Michele Santacatterina", "authors": "Nathan Kallus, Brenton Pennicooke, Michele Santacatterina", "title": "More robust estimation of sample average treatment effects using Kernel\n  Optimal Matching in an observational study of spine surgical interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse probability of treatment weighting (IPTW), which has been used to\nestimate sample average treatment effects (SATE) using observational data,\ntenuously relies on the positivity assumption and the correct specification of\nthe treatment assignment model, both of which are problematic assumptions in\nmany observational studies. Various methods have been proposed to overcome\nthese challenges, including truncation, covariate-balancing propensity scores,\nand stable balancing weights. Motivated by an observational study in spine\nsurgery, in which positivity is violated and the true treatment assignment\nmodel is unknown, we present the use of optimal balancing by Kernel Optimal\nMatching (KOM) to estimate SATE. By uniformly controlling the conditional mean\nsquared error of a weighted estimator over a class of models, KOM\nsimultaneously mitigates issues of possible misspecification of the treatment\nassignment model and is able to handle practical violations of the positivity\nassumption, as shown in our simulation study. Using data from a clinical\nregistry, we apply KOM to compare two spine surgical interventions and\ndemonstrate how the result matches the conclusions of clinical trials that IPTW\nestimates spuriously refute.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 15:47:18 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 18:27:28 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Kallus", "Nathan", ""], ["Pennicooke", "Brenton", ""], ["Santacatterina", "Michele", ""]]}, {"id": "1811.04500", "submitter": "Henry Lam", "authors": "Henry Lam and Huajie Qian", "title": "Subsampling to Enhance Efficiency in Input Uncertainty Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stochastic simulation, input uncertainty refers to the output variability\narising from the statistical noise in specifying the input models. This\nuncertainty can be measured by a variance contribution in the output, which, in\nthe nonparametric setting, is commonly estimated via the bootstrap. However,\ndue to the convolution of the simulation noise and the input noise, the\nbootstrap consists of a two-layer sampling and typically requires substantial\nsimulation effort. This paper investigates a subsampling framework to reduce\nthe required effort, by leveraging the form of the variance and its estimation\nerror in terms of the data size and the sampling requirement in each layer. We\nshow how the total required effort can be reduced from an order bigger than the\ndata size in the conventional approach to an order independent of the data size\nin subsampling. We explicitly identify the procedural specifications in our\nframework that guarantee relative consistency in the estimation, and the\ncorresponding optimal simulation budget allocations. We substantiate our\ntheoretical results with numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 22:50:19 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 10:10:45 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lam", "Henry", ""], ["Qian", "Huajie", ""]]}, {"id": "1811.04545", "submitter": "Cheng Wang", "authors": "Cheng Wang and Binyan Jiang", "title": "An efficient ADMM algorithm for high dimensional precision matrix\n  estimation via penalized quadratic loss", "comments": "18 pages, 2 tables and 3 figures", "journal-ref": "Computational Statistics & Data Analysis,2019", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of high dimensional precision matrices has been a central\ntopic in statistical learning. However, as the number of parameters scales\nquadratically with the dimension $p$, many state-of-the-art methods do not\nscale well to solve problems with a very large $p$. In this paper, we propose a\nvery efficient algorithm for precision matrix estimation via penalized\nquadratic loss functions. Under the high dimension low sample size setting, the\ncomputation complexity of our algorithm is linear in both the sample size and\nthe number of parameters. Such a computation complexity is in some sense\noptimal, as it is the same as the complexity needed for computing the sample\ncovariance matrix. Numerical studies show that our algorithm is much more\nefficient than other state-of-the-art methods when the dimension $p$ is very\nlarge.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 03:58:14 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 05:20:28 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Wang", "Cheng", ""], ["Jiang", "Binyan", ""]]}, {"id": "1811.04573", "submitter": "Jonathan Levy", "authors": "Jonathan Levy", "title": "An Easy Implementation of CV-TMLE", "comments": "Implementation featured in the tlverse series of R packages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the world of targeted learning, cross-validated targeted maximum\nlikelihood estimators, CV-TMLE [Zheng:2010aa], has a distinct advantage over\nTMLE [Laan:2006aa] in that one less condition is required of CV-TMLE in order\nto achieve asymptotic efficiency in the nonparametric or semiparametric\nsettings. CV-TMLE as originally formulated, consists of averaging usually 10\n(for 10-fold cross-validation) parameter estimates, each of which is performed\non a validation set separate from where the initial fit was trained. The\ntargeting step is usually performed as a pooled regression over all validation\nfolds but in each fold, we separately evaluate any means as well as the\nparameter estimate. One nice thing about CV-TMLE, is that we average 10 plug-in\nestimates so the plug-in quality of preserving the natural parameter bounds is\nrespected. Our adjustment of this procedure also preserves the plug-in\ncharacteristic as well as avoids the donsker condtion. The advantage of our\nprocedure is the implementation of the targeting is identical to that of a\nregular TMLE, once all the validation set initial predictions have been formed.\nIn short, we stack the validation set predictions and pretend as if we have a\nregular TMLE, which is not necessarily quite a plug-in estimator on each fold\nbut overall will perform asymptotically the same and might have some slight\nadvantage, a subject for future research. In the case of average treatment\neffect, treatment specific mean and mean outcome under a stochastic\nintervention, the procedure overlaps exactly with the originally formulated\nCV-TMLE with a pooled regression for the targeting.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 06:09:08 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 02:31:45 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Levy", "Jonathan", ""]]}, {"id": "1811.04788", "submitter": "Sourish Das", "authors": "Rajiv Sambasivan and Sourish Das and Sujit K Sahu", "title": "A Bayesian Perspective of Statistical Machine Learning for Big Data", "comments": "26 pages, 3 figures, Review paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical Machine Learning (SML) refers to a body of algorithms and methods\nby which computers are allowed to discover important features of input data\nsets which are often very large in size. The very task of feature discovery\nfrom data is essentially the meaning of the keyword `learning' in SML.\nTheoretical justifications for the effectiveness of the SML algorithms are\nunderpinned by sound principles from different disciplines, such as Computer\nScience and Statistics. The theoretical underpinnings particularly justified by\nstatistical inference methods are together termed as statistical learning\ntheory.\n  This paper provides a review of SML from a Bayesian decision theoretic point\nof view -- where we argue that many SML techniques are closely connected to\nmaking inference by using the so called Bayesian paradigm. We discuss many\nimportant SML techniques such as supervised and unsupervised learning, deep\nlearning, online learning and Gaussian processes especially in the context of\nvery large data sets where these are often employed. We present a dictionary\nwhich maps the key concepts of SML from Computer Science and Statistics. We\nillustrate the SML techniques with three moderately large data sets where we\nalso discuss many practical implementation issues. Thus the review is\nespecially targeted at statisticians and computer scientists who are aspiring\nto understand and apply SML for moderately large to big data sets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 14:26:55 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 01:43:53 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Sambasivan", "Rajiv", ""], ["Das", "Sourish", ""], ["Sahu", "Sujit K", ""]]}, {"id": "1811.05073", "submitter": "Leah F. South", "authors": "Leah F. South and Chris J. Oates and Antonietta Mira and Christopher\n  Drovandi", "title": "Regularised Zero-Variance Control Variates for High-Dimensional Variance\n  Reduction", "comments": "21 pages plus 14 pages of appendices. The revised file has overall\n  efficiency including run-time and also some adjusted discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-variance control variates (ZV-CV) are a post-processing method to reduce\nthe variance of Monte Carlo estimators of expectations using the derivatives of\nthe log target. Once the derivatives are available, the only additional\ncomputational effort lies in solving a linear regression problem. Significant\nvariance reductions have been achieved with this method in low dimensional\nexamples, but the number of covariates in the regression rapidly increases with\nthe dimension of the target. In this paper, we present compelling empirical\nevidence that the use of penalised regression techniques in the selection of\nhigh-dimensional control variates provides performance gains over the classical\nleast squares method. Another type of regularisation based on using subsets of\nderivatives, or a priori regularisation as we refer to it in this paper, is\nalso proposed to reduce computational and storage requirements. Several\nexamples showing the utility and limitations of regularised ZV-CV for Bayesian\ninference are given. The methods proposed in this paper are accessible through\nthe R package ZVCV.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:22:07 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 04:03:03 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 18:37:43 GMT"}, {"version": "v4", "created": "Tue, 11 Feb 2020 17:35:14 GMT"}, {"version": "v5", "created": "Sun, 7 Jun 2020 13:27:56 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["South", "Leah F.", ""], ["Oates", "Chris J.", ""], ["Mira", "Antonietta", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1811.05076", "submitter": "Miaoyan Wang", "authors": "Miaoyan Wang and Lexin Li", "title": "Learning from Binary Multiway Data: Probabilistic Tensor Decomposition\n  and its Statistical Optimality", "comments": "35 pages, 7 figures, 4 tables", "journal-ref": "Journal of Machine Learning Research, 21(154): 1-38, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decomposing a higher-order tensor with binary\nentries. Such data problems arise frequently in applications such as\nneuroimaging, recommendation system, topic modeling, and sensor network\nlocalization. We propose a multilinear Bernoulli model, develop a\nrank-constrained likelihood-based estimation method, and obtain the theoretical\naccuracy guarantees. In contrast to continuous-valued problems, the binary\ntensor problem exhibits an interesting phase transition phenomenon according to\nthe signal-to-noise ratio. The error bound for the parameter tensor estimation\nis established, and we show that the obtained rate is minimax optimal under the\nconsidered model. Furthermore, we develop an alternating optimization algorithm\nwith convergence guarantees. The efficacy of our approach is demonstrated\nthrough both simulations and analyses of multiple data sets on the tasks of\ntensor completion and clustering.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:49:17 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 04:48:47 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 04:05:02 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wang", "Miaoyan", ""], ["Li", "Lexin", ""]]}, {"id": "1811.05137", "submitter": "Luca Faes", "authors": "Luca Faes, Margarida Almeida Pereira, Maria Eduarda Silva, Riccardo\n  Pernice, Alessandro Busacca, Michal Javorka, Ana Paula Rocha", "title": "Multiscale Information Storage of Linear Long-Range Correlated\n  Stochastic Processes", "comments": null, "journal-ref": "Phys. Rev. E 99, 032115 (2019)", "doi": "10.1103/PhysRevE.99.032115", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information storage, reflecting the capability of a dynamical system to keep\npredictable information during its evolution over time, is a key element of\nintrinsic distributed computation, useful for the description of the dynamical\ncomplexity of several physical and biological processes. Here we introduce a\nparametric framework which allows to compute information storage across\nmultiple time scales in stochastic processes displaying both short-term\ndynamics and long-range correlations (LRC). The framework exploits the theory\nof state space models to provide the multiscale representation of linear\nfractionally integrated autoregressive (ARFI) processes, from which information\nstorage is computed at any given time scale relating the process variance to\nthe prediction error variance. This enables the theoretical assessment and a\ncomputationally reliable quantification of a complexity measure which\nincorporates the effects of LRC together with that of short-term dynamics. The\nproposed measure is first assessed in simulated ARFI processes reproducing\ndifferent types of autoregressive (AR) dynamics and different degrees of LRC,\nstudying both the theoretical values and the finite sample performance. We find\nthat LRC alter substantially the complexity of ARFI processes even at short\ntime scales, and that reliable estimation of complexity can be achieved at\nlonger time scales only when LRC are properly modeled. Then, we assess\nmultiscale information storage in physiological time series measured in humans\nduring resting state and postural stress, revealing unprecedented responses to\nstress of the complexity of heart period and systolic arterial pressure\nvariability, which are related to the different role played by LRC in the two\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 07:28:24 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Faes", "Luca", ""], ["Pereira", "Margarida Almeida", ""], ["Silva", "Maria Eduarda", ""], ["Pernice", "Riccardo", ""], ["Busacca", "Alessandro", ""], ["Javorka", "Michal", ""], ["Rocha", "Ana Paula", ""]]}, {"id": "1811.05169", "submitter": "Matias Heikkil\\\"a", "authors": "Matias Heikkil\\\"a", "title": "Nonparametric geometric outlier detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is a major topic in robust statistics due to the high\npractical significance of anomalous observations. Many existing methods are,\nhowever, either parametric or cease to perform well when the data is far from\nlinearly structured. In this paper, we propose a quantity, Delaunay\noutlyingness, that is a nonparametric outlyingness score applicable to data\nwith complicated structure. The approach is based a well known triangulation of\nthe sample, which seems to reflect the sparsity of the pointset to different\ndirections in a useful way. In addition to appealing to heuristics, we derive\nresults on the asymptotic behaviour of Delaunay outlyingness in the case of a\nsufficiently simple set of observations. Simulations and an application to\nfinancial data are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 08:59:41 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Heikkil\u00e4", "Matias", ""]]}, {"id": "1811.05336", "submitter": "Xingwei Hu Dr", "authors": "Xingwei Hu", "title": "On Asymptotic Covariances of A Few Unrotated Factor Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide explicit formulas, in terms of the covariances of\nsample covariances or sample correlations, for the asymptotic covariances of\nunrotated factor loading estimates and unique variance estimates. These\nestimates are extracted from least square, principal, iterative principal\ncomponent, alpha or image factor analysis. If the sample is taken from a\nmultivariate normal population, these formulas, together with the delta\nmethods, will produce the standard errors for the rotated loading estimates. A\nsimulation study shows that the formulas provide reasonable results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 05:10:11 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hu", "Xingwei", ""]]}, {"id": "1811.05337", "submitter": "Giona Casiraghi", "authors": "Giona Casiraghi", "title": "Analytical Formulation of the Block-Constrained Configuration Model", "comments": "24 pages, 6 figures, 3 tables", "journal-ref": "Applied Network Science volume 4 (2019) 123", "doi": "10.1007/s41109-019-0241-1", "report-no": null, "categories": "physics.soc-ph cs.SI math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel family of generative block-models for random graphs that\nnaturally incorporates degree distributions: the block-constrained\nconfiguration model. Block-constrained configuration models build on the\ngeneralised hypergeometric ensemble of random graphs and extend the well-known\nconfiguration model by enforcing block-constraints on the edge generation\nprocess. The resulting models are analytically tractable and practical to fit\neven to large networks. These models provide a new, flexible tool for the study\nof community structure and for network science in general, where modelling\nnetworks with heterogeneous degree distributions is of central importance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:54:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Casiraghi", "Giona", ""]]}, {"id": "1811.05352", "submitter": "Domagoj \\'Cevid MMath", "authors": "Domagoj \\'Cevid, Peter B\\\"uhlmann, Nicolai Meinshausen", "title": "Spectral Deconfounding via Perturbed Sparse Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard high-dimensional regression methods assume that the underlying\ncoefficient vector is sparse. This might not be true in some cases, in\nparticular in presence of hidden, confounding variables. Such hidden\nconfounding can be represented as a high-dimensional linear model where the\nsparse coefficient vector is perturbed. For this model, we develop and\ninvestigate a class of methods that are based on running the Lasso on\npreprocessed data. The preprocessing step consists of applying certain spectral\ntransformations that change the singular values of the design matrix. We show\nthat, under some assumptions, one can achieve the optimal $\\ell_1$-error rate\nfor estimating the underlying sparse coefficient vector. Our theory also covers\nthe Lava estimator (Chernozhukov et al. [2017]) for a special model class. The\nperformance of the method is illustrated on simulated data and a genomic\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 14:59:55 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 12:56:25 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 20:53:54 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["\u0106evid", "Domagoj", ""], ["B\u00fchlmann", "Peter", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1811.05359", "submitter": "John Matthews", "authors": "John N. S. Matthews", "title": "Highly Efficient Stepped Wedge Designs for Clusters of Unequal Size", "comments": "20 pages; 2 figures; 4 tables; 4 appendices", "journal-ref": null, "doi": "10.1111/biom.13218", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stepped Wedge Design (SWD) is a form of cluster randomized trial, usually\ncomparing two treatments, which is divided into time periods and sequences,\nwith clusters allocated to sequences. Typically all sequences start with the\nstandard treatment and end with the new treatment, with the change happening at\ndifferent times in the different sequences. The clusters will usually differ in\nsize but this is overlooked in much of the existing literature. This paper\nconsiders the case when clusters have different sizes and determines how\nefficient designs can be found. The approach uses an approximation to the the\nvariance of the treatment effect which is expressed in terms of the proportions\nof clusters and of individuals allocated to each sequence of the design. The\nroles of these sets of proportions in determining an efficient design are\ndiscussed and illustrated using two SWDs, one in the treatment of sexually\ntransmitted diseases and one in renal replacement therapy. Cluster-balanced\ndesigns, which allocate equal numbers of clusters to each sequence, are shown\nto have excellent statistical and practical properties; suggestions are made\nabout the practical application of the results for these designs. The paper\nconcentrates on the cross-sectional case, where subjects are measured once, but\nit is briefly indicated how the methods can be extended to the closed-cohort\ndesign.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:17:13 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 16:28:18 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Matthews", "John N. S.", ""]]}, {"id": "1811.05379", "submitter": "Hirofumi Ota", "authors": "Hirofumi Ota, Kengo Kato, Satoshi Hara", "title": "Quantile regression approach to conditional mode estimation", "comments": "This paper supersedes \"On estimation of conditional modes using\n  multiple quantile regressions\" (Hirofumi Ohta and Satoshi Hara,\n  arXiv:1712.08754)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider estimation of the conditional mode of an outcome\nvariable given regressors. To this end, we propose and analyze a\ncomputationally scalable estimator derived from a linear quantile regression\nmodel and develop asymptotic distributional theory for the estimator.\nSpecifically, we find that the pointwise limiting distribution is a scale\ntransformation of Chernoff's distribution despite the presence of regressors.\nIn addition, we consider analytical and subsampling-based confidence intervals\nfor the proposed estimator. We also conduct Monte Carlo simulations to assess\nthe finite sample performance of the proposed estimator together with the\nanalytical and subsampling confidence intervals. Finally, we apply the proposed\nestimator to predicting the net hourly electrical energy output using Combined\nCycle Power Plant Data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 16:06:12 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 14:16:20 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Ota", "Hirofumi", ""], ["Kato", "Kengo", ""], ["Hara", "Satoshi", ""]]}, {"id": "1811.05405", "submitter": "Priyam Das", "authors": "Priyam Das, Christine Peterson, Kim-Anh Do, Rehan Akbani, Veerabhadran\n  Baladandayuthapani", "title": "NExUS: Bayesian simultaneous network estimation across unequal sample\n  sizes", "comments": "8 pages, 8 figues", "journal-ref": null, "doi": "10.1093/bioinformatics/btz636", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network-based analyses of high-throughput genomics data provide a holistic,\nsystems-level understanding of various biological mechanisms for a common\npopulation. However, when estimating multiple networks across heterogeneous\nsub-populations, varying sample sizes pose a challenge in the estimation and\ninference, as network differences may be driven by differences in power. We are\nparticularly interested in addressing this challenge in the context of\nproteomic networks for related cancers, as the number of subjects available for\nrare cancer (sub-)types is often limited. We develop NExUS (Network Estimation\nacross Unequal Sample sizes), a Bayesian method that enables joint learning of\nmultiple networks while avoiding artefactual relationship between sample size\nand network sparsity. We demonstrate through simulations that NExUS outperforms\nexisting network estimation methods in this context, and apply it to learn\nnetwork similarity and shared pathway activity for groups of cancers with\nrelated origins represented in The Cancer Genome Atlas (TCGA) proteomic data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:50:55 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Das", "Priyam", ""], ["Peterson", "Christine", ""], ["Do", "Kim-Anh", ""], ["Akbani", "Rehan", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1811.05422", "submitter": "Carlo A. Furia", "authors": "Carlo A. Furia, Robert Feldt, Richard Torkar", "title": "Bayesian Data Analysis in Empirical Software Engineering Research", "comments": "To appear in IEEE Transactions on Software Engineering", "journal-ref": null, "doi": "10.1109/TSE.2019.2935974", "report-no": null, "categories": "cs.SE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics comes in two main flavors: frequentist and Bayesian. For\nhistorical and technical reasons, frequentist statistics have traditionally\ndominated empirical data analysis, and certainly remain prevalent in empirical\nsoftware engineering. This situation is unfortunate because frequentist\nstatistics suffer from a number of shortcomings---such as lack of flexibility\nand results that are unintuitive and hard to interpret---that curtail their\neffectiveness when dealing with the heterogeneous data that is increasingly\navailable for empirical analysis of software engineering practice.\n  In this paper, we pinpoint these shortcomings, and present Bayesian data\nanalysis techniques that provide tangible benefits---as they can provide\nclearer results that are simultaneously robust and nuanced. After a short,\nhigh-level introduction to the basic tools of Bayesian statistics, we present\nthe reanalysis of two empirical studies on the effectiveness of automatically\ngenerated tests and the performance of programming languages. By contrasting\nthe original frequentist analyses with our new Bayesian analyses, we\ndemonstrate the concrete advantages of the latter. To conclude we advocate a\nmore prominent role for Bayesian statistical techniques in empirical software\nengineering research and practice.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 17:24:51 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 14:31:05 GMT"}, {"version": "v3", "created": "Sat, 13 Jul 2019 14:19:11 GMT"}, {"version": "v4", "created": "Wed, 14 Aug 2019 17:16:17 GMT"}, {"version": "v5", "created": "Mon, 26 Aug 2019 08:41:43 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Furia", "Carlo A.", ""], ["Feldt", "Robert", ""], ["Torkar", "Richard", ""]]}, {"id": "1811.05494", "submitter": "Alvin Chua", "authors": "Alvin J. K. Chua", "title": "Sampling from manifold-restricted distributions using tangent bundle\n  projections", "comments": "Published version; Python implementation available at\n  https://github.com/alvincjk/sampling-manifold-restricted-gaussians", "journal-ref": "Stat. Comput. 30, 587 (2020)", "doi": "10.1007/s11222-019-09907-8", "report-no": null, "categories": "stat.CO astro-ph.IM gr-qc stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in Bayesian inference is the sampling of target probability\ndistributions at sufficient resolution and accuracy to estimate the probability\ndensity, and to compute credible regions. Often by construction, many target\ndistributions can be expressed as some higher-dimensional closed-form\ndistribution with parametrically constrained variables, i.e., one that is\nrestricted to a smooth submanifold of Euclidean space. I propose a\nderivative-based importance sampling framework for such distributions. A base\nset of $n$ samples from the target distribution is used to map out the tangent\nbundle of the manifold, and to seed $nm$ additional points that are projected\nonto the tangent bundle and weighted appropriately. The method essentially acts\nas an upsampling complement to any standard algorithm. It is designed for the\nefficient production of approximate high-resolution histograms from\nmanifold-restricted Gaussian distributions, and can provide large computational\nsavings when sampling directly from the target distribution is expensive.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 19:00:48 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 19:36:18 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 20:27:51 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Chua", "Alvin J. K.", ""]]}, {"id": "1811.05560", "submitter": "John Galati", "authors": "John C Galati", "title": "A fresh look at ignorability for likelihood inference", "comments": "8 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data are incomplete, a random vector Y for the data process together\nwith a binary random vector R for the process that causes missing data, are\nmodelled jointly. We review conditions under which R can be ignored for drawing\nlikelihood inferences about the distribution for Y. The standard approach of\nRubin (1976) and Seaman et. al. (2013) Statist. Sci., 28:2 pp. 257--268\nemulates complete-data methods exactly, and directs an investigator to choose a\nfull model in which missing at random (MAR) and distinct of parameters holds if\nthe goal is not to use a full model. Another interpretation of ignorability\nlurking in the literature considers ignorable likelihood estimation\nindependently of any model for the conditional distribution R given Y. We\ndiscuss shortcomings of the standard approach, and argue that the alternative\ngives the `right' conditions for ignorability because it treats the problem on\nits merits, rather than emulating methodology developed for when the\ninvestigator is in possession of all of the data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 23:03:17 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 06:17:57 GMT"}, {"version": "v3", "created": "Sun, 10 Mar 2019 23:31:45 GMT"}, {"version": "v4", "created": "Thu, 14 Mar 2019 09:47:00 GMT"}, {"version": "v5", "created": "Tue, 19 Mar 2019 00:39:46 GMT"}, {"version": "v6", "created": "Mon, 25 Mar 2019 03:15:34 GMT"}, {"version": "v7", "created": "Thu, 28 Mar 2019 22:18:21 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Galati", "John C", ""]]}, {"id": "1811.05575", "submitter": "Sijia Xiang", "authors": "Sijia Xiang, Weixin Yao, Guangren Yang", "title": "An Overview of Semiparametric Extensions of Finite Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models have been a very important tool for exploring complex\ndata structures in many scientific areas, for example, economics, epidemiology,\nfinance. In the past decade, semiparametric techniques have been popularly\nintroduced into traditional finite mixture models, and so semiparametric\nmixture models have experienced exciting development in methodologies, theories\nand applications. In this article, we provide a selective overview of\nnewly-developed semiparametric mixture models, discuss their estimation\nmethodologies, theoretical properties if applied, and some open questions.\nRecent developments and some open questions are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 00:23:05 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Xiang", "Sijia", ""], ["Yao", "Weixin", ""], ["Yang", "Guangren", ""]]}, {"id": "1811.05956", "submitter": "Holger Dette", "authors": "Holger Dette, Theresa Sch\\\"uler, Mathias Vetter", "title": "Multiscale change point detection for dependent data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the theoretical properties of the simultaneous\nmultiscale change point estimator (SMUCE) proposed by Frick et al. (2014) in\nregression models with dependent error processes. Empirical studies show that\nin this case the change point estimate is inconsistent, but it is not known if\nalternatives suggested in the literature for correlated data are consistent. We\npropose a modification of SMUCE scaling the basic statistic by the long run\nvariance of the error process, which is estimated by a difference-type variance\nestimator calculated from local means from different blocks. For this\nmodification we prove model consistency for physical dependent error processes\nand illustrate the finite sample performance by means of a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:42:46 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Dette", "Holger", ""], ["Sch\u00fcler", "Theresa", ""], ["Vetter", "Mathias", ""]]}, {"id": "1811.05997", "submitter": "Rachel Nethery", "authors": "Rachel C. Nethery, Yue Yang, Anna J. Brown, Francesca Dominici", "title": "A causal inference framework for cancer cluster investigations using\n  publicly available data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, a community becomes alarmed when high rates of cancer are noticed, and\nresidents suspect that the cancer cases could be caused by a known source of\nhazard. In response, the CDC recommends that departments of health perform a\nstandardized incidence ratio (SIR) analysis to determine whether the observed\ncancer incidence is higher than expected. This approach has several limitations\nthat are well documented in the literature. In this paper we propose a novel\ncausal inference approach to cancer cluster investigations, rooted in the\npotential outcomes framework. Assuming that a source of hazard representing a\npotential cause of increased cancer rates in the community is identified a\npriori, we introduce a new estimand called the causal SIR (cSIR). The cSIR is a\nratio defined as the expected cancer incidence in the exposed population\ndivided by the expected cancer incidence under the (counterfactual) scenario of\nno exposure. To estimate the cSIR we need to overcome two main challenges: 1)\nidentify unexposed populations that are as similar as possible to the exposed\none to inform estimation under the counterfactual scenario of no exposure, and\n2) make inference on cancer incidence in these unexposed populations using\npublicly available data that are often available at a much higher level of\nspatial aggregation than what is desired. We overcome the first challenge by\nrelying on matching. We overcome the second challenge by developing a Bayesian\nhierarchical model that borrows information from other sources to impute cancer\nincidence at the desired finer level of spatial aggregation. We apply our\nproposed approach to determine whether trichloroethylene vapor exposure has\ncaused increased cancer incidence in Endicott, NY.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:01:52 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 20:36:52 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Nethery", "Rachel C.", ""], ["Yang", "Yue", ""], ["Brown", "Anna J.", ""], ["Dominici", "Francesca", ""]]}, {"id": "1811.06198", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee, Jaeyong Lee and Lizhen Lin", "title": "Minimax Posterior Convergence Rates and Model Selection Consistency in\n  High-dimensional DAG Models based on Sparse Cholesky Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the high-dimensional sparse directed acyclic graph\n(DAG) models under the empirical sparse Cholesky prior. Among our results,\nstrong model selection consistency or graph selection consistency is obtained\nunder more general conditions than those in the existing literature. Compared\nto Cao, Khare and Ghosh (2017), the required conditions are weakened in terms\nof the dimensionality, sparsity and lower bound of the nonzero elements in the\nCholesky factor. Furthermore, our result does not require the irrepresentable\ncondition, which is necessary for Lasso type methods. We also derive the\nposterior convergence rates for precision matrices and Cholesky factors with\nrespect to various matrix norms. The obtained posterior convergence rates are\nthe fastest among those of the existing Bayesian approaches. In particular, we\nprove that our posterior convergence rates for Cholesky factors are the minimax\nor at least nearly minimax depending on the relative size of true sparseness\nfor the entire dimension. The simulation study confirms that the proposed\nmethod outperforms the competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 06:26:25 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Lee", "Jaeyong", ""], ["Lin", "Lizhen", ""]]}, {"id": "1811.06211", "submitter": "Huijuan Ma", "authors": "Huijuan Ma, Limin Peng, Chiung-Yu Huang, and Haoda Fu", "title": "Quantile Regression Modeling of Recurrent Event Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progression of chronic disease is often manifested by repeated occurrences of\ndisease-related events over time. Delineating the heterogeneity in the risk of\nsuch recurrent events can provide valuable scientific insight for guiding\ncustomized disease management. In this paper, we present a new modeling\nframework for recurrent event data, which renders a flexible and robust\ncharacterization of individual multiplicative risk of recurrent event through\nquantile regression that accommodates both observed covariates and unobservable\nfrailty. The proposed modeling requires no distributional specification of the\nunobservable frailty, while permitting the exploration of dynamic covariate\neffects. We develop estimation and inference procedures for the proposed model\nthrough a novel adaptation of the principle of conditional score. The\nasymptotic properties of the proposed estimator, including the uniform\nconsistency and weak convergence, are established. Extensive simulation studies\ndemonstrate satisfactory finite-sample performance of the proposed method. We\nillustrate the practical utility of the new method via an application to a\ndiabetes clinical trial that explores the risk patterns of hypoglycemia in Type\n2 diabetes patients.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 07:38:14 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ma", "Huijuan", ""], ["Peng", "Limin", ""], ["Huang", "Chiung-Yu", ""], ["Fu", "Haoda", ""]]}, {"id": "1811.06340", "submitter": "Tom\\'a\\v{s} Rub\\'in", "authors": "Tom\\'a\\v{s} Rub\\'in and Victor M. Panaretos", "title": "Sparsely Observed Functional Time Series: Estimation and Prediction", "comments": null, "journal-ref": null, "doi": "10.1214/20-EJS1690", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional time series analysis, whether based on time of frequency domain\nmethodology, has traditionally been carried out under the assumption of\ncomplete observation of the constituent series of curves, assumed stationary.\nNevertheless, as is often the case with independent functional data, it may\nwell happen that the data available to the analyst are not the actual sequence\nof curves, but relatively few and noisy measurements per curve, potentially at\ndifferent locations in each curve's domain. Under this sparse sampling regime,\nneither the established estimators of the time series' dynamics, nor their\ncorresponding theoretical analysis will apply. The subject of this paper is to\ntackle the problem of estimating the dynamics and of recovering the latent\nprocess of smooth curves in the sparse regime. Assuming smoothness of the\nlatent curves, we construct a consistent nonparametric estimator of the series'\nspectral density operator and use it develop a frequency-domain recovery\napproach, that predicts the latent curve at a given time by borrowing strength\nfrom the (estimated) dynamic correlations in the series across time. Further to\npredicting the latent curves from their noisy point samples, the method fills\nin gaps in the sequence (curves nowhere sampled), denoises the data, and serves\nas a basis for forecasting. Means of providing corresponding confidence bands\nare also investigated. A simulation study interestingly suggests that sparse\nobservation for a longer time period, may be provide better performance than\ndense observation for a shorter period, in the presence of smoothness. The\nmethodology is further illustrated by application to an environmental data set\non fair-weather atmospheric electricity, which naturally leads to a sparse\nfunctional time-series.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 13:33:59 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 12:37:19 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Rub\u00edn", "Tom\u00e1\u0161", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1811.06433", "submitter": "Anja Jan{\\ss}en", "authors": "Holger Drees, Anja Jan{\\ss}en, Sidney I. Resnick and Tiandong Wang", "title": "On a minimum distance procedure for threshold selection in tail analysis", "comments": null, "journal-ref": "SIAM Journal on Mathematics of Data Science 2020 2:1, 75-102", "doi": "10.1137/19M1260463", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power-law distributions have been widely observed in different areas of\nscientific research. Practical estimation issues include how to select a\nthreshold above which observations follow a power-law distribution and then how\nto estimate the power-law tail index. A minimum distance selection procedure\n(MDSP) is proposed in Clauset et al. (2009) and has been widely adopted in\npractice, especially in the analyses of social networks. However, theoretical\njustifications for this selection procedure remain scant. In this paper, we\nstudy the asymptotic behavior of the selected threshold and the corresponding\npower-law index given by the MDSP. We find that the MDSP tends to choose too\nhigh a threshold level and leads to Hill estimates with large variances and\nroot mean squared errors for simulated data with Pareto-like tails.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 15:38:36 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 12:32:13 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Drees", "Holger", ""], ["Jan\u00dfen", "Anja", ""], ["Resnick", "Sidney I.", ""], ["Wang", "Tiandong", ""]]}, {"id": "1811.06514", "submitter": "Jonathan Levy", "authors": "Jonathan Levy and Mark van der Laan", "title": "Kernel Smoothing of the Treatment Effect CDF", "comments": "arXiv admin note: text overlap with arXiv:1811.03745", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strata-specific treatment effect or so-called blip for a randomly drawn\nstrata of confounders defines a random variable and a corresponding cumulative\ndistribution function. However, the CDF is not pathwise differentiable,\nnecessitating a kernel smoothing approach to estimate it at a given point or\nperhaps many points. Assuming the CDF is continuous, we derive the efficient\ninfluence curve of the kernel smoothed version of the blip CDF and a CV-TMLE\nestimator. The estimator is asymptotically efficient under two conditions, one\nof which involves a second order remainder term which, in this case, shows us\nthat knowledge of the treatment mechanism does not guarantee a consistent\nestimate. The remainder term also teaches us exactly how well we need to\nestimate the nuisance parameters (outcome model and treatment mechanism) to\nguarantee asymptotic efficiency. Through simulations we verify theoretical\nproperties of the estimator and show the importance of machine learning over\nconventional regression approaches to fitting the nuisance parameters. We also\nderive the bias and variance of the estimator, the orders of which are\nanalogous to a kernel density estimator. This estimator opens up the\npossibility of developing methodology for optimal choice of the kernel and\nbandwidth to form confidence bounds for the CDF itself.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 18:21:15 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 07:47:37 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 20:15:32 GMT"}, {"version": "v4", "created": "Wed, 26 Dec 2018 22:47:09 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Levy", "Jonathan", ""], ["van der Laan", "Mark", ""]]}, {"id": "1811.06601", "submitter": "Shyamalendu Sinha", "authors": "Shyamalendu Sinha and Jeffrey D. Hart", "title": "Estimating the Mean and Variance of a High-dimensional Normal\n  Distribution Using a Mixture Prior", "comments": null, "journal-ref": "Computational Statistics and Data Analysis 138 (2019) 201-221", "doi": "10.1016/j.csda.2019.04.006", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a framework for estimating the mean and variance of a\nhigh-dimensional normal density. The main setting considered is a fixed number\nof vector following a high-dimensional normal distribution with unknown mean\nand diagonal covariance matrix. The diagonal covariance matrix can be known or\nunknown. If the covariance matrix is unknown, the sample size can be as small\nas $2$. The proposed estimator is based on the idea that the unobserved pairs\nof mean and variance for each dimension are drawn from an unknown bivariate\ndistribution, which we model as a mixture of normal-inverse gammas. The mixture\nof normal-inverse gamma distributions provides advantages over more traditional\nempirical Bayes methods, which are based on a normal-normal model. When fitting\na mixture model, we are essentially clustering the unobserved mean and variance\npairs for each dimension into different groups, with each group having a\ndifferent normal-inverse gamma distribution. The proposed estimator of each\nmean is the posterior mean of shrinkage estimates, each of which shrinks a\nsample mean towards a different component of the mixture distribution.\nSimilarly, the proposed estimator of variance has an analogous interpretation\nin terms of sample variances and components of the mixture distribution. If\ndiagonal covariance matrix is known, then the sample size can be as small as\n$1$, and we treat the pairs of known variance and unknown mean for each\ndimension as random observations coming from a flexible mixture of\nnormal-inverse gamma distributions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 21:49:40 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Sinha", "Shyamalendu", ""], ["Hart", "Jeffrey D.", ""]]}, {"id": "1811.06687", "submitter": "Yaniv Romano", "authors": "Yaniv Romano, Matteo Sesia, Emmanuel J. Cand\\`es", "title": "Deep Knockoffs", "comments": "37 pages, 23 figures, 1 table", "journal-ref": "J. Am. Stat. Assoc., Volume 0, Issue 0, 17 Oct 2019, Pages 1-12", "doi": "10.1080/01621459.2019.1660174", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a machine for sampling approximate model-X knockoffs\nfor arbitrary and unspecified data distributions using deep generative models.\nThe main idea is to iteratively refine a knockoff sampling mechanism until a\ncriterion measuring the validity of the produced knockoffs is optimized; this\ncriterion is inspired by the popular maximum mean discrepancy in machine\nlearning and can be thought of as measuring the distance to pairwise\nexchangeability between original and knockoff features. By building upon the\nexisting model-X framework, we thus obtain a flexible and model-free\nstatistical tool to perform controlled variable selection. Extensive numerical\nexperiments and quantitative tests confirm the generality, effectiveness, and\npower of our deep knockoff machines. Finally, we apply this new method to a\nreal study of mutations linked to changes in drug resistance in the human\nimmunodeficiency virus.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 06:26:33 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Romano", "Yaniv", ""], ["Sesia", "Matteo", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1811.06766", "submitter": "Arman Hassanniakalager", "authors": "Georgios Sermpinis, Arman Hassanniakalager, Charalampos Stasinakis,\n  Ioannis Psaradellis", "title": "Technical Analysis and Discrete False Discovery Rate: Evidence from MSCI\n  Indices", "comments": "72 pages, 2 figues, 14 (main) and 13 (appendix) tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of dynamic portfolios constructed using more\nthan 21,000 technical trading rules on 12 categorical and country-specific\nmarkets over the 2004-2015 study period, on rolling forward structures of\ndifferent lengths. We also introduce a discrete false discovery rate (DFRD+/-)\nmethod for controlling data snooping bias. Compared to the existing methods,\nDFRD+/- is adaptive and more powerful, and accommodates for discrete p-values.\nThe profitability, persistence and robustness of the technical rules are\nexamined. Technical analysis still has short-term value in advanced, emerging\nand frontier markets. Financial stress, the economic environment and market\ndevelopment seem to affect the performance of trading rules. A cross-validation\nexercise highlights the importance of frequent rebalancing and the variability\nof profitability in trading with technical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 11:41:39 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 13:40:00 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Sermpinis", "Georgios", ""], ["Hassanniakalager", "Arman", ""], ["Stasinakis", "Charalampos", ""], ["Psaradellis", "Ioannis", ""]]}, {"id": "1811.06899", "submitter": "Luca Greco", "authors": "Luca Greco and Claudio Agostinelli", "title": "Weighted likelihood mixture modeling and model based clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A weighted likelihood approach for robust fitting of a mixture of\nmultivariate Gaussian components is developed in this work. Two approaches have\nbeen proposed that are driven by a suitable modification of the standard EM and\nCEM algorithms, respectively. In both techniques, the M-step is enhanced by the\ncomputation of weights aimed at downweighting outliers. The weights are based\non Pearson residuals stemming from robust Mahalanobis-type distances. Formal\nrules for robust clustering and outlier detection can be also defined based on\nthe fitted mixture model. The behavior of the proposed methodologies has been\ninvestigated by some numerical studies and real data examples in terms of both\nfitting and classification accuracy and outlier detection.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 12:55:02 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Greco", "Luca", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "1811.07025", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Isabella Gollini", "title": "A multilayer exponential random graph modelling approach for weighted\n  networks", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new modelling approach for the analysis of weighted networks with\nordinal/polytomous dyadic values is introduced. Specifically, it is proposed to\nmodel the weighted network connectivity structure using a hierarchical\nmultilayer exponential random graph model (ERGM) generative process where each\nnetwork layer represents a different ordinal dyadic category. The network\nlayers are assumed to be generated by an ERGM process conditional on their\nclosest lower network layers. A crucial advantage of the proposed method is the\npossibility of adopting the binary network statistics specification to describe\nboth the between-layer and across-layer network processes and thus facilitating\nthe interpretation of the parameter estimates associated to the network effects\nincluded in the model. The Bayesian approach provides a natural way to quantify\nthe uncertainty associated to the model parameters. From a computational point\nof view, an extension of the approximate exchange algorithm is proposed to\nsample from the doubly-intractable parameter posterior distribution. A\nsimulation study is carried out on artificial data and applications of the\nmethodology are illustrated on well-known datasets. Finally, a goodness-of-fit\ndiagnostic procedure for model assessment is proposed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:32:29 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 15:59:20 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 16:29:22 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Caimo", "Alberto", ""], ["Gollini", "Isabella", ""]]}, {"id": "1811.07179", "submitter": "Sophia Wright", "authors": "Sophia K. Wright and Jim Q. Smith", "title": "Bayesian Networks, Total Variation and Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now that Bayesian Networks (BNs) have become widely used, an appreciation is\ndeveloping of just how critical an awareness of the sensitivity and robustness\nof certain target variables are to changes in the model. When time resources\nare limited, such issues impact directly on the chosen level of complexity of\nthe BN as well as the quantity of missing probabilities we are able to elicit.\nCurrently most such analyses are performed once the whole BN has been elicited\nand are based on Kullback-Leibler information measures. In this paper we argue\nthat robustness methods based instead on the familiar total variation distance\nprovide simple and more useful bounds on robustness to misspecification which\nare both formally justifiable and transparent. We demonstrate how such formal\nrobustness considerations can be embedded within the process of building a BN.\nHere we focus on two particular choices a modeller needs to make: the choice of\nthe parents of each node and the number of levels to choose for each variable\nwithin the system. Our analyses are illustrated throughout using two BNs drawn\nfrom the recent literature.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 15:28:47 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wright", "Sophia K.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1811.07356", "submitter": "Maxime Turgeon", "authors": "Maxime Turgeon and Celia MT Greenwood and Aurelie Labbe", "title": "A Tracy-Widom Empirical Estimator For Valid P-values With\n  High-Dimensional Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological advances in many domains including both genomics and\nbrain imaging have led to an abundance of high-dimensional and correlated data\nbeing routinely collected. Classical multivariate approaches like Multivariate\nAnalysis of Variance (MANOVA) and Canonical Correlation Analysis (CCA) can be\nused to study relationships between such multivariate datasets. Yet, special\ncare is required with high-dimensional data, as the test statistics may be\nill-defined and classical inference procedures break down.\n  In this work, we explain how valid p-values can be derived for these\nmultivariate methods even in high dimensional datasets. Our main contribution\nis an empirical estimator for the largest root distribution of a singular\ndouble Wishart problem; this general framework underlies many common\nmultivariate analysis approaches. From a small number of permutations of the\ndata, we estimate the location and scale parameters of a parametric Tracy-Widom\nfamily that provides a good approximation of this distribution. Through\nsimulations, we show that this estimated distribution also leads to valid\np-values that can be used for high-dimensional inference. We then apply our\napproach to a pathway-based analysis of the association between DNA methylation\nand disease type in patients with systemic auto-immune rheumatic diseases.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 17:10:48 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Turgeon", "Maxime", ""], ["Greenwood", "Celia MT", ""], ["Labbe", "Aurelie", ""]]}, {"id": "1811.07415", "submitter": "Harsh Parikh", "authors": "Harsh Parikh, Cynthia Rudin, Alexander Volfovsky", "title": "MALTS: Matching After Learning to Stretch", "comments": "25 pages, 4 Tables, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a flexible framework that produces high-quality almost-exact\nmatches for causal inference. Most prior work in matching uses ad-hoc distance\nmetrics, often leading to poor quality matches, particularly when there are\nirrelevant covariates. In this work, we learn an interpretable distance metric\nfor matching, which leads to substantially higher quality matches. The learned\ndistance metric stretches the covariates according to their contribution to\noutcome prediction. The framework is flexible in that the user can choose the\nform of the distance metric and the type of optimization algorithm. Our ability\nto learn flexible distance metrics leads to matches that are interpretable and\nuseful for the estimation of conditional average treatment effects.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 22:29:59 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 08:26:20 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 01:24:24 GMT"}, {"version": "v4", "created": "Wed, 16 Oct 2019 23:14:28 GMT"}, {"version": "v5", "created": "Sat, 15 Aug 2020 10:57:46 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Parikh", "Harsh", ""], ["Rudin", "Cynthia", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1811.07546", "submitter": "Takashi Goda", "authors": "Takashi Goda, Tomohiko Hironaka, Takeru Iwamoto", "title": "Multilevel Monte Carlo estimation of expected information gains", "comments": null, "journal-ref": "Stochastic Analysis and Applications, Volume 38, Issue 4, 581-600,\n  2020", "doi": "10.1080/07362994.2019.1705168", "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected information gain is an important quality criterion of Bayesian\nexperimental designs, which measures how much the information entropy about\nuncertain quantity of interest $\\theta$ is reduced on average by collecting\nrelevant data $Y$. However, estimating the expected information gain has been\nconsidered computationally challenging since it is defined as a nested\nexpectation with an outer expectation with respect to $Y$ and an inner\nexpectation with respect to $\\theta$. In fact, the standard, nested Monte Carlo\nmethod requires a total computational cost of $O(\\varepsilon^{-3})$ to achieve\na root-mean-square accuracy of $\\varepsilon$. In this paper we develop an\nefficient algorithm to estimate the expected information gain by applying a\nmultilevel Monte Carlo (MLMC) method. To be precise, we introduce an antithetic\nMLMC estimator for the expected information gain and provide a sufficient\ncondition on the data model under which the antithetic property of the MLMC\nestimator is well exploited such that optimal complexity of\n$O(\\varepsilon^{-2})$ is achieved. Furthermore, we discuss how to incorporate\nimportance sampling techniques within the MLMC estimator to avoid arithmetic\nunderflow. Numerical experiments show the considerable computational cost\nsavings compared to the nested Monte Carlo method for a simple test case and a\nmore realistic pharmacokinetic model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:11:28 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 05:09:04 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 08:29:38 GMT"}, {"version": "v4", "created": "Mon, 7 Oct 2019 03:34:12 GMT"}, {"version": "v5", "created": "Fri, 6 Dec 2019 03:38:05 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Goda", "Takashi", ""], ["Hironaka", "Tomohiko", ""], ["Iwamoto", "Takeru", ""]]}, {"id": "1811.07625", "submitter": "Christos Merkatas", "authors": "Spyridon J. Hatjispyros, Christos Merkatas", "title": "Joint reconstruction and prediction of random dynamical systems under\n  borrowing of strength", "comments": null, "journal-ref": null, "doi": "10.1063/1.5054656", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric model based on Markov Chain Monte Carlo\n(MCMC) methods for the joint reconstruction and prediction of discrete time\nstochastic dynamical systems, based on $m$-multiple time-series data, perturbed\nby additive dynamical noise. We introduce the Pairwise Dependent Geometric\nStick-Breaking Reconstruction (PD-GSBR) model, which relies on the construction\nof a $m$-variate nonparametric prior over the space of densities supported over\n$\\mathbb{R}^m$. We are focusing in the case where at least one of the\ntime-series has a sufficiently large sample size representation for an\nindependent and accurate Geometric Stick-Breaking estimation, as defined in\nMerkatas et al. (2017). Our contention, is that whenever the dynamical error\nprocesses perturbing the underlying dynamical systems share common\ncharacteristics, underrepresented data sets can benefit in terms of model\nestimation accuracy. The PD-GSBR estimation and prediction procedure is\ndemonstrated specifically in the case of maps with polynomial nonlinearities of\nan arbitrary degree. Simulations based on synthetic time-series are presented.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:34:27 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 07:05:32 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hatjispyros", "Spyridon J.", ""], ["Merkatas", "Christos", ""]]}, {"id": "1811.07829", "submitter": "Simon Lunagomez", "authors": "Sim\\'on Lunag\\'omez, Marios Papamichalis, Patrick J. Wolfe, Edoardo M.\n  Airoldi", "title": "Evaluating and Optimizing Network Sampling Designs: Decision Theory and\n  Information Theory Perspectives", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the most used sampling mechanisms that implicitly leverage a social\nnetwork depend on tuning parameters; for instance, Respondent-Driven Sampling\n(RDS) is specified by the number of seeds and maximum number of referrals. We\nare interested in the problem of optimizing these sampling mechanisms with\nrespect to their tuning parameters in order to optimize the inference on a\npopulation quantity, where such quantity is a function of the network and\nmeasurements taken at the nodes. This is done by formulating the problem in\nterms of decision theory and information theory, in turn. We discuss how the\napproaches discussed in this paper relate, via theoretical results, to other\nformalisms aimed to compare sampling designs, namely sufficiency and the\nGoel-DeGroot Criterion. The optimization procedure for different network\nsampling mechanisms is illustrated via simulations in the fashion of the ones\nused for Bayesian clinical trials.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:40:02 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 16:22:57 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 09:44:19 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 13:34:05 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lunag\u00f3mez", "Sim\u00f3n", ""], ["Papamichalis", "Marios", ""], ["Wolfe", "Patrick J.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1811.08042", "submitter": "Yongqiang Tang", "authors": "Yongqiang Tang", "title": "A monotone data augmentation algorithm for multivariate nonnormal data:\n  with applications to controlled imputations for longitudinal trials", "comments": "20 pages, 2 figures, 3 tables", "journal-ref": "Statistics in Medicine 2018", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient monotone data augmentation (MDA) algorithm is proposed for\nmissing data imputation for incomplete multivariate nonnormal data that may\ncontain variables of different types, and are modeled by a sequence of\nregression models including the linear, binary logistic, multinomial logistic,\nproportional odds, Poisson, negative binomial, skew-normal, skew-t regressions\nor a mixture of these models. The MDA algorithm is applied to the sensitivity\nanalyses of longitudinal trials with nonignorable dropout using the controlled\npattern imputations that assume the treatment effect reduces or disappears\nafter subjects in the experimental arm discontinue the treatment. We also\ndescribe a heuristic approach to implement the controlled imputation, in which\nthe fully conditional specification method is used to impute the intermediate\nmissing data to create a monotone missing pattern, and the missing data after\ndropout are then imputed according to the assumed nonignorable mechanisms. The\nproposed methods are illustrated by simulation and real data analyses.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 02:13:46 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Tang", "Yongqiang", ""]]}, {"id": "1811.08083", "submitter": "Youngki Shin", "authors": "Seojeong Lee and Youngki Shin", "title": "Complete Subset Averaging with Many Instruments", "comments": "56 pages, 3 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a two-stage least squares (2SLS) estimator whose first stage is\nthe equal-weighted average over a complete subset with $k$ instruments among\n$K$ available, which we call the complete subset averaging (CSA) 2SLS. The\napproximate mean squared error (MSE) is derived as a function of the subset\nsize $k$ by the Nagar (1959) expansion. The subset size is chosen by minimizing\nthe sample counterpart of the approximate MSE. We show that this method\nachieves the asymptotic optimality among the class of estimators with different\nsubset sizes. To deal with averaging over a growing set of irrelevant\ninstruments, we generalize the approximate MSE to find that the optimal $k$ is\nlarger than otherwise. An extensive simulation experiment shows that the\nCSA-2SLS estimator outperforms the alternative estimators when instruments are\ncorrelated. As an empirical illustration, we estimate the logistic demand\nfunction in Berry, Levinsohn, and Pakes (1995) and find the CSA-2SLS estimate\nis better supported by economic theory than the alternative estimates.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 05:46:37 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 16:55:18 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 12:21:40 GMT"}, {"version": "v4", "created": "Mon, 9 Dec 2019 13:52:19 GMT"}, {"version": "v5", "created": "Thu, 28 May 2020 16:07:57 GMT"}, {"version": "v6", "created": "Wed, 26 Aug 2020 04:25:08 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Lee", "Seojeong", ""], ["Shin", "Youngki", ""]]}, {"id": "1811.08357", "submitter": "Danica J. Sutherland", "authors": "Li Wenliang, Danica J. Sutherland, Heiko Strathmann, Arthur Gretton", "title": "Learning deep kernels for exponential family densities", "comments": null, "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning (ICML 2019), PMLR 97:6737-6746", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel exponential family is a rich class of distributions, which can be\nfit efficiently and with statistical guarantees by score matching. Being\nrequired to choose a priori a simple kernel such as the Gaussian, however,\nlimits its practical applicability. We provide a scheme for learning a kernel\nparameterized by a deep network, which can find complex location-dependent\nlocal features of the data geometry. This gives a very rich class of density\nmodels, capable of fitting complex structures on moderate-dimensional problems.\nCompared to deep density models fit via maximum likelihood, our approach\nprovides a complementary set of strengths and tradeoffs: in empirical studies,\nthe former can yield higher likelihoods, whereas the latter gives better\nestimates of the gradient of the log density, the score, which describes the\ndistribution's shape.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:40:45 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 18:32:27 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 22:32:00 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 18:37:55 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wenliang", "Li", ""], ["Sutherland", "Danica J.", ""], ["Strathmann", "Heiko", ""], ["Gretton", "Arthur", ""]]}, {"id": "1811.08472", "submitter": "Giri Gopalan", "authors": "Giri Gopalan, Birgir Hrafnkelsson, Christopher K. Wikle, H{\\aa}vard\n  Rue, Gu{\\dh}finna A{\\dh}algeirsd\\'ottir, Alexander H. Jarosch, Finnur\n  P\\'alsson", "title": "A Hierarchical Spatio-Temporal Statistical Model Motivated by Glaciology", "comments": "Revision accepted for publication by the Journal of Agricultural,\n  Biological, and Environmental Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend and analyze a Bayesian hierarchical spatio-temporal\nmodel for physical systems. A novelty is to model the discrepancy between the\noutput of a computer simulator for a physical process and the actual process\nvalues with a multivariate random walk. For computational efficiency, linear\nalgebra for bandwidth limited matrices is utilized, and first-order emulator\ninference allows for the fast emulation of a numerical partial differential\nequation (PDE) solver. A test scenario from a physical system motivated by\nglaciology is used to examine the speed and accuracy of the computational\nmethods used, in addition to the viability of modeling assumptions. We conclude\nby discussing how the model and associated methodology can be applied in other\nphysical contexts besides glaciology.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 20:29:44 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 23:16:07 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Gopalan", "Giri", ""], ["Hrafnkelsson", "Birgir", ""], ["Wikle", "Christopher K.", ""], ["Rue", "H\u00e5vard", ""], ["A\u00f0algeirsd\u00f3ttir", "Gu\u00f0finna", ""], ["Jarosch", "Alexander H.", ""], ["P\u00e1lsson", "Finnur", ""]]}, {"id": "1811.08478", "submitter": "Sandipan Pramanik", "authors": "Sandipan Pramanik, Valen E. Johnson and Anirban Bhattacharya", "title": "A Modified Sequential Probability Ratio Test", "comments": "50 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a modified sequential probability ratio test that can be used to\nreduce the average sample size required to perform statistical hypothesis tests\nat specified levels of significance and power. Examples are provided for $z$\ntests, $t$ tests, and tests of binomial success probabilities. A description of\na software package to implement the test designs is provided. We compare the\nsample sizes required in fixed design tests conducted at 5$\\%$ significance\nlevels to the average sample sizes required in sequential tests conducted at\n0.5$\\%$ significance levels, and we find that the two sample sizes are\napproximately equal.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 20:45:49 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 00:01:38 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 22:06:57 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 17:06:09 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Pramanik", "Sandipan", ""], ["Johnson", "Valen E.", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1811.08595", "submitter": "Vahid Tadayon", "authors": "Vahid Tadayon", "title": "On an Extension of Stochastic Approximation EM Algorithm for Incomplete\n  Data Problems", "comments": "14th Iranian Statistics Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stochastic Approximation EM (SAEM) algorithm, a variant stochastic\napproximation of EM, is a versatile tool for inference in incomplete data\nmodels. In this paper, we review the fundamental EM algorithm and then focus\nespecially on the stochastic version of EM. In order to construct the SAEM, the\nalgorithm combines EM with a variant of stochastic approximation that uses\nMarkov chain Monte-Carlo to deal with the missing data. The algorithm is\nintroduced in general form and can be used to a wide range of problems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 04:23:45 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 19:30:57 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Tadayon", "Vahid", ""]]}, {"id": "1811.08803", "submitter": "Luke O'Connor", "authors": "Luke J. O'Connor and Alkes L. Price", "title": "Distinguishing correlation from causation using genome-wide association\n  studies", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": "O'Connor, Luke J. and Alkes L. Price. \"Distinguishing genetic\n  correlation from causation across 52 diseases and complex traits.\" Nature\n  genetics (2018)", "doi": null, "report-no": "ML4H/2018/4", "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Genome-wide association studies (GWAS) have emerged as a rich source of\ngenetic clues into disease biology, and they have revealed strong genetic\ncorrelations among many diseases and traits. Some of these genetic correlations\nmay reflect causal relationships. We developed a method to quantify causal\nrelationships between genetically correlated traits using GWAS summary\nassociation statistics. In particular, our method quantifies what part of the\ngenetic component of trait 1 is also causal for trait 2 using mixed fourth\nmoments $E(\\alpha_1^2\\alpha_1\\alpha_2)$ and $E(\\alpha_2^2\\alpha_1\\alpha_2)$ of\nthe bivariate effect size distribution. If trait 1 is causal for trait 2, then\nSNPs affecting trait 1 (large $\\alpha_1^2$) will have correlated effects on\ntrait 2 (large $\\alpha_1\\alpha_2$), but not vice versa. We validated this\napproach in extensive simulations. Across 52 traits (average $N=331$k), we\nidentified 30 putative genetically causal relationships, many novel, including\nan effect of LDL cholesterol on decreased bone mineral density. More broadly,\nwe demonstrate that it is possible to distinguish between genetic correlation\nand causation using genetic association data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:01:51 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["O'Connor", "Luke J.", ""], ["Price", "Alkes L.", ""]]}, {"id": "1811.08836", "submitter": "Albert Vexler", "authors": "Albert Vexler, Georgios Afendras, Marianthi Markatou", "title": "Multi-Panel Kendall Plot in Light of an ROC Curve Analysis Applied to\n  Measuring Dependence", "comments": "Statistics: A Journal of Theoretical and Applied Statistics. Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kendall plot ($\\K$-plot) is a plot measuring dependence between the\ncomponents of a bivariate random variable. The $\\K$-plot graphs the Kendall\ndistribution function against the distribution function of $VU$, where $V$ and\n$U$ are independent uniform $[0,1]$ random variables. We associate $\\K$-plots\nwith the receiver operating characteristic ($\\ROC$) curve, a well-accepted\ngraphical tool in biostatistics for evaluating the ability of a biomarker to\ndiscriminate between two populations. The most commonly used global index of\ndiagnostic accuracy of biomarkers is the area under the $\\ROC$ curve ($\\AUC$).\nIn parallel with the $\\AUC$, we propose a novel strategy to measure the\nassociation between random variables from a continuous bivariate distribution.\nFirst, we discuss why the area under the conventional Kendall curve ($\\AUK$)\ncannot be used as an index of dependence. We then suggest a simple and\nmeaningful extension of the definition of the $\\K$-plots and define an index of\ndependence that is based on $\\AUK$. This measure characterizes a wide range of\ntwo-variable relationships, thereby completely detecting the underlying\ndependence structure. Properties of the proposed index satisfy the mathematical\ndefinition of a measure. Finally, simulations and real data examples illustrate\nthe applicability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:27:05 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Vexler", "Albert", ""], ["Afendras", "Georgios", ""], ["Markatou", "Marianthi", ""]]}, {"id": "1811.08872", "submitter": "Aditi Shenvi", "authors": "Aditi Shenvi (1) and Jim Q. Smith (1 and 2) ((1) The University of\n  Warwick, (2) The Alan Turing Institute)", "title": "A Bayesian Dynamic Graphical Model for Recurrent Events in Public Health", "comments": "50 pages including supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyze the impacts of certain types of public health interventions we\nneed to estimate the treatment effects and outcomes as these apply to\nheterogeneous open populations. Dynamically modifying populations containing\nrisk groups that can react very differently to changes in covariates are\ninferentially challenging. Here we propose a novel Bayesian graphical model\ncalled the Reduced Dynamic Chain Event Graph (RDCEG) customized to such\npopulations. These models generalize the tree-based Chain Event Graphs to a\nparticular class of graphically supported semi-Markov processes. They provide\nan interface between natural language explanations about what might be\nhappening to individuals and a formal statistical analysis. Here we show how\nthe RDCEG is able to express the different possible progressions of each\nvulnerable individual as well as hypotheses about probabilistic symmetries\nwithin these progressions across different individuals within that population.\nWe demonstrate how well-developed Bayesian Network technologies can be\ntransferred almost seamlessly to this class. Our work is motivated by the\nchallenge of modeling non-pharmacological interventions for recurrent event\nprocesses. We illustrate our methodology in two settings: an intervention to\nreduce falls among the elderly and a trial to examine effects of deferred\ntreatment among individuals presenting with early epilepsy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 18:36:50 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 15:31:45 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Shenvi", "Aditi", "", "1 and 2"], ["Smith", "Jim Q.", "", "1 and 2"]]}, {"id": "1811.09101", "submitter": "Anthony J  Webster", "authors": "A.J. Webster", "title": "Multi-stage models for the failure of complex systems, cascading\n  disasters, and the onset of disease", "comments": null, "journal-ref": "PLoS ONE 14 (5): e0216422, (2019)", "doi": "10.1371/journal.pone.0216422", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems can fail through different routes, often progressing through\na series of (rate-limiting) steps and modified by environmental exposures. The\nonset of disease, cancer in particular, is no different. Multi-stage models\nprovide a simple but very general mathematical framework for studying the\nfailure of complex systems, or equivalently, the onset of disease. They include\nthe Armitage Doll multi-stage cancer model as a particular case, and have\npotential to provide new insights into how failures and disease, arise and\nprogress. A method described by E.T. Jaynes is developed to provide an\nanalytical solution for a large class of these models, and highlights\nconnections between the convolution of Laplace transforms, sums of random\nvariables, and Schwinger/Feynman parameterisations. Examples include: exact\nsolutions to the Armitage-Doll model, the sum of Gamma-distributed variables\nwith integer-valued shape parameters, a clonal-growth cancer model, and a model\nfor cascading disasters. Applications and limitations of the approach are\ndiscussed in the context of recent cancer research. The model is sufficiently\ngeneral to be used in many contexts, such as engineering, project management,\ndisease progression, and disaster risk for example, allowing the estimation of\nfailure rates in complex systems and projects. The intended result is a\nmathematical toolkit for applying multi-stage models to the study of failure\nrates in complex systems and to the onset of disease, cancer in particular.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 10:46:44 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 15:57:55 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 13:07:31 GMT"}, {"version": "v4", "created": "Tue, 23 Apr 2019 08:58:34 GMT"}, {"version": "v5", "created": "Tue, 30 Apr 2019 13:47:34 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Webster", "A. J.", ""]]}, {"id": "1811.09540", "submitter": "Sokbae Lee", "authors": "Le-Yu Chen and Sokbae Lee", "title": "High Dimensional Classification through $\\ell_0$-Penalized Empirical\n  Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a high dimensional binary classification problem and construct a\nclassification procedure by minimizing the empirical misclassification risk\nwith a penalty on the number of selected features. We derive non-asymptotic\nprobability bounds on the estimated sparsity as well as on the excess\nmisclassification risk. In particular, we show that our method yields a sparse\nsolution whose l0-norm can be arbitrarily close to true sparsity with high\nprobability and obtain the rates of convergence for the excess\nmisclassification risk. The proposed procedure is implemented via the method of\nmixed integer linear programming. Its numerical performance is illustrated in\nMonte Carlo experiments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:26:27 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Chen", "Le-Yu", ""], ["Lee", "Sokbae", ""]]}, {"id": "1811.09623", "submitter": "Qing Xu", "authors": "Qing Xu, Xiaohua Xuan", "title": "Nonlinear Regression without i.i.d. Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of nonlinear regression problems without\nthe assumption of being independent and identically distributed. We propose a\ncorrespondent mini-max problem for nonlinear regression and give a numerical\nalgorithm. Such an algorithm can be applied in regression and machine learning\nproblems, and yield better results than traditional least square and machine\nlearning methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 06:25:20 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 10:33:21 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xu", "Qing", ""], ["Xuan", "Xiaohua", ""]]}, {"id": "1811.09734", "submitter": "Won Chang", "authors": "Won Chang, Sunghoon Kim, Heewon Chae", "title": "A Regularized Spatial Market Segmentation Method with Dirichlet Process\n  Gaussian Mixture Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially referenced data are increasingly available thanks to the\ndevelopment of modern GPS technology. They also provide rich opportunities for\nspatial analytics in the field of marketing science. Our main interest is to\npropose a new efficient statistical framework to conduct spatial segmentation\nanalysis for restaurants located in a metropolitan area in the U.S. The spatial\nsegmentation problem poses important statistical challenges: selecting the\noptimal number of underlying structures of market segments, capturing complex\nand flexible spatial structures, and resolving any possible small n and large p\nissue which can be typical in latent class analysis. Existing approaches try to\ntackle these issues in heuristic ways or seem silent on them. To overcome these\nchallenges, we propose a new statistical framework based on regularized\nBayesian spatial mixture regressions with Dirichlet process integrating ridge\nor lasso regularization. Our simulation study demonstrates that the proposed\nmodels successfully recover the underlying spatial clustering structures and\noutperforms two existing benchmark models. In the empirical analysis using\nonline customer satisfaction data from the Yelp, our models provides\ninteresting insights on segment-level key drivers of customer satisfaction and\ninterpretable relationships between regional demographics and restaurants'\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 01:01:44 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Chang", "Won", ""], ["Kim", "Sunghoon", ""], ["Chae", "Heewon", ""]]}, {"id": "1811.09824", "submitter": "Marius Thomas", "authors": "Marius Thomas, Bj\\\"orn Bornkamp, Martin Posch, Franz K\\\"onig", "title": "A multiple comparison procedure for dose-finding trials with\n  subpopulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying subgroups of patients with an enhanced response to a new\ntreatment has become an area of increased interest in the last few years. When\nthere is knowledge about possible subpopulations with an enhanced treatment\neffect before the start of a trial it might be beneficial to set up a testing\nstrategy, which tests for a significant treatment effect not only in the full\npopulation, but also in these prespecified subpopulations. In this paper we\npresent a parametric multiple testing approach for tests in multiple\npopulations for dose-finding trials. Our approach is based on the MCP-Mod\nmethodology, which uses multiple comparison procedures to test for a\ndose-response signal, while considering multiple possible candidate\ndose-response shapes. Our proposed methods allow for heteroscedasticity between\npopulations and control the FWER over tests in multiple populations and for\nmultiple candidate models. We show in simulations, that the proposed\nmulti-population testing approaches can increase the power to detect a\nsignificant dose-response signal over the standard single-population MCP-Mod,\nwhen the considered subpopulation has an enhanced treatment effect.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 12:44:44 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Thomas", "Marius", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Posch", "Martin", ""], ["K\u00f6nig", "Franz", ""]]}, {"id": "1811.09837", "submitter": "Sami Stouli", "authors": "Whitney K. Newey and Sami Stouli", "title": "Heterogenous Coefficients, Discrete Instruments, and Identification of\n  Treatment Effects", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multidimensional heterogeneity and endogeneity are important features of a\nwide class of econometric models. We consider heterogenous coefficients models\nwhere the outcome is a linear combination of known functions of treatment and\nheterogenous coefficients. We use control variables to obtain identification\nresults for average treatment effects. With discrete instruments in a\ntriangular model we find that average treatment effects cannot be identified\nwhen the number of support points is less than or equal to the number of\ncoefficients. A sufficient condition for identification is that the second\nmoment matrix of the treatment functions given the control is nonsingular with\nprobability one. We relate this condition to identification of average\ntreatment effects with multiple treatments.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 14:08:46 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Newey", "Whitney K.", ""], ["Stouli", "Sami", ""]]}, {"id": "1811.09965", "submitter": "Jingyi Jessica Li", "authors": "Jingyi Jessica Li, Xin Tong, Peter J. Bickel", "title": "Generalized Pearson correlation squares for capturing mixtures of\n  bivariate linear dependences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivated by the pressing needs for capturing complex but interpretable\nvariable relationships in scientific research, here we generalize the squared\nPearson correlation to capture a mixture of linear dependences between two\nreal-valued random variables, with or without an index variable that specifies\nthe line memberships. We construct generalized Pearson correlation squares by\nfocusing on three aspects: the exchangeability of the two variables, the\nindependence of parametric model assumptions, and the availability of\npopulation-level parameters. For the computation of the generalized Pearson\ncorrelation square from a sample without line-membership specification, we\ndevelop a K-lines clustering algorithm, where K, the number of lines, can be\nchosen in a data-adaptive way. With our defined population-level generalized\nPearson correlation squares, we derive the asymptotic distributions of the\nsample-level statistics to enable efficient statistical inference. Simulation\nstudies verify the theoretical results and compare the generalized Pearson\ncorrelation squares with other widely-used association measures in terms of\npower. Gene expression data analysis demonstrates the effectiveness of the\ngeneralized Pearson correlation squares in capturing interpretable gene-gene\nrelationships missed by other measures. We implement the estimation and\ninference procedures in an R package gR2.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 07:15:49 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 05:14:41 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 23:42:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Li", "Jingyi Jessica", ""], ["Tong", "Xin", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1811.10147", "submitter": "Pamela Shaw", "authors": "Pamela Shaw, Jiwei He, Bryan Shepherd", "title": "Regression calibration to correct correlated errors in outcome and\n  exposure", "comments": "48 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement error arises through a variety of mechanisms. A rich literature\nexists on the bias introduced by covariate measurement error and on methods of\nanalysis to address this bias. By comparison, less attention has been given to\nerrors in outcome assessment and non-classical covariate measurement error. We\nconsider an extension of the regression calibration method to settings with\nerrors in a continuous outcome, where the errors may be correlated with\nprognostic covariates or with covariate measurement error. This method adjusts\nfor the measurement error in the data and can be applied with either a\nvalidation subset, on which the true data are also observed (e.g., a study\naudit), or a reliability subset, where a second observation of error prone\nmeasurements are available. For each case, we provide conditions under which\nthe proposed method is identifiable and leads to unbiased estimates of the\nregression parameter. When the second measurement on the reliability subset has\nno error or classical unbiased measurement error, the proposed method is\nunbiased even when the primary outcome and exposures of interest are subject to\nboth systematic and random error. We examine the performance of the method with\nsimulations for a variety of measurement error scenarios and sizes of the\nreliability subset. We illustrate the method's application using data from the\nWomen's Health Initiative Dietary Modification Trial.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 02:33:29 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Shaw", "Pamela", ""], ["He", "Jiwei", ""], ["Shepherd", "Bryan", ""]]}, {"id": "1811.10179", "submitter": "Chitradipa Chakraborty", "authors": "Chitradipa Chakraborty", "title": "Testing Multivariate Scatter Parameter in Elliptical Model based on\n  Forward Search Method", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we establish a test for multivariate scatter parameter in\nelliptical model, where the location parameter is known, and the scatter\nparameter is estimated by the multivariate forward search method. The\nconsistency property of the test is also studied here. Inter alia, we\ninvestigate the performances of the test for various simulated data, and\ncompare them with those of a classical one.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 05:13:30 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Chakraborty", "Chitradipa", ""]]}, {"id": "1811.10192", "submitter": "He Zhou", "authors": "He Zhou, Yi Yang and Wei Qian", "title": "Tweedie Gradient Boosting for Extremely Unbalanced Zero-inflated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweedie's compound Poisson model is a popular method to model insurance\nclaims with probability mass at zero and nonnegative, highly right-skewed\ndistribution. In particular, it is not uncommon to have extremely unbalanced\ndata with excessively large proportion of zero claims, and even traditional\nTweedie model may not be satisfactory for fitting the data. In this paper, we\npropose a boosting-assisted zero-inflated Tweedie model, called EMTboost, that\nallows zero probability mass to exceed a traditional model. We makes a\nnonparametric assumption on its Tweedie model component, that unlike a linear\nmodel, is able to capture nonlinearities, discontinuities, and complex higher\norder interactions among predictors. A specialized Expectation-Maximization\nalgorithm is developed that integrates a blockwise coordinate descent strategy\nand a gradient tree-boosting algorithm to estimate key model parameters. We use\nextensive simulation and data analysis on synthetic zero-inflated\nauto-insurance claim data to illustrate our method's prediction performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:11:34 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 00:22:24 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Zhou", "He", ""], ["Yang", "Yi", ""], ["Qian", "Wei", ""]]}, {"id": "1811.10223", "submitter": "Jia Zhao", "authors": "Jia Zhao, Jingsi Ming, Xianghong Hu, Gang Chen, Jin Liu and Can Yang", "title": "Bayesian Weighted Mendelian Randomization for Causal Inference based on\n  Summary Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results from Genome-Wide Association Studies (GWAS) on thousands of\nphenotypes provide an unprecedented opportunity to infer the causal effect of\none phenotype (exposure) on another (outcome). Mendelian randomization (MR), an\ninstrumental variable (IV) method, has been introduced for causal inference\nusing GWAS data. Due to the polygenic architecture of complex traits/diseases\nand the ubiquity of pleiotropy, however, MR has many unique challenges compared\nto conventional IV methods. We propose a Bayesian weighted Mendelian\nrandomization (BWMR) for causal inference to address these challenges. In our\nBWMR model, the uncertainty of weak effects owing to polygenicity has been\ntaken into account and the violation of IV assumption due to pleiotropy has\nbeen addressed through outlier detection by Bayesian weighting. To make the\ncausal inference based on BWMR computationally stable and efficient, we\ndeveloped a variational expectation-maximization (VEM) algorithm. Moreover, we\nhave also derived an exact closed-form formula to correct the posterior\ncovariance which is often underestimated in variational inference. Through\ncomprehensive simulation studies, we evaluated the performance of BWMR,\ndemonstrating the advantage of BWMR over its competitors. Then we applied BWMR\nto make causal inference between 130 metabolites and 93 complex human traits,\nuncovering novel causal relationship between exposure and outcome traits. The\nBWMR software is available at https://github.com/jiazhao97/BWMR.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 08:01:26 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 07:30:00 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zhao", "Jia", ""], ["Ming", "Jingsi", ""], ["Hu", "Xianghong", ""], ["Chen", "Gang", ""], ["Liu", "Jin", ""], ["Yang", "Can", ""]]}, {"id": "1811.10287", "submitter": "Leonhard Held", "authors": "Leonhard Held", "title": "A New Standard for the Analysis and Design of Replication Studies", "comments": "Revised Manuscript for RSS Discussion Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new standard is proposed for the evidential assessment of replication\nstudies. The approach combines a specific reverse-Bayes technique with\nprior-predictive tail probabilities to define replication success. The method\ngives rise to a quantitative measure for replication success, called the\nsceptical p-value. The sceptical p-value integrates traditional significance of\nboth the original and replication study with a comparison of the respective\neffect sizes. It incorporates the uncertainty of both the original and\nreplication effect estimates and reduces to the ordinary p-value of the\nreplication study if the uncertainty of the original effect estimate is\nignored. The proposed framework can also be used to determine the power or the\nrequired replication sample size to achieve replication success. Numerical\ncalculations highlight the difficulty to achieve replication success if the\nevidence from the original study is only suggestive. An application to data\nfrom the Open Science Collaboration project on the replicability of\npsychological science illustrates the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 11:04:24 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 11:10:53 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Held", "Leonhard", ""]]}, {"id": "1811.10292", "submitter": "Alexander Meier", "authors": "Alexander Meier, Claudia Kirch, Renate Meyer", "title": "Bayesian Nonparametric Analysis of Multivariate Time Series: A Matrix\n  Gamma Process Approach", "comments": "40 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there is an increasing amount of literature about Bayesian time series\nanalysis, only a few Bayesian nonparametric approaches to multivariate time\nseries exist. Most methods rely on Whittle's Likelihood, involving the second\norder structure of a stationary time series by means of its spectral density\nmatrix. This is often modeled in terms of the Cholesky decomposition to ensure\npositive definiteness. However, asymptotic properties such as posterior\nconsistency or posterior contraction rates are not known. A different idea is\nto model the spectral density matrix by means of random measures. This is in\nline with existing approaches for the univariate case, where the normalized\nspectral density is modeled similar to a probability density, e.g. with a\nDirichlet process mixture of Beta densities. In this work, we present a related\napproach for multivariate time series, with matrix-valued mixture weights\ninduced by a Hermitian positive definite Gamma process. The proposed procedure\nis shown to perform well for both simulated and real data. Posterior\nconsistency and contraction rates are also established.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 11:14:43 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Meier", "Alexander", ""], ["Kirch", "Claudia", ""], ["Meyer", "Renate", ""]]}, {"id": "1811.10347", "submitter": "Sonali Parbhoo", "authors": "Sonali Parbhoo, Mario Wieser and Volker Roth", "title": "Estimating Causal Effects With Partial Covariates For Clinical\n  Interpretability", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the causal effects of an intervention in the presence of\nconfounding is a frequently occurring problem in applications such as medicine.\nThe task is challenging since there may be multiple confounding factors, some\nof which may be missing, and inferences must be made from high-dimensional,\nnoisy measurements. In this paper, we propose a decision-theoretic approach to\nestimate the causal effects of interventions where a subset of the covariates\nis unavailable for some patients during testing. Our approach uses the\ninformation bottleneck principle to perform a discrete, low-dimensional\nsufficient reduction of the covariate data to estimate a distribution over\nconfounders. In doing so, we can estimate the causal effect of an intervention\nwhere only partial covariate information is available. Our results on a causal\ninference benchmark and a real application for treating sepsis show that our\nmethod achieves state-of-the-art performance, without sacrificing\ninterpretability.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:10:42 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Parbhoo", "Sonali", ""], ["Wieser", "Mario", ""], ["Roth", "Volker", ""]]}, {"id": "1811.10411", "submitter": "Rida Benhaddou", "authors": "Rida Benhaddou and Qing Liu", "title": "Minimax adaptive wavelet estimator for the anisotropic functional\n  deconvolution model with unknown kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we consider the estimation of a periodic\ntwo-dimensional function $f(\\cdot,\\cdot)$ based on observations from its noisy\nconvolution, and convolution kernel $g(\\cdot,\\cdot)$ unknown. We derive the\nminimax lower bounds for the mean squared error assuming that $f$ belongs to\ncertain Besov space and the kernel function $g$ satisfies some smoothness\nproperties. We construct an adaptive hard-thresholding wavelet estimator that\nis asymptotically near-optimal within a logarithmic factor in a wide range of\nBesov balls. The proposed estimation algorithm implements a truncation to\nestimate the wavelet coefficients, in addition to the conventional\nhard-thresholds. A limited simulations study confirms theoretical claims of the\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 02:30:59 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 18:55:09 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Benhaddou", "Rida", ""], ["Liu", "Qing", ""]]}, {"id": "1811.10438", "submitter": "Faustino Prieto", "authors": "Faustino Prieto, Jos\\'e Mar\\'ia Sarabia, Enrique Calder\\'in-Ojeda", "title": "The nonlinear distribution of employment across municipalities", "comments": "This is a preprint (29 pages, 5 tables, 8 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the nonlinear distribution of employment across Spanish\nmunicipalities is analyzed. In addition, we explore the properties of the\nfamily of generalized power law (GPL) distributions, and test its adequacy for\nmodelling employment data. The hierarchical structure of the GPL family that\nincludes the hierarchy of Pareto (power law) distributions is deeply studied. A\nnew subfamily of heavy-tailed GPL distributions that is right tail equivalent\nto a Pareto (power law) model is derived. Our findings show on the one hand\nthat the distribution of employment across Spanish municipalities follows a\npower law behavior in the upper tail and, on the other hand, the adequacy of\nGPL models for modelling employment data in the whole range of the\ndistribution.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:38:31 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Prieto", "Faustino", ""], ["Sarabia", "Jos\u00e9 Mar\u00eda", ""], ["Calder\u00edn-Ojeda", "Enrique", ""]]}, {"id": "1811.10443", "submitter": "Andreas Elsener", "authors": "Andreas Elsener and Sara van de Geer", "title": "Sparse spectral estimation with missing and corrupted measurements", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning methods with missing data have been extensively studied\nnot just due to the techniques related to low-rank matrix completion. Also in\nunsupervised learning one often relies on imputation methods. As a matter of\nfact, missing values induce a bias in various estimators such as the sample\ncovariance matrix. In the present paper, a convex method for sparse subspace\nestimation is extended to the case of missing and corrupted measurements. This\nis done by correcting the bias instead of imputing the missing values. The\nestimator is then used as an initial value for a nonconvex procedure to improve\nthe overall statistical performance. The methodological as well as theoretical\nframeworks are applied to a wide range of statistical problems. These include\nsparse Principal Component Analysis with different types of randomly missing\ndata and the estimation of eigenvectors of low-rank matrices with missing\nvalues. Finally, the statistical performance is demonstrated on synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 15:24:31 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Elsener", "Andreas", ""], ["van de Geer", "Sara", ""]]}, {"id": "1811.10446", "submitter": "Truong-Vinh Hoang", "authors": "Truong-Vinh Hoang and Hermann G. Matthies", "title": "Non-deterministic inference using random set models: theory,\n  approximation, and sampling method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random set is a generalisation of a random variable, i.e. a set-valued\nrandom variable. The random set theory allows a unification of other\nuncertainty descriptions such as interval variable, mass belief function in\nDempster-Shafer theory of evidence, possibility theory, and set of probability\ndistributions. The aim of this work is to develop a non-deterministic inference\nframework, including theory, approximation and sampling method, that deals with\nthe inverse problems in which uncertainty is represented using random sets. The\nproposed inference method yields the posterior random set based on the\nintersection of the prior and the measurement induced random sets. That\ninference method is an extension of Dempster's rule of combination, and a\ngeneralisation of Bayesian inference as well. A direct evaluation of the\nposterior random set might be impractical. We approximate the posterior random\nset by a random discrete set whose domain is the set of samples generated using\na proposed probability distribution. We use the capacity transform density\nfunction of the posterior random set for this proposed distribution. This\nfunction has a special property: it is the posterior density function yielded\nby Bayesian inference of the capacity transform density function of the prior\nrandom set. The samples of such proposed probability distribution can be\ndirectly obtained using the methods developed in the Bayesian inference\nframework. With this approximation method, the evaluation of the posterior\nrandom set becomes tractable.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 15:27:05 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Hoang", "Truong-Vinh", ""], ["Matthies", "Hermann G.", ""]]}, {"id": "1811.10453", "submitter": "Katrina Devick", "authors": "Katrina L. Devick, Jennifer F. Bobb, Maitreyi Mazumdar, Birgit Claus\n  Henn, David C. Bellinger, David C. Christiani, Robert O. Wright, Paige L.\n  Williams, Brent A. Coull, and Linda Valeri", "title": "Bayesian kernel machine regression-causal mediation analysis", "comments": "18 pages, 9 figures, and 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greater understanding of the pathways through which an environmental mixture\noperates is important to design effective interventions. We present new\nmethodology to estimate the natural direct effect (NDE), natural indirect\neffect (NIE), and controlled direct effects (CDEs) of a complex mixture\nexposure on an outcome through a mediator variable. We implement Bayesian\nKernel Machine Regression (BKMR) to allow for all possible interactions and\nnonlinear effects of 1) the co-exposures on the mediator, 2) the co-exposures\nand mediator on the outcome, and 3) selected covariates on the mediator and/or\noutcome. From the posterior predictive distributions of the mediator and\noutcome, we simulate counterfactuals to obtain posterior samples, estimates,\nand credible intervals of the mediation effects. Our simulation study\ndemonstrates that when the exposure-mediator and exposure-mediator-outcome\nrelationships are complex, BKMR-Causal Mediation Analysis performs better than\ncurrent mediation methods. We applied our methodology to quantify the\ncontribution of birth length as a mediator between in utero co-exposure to\narsenic, manganese and lead, and children's neurodevelopmental scores, in a\nprospective birth cohort in Bangladesh. Among younger children, we found a\nnegative association between the metal mixture and neurodevelopment. We also\nfound evidence that birth length mediates the effect of exposure to the metal\nmixture on neurodevelopment for younger children. If birth length were fixed to\nits $75^{th}$ percentile value, the effect of the metal mixture on\nneurodevelopment decreases, suggesting that nutritional interventions to help\nincrease birth length could potentially block the harmful effects of the metal\nmixture on neurodevelopment.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 15:32:54 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 09:13:35 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Devick", "Katrina L.", ""], ["Bobb", "Jennifer F.", ""], ["Mazumdar", "Maitreyi", ""], ["Henn", "Birgit Claus", ""], ["Bellinger", "David C.", ""], ["Christiani", "David C.", ""], ["Wright", "Robert O.", ""], ["Williams", "Paige L.", ""], ["Coull", "Brent A.", ""], ["Valeri", "Linda", ""]]}, {"id": "1811.10486", "submitter": "Krzysztof Domino", "authors": "Krzysztof Domino", "title": "Selected Methods for non-Gaussian Data Analysis", "comments": "ISBN: 978-83-926054-3-0", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic goal of computer engineering is the analysis of data. Such data are\noften large data sets distributed according to various distribution models. In\nthis manuscript we focus on the analysis of non-Gaussian distributed data. In\nthe case of univariate data analysis we discuss stochastic processes with\nauto-correlated increments and univariate distributions derived from specific\nstochastic processes, i.e. Levy and Tsallis distributions. Deep investigation\nof multivariate non-Gaussian distributions requires the copula approach. A\ncopula is an component of multivariate distribution that models the mutual\ninterdependence between marginals. There are many copula families characterised\nby various measures of the dependence between marginals. Importantly, one of\nthose are `tail' dependencies that model the simultaneous appearance of extreme\nvalues in many marginals. Those extreme events may reflect a crisis given\nfinancial data, outliers in machine learning, or a traffic congestion. Next we\ndiscuss higher order multivariate cumulants that are non-zero if multivariate\ndistribution is non-Gaussian. However, the relation between cumulants and\ncopulas is not straight forward and rather complicated. We discuss the\napplication of those cumulants to extract information about non-Gaussian\nmultivariate distributions, such that information about non-Gaussian copulas.\nThe use of higher order multivariate cumulants in computer science is inspired\nby financial data analysis, especially by the safe investment portfolio\nevaluation. There are many other applications of higher order multivariate\ncumulants in data engineering, especially in: signal processing, non-linear\nsystem identification, blind sources separation, and direction finding\nalgorithms of multi-source signals.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 15:39:59 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 11:33:22 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 10:41:03 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Domino", "Krzysztof", ""]]}, {"id": "1811.10488", "submitter": "Marius Thomas", "authors": "Marius Thomas, Bj\\\"orn Bornkamp, Katja Ickstadt", "title": "Identifying treatment effect heterogeneity in dose-finding trials using\n  Bayesian hierarchical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in drug development is to identify patients, which respond\nbetter or worse to an experimental treatment. Identifying predictive\ncovariates, which influence the treatment effect and can be used to define\nsubgroups of patients, is a key aspect of this task. Analyses of treatment\neffect heterogeneity are however known to be challenging, since the number of\npossible covariates or subgroups is often large, while samples sizes in earlier\nphases of drug development are often small. In addition, distinguishing\npredictive covariates from prognostic covariates, which influence the response\nindependent of the given treatment, can often be difficult. While many\napproaches for these types of problems have been proposed, most of them focus\non the two-arm clinical trial setting, where patients are given either the\ntreatment or a control. In this paper we consider parallel groups dose-finding\ntrials, in which patients are administered different doses of the same\ntreatment. To investigate treatment effect heterogeneity in this setting we\npropose a Bayesian hierarchical dose-response model with covariate effects on\ndose-response parameters. We make use of shrinkage priors to prevent\noverfitting, which can easily occur, when the number of considered covariates\nis large and sample sizes are small. We compare several such priors in\nsimulations and also investigate dependent modeling of prognostic and\npredictive effects to better distinguish these two types of effects. We\nillustrate the use of our proposed approach using a Phase II dose-finding trial\nand show how it can be used to identify predictive covariates and subgroups of\npatients with increased treatment effects.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:26:26 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Thomas", "Marius", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Ickstadt", "Katja", ""]]}, {"id": "1811.10957", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic and Kristina Stemikovskaya", "title": "Subsampling (weighted smooth) empirical copula processes", "comments": "34 pages, 5 figures, 4 + 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key tool to carry out inference on the unknown copula when modeling a\ncontinuous multivariate distribution is a nonparametric estimator known as the\nempirical copula. One popular way of approximating its sampling distribution\nconsists of using the multiplier bootstrap. The latter is however characterized\nby a high implementation cost. Given the rank-based nature of the empirical\ncopula, the classical empirical bootstrap of Efron does not appear to be a\nnatural alternative, as it relies on resamples which contain ties. The aim of\nthis work is to investigate the use of subsampling in the aforementioned\nframework. The latter consists of basing the inference on statistic values\ncomputed from subsamples of the initial data. One of its advantages in the\nrank-based context under consideration is that the formed subsamples do not\ncontain ties. Another advantage is its asymptotic validity under minimalistic\nconditions. In this work, we show the asymptotic validity of subsampling for\nseveral (weighted, smooth) empirical copula processes both in the case of\nserially independent observations and time series. In the former case,\nsubsampling is observed to be substantially better than the empirical bootstrap\nand equivalent, overall, to the multiplier bootstrap in terms of finite-sample\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:17:28 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 14:23:28 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Stemikovskaya", "Kristina", ""]]}, {"id": "1811.10973", "submitter": "Eric Nyarko Mr.", "authors": "Eric Nyarko, Rainer Schwabe", "title": "Optimal Designs for Second-Order Interactions in Paired Comparison\n  Experiments with Binary Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In paired comparison experiments respondents usually evaluate pairs of\ncompeting options. For this situation we introduce an appropriate model and\nderive optimal designs in the presence of second-order interactions when all\nattributes are dichotomous.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:38:03 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 06:51:02 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Nyarko", "Eric", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1811.11011", "submitter": "John Galati", "authors": "John C. Galati", "title": "What is meant by 'P(R|Yobs)'?", "comments": "9 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1811.04161", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing at Random (MAR) is a central concept in incomplete data methods, and\noften it is stated as $P(R\\mspace{3mu}|\\,Y_{obs}, Y_{mis}) =\nP(R\\mspace{3mu}|\\,Y_{obs})$. This notation has been used in the literature for\nmore than three decades and has become the de facto standard. In some cases,\nthe notation has been misinterpreted to be a statement about conditional\nindependence. While previous work has sought to clarify the required\ndefinitions, a clear explanation of how to interpret the standard notation is\nlacking, and a definition of the function $P(R\\mspace{3mu}|\\,Y_{obs})$ for\nnon-MAR mechanisms is difficult to locate in the literature. The aim of this\npaper is to fill these gaps.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 04:00:30 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 22:16:34 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 00:11:12 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Galati", "John C.", ""]]}, {"id": "1811.11025", "submitter": "Wenying Deng", "authors": "Wenying Deng, Jeremiah Zhe Liu, Erin Lake, Brent A. Coull", "title": "CVEK: Robust Estimation and Testing for Nonlinear Effects using Kernel\n  Machine Ensemble", "comments": "5 figures. arXiv admin note: text overlap with arXiv:1710.01406", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package CVEK introduces a suite of flexible machine learning models and\nrobust hypothesis tests for learning the joint nonlinear effects of multiple\ncovariates in limited samples. It implements the Cross-validated Ensemble of\nKernels (CVEK)(Liu and Coull 2017), an ensemble-based kernel machine learning\nmethod that adaptively learns the joint nonlinear effect of multiple covariates\nfrom data, and provides powerful hypothesis tests for both main effects of\nfeatures and interactions among features. The R Package CVEK provides a\nflexible, easy-to-use implementation of CVEK, and offers a wide range of\nchoices for the kernel family (for instance, polynomial, radial basis\nfunctions, Mat\\'ern, neural network, and others), model selection criteria,\nensembling method (averaging, exponential weighting, cross-validated stacking),\nand the type of hypothesis test (asymptotic or parametric bootstrap). Through\nextensive simulations we demonstrate the validity and robustness of this\napproach, and provide practical guidelines on how to design an estimation\nstrategy for optimal performance in different data scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 09:20:47 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 23:04:02 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Deng", "Wenying", ""], ["Liu", "Jeremiah Zhe", ""], ["Lake", "Erin", ""], ["Coull", "Brent A.", ""]]}, {"id": "1811.11072", "submitter": "Luis Fernando Campos", "authors": "Luis F. Campos, Mark E. Glickman, Kristen B. Hunter", "title": "Measuring Effects of Medication Adherence on Time-Varying Health\n  Outcomes using Bayesian Dynamic Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most significant barriers to medication treatment is patients'\nnon-adherence to a prescribed medication regimen. The extent of the impact of\npoor adherence on resulting health measures is often unknown, and typical\nanalyses ignore the time-varying nature of adherence. This paper develops a\nmodeling framework for longitudinally recorded health measures modeled as a\nfunction of time-varying medication adherence or other time-varying covariates.\nOur framework, which relies on normal Bayesian dynamic linear models (DLMs),\naccounts for time-varying covariates such as adherence and non-dynamic\ncovariates such as baseline health characteristics. Given the inefficiencies\nusing standard inferential procedures for DLMs associated with infrequent and\nirregularly recorded response data, we develop an approach that relies on\nfactoring the posterior density into a product of two terms; a marginal\nposterior density for the non-dynamic parameters, and a multivariate normal\nposterior density of the dynamic parameters conditional on the non-dynamic\nones. This factorization leads to a two-stage process for inference in which\nthe non-dynamic parameters can be inferred separately from the time-varying\nparameters. We demonstrate the application of this model to the time-varying\neffect of anti-hypertensive medication on blood pressure levels from a cohort\nof patients diagnosed with hypertension. Our model results are compared to ones\nin which adherence is incorporated through non-dynamic summaries.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:08:45 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 22:15:05 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Campos", "Luis F.", ""], ["Glickman", "Mark E.", ""], ["Hunter", "Kristen B.", ""]]}, {"id": "1811.11139", "submitter": "Maria D. Ruiz-Medina", "authors": "M.P. Fr\\'ias, A. Torres-Signes, M.D. Ruiz-Medina and J. Mateu", "title": "Spatial Cox processes in an infinite-dimensional framework", "comments": "Submitted (25 pages, 18 figures and seven tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of spatial Cox processes driven by a Hilbert--valued\nrandom log--intensity. We adopt a parametric framework in the spectral domain,\nto estimate its spatial functional correlation structure. Specifically, we\nconsider a spectral functional, based on the periodogram operator, inspired on\nWhittle estimation methodology. Strong-consistency of the parametric estimator\nis proved in the linear case. We illustrate this property in a simulation study\nunder a Gaussian first order Spatial Autoregressive Hilbertian scenario for the\nlog--intensity model. Our method is applied to the spatial functional\nprediction of respiratory disease mortality in the Spanish Iberian Peninsula,\nin the period 1980--2015.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:01:36 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 16:01:22 GMT"}, {"version": "v3", "created": "Mon, 21 Jan 2019 22:25:38 GMT"}, {"version": "v4", "created": "Wed, 6 Feb 2019 19:17:57 GMT"}, {"version": "v5", "created": "Wed, 9 Oct 2019 14:40:30 GMT"}, {"version": "v6", "created": "Sun, 19 Jul 2020 19:44:03 GMT"}, {"version": "v7", "created": "Wed, 10 Mar 2021 18:20:18 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Fr\u00edas", "M. P.", ""], ["Torres-Signes", "A.", ""], ["Ruiz-Medina", "M. D.", ""], ["Mateu", "J.", ""]]}, {"id": "1811.11368", "submitter": "Yichen Zhang", "authors": "Xi Chen, Weidong Liu, Yichen Zhang", "title": "First-order Newton-type Estimator for Distributed Estimation and\n  Inference", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies distributed estimation and inference for a general\nstatistical problem with a convex loss that could be non-differentiable. For\nthe purpose of efficient computation, we restrict ourselves to stochastic\nfirst-order optimization, which enjoys low per-iteration complexity. To\nmotivate the proposed method, we first investigate the theoretical properties\nof a straightforward Divide-and-Conquer Stochastic Gradient Descent (DC-SGD)\napproach. Our theory shows that there is a restriction on the number of\nmachines and this restriction becomes more stringent when the dimension $p$ is\nlarge. To overcome this limitation, this paper proposes a new multi-round\ndistributed estimation procedure that approximates the Newton step only using\nstochastic subgradient. The key component in our method is the proposal of a\ncomputationally efficient estimator of $\\Sigma^{-1} w$, where $\\Sigma$ is the\npopulation Hessian matrix and $w$ is any given vector. Instead of estimating\n$\\Sigma$ (or $\\Sigma^{-1}$) that usually requires the second-order\ndifferentiability of the loss, the proposed First-Order Newton-type Estimator\n(FONE) directly estimates the vector of interest $\\Sigma^{-1} w$ as a whole and\nis applicable to non-differentiable losses. Our estimator also facilitates the\ninference for the empirical risk minimizer. It turns out that the key term in\nthe limiting covariance has the form of $\\Sigma^{-1} w$, which can be estimated\nby FONE.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:58:28 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 17:10:35 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Chen", "Xi", ""], ["Liu", "Weidong", ""], ["Zhang", "Yichen", ""]]}, {"id": "1811.11440", "submitter": "Max Baak", "authors": "M. Baak, R. Koopman, H. Snoek, S. Klous", "title": "A new correlation coefficient between categorical, ordinal and interval\n  variables with Pearson characteristics", "comments": "Submitted to Computational Statistics and Data analysis. See for\n  examples and Python code: https://phik.rtfd.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prescription is presented for a new and practical correlation coefficient,\n$\\phi_K$, based on several refinements to Pearson's hypothesis test of\nindependence of two variables. The combined features of $\\phi_K$ form an\nadvantage over existing coefficients. First, it works consistently between\ncategorical, ordinal and interval variables. Second, it captures non-linear\ndependency. Third, it reverts to the Pearson correlation coefficient in case of\na bi-variate normal input distribution. These are useful features when studying\nthe correlation between variables with mixed types. Particular emphasis is paid\nto the proper evaluation of statistical significance of correlations and to the\ninterpretation of variable relationships in a contingency table, in particular\nin case of low statistics samples and significant dependencies. Three practical\napplications are discussed. The presented algorithms are easy to use and\navailable through a public Python library.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 08:34:55 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 08:54:42 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Baak", "M.", ""], ["Koopman", "R.", ""], ["Snoek", "H.", ""], ["Klous", "S.", ""]]}, {"id": "1811.11474", "submitter": "Jakub Pr\\\"uher", "authors": "Jakub Pr\\\"uher, Toni Karvonen, Chris J. Oates, Ond\\v{r}ej Straka, Simo\n  S\\\"arkk\\\"a", "title": "Improved Calibration of Numerical Integration Error in Sigma-Point\n  Filters", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sigma-point filters, such as the UKF, which exploit numerical quadrature\nto obtain an additional order of accuracy in the moment transformation step,\nare popular alternatives to the ubiquitous EKF. The classical quadrature rules\nused in the sigma-point filters are motivated via polynomial approximation of\nthe integrand, however in the applied context these assumptions cannot always\nbe justified. As a result, quadrature error can introduce bias into estimated\nmoments, for which there is no compensatory mechanism in the classical\nsigma-point filters. This can lead in turn to estimates and predictions that\nare poorly calibrated. In this article, we investigate the Bayes-Sard\nquadrature method in the context of sigma-point filters, which enables\nuncertainty due to quadrature error to be formalised within a probabilistic\nmodel. Our first contribution is to derive the well-known classical quadratures\nas special cases of the Bayes-Sard quadrature method. Then a general-purpose\nmoment transform is developed and utilised in the design of novel sigma-point\nfilters, so that uncertainty due to quadrature error is explicitly quantified.\nNumerical experiments on a challenging tracking example with misspecified\ninitial conditions show that the additional uncertainty quantification built\ninto our method leads to better-calibrated state estimates with improved RMSE.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:07:16 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 11:37:33 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Pr\u00fcher", "Jakub", ""], ["Karvonen", "Toni", ""], ["Oates", "Chris J.", ""], ["Straka", "Ond\u0159ej", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1811.11512", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Michael Jansson, Xinwei Ma", "title": "Simple Local Polynomial Density Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an intuitive and easy-to-implement nonparametric\ndensity estimator based on local polynomial techniques. The estimator is fully\nboundary adaptive and automatic, but does not require pre-binning or any other\ntransformation of the data. We study the main asymptotic properties of the\nestimator, and use these results to provide principled estimation, inference,\nand bandwidth selection methods. As a substantive application of our results,\nwe develop a novel discontinuity in density testing procedure, an important\nproblem in regression discontinuity designs and other program evaluation\nsettings. An illustrative empirical application is given. Two companion Stata\nand R software packages are provided.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 11:55:10 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 22:13:11 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Ma", "Xinwei", ""]]}, {"id": "1811.11603", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, and Siyi Luo", "title": "Distribution Regression with Sample Selection, with an Application to\n  Wage Decompositions in the UK", "comments": "79 pages, 4 tables, 37 figures, includes supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a distribution regression model under endogenous sample selection.\nThis model is a semiparametric generalization of the Heckman selection model\nthat accommodates much richer patterns of heterogeneity in the selection\nprocess and effect of the covariates. The model applies to continuous, discrete\nand mixed outcomes. We study the identification of the model, and develop a\ncomputationally attractive two-step method to estimate the model parameters,\nwhere the first step is a probit regression for the selection equation and the\nsecond step consists of multiple distribution regressions with selection\ncorrections for the outcome equation. We construct estimators of functionals of\ninterest such as actual and counterfactual distributions of latent and observed\noutcomes via plug-in rule. We derive functional central limit theorems for all\nthe estimators and show the validity of multiplier bootstrap to carry out\nfunctional inference. We apply the methods to wage decompositions in the UK\nusing new data. Here we decompose the difference between the male and female\nwage distributions into four effects: composition, wage structure, selection\nstructure and selection sorting. After controlling for endogenous employment\nselection, we still find substantial gender wage gap -- ranging from 21% to 40%\nthroughout the (latent) offered wage distribution that is not explained by\nobservable labor market characteristics. We also uncover positive sorting for\nsingle men and negative sorting for married women that accounts for a\nsubstantive fraction of the gender wage gap at the top of the distribution.\nThese findings can be interpreted as evidence of assortative matching in the\nmarriage market and glass-ceiling in the labor market.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 14:56:32 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 09:33:13 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 17:51:31 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Luo", "Siyi", ""]]}, {"id": "1811.11709", "submitter": "Yuchen Zhou", "authors": "Pixu Shi and Yuchen Zhou and Anru R. Zhang", "title": "High-dimensional Log-Error-in-Variable Regression with Applications to\n  Microbial Compositional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In microbiome and genomic studies, the regression of compositional data has\nbeen a crucial tool for identifying microbial taxa or genes that are associated\nwith clinical phenotypes. To account for the variation in sequencing depth, the\nclassic log-contrast model is often used where read counts are normalized into\ncompositions. However, zero read counts and the randomness in covariates remain\ncritical issues. In this article, we introduce a surprisingly simple,\ninterpretable, and efficient method for the estimation of compositional data\nregression through the lens of a novel high-dimensional log-error-in-variable\nregression model. The proposed method provides both corrections on sequencing\ndata with possible overdispersion and simultaneously avoids any subjective\nimputation of zero read counts. We provide theoretical justifications with\nmatching upper and lower bounds for the estimation error. The merit of the\nprocedure is illustrated through real data analysis and simulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:57:59 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 01:29:35 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 17:59:25 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Shi", "Pixu", ""], ["Zhou", "Yuchen", ""], ["Zhang", "Anru R.", ""]]}, {"id": "1811.11733", "submitter": "Ruoqing Zhu", "authors": "Ruoqing Zhu, Jiyang Zhang, Ruilin Zhao, Peng Xu, Wenzhuo Zhou and Xin\n  Zhang", "title": "orthoDr: Semiparametric Dimension Reduction via Orthogonality\n  Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  orthoDr is a package in R that solves dimension reduction problems using\northogonality constrained optimization approach. The package serves as a\nunified framework for many regression and survival analysis dimension reduction\nmodels that utilize semiparametric estimating equations. The main computational\nmachinery of orthoDr is a first-order algorithm developed by\n\\cite{wen2013feasible} for optimization within the Stiefel manifold. We\nimplement the algorithm through Rcpp and OpenMP for fast computation. In\naddition, we developed a general-purpose solver for such constrained problems\nwith user-specified objective functions, which works as a drop-in version of\noptim(). The package also serves as a platform for future methodology\ndevelopments along this line of work.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:34:56 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 07:21:27 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Zhu", "Ruoqing", ""], ["Zhang", "Jiyang", ""], ["Zhao", "Ruilin", ""], ["Xu", "Peng", ""], ["Zhou", "Wenzhuo", ""], ["Zhang", "Xin", ""]]}, {"id": "1811.11834", "submitter": "Shouto Yonekura", "authors": "Shouto Yonekura, Alexandros Beskos and Sumeetpal S.Singh", "title": "Asymptotic Analysis of Model Selection Criteria for General Hidden\n  Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper obtains analytical results for the asymptotic properties of Model\nSelection Criteria -- widely used in practice -- for a general family of hidden\nMarkov models (HMMs), thereby substantially extending the related theory beyond\ntypical i.i.d.-like model structures and filling in an important gap in the\nrelevant literature. In particular, we look at the Bayesian and Akaike\nInformation Criteria (BIC and AIC) and the model evidence. In the setting of\nnested classes of models, we prove that BIC and the evidence are strongly\nconsistent for HMMs (under regularity conditions), whereas AIC is not weakly\nconsistent. Numerical experiments support our theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:06:50 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 00:25:05 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 11:37:44 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yonekura", "Shouto", ""], ["Beskos", "Alexandros", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1811.11922", "submitter": "Zhuoyi Yang", "authors": "Xiaozhou Wang, Zhuoyi Yang, Xi Chen, Weidong Liu", "title": "Distributed Inference for Linear Support Vector Machine", "comments": "50 pages, 11 figures", "journal-ref": "Journal of Machine Learning Research (JMLR), v20(113):1-41, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing size of modern data brings many new challenges to existing\nstatistical inference methodologies and theories, and calls for the development\nof distributed inferential approaches. This paper studies distributed inference\nfor linear support vector machine (SVM) for the binary classification task.\nDespite a vast literature on SVM, much less is known about the inferential\nproperties of SVM, especially in a distributed setting. In this paper, we\npropose a multi-round distributed linear-type (MDL) estimator for conducting\ninference for linear SVM. The proposed estimator is computationally efficient.\nIn particular, it only requires an initial SVM estimator and then successively\nrefines the estimator by solving simple weighted least squares problem.\nTheoretically, we establish the Bahadur representation of the estimator. Based\non the representation, the asymptotic normality is further derived, which shows\nthat the MDL estimator achieves the optimal statistical efficiency, i.e., the\nsame efficiency as the classical linear SVM applying to the entire data set in\na single machine setup. Moreover, our asymptotic result avoids the condition on\nthe number of machines or data batches, which is commonly assumed in\ndistributed estimation literature, and allows the case of diverging dimension.\nWe provide simulation studies to demonstrate the performance of the proposed\nMDL estimator.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:05:09 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 20:23:16 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wang", "Xiaozhou", ""], ["Yang", "Zhuoyi", ""], ["Chen", "Xi", ""], ["Liu", "Weidong", ""]]}, {"id": "1811.11930", "submitter": "Trambak Banerjee", "authors": "Trambak Banerjee, Gourab Mukherjee and Wenguang Sun", "title": "Adaptive Sparse Estimation with Side Information", "comments": "final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The article considers the problem of estimating a high-dimensional sparse\nparameter in the presence of side information that encodes the sparsity\nstructure. We develop a general framework that involves first using an\nauxiliary sequence to capture the side information, and then incorporating the\nauxiliary sequence in inference to reduce the estimation risk. The proposed\nmethod, which carries out adaptive SURE-thresholding using side information\n(ASUS), is shown to have robust performance and enjoy optimality properties. We\ndevelop new theories to characterize regimes in which ASUS far outperforms\ncompetitive shrinkage estimators, and establish precise conditions under which\nASUS is asymptotically optimal. Simulation studies are conducted to show that\nASUS substantially improves the performance of existing methods in many\nsettings. The methodology is applied for analysis of data from single cell\nvirology studies and microarray time course experiments.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:22:48 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 20:26:06 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Banerjee", "Trambak", ""], ["Mukherjee", "Gourab", ""], ["Sun", "Wenguang", ""]]}, {"id": "1811.11950", "submitter": "Gyuhyeong Goh", "authors": "Gyuhyeong Goh and Jae Kwang Kim", "title": "Accounting for model uncertainty in multiple imputation under complex\n  sampling", "comments": "23 pages, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation provides an effective way to handle missing data. When\nseveral possible models are under consideration for the data, the multiple\nimputation is typically performed under a single-best model selected from the\ncandidate models. This single model selection approach ignores the uncertainty\nassociated with the model selection and so leads to underestimation of the\nvariance of multiple imputation estimator. In this paper, we propose a new\nmultiple imputation procedure incorporating model uncertainty in the final\ninference. The proposed method incorporates possible candidate models for the\ndata into the imputation procedure using the idea of Bayesian Model Averaging\n(BMA). The proposed method is directly applicable to handling item nonresponse\nin survey sampling. Asymptotic properties of the proposed method are\ninvestigated. A limited simulation study confirms that our model averaging\napproach provides better estimation performance than the single model selection\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 03:58:00 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Goh", "Gyuhyeong", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1811.12172", "submitter": "Agnes Martine Nielsen", "authors": "Agnes Martine Nielsen and Daniela Witten", "title": "The Multiple Random Dot Product Graph Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of graphs, or networks, arise naturally in a number of\ncontexts; examples include social networks and biological networks. We are\noften faced with the availability of multiple graphs on a single set of nodes.\nIn this article, we propose the multiple random dot product graph model for\nthis setting. Our proposed model leads naturally to an optimization problem,\nwhich we solve using an efficient alternating minimization approach. We further\nuse this model as the basis for a new test for the hypothesis that the graphs\ncome from a single distribution, versus the alternative that they are drawn\nfrom different distributions. We evaluate the performance of both the fitting\nalgorithm and the hypothesis test in several simulation settings, and\ndemonstrate empirical improvement over existing approaches. We apply these new\napproaches to a Wikipedia data set and a C. elegans data set.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:20:34 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Nielsen", "Agnes Martine", ""], ["Witten", "Daniela", ""]]}, {"id": "1811.12249", "submitter": "Daniel Bonn\\'ery", "authors": "Daniel Bonn\\'ery, Yang Cheng and Partha Lahiri", "title": "An Evaluation of Design-based Properties of Different Composite\n  Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the last several decades, the US Census Bureau has been using the AK\ncomposite estimation method to produce statistics on employment from the\nCurrent Population Survey (CPS) data. The CPS uses a rotating design and AK\nestimators are linear combinations of monthly survey weighted averages (called\nmonth-in-sample estimates) in each rotation groups. Denoting by $X$ the vector\nof month-in-sample estimates and by $\\Sigma$ its design based variance, the\ncoefficients of the linear combination were optimized by the Census Bureau\nafter substituting $\\Sigma$ by an estimate and under unrealistic stationarity\nassumptions. To show the limits of this approach, we compared the AK estimator\nwith different competitors using three different synthetic populations that\nmimics the Current Population Survey (CPS) data and a simplified sample design\nthat mimics the CPS design. In our simulation setup, empirically best\nestimators have larger mean square error than simple averages. In the real data\nanalysis, the AK estimates are constantly below the survey-weighted estimates,\nindicating potential bias. Any attempt to improve on the estimated optimal\nestimator in either class would require a thorough investigation of the highly\nnon-trivial problem of estimation of $\\Sigma$ for a complex setting like the\nCPS (we did not entertain this problem in this paper). A different approach is\nto use a variant of the regression composite estimator used by Statistics\nCanada. The regression composite estimator does not require estimation of\n$\\Sigma$ and is less sensitive to the rotation group bias in our simulations.\nOur study demonstrates that there is a great potential for improving the\nestimation of levels and month to month changes in the unemployment rates by\nusing the regression composite estimator.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:31:08 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 09:30:11 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Bonn\u00e9ry", "Daniel", ""], ["Cheng", "Yang", ""], ["Lahiri", "Partha", ""]]}, {"id": "1811.12295", "submitter": "Sim\\'on Ram\\'irez-Amaya", "authors": "Adolfo Quiroz, Sim\\'on Ram\\'irez-Amaya, \\'Alvaro Riascos", "title": "Regression by clustering using Metropolis-Hastings", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality risk adjustment in health insurance markets weakens insurer\nincentives to engage in inefficient behavior to attract lower-cost enrollees.\nWe propose a novel methodology based on Markov Chain Monte Carlo methods to\nimprove risk adjustment by clustering diagnostic codes into risk groups optimal\nfor health expenditure prediction. We test the performance of our methodology\nagainst common alternatives using panel data from 500 thousand enrollees of the\nColombian Healthcare System. Results show that our methodology outperforms\ncommon alternatives and suggest that it has potential to improve access to\nquality healthcare for the chronically ill.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:36:32 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 19:06:17 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Quiroz", "Adolfo", ""], ["Ram\u00edrez-Amaya", "Sim\u00f3n", ""], ["Riascos", "\u00c1lvaro", ""]]}, {"id": "1811.12304", "submitter": "Andrea Arfe", "authors": "Andrea Arf\\'e, Stefano Peluso, Pietro Muliere", "title": "Reinforced urns and the subdistribution beta-Stacy process prior for\n  competing risks analysis", "comments": "To appear in the Scandinavian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the subdistribution beta-Stacy process, a novel\nBayesian nonparametric process prior for subdistribution functions useful for\nthe analysis of competing risks data. In particular, we i) characterize this\nprocess from a predictive perspective by means of an urn model with\nreinforcement, ii) show that it is conjugate with respect to right-censored\ndata, and iii) highlight its relations with other prior processes for competing\nrisks data. Additionally, we consider the subdistribution beta-Stacy process\nprior in a nonparametric regression model for competing risks data which,\ncontrary to most others available in the literature, is not based on the\nproportional hazards assumption.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:01:02 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Arf\u00e9", "Andrea", ""], ["Peluso", "Stefano", ""], ["Muliere", "Pietro", ""]]}, {"id": "1811.12345", "submitter": "Jia Chen", "authors": "Jia Chen and Gang Wang and Georgios B. Giannakis", "title": "Graph Multiview Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2910475", "report-no": null, "categories": "eess.SP cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview canonical correlation analysis (MCCA) seeks latent low-dimensional\nrepresentations encountered with multiview data of shared entities (a.k.a.\ncommon sources). However, existing MCCA approaches do not exploit the geometry\nof the common sources, which may be available \\emph{a priori}, or can be\nconstructed using certain domain knowledge. This prior information about the\ncommon sources can be encoded by a graph, and be invoked as a regularizer to\nenrich the maximum variance MCCA framework. In this context, the present\npaper's novel graph-regularized (G) MCCA approach minimizes the distance\nbetween the wanted canonical variables and the common low-dimensional\nrepresentations, while accounting for graph-induced knowledge of the common\nsources. Relying on a function capturing the extent low-dimensional\nrepresentations of the multiple views are similar, a generalization bound of\nGMCCA is established based on Rademacher's complexity. Tailored for setups\nwhere the number of data pairs is smaller than the data vector dimensions, a\ngraph-regularized dual MCCA approach is also developed. To further deal with\nnonlinearities present in the data, graph-regularized kernel MCCA variants are\nput forward too. Interestingly, solutions of the graph-regularized linear,\ndual, and kernel MCCA, are all provided in terms of generalized eigenvalue\ndecomposition. Several corroborating numerical tests using real datasets are\nprovided to showcase the merits of the graph-regularized MCCA variants relative\nto several competing alternatives including MCCA, Laplacian-regularized MCCA,\nand (graph-regularized) PCA.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:46:51 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 17:29:46 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chen", "Jia", ""], ["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1811.12602", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, XuanLong Nguyen and Clayton Scott", "title": "Local inversion-free estimation of spatial Gaussian processes", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.17553.17764", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing the likelihood has been widely used for estimating the unknown\ncovariance parameters of spatial Gaussian processes. However, evaluating and\noptimizing the likelihood function can be computationally intractable,\nparticularly for large number of (possibly) irregularly spaced observations,\ndue to the need to handle the inverse of ill-conditioned and large covariance\nmatrices. Extending the \"inversion-free\" method of Anitescu, Chen and Stein\n\\cite{anitescu2017inversion}, we investigate a broad class of covariance\nparameter estimators based on inversion-free surrogate losses and block\ndiagonal approximation schemes of the covariance structure. This class of\nestimators yields a spectrum for negotiating the trade-off between statistical\naccuracy and computational cost. We present fixed-domain asymptotic properties\nof our proposed method, establishing $\\sqrt{n}$-consistency and asymptotic\nnormality results for isotropic Matern Gaussian processes observed on a\nmulti-dimensional and irregular lattice. Simulation studies are also presented\nfor assessing the scalability and statistical efficiency of the proposed\nalgorithm for large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 03:44:51 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 09:00:48 GMT"}, {"version": "v3", "created": "Sun, 7 Jul 2019 19:50:59 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Nguyen", "XuanLong", ""], ["Scott", "Clayton", ""]]}, {"id": "1811.12682", "submitter": "Elena Pesce", "authors": "Elena Pesce and Eva Riccomagno", "title": "Large Datasets, Bias and Model Oriented Optimal Design of Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review recent literature that proposes to adapt ideas from classical model\nbased optimal design of experiments to problems of data selection of large\ndatasets. Special attention is given to bias reduction and to protection\nagainst confounders. Some new results are presented. Theoretical and\ncomputational comparisons are made.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 09:28:39 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Pesce", "Elena", ""], ["Riccomagno", "Eva", ""]]}, {"id": "1811.12788", "submitter": "Jerome Stenger", "authors": "Jerome Stenger, Fabrice Gamboa, Merlin Keller, Bertrand Iooss", "title": "Optimal Uncertainty Quantification on moment class using canonical\n  moments", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We gain robustness on the quantification of a risk measurement by accounting\nfor all sources of uncertainties tainting the inputs of a computer code. We\nevaluate the maximum quantile over a class of distributions defined only by\nconstraints on their moments. The methodology is based on the theory of\ncanonical moments that appears to be a well-suited framework for practical\noptimization.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:38:04 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Stenger", "Jerome", ""], ["Gamboa", "Fabrice", ""], ["Keller", "Merlin", ""], ["Iooss", "Bertrand", ""]]}]