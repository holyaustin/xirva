[{"id": "1606.00033", "submitter": "Alnur Ali", "authors": "Alnur Ali, Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam", "title": "Generalized Pseudolikelihood Methods for Inverse Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PseudoNet, a new pseudolikelihood-based estimator of the inverse\ncovariance matrix, that has a number of useful statistical and computational\nproperties. We show, through detailed experiments with synthetic and also\nreal-world finance as well as wind power data, that PseudoNet outperforms\nrelated methods in terms of estimation error and support recovery, making it\nwell-suited for use in a downstream application, where obtaining low estimation\nerror can be important. We also show, under regularity conditions, that\nPseudoNet is consistent. Our proof assumes the existence of accurate estimates\nof the diagonal entries of the underlying inverse covariance matrix; we\nadditionally provide a two-step method to obtain these estimates, even in a\nhigh-dimensional setting, going beyond the proofs for related methods. Unlike\nother pseudolikelihood-based methods, we also show that PseudoNet does not\nsaturate, i.e., in high dimensions, there is no hard limit on the number of\nnonzero entries in the PseudoNet estimate. We present a fast algorithm as well\nas screening rules that make computing the PseudoNet estimate over a range of\ntuning parameters tractable.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 20:24:23 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 14:32:03 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Ali", "Alnur", ""], ["Khare", "Kshitij", ""], ["Oh", "Sang-Yun", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1606.00092", "submitter": "Tatsushi Oka", "authors": "Tatsushi Oka and Pierre Perron", "title": "Testing for Common Breaks in a Multiple Equations System", "comments": "44 pages, 2 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue addressed in this paper is that of testing for common breaks across\nor within equations of a multivariate system. Our framework is very general and\nallows integrated regressors and trends as well as stationary regressors. The\nnull hypothesis is that breaks in different parameters occur at common\nlocations and are separated by some positive fraction of the sample size unless\nthey occur across different equations. Under the alternative hypothesis, the\nbreak dates across parameters are not the same and also need not be separated\nby a positive fraction of the sample size whether within or across equations.\nThe test considered is the quasi-likelihood ratio test assuming normal errors,\nthough as usual the limit distribution of the test remains valid with\nnon-normal errors. Of independent interest, we provide results about the rate\nof convergence of the estimates when searching over all possible partitions\nsubject only to the requirement that each regime contains at least as many\nobservations as some positive fraction of the sample size, allowing break dates\nnot separated by a positive fraction of the sample size across equations.\nSimulations show that the test has good finite sample properties. We also\nprovide an application to issues related to level shifts and persistence for\nvarious measures of inflation to illustrate its usefulness.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 01:55:19 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 01:35:15 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Oka", "Tatsushi", ""], ["Perron", "Pierre", ""]]}, {"id": "1606.00229", "submitter": "Samuel Cohen", "authors": "Samuel N. Cohen", "title": "Uncertainty and filtering of hidden Markov models in discrete time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of filtering an unseen Markov chain from noisy\nobservations, in the presence of uncertainty regarding the parameters of the\nprocesses involved. Using the theory of nonlinear expectations, we describe the\nuncertainty in terms of a penalty function, which can be propagated forward in\ntime in the place of the filter.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:19:21 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 14:50:58 GMT"}, {"version": "v3", "created": "Tue, 29 Aug 2017 14:50:26 GMT"}, {"version": "v4", "created": "Sat, 12 May 2018 11:06:33 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Cohen", "Samuel N.", ""]]}, {"id": "1606.00252", "submitter": "Lingxue Zhu", "authors": "Lingxue Zhu, Jing Lei, Bernie Devlin, Kathryn Roeder", "title": "Testing High Dimensional Covariance Matrices, with Application to\n  Detecting Schizophrenia Risk Genes", "comments": "25 pages, 5 figures, 3 tables", "journal-ref": "Ann. Appl. Stat. 11 (2017), no. 3, 1810--1831", "doi": "10.1214/17-AOAS1062", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists routinely compare gene expression levels in cases versus controls\nin part to determine genes associated with a disease. Similarly, detecting\ncase-control differences in co-expression among genes can be critical to\nunderstanding complex human diseases; however statistical methods have been\nlimited by the high dimensional nature of this problem. In this paper, we\nconstruct a sparse-Leading-Eigenvalue-Driven (sLED) test for comparing two\nhigh-dimensional covariance matrices. By focusing on the spectrum of the\ndifferential matrix, sLED provides a novel perspective that accommodates what\nwe assume to be common, namely sparse and weak signals in gene expression data,\nand it is closely related with Sparse Principal Component Analysis. We prove\nthat sLED achieves full power asymptotically under mild assumptions, and\nsimulation studies verify that it outperforms other existing procedures under\nmany biologically plausible scenarios. Applying sLED to the largest\ngene-expression dataset obtained from post-mortem brain tissue from\nSchizophrenia patients and controls, we provide a novel list of genes\nimplicated in Schizophrenia and reveal intriguing patterns in gene\nco-expression change for Schizophrenia subjects. We also illustrate that sLED\ncan be generalized to compare other gene-gene \"relationship\" matrices that are\nof practical interest, such as the weighted adjacency matrices.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:30:19 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 06:11:53 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 04:17:42 GMT"}, {"version": "v4", "created": "Wed, 7 Dec 2016 19:17:08 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Zhu", "Lingxue", ""], ["Lei", "Jing", ""], ["Devlin", "Bernie", ""], ["Roeder", "Kathryn", ""]]}, {"id": "1606.00265", "submitter": "Larry Wasserman", "authors": "Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli and\n  Larry Wasserman", "title": "Finding Singular Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for finding high density, low-dimensional structures in\nnoisy point clouds. These structures are sets with zero Lebesgue measure with\nrespect to the $D$-dimensional ambient space and belong to a $d<D$ dimensional\nspace. We call them \"singular features.\" Hunting for singular features\ncorresponds to finding unexpected or unknown structures hidden in point clouds\nbelonging to $\\R^D$. Our method outputs well defined sets of dimensions $d<D$.\nUnlike spectral clustering, the method works well in the presence of noise. We\nshow how to find singular features by first finding ridges in the estimated\ndensity, followed by a filtering step based on the eigenvalues of the Hessian\nof the density.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:50:12 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Genovese", "Christopher", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1606.00304", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett, Richard J. Samworth and Ming Yuan", "title": "Efficient multivariate entropy estimation via $k$-nearest neighbour\n  distances", "comments": "69 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical procedures, including goodness-of-fit tests and methods for\nindependent component analysis, rely critically on the estimation of the\nentropy of a distribution. In this paper, we seek entropy estimators that are\nefficient and achieve the local asymptotic minimax lower bound with respect to\nsquared error loss. To this end, we study weighted averages of the estimators\noriginally proposed by Kozachenko and Leonenko (1987), based on the $k$-nearest\nneighbour distances of a sample of $n$ independent and identically distributed\nrandom vectors in $\\mathbb{R}^d$. A careful choice of weights enables us to\nobtain an efficient estimator in arbitrary dimensions, given sufficient\nsmoothness, while the original unweighted estimator is typically only efficient\nwhen $d \\leq 3$. In addition to the new estimator proposed and theoretical\nunderstanding provided, our results facilitate the construction of\nasymptotically valid confidence intervals for the entropy of asymptotically\nminimal width.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 14:32:47 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 14:59:54 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 15:53:10 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""], ["Yuan", "Ming", ""]]}, {"id": "1606.00387", "submitter": "Mohammad Khabbazian", "authors": "Mohammad Khabbazian, Bret Hanlon, Zoe Russek, and Karl Rohe", "title": "Novel Sampling Design for Respondent-driven Sampling", "comments": "51 pages", "journal-ref": null, "doi": "10.1214/17-EJS1358", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a method of chain referral sampling\npopular for sampling hidden and/or marginalized populations. As such, even\nunder the ideal sampling assumptions, the performance of RDS is restricted by\nthe underlying social network: if the network is divided into communities that\nare weakly connected to each other, then RDS is likely to oversample one of\nthese communities. In order to diminish the \"referral bottlenecks\" between\ncommunities, we propose anti-cluster RDS (AC-RDS), an adjustment to the\nstandard RDS implementation. Using a standard model in the RDS literature,\nnamely, a Markov process on the social network that is indexed by a tree, we\nconstruct and study the Markov transition matrix for AC-RDS. We show that if\nthe underlying network is generated from the Stochastic Blockmodel with equal\nblock sizes, then the transition matrix for AC-RDS has a larger spectral gap\nand consequently faster mixing properties than the standard random walk model\nfor RDS. In addition, we show that AC-RDS reduces the covariance of the samples\nin the referral tree compared to the standard RDS and consequently leads to a\nsmaller variance and design effect. We confirm the effectiveness of the new\ndesign using both the Add-Health networks and simulated networks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:36:33 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 20:07:38 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 19:47:10 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 16:28:00 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Khabbazian", "Mohammad", ""], ["Hanlon", "Bret", ""], ["Russek", "Zoe", ""], ["Rohe", "Karl", ""]]}, {"id": "1606.00451", "submitter": "Jacob Bien", "authors": "Jacob Bien", "title": "Graph-Guided Banding of the Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization has become a primary tool for developing reliable estimators\nof the covariance matrix in high-dimensional settings. To curb the curse of\ndimensionality, numerous methods assume that the population covariance (or\ninverse covariance) matrix is sparse, while making no particular structural\nassumptions on the desired pattern of sparsity. A highly-related, yet\ncomplementary, literature studies the specific setting in which the measured\nvariables have a known ordering, in which case a banded population matrix is\noften assumed. While the banded approach is conceptually and computationally\neasier than asking for \"patternless sparsity,\" it is only applicable in very\nspecific situations (such as when data are measured over time or\none-dimensional space). This work proposes a generalization of the notion of\nbandedness that greatly expands the range of problems in which banded\nestimators apply.\n  We develop convex regularizers occupying the broad middle ground between the\nformer approach of \"patternless sparsity\" and the latter reliance on having a\nknown ordering. Our framework defines bandedness with respect to a known graph\non the measured variables. Such a graph is available in diverse situations, and\nwe provide a theoretical, computational, and applied treatment of two new\nestimators. An R package, called ggb, implements these new methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 20:01:02 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 20:27:43 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Bien", "Jacob", ""]]}, {"id": "1606.00475", "submitter": "Daniel Mirman", "authors": "Daniel Mirman, Jon-Frederick Landrigan, Spiro Kokolis, Sean Verillo,\n  and Casey Ferrara", "title": "Permutation-based cluster size correction for voxel-based lesion-symptom\n  mapping", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxel-based lesion-symptom mapping (VLSM) is a major method for studying\nbrain-behavior relationships that leverages modern neuroimaging analysis\ntechniques to build on the classic approach of examining the relationship\nbetween location of brain damage and cognitive deficits. Testing an association\nbetween deficit severity and lesion status in each voxel involves very many\nindividual tests and requires statistical correction for multiple comparisons.\nSeveral strategies have been adapted from analysis of functional neuroimaging\ndata, though VLSM faces a more difficult trade-off between avoiding false\npositives and statistical power (missing true effects). One such strategy is\nusing permutation to non-parametrically determine a null distribution of\ncluster sizes, which is then used to establish a minimum cluster size\nthreshold. This strategy is intuitively appealing because it respects the\nnecessary spatial contiguity of stroke lesions and connects with the typical\ncluster-based interpretation of VLSM results. We evaluated this strategy for\ndetecting true lesion-symptom relations using simulated deficit scores based on\npercent damage to defined brain regions (BA 45 and BA 39) in a sample of 124\nindividuals with left hemisphere stroke. Even under the most conservative\nsettings tested here, the region identified by VLSM with cluster size\ncorrection systematically extended well beyond the true region. As a result,\nthis strategy appears to be effective for ruling out situations with no true\nlesion-symptom relations, but the spatial contiguity of stroke lesions may\ncause identified lesion-symptom relations to extend beyond their true regions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 21:28:53 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Mirman", "Daniel", ""], ["Landrigan", "Jon-Frederick", ""], ["Kokolis", "Spiro", ""], ["Verillo", "Sean", ""], ["Ferrara", "Casey", ""]]}, {"id": "1606.00497", "submitter": "Avery McIntosh", "authors": "Avery McIntosh", "title": "The Jackknife Estimation Method", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical resampling methods have become feasible for parametric\nestimation, hypothesis testing, and model validation now that the computer is a\nubiquitous tool for statisticians. This essay focuses on the resampling\ntechnique for parametric estimation known as the Jackknife procedure. To\noutline the usefulness of the method and its place in the general class of\nstatistical resampling techniques, I will quickly delineate two similar\nresampling methods: the bootstrap and the permutation test. I then outline the\nJackknife method and show an example of its use.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 23:20:38 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["McIntosh", "Avery", ""]]}, {"id": "1606.00547", "submitter": "William Dunsmuir", "authors": "W. T. M. Dunsmuir and C. McKendry and R. T. Dean", "title": "Modelling discrete valued cross sectional time series with observation\n  driven models", "comments": "17 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops computationally feasible methods for estimating random\neffects models in the context of regression modelling of multiple independent\ntime series of discrete valued counts in which there is serial dependence.\nGiven covariates, random effects and process history, the observed responses at\neach time in each series are independent and have an exponential family\ndistribution. We develop maximum likelihood estimation of the mixed effects\nmodel using an observation driven generalized linear autoregressive moving\naverage specification for the serial dependence in each series. The paper\npresents an easily implementable approach which uses existing single time\nseries methods to handle the serial dependence structure in combination with\nadaptive Gaussian quadrature to approximate the integrals over the regression\nrandom effects required for the likelihood and its derivatives. The models and\nmethods presented allow extension of existing mixed model procedures for count\ndata by incorporating serial dependence which can differ in form and strength\nacross the individual series. The structure of the model has some similarities\nto longitudinal data transition models with random effects. However, in\ncontrast to that setting, where there are many cases and few to moderate\nobservations per case, the time series setting has many observations per series\nand a few to moderate number of cross sectional time series. The method is\nillustrated on time series of binary responses to musical features obtained\nfrom a panel of listeners.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 06:04:47 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 09:50:39 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Dunsmuir", "W. T. M.", ""], ["McKendry", "C.", ""], ["Dean", "R. T.", ""]]}, {"id": "1606.00583", "submitter": "Yoshiyuki Ninomiya", "authors": "Takamichi Baba and Yoshiyuki Ninomiya", "title": "$C_p$ criterion for semiparametric approach in causal inference", "comments": null, "journal-ref": "Biometrika 2017", "doi": "10.1093/biomet/asx054", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For marginal structural models, which recently play an important role in\ncausal inference, we consider a model selection problem in the framework of a\nsemiparametric approach using inverse-probability-weighted estimation or doubly\nrobust estimation. In this framework, the modeling target is a potential\noutcome which may be a missing value, and so we cannot apply the AIC nor its\nextended version to this problem. In other words, there is no analytical\ninformation criterion obtained according to its classical derivation for this\nproblem. Hence, we define a mean squared error appropriate for treating the\npotential outcome, and then we derive its asymptotic unbiased estimator as a\n$C_{p}$ criterion from an asymptotics for the semiparametric approach and using\nan ignorable treatment assignment condition. In simulation study, it is shown\nthat the proposed criterion exceeds a conventionally derived existing criterion\nin the squared error and model selection frequency. Specifically, in all\nsimulation settings, the proposed criterion provides clearly smaller squared\nerrors and higher frequencies selecting the true or nearly true model.\nMoreover, in real data analysis, we check that there is a clear difference\nbetween the selections by the two criteria.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 08:41:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Baba", "Takamichi", ""], ["Ninomiya", "Yoshiyuki", ""]]}, {"id": "1606.00614", "submitter": "Remi Servien", "authors": "Victor Picheny (MIAT INRA), R\\'emi Servien (ToxAlim), Nathalie\n  Villa-Vialaneix (MIAT INRA)", "title": "Interpretable sparse SIR for functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the issue of variable selection in functional\nregression. Unlike most work in this framework, our approach does not select\nisolated points in the definition domain of the predictors, nor does it rely on\nthe expansion of the predictors in a given functional basis. It provides an\napproach to select full intervals made of consecutive points. This feature\nimproves the interpretability of the estimated coefficients and is desirable in\nthe functional framework for which small shifts are frequent when comparing one\npredictor (curve) to another. Our method is described in a semiparametric\nframework based on Sliced Inverse Regression (SIR). SIR is an effective method\nfor dimension reduction of high-dimensional data which computes a linear\nprojection of the predictors in a low-dimensional space, without loss on\nregression information. We extend the approaches of variable selection\ndeveloped for multidimensional SIR to select intervals rather than separated\nevaluation points in the definition domain of the functional predictors.\nDifferent and equivalent formulations of SIR are combined in a shrinkage\napproach with a group-LASSO-like penalty. Finally, a fully automated iterative\nprocedure is also proposed to find the critical (interpretable) intervals. The\napproach is proved efficient on simulated and real data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 10:44:42 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 06:35:51 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 10:25:48 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2018 10:22:15 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Picheny", "Victor", "", "MIAT INRA"], ["Servien", "R\u00e9mi", "", "ToxAlim"], ["Villa-Vialaneix", "Nathalie", "", "MIAT INRA"]]}, {"id": "1606.00709", "submitter": "Sebastian Nowozin", "authors": "Sebastian Nowozin, Botond Cseke, Ryota Tomioka", "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence\n  Minimization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural samplers are probabilistic models that implement sampling\nusing feedforward neural networks: they take a random input vector and produce\na sample from a probability distribution defined by the network weights. These\nmodels are expressive and allow efficient computation of samples and\nderivatives, but cannot be used for computing likelihoods or for\nmarginalization. The generative-adversarial training method allows to train\nsuch models through the use of an auxiliary discriminative neural network. We\nshow that the generative-adversarial approach is a special case of an existing\nmore general variational divergence estimation approach. We show that any\nf-divergence can be used for training generative neural samplers. We discuss\nthe benefits of various choices of divergence functions on training complexity\nand the quality of the obtained generative models.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 14:53:33 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Nowozin", "Sebastian", ""], ["Cseke", "Botond", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1606.00787", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Eric Xing", "title": "Post-Inference Prior Swapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Bayesian methods are praised for their ability to incorporate useful\nprior knowledge, in practice, convenient priors that allow for computationally\ncheap or tractable inference are commonly used. In this paper, we investigate\nthe following question: for a given model, is it possible to compute an\ninference result with any convenient false prior, and afterwards, given any\ntarget prior of interest, quickly transform this result into the target\nposterior? A potential solution is to use importance sampling (IS). However, we\ndemonstrate that IS will fail for many choices of the target prior, depending\non its parametric form and similarity to the false prior. Instead, we propose\nprior swapping, a method that leverages the pre-inferred false posterior to\nefficiently generate accurate posterior samples under arbitrary target priors.\nPrior swapping lets us apply less-costly inference algorithms to certain\nmodels, and incorporate new or updated prior information \"post-inference\". We\ngive theoretical guarantees about our method, and demonstrate it empirically on\na number of models and priors.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 18:20:35 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 18:01:17 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Neiswanger", "Willie", ""], ["Xing", "Eric", ""]]}, {"id": "1606.00812", "submitter": "Marta Martinez-Camara", "authors": "Marta Martinez-Camara, Michael Muma, Benjamin Bejar, Abdelhak M.\n  Zoubir, Martin Vetterli", "title": "The regularized tau estimator: A robust and efficient solution to\n  ill-posed linear inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear inverse problems are ubiquitous. Often the measurements do not follow\na Gaussian distribution. Additionally, a model matrix with a large condition\nnumber can complicate the problem further by making it ill-posed. In this case,\nthe performance of popular estimators may deteriorate significantly. We have\ndeveloped a new estimator that is both nearly optimal in the presence of\nGaussian errors while being also robust against outliers. Furthermore, it\nobtains meaningful estimates when the problem is ill-posed through the\ninclusion of $\\ell_1$ and $\\ell_2$ regularizations. The computation of our\nestimate involves minimizing a non-convex objective function. Hence, we are not\nguaranteed to find the global minimum in a reasonable amount of time. Thus, we\npropose two algorithms that converge to a good local minimum in a reasonable\n(and adjustable) amount of time, as an approximation of the global minimum. We\nalso analyze how the introduction of the regularization term affects the\nstatistical properties of our estimator. We confirm high robustness against\noutliers and asymptotic efficiency for Gaussian distributions by deriving\nmeasures of robustness such as the influence function, sensitivity curve, bias,\nasymptotic variance, and mean square error. We verify the theoretical results\nusing numerical experiments and show that the proposed estimator outperforms\nrecently proposed methods, especially for increasing amounts of outlier\ncontamination. Python code for all of the algorithms are available online in\nthe spirit of reproducible research.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 13:29:37 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Martinez-Camara", "Marta", ""], ["Muma", "Michael", ""], ["Bejar", "Benjamin", ""], ["Zoubir", "Abdelhak M.", ""], ["Vetterli", "Martin", ""]]}, {"id": "1606.00921", "submitter": "Lu Wang", "authors": "Lu Wang, Daniele Durante, Rex E. Jung and David B. Dunson", "title": "Bayesian Network--Response Regression", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/btx050", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing interest in learning how human brain networks vary as a\nfunction of a continuous trait, but flexible and efficient procedures to\naccomplish this goal are limited. We develop a Bayesian semiparametric model,\nwhich combines low-rank factorizations and flexible Gaussian process priors to\nlearn changes in the conditional expectation of a network-valued random\nvariable across the values of a continuous predictor, while including\nsubject-specific random effects. The formulation leads to a general framework\nfor inference on changes in brain network structures across human traits,\nfacilitating borrowing of information and coherently characterizing\nuncertainty. We provide an efficient Gibbs sampler for posterior computation\nalong with simple procedures for inference, prediction and goodness-of-fit\nassessments. The model is applied to learn how human brain networks vary across\nindividuals with different intelligence scores. Results provide interesting\ninsights on the association between intelligence and brain connectivity, while\ndemonstrating good predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 22:14:38 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 02:40:04 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Wang", "Lu", ""], ["Durante", "Daniele", ""], ["Jung", "Rex E.", ""], ["Dunson", "David B.", ""]]}, {"id": "1606.00980", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en, Anders Eklund, David Bolin and Mattias Villani", "title": "Fast Bayesian whole-brain fMRI analysis with spatial 3D priors", "comments": null, "journal-ref": "NeuroImage (2017), vol. 146, 211-225", "doi": "10.1016/j.neuroimage.2016.11.040", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial whole-brain Bayesian modeling of task-related functional magnetic\nresonance imaging (fMRI) is a great computational challenge. Most of the\ncurrently proposed methods therefore do inference in subregions of the brain\nseparately or do approximate inference without comparison to the true posterior\ndistribution. A popular such method, which is now the standard method for\nBayesian single subject analysis in the SPM software, is introduced in Penny et\nal. (2005b). The method processes the data slice-by-slice and uses an\napproximate variational Bayes (VB) estimation algorithm that enforces posterior\nindependence between activity coefficients in different voxels. We introduce a\nfast and practical Markov chain Monte Carlo (MCMC) scheme for exact inference\nin the same model, both slice-wise and for the whole brain using a 3D prior on\nactivity coefficients. The algorithm exploits sparsity and uses modern\ntechniques for efficient sampling from high-dimensional Gaussian distributions,\nleading to speed-ups without which MCMC would not be a practical option. Using\nMCMC, we are for the first time able to evaluate the approximate VB posterior\nagainst the exact MCMC posterior, and show that VB can lead to spurious\nactivation. In addition, we develop an improved VB method that drops the\nassumption of independent voxels a posteriori. This algorithm is shown to be\nmuch faster than both MCMC and the original VB for large datasets, with\nnegligible error compared to the MCMC posterior.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 06:40:19 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 08:40:58 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Eklund", "Anders", ""], ["Bolin", "David", ""], ["Villani", "Mattias", ""]]}, {"id": "1606.00991", "submitter": "Matthew Dawson", "authors": "Matthew Dawson and Hans-Georg M\\\"uller", "title": "Dynamic Modeling with Conditional Quantile Trajectories for Longitudinal\n  Snippet Data, with Application to Cognitive Decline of Alzheimer's Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal data are often plagued with sparsity of time points where\nmeasurements are available. The functional data analysis perspective has been\nshown to provide an effective and flexible approach to address this problem for\nthe case where measurements are sparse but their times are randomly distributed\nover an interval. Here we focus on a different scenario where available data\ncan be characterized as snippets, which are very short stretches of\nlongitudinal measurements. For each subject the stretch of available data is\nmuch shorter than the time frame of interest, a common occurrence in\naccelerated longitudinal studies. An added challenge is introduced if a time\nproxy that is basic for usual longitudinal modeling is not available. This\nsituation arises in the case of Alzheimer's disease and comparable scenarios,\nwhere one is interested in time dynamics of declining performance, but the time\nof disease onset is unknown and the chronological age does not provide a\nmeaningful time reference for longitudinal modeling. Our main methodological\ncontribution is to address this problem with a novel approach. Key quantities\nfor our approach are conditional quantile trajectories for monotonic processes\nthat emerge as solutions of a dynamic system, and for which we obtain uniformly\nconsistent estimates. These trajectories are shown to be useful to describe\nprocesses that quantify deterioration over time, such as hippocampal volumes in\nAlzheimer's patients.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 07:35:01 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 20:44:58 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 20:07:08 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Dawson", "Matthew", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1606.01009", "submitter": "Nirian Mart\\'in", "authors": "Elena Castilla, Nirian Martin and Leandro Pardo", "title": "Pseudo minimum phi-divergence estimator for multinomial logistic\n  regression with complex sample design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops the theoretical framework needed to study the\nmultinomial logistic regression model for complex sample design with pseudo\nminimum phi-divergence estimators. Through a numerical example and simulation\nstudy new estimators are proposed for the parameter of the logistic regression\nmodel with overdispersed multinomial distributions for the response variables,\nthe pseudo minimum Cressie-Read divergence estimators, as well as new\nestimators for the intra-cluster correlation coefficient. The results show that\nthe Binder's method for the intra-cluster correlation coefficient exhibits an\nexcellent performance when the pseudo minimum Cressie-Read divergence\nestimator, with lambda = 2/3 , is plugged.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 09:05:56 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Castilla", "Elena", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1606.01156", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Coupling of Particle Filters", "comments": "Technical report, 24 pages for the main document + 18 pages of\n  appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters provide Monte Carlo approximations of intractable quantities\nsuch as point-wise evaluations of the likelihood in state space models. In many\nscenarios, the interest lies in the comparison of these quantities as some\nparameter or input varies. To facilitate such comparisons, we introduce and\nstudy methods to couple two particle filters in such a way that the correlation\nbetween the two underlying particle systems is increased. The motivation stems\nfrom the classic variance reduction technique of positively correlating two\nestimators. The key challenge in constructing such a coupling stems from the\ndiscontinuity of the resampling step of the particle filter. As our first\ncontribution, we consider coupled resampling algorithms. Within bootstrap\nparticle filters, they improve the precision of finite-difference estimators of\nthe score vector and boost the performance of particle marginal\nMetropolis--Hastings algorithms for parameter inference. The second\ncontribution arises from the use of these coupled resampling schemes within\nconditional particle filters, allowing for unbiased estimators of smoothing\nfunctionals. The result is a new smoothing strategy that operates by averaging\na number of independent and unbiased estimators, which allows for 1)\nstraightforward parallelization and 2) the construction of accurate error\nestimates. Neither of the above is possible with existing particle smoothers.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 15:54:52 GMT"}, {"version": "v2", "created": "Sat, 16 Jul 2016 12:55:51 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Jacob", "Pierre E.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1606.01265", "submitter": "Hassan Maatouk", "authors": "Hassan Maatouk (Gdr Mascot-Num, Irsn, Demo-Ensmse, Limos), Xavier Bay\n  (Demo-Ensmse, Limos)", "title": "Gaussian process emulators for computer experiments with inequality\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical phenomena are observed in many fields (sciences and engineering) and\nare often studied by time-consuming computer codes. These codes are analyzed\nwith statistical models, often called emulators. In many situations, the\nphysical system (computer model output) may be known to satisfy inequality\nconstraints with respect to some or all input variables. Our aim is to build a\nmodel capable of incorporating both data interpolation and inequality\nconstraints into a Gaussian process emulator. By using a functional\ndecomposition, we propose to approximate the original Gaussian process by a\nfinite-dimensional Gaussian process such that all conditional simulations\nsatisfy the inequality constraints in the whole domain. The mean, mode (maximum\na posteriori) and prediction intervals (uncertainty quantification) of the\nconditional Gaussian process are calculated. To investigate the performance of\nthe proposed model, some conditional simulations with inequality constraints\nsuch as boundary, monotonicity or convexity conditions are given. 1.\nIntroduction. In the engineering activity, runs of a computer code can be\nexpensive and time-consuming. One solution is to use a statistical surrogate\nfor conditioning computer model outputs at some input locations (design\npoints). Gaussian process (GP) emulator is one of the most popular choices\n[23]. The reason comes from the property of the GP that uncertainty\nquantification can be calculated. Furthermore, it has several nice properties.\nFor example, the conditional GP at observation data (linear equality\nconstraints) is still a GP [5]. Additionally, some inequality constraints (such\nas monotonicity and convexity) of output computer responses are related to\npartial derivatives. In such cases, the partial derivatives of the GP are also\nGPs. Incorporating an infinite number of linear inequality constraints into a\nGP emulator, the problem becomes more difficult. The reason is that the\nresulting conditional process is not a GP. In the literature of interpolation\nwith inequality constraints, we find two types of meth-ods. The first one is\ndeterministic and based on splines, which have the advantage that the\ninequality constraints are satisfied in the whole input domain (see e.g. [16],\n[24] and [25]). The second one is based on the simulation of the conditional GP\nby using the subdivision of the input set (see e.g. [1], [6] and [11]). In that\ncase, the inequality constraints are satisfied in a finite number of input\nlocations. Notice that the advantage of such method is that un-certainty\nquantification can be calculated. In previous work, some methodologies have\nbeen based on the knowledge of the derivatives of the GP at some input\nlocations ([11], [21] and [26]). For monotonicity constraints with noisy data,\na Bayesian approach was developed in [21]. In [11] the problem is to build a GP\nemulator by using the prior monotonicity\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:48:19 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Maatouk", "Hassan", "", "Gdr Mascot-Num, Irsn, Demo-Ensmse, Limos"], ["Bay", "Xavier", "", "Demo-Ensmse, Limos"]]}, {"id": "1606.01324", "submitter": "Masaaki Doi", "authors": "Masaaki Doi", "title": "Bayesian index of superiority and the p-value of the conditional test\n  for Poisson parameters", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of comparing two Poisson parameters from the Bayesian\nperspective. Kawasaki and Miyaoka (2012b) proposed the Bayesian index\n$P(\\lambda_1 < \\lambda_2 | X_1,X_2)$ and expressed it using the hypergeometric\nseries. In this paper, under some conditions, we give four other expressions of\nthe Bayesian index in terms of the cumulative distribution functions of beta,\n$F$, binomial, and negative binomial distribution. Next, we investigate the\nrelationship between the Bayesian index and the $p$-value of the conditional\ntest with the null hypothesis $H_0: \\lambda_1 \\geq \\lambda_2 $ versus an\nalternative hypothesis $H_1: \\lambda_1<\\lambda_2 $. Additionally, we\ninvestigate the generalized relationship between $P(\\lambda_1/\\lambda_2 <c |\nX_1, X_2)$ and the $p$-value of the conditional test with the null hypothesis\n$H_0: \\lambda_1/\\lambda_2 \\geq c$ versus the alternative $H_1:\n\\lambda_1/\\lambda_2 < c$. We illustrate the utility of the Bayesian index using\nanalyses of real data. Our finding suggests that the Bayesian index can\npotentially be useful in an epidemiology and in a clinical trial.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 04:08:51 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Doi", "Masaaki", ""]]}, {"id": "1606.01385", "submitter": "Elif Fidan Acar PhD", "authors": "Candida Geerdens, Elif Fidan Acar, Paul Janssen", "title": "Conditional Copula Models for Right-Censored Clustered Event Time Data", "comments": "23 pages, 5 figures, appendix and supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a modelling strategy to infer the impact of a covariate\non the dependence structure of right-censored clustered event time data. The\njoint survival function of the event times is modelled using a parametric\nconditional copula whose parameter depends on a cluster-level covariate in a\nfunctional way. We use a local likelihood approach to estimate the form of the\ncopula parameter and outline a generalized likelihood ratio-type test strategy\nto formally test its constancy. A bootstrap procedure is employed to obtain an\napproximate $p$-value for the test. The performance of the proposed estimation\nand testing methods are evaluated in simulations under different rates of\nright-censoring and for various parametric copula families, considering both\nparametrically and nonparametrically estimated margins. We apply the methods to\ndata from the Diabetic Retinopathy Study to assess the impact of disease onset\nage on the loss of visual acuity.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 15:22:02 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Geerdens", "Candida", ""], ["Acar", "Elif Fidan", ""], ["Janssen", "Paul", ""]]}, {"id": "1606.01472", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Eric B. Laber, Anastasios Tsiatis, Marie Davidian", "title": "Interpretable Dynamic Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision medicine is currently a topic of great interest in clinical and\nintervention science. One way to formalize precision medicine is through a\ntreatment regime, which is a sequence of decision rules, one per stage of\nclinical intervention, that map up-to-date patient information to a recommended\ntreatment. An optimal treatment regime is defined as maximizing the mean of\nsome cumulative clinical outcome if applied to a population of interest. It is\nwell-known that even under simple generative models an optimal treatment regime\ncan be a highly nonlinear function of patient information. Consequently, a\nfocal point of recent methodological research has been the development of\nflexible models for estimating optimal treatment regimes. However, in many\nsettings, estimation of an optimal treatment regime is an exploratory analysis\nintended to generate new hypotheses for subsequent research and not to directly\ndictate treatment to new patients. In such settings, an estimated treatment\nregime that is interpretable in a domain context may be of greater value than\nan unintelligible treatment regime built using \"black-box\" estimation methods.\nWe propose an estimator of an optimal treatment regime composed of a sequence\nof decision rules, each expressible as a list of \"if-then\" statements that can\nbe presented as either a paragraph or as a simple flowchart that is immediately\ninterpretable to domain experts. The discreteness of these lists precludes\nsmooth, i.e., gradient-based, methods of estimation and leads to non-standard\nasymptotics. Nevertheless, we provide a computationally efficient estimation\nalgorithm, prove consistency of the proposed estimator, and derive rates of\nconvergence. We illustrate the proposed methods using a series of simulation\nexamples and application to data from a sequential clinical trial on bipolar\ndisorder.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 07:29:52 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Zhang", "Yichi", ""], ["Laber", "Eric B.", ""], ["Tsiatis", "Anastasios", ""], ["Davidian", "Marie", ""]]}, {"id": "1606.01604", "submitter": "Alan Huang", "authors": "Alan Huang, Paul J. Rathouz", "title": "Orthogonality of the mean and error distribution in generalized linear\n  models", "comments": "7 pages, 1 table, Communications in Statistics - Theory and Methods\n  (acc. 20 Sep 2013)", "journal-ref": null, "doi": "10.1080/03610926.2013.851241", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the mean-model parameter is always orthogonal to the error\ndistribution in generalized linear models. Thus, the maximum likelihood\nestimator of the mean-model parameter will be asymptotically efficient\nregardless of whether the error distribution is known completely, known up to a\nfinite vector of parameters, or left completely unspecified, in which case the\nlikelihood is taken to be an appropriate semiparametric likelihood. Moreover,\nthe maximum likelihood estimator of the mean-model parameter will be\nasymptotically independent of the maximum likelihood estimator of the error\ndistribution. This generalizes some well-known results for the special cases of\nnormal, gamma and multinomial regression models, and, perhaps more\ninterestingly, suggests that asymptotically efficient estimation and inferences\ncan always be obtained if the error distribution is nonparametrically estimated\nalong with the mean. In contrast, estimation and inferences using misspecified\nerror distributions or variance functions are generally not efficient.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 03:15:52 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 02:58:47 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Huang", "Alan", ""], ["Rathouz", "Paul J.", ""]]}, {"id": "1606.01701", "submitter": "Guanhao Feng", "authors": "Guanhao Feng and Nicholas G. Polson", "title": "Regularizing Bayesian Predictive Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that regularizing Bayesian predictive regressions provides a\nframework for prior sensitivity analysis. We develop a procedure that jointly\nregularizes expectations and variance-covariance matrices using a pair of\nshrinkage priors. Our methodology applies directly to vector autoregressions\n(VAR) and seemingly unrelated regressions (SUR). The regularization path\nprovides a prior sensitivity diagnostic. By exploiting a duality between\nregularization penalties and predictive prior distributions, we reinterpret two\nclassic Bayesian analyses of macro-finance studies: equity premium\npredictability and forecasting macroeconomic growth rates. We find there exist\nplausible prior specifications for predictability in excess S&P 500 index\nreturns using book-to-market ratios, CAY (consumption, wealth, income ratio),\nand T-bill rates. We evaluate the forecasts using a market-timing strategy, and\nwe show the optimally regularized solution outperforms a buy-and-hold approach.\nA second empirical application involves forecasting industrial production,\ninflation, and consumption growth rates, and demonstrates the feasibility of\nour approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 11:44:09 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 15:32:12 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 20:49:46 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 19:31:11 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Feng", "Guanhao", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1606.01746", "submitter": "Amelia Sim\\'o", "authors": "Sonia Barahona, Ximo Gual-Arnau, Maria Victoria Ib\\'a\\~nez and Amelia\n  Sim\\'o", "title": "Unsupervised classification of children's bodies using currents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object classification according to their shape and size is of key importance\nin many scientific fields. This work focuses on the case where the size and\nshape of an object is characterized by a current}. A current is a mathematical\nobject which has been proved relevant to the modeling of geometrical data, like\nsubmanifolds, through integration of vector fields along them. As a consequence\nof the choice of a vector-valued Reproducing Kernel Hilbert Space (RKHS) as a\ntest space for integrating manifolds, it is possible to consider that shapes\nare embedded in this Hilbert Space. A vector-valued RKHS is a Hilbert space of\nvector fields; therefore, it is possible to compute a mean of shapes, or to\ncalculate a distance between two manifolds. This embedding enables us to\nconsider size-and-shape classification algorithms.\n  These algorithms are applied to a 3D database obtained from an anthropometric\nsurvey of the Spanish child population with a potential application to online\nsales of children's wear.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 13:52:24 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Barahona", "Sonia", ""], ["Gual-Arnau", "Ximo", ""], ["Ib\u00e1\u00f1ez", "Maria Victoria", ""], ["Sim\u00f3", "Amelia", ""]]}, {"id": "1606.01749", "submitter": "Tsung Fei Khang", "authors": "T. F. Khang", "title": "A gamma approximation to the Bayesian posterior distribution of a\n  discrete parameter of the Generalized Poisson model", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ have a Generalized Poisson distribution with mean $kb$, where $b$ is\na known constant in the unit interval and $k$ is a discrete, non-negative\nparameter. We show that if an uninformative uniform prior for $k$ is assumed,\nthen the posterior distribution of $k$ can be approximated using the gamma\ndistribution when $b$ is small.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 14:01:44 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Khang", "T. F.", ""]]}, {"id": "1606.01969", "submitter": "Lihua Lei", "authors": "Lihua Lei and William Fithian", "title": "Power of Ordered Hypothesis Testing", "comments": "18 pages. To appear at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered testing procedures are multiple testing procedures that exploit a\npre-specified ordering of the null hypotheses, from most to least promising. We\nanalyze and compare the power of several recent proposals using the asymptotic\nframework of Li & Barber (2015). While accumulation tests including ForwardStop\ncan be quite powerful when the ordering is very informative, they are\nasymptotically powerless when the ordering is weaker. By contrast, Selective\nSeqStep, proposed by Barber & Cand\\`es (2015), is much less sensitive to the\nquality of the ordering. We compare the power of these procedures in different\nr\\'egimes, concluding that Selective SeqStep dominates accumulation tests if\neither the ordering is weak or non-null hypotheses are sparse or weak.\nMotivated by our asymptotic analysis, we derive an improved version of\nSelective SeqStep which we call Adaptive SeqStep, analogous to Storey's\nimprovement on the Benjamini-Hochberg procedure. We compare these methods using\nthe GEOQuery data set analyzed by Li & Barber (2015) and find Adaptive SeqStep\nhas favorable performance for both good and bad prior orderings.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 22:59:13 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Lei", "Lihua", ""], ["Fithian", "William", ""]]}, {"id": "1606.01984", "submitter": "Linglong Kong", "authors": "Rui Zhu, Di Niu, Linglong Kong, Zongpeng Li", "title": "Expectile Matrix Factorization for Skewed Data Analysis", "comments": "8 page main text with 5 page supplementary documents, published in\n  AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is a popular approach to solving matrix estimation\nproblems based on partial observations. Existing matrix factorization is based\non least squares and aims to yield a low-rank matrix to interpret the\nconditional sample means given the observations. However, in many real\napplications with skewed and extreme data, least squares cannot explain their\ncentral tendency or tail distributions, yielding undesired estimates. In this\npaper, we propose \\emph{expectile matrix factorization} by introducing\nasymmetric least squares, a key concept in expectile regression analysis, into\nthe matrix factorization framework. We propose an efficient algorithm to solve\nthe new problem based on alternating minimization and quadratic programming. We\nprove that our algorithm converges to a global optimum and exactly recovers the\ntrue underlying low-rank matrices when noise is zero. For synthetic data with\nskewed noise and a real-world dataset containing web service response times,\nthe proposed scheme achieves lower recovery errors than the existing matrix\nfactorization method based on least squares in a wide range of settings.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 00:53:13 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 18:50:48 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 06:04:43 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Zhu", "Rui", ""], ["Niu", "Di", ""], ["Kong", "Linglong", ""], ["Li", "Zongpeng", ""]]}, {"id": "1606.02011", "submitter": "Long Feng", "authors": "Long Feng, Lee H. Dicker", "title": "Approximate nonparametric maximum likelihood inference for mixture\n  models via convex optimization", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric maximum likelihood (NPML) for mixture models is a technique for\nestimating mixing distributions that has a long and rich history in statistics\ngoing back to the 1950s, and is closely related to empirical Bayes methods.\nHistorically, NPML-based methods have been considered to be relatively\nimpractical because of computational and theoretical obstacles. However, recent\nwork focusing on approximate NPML methods suggests that these methods may have\ngreat promise for a variety of modern applications. Building on this recent\nwork, a class of flexible, scalable, and easy to implement approximate NPML\nmethods is studied for problems with multivariate mixing distributions.\nConcrete guidance on implementing these methods is provided, with theoretical\nand empirical support; topics covered include identifying the support set of\nthe mixing distribution, and comparing algorithms (across a variety of metrics)\nfor solving the simple convex optimization problem at the core of the\napproximate NPML problem. Additionally, three diverse real data applications\nare studied to illustrate the methods' performance: (i) A baseball data\nanalysis (a classical example for empirical Bayes methods), (ii)\nhigh-dimensional microarray classification, and (iii) online prediction of\nblood-glucose density for diabetes patients. Among other things, the empirical\nresults demonstrate the relative effectiveness of using multivariate (as\nopposed to univariate) mixing distributions for NPML-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 03:29:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 03:24:44 GMT"}, {"version": "v3", "created": "Fri, 12 Jan 2018 16:39:24 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Feng", "Long", ""], ["Dicker", "Lee H.", ""]]}, {"id": "1606.02090", "submitter": "Tom Reynkens", "authors": "Jan Beirlant, Isabel Fraga Alves and Tom Reynkens", "title": "Fitting tails affected by truncation", "comments": null, "journal-ref": "Electron. J. Stat. 11 (2017) 2026-2065", "doi": "10.1214/17-EJS1286", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several applications, ultimately at the largest data, truncation effects\ncan be observed when analysing tail characteristics of statistical\ndistributions. In some cases truncation effects are forecasted through physical\nmodels such as the Gutenberg-Richter relation in geophysics, while at other\ninstances the nature of the measurement process itself may cause under recovery\nof large values, for instance due to flooding in river discharge readings.\nRecently Beirlant et al. (2016) discussed tail fitting for truncated\nPareto-type distributions. Using examples from earthquake analysis, hydrology\nand diamond valuation we demonstrate the need for a unified treatment of\nextreme value analysis for truncated heavy and light tails. We generalise the\nclassical Peaks over Threshold approach for the different max-domains of\nattraction with shape parameter $\\xi>-1/2$ to allow for truncation effects. We\nuse a pseudo-maximum likelihood approach to estimate the model parameters and\nconsider extreme quantile estimation and reconstruction of quantile levels\nbefore truncation whenever appropriate. We report on some simulation\nexperiments and provide some basic asymptotic results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 10:43:03 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 09:07:21 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 13:51:22 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Beirlant", "Jan", ""], ["Alves", "Isabel Fraga", ""], ["Reynkens", "Tom", ""]]}, {"id": "1606.02109", "submitter": "Onur Dikmen", "authors": "Antti Honkela, Mrinal Das, Arttu Nieminen, Onur Dikmen and Samuel\n  Kaski", "title": "Efficient differentially private learning improves drug sensitivity\n  prediction", "comments": "14 pages + 13 pages supplementary information, 3 + 3 figures", "journal-ref": "Biology Direct (2018) 13:1", "doi": "10.1186/s13062-017-0203-4", "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of a personalised recommendation system face a dilemma: recommendations\ncan be improved by learning from data, but only if the other users are willing\nto share their private information. Good personalised predictions are vitally\nimportant in precision medicine, but genomic information on which the\npredictions are based is also particularly sensitive, as it directly identifies\nthe patients and hence cannot easily be anonymised. Differential privacy has\nemerged as a potentially promising solution: privacy is considered sufficient\nif presence of individual patients cannot be distinguished. However,\ndifferentially private learning with current methods does not improve\npredictions with feasible data sizes and dimensionalities. Here we show that\nuseful predictors can be learned under powerful differential privacy\nguarantees, and even from moderately-sized data sets, by demonstrating\nsignificant improvements with a new robust private regression method in the\naccuracy of private drug sensitivity prediction. The method combines two key\nproperties not present even in recent proposals, which can be generalised to\nother predictors: we prove it is asymptotically consistently and efficiently\nprivate, and demonstrate that it performs well on finite data. Good finite data\nperformance is achieved by limiting the sharing of private information by\ndecreasing the dimensionality and by projecting outliers to fit tighter bounds,\ntherefore needing to add less noise for equal privacy. As already the\nsimple-to-implement method shows promise on the challenging genomic data, we\nanticipate rapid progress towards practical applications in many fields, such\nas mobile sensing and social media, in addition to the badly needed precision\nmedicine solutions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 11:52:28 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 11:38:00 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Honkela", "Antti", ""], ["Das", "Mrinal", ""], ["Nieminen", "Arttu", ""], ["Dikmen", "Onur", ""], ["Kaski", "Samuel", ""]]}, {"id": "1606.02234", "submitter": "Feipeng Zhang", "authors": "Feipeng Zhang and Qunhua Li", "title": "Robust bent line regression", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a rank-based bent linear regression with an unknown change\npoint. Using a linear reparameterization technique, we propose a rank-based\nestimate that can make simultaneous inference on all model parameters,\nincluding the location of the change point, in a computationally efficient\nmanner. We also develop a score-like test for the existence of a change point,\nbased on a weighted CUSUM process. This test only requires fitting the model\nunder the null hypothesis in absence of a change point, thus it is\ncomputationally more efficient than likelihood-ratio type tests. The asymptotic\nproperties of the test are derived under both the null and the local\nalternative models. Simulation studies and two real data examples show that the\nproposed methods are robust against outliers and heavy-tailed errors in both\nparameter estimation and hypothesis testing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 17:54:20 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Zhang", "Feipeng", ""], ["Li", "Qunhua", ""]]}, {"id": "1606.02235", "submitter": "James Johndrow", "authors": "James E. Johndrow, Kristian Lum, Daniel Manrique-Vallier", "title": "Estimating the observable population size from biased samples: a new\n  approach to population estimation with capture heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capture-recapture methods aim to estimate the size of a closed population on\nthe basis of multiple incomplete enumerations of individuals. In many\napplications, the individual probability of being recorded is heterogeneous in\nthe population. Previous studies have suggested that it is not possible to\nreliably estimate the total population size when capture heterogeneity exists.\nHere we approach population estimation in the presence of capture heterogeneity\nas a latent length biased nonparametric density estimation problem on the unit\ninterval. We show that in this setting it is generally impossible to estimate\nthe density on the entire unit interval in finite samples, and that estimators\nof the population size have high and sometimes unbounded risk when the density\nhas significant mass near zero. As an alternative, we propose estimating the\npopulation of individuals with capture probability exceeding some threshold. We\nprovide methods for selecting an appropriate threshold, and show that this\napproach results in estimators with substantially lower risk than estimators of\nthe total population size, with correspondingly smaller uncertainty, even when\nthe parameter of interest is the total population. The alternative paradigm is\ndemonstrated in extensive simulation studies and an application to snowshoe\nhare multiple recapture data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 17:58:28 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Johndrow", "James E.", ""], ["Lum", "Kristian", ""], ["Manrique-Vallier", "Daniel", ""]]}, {"id": "1606.02274", "submitter": "Alexander D\\\"urre", "authors": "Alexander D\\\"urre, Roland Fried, Daniel Vogel", "title": "The spatial sign covariance matrix and its application for robust\n  correlation estimation", "comments": "8 pages, 2 figures, to be published in the conference proceedings of\n  11th international conference \"Computer Data Analysis & Modeling 2016\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We summarize properties of the spatial sign covariance matrix and especially\nlook at the relationship between its eigenvalues and those of the shape matrix\nof an elliptical distribution. The explicit relationship known in the bivariate\ncase was used to construct the spatial sign correlation coefficient, which is a\nnon-parametric and robust estimator for the correlation coefficient within the\nelliptical model. We consider a multivariate generalization, which we call the\nmultivariate spatial sign correlation matrix.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:37:30 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["D\u00fcrre", "Alexander", ""], ["Fried", "Roland", ""], ["Vogel", "Daniel", ""]]}, {"id": "1606.02352", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "A statistical inference course based on p-values", "comments": "16 pages, 2 figures", "journal-ref": "The American Statistician, 2017, volume 71, number 2, pages\n  128--136", "doi": "10.1080/00031305.2016.1208629", "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introductory statistical inference texts and courses treat the point\nestimation, hypothesis testing, and interval estimation problems separately,\nwith primary emphasis on large-sample approximations. Here I present an\nalternative approach to teaching this course, built around p-values,\nemphasizing provably valid inference for all sample sizes. Details about\ncomputation and marginalization are also provided, with several illustrative\nexamples, along with a course outline.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 23:36:00 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1606.02359", "submitter": "Mathias Drton", "authors": "Mathias Drton and Marloes H. Maathuis", "title": "Structure Learning in Graphical Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graphical model is a statistical model that is associated to a graph whose\nnodes correspond to variables of interest. The edges of the graph reflect\nallowed conditional dependencies among the variables. Graphical models admit\ncomputationally convenient factorization properties and have long been a\nvaluable tool for tractable modeling of multivariate distributions. More\nrecently, applications such as reconstructing gene regulatory networks from\ngene expression data have driven major advances in structure learning, that is,\nestimating the graph underlying a model. We review some of these advances and\ndiscuss methods such as the graphical lasso and neighborhood selection for\nundirected graphical models (or Markov random fields), and the PC algorithm and\nscore-based search methods for directed graphical models (or Bayesian\nnetworks). We further review extensions that account for effects of latent\nvariables and heterogeneous data sources.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 23:58:09 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Drton", "Mathias", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1606.02386", "submitter": "Ali Shojaie", "authors": "Kasra Alishahi, Ahmad Reza Ehyaei, and Ali Shojaie", "title": "A Generalized Benjamini-Hochberg Procedure for Multivariate Hypothesis\n  Testing", "comments": "33 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of the false discovery rate (FDR) by Benjamini and Hochberg\nhas spurred a great interest in developing methodologies to control the FDR in\nvarious settings. The majority of existing approaches, however, address the FDR\ncontrol for the case where an appropriate univariate test statistic is\navailable. Modern hypothesis testing and data integration applications, on the\nother hand, routinely involve multivariate test statistics. The goal, in such\nsettings, is to combine the evidence for each hypothesis and achieve greater\npower, while controlling the number of false discoveries. This paper considers\ndata-adaptive methods for constructing nested rejection regions based on\nmultivariate test statistics (z-values). It is proved that the FDR can be\ncontrolled for appropriately constructed rejection regions, even when the\nregions depend on data and are hence random. This flexibility is then exploited\nto develop optimal multiple comparison procedures in higher dimensions, where\nthe distribution of non-null z-values is unknown. Results are illustrated using\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 03:31:07 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Alishahi", "Kasra", ""], ["Ehyaei", "Ahmad Reza", ""], ["Shojaie", "Ali", ""]]}, {"id": "1606.02401", "submitter": "Soumendu Sundar Mukherjee", "authors": "Soumendu Sundar Mukherjee, Purnamrita Sarkar, Lizhen Lin", "title": "On clustering network-valued data", "comments": "Updated title, added new materials; 21 pages, 3 figures, 3 tables;\n  conference version to appear in NIPS-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection, which focuses on clustering nodes or detecting\ncommunities in (mostly) a single network, is a problem of considerable\npractical interest and has received a great deal of attention in the research\ncommunity. While being able to cluster within a network is important, there are\nemerging needs to be able to cluster multiple networks. This is largely\nmotivated by the routine collection of network data that are generated from\npotentially different populations. These networks may or may not have node\ncorrespondence. When node correspondence is present, we cluster networks by\nsummarizing a network by its graphon estimate, whereas when node correspondence\nis not present, we propose a novel solution for clustering such networks by\nassociating a computationally feasible feature vector to each network based on\ntrace of powers of the adjacency matrix. We illustrate our methods using both\nsimulated and real data sets, and theoretical justifications are provided in\nterms of consistency.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 05:25:20 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 09:24:48 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 11:35:19 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Mukherjee", "Soumendu Sundar", ""], ["Sarkar", "Purnamrita", ""], ["Lin", "Lizhen", ""]]}, {"id": "1606.02615", "submitter": "David Darmon", "authors": "David Darmon", "title": "Specific Differential Entropy Rate Estimation for Continuous-Valued Time\n  Series", "comments": null, "journal-ref": "Entropy 18.5 (2016): 190", "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for quantifying the inherent unpredictability of a\ncontinuous-valued time series via an extension of the differential Shannon\nentropy rate. Our extension, the specific entropy rate, quantifies the amount\nof predictive uncertainty associated with a specific state, rather than\naveraged over all states. We relate the specific entropy rate to popular\n`complexity' measures such as Approximate and Sample Entropies. We provide a\ndata-driven approach for estimating the specific entropy rate of an observed\ntime series. Finally, we consider three case studies of estimating specific\nentropy rate from synthetic and physiological data relevant to the analysis of\nheart rate variability.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 15:57:35 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Darmon", "David", ""]]}, {"id": "1606.02682", "submitter": "Avi Feller", "authors": "Avi Feller and Fabrizia Mealli and Luke Miratrix", "title": "Principal Score Methods: Assumptions and Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers addressing post-treatment complications in randomized trials\noften turn to principal stratification to define relevant assumptions and\nquantities of interest. One approach for estimating causal effects in this\nframework is to use methods based on the \"principal score,\" typically assuming\nthat stratum membership is as-good-as-randomly assigned given a set of\ncovariates. In this paper, we clarify the key assumption in this context, known\nas Principal Ignorability, and argue that versions of this assumption are quite\nstrong in practice. We describe different estimation approaches and demonstrate\nthat weighting-based methods are generally preferable to subgroup-based\napproaches that discretize the principal score. We then extend these ideas to\nthe case of two-sided noncompliance and propose a natural framework for\ncombining Principal Ignorability with exclusion restrictions and other\nassumptions. Finally, we apply these ideas to the Head Start Impact Study, a\nlarge-scale randomized evaluation of the Head Start program. Overall, we argue\nthat, while principal score methods are useful tools, applied researchers\nshould fully understand the relevant assumptions when using them in practice.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 18:42:23 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Feller", "Avi", ""], ["Mealli", "Fabrizia", ""], ["Miratrix", "Luke", ""]]}, {"id": "1606.02690", "submitter": "Sandra E. Safo", "authors": "Sandra E. Safo, Shuzhao Li, Qi Long", "title": "Integrative analysis of transcriptomic and metabolomic data via sparse\n  canonical correlation analysis with incorporation of biological information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrative analyses of different high dimensional data types are becoming\nincreasingly popular. Similarly, incorporating prior functional relationships\namong variables in data analysis has been a topic of increasing interest as it\nhelps elucidate underlying mechanisms among complex diseases. In this paper,\nthe goal is to assess association between transcriptomic and metabolomic data\nfrom a Predictive Health Institute (PHI) study including healthy adults at high\nrisk of developing cardiovascular diseases. To this end, we develop statistical\nmethods for identifying sparse structure in canonical correlation analysis\n(CCA) with incorporation of biological/structural information. Our proposed\nmethods use prior network structural information among genes and among\nmetabolites to guide selection of relevant genes and metabolites in sparse CCA,\nproviding insight on the molecular underpinning of cardiovascular disease. Our\nsimulations demonstrate that the structured sparse CCA methods outperform\nseveral existing sparse CCA methods in selecting relevant genes and metabolites\nwhen structural information is informative and are robust to mis-specified\nstructural information. Our analysis of the PHI study reveals that a number of\ngenes and metabolic pathways including some known to be associated with\ncardiovascular diseases are enriched in the subset of genes and metabolites\nselected by our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 19:03:39 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Safo", "Sandra E.", ""], ["Li", "Shuzhao", ""], ["Long", "Qi", ""]]}, {"id": "1606.02931", "submitter": "Anna Simoni", "authors": "Siddhartha Chib, Minchul Shin, Anna Simoni", "title": "Bayesian Estimation and Comparison of Moment Condition Models", "comments": "46 pages, 2 figures; revised results in Section 3, revised examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of inference in statistical models\ncharacterized by moment restrictions by casting the problem within the\nExponentially Tilted Empirical Likelihood (ETEL) framework. Because the ETEL\nfunction has a well defined probabilistic interpretation and plays the role of\na nonparametric likelihood, a fully Bayesian semiparametric framework can be\ndeveloped. We establish a number of powerful results surrounding the Bayesian\nETEL framework in such models. One major concern driving our work is the\npossibility of misspecification. To accommodate this possibility, we show how\nthe moment conditions can be reexpressed in terms of additional nuisance\nparameters and that, even under misspecification, the Bayesian ETEL posterior\ndistribution satisfies a Bernstein-von Mises result. A second key contribution\nof the paper is the development of a framework based on marginal likelihoods\nand Bayes factors to compare models defined by different moment conditions.\nComputation of the marginal likelihoods is by the method of Chib (1995) as\nextended to Metropolis-Hastings samplers in Chib and Jeliazkov (2001). We\nestablish the model selection consistency of the marginal likelihood and show\nthat the marginal likelihood favors the model with the minimum number of\nparameters and the maximum number of valid moment restrictions. When the models\nare misspecified, the marginal likelihood model selection procedure selects the\nmodel that is closer to the (unknown) true data generating process in terms of\nthe Kullback-Leibler divergence. The ideas and results in this paper provide a\nfurther broadening of the theoretical underpinning and value of the Bayesian\nETEL framework with likely far-reaching practical consequences. The discussion\nis illuminated through several examples.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 12:29:35 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 13:48:07 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Chib", "Siddhartha", ""], ["Shin", "Minchul", ""], ["Simoni", "Anna", ""]]}, {"id": "1606.03033", "submitter": "Wei Fu", "authors": "Wei Fu, Jeffrey S. Simonoff", "title": "Survival trees for left-truncated and right-censored data, with\n  application to time-varying covariate data", "comments": "53 pages, 17 figures", "journal-ref": null, "doi": "10.1093/biostatistics/kxw047", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree methods (recursive partitioning) are a popular class of nonparametric\nmethods for analyzing data. One extension of the basic tree methodology is the\nsurvival tree, which applies recursive partitioning to censored survival data.\nThere are several existing survival tree methods in the literature, which are\nmainly designed for right-censored data. We propose two new survival trees for\nleft-truncated and right-censored (LTRC) data, which can be seen as a\ngeneralization of the traditional survival tree for right-censored data.\nFurther, we show that such trees can be used to analyze survival data with\ntime-varying covariates, essentially building a time-varying covariates\nsurvival tree. Implementation of the methods is easy, and simulations and real\ndata analysis results show that the proposed methods work well for LTRC data\nand survival data with time-varying covariates, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 17:40:12 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Fu", "Wei", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "1606.03214", "submitter": "Alan Huang", "authors": "Alan Huang", "title": "Mean-parametrized Conway-Maxwell-Poisson regression models for dispersed\n  counts", "comments": "To appear in Statistical Modelling: An International Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conway-Maxwell-Poisson (CMP) distributions are flexible generalizations of\nthe Poisson distribution for modelling overdispersed or underdispersed counts.\nThe main hindrance to their wider use in practice seems to be the inability to\ndirectly model the mean of counts, making them not compatible with nor\ncomparable to competing count regression models, such as the log-linear\nPoisson, negative-binomial or generalized Poisson regression models. This note\nillustrates how CMP distributions can be parametrized via the mean, so that\nsimpler and more easily-interpretable mean-models can be used, such as a\nlog-linear model. Other link functions are also available, of course. In\naddition to establishing attractive theoretical and asymptotic properties of\nthe proposed model, its good finite-sample performance is exhibited through\nvarious examples and a simulation study based on real datasets. Moreover, the\nMATLAB routine to fit the model to data is demonstrated to be up to an order of\nmagnitude faster than the current software to fit standard CMP models, and over\ntwo orders of magnitude faster than the recently proposed hyper-Poisson model.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 07:41:07 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 04:25:28 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Huang", "Alan", ""]]}, {"id": "1606.03376", "submitter": "Gabriela Cybis", "authors": "Gabriela Bettella Cybis, Marcio Valk, Silvia Regina Costa Lopes", "title": "Clustering and Classification of Genetic Data Through U-Statistics", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic data are frequently categorical and have complex dependence\nstructures that are not always well understood. For this reason, clustering and\nclassification based on genetic data, while highly relevant, are challenging\nstatistical problems. Here we consider a highly versatile U-statistics based\napproach built on dissimilarities between pairs of data points for\nnonparametric clustering. In this work we propose statistical tests to assess\ngroup homogeneity taking into account the multiple testing issues, and a\nclustering algorithm based on dissimilarities within and between groups that\nhighly speeds up the homogeneity test. We also propose a test to verify\nclassification significance of a sample in one of two groups. A Monte Carlo\nsimulation study is presented to evaluate power of the classification test,\nconsidering different group sizes and degree of separation. Size and power of\nthe homogeneity test are also analyzed through simulations that compare it to\ncompeting methods. Finally, the methodology is applied to three different\ngenetic datasets: global human genetic diversity, breast tumor gene expression\nand Dengue virus serotypes. These applications showcase this statistical\nframework's ability to answer diverse biological questions while adapting to\nthe specificities of the different datatypes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 15:57:20 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Cybis", "Gabriela Bettella", ""], ["Valk", "Marcio", ""], ["Lopes", "Silvia Regina Costa", ""]]}, {"id": "1606.03552", "submitter": "Sangwon Hyun", "authors": "Sangwon Hyun, Max G'Sell, Ryan J. Tibshirani", "title": "Exact Post-Selection Inference for Changepoint Detection and Other\n  Generalized Lasso Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study tools for inference conditioned on model selection events that are\ndefined by the generalized lasso regularization path. The generalized lasso\nestimate is given by the solution of a penalized least squares regression\nproblem, where the penalty is the l1 norm of a matrix D times the coefficient\nvector. The generalized lasso path collects these estimates for a range of\npenalty parameter ({\\lambda}) values. Leveraging a sequential characterization\nof this path from Tibshirani & Taylor (2011), and recent advances in\npost-selection inference from Lee et al. (2016), Tibshirani et al. (2016), we\ndevelop exact hypothesis tests and confidence intervals for linear contrasts of\nthe underlying mean vector, conditioned on any model selection event along the\ngeneralized lasso path (assuming Gaussian errors in the observations). By\ninspecting specific choices of D, we obtain post-selection tests and confidence\nintervals for specific cases of generalized lasso estimates, such as the fused\nlasso, trend filtering, and the graph fused lasso. In the fused lasso case, the\nunderlying coordinates of the mean are assigned a linear ordering, and our\nframework allows us to test selectively chosen breakpoints or changepoints in\nthese mean coordinates. This is an interesting and well-studied problem with\nbroad applications, our framework applied to the trend filtering and graph\nfused lasso serves several applications as well. Aside from the development of\nselective inference tools, we describe several practical aspects of our methods\nsuch as valid post-processing of generalized estimates before performing\ninference in order to improve power, and problem-specific visualization aids\nthat may be given to the data analyst for he/she to choose linear contrasts to\nbe tested. Many examples, both from simulated and real data sources, are\npresented to examine the empirical properties of our inference methods.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 04:21:01 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Hyun", "Sangwon", ""], ["G'Sell", "Max", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1606.03658", "submitter": "Yanguang Chen", "authors": "Yanguang Chen", "title": "New Approaches for Calculating Moran's Index of Spatial Autocorrelation", "comments": "27 pages, 5 figures, 3 tables, PLoS ONE, 2013", "journal-ref": "PLoS ONE, 2013, 8(7): e68336", "doi": "10.1371/journal.pone.0068336", "report-no": null, "categories": "physics.soc-ph stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatial autocorrelation plays an important role in geographical analysis,\nhowever, there is still room for improvement of this method. The formula for\nMoran's index is complicated, and several basic problems remain to be solved.\nTherefore, I will reconstruct its mathematical framework using mathematical\nderivation based on linear algebra and present four simple approaches to\ncalculating Moran's index. Moran's scatterplot will be ameliorated, and new\ntest methods will be proposed. The relationship between the global Moran's\nindex and Geary's coefficient will be discussed from two different vantage\npoints: spatial population and spatial sample. The sphere of applications for\nboth Moran's index and Geary's coefficient will be clarified and defined. One\nof theoretical findings is that Moran's index is a characteristic parameter of\nspatial weight matrices, so the selection of weight functions is very\nsignificant for autocorrelation analysis of geographical systems. A case study\nof 29 Chinese cities in 2000 will be employed to validate the innovatory models\nand methods. This work is a methodological study, which will simplify the\nprocess of autocorrelation analysis. The results of this study will lay the\nfoundation for the scaling analysis of spatial autocorrelation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 02:55:58 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Chen", "Yanguang", ""]]}, {"id": "1606.03729", "submitter": "Stephen Burgess", "authors": "Stephen Burgess, Jack Bowden, Frank Dudbridge, Simon G Thompson", "title": "Robust instrumental variable methods using multiple candidate\n  instruments with application to Mendelian randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mendelian randomization is the use of genetic variants to make causal\ninferences from observational data. The field is currently undergoing a\nrevolution fuelled by increasing numbers of genetic variants demonstrated to be\nassociated with exposures in genome-wide association studies, and the public\navailability of summarized data on genetic associations with exposures and\noutcomes from large consortia. A Mendelian randomization analysis with many\ngenetic variants can be performed relatively simply using summarized data.\nHowever, a causal interpretation is only assured if each genetic variant\nsatisfies the assumptions of an instrumental variable. To provide some\nprotection against failure of these assumptions, robust methods for\ninstrumental variable analysis have been proposed. Here, we develop three\nextensions to instrumental variable methods using: i) robust regression, ii)\nthe penalization of weights from candidate instruments with heterogeneous\ncausal estimates, and iii) L1 penalization. Results from a wide variety of\nrobust methods, including the recently-proposed MR-Egger and median-based\nmethods, are compared in an extensive simulation study. We demonstrate that two\nmethods, robust regression in an inverse-variance weighted method and a simple\nmedian of the causal estimates from the individual variants, have considerably\nimproved Type 1 error rates compared with conventional methods in a wide\nvariety of scenarios when up to 30% of the genetic variants are invalid\ninstruments. While the MR-Egger method gives unbiased estimates when its\nassumptions are satisfied, these estimates are less efficient than those from\nother methods and are highly sensitive to violations of the assumptions.\nMethods that make different assumptions should be used routinely to assess the\nrobustness of findings from applied Mendelian randomization investigations with\nmultiple genetic variants.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 15:12:43 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 13:46:18 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Burgess", "Stephen", ""], ["Bowden", "Jack", ""], ["Dudbridge", "Frank", ""], ["Thompson", "Simon G", ""]]}, {"id": "1606.03775", "submitter": "Janet Kim", "authors": "Janet S. Kim, Ana-Maria Staicu, Arnab Maity, Raymond J. Carroll, David\n  Ruppert", "title": "Additive Function-on-Function Regression", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study additive function-on-function regression where the mean response at\na particular time point depends on the time point itself as well as the entire\ncovariate trajectory. We develop a computationally efficient estimation\nmethodology based on a novel combination of spline bases with an eigenbasis to\nrepresent the trivariate kernel function. We discuss prediction of a new\nresponse trajectory, propose an inference procedure that accounts for total\nvariability in the predicted response curves, and construct pointwise\nprediction intervals. The estimation/inferential procedure accommodates\nrealistic scenarios such as correlated error structure as well as sparse and/or\nirregular designs. We investigate our methodology in finite sample size through\nsimulations and two real data applications.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 22:44:53 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 16:55:15 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Kim", "Janet S.", ""], ["Staicu", "Ana-Maria", ""], ["Maity", "Arnab", ""], ["Carroll", "Raymond J.", ""], ["Ruppert", "David", ""]]}, {"id": "1606.03803", "submitter": "Zhao Ren", "authors": "Zhao Ren, Yongjian Kang, Yingying Fan, Jinchi Lv", "title": "Tuning-Free Heterogeneity Pursuit in Massive Networks", "comments": "29 pages for the main text including 1 figure and 7 tables, 28 pages\n  for the Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity is often natural in many contemporary applications involving\nmassive data. While posing new challenges to effective learning, it can play a\ncrucial role in powering meaningful scientific discoveries through the\nunderstanding of important differences among subpopulations of interest. In\nthis paper, we exploit multiple networks with Gaussian graphs to encode the\nconnectivity patterns of a large number of features on the subpopulations. To\nuncover the heterogeneity of these structures across subpopulations, we suggest\na new framework of tuning-free heterogeneity pursuit (THP) via large-scale\ninference, where the number of networks is allowed to diverge. In particular,\ntwo new tests, the chi-based test and the linear functional-based test, are\nintroduced and their asymptotic null distributions are established. Under mild\nregularity conditions, we establish that both tests are optimal in achieving\nthe testable region boundary and the sample size requirement for the latter\ntest is minimal. Both theoretical guarantees and the tuning-free feature stem\nfrom efficient multiple-network estimation by our newly suggested approach of\nheterogeneous group square-root Lasso (HGSL) for high-dimensional\nmulti-response regression with heterogeneous noises. To solve this convex\nprogram, we further introduce a tuning-free algorithm that is scalable and\nenjoys provable convergence to the global optimum. Both computational and\ntheoretical advantages of our procedure are elucidated through simulation and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 03:58:23 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Ren", "Zhao", ""], ["Kang", "Yongjian", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1606.03814", "submitter": "Young-Geun Choi", "authors": "Young-Geun Choi, Johan Lim, Anindya Roy, and Junyong Park", "title": "Fixed support positive-definite modification of covariance matrix\n  estimators via linear shrinkage", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2018.12.002", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the positive definiteness (PDness) problem in\ncovariance matrix estimation. For high dimensional data, many regularized\nestimators are proposed under structural assumptions on the true covariance\nmatrix including sparsity. They are shown to be asymptotically consistent and\nrate-optimal in estimating the true covariance matrix and its structure.\nHowever, many of them do not take into account the PDness of the estimator and\nproduce a non-PD estimate. To achieve the PDness, researchers consider\nadditional regularizations (or constraints) on eigenvalues, which make both the\nasymptotic analysis and computation much harder. In this paper, we propose a\nsimple modification of the regularized covariance matrix estimator to make it\nPD while preserving the support. We revisit the idea of linear shrinkage and\npropose to take a convex combination between the first-stage estimator (the\nregularized covariance matrix without PDness) and a given form of diagonal\nmatrix. The proposed modification, which we denote as FSPD (Fixed Support and\nPositive Definiteness) estimator, is shown to preserve the asymptotic\nproperties of the first-stage estimator, if the shrinkage parameters are\ncarefully selected. It has a closed form expression and its computation is\noptimization-free, unlike existing PD sparse estimators. In addition, the FSPD\nis generic in the sense that it can be applied to any non-PD matrix including\nthe precision matrix. The FSPD estimator is numerically compared with other\nsparse PD estimators to understand its finite sample properties as well as its\ncomputational gain. It is also applied to two multivariate procedures relying\non the covariance matrix estimator -- the linear minimax classification problem\nand the Markowitz portfolio optimization problem -- and is shown to\nsubstantially improve the performance of both procedures.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 05:21:18 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 07:55:13 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Choi", "Young-Geun", ""], ["Lim", "Johan", ""], ["Roy", "Anindya", ""], ["Park", "Junyong", ""]]}, {"id": "1606.03844", "submitter": "Lu-Hung Chen", "authors": "Lu-Hung Chen and Ci-Ren Jiang", "title": "Sensible Functional Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is to extend Fisher's linear discriminant analysis\n(LDA) to both densely re-corded functional data and sparsely observed\nlongitudinal data for general $c$-category classification problems. We propose\nan efficient approach to identify the optimal LDA projections in addition to\nmanaging the noninvertibility issue of the covariance operator emerging from\nthis extension. A conditional expectation technique is employed to tackle the\nchallenge of projecting sparse data to the LDA directions. We study the\nasymptotic properties of the proposed estimators and show that asymptotically\nperfect classification can be achieved in certain circumstances. The\nperformance of this new approach is further demonstrated with numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 07:28:55 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 16:23:23 GMT"}, {"version": "v3", "created": "Wed, 6 Sep 2017 03:53:41 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Chen", "Lu-Hung", ""], ["Jiang", "Ci-Ren", ""]]}, {"id": "1606.03940", "submitter": "Ruben Dezeure", "authors": "Ruben Dezeure, Peter B\\\"uhlmann, Cun-Hui Zhang", "title": "High-dimensional simultaneous inference with the bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a residual and wild bootstrap methodology for individual and\nsimultaneous inference in high-dimensional linear models with possibly\nnon-Gaussian and heteroscedastic errors. We establish asymptotic consistency\nfor simultaneous inference for parameters in groups $G$, where $p \\gg n$, $s_0\n= o(n^{1/2}/\\{\\log(p) \\log(|G|)^{1/2}\\})$ and $\\log(|G|) = o(n^{1/7})$, with\n$p$ the number of variables, $n$ the sample size and $s_0$ denoting the\nsparsity. The theory is complemented by many empirical results. Our proposed\nprocedures are implemented in the R-package hdi.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 13:35:05 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Dezeure", "Ruben", ""], ["B\u00fchlmann", "Peter", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1606.03973", "submitter": "Frank Konietschke", "authors": "Edgar Brunner, Frank Konietschke, Markus Pauly, Madan L. Puri", "title": "Rank-Based Procedures in Factorial Designs: Hypotheses about\n  Nonparametric Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing tests for factorial designs in the nonparametric case are based on\nhypotheses formulated in terms of distribution functions. Typical null\nhypotheses, however, are formulated in terms of some parameters or effect\nmeasures, particularly in heteroscedastic settings. Here this idea is extended\nto nonparametric models by introducing a novel nonparametric\nANOVA-type-statistic based on ranks which is suitable for testing hypotheses\nformulated in meaningful nonparametric treatment effects in general factorial\ndesigns. This is achieved by a careful in-depth study of the common\ndistribution of rank-based estimators for the treatment effects. Since the\nstatistic is asymptotically not a pivotal quantity we propose three different\napproximation techniques, discuss their theoretic properties and compare them\nin extensive simulations together with two additionalWald-type tests. An\nextension of the presented idea to general repeated measures designs is briefly\noutlined. The proposed rank-based procedures maintain the pre-assigned type-I\nerror rate quite accurately, also in unbalanced and heteroscedastic models.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:38:26 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 21:19:26 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Brunner", "Edgar", ""], ["Konietschke", "Frank", ""], ["Pauly", "Markus", ""], ["Puri", "Madan L.", ""]]}, {"id": "1606.03987", "submitter": "Thomas Ondra", "authors": "Thomas Ondra, Sebastian Jobj\\\"ornsson, Robert A. Beckman, Carl-Fredrik\n  Burman, Franz K\\\"onig, Nigel Stallard, Martin Posch", "title": "Optimizing Trial Designs for Targeted Therapies", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0163726", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important objective in the development of targeted therapies is to\nidentify the populations where the treatment under consideration has positive\nbenefit risk balance. We consider pivotal clinical trials, where the efficacy\nof a treatment is tested in an overall population and/or in a pre-specified\nsubpopulation. Based on a decision theoretic framework we derive optimized\ntrial designs by maximizing utility functions. Features to be optimized include\nthe sample size and the population in which the trial is performed (the full\npopulation or the targeted subgroup only) as well as the underlying multiple\ntest procedure. The approach accounts for prior knowledge of the efficacy of\nthe drug in the considered populations using a two dimensional prior\ndistribution. The considered utility functions account for the costs of the\nclinical trial as well as the expected benefit when demonstrating efficacy in\nthe different subpopulations. We model utility functions from a sponsor's as\nwell as from a public health perspective, reflecting actual civil interests.\nExamples of optimized trial designs obtained by numerical optimization are\npresented for both perspectives.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 15:03:50 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Ondra", "Thomas", ""], ["Jobj\u00f6rnsson", "Sebastian", ""], ["Beckman", "Robert A.", ""], ["Burman", "Carl-Fredrik", ""], ["K\u00f6nig", "Franz", ""], ["Stallard", "Nigel", ""], ["Posch", "Martin", ""]]}, {"id": "1606.04010", "submitter": "Joost Kruis", "authors": "Joost Kruis, Gunter Maris", "title": "Three representations of the Ising model", "comments": "11 pages, 1 figure", "journal-ref": "Sci. Rep. 6, 34175 (2016)", "doi": "10.1038/srep34175", "report-no": null, "categories": "stat.ME cond-mat.dis-nn cond-mat.stat-mech math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical models that analyse (pairwise) relations between variables\nencompass assumptions about the underlying mechanism that generated the\nassociations in the observed data. In the present paper we demonstrate that\nthree Ising model representations exist that, although each proposes a distinct\ntheoretical explanation for the observed associations, are mathematically\nequivalent. This equivalence allows the researcher to interpret the results of\none model in three different ways. We illustrate the ramifications of this by\ndiscussing concepts that are conceived as problematic in their traditional\nexplanation, yet when interpreted in the context of another explanation make\nimmediate sense.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 16:07:45 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 11:53:00 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 13:11:38 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Kruis", "Joost", ""], ["Maris", "Gunter", ""]]}, {"id": "1606.04082", "submitter": "Moritz Schauer", "authors": "Frank van der Meulen, Moritz Schauer", "title": "Bayesian estimation of incompletely observed diffusions", "comments": null, "journal-ref": "Stochastics 90 (5), 2018, pp. 641-662", "doi": "10.1080/17442508.2017.1381097", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for Bayesian estimation of incompletely\nobserved multivariate diffusion processes. Observations are assumed to be\ndiscrete in time, noisy and incomplete. We assume the drift and diffusion\ncoefficient depend on an unknown parameter. A data-augmentation algorithm for\ndrawing from the posterior distribution is presented which is based on\nsimulating diffusion bridges conditional on a noisy incomplete observation at\nan intermediate time. The dynamics of such filtered bridges are derived and it\nis shown how these can be simulated using a generalised version of the guided\nproposals introduced in Schauer et al. (2016).\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:45:23 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 12:49:40 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "1606.04086", "submitter": "Michal Koles\\'ar", "authors": "Michal Koles\\'ar and Christoph Rothe", "title": "Inference in Regression Discontinuity Designs with a Discrete Running\n  Variable", "comments": "47 pages plus supplemental materials", "journal-ref": "American Economic Review, vol. 108, no. 8, August 2018 (pp.\n  2277-2304)", "doi": "10.1257/aer.20160945", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference in regression discontinuity designs when the running\nvariable only takes a moderate number of distinct values. In particular, we\nstudy the common practice of using confidence intervals (CIs) based on standard\nerrors that are clustered by the running variable as a means to make inference\nrobust to model misspecification (Lee and Card, 2008). We derive theoretical\nresults and present simulation and empirical evidence showing that these CIs do\nnot guard against model misspecification, and that they have poor coverage\nproperties. We therefore recommend against using these CIs in practice. We\ninstead propose two alternative CIs with guaranteed coverage properties under\neasily interpretable restrictions on the conditional expectation function.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:55:31 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 02:48:18 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 18:10:35 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 23:14:51 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Koles\u00e1r", "Michal", ""], ["Rothe", "Christoph", ""]]}, {"id": "1606.04146", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, Laura Peck, and Luke Keele", "title": "Inference for Instrumental Variables: A Randomization Inference Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of instrumental variables (IV) provides a framework to study\ncausal effects in both randomized experiments with noncompliance and in\nobservational studies where natural circumstances produce as-if random nudges\nto accept treatment. Traditionally, inference for IV relied on asymptotic\napproximations of the distribution of the Wald estimator or two-stage least\nsquares, often with structural modeling assumptions and/or moment conditions.\nIn this paper, we utilize the randomization inference approach to IV inference.\nFirst, we outline the exact method, which uses the randomized assignment of\ntreatment in experiments as a basis for inference, but lacks a closed-form\nsolution and may be computationally infeasible in many applications. We then\nprovide an alternative to the exact method, the almost exact method, which is\ncomputationally feasible but retains the advantages of the exact method. We\nalso review asymptotic methods of inference, including those associated with\ntwo-stage least squares, and analytically compare them to randomization\ninference methods. We also perform additional comparisons using a set of\nsimulations. We conclude with three different applications from the social\nsciences.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 21:27:27 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 18:12:01 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Kang", "Hyunseung", ""], ["Peck", "Laura", ""], ["Keele", "Luke", ""]]}, {"id": "1606.04172", "submitter": "Yan Yuan", "authors": "Yan Yuan, Qian M. Zhou, Bingying Li, Hengrui Cai, Eric J. Chow,\n  Gregory T. Armstrong", "title": "A Threshold-free Prospective Prediction Accuracy Measure for Censored\n  Time to Event Data", "comments": "17 pages, 2 figures, 3 tables", "journal-ref": "Statistics in Medicine 37(10):1671-1681, 2018", "doi": "10.1002/sim.7606", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction performance of a risk scoring system needs to be carefully\nassessed before its adoption in clinical practice. Clinical preventive care\noften uses risk scores to screen asymptomatic population. The primary clinical\ninterest is to predict the risk of having an event by a pre-specified future\ntime $t_0$. Prospective accuracy measures such as positive predictive values\nhave been recommended for evaluating the predictive performance. However, for\ncommonly used continuous or ordinal risk score systems, these measures require\na subjective cutoff threshold value that dichotomizes the risk scores. The need\nfor a cut-off value created barriers for practitioners and researchers. In this\npaper, we propose a threshold-free summary index of positive predictive values\nthat accommodates time-dependent event status. We develop a nonparametric\nestimator and provide an inference procedure for comparing this summary measure\nbetween competing risk scores for censored time to event data. We conduct a\nsimulation study to examine the finite-sample performance of the proposed\nestimation and inference procedures. Lastly, we illustrate the use of this\nmeasure on a real data example, comparing two risk score systems for predicting\nheart failure in childhood cancer survivors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 23:32:06 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 15:08:43 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Yuan", "Yan", ""], ["Zhou", "Qian M.", ""], ["Li", "Bingying", ""], ["Cai", "Hengrui", ""], ["Chow", "Eric J.", ""], ["Armstrong", "Gregory T.", ""]]}, {"id": "1606.04371", "submitter": "Richard Darlington", "authors": "Richard B. Darlington", "title": "Minimax is the best electoral system after all", "comments": "40 pages, no figures. Section 3 now recommends three tie-breaking\n  methods, not just one. Elsewhere, many small changes were made to increase\n  clarity or correct typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When each voter rates or ranks several candidates for a single office, a\nstrong Condorcet winner (SCW) is one who beats all others in two-way races.\nAmong 21 electoral systems examined, 18 will sometimes make candidate X the\nwinner even if thousands of voters would need to change their votes to make X a\nSCW while another candidate Y could become a SCW with only one such change.\nAnalysis supports the intuitive conclusion that these 18 systems are\nunacceptable.\n  The well-known minimax system survives this test. It fails 10 others, but\nthere are good reasons to ignore all 10. Minimax-T adds a new tie-breaker. It\nsurpasses competing systems on a combination of simplicity, transparency, voter\nprivacy, input flexibility, resistance to strategic voting, and rarity of ties.\nIt allows write-ins, machine counting except for write-ins, voters who don't\nrate or rank every candidate, and tied ratings or ranks.\n  Eleven computer simulation studies used 6 different definitions (one at a\ntime) of the best candidate, and found that minimax-T always soundly beat all\nother tested systems at picking that candidate. A new maximum-likelihood\nelectoral system named CMO is the theoretically optimum system under reasonable\nconditions, but is too complex for use in real-world elections. In computer\nsimulations, minimax and minimax-T nearly always pick the same winners as CMO.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 13:50:57 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 16:27:33 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Darlington", "Richard B.", ""]]}, {"id": "1606.04406", "submitter": "Valdemar Melicher", "authors": "Valdemar Melicher, Tom Haber, Wim Vanroose", "title": "Fast derivatives of likelihood functionals for ODE based models using\n  adjoint-state method", "comments": "5 figures", "journal-ref": "Computational Statistics, 2017. The final publication is available\n  at Springer via http://dx.doi.org/10.1007/s00180-017-0765-8", "doi": "10.1007/s00180-017-0765-8", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider time series data modeled by ordinary differential equations\n(ODEs), widespread models in physics, chemistry, biology and science in\ngeneral. The sensitivity analysis of such dynamical systems usually requires\ncalculation of various derivatives with respect to the model parameters.\n  We employ the adjoint state method (ASM) for efficient computation of the\nfirst and the second derivatives of likelihood functionals constrained by ODEs\nwith respect to the parameters of the underlying ODE model. Essentially, the\ngradient can be computed with a cost (measured by model evaluations) that is\nindependent of the number of the ODE model parameters and the Hessian with a\nlinear cost in the number of the parameters instead of the quadratic one. The\nsensitivity analysis becomes feasible even if the parametric space is\nhigh-dimensional.\n  The main contributions are derivation and rigorous analysis of the ASM in the\nstatistical context, when the discrete data are coupled with the continuous ODE\nmodel. Further, we present a highly optimized implementation of the results and\nits benchmarks on a number of problems.\n  The results are directly applicable in (e.g.) maximum-likelihood estimation\nor Bayesian sampling of ODE based statistical models, allowing for faster, more\nstable estimation of parameters of the underlying ODE model.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 14:53:21 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 07:29:23 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 14:39:48 GMT"}, {"version": "v4", "created": "Mon, 12 Dec 2016 20:51:02 GMT"}, {"version": "v5", "created": "Tue, 19 Sep 2017 09:32:14 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Melicher", "Valdemar", ""], ["Haber", "Tom", ""], ["Vanroose", "Wim", ""]]}, {"id": "1606.04431", "submitter": "Shu Li", "authors": "Shu Li, Jan Ernest, Peter B\\\"uhlmann", "title": "Nonparametric causal inference from observational time series through\n  marginal integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference from observational data is an ambitious but highly relevant\ntask, with diverse applications ranging from natural to social sciences. Within\nthe scope of nonparametric time series, causal inference defined through\ninterventions (cf. Pearl (2000)) is largely unexplored, although time order\nsimplifies the problem substantially. We consider a marginal integration scheme\nfor inferring causal effects from observational time series data, MINT-T\n(marginal integration in time series), which is an adaptation for time series\nof a method proposed by Ernest and B\\\"{u}hlmann (Electron. J. Statist, pp.\n3155-3194, vol. 9, 2015) for the case of independent data. Our approach for\nstationary stochastic processes is fully nonparametric and, assuming no\ninstantaneous effects consistently recovers the total causal effect of a single\nintervention with optimal one-dimensional nonparametric convergence rate\n$n^{-2/5}$ assuming regularity conditions and twice differentiability of a\ncertain corresponding regression function. Therefore, MINT-T remains largely\nunaffected by the curse of dimensionality as long as smoothness conditions hold\nin higher dimensions and it is feasible for a large class of stationary time\nseries, including nonlinear and multivariate processes. For the case with\ninstantaneous effects, we provide a procedure which guards against false\npositive causal statements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 15:49:06 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 15:58:59 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 14:40:28 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Li", "Shu", ""], ["Ernest", "Jan", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1606.04457", "submitter": "Maria DeYoreo", "authors": "Maria DeYoreo and Jerome P. Reiter", "title": "Bayesian mixture modeling for multivariate conditional distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian mixture model for estimating the joint distribution of\nmixed ordinal, nominal, and continuous data conditional on a set of fixed\nvariables. The model uses multivariate normal and categorical mixture kernels\nfor the random variables. It induces dependence between the random and fixed\nvariables through the means of the multivariate normal mixture kernels and via\na truncated local Dirichlet process. The latter encourages observations with\nsimilar values of the fixed variables to share mixture components. Using a\nsimulation of data fusion, we illustrate that the model can estimate underlying\nrelationships in the data and the distributions of the missing values more\naccurately than a mixture model applied to the random and fixed variables\njointly. We use the model to analyze consumers' reading behaviors using a quota\nsample, i.e., a sample where the empirical distribution of some variables is\nfixed by design and so should not be modeled as random, conducted by the book\npublisher HarperCollins.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:56:19 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 17:50:54 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 18:07:42 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["DeYoreo", "Maria", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1606.04771", "submitter": "Corinne Sinner", "authors": "Corinne Sinner and Patrick Weber", "title": "Moments and Entropy of the Interpolating Family of Size Distributions", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sinner et al. (2016) recently introduced a five-parameter family of size\ndistributions, coined Interpolating Family or IF distribution for short. In\nthis complementary note, we take advantage of the tractability of the IF\ndistribution to compute the moments and the differential entropy. As a\nconsequence, we deduce at a single stroke the corresponding expressions for\nmany well-known size distributions arising as special cases of the IF\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:05:02 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Sinner", "Corinne", ""], ["Weber", "Patrick", ""]]}, {"id": "1606.04802", "submitter": "Simon Wood", "authors": "Simon N. Wood and Matteo Fasiolo", "title": "A generalized Fellner-Schall method for smoothing parameter estimation\n  with application to Tweedie location, scale and shape models", "comments": null, "journal-ref": null, "doi": "10.1111/biom.12666", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of smoothing parameters and variance components in\nmodels with a regular log likelihood subject to quadratic penalization of the\nmodel coefficients, via a generalization of the method of Fellner (1986) and\nSchall (1991). In particular: (i) we generalize the original method to the case\nof penalties that are linear in several smoothing parameters, thereby covering\nthe important cases of tensor product and adaptive smoothers; (ii) we show why\nthe method's steps increase the restricted marginal likelihood of the model,\nthat it tends to converge faster than the EM algorithm, or obvious\naccelerations of this, and investigate its relation to Newton optimization;\n(iii) we generalize the method to any Fisher regular likelihood. The method\nrepresents a considerable simplification over existing methods of estimating\nsmoothing parameters in the context of regular likelihoods, without sacrificing\ngenerality: for example, it is only necessary to compute with the same first\nand second derivatives of the log-likelihood required for coefficient\nestimation, and not with the third or fourth order derivatives required by\nalternative approaches. Examples are provided which would have been impossible\nor impractical with pre-existing Fellner-Schall methods, along with an example\nof a Tweedie location, scale and shape model which would be a challenge for\nalternative methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:57:01 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wood", "Simon N.", ""], ["Fasiolo", "Matteo", ""]]}, {"id": "1606.04891", "submitter": "James Weber PhD", "authors": "James S. Weber", "title": "Calculating Method of Moments Uniform Bin Width Histograms", "comments": "eleven figures & tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clear articulation of Method of Moments (MOM) Histograms is instructive and\nhas waited 121 years since 1895. Also of interest are enabling uniform bin\nwidth (UBW) shape level sets. Mean-variance MOM uniform bin width frequency and\ndensity histograms are not unique, however ranking them by histogram skewness\ncompared to data skewness helps. Although theoretical issues rarely take second\nplace to calculations, here calculations based on shape level sets are central\nand challenge uncritically accepted practice. Complete understanding requires\nfamiliarity with histogram shape level sets and arithmetic progressions in the\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 18:04:00 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Weber", "James S.", ""]]}, {"id": "1606.04969", "submitter": "Christian R\\\"over", "authors": "Tim Friede, Christian R\\\"over, Simon Wandel and Beat Neuenschwander", "title": "Meta-analysis of two studies in the presence of heterogeneity with\n  applications in rare diseases", "comments": "12 pages, 4 figures", "journal-ref": "Biometrical Journal, 59 (4): 658-671 (2017)", "doi": "10.1002/bimj.201500236", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects meta-analyses are used to combine evidence of treatment\neffects from multiple studies. Since treatment effects may vary across trials\ndue to differences in study characteristics, heterogeneity in treatment effects\nbetween studies must be accounted for to achieve valid inference. The standard\nmodel for random-effects meta-analysis assumes approximately normal effect\nestimates and a normal random-effects model. However, standard methods based on\nthis model ignore the uncertainty in estimating the between-trial\nheterogeneity. In the special setting of only two studies and in the presence\nof heterogeneity we investigate here alternatives such as the\nHartung-Knapp-Sidik-Jonkman method (HKSJ), the modified Knapp-Hartung method\n(mKH, a variation of the HKSJ method) and Bayesian random-effects meta-analyses\nwith priors covering plausible heterogeneity values. The properties of these\nmethods are assessed by applying them to five examples from various rare\ndiseases and by a simulation study. Whereas the standard method based on normal\nquantiles has poor coverage, the HKSJ and mKH generally lead to very long, and\ntherefore inconclusive, confidence intervals. The Bayesian intervals on the\nwhole show satisfying properties and offer a reasonable compromise between\nthese two extremes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 20:29:03 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Friede", "Tim", ""], ["R\u00f6ver", "Christian", ""], ["Wandel", "Simon", ""], ["Neuenschwander", "Beat", ""]]}, {"id": "1606.05000", "submitter": "C Ben Gibson", "authors": "C. Ben Gibson and Burrel Vann Jr", "title": "The Bootstrapped Robustness Assessment for Qualitative Comparative\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qualitative Comparative Analysis (QCA) has been increasingly used in recent\nyears due to its purported construction of a middle path between case-oriented\nand variable-oriented methods. Despite its popularity, a key element of the\nmethod has been criticized for possibly not distinguishing random from real\npatterns in data, rendering its usefulness questionable. Critics of the method\nsuggest a straightforward technique to test whether QCA will return a\nconfiguration when given random data. We adapt this technique to determine the\nprobability that a given QCA application would return a random result. This\nassessment can be used as a hypothesis test for QCA, with an interpretation\nsimilar to a p-value. Using repeated applications of QCA to randomly-generated\ndata, we first show that generally, the tendency for QCA to return spurious\nresults is attenuated by using reasonable consistency score and configurational\nN thresholds; however, this varies considerably according to the basic\nstructure of the data. Second, we suggest an application-specific assessment of\nQCA results, illustrated using the case of Tea Party rallies in Florida. This\nmethod, which we coin the Bootstrapped Robustness Assessment for QCA (baQCA),\ncan provide researchers with recommendations for consistency score and\nconfigurational N thresholds.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 23:04:49 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Gibson", "C. Ben", ""], ["Vann", "Burrel", "Jr"]]}, {"id": "1606.05021", "submitter": "Minsuk Shin", "authors": "Minsuk Shin, Anirban Bhattacharya, and Valen E. Johnson", "title": "Functional Horseshoe Priors for Subspace Shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new shrinkage prior on function spaces, called the functional\nhorseshoe prior (fHS), that encourages shrinkage towards parametric classes of\nfunctions. Unlike other shrinkage priors for parametric models, the fHS\nshrinkage acts on the shape of the function rather than inducing sparsity on\nmodel parameters. We study the efficacy of the proposed approach by showing an\nadaptive posterior concentration property on the function. We also demonstrate\nconsistency of the model selection procedure that thresholds the shrinkage\nparameter of the functional horseshoe prior. We apply the fHS prior to\nnonparametric additive models and compare its performance with procedures based\non the standard horseshoe prior and several penalized likelihood approaches. We\nfind that the new procedure achieves smaller estimation error and more accurate\nmodel selection than other procedures in several simulated and real examples.\nThe supplementary material for this article, which contains additional\nsimulated and real data examples, MCMC diagnostics, and proofs of the\ntheoretical results, is available online.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 01:30:47 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 19:23:32 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 22:11:49 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Shin", "Minsuk", ""], ["Bhattacharya", "Anirban", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1606.05022", "submitter": "Worapree Ole Maneesoonthorn", "authors": "Michael Stanley Smith and Worapree Maneesoonthorn", "title": "Inversion Copulas from Nonlinear State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to construct copulas from the inversion of nonlinear state space\nmodels. These allow for new time series models that have the same serial\ndependence structure of a state space model, but with an arbitrary marginal\ndistribution, and flexible density forecasts. We examine the time series\nproperties of the copulas, outline serial dependence measures, and estimate the\nmodels using likelihood-based methods. Copulas constructed from three example\nstate space models are considered: a stochastic volatility model with an\nunobserved component, a Markov switching autoregression, and a Gaussian linear\nunobserved component model. We show that all three inversion copulas with\nflexible margins improve the fit and density forecasts of quarterly U.S. broad\ninflation and electricity inflation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 01:32:45 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 10:01:04 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Smith", "Michael Stanley", ""], ["Maneesoonthorn", "Worapree", ""]]}, {"id": "1606.05049", "submitter": "Nan Zou", "authors": "Minghua Wu, Pin You, and Nan Zou", "title": "On spurious regressions with trending variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines three types of spurious regressions where both the\ndependent and independent variables contain deterministic trends, stochastic\ntrends, or breaking trends. We show that the problem of spurious regression\ndisappears if the trend functions are included as additional regressors. In the\npresence of autocorrelation, we show that using a Feasible General Least Square\n(FGLS) estimator can help alleviate or eliminate the problem. Our theoretical\nresults are clearly reflected in finite samples. As an illustration, we apply\nour methods to revisit the seminal study of Yule (1926).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 04:55:13 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Wu", "Minghua", ""], ["You", "Pin", ""], ["Zou", "Nan", ""]]}, {"id": "1606.05188", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "A Framework for Optimal Matching for Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for matching estimators for causal effect from\nobservational data that is based on minimizing the dual norm of estimation\nerror when expressed as an operator. We show that many popular matching\nestimators can be expressed as optimal in this framework, including\nnearest-neighbor matching, coarsened exact matching, and mean-matched sampling.\nThis reveals their motivation and aptness as structural priors formulated by\nembedding the effect in a particular functional space. This also gives rise to\na range of new, kernel-based matching estimators that arise when one embeds the\neffect in a reproducing kernel Hilbert space. Depending on the case, these\nestimators can be found using either quadratic optimization or integer\noptimization. We show that estimators based on universal kernels are\nuniversally consistent without model specification. In empirical results using\nboth synthetic and real data, the new, kernel-based estimators outperform all\nstandard causal estimators in estimation error.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 14:02:23 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 14:59:57 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1606.05203", "submitter": "Rose Baker", "authors": "Rose D. Baker", "title": "A new asymmetric generalisation of the t-distribution", "comments": "13 pages, 2 figures, Statistics, exact distribution theoru", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 6-parameter fat-tailed distribution is proposed that generalises the\nt-distribution and allows asymmetry of scale and also of tail power, whilst\navoiding the discontinuity of the second derivative of the split-t (AST)\ndistribution. With the sixth parameter set to unity and no asymmetry, the\ndistribution reduces to a t-distribution, but with the sixth parameter reduced,\nfatter tails than those of the t-distribution are allowed (the tails start\nearlier) and the distribution generalises Johnson's $S_U$ distribution. Data\nfitting is illustrated with examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 14:35:17 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Baker", "Rose D.", ""]]}, {"id": "1606.05204", "submitter": "Simon Grund", "authors": "Simon Grund, Oliver L\\\"udtke, Alexander Robitzsch", "title": "Multiple imputation of missing covariate values in multilevel models\n  with random slopes: A cautionary note", "comments": null, "journal-ref": "Behavior Research Methods, 48, 640-649 (2016)", "doi": "10.3758/s13428-015-0590-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) has become one of the main procedures used to treat\nmissing data, but the guidelines from the methodological literature are not\neasily transferred to multilevel research. For models including random slopes,\nproper MI can be difficult, especially when the covariate values are partially\nmissing. In the present article, we discuss applications of MI in multilevel\nrandom-coefficient models, theoretical challenges posed by slope variation, and\nthe current limitations of standard MI software. Our findings from three\nsimulation studies suggest that (a) MI is able to recover most parameters, but\nis currently not well suited to capture slope variation entirely when covariate\nvalues are missing; (b) MI offers reasonable estimates for most parameters,\neven in smaller samples or when its assumptions are not met; and (c) listwise\ndeletion can be an alternative worth considering when preserving the slope\nvariance is particularly important.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 14:36:01 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Grund", "Simon", ""], ["L\u00fcdtke", "Oliver", ""], ["Robitzsch", "Alexander", ""]]}, {"id": "1606.05277", "submitter": "Abel Rodriguez", "authors": "Perla Reyes and Abel Rodriguez", "title": "Stochastic blockmodels for exchangeable collections of networks", "comments": "7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a novel class of stochastic blockmodels using Bayesian\nnonparametric mixtures. These model allows us to jointly estimate the structure\nof multiple networks and explicitly compare the community structures underlying\nthem, while allowing us to capture realistic properties of the underlying\nnetworks. Inference is carried out using MCMC algorithms that incorporates\nsequentially allocated split-merge steps to improve mixing. The models are\nillustrated using a simulation study and a variety of real-life examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:19:30 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Reyes", "Perla", ""], ["Rodriguez", "Abel", ""]]}, {"id": "1606.05279", "submitter": "Tirthankar Dasgupta", "authors": "Rahul Mukerjee, Tirthankar Dasgupta, Donald B. Rubin", "title": "Causal Inference in Rebuilding and Extending the Recondite Bridge\n  between Finite Population Sampling and Experimental Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers causal inference for treatment contrasts from a\nrandomized experiment using potential outcomes in a finite population setting.\nAdopting a Neymanian repeated sampling approach that integrates such causal\ninference with finite population survey sampling, an inferential framework is\ndeveloped for general mechanisms of assigning experimental units to multiple\ntreatments. This framework extends classical methods by allowing the\npossibility of randomization restrictions and unequal replications. Novel\nconditions that are \"milder\" than strict additivity of treatment effects, yet\npermit unbiased estimation of the finite population sampling variance of any\ntreatment contrast estimator, are derived. The consequences of departures from\nsuch conditions are also studied under the criterion of minimax bias, and a new\njustification for using the Neymanian conservative sampling variance estimator\nin experiments is provided. The proposed approach can readily be extended to\nthe case of treatments with a general factorial structure.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:22:11 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Mukerjee", "Rahul", ""], ["Dasgupta", "Tirthankar", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1606.05333", "submitter": "Piotr Sobczyk", "authors": "Piotr Sobczyk and Malgorzata Bogdan and Julie Josse", "title": "Bayesian dimensionality reduction with PCA using penalized\n  semi-integrated likelihood", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem of estimating the number of principal components in\nPrincipal Com- ponents Analysis (PCA). Despite of the importance of the problem\nand the multitude of solutions proposed in the literature, it comes as a\nsurprise that there does not exist a coherent asymptotic framework which would\njustify different approaches depending on the actual size of the data set. In\nthis paper we address this issue by presenting an approximate Bayesian approach\nbased on Laplace approximation and introducing a general method for building\nthe model selection criteria, called PEnalized SEmi-integrated Likelihood\n(PESEL). Our general framework encompasses a variety of existing approaches\nbased on probabilistic models, like e.g. Bayesian Information Criterion for the\nProbabilistic PCA (PPCA), and allows for construction of new criteria,\ndepending on the size of the data set at hand. Specifically, we define PESEL\nwhen the number of variables substantially exceeds the number of observations.\nWe also report results of extensive simulation studies and real data analysis,\nwhich illustrate good properties of our proposed criteria as compared to the\nstate-of- the-art methods and very recent proposals. Specifially, these\nsimulations show that PESEL based criteria can be quite robust against\ndeviations from the probabilistic model assumptions. Selected PESEL based\ncriteria for the estimation of the number of principal components are\nimplemented in R package varclust, which is available on github\n(https://github.com/psobczyk/varclust).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:50:58 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 09:22:57 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Sobczyk", "Piotr", ""], ["Bogdan", "Malgorzata", ""], ["Josse", "Julie", ""]]}, {"id": "1606.05360", "submitter": "Mertens Bart", "authors": "Bart J. A. Mertens", "title": "Transformation, normalization and batch effect in the analysis of mass\n  spectrometry data for omics studies", "comments": "This is a draft version for a chapter to be published in the new\n  edited volume \"Statistical Analysis of Proteomics, Metabolomics, and\n  Lipidomics Data Using Mass Spectrometry\" (eds, Datta, S. and Mertens, B. J.\n  A.) to be published by Springer in the new series \"Frontiers in Probability\n  and the Statistical Sciences\" (anticipated publication year 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data transformation, normalization and handling of batch effect are a key\npart of data analysis for almost all spectrometry-based omics data. This paper\nreviews and contrasts these three distinct aspects. We present a systematic\noverview of the key approaches and critically review some common procedures.\nMuch of this paper is inspired by mass spectrometry based experimentation, but\nmost of our discussion carries over to omics data using distinct spectrometric\napproaches generally.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 20:16:11 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Mertens", "Bart J. A.", ""]]}, {"id": "1606.05407", "submitter": "Yanan Fan Dr", "authors": "T. Rodrigues, J.-L. Dortet-Bernadet, Y. Fan", "title": "Pyramid quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression models provide a wide picture of the conditional\ndistributions of the response variable by capturing the effect of the\ncovariates at different quantile levels. In most applications, the parametric\nform of those conditional distributions is unknown and varies across the\ncovariate space, so fitting the given quantile levels simultaneously without\nrelying on parametric assumptions is crucial. In this work we propose a\nBayesian model for simultaneous linear quantile regression. More specifically,\nwe propose to model the conditional distributions by using random probability\nmeasures known as quantile pyramids. Unlike many existing approaches, our\nframework allows us to specify meaningful priors on the conditional\ndistributions, whilst retaining the flexibility afforded by the nonparametric\nerror distribution formulation. Simulation studies demonstrate the flexibility\nof the proposed approach in estimating diverse scenarios, generally\noutperforming other competitive methods. The method is particularly promising\nfor modelling the extremal quantiles. Applications to linear splines and\nextreme value analysis are also explored through real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 02:31:38 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 05:52:45 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Rodrigues", "T.", ""], ["Dortet-Bernadet", "J. -L.", ""], ["Fan", "Y.", ""]]}, {"id": "1606.05418", "submitter": "Jiannan Lu", "authors": "Jiannan Lu", "title": "Covariate adjustment in randomization-based causal inference for 2K\n  factorial designs", "comments": "To appear in Statistics and Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop finite-population asymptotic theory for covariate adjustment in\nrandomization-based causal inference for 2K factorial designs. In particular,\nwe confirm that both the unadjusted and covariate-adjusted estimators of the\nfactorial effects are asymptotically normal, and the latter is more precise\nthan the former.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 05:00:11 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 15:07:12 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Lu", "Jiannan", ""]]}, {"id": "1606.05562", "submitter": "Renato J Cintra", "authors": "T. L. T. da Silveira, F. M. Bayer, R. J. Cintra, S. Kulasekera, A.\n  Madanayake, A. J. Kozakevicius", "title": "An Orthogonal 16-point Approximate DCT for Image and Video Compression", "comments": "18 pages, 7 figures, 6 tables", "journal-ref": "Multidimensional Systems and Signal Processing, vol. 27, no. 1,\n  pp. 87-104, 2016", "doi": "10.1007/s11045-014-0291-6", "report-no": null, "categories": "cs.IT cs.AR cs.NA math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-complexity orthogonal multiplierless approximation for the 16-point\ndiscrete cosine transform (DCT) was introduced. The proposed method was\ndesigned to possess a very low computational cost. A fast algorithm based on\nmatrix factorization was proposed requiring only 60~additions. The proposed\narchitecture outperforms classical and state-of-the-art algorithms when\nassessed as a tool for image and video compression. Digital VLSI hardware\nimplementations were also proposed being physically realized in FPGA technology\nand implemented in 45 nm up to synthesis and place-route levels. Additionally,\nthe proposed method was embedded into a high efficiency video coding (HEVC)\nreference software for actual proof-of-concept. Obtained results show\nnegligible video degradation when compared to Chen DCT algorithm in HEVC.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 01:19:46 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["da Silveira", "T. L. T.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""], ["Kozakevicius", "A. J.", ""]]}, {"id": "1606.05584", "submitter": "M.I. Borrajo", "authors": "Mar\\'ia Isabel Borrajo, Wenceslao Gonz\\'alez-Manteiga and Mar\\'ia\n  Dolores Mart\\'inez-Miranda", "title": "Bandwidth selection for kernel density estimation with length-biased\n  data", "comments": "35 pages", "journal-ref": null, "doi": "10.1080/10485252.2017.1339309", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Length-biased data are a particular case of weighted data, which arise in\nmany situations: biomedicine, quality control or epidemiology among others. In\nthis paper we study the theoretical properties of kernel density estimation in\nthe context of length-biased data, proposing two consistent bootstrap methods\nthat we use for bandwidth selection. Apart from the bootstrap bandwidth\nselectors we suggest a rule-of-thumb. These bandwidth selection proposals are\ncompared with a least-squares cross-validation method. A simulation study is\naccomplished to understand the behaviour of the procedures in finite samples.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 16:41:21 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 15:39:57 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Borrajo", "Mar\u00eda Isabel", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Mart\u00ednez-Miranda", "Mar\u00eda Dolores", ""]]}, {"id": "1606.05771", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp", "title": "Brief Report on Estimating Regularized Gaussian Networks from Continuous\n  and Ordinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent literature, the Gaussian Graphical model (GGM; Lauritzen, 1996),a\nnetwork of partial correlation coefficients, has been used to capture potential\ndynamic relationships between observed variables. The GGM can be estimated\nusing regularization in combination with model selection using the extended\nBayesian Information Criterion (Foygel and Drton, 2010). I term this\nmethodology GeLasso, and asses its performance using a plausible psychological\nnetwork structure with both continuous and ordinal datasets.Simulation results\nindicate that GeLasso works well as an out-of-the-box method to estimate\nnetwork structures.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 16:02:48 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 04:43:41 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Epskamp", "Sacha", ""]]}, {"id": "1606.05892", "submitter": "David Woods", "authors": "David C. Woods and Antony M. Overstall and Maria Adamou and Timothy W.\n  Waite", "title": "Bayesian design of experiments for generalised linear models and\n  dimensional analysis with industrial and scientific application", "comments": null, "journal-ref": "Quality Engineering, 29, 91-103, 2017", "doi": "10.1080/08982112.2016.12460452", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of an experiment can be always be considered at least implicitly\nBayesian, with prior knowledge used informally to aid decisions such as the\nvariables to be studied and the choice of a plausible relationship between the\nexplanatory variables and measured responses. Bayesian methods allow\nuncertainty in these decisions to be incorporated into design selection through\nprior distributions that encapsulate information available from scientific\nknowledge or previous experimentation. Further, a design may be explicitly\ntailored to the aim of the experiment through a decision-theoretic approach\nusing an appropriate loss function. We review the area of decision-theoretic\nBayesian design, with particular emphasis on recent advances in computational\nmethods. For many problems arising in industry and science, experiments result\nin a discrete response that is well described by a member of the class of\ngeneralised linear models. We describe how Gaussian process emulation, commonly\nused in computer experiments, can play an important role in facilitating\nBayesian design for realistic problems. A main focus is the combination of\nGaussian process regression to approximate the expected loss with cyclic\ndescent (coordinate exchange) optimisation algorithms to allow optimal designs\nto be found for previously infeasible problems. We also present the first\noptimal design results for statistical models formed from dimensional analysis,\na methodology widely employed in the engineering and physical sciences to\nproduce parsimonious and interpretable models. Using the famous paper\nhelicopter experiment, we show the potential for the combination of Bayesian\ndesign, generalised linear models and dimensional analysis to produce small but\ninformative experiments.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 17:34:21 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 13:52:26 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 16:19:47 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Woods", "David C.", ""], ["Overstall", "Antony M.", ""], ["Adamou", "Maria", ""], ["Waite", "Timothy W.", ""]]}, {"id": "1606.05988", "submitter": "Sungkyu Jung", "authors": "Sungkyu Jung", "title": "Continuum directions for supervised dimension reduction", "comments": null, "journal-ref": "Comput. Stat. Data Anal. 125 (2018) 27-43", "doi": "10.1016/j.csda.2018.03.015", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction of multivariate data supervised by auxiliary information\nis considered. A series of basis for dimension reduction is obtained as\nminimizers of a novel criterion. The proposed method is akin to continuum\nregression, and the resulting basis is called continuum directions. With a\npresence of binary supervision data, these directions continuously bridge the\nprincipal component, mean difference and linear discriminant directions, thus\nranging from unsupervised to fully supervised dimension reduction.\nHigh-dimensional asymptotic studies of continuum directions for binary\nsupervision reveal several interesting facts. The conditions under which the\nsample continuum directions are inconsistent, but their classification\nperformance is good, are specified. While the proposed method can be directly\nused for binary and multi-category classification, its generalizations to\nincorporate any form of auxiliary data are also presented. The proposed method\nenjoys fast computation, and the performance is better or on par with more\ncomputer-intensive alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 06:52:41 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 19:43:32 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 17:04:18 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Jung", "Sungkyu", ""]]}, {"id": "1606.06130", "submitter": "Romain Aza\\\"is", "authors": "Romain Aza\\\"is and Alexandre Genadot", "title": "A new characterization of the jump rate for piecewise-deterministic\n  Markov processes with discrete transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise-deterministic Markov processes form a general class of\nnon-diffusion stochastic models that involve both deterministic trajectories\nand random jumps at random times. In this paper, we state a new\ncharacterization of the jump rate of such a process with discrete transitions.\nWe deduce from this result a nonparametric technique for estimating this\nfeature of interest. We state the uniform convergence in probability of the\nestimator. The methodology is illustrated on a numerical example.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:14:46 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 21:02:05 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Aza\u00efs", "Romain", ""], ["Genadot", "Alexandre", ""]]}, {"id": "1606.06246", "submitter": "Richard Samworth", "authors": "Tengyao Wang, Richard J. Samworth", "title": "High-dimensional changepoint estimation via sparse projection", "comments": "59 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changepoints are a very common feature of Big Data that arrive in the form of\na data stream. In this paper, we study high-dimensional time series in which,\nat certain time points, the mean structure changes in a sparse subset of the\ncoordinates. The challenge is to borrow strength across the coordinates in\norder to detect smaller changes than could be observed in any individual\ncomponent series. We propose a two-stage procedure called `inspect' for\nestimation of the changepoints: first, we argue that a good projection\ndirection can be obtained as the leading left singular vector of the matrix\nthat solves a convex optimisation problem derived from the CUSUM transformation\nof the time series. We then apply an existing univariate changepoint estimation\nalgorithm to the projected series. Our theory provides strong guarantees on\nboth the number of estimated changepoints and the rates of convergence of their\nlocations, and our numerical studies validate its highly competitive empirical\nperformance for a wide range of data generating mechanisms. Software\nimplementing the methodology is available in the R package\n`InspectChangepoint'.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:54:28 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 18:29:12 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1606.06328", "submitter": "Ian Barnett", "authors": "Ian Barnett and Jukka-Pekka Onnela", "title": "Inferring Mobility Measures from GPS Traces with Missing Data", "comments": "33 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing availability of smartphones with GPS capabilities,\nlarge-scale studies relating individual-level mobility patterns to a wide\nvariety of patient-centered outcomes, from mood disorders to surgical recovery,\nare becoming a reality. Similar past studies have been small in scale and have\nprovided wearable GPS devices to subjects. These devices typically collect\nmobility traces continuously without significant gaps in the data, and\nconsequently the problem of data missingness has been safely ignored.\nLeveraging subjects' own smartphones makes it possible to scale up and extend\nthe duration of these types of studies, but at the same time introduces a\nsubstantial challenge: to preserve a smartphone's battery, GPS can be active\nonly for a small portion of the time, frequently less than $10\\%$, leading to a\ntremendous missing data problem. We introduce a principled statistical\napproach, based on weighted resampling of the observed data, to impute the\nmissing mobility traces, which we then summarize using different mobility\nmeasures. We compare the strengths of our approach to linear interpolation, a\npopular approach for dealing with missing data, both analytically and through\nsimulation of missingness for empirical data. We conclude that our imputation\napproach better mirrors human mobility both theoretically and over a sample of\nGPS mobility traces from 182 individuals in the Geolife data set, where,\nrelative to linear interpolation, imputation resulted in a 10-fold reduction in\nthe error averaged across all mobility features.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 20:55:05 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 14:36:10 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Barnett", "Ian", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1606.06455", "submitter": "Alan Heavens", "authors": "Alan Heavens", "title": "Generalisations of Fisher Matrices", "comments": "Invited review article for Entropy special issue on 'Applications of\n  Fisher Information in Sciences'. Accepted version", "journal-ref": null, "doi": "10.3390/e18060236", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher matrices play an important role in experimental design and in data\nanalysis. Their primary role is to make predictions for the inference of model\nparameters - both their errors and covariances. In this short review, I outline\na number of extensions to the simple Fisher matrix formalism, covering a number\nof recent developments in the field. These are: (a) situations where the data\n(in the form of (x,y) pairs) have errors in both x and y; (b) modifications to\nparameter inference in the presence of systematic errors, or through fixing the\nvalues of some model parameters; (c) Derivative Approximation for LIkelihoods\n(DALI) - higher-order expansions of the likelihood surface, going beyond the\nGaussian shape approximation; (d) extensions of the Fisher-like formalism, to\ntreat model selection problems with Bayesian evidence.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 07:34:36 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Heavens", "Alan", ""]]}, {"id": "1606.06522", "submitter": "Wagner Hugo Bonat Bonat W. H.", "authors": "Ana Beatriz Tozo Martins and Wagner Hugo Bonat and Paulo Justiniano\n  Ribeiro Junior", "title": "Likelihood analysis for a class of spatial geostatistical compositional\n  models", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2016.06.008", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model-based geostatistical approach to deal with regionalized\ncompositions. We combine the additive-log-ratio transformation with\nmultivariate geostatistical models whose covariance matrix is adapted to take\ninto account the correlation induced by the compositional structure. Such\nspecification allows the usage of standard likelihood methods for parameters\nestimation. For spatial prediction we combined a back-transformation with the\nGauss-Hermite method to approximate the conditional expectation of the\ncompositions. We analyze particle size fractions of the top layer of a soil for\nagronomic purposes which are typically expressed as proportions of sand, clay\nand silt. Additionally a simulation study assess the small sample properties of\nthe maximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 11:27:34 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Martins", "Ana Beatriz Tozo", ""], ["Bonat", "Wagner Hugo", ""], ["Junior", "Paulo Justiniano Ribeiro", ""]]}, {"id": "1606.06645", "submitter": "Henry Lam", "authors": "Henry Lam", "title": "Sensitivity to Serial Dependency of Input Processes: A Robust Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedures in assessing the impact of serial dependency on performance\nanalysis are usually built on parametrically specified models. In this paper,\nwe propose a robust, nonparametric approach to carry out this assessment, by\ncomputing the worst-case deviation of the performance measure due to arbitrary\ndependence. The approach is based on optimizations, posited on the model space,\nthat have constraints specifying the level of dependency measured by a\nnonparametric distance to some nominal i.i.d. input model. We study\napproximation methods for these optimizations via simulation and\nanalysis-of-variance (ANOVA). Numerical experiments demonstrate how the\nproposed approach can discover the hidden impacts of dependency beyond those\nrevealed by conventional parametric modeling and correlation studies.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 16:36:32 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Lam", "Henry", ""]]}, {"id": "1606.06658", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko", "title": "On the Quasi-Stationary Distribution of the Shiryaev-Roberts Diffusion", "comments": "25 pages; 2 figures", "journal-ref": "Sequential Analysis, Vol. 36, No. 1, pp. 126-149, March 2017", "doi": "10.1080/07474946.2016.1275512", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the diffusion $(R_t^r)_{t\\ge0}$ generated by the equation\n$dR_t^r=dt+\\mu R_t^r dB_t$ with $R_0^r\\triangleq r\\ge0$ fixed, and where\n$\\mu\\neq0$ is given, and $(B_t)_{t\\ge0}$ is standard Brownian motion. We assume\nthat $(R_t^r)_{t\\ge0}$ is stopped at\n$\\mathcal{S}_A^r\\triangleq\\inf\\{t\\ge0\\colon R_t^r=A\\}$ with $A>0$ preset, and\nobtain a closed-from formula for the quasi-stationary distribution of\n$(R_t^r)_{t\\ge0}$, i.e., the limit\n$Q_A(x)\\triangleq\\lim_{t\\to+\\infty}\\Pr(R_t^r\\le x|\\mathcal{S}_A^r>t)$,\n$x\\in[0,A]$. Further, we also prove $Q_A(x)$ to be unimodal for any $A>0$, and\nobtain its entire moment series. More importantly, the pair\n$(\\mathcal{S}_A^r,R_t^r)$ with $r\\ge0$ and $A>0$ is the well-known Generalized\nShiryaev-Roberts change-point detection procedure, and its characteristics for\n$r\\sim Q_A(x)$ are of particular interest, especially when $A>0$ is large. In\nview of this circumstance we offer an order-three large-$A$ asymptotic\napproximation of $Q_A(x)$ valid for all $x\\in[0,A]$. The approximation is\nrather accurate even if $A$ is lower than what would be considered \"large\" in\npractice.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 17:00:13 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 17:38:01 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 13:31:42 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Polunchenko", "Aleksey S.", ""]]}, {"id": "1606.06746", "submitter": "Kevin Lin", "authors": "Kevin Lin, James Sharpnack, Alessandro Rinaldo, Ryan J. Tibshirani", "title": "Approximate Recovery in Changepoint Problems, from $\\ell_2$ Estimation\n  Error Rates", "comments": "43 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 1-dimensional multiple changepoint detection problem, we prove that\nany procedure with a fast enough $\\ell_2$ error rate, in terms of its\nestimation of the underlying piecewise constant mean vector, automatically has\nan (approximate) changepoint screening property---specifically, each true jump\nin the underlying mean vector has an estimated jump nearby. We also show, again\nassuming only knowledge of the $\\ell_2$ error rate, that a simple\npost-processing step can be used to eliminate spurious estimated changepoints,\nand thus delivers an (approximate) changepoint recovery\nproperty---specifically, in addition to the screening property described above,\nwe are assured that each estimated jump has a true jump nearby. As a special\ncase, we focus on the application of these results to the 1-dimensional fused\nlasso, i.e., 1-dimensional total variation denoising, and compare the\nimplications with existing results from the literature. We also study\nextensions to related problems, such as changepoint detection over graphs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 20:02:30 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 21:41:22 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Lin", "Kevin", ""], ["Sharpnack", "James", ""], ["Rinaldo", "Alessandro", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1606.06828", "submitter": "Gertraud Malsiner-Walli", "authors": "Gertraud Malsiner-Walli and Sylvia Fr\\\"uhwirth-Schnatter and Bettina\n  Gr\\\"un", "title": "Model-based clustering based on sparse finite Gaussian mixtures", "comments": "22 pages", "journal-ref": "Statistics and Computing (2016) 26:303-324", "doi": "10.1007/s11222-014-9500-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of Bayesian model-based clustering based on a finite mixture\nof Gaussian distributions, we present a joint approach to estimate the number\nof mixture components and identify cluster-relevant variables simultaneously as\nwell as to obtain an identified model. Our approach consists in specifying\nsparse hierarchical priors on the mixture weights and component means. In a\ndeliberately overfitting mixture model the sparse prior on the weights empties\nsuperfluous components during MCMC. A straightforward estimator for the true\nnumber of components is given by the most frequent number of non-empty\ncomponents visited during MCMC sampling. Specifying a shrinkage prior, namely\nthe normal gamma prior, on the component means leads to improved parameter\nestimates as well as identification of cluster-relevant variables. After\nestimating the mixture model using MCMC methods based on data augmentation and\nGibbs sampling, an identified model is obtained by relabeling the MCMC output\nin the point process representation of the draws. This is performed using\n$K$-centroids cluster analysis based on the Mahalanobis distance. We evaluate\nour proposed strategy in a simulation setup with artificial data and by\napplying it to benchmark data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 06:59:15 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Malsiner-Walli", "Gertraud", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Gr\u00fcn", "Bettina", ""]]}, {"id": "1606.06841", "submitter": "Chris Oates", "authors": "Chris. J. Oates, Steven Niederer, Angela Lee, Fran\\c{c}ois-Xavier\n  Briol, Mark Girolami", "title": "Probabilistic Models for Integration Error in the Assessment of\n  Functional Cardiac Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the numerical computation of integrals, representing\nestimates or predictions, over the output $f(x)$ of a computational model with\nrespect to a distribution $p(\\mathrm{d}x)$ over uncertain inputs $x$ to the\nmodel. For the functional cardiac models that motivate this work, neither $f$\nnor $p$ possess a closed-form expression and evaluation of either requires\n$\\approx$ 100 CPU hours, precluding standard numerical integration methods. Our\nproposal is to treat integration as an estimation problem, with a joint model\nfor both the a priori unknown function $f$ and the a priori unknown\ndistribution $p$. The result is a posterior distribution over the integral that\nexplicitly accounts for dual sources of numerical approximation error due to a\nseverely limited computational budget. This construction is applied to account,\nin a statistically principled manner, for the impact of numerical errors that\n(at present) are confounding factors in functional cardiac model assessment.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 08:04:40 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 01:19:43 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 13:13:43 GMT"}, {"version": "v4", "created": "Thu, 15 Jun 2017 09:53:23 GMT"}, {"version": "v5", "created": "Tue, 12 Dec 2017 08:53:38 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Oates", "Chris. J.", ""], ["Niederer", "Steven", ""], ["Lee", "Angela", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Girolami", "Mark", ""]]}, {"id": "1606.06885", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Takahiro Yoshida, Hajime Seya, Daniel A. Griffith,\n  Yoshiki Yamagata", "title": "A Moran coefficient-based mixed effects approach to investigate\n  spatially varying relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops a spatially varying coefficient model by extending the\nrandom effects eigenvector spatial filtering model. The developed model has the\nfollowing properties: its coefficients are interpretable in terms of the Moran\ncoefficient; each of its coefficients can have a different degree of spatial\nsmoothness; and it yields a variant of a Bayesian spatially varying coefficient\nmodel. Also, parameter estimation of the model can be executed with a\nrelatively small computationally burden. Results of a Monte Carlo simulation\nreveal that our model outperforms a conventional eigenvector spatial filtering\n(ESF) model and geographically weighted regression (GWR) models in terms of the\naccuracy of the coefficient estimates and computational time. We empirically\napply our model to the hedonic land price analysis of flood risk in Japan.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 10:37:44 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 07:04:02 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Murakami", "Daisuke", ""], ["Yoshida", "Takahiro", ""], ["Seya", "Hajime", ""], ["Griffith", "Daniel A.", ""], ["Yamagata", "Yoshiki", ""]]}, {"id": "1606.06912", "submitter": "Silvia Montagna", "authors": "Silvia Montagna, Tor Wager, Lisa Feldman-Barrett, Timothy D. Johnson,\n  and Thomas E. Nichols", "title": "Spatial Bayesian Latent Factor Regression Modeling of Coordinate-based\n  Meta-analysis Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now over 20 years old, functional MRI (fMRI) has a large and growing\nliterature that is best synthesised with meta-analytic tools. As most authors\ndo not share image data, only the peak activation coordinates (foci) reported\nin the paper are available for Coordinate-based Meta-analysis (CBMA).\nNeuroimaging meta-analysis is used to 1) identify areas of consistent\nactivation; and 2) build a predictive model of task type or cognitive process\nfor new studies (reverse inference). To simultaneously address these aims, we\npropose a Bayesian point process hierarchical model for CBMA. We model the foci\nfrom each study as a doubly stochastic Poisson process, where the\nstudy-specific log intensity function is characterised as a linear combination\nof a high-dimensional basis set. A sparse representation of the intensities is\nguaranteed through latent factor modeling of the basis coefficients. Within our\nframework, it is also possible to account for the effect of study-level\ncovariates (meta-regression), significantly expanding the capabilities of the\ncurrent neuroimaging meta-analysis methods available. We apply our methodology\nto synthetic data and a neuroimaging meta-analysis dataset.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 11:49:18 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Montagna", "Silvia", ""], ["Wager", "Tor", ""], ["Feldman-Barrett", "Lisa", ""], ["Johnson", "Timothy D.", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "1606.06993", "submitter": "Vitaliy Oryshchenko", "authors": "Vitaliy Oryshchenko", "title": "Exact mean integrated squared error and bandwidth selection for kernel\n  distribution function estimators", "comments": "21 pages, 6 figures, 1 table", "journal-ref": "Comm. Statist. Theory Methods, 49:7 (2020) 1603-1628", "doi": "10.1080/03610926.2018.1563182", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exact, closed form, and easy to compute expression for the mean integrated\nsquared error (MISE) of a kernel estimator of a normal mixture cumulative\ndistribution function is derived for the class of arbitrary order\nGaussian-based kernels. Comparisons are made with MISE of the empirical\ndistribution function, the infeasible minimum MISE of kernel estimators, and\nthe asymptotically optimal second order uniform kernel. The results afford\nstraightforward extensions to other classes of kernel functions and\ndistributions. The analysis also offers a guide on when to use higher order\nkernels in distribution function estimation.\n  A simple plug-in method of simultaneously selecting the optimal bandwidth and\nkernel order is proposed based on a non-asymptotic approximation of the unknown\ndistribution by a normal mixture. A simulation study shows that the method\nworks well in finite samples, thus providing a viable alternative to existing\nbandwidth selection procedures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 15:59:36 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 15:54:45 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 20:55:45 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Oryshchenko", "Vitaliy", ""]]}, {"id": "1606.07153", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick, Rachael Meager, Jonathan Huggins,\n  Michael Jordan", "title": "Fast robustness quantification with variational Bayes", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical models are increasing popular in economics. When using\nhierarchical models, it is useful not only to calculate posterior expectations,\nbut also to measure the robustness of these expectations to reasonable\nalternative prior choices. We use variational Bayes and linear response methods\nto provide fast, accurate posterior means and robustness measures with an\napplication to measuring the effectiveness of microcredit in the developing\nworld.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 01:19:17 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""], ["Meager", "Rachael", ""], ["Huggins", "Jonathan", ""], ["Jordan", "Michael", ""]]}, {"id": "1606.07268", "submitter": "Anru Zhang", "authors": "Anru Zhang and Lawrence D. Brown and T. Tony Cai", "title": "Semi-supervised Inference: General Theory and Estimation of Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a general semi-supervised inference framework focused on the\nestimation of the population mean. As usual in semi-supervised settings, there\nexists an unlabeled sample of covariate vectors and a labeled sample consisting\nof covariate vectors along with real-valued responses (\"labels\"). Otherwise,\nthe formulation is \"assumption-lean\" in that no major conditions are imposed on\nthe statistical or functional form of the data. We consider both the ideal\nsemi-supervised setting where infinitely many unlabeled samples are available,\nas well as the ordinary semi-supervised setting in which only a finite number\nof unlabeled samples is available.\n  Estimators are proposed along with corresponding confidence intervals for the\npopulation mean. Theoretical analysis on both the asymptotic distribution and\n$\\ell_2$-risk for the proposed procedures are given. Surprisingly, the proposed\nestimators, based on a simple form of the least squares method, outperform the\nordinary sample mean. The simple, transparent form of the estimator lends\nconfidence to the perception that its asymptotic improvement over the ordinary\nsample mean also nearly holds even for moderate size samples. The method is\nfurther extended to a nonparametric setting, in which the oracle rate can be\nachieved asymptotically. The proposed estimators are further illustrated by\nsimulation studies and a real data example involving estimation of the homeless\npopulation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 10:53:05 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 01:07:04 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhang", "Anru", ""], ["Brown", "Lawrence D.", ""], ["Cai", "T. Tony", ""]]}, {"id": "1606.07282", "submitter": "Irene C\\'ordoba", "authors": "Irene C\\'ordoba, Concha Bielza and Pedro Larra\\~naga", "title": "A review of Gaussian Markov models for conditional independence", "comments": "Fix author signature", "journal-ref": "Journal of Statistical Planning and Inference, 206:127-144, 2020", "doi": "10.1016/j.jspi.2019.09.008", "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov models lie at the interface between statistical independence in a\nprobability distribution and graph separation properties. We review model\nselection and estimation in directed and undirected Markov models with Gaussian\nparametrization, emphasizing the main similarities and differences. These two\nmodel classes are similar but not equivalent, although they share a common\nintersection. We present the existing results from a historical perspective,\ntaking into account the amount of literature existing from both the artificial\nintelligence and statistics research communities, where these models were\noriginated. We cover classical topics such as maximum likelihood estimation and\nmodel selection via hypothesis testing, but also more modern approaches like\nregularization and Bayesian methods. We also discuss how the Markov models\nreviewed fit in the rich hierarchy of other, higher level Markov model classes.\nFinally, we close the paper overviewing relaxations of the Gaussian assumption\nand pointing out the main areas of application where these Markov models are\nnowadays used.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:12:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 16:27:04 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 11:10:43 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 08:35:01 GMT"}, {"version": "v5", "created": "Wed, 2 Oct 2019 08:33:09 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["C\u00f3rdoba", "Irene", ""], ["Bielza", "Concha", ""], ["Larra\u00f1aga", "Pedro", ""]]}, {"id": "1606.07309", "submitter": "Heiko Sch\\\"utt", "authors": "Heiko H. Sch\\\"utt, Lars Rothkegel, Hans A. Trukenbrod, Sebastian\n  Reich, Felix A. Wichmann, Ralf Engbert", "title": "Likelihood-based Parameter Estimation and Comparison of Dynamical\n  Cognitive Models", "comments": "29 pages, 10 figures, to appear in Psychological Review as a\n  theoretical note", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical models of cognition play an increasingly important role in driving\ntheoretical and experimental research in psychology. Therefore, parameter\nestimation, model analysis and comparison of dynamical models are of essential\nimportance. Here we propose a maximum-likelihood approach for model analysis in\na fully dynamical framework that includes time-ordered experimental data. Our\nmethods can be applied to dynamical models for the prediction of discrete\nbehavior (e.g., movement onsets), in particular, we use a dynamical model of\nsaccade generation in scene viewing as a case study for our approach. For this\nmodel, the likelihood function can be computed directly by numerical\nsimulation, which enables more efficient parameter estimation including\nBayesian inference to obtain reliable estimates and corresponding credible\nintervals. Using hierarchical models inference is even possible for individual\nobservers. Furthermore, our likelihood approach can be used to compare\ndifferent models. In our example, the dynamical framework is shown to\noutperform non-dynamical statistical models. Additionally, the likelihood based\nevaluation differentiates model variants, which produced indistinguishable\npredictions on hitherto used statistics. Our results indicate that the\nlikelihood approach is a promising framework for dynamical cognitive models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 13:35:00 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 15:48:16 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Sch\u00fctt", "Heiko H.", ""], ["Rothkegel", "Lars", ""], ["Trukenbrod", "Hans A.", ""], ["Reich", "Sebastian", ""], ["Wichmann", "Felix A.", ""], ["Engbert", "Ralf", ""]]}, {"id": "1606.07358", "submitter": "Peng Wang", "authors": "Yang Liu and Peng Wang", "title": "Selection by Partitioning the Solution Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of penalized likelihood approaches depends profoundly on the\nselection of the tuning parameter; however, there is no commonly agreed-upon\ncriterion for choosing the tuning parameter. Moreover, penalized likelihood\nestimation based on a single value of the tuning parameter suffers from several\ndrawbacks. This article introduces a novel approach for feature selection based\non the entire solution paths rather than the choice of a single tuning\nparameter, which significantly improves the accuracy of the selection.\nMoreover, the approach allows for feature selection using ridge or other\nstrictly convex penalties. The key idea is to classify variables as relevant or\nirrelevant at each tuning parameter and then to select all of the variables\nwhich have been classified as relevant at least once. We establish the\ntheoretical properties of the method, which requires significantly weaker\nconditions than existing methods in the literature. We also illustrate the\nadvantages of the proposed approach with simulation studies and a data example.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:09:36 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 22:40:33 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Liu", "Yang", ""], ["Wang", "Peng", ""]]}, {"id": "1606.07414", "submitter": "Renato J Cintra", "authors": "T. L. T. Silveira, R. S. Oliveira, F. M. Bayer, R. J. Cintra, A.\n  Madanayake", "title": "Multiplierless 16-point DCT Approximation for Low-complexity Image and\n  Video Coding", "comments": "12 pages, 5 figures, 3 tables", "journal-ref": null, "doi": "10.1007/s11760-016-0923-4", "report-no": null, "categories": "cs.CV cs.MM cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An orthogonal 16-point approximate discrete cosine transform (DCT) is\nintroduced. The proposed transform requires neither multiplications nor\nbit-shifting operations. A fast algorithm based on matrix factorization is\nintroduced, requiring only 44 additions---the lowest arithmetic cost in\nliterature. To assess the introduced transform, computational complexity,\nsimilarity with the exact DCT, and coding performance measures are computed.\nClassical and state-of-the-art 16-point low-complexity transforms were used in\na comparative analysis. In the context of image compression, the proposed\napproximation was evaluated via PSNR and SSIM measurements, attaining the best\ncost-benefit ratio among the competitors. For video encoding, the proposed\napproximation was embedded into a HEVC reference software for direct comparison\nwith the original HEVC standard. Physically realized and tested using FPGA\nhardware, the proposed transform showed 35% and 37% improvements of area-time\nand area-time-squared VLSI metrics when compared to the best competing\ntransform in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 19:26:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Silveira", "T. L. T.", ""], ["Oliveira", "R. S.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Madanayake", "A.", ""]]}, {"id": "1606.07488", "submitter": "Terrance Savitsky", "authors": "Terrance D. Savitsky, Sanvesh Srivastava", "title": "Scalable Bayes under Informative Sampling", "comments": "34 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United States Bureau of Labor Statistics collects data using survey\ninstruments under informative sampling designs that assign probabilities of\ninclusion to be correlated with the response. The bureau extensively uses\nBayesian hierarchical models and posterior sampling to impute missing items in\nrespondent-level data and to infer population parameters. Posterior sampling\nfor survey data collected based on informative designs are computationally\nexpensive and do not support production schedules of the bureau. Motivated by\nthis problem, we propose a new method to scale Bayesian computations in\ninformative sampling designs. Our method divides the data into smaller subsets,\nperforms posterior sampling in parallel for every subset, and combines the\ncollection of posterior samples from all the subsets through their mean in the\nWasserstein space of order 2. Theoretically, we construct conditions on a class\nof sampling designs where posterior consistency of the proposed method is\nachieved. Empirically, we demonstrate that our method is competitive with\ntraditional methods while being significantly faster in many simulations and in\nthe Current Employment Statistics survey conducted by the bureau.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 21:41:20 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 21:56:57 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 18:45:01 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Srivastava", "Sanvesh", ""]]}, {"id": "1606.07610", "submitter": "Mahdi Mahdizadeh", "authors": "M. Mahdizadeh and Ehsan Zamanzade", "title": "Goodness-of-fit testing for the Cauchy distribution with application to\n  financial modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with goodness-of-fit test for the Cauchy distribution.\nSome tests based on Kullback-Leibler information are proposed, and shown to be\nconsistent. Monte Carlo evidence indicates that the tests have satisfactory\nperformances against symmetric alternatives. An empirical application to\nquantitative finance is provided.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 08:56:55 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Mahdizadeh", "M.", ""], ["Zamanzade", "Ehsan", ""]]}, {"id": "1606.07667", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Oli Pall Geirsson, Birgir Hrafnkelsson, Olafur\n  Birgir Davidsson and Sigurdur Magnus Gardarsson", "title": "A Bayesian hierarchical model for monthly maxima of instantaneous flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a comprehensive Bayesian hierarchical model for monthly maxima of\ninstantaneous flow in river catchments. The Gumbel distribution is used as the\nprobabilistic model for the observations, which are assumed to come from\nseveral catchments. Our suggested latent model is Gaussian and designed for\nmonthly maxima, making better use of the data than the standard approach using\nannual maxima. At the latent level, linear mixed models are used for both the\nlocation and scale parameters of the Gumbel distribution, accounting for\nseasonal dependence and covariates from the catchments. The specification of\nprior distributions makes use of penalised complexity (PC) priors, to ensure\nrobust inference for the latent parameters. The main idea behind the PC priors\nis to shrink toward a base model, thus avoiding overfitting. PC priors also\nprovide a convenient framework for prior elicitation based on simple notions of\nscale. Prior distributions for regression coefficients are also elicited based\non hydrological and meteorological knowledge. Posterior inference was done\nusing the MCMC split sampler, an efficient Gibbs blocking scheme tailored to\nlatent Gaussian models. The proposed model was applied to observed data from\neight river catchments in Iceland. A cross-validation study demonstrates good\npredictive performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 12:50:11 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Geirsson", "Oli Pall", ""], ["Hrafnkelsson", "Birgir", ""], ["Davidsson", "Olafur Birgir", ""], ["Gardarsson", "Sigurdur Magnus", ""]]}, {"id": "1606.07926", "submitter": "Rina Foygel Barber", "authors": "Ang Li and Rina Foygel Barber", "title": "Multiple testing with the structure adaptive Benjamini-Hochberg\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple testing problems, where a large number of hypotheses are tested\nsimultaneously, false discovery rate (FDR) control can be achieved with the\nwell-known Benjamini-Hochberg procedure, which adapts to the amount of signal\npresent in the data. Many modifications of this procedure have been proposed to\nimprove power in scenarios where the hypotheses are organized into groups or\ninto a hierarchy, as well as other structured settings. Here we introduce\nSABHA, the \"structure-adaptive Benjamini-Hochberg algorithm\", as a\ngeneralization of these adaptive testing methods. SABHA incorporates prior\ninformation about any pre-determined type of structure in the pattern of\nlocations of the signals and nulls within the list of hypotheses, to reweight\nthe p-values in a data-adaptive way. This raises the power by making more\ndiscoveries in regions where signals appear to be more common. Our main\ntheoretical result proves that SABHA controls FDR at a level that is at most\nslightly higher than the target FDR level, as long as the adaptive weights are\nconstrained sufficiently so as not to overfit too much to the\ndata-interestingly, the excess FDR can be related to the Rademacher complexity\nor Gaussian width of the class from which we choose our data-adaptive weights.\nWe apply this general framework to various structured settings, including\nordered, grouped, and low total variation structures, and get the bounds on FDR\nfor each specific setting. We also examine the empirical performance of SABHA\non fMRI activity data and on gene/drug response data, as well as on simulated\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 16:07:58 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 01:19:00 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 17:03:21 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Li", "Ang", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1606.08046", "submitter": "Eric Lock", "authors": "Tianmeng Lyu, Eric F. Lock, and Lynn E. Eberly", "title": "Discriminating sample groups with multi-way data", "comments": "25 pages, 5 figures, 2 tables", "journal-ref": "Biostatistics 18(3), 434-450, 2017", "doi": "10.1093/biostatistics/kxw057", "report-no": null, "categories": "stat.ME q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional linear classifiers, such as the support vector machine (SVM)\nand distance weighted discrimination (DWD), are commonly used in biomedical\nresearch to distinguish groups of subjects based on a large number of features.\nHowever, their use is limited to applications where a single vector of features\nis measured for each subject. In practice data are often multi-way, or measured\nover multiple dimensions. For example, metabolite abundance may be measured\nover multiple regions or tissues, or gene expression may be measured over\nmultiple time points, for the same subjects. We propose a framework for linear\nclassification of high-dimensional multi-way data, in which coefficients can be\nfactorized into weights that are specific to each dimension. More generally,\nthe coefficients for each measurement in a multi-way dataset are assumed to\nhave low-rank structure. This framework extends existing classification\ntechniques, and we have implemented multi-way versions of SVM and DWD. We\ndescribe informative simulation results, and apply multi-way DWD to data for\ntwo very different clinical research studies. The first study uses metabolite\nmagnetic resonance spectroscopy data over multiple brain regions to compare\npatients with and without spinocerebellar ataxia, the second uses publicly\navailable gene expression time-course data to compare treatment responses for\npatients with multiple sclerosis. Our method improves performance and\nsimplifies interpretation over naive applications of full rank linear\nclassification to multi-way data. An R package is available at\nhttps://github.com/lockEF/MultiwayClassification .\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 15:39:04 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Lyu", "Tianmeng", ""], ["Lock", "Eric F.", ""], ["Eberly", "Lynn E.", ""]]}, {"id": "1606.08052", "submitter": "Fang Liu", "authors": "Fang Liu", "title": "Model-based Differentially Private Data Synthesis and Statistical\n  Inference in Multiply Synthetic Differentially Private Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the approach of model-based differentially private synthesis\n(modips) in the Bayesian framework for releasing individual-level\nsurrogate/synthetic datasets with privacy guarantees given the original data.\nThe modips technique integrates the concept of differential privacy into\nmodel-based data synthesis. We introduce several variants for the general\nmodips approach and different procedures to obtaining privacy-preserving\nposterior samples, a key step in modips. The uncertainty from the sanitization\nand synthetic process in modips can be accounted for by releasing multiple\nsynthetic datasets and quantified via an inferential combination rule that is\nproposed in this paper. We run empirical studies to examine the impacts of the\nnumber of synthetic sets and the privacy budget allocation schemes on the\ninference based on synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 16:31:03 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 08:32:33 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 04:54:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liu", "Fang", ""]]}, {"id": "1606.08106", "submitter": "Michael Evans", "authors": "Luai Al-Labadi and Michael Evans", "title": "Prior based model checking", "comments": "arXiv admin note: text overlap with arXiv:1411.3427 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model checking procedures are considered based on the use of the Dirichlet\nprocess and relative belief. This combination is seen to lead to some unique\nadvantages for this problem. In particular, it avoids double use of the data\nand prior-data conflict. Several examples have been incorporated, in which the\nproposed approach exhibits excellent performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 01:32:13 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Evans", "Michael", ""]]}, {"id": "1606.08162", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "On Keiding's Equation and its relation to differential equations about\n  prevalence and incidence in chronic disease epidemiology", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relation between the age-specific prevalence, incidence and\nmortality in an illness-death model consisting of the three states Healthy,\nIll, Dead. The dependency on three different time scales (age, calendar time,\ndisease duration) is considered. It is shown that Keiding's equation published\nin 1991 is a generalisation of the solution of Brunet and Struchiner's partial\ndifferential equation from 1999. In a special case, we propose a particularly\nsimple estimate of the incidence from prevalence data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 08:47:26 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1606.08199", "submitter": "Guillaume Flandin", "authors": "Guillaume Flandin and Karl J. Friston", "title": "Analysis of family-wise error rates in statistical parametric mapping\n  using random field theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report revisits the analysis of family-wise error rates in\nstatistical parametric mapping - using random field theory - reported in\n(Eklund et al., 2015). Contrary to the understandable spin that these sorts of\nanalyses attract, a review of their results suggests that they endorse the use\nof parametric assumptions - and random field theory - in the analysis of\nfunctional neuroimaging data. We briefly rehearse the advantages parametric\nanalyses offer over nonparametric alternatives and then unpack the implications\nof (Eklund et al., 2015) for parametric procedures.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 10:28:12 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Flandin", "Guillaume", ""], ["Friston", "Karl J.", ""]]}, {"id": "1606.08291", "submitter": "Mike West", "authors": "Lutz F. Gruber and Mike West", "title": "Bayesian forecasting and scalable multivariate volatility analysis using\n  simultaneous graphical dynamic models", "comments": "28 pages, 9 figures, 7 tables", "journal-ref": "Econometrics and Statistics, 2017, Volume 3, pages 3-22", "doi": "10.1016/j.ecosta.2017.03.003", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced class of simultaneous graphical dynamic linear models\n(SGDLMs) defines an ability to scale on-line Bayesian analysis and forecasting\nto higher-dimensional time series. This paper advances the methodology of\nSGDLMs, developing and embedding a novel, adaptive method of simultaneous\npredictor selection in forward filtering for on-line learning and forecasting.\nThe advances include developments in Bayesian computation for scalability, and\na case study in exploring the resulting potential for improved short-term\nforecasting of large-scale volatility matrices. A case study concerns financial\nforecasting and portfolio optimization with a 400-dimensional series of daily\nstock prices. Analysis shows that the SGDLM forecasts volatilities and\nco-volatilities well, making it ideally suited to contributing to quantitative\ninvestment strategies to improve portfolio returns. We also identify\nperformance metrics linked to the sequential Bayesian filtering analysis that\nturn out to define a leading indicator of increased financial market stresses,\ncomparable to but leading the standard St. Louis Fed Financial Stress Index\n(STLFSI) measure. Parallel computation using GPU implementations substantially\nadvance the ability to fit and use these models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:40:23 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Gruber", "Lutz F.", ""], ["West", "Mike", ""]]}, {"id": "1606.08298", "submitter": "David Bolin", "authors": "David Bolin and Jonas Wallin", "title": "Multivariate type G Mat\\'ern stochastic partial differential equation\n  random fields", "comments": null, "journal-ref": "J. Royal Stat. Soc. Series B (2019)", "doi": "10.1111/rssb.12351", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications with multivariate data, random field models capturing\ndepartures from Gaussianity within realisations are appropriate. For this\nreason, we formulate a new class of multivariate non-Gaussian models based on\nsystems of stochastic partial differential equations with additive type G noise\nwhose marginal covariance functions are of Mat\\'ern type. We consider four\nincreasingly flexible constructions of the noise, where the first two are\nsimilar to existing copula-based models. In contrast to these, the latter two\nconstructions can model non-Gaussian spatial data without replicates.\nComputationally efficient methods for likelihood-based parameter estimation and\nprobabilistic prediction are proposed, and the flexibility of the suggested\nmodels is illustrated by numerical examples and two statistical applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:50:12 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 08:36:34 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 12:09:24 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bolin", "David", ""], ["Wallin", "Jonas", ""]]}, {"id": "1606.08337", "submitter": "Mike West", "authors": "Andrew J. Cron and Mike West", "title": "Models of random sparse eigenmatrices matrices and Bayesian analysis of\n  multivariate structure", "comments": "25 pages, 7 figures. 1 table", "journal-ref": "In: Statistical Analysis for High Dimensional Data, (Eds: Frigessi\n  et al), Abel Symposium 11, Springer International Publishing, Switzerland,\n  2016, 123-154", "doi": "10.1007/978-3-319-27099-9_7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss probabilistic models of random covariance structures defined by\ndistributions over sparse eigenmatrices. The decomposition of orthogonal\nmatrices in terms of Givens rotations defines a natural, interpretable\nframework for defining distributions on sparsity structure of random\neigenmatrices. We explore theoretical aspects and implications for conditional\nindependence structures arising in multivariate Gaussian models, and discuss\nconnections with sparse PCA, factor analysis and Gaussian graphical models.\nMethodology includes model-based exploratory data analysis and Bayesian\nanalysis via reversible jump Markov chain Monte Carlo. A simulation study\nexamines the ability to identify sparse multivariate structures compared to the\nbenchmark graphical modelling approach. Extensions to multivariate normal\nmixture models with additional measurement errors move into the framework of\nlatent structure analysis of broad practical interest. We explore the\nimplications and utility of the new models with summaries of a detailed applied\nstudy of a 20-dimensional breast cancer genomics data set.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 16:04:59 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Cron", "Andrew J.", ""], ["West", "Mike", ""]]}, {"id": "1606.08339", "submitter": "Mike West", "authors": "Zoey Yi Zhao, Meng Xie and Mike West", "title": "Dynamic dependence networks: Financial time series forecasting and\n  portfolio decisions (with discussion)", "comments": "31 pages, 9 figures, 3 tables", "journal-ref": "Applied Stochastic Models in Business and Industry, 2016, 32,\n  311-339", "doi": "10.1002/asmb.2161", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Bayesian forecasting of increasingly high-dimensional time series,\na key area of application of stochastic dynamic models in the financial\nindustry and allied areas of business. Novel state-space models characterizing\nsparse patterns of dependence among multiple time series extend existing\nmultivariate volatility models to enable scaling to higher numbers of\nindividual time series. The theory of these \"dynamic dependence network\" models\nshows how the individual series can be \"decoupled\" for sequential analysis, and\nthen \"recoupled\" for applied forecasting and decision analysis. Decoupling\nallows fast, efficient analysis of each of the series in individual univariate\nmodels that are linked-- for later recoupling-- through a theoretical\nmultivariate volatility structure defined by a sparse underlying graphical\nmodel. Computational advances are especially significant in connection with\nmodel uncertainty about the sparsity patterns among series that define this\ngraphical model; Bayesian model averaging using discounting of historical\ninformation builds substantially on this computational advance. An extensive,\ndetailed case study showcases the use of these models, and the improvements in\nforecasting and financial portfolio investment decisions that are achievable.\nUsing a long series of daily international currency, stock indices and\ncommodity prices, the case study includes evaluations of multi-day forecasts\nand Bayesian portfolio analysis with a variety of practical utility functions,\nas well as comparisons against commodity trading advisor benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 16:05:42 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Zhao", "Zoey Yi", ""], ["Xie", "Meng", ""], ["West", "Mike", ""]]}, {"id": "1606.08400", "submitter": "Nicholas Syring", "authors": "Nicholas Syring and Ryan Martin", "title": "Robust and rate-optimal Gibbs posterior inference on the boundary of a\n  noisy image", "comments": "19 pages, 1 figure, 2 tables", "journal-ref": "Annals of Statistics. Volume 48, Number 3 (2020), 1498-1513", "doi": "10.1214/19-AOS1856", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of an image boundary when the pixel intensities are measured with\nnoise is an important problem in image segmentation, with numerous applications\nin medical imaging and engineering. From a statistical point of view, the\nchallenge is that likelihood-based methods require modeling the pixel\nintensities inside and outside the image boundary, even though these are\ntypically of no practical interest. Since misspecification of the pixel\nintensity models can negatively affect inference on the image boundary, it\nwould be desirable to avoid this modeling step altogether. Towards this, we\ndevelop a robust Gibbs approach that constructs a posterior distribution for\nthe image boundary directly, without modeling the pixel intensities. We prove\nthat, for a suitable prior on the image boundary, the Gibbs posterior\nconcentrates asymptotically at the minimax optimal rate, adaptive to the\nboundary smoothness. Monte Carlo computation of the Gibbs posterior is\nstraightforward, and simulation experiments show that the corresponding\ninference is more accurate than that based on existing Bayesian methodology.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 18:31:12 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 02:11:56 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 14:20:17 GMT"}, {"version": "v4", "created": "Fri, 1 Jun 2018 14:32:22 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Syring", "Nicholas", ""], ["Martin", "Ryan", ""]]}, {"id": "1606.08416", "submitter": "Yi-Hui Zhou", "authors": "Yi-Hui Zhou, J.S. Marron, Fred Wright", "title": "Computation of ancestry scores with mixed families and unrelated\n  individuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of robustness to family relationships in computing genotype\nancestry scores such as eigenvector projections has received increased\nattention in genetic association, as the scores are widely used to control\nspurious association. We use a motivational example from the North American\nCystic Fibrosis (CF) Consortium genetic association study with 3444 individuals\nand 898 family members to illustrate the challenge of computing ancestry scores\nwhen sets of both unrelated individuals and closely-related family members are\nincluded. We propose novel methods to obtain ancestry scores and demonstrate\nthat the proposed methods outperform existing methods. The current standard is\nto compute loadings (left singular vectors) using unrelated individuals and to\ncompute projected scores for remaining family members. However, projected\nancestry scores from this approach suffer from shrinkage toward zero. We\nconsider in turn alternate strategies: (i) within-family data\northogonalization, (ii) matrix substitution based on decomposition of a target\nfamily-orthogonalized covariance matrix, (iii) covariance-preserving whitening,\nretaining covariances between unrelated pairs while orthogonalizing family\nmembers, and (iv) using family-averaged data to obtain loadings. Except for\nwithin-family orthogonalization, our proposed approaches offer similar\nperformance and are superior to the standard approaches. We illustrate the\nperformance via simulation and analysis of the CF dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 19:35:01 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Zhou", "Yi-Hui", ""], ["Marron", "J. S.", ""], ["Wright", "Fred", ""]]}, {"id": "1606.08535", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad", "title": "Semiparametric two-component mixture models under L-moments constraints", "comments": "This paper was combined with my other paper on semiparametric mixture\n  models under linear constraints and was accepted for publication in IEEE\n  Transactions on Information Thoery. See published paper for final remarks.\n  arXiv admin note: text overlap with arXiv:1603.05694", "journal-ref": null, "doi": "10.1109/TIT.2017.2786345", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a structure of a semiparametric two-component mixture model when\none component is parametric and the other is defined through L-moments\nconditions. Estimation of a two-component mixture model with an unknown\ncomponent is very difficult when no particular assumption is made on the\nstructure of the unknown component. A previous work was proposed to incorporate\na prior linear information concerning the distribution function of the unknown\ncomponent such as moment constraints. We propose here to incorporate a prior\nlinear information about the quantile function of the unknown component\ninstead. This information is translated by L-moments constraints. L-moments\nhold better information about the tail of the distribution and are considered\nas good alternatives for moments especially for heavy tailed distributions\nsince they can be defined as soon as the distribution has finite expectation.\nThe new semiparametric mixture model is estimated using $\\varphi-$divergences\nwhich permit to build feasible algorithms. Asymptotic properties of the\nresulting estimators are studied and proved under standard assumptions.\nSimulations on data generated by several mixtures models demonstrate the\nviability and the interest of our novel approach and the gain from using\nL-moment constraints in comparison to the use of moments constraints.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 02:00:28 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 07:43:37 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Mohamad", "Diaa Al", ""]]}, {"id": "1606.08650", "submitter": "Axel Finke", "authors": "Axel Finke and Sumeetpal S. Singh", "title": "Approximate Smoothing and Parameter Estimation in High-Dimensional\n  State-Space Models", "comments": "Includes supplementary materials", "journal-ref": "IEEE Transactions on Signal Processing, 65(22), 5982-5994, 2017", "doi": "10.1109/TSP.2017.2733504", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present approximate algorithms for performing smoothing in a class of\nhigh-dimensional state-space models via sequential Monte Carlo methods\n(\"particle filters\"). In high dimensions, a prohibitively large number of Monte\nCarlo samples (\"particles\") -- growing exponentially in the dimension of the\nstate space -- is usually required to obtain a useful smoother. Using blocking\nstrategies as in Rebeschini and Van Handel (2015) (and earlier pioneering work\non blocking), we exploit the spatial ergodicity properties of the model to\ncircumvent this curse of dimensionality. We thus obtain approximate smoothers\nthat can be computed recursively in time and in parallel in space. First, we\nshow that the bias of our blocked smoother is bounded uniformly in the time\nhorizon and in the model dimension. We then approximate the blocked smoother\nwith particles and derive the asymptotic variance of idealised versions of our\nblocked particle smoother to show that variance is no longer adversely effected\nby the dimension of the model. Finally, we employ our method to successfully\nperform maximum-likelihood estimation via stochastic gradient-ascent and\nstochastic expectation--maximisation algorithms in a 100-dimensional\nstate-space model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 11:09:51 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 16:27:15 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 11:55:54 GMT"}, {"version": "v4", "created": "Wed, 20 Sep 2017 15:01:25 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Finke", "Axel", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1606.08759", "submitter": "Mike West", "authors": "Fernando V. Bonassi, Cliburn Chan and Mike West", "title": "Bayesian analysis of immune response dynamics with sparse time series\n  data", "comments": "Main paper: 14 pages, 10 figures. Supplementary material: 16 pages,\n  20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vaccine development, the temporal profiles of relative abundance of\nsubtypes of immune cells (T-cells) is key to understanding vaccine efficacy.\nComplex and expensive experimental studies generate very sparse time series\ndata on this immune response. Fitting multi-parameter dynamic models of the\nimmune response dynamics-- central to evaluating mechanisms underlying vaccine\nefficacy-- is challenged by data sparsity. The research reported here addresses\nthis challenge. For HIV/SIV vaccine studies in macaques, we: (a) introduce\nnovel dynamic models of progression of cellular populations over time with\nrelevant, time-delayed components reflecting the vaccine response; (b) define\nan effective Bayesian model fitting strategy that couples Markov chain Monte\nCarlo (MCMC) with Approximate Bayesian Computation (ABC)-- building on the\ncomplementary strengths of the two approaches, neither of which is effective\nalone; (c) explore questions of information content in the sparse time series\nfor each of the model parameters, linking into experimental design and model\nsimplification for future experiments; and (d) develop, apply and compare the\nanalysis with samples from a recent HIV/SIV experiment, with novel insights and\nconclusions about the progressive response to the vaccine, and how this varies\nacross subjects.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 15:40:17 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Bonassi", "Fernando V.", ""], ["Chan", "Cliburn", ""], ["West", "Mike", ""]]}, {"id": "1606.08861", "submitter": "Jenny Farmer", "authors": "Jenny Farmer and Donald J. Jacobs", "title": "Nonparametric Maximum Entropy Probability Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sample of independent and identically distributed random variables, a\nnovel nonparametric maximum entropy method is presented to estimate the\nunderlying continuous univariate probability density function (pdf). Estimates\nare found by maximizing a log-likelihood function based on single order\nstatistics after transforming through a sequence of trial cumulative\ndistribution functions that iteratively improve using a Monte Carlo random\nsearch method. Improvement is quantified by assessing the random variables\nagainst the statistical properties of sampled uniform random data. Quality is\ndetermined using an empirically derived scoring function that is scaled to be\nsample size invariant. The scoring function identifies atypical fluctuations,\nfor which threshold values are set to define objective criteria that prevent\nunder-fitting as trial iterations continue to improve the model pdf, and,\nstopping the iteration cycle before over-fitting occurs. No prior knowledge\nabout the data is required. An ensemble of pdf models is used to reflect\nuncertainties due to statistical fluctuations in random samples, and the\nquality of the estimates is visualized using scaled residual quantile plots\nthat show deviations from size-invariant statistics. These considerations\nresult in a tractable method that holistically employs key principles of random\nvariables and their statistical properties combined with employing orthogonal\nbasis functions and data-driven adaptive algorithms. Benchmark tests show that\nthe pdf estimates readily converge to the true pdf as sample size increases.\nRobust results are demonstrated on several test probability densities that\ninclude cases with discontinuities, multi-resolution scales, heavy tails and\nsingularities in the pdf, suggesting a generally applicable approach for\nstatistical inference.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 20:01:20 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Farmer", "Jenny", ""], ["Jacobs", "Donald J.", ""]]}, {"id": "1606.08903", "submitter": "Lynn Lin", "authors": "Lin Lin and Jia Li", "title": "Hidden Markov Models on Variable Blocks with a Modal Clustering\n  Algorithm and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by high-throughput single-cell cytometry data with applications to\nvaccine development and immunological research, we consider statistical\nclustering in large-scale data that contain multiple rare clusters. We propose\na new hierarchical mixture model, namely Hidden Markov Model on Variable Blocks\n(HMM-VB), and a new mode search algorithm called Modal Baum-Welch (MBW) for\nefficient clustering. Exploiting the widely accepted chain-like dependence\namong groups of variables in the cytometry data, we propose to treat the\nhierarchy of variable groups as a figurative time line and employ a HMM-type\nmodel, namely HMM-VB. We also propose to use mode-based clustering, aka modal\nclustering, and overcome the exponential computational complexity by MBW. In a\nseries of experiments on simulated data HMM-VB and MBW have better performance\nthan existing methods. We also apply our method to identify rare cell subsets\nin cytometry data and examine its strengths and limitations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 22:20:08 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Lin", "Lin", ""], ["Li", "Jia", ""]]}, {"id": "1606.08925", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Xiaoou Li, Jingchen Liu, Zhiliang Ying", "title": "A Fused Latent and Graphical Model for Multivariate Binary Data", "comments": "49 pages, 6 figures, and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider modeling, inference, and computation for analyzing multivariate\nbinary data. We propose a new model that consists of a low dimensional latent\nvariable component and a sparse graphical component. Our study is motivated by\nanalysis of item response data in cognitive assessment and has applications to\nmany disciplines where item response data are collected. Standard approaches to\nitem response data in cognitive assessment adopt the multidimensional item\nresponse theory (IRT) models. However, human cognition is typically a\ncomplicated process and thus may not be adequately described by just a few\nfactors. Consequently, a low-dimensional latent factor model, such as the\nmultidimensional IRT models, is often insufficient to capture the structure of\nthe data. The proposed model adds a sparse graphical component that captures\nthe remaining ad hoc dependence. It reduces to a multidimensional IRT model\nwhen the graphical component becomes degenerate. Model selection and parameter\nestimation are carried out simultaneously through construction of a\npseudo-likelihood function and properly chosen penalty terms. The convexity of\nthe pseudo-likelihood function allows us to develop an efficient algorithm,\nwhile the penalty terms generate a low-dimensional latent component and a\nsparse graphical structure. Desirable theoretical properties are established\nunder suitable regularity conditions. The method is applied to the revised\nEysenck's personality questionnaire, revealing its usefulness in item analysis.\nSimulation results are reported that show the new method works well in\npractical situations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 00:53:20 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1606.09123", "submitter": "Alexia Kakourou", "authors": "Alexia Kakourou, Werner Vach and Bart Mertens", "title": "Adapting censored regression methods to adjust for the limit of\n  detection in the calibration of diagnostic rules for clinical mass\n  spectrometry proteomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in mass spectrometry (MS), summarizing and\nanalyzing high-throughput mass-spectrometry data remains a challenging task.\nThis is, on the one hand, due to the complexity of the spectral signal which is\nmeasured, and on the other, due to the limit of detection (LOD). The LOD is\nrelated to the limitation of instruments in measuring markers at a relatively\nlow level. As a consequence, the outcome data set from the quantification step\nof proteomic analysis often consists of a reduced list of peaks where any peak\nintensities below the detection limit threshold are reported as missings. In\nthis work, we propose the use of censored data methodology to handle spectral\nmeasurements within the presence of LOD, recognizing that those have been\ncensored due to left-censoring mechanisms on low-abundance proteins. We apply\nthis approach to the particular problem of calibrating prediction rules through\nprior estimation of the average isotope expression in MALDI-FTICR\nmass-spectrometry data, collected in the context of a pancreatic cancer\ncase-control study. Our idea is to replace the set of incomplete spectral\nmeasurements with the average intensity estimates and use those as new input to\na prediction model. We evaluate the proposed methods, with respect to their\npredictive ability, by comparing their performance with the one achieved using\nthe complete information as well as alternative/competitive methods to deal\nwith the LOD.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 14:38:55 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Kakourou", "Alexia", ""], ["Vach", "Werner", ""], ["Mertens", "Bart", ""]]}, {"id": "1606.09308", "submitter": "James Wilson", "authors": "Ross Sparks and James D. Wilson", "title": "Monitoring communication outbreaks among an unknown team of actors in\n  dynamic networks", "comments": "23 pages, 2 figures, Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the detection of communication outbreaks among a\nsmall team of actors in time-varying networks. We propose monitoring plans for\nknown and unknown teams based on generalizations of the exponentially weighted\nmoving average (EWMA) statistic. For unknown teams, we propose an efficient\nneighborhood-based search to estimate a collection of candidate teams. This\nprocedure dramatically reduces the computational complexity of an exhaustive\nsearch. Our procedure consists of two steps: communication counts between\nactors are first smoothed using a multivariate EWMA strategy. Densely connected\nteams are identified as candidates using a neighborhood search approach. These\ncandidate teams are then monitored using a surveillance plan derived from a\ngeneralized EWMA statistic. Monitoring plans are established for collaborative\nteams, teams with a dominant leader, as well as for global outbreaks. We\nconsider weighted heterogeneous dynamic networks, where the expected\ncommunication count between each pair of actors is potentially different across\npairs and time, as well as homogeneous networks, where the expected\ncommunication count is constant across time and actors. Our monitoring plans\nare evaluated on a test bed of simulated networks as well as on the U.S. Senate\nco-voting network, which models the Senate voting patterns from 1857 to 2015.\nOur analysis suggests that our surveillance strategies can efficiently detect\nrelevant and significant changes in dynamic networks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 23:36:10 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Sparks", "Ross", ""], ["Wilson", "James D.", ""]]}, {"id": "1606.09415", "submitter": "Massimiliano Russo", "authors": "Massimiliano Russo, Daniele Durante, Bruno Scarpa", "title": "Bayesian inference on group differences in multivariate categorical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate categorical data are common in many fields. We are motivated by\nelection polls studies assessing evidence of changes in voters opinions with\ntheir candidates preferences in the 2016 United States Presidential primaries\nor caucuses. Similar goals arise routinely in several applications, but current\nliterature lacks a general methodology which combines flexibility, efficiency,\nand tractability in testing for group differences in multivariate categorical\ndata at different---potentially complex---scales. We address this goal by\nleveraging a Bayesian representation which factorizes the joint probability\nmass function for the group variable and the multivariate categorical data as\nthe product of the marginal probabilities for the groups, and the conditional\nprobability mass function of the multivariate categorical data, given the group\nmembership. To enhance flexibility, we define the conditional probability mass\nfunction of the multivariate categorical data via a group-dependent mixture of\ntensor factorizations, thus facilitating dimensionality reduction and borrowing\nof information, while providing tractable procedures for computation, and\naccurate tests assessing global and local group differences. We compare our\nmethods with popular competitors, and discuss improved performance in\nsimulations and in American election polls studies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 10:18:24 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 22:02:44 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 12:26:15 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Russo", "Massimiliano", ""], ["Durante", "Daniele", ""], ["Scarpa", "Bruno", ""]]}, {"id": "1606.09539", "submitter": "Konstantinos Spiliopoulos", "authors": "Jianfeng Lu and Konstantinos Spiliopoulos", "title": "Analysis of multiscale integrators for multiple attractors and\n  irreversible Langevin samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multiscale integrator numerical schemes for a class of stiff\nstochastic differential equations (SDEs). We consider multiscale SDEs with\npotentially multiple attractors that behave as diffusions on graphs as the\nstiffness parameter goes to its limit. Classical numerical discretization\nschemes, such as the Euler-Maruyama scheme, become unstable as the stiffness\nparameter converges to its limit and appropriate multiscale integrators can\ncorrect for this. We rigorously establish the convergence of the numerical\nmethod to the related diffusion on graph, identifying the appropriate choice of\ndiscretization parameters. Theoretical results are supplemented by numerical\nstudies on the problem of the recently developing area of introducing\nirreversibility in Langevin samplers in order to accelerate convergence to\nequilibrium.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 15:31:29 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 00:29:15 GMT"}, {"version": "v3", "created": "Sat, 3 Feb 2018 02:53:42 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 14:24:25 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Lu", "Jianfeng", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1606.09585", "submitter": "Mevin Hooten", "authors": "Mevin B. Hooten, Frances E. Buderman, Brian M. Brost, Ephraim M.\n  Hanks, Jacob S. Ivan", "title": "Hierarchical animal movement models for population-level inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New methods for modeling animal movement based on telemetry data are\ndeveloped regularly. With advances in telemetry capabilities, animal movement\nmodels are becoming increasingly sophisticated. Despite a need for\npopulation-level inference, animal movement models are still predominantly\ndeveloped for individual-level inference. Most efforts to upscale the inference\nto the population-level are either post hoc or complicated enough that only the\ndeveloper can implement the model. Hierarchical Bayesian models provide an\nideal platform for the development of population-level animal movement models\nbut can be challenging to fit due to computational limitations or extensive\ntuning required. We propose a two-stage procedure for fitting hierarchical\nanimal movement models to telemetry data. The two-stage approach is\nstatistically rigorous and allows one to fit individual-level movement models\nseparately, then resample them using a secondary MCMC algorithm. The primary\nadvantages of the two-stage approach are that the first stage is easily\nparallelizable and the second stage is completely unsupervised, allowing for a\ncompletely automated fitting procedure in many cases. We demonstrate the\ntwo-stage procedure with two applications of animal movement models. The first\napplication involves a spatial point process approach to modeling telemetry\ndata and the second involves a more complicated continuous-time discrete-space\nanimal movement model. We fit these models to simulated data and real telemetry\ndata arising from a population of monitored Canada lynx in Colorado, USA.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 17:39:18 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Hooten", "Mevin B.", ""], ["Buderman", "Frances E.", ""], ["Brost", "Brian M.", ""], ["Hanks", "Ephraim M.", ""], ["Ivan", "Jacob S.", ""]]}]