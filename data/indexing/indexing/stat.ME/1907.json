[{"id": "1907.00004", "submitter": "Junmo Song", "authors": "Junmo Song and Jiwon Kang", "title": "Test for parameter change in the presence of outliers: the density power\n  divergence based approach", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the problem of testing for a parameter change in the\npresence of outliers. For this, we propose a robust test using the objective\nfunction of minimum density power divergence estimator (MDPDE) by Basu et al.\n(Biometrika, 1998), and then derive its limiting null distribution. Our test\nprocedure can be naturally extended to any parametric model to which MDPDE can\nbe applied. To illustrate this, we apply our test procedure to GARCH models. We\ndemonstrate the validity and robustness of the proposed test through a\nsimulation study. In a real data application to the Hang Seng index, our test\nlocates some change-points that are not detected by the previous tests such as\nthe score test and the residual-based CUSUM test.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:39:56 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 02:25:11 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 14:59:11 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Song", "Junmo", ""], ["Kang", "Jiwon", ""]]}, {"id": "1907.00085", "submitter": "Guenther Walther", "authors": "Jiyao Kou and Guenther Walther", "title": "Large-scale inference with block structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of weak and rare effects in large amounts of data arises in a\nnumber of modern data analysis problems. Known results show that in this\nsituation the potential of statistical inference is severely limited by the\nlarge-scale multiple testing that is inherent in these problems. Here we show\nthat fundamentally more powerful statistical inference is possible when there\nis some structure in the signal that can be exploited, e.g. if the signal is\nclustered in many small blocks, as is the case in some relevant applications.\nWe derive the detection boundary in such a situation where we allow both the\nnumber of blocks and the block length to grow polynomially with sample size. We\nderive these results both for the univariate and the multivariate settings as\nwell as for the problem of detecting clusters in a network. These results\nrecover as special cases the heterogeneous mixture detection problem [1] where\nthere is no structure in the signal, as well as scan problem [2] where the\nsignal comprises a single interval. We develop methodology that allows optimal\nadaptive detection in the general setting, thus exploiting the structure if it\nis present without incurring a relevant penalty in the case where there is no\nstructure. The advantage of this methodology can be considerable, as in the\ncase of no structure the means need to increase at the rate $\\sqrt{\\log n}$ to\nensure detection, while the presence of structure allows detection even if the\nmeans \\emph{decrease} at a polynomial rate.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 21:22:13 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kou", "Jiyao", ""], ["Walther", "Guenther", ""]]}, {"id": "1907.00113", "submitter": "Ziwei Zhu", "authors": "Ziwei Zhu, Xudong Li, Mengdi Wang, Anru Zhang", "title": "Learning Markov models via low-rank optimization", "comments": "52 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1804.00795", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling unknown systems from data is a precursor of system optimization and\nsequential decision making. In this paper, we focus on learning a Markov model\nfrom a single trajectory of states. Suppose that the transition model has a\nsmall rank despite of having a large state space, meaning that the system\nadmits a low-dimensional latent structure. We show that one can estimate the\nfull transition model accurately using a trajectory of length that is\nproportional to the total number of states. We propose two maximum likelihood\nestimation methods: a convex approach with nuclear-norm regularization and a\nnonconvex approach with rank constraint. We explicitly derive the statistical\nrates of both estimators in terms of the Kullback-Leiber divergence and the\n$\\ell_2$ error and also establish a minimax lower bound to assess the tightness\nof these rates. For computing the nonconvex estimator, we develop a novel DC\n(difference of convex function) programming algorithm that starts with the\nconvex M-estimator and then successively refines the solution till convergence.\nEmpirical experiments demonstrate consistent superiority of the nonconvex\nestimator over the convex one.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:58:26 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 05:52:34 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhu", "Ziwei", ""], ["Li", "Xudong", ""], ["Wang", "Mengdi", ""], ["Zhang", "Anru", ""]]}, {"id": "1907.00161", "submitter": "Kristian Brock", "authors": "Kristian Brock", "title": "trialr: Bayesian Clinical Trial Designs in R and Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript introduces an \\proglang{R} package called \\pkg{trialr} that\nimplements a collection of clinical trial methods in \\proglang{Stan} and\n\\proglang{R}. In this article, we explore three methods in detail. The first is\nthe continual reassessment method for conducting phase I dose-finding trials\nthat seek a maximum tolerable dose. The second is EffTox, a dose-finding design\nthat scrutinises doses by joint efficacy and toxicity outcomes. The third is\nthe augmented binary method for modelling the probability of treatment success\nin phase II oncology trials with reference to repeated measures of continuous\ntumour size and binary indicators of treatment failure. We emphasise in this\narticle the benefits that stem from having access to posterior samples,\nincluding flexible inference and powerful visualisation. We hope that this\npackage encourages the use of Bayesian methods in clinical trials.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 07:33:20 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Brock", "Kristian", ""]]}, {"id": "1907.00287", "submitter": "Jelena Bradic", "authors": "Jue Hou and Jelena Bradic and Ronghui Xu", "title": "Estimating Treatment Effect under Additive Hazards Models with\n  High-dimensional Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal effects for survival outcomes in the high-dimensional\nsetting is an extremely important topic for many biomedical applications as\nwell as areas of social sciences. We propose a new orthogonal score method for\ntreatment effect estimation and inference that results in asymptotically valid\nconfidence intervals assuming only good estimation properties of the hazard\noutcome model and the conditional probability of treatment. This guarantee\nallows us to provide valid inference for the conditional treatment effect under\nthe high-dimensional additive hazards model under considerably more generality\nthan existing approaches. In addition, we develop a new Hazards Difference\n(HDi), estimator. We showcase that our approach has double-robustness\nproperties in high dimensions: with cross-fitting, the HDi estimate is\nconsistent under a wide variety of treatment assignment models; the HDi\nestimate is also consistent when the hazards model is misspecified and instead\nthe true data generating mechanism follows a partially linear additive hazards\nmodel. We further develop a novel sparsity doubly robust result, where either\nthe outcome or the treatment model can be a fully dense high-dimensional model.\nWe apply our methods to study the treatment effect of radical prostatectomy\nversus conservative management for prostate cancer patients using the\nSEER-Medicare Linked Data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 22:15:59 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Hou", "Jue", ""], ["Bradic", "Jelena", ""], ["Xu", "Ronghui", ""]]}, {"id": "1907.00307", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Wei Zhang, Junyi Zuo and Heping Wang", "title": "Outlier-robust Kalman filters with mixture correntropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the robust filtering problem for a nonlinear state-space model\nwith outliers in measurements. To improve the robustness of the traditional\nKalman filtering algorithm, we propose in this work two robust filters based on\nmixture correntropy, especially the double-Gaussian mixture correntropy and\nLaplace-Gaussian mixture correntropy. We have formulated the robust filtering\nproblem by adopting the mixture correntropy induced cost to replace the\nquadratic one in the conventional Kalman filter for measurement fitting errors.\nIn addition, a tradeoff weight coefficient is introduced to make sure the\nproposed approaches can provide reasonable state estimates in scenarios where\nmeasurement fitting errors are small. The formulated robust filtering problems\nare iteratively solved by utilizing the cubature Kalman filtering framework\nwith a reweighted measurement covariance. Numerical results show that the\nproposed methods can achieve a performance improvement over existing robust\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 02:52:50 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 02:12:50 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhang", "Wei", ""], ["Zuo", "Junyi", ""], ["Wang", "Heping", ""]]}, {"id": "1907.00345", "submitter": "Hisashi Noma", "authors": "Yuta Hamaguchi, Hisashi Noma, Kengo Nagashima, Tomohide Yamada and\n  Toshi A. Furukawa", "title": "Frequentist performances of Bayesian prediction intervals for\n  random-effects meta-analysis", "comments": "23 pages, 4 figures, 1 table", "journal-ref": "Biom J 2021;63(2):394-405", "doi": "10.1002/bimj.201900351", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction interval has been increasingly used in meta-analyses as a\nuseful measure for assessing the magnitude of treatment effect and\nbetween-studies heterogeneity. In calculations of the prediction interval,\nalthough the Higgins-Thompson-Spiegelhalter method is used most often in\npractice, it might not have adequate coverage probability for the true\ntreatment effect of a future study under realistic situations. An effective\nalternative candidate is the Bayesian prediction interval, which has also been\nwidely used in general prediction problems. However, these prediction intervals\nare constructed based on the Bayesian philosophy, and their frequentist\nvalidities are only justified by large-sample approximations even if\nnon-informative priors are adopted. There has been no certain evidence that\nevaluated their frequentist performances under realistic situations of\nmeta-analyses. In this study, we conducted extensive simulation studies to\nassess the frequentist coverage performances of Bayesian prediction intervals\nwith 11 non-informative prior distributions under general meta-analysis\nsettings. Through these simulation studies, we found that frequentist coverage\nperformances strongly depended on what prior distributions were adopted. In\naddition, when the number of studies was smaller than 10, there were no prior\ndistributions that retained accurate frequentist coverage properties. We also\nillustrated these methods via applications to eight real meta-analysis\ndatasets. The resultant prediction intervals also differed according to the\nadopted prior distributions. Inaccurate prediction intervals may provide\ninvalid evidence and misleading conclusions. Thus, if frequentist accuracy is\nrequired, Bayesian prediction intervals should be used cautiously in practice.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 09:38:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Hamaguchi", "Yuta", ""], ["Noma", "Hisashi", ""], ["Nagashima", "Kengo", ""], ["Yamada", "Tomohide", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "1907.00389", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Ricardo Baptista, Youssef Marzouk", "title": "Coupling techniques for nonlinear ensemble filtering", "comments": "43 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider filtering in high-dimensional non-Gaussian state-space models\nwith intractable transition kernels, nonlinear and possibly chaotic dynamics,\nand sparse observations in space and time. We propose a novel filtering\nmethodology that harnesses transportation of measures, convex optimization, and\nideas from probabilistic graphical models to yield robust ensemble\napproximations of the filtering distribution in high dimensions. Our approach\ncan be understood as the natural generalization of the ensemble Kalman filter\n(EnKF) to nonlinear updates, using stochastic or deterministic couplings. The\nuse of nonlinear updates can reduce the intrinsic bias of the EnKF at a\nmarginal increase in computational cost. We avoid any form of importance\nsampling and introduce non-Gaussian localization approaches for dimension\nscalability. Our framework achieves state-of-the-art tracking performance on\nchallenging configurations of the Lorenz-96 model in the chaotic regime.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 14:51:05 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Spantini", "Alessio", ""], ["Baptista", "Ricardo", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1907.00519", "submitter": "Sanjay Kumar", "authors": "Sanjay Kumar and Nirmal Tiwari", "title": "Transformed Naive Ratio and Product Based Estimators for Estimating\n  Population Mode in Simple Random Sampling", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a transformed na\\\"ive ratio and product based\nestimators using the characterizing scalar in presence of auxiliary information\nof the study variable for estimating the population mode following simple\nrandom sampling without replacement. The bias, mean square errors, relative\nefficiency, ratios of the exact values of mean square errors to the simulated\nmean square errors and confidence interval are studied for the performance of\nthe proposed transformed na\\\"ive ratio type estimator with the certain natural\npopulation as well as artificially generated data sets. We have shown that\nproposed transformed na\\\"ive ratio based estimator is more efficient than the\nna\\\"ive estimator and na\\\"ive ratio estimator of the population mode.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 02:56:18 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kumar", "Sanjay", ""], ["Tiwari", "Nirmal", ""]]}, {"id": "1907.00786", "submitter": "Willi Sauerbrei", "authors": "Willi Sauerbrei, Aris Perperoglou, Matthias Schmid, Michal\n  Abrahamowicz, Heiko Becher, Harald Binder, Daniela Dunkler, Frank E. Harrell\n  Jr, Patrick Royston, Georg Heinze (for TG2 of the STRATOS initiative)", "title": "State-of-the-art in selection of variables and functional forms in\n  multivariable analysis -- outstanding issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to select variables and identify functional forms for continuous\nvariables is a key concern when creating a multivariable model. Ad hoc\n'traditional' approaches to variable selection have been in use for at least 50\nyears. Similarly, methods for determining functional forms for continuous\nvariables were first suggested many years ago. More recently, many alternative\napproaches to address these two challenges have been proposed, but knowledge of\ntheir properties and meaningful comparisons between them are scarce. To define\na state-of-the-art and to provide evidence-supported guidance to researchers\nwho have only a basic level of statistical knowledge many outstanding issues in\nmultivariable modelling remain. Our main aims are to identify and illustrate\nsuch gaps in the literature and present them at a moderate technical level to\nthe wide community of practitioners, researchers and students of statistics. We\nbriefly discuss general issues in building descriptive regression models,\nstrategies for variable selection, different ways of choosing functional forms\nfor continuous variables, and methods for combining the selection of variables\nand functions. We discuss two examples, taken from the medical literature, to\nillustrate problems in the practice of modelling. Our overview revealed that\nthere is not yet enough evidence on which to base recommendations for the\nselection of variables and functional forms in multivariable analysis. Such\nevidence may come from comparisons between alternative methods. In particular,\nwe highlight seven important topics that require further investigation and make\nsuggestions for the direction of further research.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 13:53:18 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Sauerbrei", "Willi", "", "for TG2 of the STRATOS initiative"], ["Perperoglou", "Aris", "", "for TG2 of the STRATOS initiative"], ["Schmid", "Matthias", "", "for TG2 of the STRATOS initiative"], ["Abrahamowicz", "Michal", "", "for TG2 of the STRATOS initiative"], ["Becher", "Heiko", "", "for TG2 of the STRATOS initiative"], ["Binder", "Harald", "", "for TG2 of the STRATOS initiative"], ["Dunkler", "Daniela", "", "for TG2 of the STRATOS initiative"], ["Harrell", "Frank E.", "Jr", "for TG2 of the STRATOS initiative"], ["Royston", "Patrick", "", "for TG2 of the STRATOS initiative"], ["Heinze", "Georg", "", "for TG2 of the STRATOS initiative"]]}, {"id": "1907.01049", "submitter": "Andreas Hagemann", "authors": "Andreas Hagemann", "title": "Permutation inference with a finite number of heterogeneous clusters", "comments": "24 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce a simple permutation procedure to test conventional (non-sharp)\nhypotheses about the effect of a binary treatment in the presence of a finite\nnumber of large, heterogeneous clusters when the treatment effect is identified\nby comparisons across clusters. The procedure asymptotically controls size by\napplying a level-adjusted permutation test to a suitable statistic. The\nadjustments needed for most empirically relevant situations are tabulated in\nthe paper. The adjusted permutation test is easy to implement in practice and\nperforms well at conventional levels of significance with at least four treated\nclusters and a similar number of control clusters. It is particularly robust to\nsituations where some clusters are much more variable than others. Examples and\nan empirical application are provided.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 20:15:04 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Hagemann", "Andreas", ""]]}, {"id": "1907.01136", "submitter": "Paul McNicholas", "authors": "Katharine M. Clark and Paul D. McNicholas", "title": "Using Subset Log-Likelihoods to Trim Outliers in Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of Gaussian distributions are a popular choice in model-based\nclustering. Outliers can affect parameters estimation and, as such, must be\naccounted for. Predicting the proportion of outliers correctly is paramount as\nit minimizes misclassification error. It is proved that, for a finite Gaussian\nmixture model, the log-likelihoods of the subset models are distributed\naccording to a mixture of beta distributions. An algorithm is then proposed\nthat predicts the proportion of outliers by measuring the adherence of a set of\nsubset log-likelihoods to a beta mixture reference distribution. This algorithm\nremoves the least likely points, which are deemed outliers, until model\nassumptions are met.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 03:02:20 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 19:44:17 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 01:11:26 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Clark", "Katharine M.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1907.01170", "submitter": "Anwesha Bhattacharyya", "authors": "Anwesha Bhattacharyya and Yves Atchade", "title": "Bayesian Analysis of High-dimensional Discrete Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a Bayesian methodology for fitting large discrete\ngraphical models with spike-and-slab priors to encode sparsity. We consider a\nquasi-likelihood approach that enables node-wise parallel computation resulting\nin reduced computational complexity. We introduce a scalable Langevin MCMC\nalgorithm for sampling from the quasi-posterior distribution which enables\nvariable selection and estimation simultaneously. We present extensive\nsimulation results to demonstrate scalability and accuracy of the method. We\nalso analyze the 16 Personality Factors (PF) dataset to illustrate performance\nof the method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 05:10:15 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 20:41:42 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Bhattacharyya", "Anwesha", ""], ["Atchade", "Yves", ""]]}, {"id": "1907.01175", "submitter": "Xinyu Song", "authors": "Xinyu Song, Donggyu Kim, Huiling Yuan, Xiangyu Cui, Zhiping Lu, Yong\n  Zhou, Yazhen Wang", "title": "Volatility Analysis with Realized GARCH-Ito Models", "comments": "39 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a unified approach for modeling high-frequency\nfinancial data that can accommodate both the continuous-time jump-diffusion and\ndiscrete-time realized GARCH model by embedding the discrete realized GARCH\nstructure in the continuous instantaneous volatility process. The key feature\nof the proposed model is that the corresponding conditional daily integrated\nvolatility adopts an autoregressive structure where both integrated volatility\nand jump variation serve as innovations. We name it as the realized GARCH-Ito\nmodel. Given the autoregressive structure in the conditional daily integrated\nvolatility, we propose a quasi-likelihood function for parameter estimation and\nestablish its asymptotic properties. To improve the parameter estimation, we\npropose a joint quasi-likelihood function that is built on the marriage of\ndaily integrated volatility estimated by high-frequency data and nonparametric\nvolatility estimator obtained from option data. We conduct a simulation study\nto check the finite sample performance of the proposed methodologies and an\nempirical study with the S&P500 stock index and option data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 05:26:36 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:26:35 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Song", "Xinyu", ""], ["Kim", "Donggyu", ""], ["Yuan", "Huiling", ""], ["Cui", "Xiangyu", ""], ["Lu", "Zhiping", ""], ["Zhou", "Yong", ""], ["Wang", "Yazhen", ""]]}, {"id": "1907.01181", "submitter": "Sonja Surjanovic", "authors": "Sonja Surjanovic and William J. Welch", "title": "Adaptive Partitioning Design and Analysis for Emulation of a Complex\n  Computer Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models are used as replacements for physical experiments in a large\nvariety of applications. Nevertheless, direct use of the computer model for the\nultimate scientific objective is often limited by the complexity and cost of\nthe model. Historically, Gaussian process regression has proven to be the\nalmost ubiquitous choice for a fast statistical emulator for such a computer\nmodel, due to its flexible form and analytical expressions for measures of\npredictive uncertainty. However, even this statistical emulator can be\ncomputationally intractable for large designs, due to computing time increasing\nwith the cube of the design size. Multiple methods have been proposed for\naddressing this problem. We discuss several of them, and compare their\npredictive and computational performance in several scenarios.\n  We then propose solving this problem using an adaptive partitioning emulator\n(APE). The new approach is motivated by the idea that most computer models are\nonly complex in particular regions of the input space. By taking a\ndata-adaptive approach to the development of a design, and choosing to\npartition the space in the regions of highest variability, we obtain a higher\ndensity of points in these regions and hence accurate prediction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 05:51:12 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Surjanovic", "Sonja", ""], ["Welch", "William J.", ""]]}, {"id": "1907.01333", "submitter": "Shonosuke Sugasawa", "authors": "Yasuyuki Hamura, Kaoru Irie and Shonosuke Sugasawa", "title": "On Global-local Shrinkage Priors for Count Data", "comments": "28 pages (main text) + 14 pages (supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global-local shrinkage prior has been recognized as useful class of priors\nwhich can strongly shrink small signals towards prior means while keeping large\nsignals unshrunk. Although such priors have been extensively discussed under\nGaussian responses, we intensively encounter count responses in practice in\nwhich the previous knowledge of global-local shrinkage priors cannot be\ndirectly imported. In this paper, we discuss global-local shrinkage priors for\nanalyzing sequence of counts. We provide sufficient conditions under which the\nposterior mean keeps the observation as it is for very large signals, known as\ntail robustness property. Then, we propose tractable priors to meet the derived\nconditions approximately or exactly and develop an efficient posterior\ncomputation algorithm for Bayesian inference. The proposed methods are free\nfrom tuning parameters, that is, all the hyperparameters are automatically\nestimated based on the data. We demonstrate the proposed methods through\nsimulation and an application to a real dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 12:55:51 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 04:01:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Hamura", "Yasuyuki", ""], ["Irie", "Kaoru", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "1907.01458", "submitter": "Uri Keich", "authors": "Kristen Emery, Syamand Hasam, William Stafford Noble, Uri Keich", "title": "Multiple competition-based FDR control for peptide detection", "comments": "Numerous changes from the initial submission including an expanded\n  section on peptide detection (context/motivation and results), refocused and\n  streamlined methods development section, revised and more selective figures\n  reflecting the most recent analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competition-based FDR control has been commonly used for over a decade in the\ncomputational mass spectrometry community (Elias and Gygi, 2007). Recently, the\napproach has gained significant popularity in other fields after Barber and\nCandes (2015) laid its theoretical foundation in a more general setting that\nincluded the feature selection problem. In both cases, the competition is based\non a head-to-head comparison between an observed score and a corresponding\ndecoy / knockoff. Keich and Noble (2017b) recently demonstrated some advantages\nof using multiple rather than a single decoy when addressing the problem of\nassigning peptide sequences to observed mass spectra. In this work, we consider\na related problem -- detecting peptides based on a collection of mass spectra\n-- and we develop a new framework for competition-based FDR control using\nmultiple null scores. Within this framework, we offer several methods, all of\nwhich are based on a novel procedure that rigorously controls the FDR in the\nfinite sample setting. Using real data to study the peptide detection problem\nwe show that, relative to existing single-decoy methods, our approach can\nincrease the number of discovered peptides by up to 50% at small FDR\nthresholds.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:39:57 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 10:32:44 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Emery", "Kristen", ""], ["Hasam", "Syamand", ""], ["Noble", "William Stafford", ""], ["Keich", "Uri", ""]]}, {"id": "1907.01505", "submitter": "Umberto Simola Mr.", "authors": "Umberto Simola, Jessica Cisewski-Kehe, Michael U. Gutmann, Jukka\n  Corander", "title": "Adaptive Approximate Bayesian Computation Tolerance Selection", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) methods are increasingly used for\ninference in situations in which the likelihood function is either\ncomputationally costly or intractable to evaluate. Extensions of the basic ABC\nrejection algorithm have improved the computational efficiency of the procedure\nand broadened its applicability. The ABC-Population Monte Carlo (ABC-PMC)\napproach of Beaumont et al. (2009) has become a popular choice for approximate\nsampling from the posterior. ABC-PMC is a sequential sampler with an\niteratively decreasing value of the tolerance, which specifies how close the\nsimulated data need to be to the real data for acceptance. We propose a method\nfor adaptively selecting a sequence of tolerances that improves the\ncomputational efficiency of the algorithm over other common techniques. In\naddition we define a stopping rule as a by-product of the adaptation procedure,\nwhich assists in automating termination of sampling. The proposed automatic\nABC-PMC algorithm can be easily implemented and we present several examples\ndemonstrating its benefits in terms of computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 16:02:58 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 07:47:45 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Simola", "Umberto", ""], ["Cisewski-Kehe", "Jessica", ""], ["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""]]}, {"id": "1907.01511", "submitter": "Kevin Burke Dr", "authors": "Fatima-Zahra Jaouimaa, Il Do Ha, Kevin Burke", "title": "Penalized Variable Selection in Multi-Parameter Regression Survival\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-parameter regression (MPR) modelling refers to the approach whereby\ncovariates are allowed to enter the model through multiple distributional\nparameters simultaneously. This is in contrast to the standard approaches where\ncovariates enter through a single parameter (e.g., a location parameter).\nPenalized variable selection has received a significant amount of attention in\nrecent years: methods such as the least absolute shrinkage and selection\noperator (LASSO), smoothly clipped absolute deviation (SCAD), and adaptive\nLASSO are used to simultaneously select variables and estimate their regression\ncoefficients. Therefore, in this paper, we develop penalized multi-parameter\nregression methods and investigate their associated performance through\nsimulation studies and real data; as an example, we consider the Weibull MPR\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 17:22:41 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Jaouimaa", "Fatima-Zahra", ""], ["Ha", "Il Do", ""], ["Burke", "Kevin", ""]]}, {"id": "1907.01670", "submitter": "Linjun Zhang", "authors": "Xianli Zeng, Yingcun Xia, Linjun Zhang", "title": "Double Cross Validation for the Number of Factors in Approximate Factor\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the number of factors is essential to factor analysis. In this\npaper, we propose {an efficient cross validation (CV)} method to determine the\nnumber of factors in approximate factor models. The method applies CV twice,\nfirst along the directions of observations and then variables, and hence is\nreferred to hereafter as double cross-validation (DCV). Unlike most CV methods,\nwhich are prone to overfitting, the DCV is statistically consistent in\ndetermining the number of factors when both dimension of variables and sample\nsize are sufficiently large. Simulation studies show that DCV has outstanding\nperformance in comparison to existing methods in selecting the number of\nfactors, especially when the idiosyncratic error has heteroscedasticity, or\nheavy tail, or relatively large variance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 22:33:58 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zeng", "Xianli", ""], ["Xia", "Yingcun", ""], ["Zhang", "Linjun", ""]]}, {"id": "1907.01736", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al-Labadi, Forough Fazeli Asl and Zahra Saberi", "title": "A Bayesian Semiparametric Gaussian Copula Approach to a Multivariate\n  Normality Test", "comments": "30 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Bayesian semiparametric copula approach is used to model the\nunderlying multivariate distribution $F_{true}$. First, the Dirichlet process\nis constructed on the unknown marginal distributions of $F_{true}$. Then a\nGaussian copula model is utilized to capture the dependence structure of\n$F_{true}$. As a result, a Bayesian multivariate normality test is developed by\ncombining the relative belief ratio and the Energy distance. Several\ninteresting theoretical results of the approach are derived. Finally, through\nseveral simulated examples and a real data set, the proposed approach reveals\nexcellent performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 04:45:54 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 04:50:09 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Asl", "Forough Fazeli", ""], ["Saberi", "Zahra", ""]]}, {"id": "1907.01938", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang and Michael P. B. Gallaugher and Ryan P. Browne and\n  Paul D. McNicholas", "title": "Model-based clustering and classification using mixtures of multivariate\n  skewed power exponential distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Families of mixtures of multivariate power exponential (MPE) distributions\nhave been previously introduced and shown to be competitive for cluster\nanalysis in comparison to other elliptical mixtures including mixtures of\nGaussian distributions. Herein, we propose a family of mixtures of multivariate\nskewed power exponential distributions to combine the flexibility of the MPE\ndistribution with the ability to model skewness. These mixtures are more robust\nto variations from normality and can account for skewness, varying tail weight,\nand peakedness of data. A generalized expectation-maximization approach\ncombining minorization-maximization and optimization based on accelerated line\nsearch algorithms on the Stiefel manifold is used for parameter estimation.\nThese mixtures are implemented both in the model-based clustering and\nclassification frameworks. Both simulated and benchmark data are used for\nillustration and comparison to other mixture families.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:38:47 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:34:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["Gallaugher", "Michael P. B.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1907.01943", "submitter": "Zach Branson", "authors": "Zach Branson and Luke Keele", "title": "Evaluating A Key Instrumental Variable Assumption Using Randomization\n  Tests", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable (IV) analyses are becoming common in health services\nresearch and epidemiology. Most IV analyses use naturally occurring\ninstruments, such as distance to a hospital. In these analyses, investigators\nmust assume the instrument is as-if randomly assigned. This assumption cannot\nbe tested directly, but it can be falsified. Most falsification tests in the\nliterature compare relative prevalence or bias in observed covariates between\nthe instrument and the exposure. These tests require investigators to make a\ncovariate-by-covariate judgment about the validity of the IV design. Often,\nonly some of the covariates are well-balanced, making it unclear if as-if\nrandomization can be assumed for the instrument across all covariates. We\npropose an alternative falsification test that compares IV balance or bias to\nthe balance or bias that would have been produced under randomization. A key\nadvantage of our test is that it allows for global balance measures as well as\neasily interpretable graphical comparisons. Furthermore, our test does not rely\non any parametric assumptions and can be used to validly assess if the\ninstrument is significantly closer to being as-if randomized than the exposure.\nWe demonstrate our approach on a recent IV application that uses bed\navailability in the intensive care unit (ICU) as an instrument for admission to\nthe ICU.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:44:45 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Branson", "Zach", ""], ["Keele", "Luke", ""]]}, {"id": "1907.01945", "submitter": "Marco Geraci", "authors": "Marco Geraci, Alessio Farcomeni", "title": "Mid-quantile regression for discrete responses", "comments": "30 pages, 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop quantile regression methods for discrete responses by extending\nParzen's definition of marginal mid-quantiles. As opposed to existing\napproaches, which are based on either jittering or latent constructs, we use\ninterpolation and define the conditional mid-quantile function as the inverse\nof the conditional mid-distribution function. We propose a two-step estimator\nwhereby, in the first step, conditional mid-probabilities are obtained\nnonparametrically and, in the second step, regression coefficients are\nestimated by solving an implicit equation. When constraining the quantile index\nto a data-driven admissible range, the second-step estimating equation has a\nleast-squares type, closed-form solution. The proposed estimator is shown to be\nstrongly consistent and asymptotically normal. A simulation study and real data\napplications are presented. Our methods can be applied to a large variety of\ndiscrete responses, including binary, ordinal, and count variables.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:48:51 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Geraci", "Marco", ""], ["Farcomeni", "Alessio", ""]]}, {"id": "1907.01952", "submitter": "Erik \\v{S}trumbelj", "authors": "Jure Dem\\v{s}ar, Grega Repov\\v{s}, Erik \\v{S}trumbelj", "title": "bayes4psy -- an Open Source R Package for Bayesian Statistics in\n  Psychology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.MS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in psychology generates interesting data sets and unique statistical\nmodelling tasks. However, these tasks, while important, are often very\nspecific, so appropriate statistical models and methods cannot be found in\naccessible Bayesian tools. As a result, the use of Bayesian methods is limited\nto those that have the technical and statistical fundamentals that are required\nfor probabilistic programming. Such knowledge is not part of the typical\npsychology curriculum and is a difficult obstacle for psychology students and\nresearchers to overcome. The goal of the bayes4psy package is to bridge this\ngap and offer a collection of models and methods to be used for data analysis\nthat arises from psychology experiments and as a teaching tool for Bayesian\nstatistics in psychology. The package contains Bayesian t-test and\nbootstrapping and models for analyzing reaction times, success rates, and\ncolors. It also provides all the diagnostic, analytic and visualization tools\nfor the modern Bayesian data analysis workflow.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 14:01:23 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Dem\u0161ar", "Jure", ""], ["Repov\u0161", "Grega", ""], ["\u0160trumbelj", "Erik", ""]]}, {"id": "1907.02088", "submitter": "Sambit Panda", "authors": "Sambit Panda, Satish Palaniappan, Junhao Xiong, Eric W. Bridgeford,\n  Ronak Mehta, Cencheng Shen, Joshua T. Vogelstein", "title": "hyppo: A Multivariate Hypothesis Testing Python Package", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce hyppo, a unified library for performing multivariate hypothesis\ntesting, including independence, two-sample, and k-sample testing. While many\nmultivariate independence tests have R packages available, the interfaces are\ninconsistent and most are not available in Python. hyppo includes many state of\nthe art multivariate testing procedures. The package is easy-to-use and is\nflexible enough to enable future extensions. The documentation and all releases\nare available at https://hyppo.neurodata.io.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:05:25 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 19:20:43 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 18:29:49 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 15:21:36 GMT"}, {"version": "v5", "created": "Thu, 20 Aug 2020 12:28:44 GMT"}, {"version": "v6", "created": "Thu, 1 Apr 2021 15:13:13 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Panda", "Sambit", ""], ["Palaniappan", "Satish", ""], ["Xiong", "Junhao", ""], ["Bridgeford", "Eric W.", ""], ["Mehta", "Ronak", ""], ["Shen", "Cencheng", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1907.02241", "submitter": "Michael Byrd", "authors": "Michael Byrd, Linh Nghiem, and Monnie McGee", "title": "Bayesian Regularization of Gaussian Graphical Models with Measurement\n  Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a framework for determining and estimating the conditional\npairwise relationships of variables when the observed samples are contaminated\nwith measurement error in high dimensional settings. Assuming the true\nunderlying variables follow a multivariate Gaussian distribution, if no\nmeasurement error is present, this problem is often solved by estimating the\nprecision matrix under sparsity constraints. However, when measurement error is\npresent, not correcting for it leads to inconsistent estimates of the precision\nmatrix and poor identification of relationships. We propose a new Bayesian\nmethodology to correct for the measurement error from the observed samples.\nThis Bayesian procedure utilizes a recent variant of the spike-and-slab Lasso\nto obtain a point estimate of the precision matrix, and corrects for the\ncontamination via the recently proposed Imputation-Regularization Optimization\nprocedure designed for missing data. Our method is shown to perform better than\nthe naive method that ignores measurement error in both identification and\nestimation accuracy. To show the utility of the method, we apply the new method\nto establish a conditional gene network from a microarray dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 06:15:34 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Byrd", "Michael", ""], ["Nghiem", "Linh", ""], ["McGee", "Monnie", ""]]}, {"id": "1907.02437", "submitter": "Liang Guo", "authors": "Liang Guo, Jianya Liu, Ruodan Lu", "title": "Subsampling Bias and The Best-Discrepancy Systematic Cross Validation", "comments": "SCIENCE China Mathematics. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning models should be evaluated and validated before\nputting to work. Conventional k-fold Monte Carlo Cross-Validation (MCCV)\nprocedure uses a pseudo-random sequence to partition instances into k subsets,\nwhich usually causes subsampling bias, inflates generalization errors and\njeopardizes the reliability and effectiveness of cross-validation. Based on\nordered systematic sampling theory in statistics and low-discrepancy sequence\ntheory in number theory, we propose a new k-fold cross-validation procedure by\nreplacing a pseudo-random sequence with a best-discrepancy sequence, which\nensures low subsampling bias and leads to more precise\nExpected-Prediction-Error estimates. Experiments with 156 benchmark datasets\nand three classifiers (logistic regression, decision tree and naive bayes) show\nthat in general, our cross-validation procedure can extrude subsampling bias in\nthe MCCV by lowering the EPE around 7.18% and the variances around 26.73%. In\ncomparison, the stratified MCCV can reduce the EPE and variances of the MCCV\naround 1.58% and 11.85% respectively. The Leave-One-Out (LOO) can lower the EPE\naround 2.50% but its variances are much higher than the any other CV procedure.\nThe computational time of our cross-validation procedure is just 8.64% of the\nMCCV, 8.67% of the stratified MCCV and 16.72% of the LOO. Experiments also show\nthat our approach is more beneficial for datasets characterized by relatively\nsmall size and large aspect ratio. This makes our approach particularly\npertinent when solving bioscience classification problems. Our proposed\nsystematic subsampling technique could be generalized to other machine learning\nalgorithms that involve random subsampling mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 14:55:02 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Guo", "Liang", ""], ["Liu", "Jianya", ""], ["Lu", "Ruodan", ""]]}, {"id": "1907.02443", "submitter": "Tianxi Li", "authors": "Tianxi Li, Cheng Qian, Elizaveta Levina, Ji Zhu", "title": "High-dimensional Gaussian graphical model for network-linked data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are commonly used to represent conditional dependence\nrelationships between variables. There are multiple methods available for\nexploring them from high-dimensional data, but almost all of them rely on the\nassumption that the observations are independent and identically distributed.\nAt the same time, observations connected by a network are becoming increasingly\ncommon, and tend to violate these assumptions. Here we develop a Gaussian\ngraphical model for observations connected by a network with potentially\ndifferent mean vectors, varying smoothly over the network. We propose an\nefficient estimation algorithm and demonstrate its effectiveness on both\nsimulated and real data, obtaining meaningful and interpretable results on a\nstatistics coauthorship network. We also prove that our method estimates both\nthe inverse covariance matrix and the corresponding graph structure correctly\nunder the assumption of network \u00e2\u0080\u009ccohesion\u00e2\u0080\u009d, which refers to the empirically\nobserved phenomenon of network neighbors sharing similar traits.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 15:08:24 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 14:40:31 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Tianxi", ""], ["Qian", "Cheng", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1907.02447", "submitter": "Arthur Guillaumin", "authors": "Arthur P. Guillaumin, Adam M. Sykulski, Sofia C. Olhede, Frederik J.\n  Simons", "title": "The Debiased Spatial Whittle Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a computationally and statistically efficient method for\nestimating the parameters of a stochastic Gaussian model observed on a regular\nspatial grid in any number of dimensions. Our proposed method, which we call\nthe debiased spatial Whittle likelihood, makes important corrections to the\nwell-known Whittle likelihood to account for large sources of bias caused by\nboundary effects and aliasing. We generalise the approach to flexibly allow for\nsignificant volumes of missing data, for the usage of irregular sampling\nschemes including those with lower-dimensional substructure, and for irregular\nsampling boundaries. We build a theoretical framework under relatively weak\nassumptions which ensures consistency and asymptotic normality in numerous\npractical settings. We provide detailed implementation guidelines which ensure\nthe estimation procedure can still be conducted in $\\mathcal{O}(n\\log n)$\noperations, where $n$ is the number of points of the encapsulating rectangular\ngrid, thus keeping the computational scalability of Fourier and Whittle-based\nmethods for large data sets. We validate our procedure over a range of\nsimulated and real world settings, and compare with state-of-the-art\nalternatives, demonstrating the enduring significant practical appeal of\nFourier-based methods, provided they are corrected by the constructive\nprocedures developed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 15:11:36 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:33:14 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 21:17:15 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Guillaumin", "Arthur P.", ""], ["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Simons", "Frederik J.", ""]]}, {"id": "1907.02473", "submitter": "Richard Lockhart", "authors": "Richard A Lockhart", "title": "Bayes factors with (overly) informative priors", "comments": "1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priors in which a large number of parameters are specified to be independent\nare dangerous; they make it hard to learn from data. I present a couple of\nexamples from the literature and work through a bit of large sample theory to\nshow what happens.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 15:56:43 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 04:44:04 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Lockhart", "Richard A", ""]]}, {"id": "1907.02493", "submitter": "Tommaso Rigon", "authors": "Tommaso Rigon", "title": "An enriched mixture model for functional clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasingly rich literature about Bayesian nonparametric models\nfor clustering functional observations. However, most of the recent proposals\nrely on infinite-dimensional characterizations that might lead to overly\ncomplex cluster solutions. In addition, while prior knowledge about the\nfunctional shapes is typically available, its practical exploitation might be a\ndifficult modeling task. Motivated by an application in e-commerce, we propose\na novel enriched Dirichlet mixture model for functional data. Our proposal\naccommodates the incorporation of functional constraints while bounding the\nmodel complexity. To clarify the underlying partition mechanism, we\ncharacterize the prior process through a P\\'olya urn scheme. These features\nlead to a very interpretable clustering method compared to available\ntechniques. To overcome computational bottlenecks, we employ a variational\nBayes approximation for tractable posterior inference.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 17:09:04 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Rigon", "Tommaso", ""]]}, {"id": "1907.02569", "submitter": "George Leckie", "authors": "George Leckie", "title": "Cross-classified multilevel models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-classified multilevel modelling is an extension of standard multilevel\nmodelling for non-hierarchical data that have cross-classified structures.\nTraditional multilevel models involve hierarchical data structures whereby\nlower level units such as students are nested within higher level units such as\nschools and where these higher level units may in turn be nested within further\ngroupings or clusters such as school districts, regions, and countries. With\nhierarchical data structures, there is an exact nesting of each lower level\nunit in one and only one higher level unit. For example, each student attends\none school, each school is located within one school district, and so on.\nHowever, social reality is more complicated than this, and so social and\nbehavioural data often do not follow pure or strict hierarchies. Two types of\nnon-hierarchical data structures which often appear in practice are\ncross-classified and multiple membership structures. In this article, we\ndescribe cross-classified data structures and cross-classified hierarchical\nlinear modelling which can be used to analyse them.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 19:45:08 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Leckie", "George", ""]]}, {"id": "1907.02579", "submitter": "Nina Golyandina", "authors": "Nina Golyandina", "title": "Particularities and commonalities of singular spectrum analysis as a\n  method of time series analysis and signal processing", "comments": null, "journal-ref": "WIREs Computational Statistics, 2020, Vol.12, No 4., e1487, 39pp", "doi": "10.1002/wics.1487", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Singular spectrum analysis (SSA), starting from the second half of the XX\ncentury, has been a rapidly developing method of time series analysis. Since it\ncan be called principal component analysis for time series, SSA will definitely\nbe a standard method in time series analysis and signal processing in the\nfuture. Moreover, the problems solved by SSA are considerably wider than that\nfor principal component analysis. In particular, the problems of frequency\nestimation, forecasting and missing values imputation can be solved within the\nframework of SSA. The idea of SSA came from different scientific communities,\nsuch as that of researchers in time series analysis (Karhunen-Loeve\ndecomposition), signal processing (low-rank approximation and frequency\nestimation) and multivariate data analysis (principal component analysis).\nAlso, depending on the area of applications, different viewpoints on the same\nalgorithms, choice of parameters, and methodology as a whole are considered.\nThus, the aim of the paper is to describe and compare different viewpoints on\nSSA and its modifications and extensions to give people from different\nscientific communities the possibility to be aware of potentially new aspects\nof the method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 20:28:52 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 20:41:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Golyandina", "Nina", ""]]}, {"id": "1907.02706", "submitter": "Unn Dahlen", "authors": "Unn Dahlen and Johan Linstr\\\"om and Marko Scholze", "title": "Spatio-Temporal Reconstructions of Global CO2-Fluxes using Gaussian\n  Markov Random Fields", "comments": "Article: 37 pages, 11 figures, including references and appendix.\n  Supplemental material: 11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atmospheric inverse modelling is a method for reconstructing historical\nfluxes of green-house gas between land and atmosphere, using observed\natmospheric concentrations and an atmospheric tracer transport model. The small\nnumber of observed atmospheric concentrations in relation to the number of\nunknown flux components makes the inverse problem ill-conditioned, and\nassumptions on the fluxes are needed to constrain the solution. A common\npractise is to model the fluxes using latent Gaussian fields with a mean\nstructure based on estimated fluxes from combinations of process modelling\n(natural fluxes) and statistical bookkeeping (anthropogenic emissions). Here,\nwe reconstruct global \\CO flux fields by modelling fluxes using Gaussian Markov\nRandom Fields (GMRF), resulting in a flexible and computational beneficial\nmodel with a Mat\\'ern-like spatial covariance, and a temporal covariance\ndefined through an auto-regressive model with seasonal dependence.\n  In contrast to previous inversions, the flux is defined on a spatially\ncontinuous domain, and the traditionally discrete flux representation is\nreplaced by integrated fluxes at the resolution specified by the transport\nmodel. This formulation removes aggregation errors in the flux covariance, due\nto the traditional representation of area integrals by fluxes at discrete\npoints, and provides a model closer resembling real-life space-time continuous\nfluxes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:31:30 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Dahlen", "Unn", ""], ["Linstr\u00f6m", "Johan", ""], ["Scholze", "Marko", ""]]}, {"id": "1907.02764", "submitter": "Kellyn Arnold", "authors": "Peter W. G. Tennant, Kellyn F. Arnold, George T. H. Ellison, Mark S.\n  Gilthorpe", "title": "Analyses of 'change scores' do not estimate causal effects in\n  observational data", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: In longitudinal data, it is common to create 'change scores' by\nsubtracting measurements taken at baseline from those taken at follow-up, and\nthen to analyse the resulting 'change' as the outcome variable. In\nobservational data, this approach can produce misleading causal effect\nestimates. The present article uses directed acyclic graphs (DAGs) and simple\nsimulations to provide an accessible explanation of why change scores do not\nestimate causal effects in observational data.\n  Methods: Data were simulated to match three general scenarios where the\nvariable representing measurements of the outcome at baseline was a 1)\ncompeting exposure, 2) confounder, or 3) mediator for the total causal effect\nof the exposure on the variable representing measurements of the outcome at\nfollow-up. Regression coefficients were compared between change-score analyses\nand DAG-informed analyses.\n  Results: Change-score analyses do not provide meaningful causal effect\nestimates unless the variable representing measurements of the outcome at\nbaseline is a competing exposure, as in a randomised experiment. Where such\nvariables (i.e. baseline measurements of the outcome) are confounders or\nmediators, the conclusions drawn from analyses of change scores diverge\n(potentially substantially) from those of DAG-informed analyses.\n  Conclusions: Future observational studies that seek causal effect estimates\nshould avoid analysing change scores and adopt alternative analytical\nstrategies.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 10:42:38 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Tennant", "Peter W. G.", ""], ["Arnold", "Kellyn F.", ""], ["Ellison", "George T. H.", ""], ["Gilthorpe", "Mark S.", ""]]}, {"id": "1907.02829", "submitter": "Adam Brentnall", "authors": "Adam R Brentnall, Jack Cuzick", "title": "Risk models for breast cancer and their validation", "comments": null, "journal-ref": "Statist. Sci., Volume 35, Number 1 (2020), 14-30", "doi": "10.1214/19-STS729", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strategies to prevent cancer and diagnose it early when it is most treatable\nare needed to reduce the public health burden from rising disease incidence.\nRisk assessment is playing an increasingly important role in targeting\nindividuals in need of such interventions. For breast cancer many individual\nrisk factors have been well understood for a long time, but the development of\na fully comprehensive risk model has not been straightforward, in part because\nthere have been limited data where joint effects of an extensive set of risk\nfactors may be estimated with precision. In this article we first review the\napproach taken to develop the IBIS (Tyrer-Cuzick) model, and describe recent\nupdates. We then review and develop methods to assess calibration of models\nsuch as this one, where the risk of disease allowing for competing mortality\nover a long follow-up time or lifetime is estimated. The breast cancer risk\nmodel model and calibration assessment methods are demonstrated using a cohort\nof 132 139 women attending mammography screening in Washington, USA.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 13:43:00 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 16:29:19 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Brentnall", "Adam R", ""], ["Cuzick", "Jack", ""]]}, {"id": "1907.02844", "submitter": "Joshua Vogelstein", "authors": "Meghana Madhyastha, Percy Li, James Browne, Veronika Strnadova-Neeley,\n  Carey E. Priebe, Randal Burns, Joshua T. Vogelstein", "title": "Geodesic Learning via Unsupervised Decision Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic distance is the shortest path between two points in a Riemannian\nmanifold. Manifold learning algorithms, such as Isomap, seek to learn a\nmanifold that preserves geodesic distances. However, such methods operate on\nthe ambient dimensionality, and are therefore fragile to noise dimensions. We\ndeveloped an unsupervised random forest method (URerF) to approximately learn\ngeodesic distances in linear and nonlinear manifolds with noise. URerF operates\non low-dimensional sparse linear combinations of features, rather than the full\nobserved dimensionality. To choose the optimal split in a computationally\nefficient fashion, we developed a fast Bayesian Information Criterion statistic\nfor Gaussian mixture models. We introduce geodesic precision-recall curves\nwhich quantify performance relative to the true latent manifold. Empirical\nresults on simulated and real data demonstrate that URerF is robust to\nhigh-dimensional noise, where as other methods, such as Isomap, UMAP, and\nFLANN, quickly deteriorate in such settings. In particular, URerF is able to\nestimate geodesic distances on a real connectome dataset better than other\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:15:07 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Madhyastha", "Meghana", ""], ["Li", "Percy", ""], ["Browne", "James", ""], ["Strnadova-Neeley", "Veronika", ""], ["Priebe", "Carey E.", ""], ["Burns", "Randal", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1907.03025", "submitter": "Wojciech Rejchel", "authors": "Piotr Pokarowski, Wojciech Rejchel, Agnieszka Soltys, Michal Frej and\n  Jan Mielniczuk", "title": "Improving Lasso for model selection and prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the Thresholded Lasso (TL), SCAD or MCP correct intrinsic\nestimation bias of the Lasso. In this paper we propose an alternative method of\nimproving the Lasso for predictive models with general convex loss functions\nwhich encompass normal linear models, logistic regression, quantile regression\nor support vector machines. For a given penalty we order the absolute values of\nthe Lasso non-zero coefficients and then select the final model from a small\nnested family by the Generalized Information Criterion. We derive exponential\nupper bounds on the selection error of the method. These results confirm that,\nat least for normal linear models, our algorithm seems to be the benchmark for\nthe theory of model selection as it is constructive, computationally efficient\nand leads to consistent model selection under weak assumptions. Constructivity\nof the algorithm means that, in contrast to the TL, SCAD or MCP, consistent\nselection does not rely on the unknown parameters as the cone invertibility\nfactor. Instead, our algorithm only needs the sample size, the number of\npredictors and an upper bound on the noise parameter. We show in numerical\nexperiments on synthetic and real-world data sets that an implementation of our\nalgorithm is more accurate than implementations of studied concave\nregularizations. Our procedure is contained in the R package \"DMRnet\" and\navailable on the CRAN repository.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 21:05:39 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 16:07:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pokarowski", "Piotr", ""], ["Rejchel", "Wojciech", ""], ["Soltys", "Agnieszka", ""], ["Frej", "Michal", ""], ["Mielniczuk", "Jan", ""]]}, {"id": "1907.03153", "submitter": "Cl\\'emence Karmann Mrs", "authors": "Anne G\\'egout-Petit, Aur\\'elie Gueudin-Muller, Cl\\'emence Karmann", "title": "The revisited knockoffs method for variable selection in L1-penalised\n  regressions", "comments": "arXiv admin note: text overlap with arXiv:1805.10097", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection in regression models. In\nparticular, we are interested in selecting explanatory covariates linked with\nthe response variable and we want to determine which covariates are relevant,\nthat is which covariates are involved in the model. In this framework, we deal\nwith L1-penalised regression models. To handle the choice of the penalty\nparameter to perform variable selection, we develop a new method based on the\nknockoffs idea. This revisited knockoffs method is general, suitable for a wide\nrange of regressions with various types of response variables. Besides, it also\nworks when the number of observations is smaller than the number of covariates\nand gives an order of importance of the covariates. Finally, we provide many\nexperimental results to corroborate our method and compare it with other\nvariable selection methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 16:44:07 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["G\u00e9gout-Petit", "Anne", ""], ["Gueudin-Muller", "Aur\u00e9lie", ""], ["Karmann", "Cl\u00e9mence", ""]]}, {"id": "1907.03155", "submitter": "Helton Graziadei", "authors": "Helton Graziadei, Hedibert F. Lopes, Paulo C. Marques F", "title": "Learning a latent pattern of heterogeneity in the innovation rates of a\n  time series of counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian hierarchical semiparametric model for phenomena related\nto time series of counts. The main feature of the model is its capability to\nlearn a latent pattern of heterogeneity in the distribution of the process\ninnovation rates, which are softly clustered through time with the help of a\nDirichlet process placed at the top of the model hierarchy. The probabilistic\nforecasting capabilities of the model are put to test in the analysis of crime\ndata in Pittsburgh, with favorable results.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 17:03:48 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Graziadei", "Helton", ""], ["Lopes", "Hedibert F.", ""], ["F", "Paulo C. Marques", ""]]}, {"id": "1907.03178", "submitter": "Alexander M\\\"arz", "authors": "Alexander M\\\"arz", "title": "XGBoostLSS -- An extension of XGBoost to probabilistic forecasting", "comments": "Bayesian Optimization; Distributional Modeling; Expectile Regression;\n  GAMLSS; Probabilistic Forecast; Uncertainty Quantification; XGBoost", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework of XGBoost that predicts the entire conditional\ndistribution of a univariate response variable. In particular, XGBoostLSS\nmodels all moments of a parametric distribution (i.e., mean, location, scale\nand shape [LSS]) instead of the conditional mean only. Choosing from a wide\nrange of continuous, discrete and mixed discrete-continuous distribution,\nmodelling and predicting the entire conditional distribution greatly enhances\nthe flexibility of XGBoost, as it allows to gain additional insight into the\ndata generating process, as well as to create probabilistic forecasts from\nwhich prediction intervals and quantiles of interest can be derived. We present\nboth a simulation study and real world examples that demonstrate the virtues of\nour approach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:25:16 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 10:32:31 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 09:16:42 GMT"}, {"version": "v4", "created": "Sun, 25 Aug 2019 09:30:15 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["M\u00e4rz", "Alexander", ""]]}, {"id": "1907.03186", "submitter": "Wei Shi", "authors": "Junxian Geng, Wei Shi, Guanyu Hu", "title": "Bayesian Nonparametric Nonhomogeneous Poisson Process with Applications\n  to USGS Earthquake Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intensity estimation is a common problem in statistical analysis of spatial\npoint pattern data. This paper proposes a nonparametric Bayesian method for\nestimating the spatial point process intensity based on mixture of finite\nmixture (MFM) model. MFM approach leads to a consistent estimate of the\nintensity of spatial point patterns in different areas while considering\nheterogeneity. An efficient Markov chain Monte Carlo (MCMC) algorithm is\nproposed for our method. Extensive simulation studies are carried out to\nexamine empirical performance of the proposed method. The usage of our proposed\nmethod is further illustrated with the analysis of the Earthquake Hazards\nProgram of United States Geological Survey (USGS) earthquake data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 21:05:12 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Geng", "Junxian", ""], ["Shi", "Wei", ""], ["Hu", "Guanyu", ""]]}, {"id": "1907.03385", "submitter": "Qiang Sun", "authors": "Zhenhua Lin, Dehan Kong, Qiang Sun", "title": "Modeling Symmetric Positive Definite Matrices with An Application to\n  Functional Brain Connectivity", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, functional brain connectivity describes the connectivity\nbetween brain regions that share functional properties. Neuroscientists often\ncharacterize it by a time series of covariance matrices between functional\nmeasurements of distributed neuron areas. An effective statistical model for\nfunctional connectivity and its changes over time is critical for better\nunderstanding the mechanisms of brain and various neurological diseases. To\nthis end, we propose a matrix-log mean model with an additive heterogeneous\nnoise for modeling random symmetric positive definite matrices that lie in a\nRiemannian manifold. The heterogeneity of error terms is introduced\nspecifically to capture the curved nature of the manifold. We then propose to\nuse the local scan statistics to detect change patterns in the functional\nconnectivity. Theoretically, we show that our procedure can recover all change\npoints consistently. Simulation studies and an application to the Human\nConnectome Project lend further support to the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 02:28:39 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Lin", "Zhenhua", ""], ["Kong", "Dehan", ""], ["Sun", "Qiang", ""]]}, {"id": "1907.03441", "submitter": "Amit Huppert", "authors": "Rami Yaari, Amit Huppert, Itai Dattner", "title": "A statistical methodology for data-driven partitioning of infectious\n  disease incidence into age-groups", "comments": "18 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding age-group dynamics of infectious diseases is a fundamental\nissue for both scientific study and policymaking. Age-structure epidemic models\nwere developed in order to study and improve our understanding of these\ndynamics. By fitting the models to incidence data of real outbreaks one can\ninfer estimates of key epidemiological parameters. However, estimation of the\ntransmission in an age-structured populations requires first to define the\nage-groups of interest. Misspecification in representing the heterogeneity in\nthe age-dependent transmission rates can potentially lead to biased estimation\nof parameters. We develop the first statistical, data-driven methodology for\ndeciding on the best partition of incidence data into age-groups. The method\nemploys a top-down hierarchical partitioning algorithm, with a metric distance\nbuilt for maximizing mathematical identifiability of the transmission matrix,\nand a stopping criteria based on significance testing. The methodology is\ntested using simulations showing good statistical properties. The methodology\nis then applied to influenza incidence data of 14 seasons in order to extract\nthe significant age-group clusters in each season.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 08:01:04 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 08:03:12 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Yaari", "Rami", ""], ["Huppert", "Amit", ""], ["Dattner", "Itai", ""]]}, {"id": "1907.03682", "submitter": "Jiwei Zhao", "authors": "Jiwei Zhao and Yanyuan Ma", "title": "A Versatile Estimation Procedure without Estimating the Nonignorable\n  Missingness Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation problem in a regression setting where the outcome\nvariable is subject to nonignorable missingness and identifiability is ensured\nby the shadow variable approach. We propose a versatile estimation procedure\nwhere modeling of missingness mechanism is completely bypassed. We show that\nour estimator is easy to implement and we derive the asymptotic theory of the\nproposed estimator. We also investigate some alternative estimators under\ndifferent scenarios. Comprehensive simulation studies are conducted to\ndemonstrate the finite sample performance of the method. We apply the estimator\nto a children's mental health study to illustrate its usefulness.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 15:39:39 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhao", "Jiwei", ""], ["Ma", "Yanyuan", ""]]}, {"id": "1907.03807", "submitter": "Fang Xie", "authors": "Fang Xie and Johannes Lederer", "title": "Aggregating Knockoffs for False Discovery Rate Control with an\n  Application to Gut Microbiome Data", "comments": null, "journal-ref": "Entropy 23(2021) 230", "doi": "10.3390/e23020230", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent discoveries suggest that our gut microbiome plays an important role in\nour health and wellbeing. However, the gut microbiome data are intricate; for\nexample, the microbial diversity in the gut makes the data high-dimensional.\nWhile there are dedicated high-dimensional methods, such as the lasso\nestimator, they always come with the risk of false discoveries. Knockoffs are a\nrecent approach to control the number of false discoveries. In this paper, we\nshow that knockoffs can be aggregated to increase power while retaining sharp\ncontrol over the false discoveries. We support our method both in theory and\nsimulations, and we show that it can lead to new discoveries on microbiome data\nfrom the American Gut Project. In particular, our results indicate that several\nphyla that have been overlooked so far are associated with obesity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:48:57 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 09:36:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xie", "Fang", ""], ["Lederer", "Johannes", ""]]}, {"id": "1907.03808", "submitter": "Lu Yu", "authors": "Lu Yu and Tobias Kaufmann and Johannes Lederer", "title": "False Discovery Rates in Biological Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of data has generated unprecedented prospects for\nnetwork analyses in many biological fields, such as neuroscience (e.g., brain\nnetworks), genomics (e.g., gene-gene interaction networks), and ecology (e.g.,\nspecies interaction networks). A powerful statistical framework for estimating\nsuch networks is Gaussian graphical models, but standard estimators for the\ncorresponding graphs are prone to large numbers of false discoveries. In this\npaper, we introduce a novel graph estimator based on knockoffs that imitate the\npartial correlation structures of unconnected nodes. We show that this new\nestimator guarantees accurate control of the false discovery rate in theory,\nsimulations, and biological applications, and we provide easy-to-use R code.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:49:09 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 14:44:43 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 15:40:29 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Yu", "Lu", ""], ["Kaufmann", "Tobias", ""], ["Lederer", "Johannes", ""]]}, {"id": "1907.03829", "submitter": "Mattia  Zorzi", "authors": "Mattia Zorzi", "title": "Empirical Bayesian Learning in AR Graphical Models", "comments": "Automatica (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning graphical models which correspond to high\ndimensional autoregressive stationary stochastic processes. A graphical model\ndescribes the conditional dependence relations among the components of a\nstochastic process and represents an important tool in many fields. We propose\nan empirical Bayes estimator of sparse autoregressive graphical models and\nlatent-variable autoregressive graphical models. Numerical experiments show the\nbenefit to take this Bayesian perspective for learning these types of graphical\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 19:49:12 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Zorzi", "Mattia", ""]]}, {"id": "1907.03888", "submitter": "Barnaby Rowe Dr", "authors": "Barnaby Rowe", "title": "Residual Entropy", "comments": "10 pages, 5 figures, updated to v2 with minor edits following\n  feedback; code used to generate all figures available at\n  https://github.com/barnabytprowe/residual-entropy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to improving model fitting and model generalization\nthat considers the entropy of distributions of modelling residuals. We use\nsimple simulations to demonstrate the observational signatures of overfitting\non ordered sequences of modelling residuals, via the autocorrelation and power\nspectral density. These results motivate the conclusion that, as commonly\napplied, the least squares method assumes too much when it assumes that\nresiduals are uncorrelated for all possible models or values of the model\nparameters. We relax these too-stringent assumptions in favour of imposing an\nentropy prior on the (unknown, model-dependent, but potentially marginalizable)\ndistribution function for residuals. We recommend a simple extension to the\nMean Squared Error loss function that approximately incorporates this prior and\ncan be used immediately for modelling applications where meaningfully-ordered\nsequences of observations or training data can be defined.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 22:04:21 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 23:08:05 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Rowe", "Barnaby", ""]]}, {"id": "1907.04004", "submitter": "Kwangho Kim", "authors": "Kwangho Kim, Edward H. Kennedy, and Ashley I. Naimi", "title": "Incremental Intervention Effects in Studies with Dropout and Many\n  Timepoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern longitudinal studies collect feature data at many timepoints, often of\nthe same order of sample size. Such studies are typically affected by {dropout}\nand positivity violations. We tackle these problems by generalizing effects of\nrecent incremental interventions (which shift propensity scores rather than set\ntreatment values deterministically) to accommodate multiple outcomes and\nsubject dropout. We give an identifying expression for incremental effects when\ndropout is conditionally ignorable (without requiring treatment positivity),\nand derive the nonparametric efficiency bound for estimating such effects. Then\nwe present efficient nonparametric estimators, showing that they converge at\nfast parametric rates and yield uniform inferential guarantees, even when\nnuisance functions are estimated flexibly at slower rates. We also study the\nefficiency of incremental effects relative to more conventional deterministic\neffects in a novel infinite time horizon setting, where the number of\ntimepoints can grow with sample size, and show that incremental effects yield\nnear-exponential gains in this setup. Finally we conclude with simulations and\napply our methods in a study of the effect of low-dose aspirin on pregnancy\noutcomes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:26:41 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 21:39:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Kim", "Kwangho", ""], ["Kennedy", "Edward H.", ""], ["Naimi", "Ashley I.", ""]]}, {"id": "1907.04059", "submitter": "Joaqu\\'in Mart\\'inez-Minaya", "authors": "Joaqu\\'in Mart\\'inez-Minaya, Finn Lindgren, Antonio L\\'opez-Qu\\'ilez,\n  Daniel Simpson and David Conesa", "title": "The Integrated nested Laplace approximation for fitting models with\n  multivariate response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Laplace approximation to Bayesian inference in\nregression models for multivariate response variables. We focus on Dirichlet\nregression models, which can be used to analyze a set of variables on a simplex\nexhibiting skewness and heteroscedasticity, without having to transform the\ndata. These data, which mainly consist of proportions or percentages of\ndisjoint categories, are widely known as compositional data and are common in\nareas such as ecology, geology, and psychology. We provide both the theoretical\nfoundations and a description of how this Laplace approximation can be\nimplemented in the case of Dirichlet regression. The paper also introduces the\npackage dirinla in the R-language that extends the INLA package, which can not\ndeal directly with multivariate likelihoods like the Dirichlet likelihood.\nSimulation studies are presented to validate the good behaviour of the proposed\nmethod, while a real data case-study is used to show how this approach can be\napplied.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:50:07 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 08:50:04 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Mart\u00ednez-Minaya", "Joaqu\u00edn", ""], ["Lindgren", "Finn", ""], ["L\u00f3pez-Qu\u00edlez", "Antonio", ""], ["Simpson", "Daniel", ""], ["Conesa", "David", ""]]}, {"id": "1907.04078", "submitter": "Brennan Kahan", "authors": "Brennan C Kahan, Gordon Forbes, Suzie Cro", "title": "How to design a pre-specified statistical analysis approach to limit\n  p-hacking in clinical trials: the Pre-SPEC framework", "comments": "26 pages, 3 tables, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results from clinical trials can be susceptible to bias if investigators\nchoose their analysis approach after seeing trial data, as this can allow them\nto perform multiple analyses and then choose the method that provides the most\nfavourable result (commonly referred to as 'p-hacking'). Pre-specification of\nthe planned analysis approach is essential to help reduce such bias, as it\nensures analytical methods are chosen in advance of seeing the trial data.\nHowever, pre-specification is only effective if done in a way that does not\nallow p-hacking. For example, investigators may pre-specify a certain\nstatistical method such as multiple imputation, but give little detail on how\nit will be implemented. Because there are many different ways to perform\nmultiple imputation, this approach to pre-specification is ineffective, as it\nstill allows investigators to analyse the data in different ways before\ndeciding on a final approach. In this article we describe a five-point\nframework (the Pre-SPEC framework) for designing a pre-specified analysis\napproach that does not allow p-hacking. This framework is intended to be used\nin conjunction with the SPIRIT (Standard Protocol Items: Recommendations for\nInterventional Trials) statement and other similar guidelines to help\ninvestigators design the statistical analysis strategy for the trial's primary\noutcome in the trial protocol.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:07:29 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 12:10:12 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 10:34:51 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Kahan", "Brennan C", ""], ["Forbes", "Gordon", ""], ["Cro", "Suzie", ""]]}, {"id": "1907.04081", "submitter": "Alexis Bellot", "authors": "Alexis Bellot and Mihaela van der Schaar", "title": "Kernel Hypothesis Testing with Set-valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for hypothesis testing on distributions of\nsets of individual examples. Sets may represent many common data sources such\nas groups of observations in time series, collections of words in text or a\nbatch of images of a given phenomenon. This observation pattern, however,\ndiffers from the common assumptions required for hypothesis testing: each set\ndiffers in size, may have differing levels of noise, and also may incorporate\nnuisance variability, irrelevant for the analysis of the phenomenon of\ninterest; all features that bias test decisions if not accounted for. In this\npaper, we propose to interpret sets as independent samples from a collection of\nlatent probability distributions, and introduce kernel two-sample and\nindependence tests in this latent space of distributions. We prove the\nconsistency of tests and observe them to outperform in a wide range of\nsynthetic experiments. Finally, we showcase their use in practice with\nexperiments of healthcare and climate data, where previously heuristics were\nneeded for feature extraction and testing.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:17:35 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 10:43:08 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 08:21:13 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 09:38:54 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Bellot", "Alexis", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1907.04147", "submitter": "Ke Zhu", "authors": "Feiyu Jiang, Dong Li, Ke Zhu", "title": "Adaptive inference for a semiparametric generalized autoregressive\n  conditional heteroskedasticity model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a semiparametric generalized autoregressive conditional\nheteroskedasticity (S-GARCH) model. For this model, we first estimate the\ntime-varying long run component for unconditional variance by the kernel\nestimator, and then estimate the non-time-varying parameters in GARCH-type\nshort run component by the quasi maximum likelihood estimator (QMLE). We show\nthat the QMLE is asymptotically normal with the parametric convergence rate.\nNext, we construct a Lagrange multiplier test for linear parameter constraint\nand a portmanteau test for model checking, and obtain their asymptotic null\ndistributions. Our entire statistical inference procedure works for the\nnon-stationary data with two important features: first, our QMLE and two tests\nare adaptive to the unknown form of the long run component; second, our QMLE\nand two tests share the same efficiency and testing power as those in variance\ntargeting method when the S-GARCH model is stationary.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:24:40 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 12:54:27 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 02:59:06 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 08:38:14 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Jiang", "Feiyu", ""], ["Li", "Dong", ""], ["Zhu", "Ke", ""]]}, {"id": "1907.04148", "submitter": "George Leckie", "authors": "George Leckie", "title": "Multiple membership multilevel models", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.02569", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple membership multilevel models are an extension of standard multilevel\nmodels for non-hierarchical data that have multiple membership structures.\nTraditional multilevel models involve hierarchical data structures whereby\nlower-level units such as students are nested within higher-level units such as\nschools and where these higher-level units may in turn be nested within further\ngroupings or clusters such as school districts, regions, and countries. With\nhierarchical data structures, there is an exact nesting of each lower-level\nunit in one and only one higher-level unit. For example, each student attends\none school, each school is located within one school district, and so on.\nHowever, social reality is more complicated than this, and so social and\nbehavioural data often do not follow pure or strict hierarchies. Two types of\nnon-hierarchical data structures which often appear in practice are\ncross-classified and multiple membership structures. In this article, we\ndescribe multiple membership data structures and multiple membership models\nwhich can be used to analyse them.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 19:46:34 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Leckie", "George", ""]]}, {"id": "1907.04266", "submitter": "Zhanlin Liu", "authors": "Zhanlin Liu, Ashis G. Banerjee, and Youngjun Choe", "title": "Identifying the Influential Inputs for Network Output Variance Using\n  Sparse Polynomial Chaos Expansion", "comments": null, "journal-ref": null, "doi": "10.1109/TASE.2020.2993257", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity analysis (SA) is an important aspect of process automation. It\noften aims to identify the process inputs that influence the process output's\nvariance significantly. Existing SA approaches typically consider the\ninput-output relationship as a black-box and conduct extensive random sampling\nfrom the actual process or its high-fidelity simulation model to identify the\ninfluential inputs. In this paper, an alternate, novel approach is proposed\nusing a sparse polynomial chaos expansion-based model for a class of\ninput-output relationships represented as directed acyclic networks. The model\nexploits the relationship structure by recursively relating a network node to\nits direct predecessors to trace the output variance back to the inputs. It,\nthereby, estimates the Sobol indices, which measure the influence of each input\non the output variance, accurately and efficiently. Theoretical analysis\nestablishes the validity of the model as the prediction of the network output\nconverges in probability to the true output under certain regularity\nconditions. Empirical evaluation on two manufacturing processes shows that the\nmodel estimates the Sobol indices accurately with far fewer observations than a\nstate-of-the-art Monte Carlo sampling method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 16:09:11 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 08:04:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Liu", "Zhanlin", ""], ["Banerjee", "Ashis G.", ""], ["Choe", "Youngjun", ""]]}, {"id": "1907.04529", "submitter": "Nadja Klein Prof. Dr.", "authors": "Michael Stanley Smith and Nadja Klein", "title": "Bayesian Inference for Regression Copulas", "comments": "Journal of Business & Economic Statistics (2020)", "journal-ref": null, "doi": "10.1080/07350015.2020.1721295", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new semi-parametric distributional regression smoother that is\nbased on a copula decomposition of the joint distribution of the vector of\nresponse values. The copula is high-dimensional and constructed by inversion of\na pseudo regression, where the conditional mean and variance are\nsemi-parametric functions of covariates modeled using regularized basis\nfunctions. By integrating out the basis coefficients, an implicit copula\nprocess on the covariate space is obtained, which we call a `regression\ncopula'. We combine this with a non-parametric margin to define a copula model,\nwhere the entire distribution - including the mean and variance - of the\nresponse is a smooth semi-parametric function of the covariates. The copula is\nestimated using both Hamiltonian Monte Carlo and variational Bayes; the latter\nof which is scalable to high dimensions. Using real data examples and a\nsimulation study we illustrate the efficacy of these estimators and the copula\nmodel. In a substantive example, we estimate the distribution of half-hourly\nelectricity spot prices as a function of demand and two time covariates using\nradial bases and horseshoe regularization. The copula model produces\ndistributional estimates that are locally adaptive with respect to the\ncovariates, and predictions that are more accurate than those from benchmark\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 06:20:06 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 21:19:29 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Smith", "Michael Stanley", ""], ["Klein", "Nadja", ""]]}, {"id": "1907.04530", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein and Michael Stanley Smith", "title": "Bayesian Variable Selection for Non-Gaussian Responses: A Marginally\n  Calibrated Copula Approach", "comments": null, "journal-ref": "Biometrics (2020)", "doi": "10.1111/biom.13355", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new highly flexible and tractable Bayesian approach to undertake\nvariable selection in non-Gaussian regression models. It uses a copula\ndecomposition for the joint distribution of observations on the dependent\nvariable. This allows the marginal distribution of the dependent variable to be\ncalibrated accurately using a nonparametric or other estimator. The family of\ncopulas employed are `implicit copulas' that are constructed from existing\nhierarchical Bayesian models widely used for variable selection, and we\nestablish some of their properties. Even though the copulas are\nhigh-dimensional, they can be estimated efficiently and quickly using Markov\nchain Monte Carlo (MCMC). A simulation study shows that when the responses are\nnon-Gaussian the approach selects variables more accurately than contemporary\nbenchmarks. A real data example in the Web Appendix illustrates that accounting\nfor even mild deviations from normality can lead to a substantial increase in\naccuracy. To illustrate the full potential of our approach we extend it to\nspatial variable selection for fMRI. Using real data, we show our method allows\nfor voxel-specific marginal calibration of the magnetic resonance signal at\nover 6,000 voxels, leading to an increase in the quality of the activation\nmaps.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 06:20:17 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 19:22:29 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Klein", "Nadja", ""], ["Smith", "Michael Stanley", ""]]}, {"id": "1907.04620", "submitter": "Nick Koning", "authors": "Nick Koning and Paul Bekker", "title": "Sparse Unit-Sum Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers sparsity in linear regression under the restriction that\nthe regression weights sum to one. We propose an approach that combines\n$\\ell_0$- and $\\ell_1$-regularization. We compute its solution by adapting a\nrecent methodological innovation made by Bertsimas et al. (2016) for\n$\\ell_0$-regularization in standard linear regression. In a simulation\nexperiment we compare our approach to $\\ell_0$-regularization and\n$\\ell_1$-regularization and find that it performs favorably in terms of\npredictive performance and sparsity. In an application to index tracking we\nshow that our approach can obtain substantially sparser portfolios compared to\n$\\ell_1$-regularization while maintaining a similar tracking performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:16:05 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Koning", "Nick", ""], ["Bekker", "Paul", ""]]}, {"id": "1907.04763", "submitter": "Raphael Huser", "authors": "\\'Arni V. Johannesson, Stefan Siegert, Rapha\\\"el Huser, Haakon Bakka\n  and Birgir Hrafnkelsson", "title": "Approximate Bayesian inference for analysis of spatio-temporal flood\n  frequency data", "comments": "33 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme floods cause casualties, and widespread damage to property and vital\ncivil infrastructure. We here propose a Bayesian approach for predicting\nextreme floods using the generalized extreme-value (GEV) distribution within\ngauged and ungauged catchments. A major methodological challenge is to find a\nsuitable parametrization for the GEV distribution when covariates or latent\nspatial effects are involved. Other challenges involve balancing model\ncomplexity and parsimony using an appropriate model selection procedure, and\nmaking inference using a reliable and computationally efficient approach. Our\napproach relies on a latent Gaussian modeling framework with a novel\nmultivariate link function designed to separate the interpretation of the\nparameters at the latent level and to avoid unreasonable estimates of the shape\nand time trend parameters. Structured additive regression models are proposed\nfor the four parameters at the latent level. For computational efficiency with\nlarge datasets and richly parametrized models, we exploit an accurate and fast\napproximate Bayesian inference approach. We applied our proposed methodology to\nannual peak river flow data from 554 catchments across the United Kingdom (UK).\nOur model performed well in terms of flood predictions for both gauged and\nungauged catchments. The results show that the spatial model components for the\ntransformed location and scale parameters, and the time trend, are all\nimportant. Posterior estimates of the time trend parameters correspond to an\naverage increase of about $1.5\\%$ per decade and reveal a spatial structure\nacross the UK. To estimate return levels for spatial aggregates, we further\ndevelop a novel copula-based post-processing approach of posterior predictive\nsamples, in order to mitigate the effect of the conditional independence\nassumption at the data level, and we show that our approach provides accurate\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:48:49 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 16:23:27 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 12:50:32 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Johannesson", "\u00c1rni V.", ""], ["Siegert", "Stefan", ""], ["Huser", "Rapha\u00ebl", ""], ["Bakka", "Haakon", ""], ["Hrafnkelsson", "Birgir", ""]]}, {"id": "1907.04805", "submitter": "Amit Sharma", "authors": "Rathin Desai and Amit Sharma", "title": "Quantifying Error in the Presence of Confounders for Causal Inference", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating average causal effect (ACE) is useful whenever we want to know the\neffect of an intervention on a given outcome. In the absence of a randomized\nexperiment, many methods such as stratification and inverse propensity\nweighting have been proposed to estimate ACE. However, it is hard to know which\nmethod is optimal for a given dataset or which hyperparameters to use for a\nchosen method. To this end, we provide a framework to characterize the loss of\na causal inference method against the true ACE, by framing causal inference as\na representation learning problem. We show that many popular methods, including\nback-door methods can be considered as weighting or representation learning\nalgorithms, and provide general error bounds for their causal estimates. In\naddition, we consider the case when unobserved variables can confound the\ncausal estimate and extend proposed bounds using principles of robust\nstatistics, considering confounding as contamination under the Huber\ncontamination model. These bounds are also estimable; as an example, we provide\nempirical bounds for the Inverse Propensity Weighting (IPW) estimator and show\nhow the bounds can be used to optimize the threshold of clipping extreme\npropensity scores. Our work provides a new way to reason about competing\nestimators, and opens up the potential of deriving new methods by minimizing\nthe proposed error bounds.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 15:53:07 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Desai", "Rathin", ""], ["Sharma", "Amit", ""]]}, {"id": "1907.04838", "submitter": "Adrian Dobra", "authors": "Adrian Dobra, Katherine Buhikire and Joachim G. Voss", "title": "Identifying mediating variables with graphical models: an application to\n  the study of causal pathways in people living with HIV", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We empirically demonstrate that graphical models can be a valuable tool in\nthe identification of mediating variables in causal pathways. We make use of\ngraphical models to elucidate the causal pathway through which the treatment\ninfluences the levels of fatigue and weakness in people living with HIV (PLHIV)\nbased on a secondary analysis of a categorical dataset collected in a\nbehavioral clinical trial: is weakness a mediator for the treatment and\nfatigue, or is fatigue a mediator for the treatment and weakness? Causal\nmediation analysis could not offer any definite answers to these questions.\\\\\nKEYWORDS: Contingency tables; graphical models; loglinear models; HIV;\nmediation\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:36:33 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Dobra", "Adrian", ""], ["Buhikire", "Katherine", ""], ["Voss", "Joachim G.", ""]]}, {"id": "1907.04842", "submitter": "Deborshee Sen", "authors": "Andres F. Barrientos, Deborshee Sen, Garritt L Page, and David B\n  Dunson", "title": "Bayesian inferences on uncertain ranks and orderings: Application to\n  ranking players and lineups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to be interested in rankings or order relationships among\nentities. In complex settings where one does not directly measure a univariate\nstatistic upon which to base ranks, such inferences typically rely on\nstatistical models having entity-specific parameters. These can be treated as\nrandom effects in hierarchical models characterizing variation among the\nentities. In this paper, we are particularly interested in the problem of\nranking basketball players in terms of their contribution to team performance.\nUsing data from the United States National Basketball Association (NBA), we\nfind that many players have similar latent ability levels, making any single\nestimated ranking highly misleading. The current literature fails to provide\nsummaries of order relationships that adequately account for uncertainty.\nMotivated by this, we propose a Bayesian strategy for characterizing\nuncertainty in inferences on order relationships among players and lineups. Our\napproach adapts to scenarios in which uncertainty in ordering is high by\nproducing more conservative results that improve interpretability. This is\nachieved through a reward function within a decision theoretic framework. We\napply our approach to data from the 2009-10 NBA season.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:41:55 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 04:06:46 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 01:52:54 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Barrientos", "Andres F.", ""], ["Sen", "Deborshee", ""], ["Page", "Garritt L", ""], ["Dunson", "David B", ""]]}, {"id": "1907.05077", "submitter": "Nick Koning", "authors": "Nick Koning", "title": "Directing Power Towards Conic Parameter Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a high-dimensional parameter of interest, tests based on quadratic\nstatistics are known to have low power against subsets of the parameter space\n(henceforth, parameter subspaces). In addition, they typically involve an\ninverse covariance matrix which is difficult to estimate in high-dimensional\nsettings. I simultaneously address these two issues by proposing a novel test\nstatistic that is large in a conic parameter subspace of interest. This test\nstatistic generalizes the Wald statistic and nests many well-known test\nstatistics. For a given parameter subspace, the statistic is free of tuning\nparameters and suitable for high-dimensional settings if the subspace is\nsufficiently small. It can be computed using regularized linear regression,\nwhere the type of regularization and the regularization parameters are\ncompletely determined by the parameter subspace of interest. I illustrate the\nstatistic on subspaces that consist of sparse or nearly-sparse vectors, for\nwhich the computation corresponds to $\\ell_0$- and $\\ell_1$-regularized\nregression, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 09:56:23 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 15:37:18 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 15:38:53 GMT"}, {"version": "v4", "created": "Mon, 9 Sep 2019 14:26:55 GMT"}, {"version": "v5", "created": "Fri, 8 Nov 2019 23:15:58 GMT"}, {"version": "v6", "created": "Tue, 19 Nov 2019 16:11:03 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Koning", "Nick", ""]]}, {"id": "1907.05234", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok,\n  and Suttipong Thajchayapong", "title": "Identifying Linear Models in Multi-Resolution Population Data using\n  Minimum Description Length Principle to Predict Household Income", "comments": "This is the accepted manuscript for publication in TKDD. The R\n  package is available at https://github.com/DarkEyes/MRReg", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD), 15(2),\n  15 (2021)", "doi": "10.1145/3424670", "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One shirt size cannot fit everybody, while we cannot make a unique shirt that\nfits perfectly for everyone because of resource limitation. This analogy is\ntrue for the policy making. Policy makers cannot establish a single policy to\nsolve all problems for all regions because each region has its own unique\nissue. In the other extreme, policy makers also cannot create a policy for each\nsmall village due to the resource limitation. Would it be better if we can find\na set of largest regions such that the population of each region within this\nset has common issues and we can establish a single policy for them? In this\nwork, we propose a framework using regression analysis and minimum description\nlength (MDL) to find a set of largest areas that have common indicators, which\ncan be used to predict household incomes efficiently. Given a set of household\nfeatures, and a multi-resolution partition that represents administrative\ndivisions, our framework reports a set C* of largest subdivisions that have a\ncommon model for population-income prediction. We formalize a problem of\nfinding C* and propose the algorithm as a solution. We use both simulation\ndatasets as well as a real-world dataset of Thailand's population household\ninformation to demonstrate our framework performance and application. The\nresults show that our framework performance is better than the baseline\nmethods. We show the results of our method can be used to find indicators of\nincome prediction for many areas in Thailand. By increasing these indicator\nvalues, we expect people in these areas to gain more incomes. Hence, the policy\nmakers can plan to establish the policies by using these indicators in our\nresults as a guideline to solve low-income issues. Our framework can be used to\nsupport policy makers to establish policies regarding any other dependent\nvariable beyond incomes in order to combat poverty and other issues.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 03:08:31 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 15:32:25 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 08:39:38 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""], ["Surasvadi", "Navaporn", ""], ["Plangprasopchok", "Anon", ""], ["Thajchayapong", "Suttipong", ""]]}, {"id": "1907.05385", "submitter": "Lamar Hunt", "authors": "Lamar Hunt III, Irene B. Murimi, Jodi B. Segal, Marissa J. Seamans,\n  Daniel O. Scharfstein, Ravi Varadhan", "title": "Brand vs. Generic: Addressing Non-Adherence, Secular Trends, and\n  Non-Overlap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generic drugs offer a cost-effective alternative to brand name drugs,\nregulators need a method to assess therapeutic equivalence in a post market\nsetting. We develop such a method in the context of assessing the therapeutic\nequivalence of immediate release (IM) venlafaxine, based on a large insurance\nclaims dataset provided by OptumLabs\\textsuperscript{\\textregistered}. To\nproperly address this question, our methodology must deal with issues of\nnon-adherence, secular trends in health outcomes, and lack of treatment overlap\ndue to sharp uptake of the generic once it becomes available. We define,\nidentify (under assumptions) and estimate (using G-computation) a causal effect\nfor a time-to-event outcome by extending regression discontinuity to survival\ncurves. We do not find evidence for a lack of therapeutic equivalence of brand\nand generic IM venlafaxine.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 15:32:43 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Hunt", "Lamar", "III"], ["Murimi", "Irene B.", ""], ["Segal", "Jodi B.", ""], ["Seamans", "Marissa J.", ""], ["Scharfstein", "Daniel O.", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1907.05386", "submitter": "Frederic Lavancier", "authors": "Fr\\'ed\\'eric Lavancier and Thierry P\\'ecot and Liu Zengzhen and\n  Charles Kervrann", "title": "Testing independence between two random sets for the analysis of\n  colocalization in bio-imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colocalization aims at characterizing spatial associations between two\nfluorescently-tagged biomolecules by quantifying the co-occurrence and\ncorrelation between the two channels acquired in fluorescence microscopy.\nColocalization is presented either as the degree of overlap between the two\nchannels or the overlays of the red and green images, with areas of yellow\nindicating colocalization of the molecules. This problem remains an open issue\nin diffraction-limited microscopy and raises new challenges with the emergence\nof super-resolution imaging, a microscopic technique awarded by the 2014 Nobel\nprize in chemistry. We propose GcoPS, for Geo-coPositioning System, an original\nmethod that exploits the random sets structure of the tagged molecules to\nprovide an explicit testing procedure. Our simulation study shows that GcoPS\nunequivocally outperforms the best competitive methods in adverse situations\n(noise, irregularly shaped fluorescent patterns, different optical\nresolutions). GcoPS is also much faster, a decisive advantage to face the huge\namount of data in super-resolution imaging. We demonstrate the performances of\nGcoPS on two biological real datasets, obtained by conventional\ndiffraction-limited microscopy technique and by super-resolution technique,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:25:05 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Lavancier", "Fr\u00e9d\u00e9ric", ""], ["P\u00e9cot", "Thierry", ""], ["Zengzhen", "Liu", ""], ["Kervrann", "Charles", ""]]}, {"id": "1907.05409", "submitter": "Solt Kov\\'acs", "authors": "Malte Londschien, Solt Kov\\'acs, Peter B\\\"uhlmann", "title": "Change point detection for graphical models in the presence of missing\n  values", "comments": "14 pages, 6 figures, 3 tables, hdcd R package; added explanations and\n  clarifications, methodology and simulation results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose estimation methods for change points in high-dimensional\ncovariance structures with an emphasis on challenging scenarios with missing\nvalues. We advocate three imputation like methods and investigate their\nimplications on common losses used for change point detection. We also discuss\nhow model selection methods have to be adapted to the setting of incomplete\ndata. The methods are compared in a simulation study and applied to a time\nseries from an environmental monitoring system. An implementation of our\nproposals within the R-package hdcd is available via the Supplementary\nmaterials.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:50:47 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 20:51:57 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Londschien", "Malte", ""], ["Kov\u00e1cs", "Solt", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1907.05453", "submitter": "Jacob S{\\o}gaard Larsen Mr.", "authors": "Jacob S{\\o}gaard Larsen, Anders Stockmarr, Bjarne Kj{\\ae}r Ersb{\\o}ll\n  and Murat Kulahci", "title": "Model based Level Shift Detection in Autocorrelated Data Streams using a\n  moving window", "comments": "14 pages, 8 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Control Chart techniques to detect level shift in data streams\nassume independence between observations. As data today is collected with high\nfrequency, this assumption is seldom valid. To overcome this, we propose to\nadapt the off-line test procedure for detection of outliers based on one-step\nprediction errors proposed by Tsay (1988) into an on-line framework by\nconsidering a moving window. Further, we present two algorithms, that in\ncombination, estimate an appropriate test value for our control chart. We test\nour method on AR(1) processes exposed to level shifts of different sizes and\ncompare it to CUSUM applied to one-step prediction errors. We find that, even\nthough both methods perform comparable when tuned correctly, our method has\nhigher probability of identifying the correct change point of the process.\nFurthermore, for more complicated processes our method is easier to tune, as\nthe range of window size to be tested is independent of the process.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 19:12:24 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 13:52:06 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Larsen", "Jacob S\u00f8gaard", ""], ["Stockmarr", "Anders", ""], ["Ersb\u00f8ll", "Bjarne Kj\u00e6r", ""], ["Kulahci", "Murat", ""]]}, {"id": "1907.05583", "submitter": "Michael Smithson", "authors": "Michael Smithson", "title": "Can Bayes Factors \"Prove\" the Null Hypothesis?", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is possible to obtain a large Bayes Factor (BF) favoring the null\nhypothesis when both the null and alternative hypotheses have low likelihoods,\nand there are other hypotheses being ignored that are much more strongly\nsupported by the data. As sample sizes become large it becomes increasingly\nprobable that a strong BF favouring a point null against a conventional\nBayesian vague alternative co-occurs with a BF favouring various specific\nalternatives against the null. For any BF threshold q and sample mean, there is\na value n such that sample sizes larger than n guarantee that although the BF\ncomparing H0 against a conventional (vague) alternative exceeds q, nevertheless\nfor some range of hypothetical {\\mu}, a BF comparing H0 against {\\mu} in that\nrange falls below 1/q. This paper discusses the conditions under which this\nconundrum occurs and investigates methods for resolving it.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 05:47:39 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Smithson", "Michael", ""]]}, {"id": "1907.05621", "submitter": "Anirudh Tomer", "authors": "Anirudh Tomer, Dimitris Rizopoulos, Daan Nieboer, Frank-Jan Drost,\n  Monique J. Roobol, Ewout W. Steyerberg", "title": "Personalized Decision Making for Biopsies in Prostate Cancer Active\n  Surveillance Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Low-risk prostate cancer patients enrolled in active surveillance\nprograms commonly undergo biopsies for examination of cancer progression.\nBiopsies are conducted as per a fixed and frequent schedule (e.g., annual\nbiopsies). Since biopsies are burdensome, patients do not always comply with\nthe schedule, which increases the risk of delayed detection of cancer\nprogression.\n  Objective: Our aim is to better balance the number of biopsies (burden) and\nthe delay in detection of cancer progression (less is beneficial), by\npersonalizing the decision of conducting biopsies.\n  Data Sources: We use patient data of the world's largest active surveillance\nprogram (PRIAS). It enrolled 5270 patients, had 866 cancer progressions, and an\naverage of nine prostate-specific antigen (PSA) and five digital rectal\nexamination (DRE) measurements per patient.\n  Methods: Using joint models for time-to-event and longitudinal data, we model\nthe historical DRE and PSA measurements, and biopsy results of a patient at\neach follow-up visit. This results in a visit and patient-specific cumulative\nrisk of cancer progression. If this risk is above a certain threshold, we\nschedule a biopsy. We compare this personalized approach with the currently\npracticed biopsy schedules via an extensive and realistic simulation study,\nbased on a replica of the patients from the PRIAS program.\n  Results: The personalized approach saved a median of six biopsies (median: 4,\nIQR: 2-5), compared to the annual schedule (median: 10, IQR: 3-10). However,\nthe delay in detection of progression (years) is similar for the personalized\n(median: 0.7, IQR: 0.3-1.0) and the annual schedule (median: 0.5, IQR:\n0.3-0.8).\n  Conclusions: We conclude that personalized schedules provide substantially\nbetter balance in the number of biopsies per detected progression for men with\nlow-risk prostate cancer.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 08:39:06 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Tomer", "Anirudh", ""], ["Rizopoulos", "Dimitris", ""], ["Nieboer", "Daan", ""], ["Drost", "Frank-Jan", ""], ["Roobol", "Monique J.", ""], ["Steyerberg", "Ewout W.", ""]]}, {"id": "1907.05918", "submitter": "Didong Li", "authors": "Minerva Mukhopadhyay, Didong Li and David B Dunson", "title": "Estimating densities with nonlinear support using Fisher-Gaussian\n  kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current tools for multivariate density estimation struggle when the density\nis concentrated near a nonlinear subspace or manifold. Most approaches require\nchoice of a kernel, with the multivariate Gaussian by far the most commonly\nused. Although heavy-tailed and skewed extensions have been proposed, such\nkernels cannot capture curvature in the support of the data. This leads to poor\nperformance unless the sample size is very large relative to the dimension of\nthe data. This article proposes a novel generalization of the Gaussian\ndistribution, which includes an additional curvature parameter. We refer to the\nproposed class as Fisher-Gaussian (FG) kernels, since they arise by sampling\nfrom a von Mises-Fisher density on the sphere and adding Gaussian noise. The FG\ndensity has an analytic form, and is amenable to straightforward implementation\nwithin Bayesian mixture models using Markov chain Monte Carlo. We provide\ntheory on large support, and illustrate gains relative to competitors in\nsimulated and real data applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 18:45:27 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Mukhopadhyay", "Minerva", ""], ["Li", "Didong", ""], ["Dunson", "David B", ""]]}, {"id": "1907.05927", "submitter": "Daniel McDonald", "authors": "Lei Ding and Daniel J. McDonald", "title": "Predicting phenotypes from microarrays using amplified, initially\n  marginal, eigenvector regression", "comments": "22 pages, 5 figures", "journal-ref": "Bioinformatics (2017), vol. 33, pp. i350-i358", "doi": "10.1093/bioinformatics/btx265", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The discovery of relationships between gene expression\nmeasurements and phenotypic responses is hampered by both computational and\nstatistical impediments. Conventional statistical methods are less than ideal\nbecause they either fail to select relevant genes, predict poorly, ignore the\nunknown interaction structure between genes, or are computationally\nintractable. Thus, the creation of new methods which can handle many expression\nmeasurements on relatively small numbers of patients while also uncovering\ngene-gene relationships and predicting well is desirable.\n  Results: We develop a new technique for using the marginal relationship\nbetween gene expression measurements and patient survival outcomes to identify\na small subset of genes which appear highly relevant for predicting survival,\nproduce a low-dimensional embedding based on this small subset, and amplify\nthis embedding with information from the remaining genes. We motivate our\nmethodology by using gene expression measurements to predict survival time for\npatients with diffuse large B-cell lymphoma, illustrate the behavior of our\nmethodology on carefully constructed synthetic examples, and test it on a\nnumber of other gene expression datasets. Our technique is computationally\ntractable, generally outperforms other methods, is extensible to other\nphenotypes, and also identifies different genes (relative to existing methods)\nfor possible future study.\n  Key words: regression; principal components; matrix sketching;\npreconditioning\n  Availability: All of the code and data are available at\nhttps://github.com/dajmcdon/aimer/.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 19:41:09 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ding", "Lei", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "1907.05941", "submitter": "George Leckie", "authors": "George Leckie", "title": "Multilevel models for continuous outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel models (mixed-effect models or hierarchical linear models) are now\na standard approach to analysing clustered and longitudinal data in the social,\nbehavioural and medical sciences. This review article focuses on multilevel\nlinear regression models for continuous responses (outcomes or dependent\nvariables). These models can be viewed as an extension of conventional linear\nregression models to account for and learn from the clustering in the data.\nCommon clustered applications include studies of school effects on student\nachievement, hospital effects on patient health, and neighbourhood effects on\nrespondent attitudes. In all these examples, multilevel models allow one to\nstudy how the regression relationships vary across clusters, to identify those\ncluster characteristics which predict such variation, to disentangle social\nprocesses operating at different levels of analysis, and to make\ncluster-specific predictions. Common longitudinal applications include studies\nof growth curves of individual height and weight and developmental trajectories\nof individual behaviours. In these examples, multilevel models allow one to\ndescribe and explain variation in growth rates and to simultaneously explore\npredictors of both of intra- and inter-individual variation. This article\nintroduces and illustrates this powerful class of model. We start by focusing\non the most commonly applied two-level random-intercept and -slope models. We\nillustrate through two detailed examples how these models can be applied to\nboth clustered and longitudinal data and in both observational and experimental\nsettings. We then review more flexible three-level, cross-classified, multiple\nmembership and multivariate response models. We end by recommending a range of\nfurther reading on all these topics.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 20:27:55 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Leckie", "George", ""]]}, {"id": "1907.06116", "submitter": "Sai Li", "authors": "Sai Li and Tony T. Cai and Hongzhe Li", "title": "Inference for high-dimensional linear mixed-effects models: A\n  quasi-likelihood approach", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed-effects models are widely used in analyzing clustered or\nrepeated measures data. We propose a quasi-likelihood approach for estimation\nand inference of the unknown parameters in linear mixed-effects models with\nhigh-dimensional fixed effects. The proposed method is applicable to general\nsettings where the dimension of the random effects and the cluster sizes are\npossibly large. Regarding the fixed effects, we provide rate optimal estimators\nand valid inference procedures that do not rely on the structural information\nof the variance components. We also study the estimation of variance components\nwith high-dimensional fixed effects in general settings. The algorithms are\neasy to implement and computationally fast. The proposed methods are assessed\nin various simulation settings and are applied to a real study regarding the\nassociations between body mass index and genetic polymorphic markers in a\nheterogeneous stock mice population.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 18:44:13 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 17:01:45 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Li", "Sai", ""], ["Cai", "Tony T.", ""], ["Li", "Hongzhe", ""]]}, {"id": "1907.06133", "submitter": "Lihua Lei", "authors": "Lihua Lei and Peter J. Bickel", "title": "An Assumption-Free Exact Test For Fixed-Design Linear Models With\n  Exchangeable Errors", "comments": "Accepted by Biometrika; 46 pages", "journal-ref": null, "doi": "10.1093/biomet/asaa079", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Cyclic Permutation Test (CPT) to test general linear\nhypotheses for linear models. This test is non-randomized and valid in finite\nsamples with exact Type I error $\\alpha$ for an arbitrary fixed design matrix\nand arbitrary exchangeable errors, whenever $1 / \\alpha$ is an integer and $n /\np \\ge 1 / \\alpha - 1$. The test involves applying the marginal rank test to $1\n/ \\alpha$ linear statistics of the outcome vector, where the coefficient\nvectors are determined by solving a linear system such that the joint\ndistribution of the linear statistics is invariant with respect to a\nnon-standard cyclic permutation group under the null hypothesis.The power can\nbe further enhanced by solving a secondary non-linear travelling salesman\nproblem, for which the genetic algorithm can find a reasonably good solution.\nExtensive simulation studies show that the CPT has comparable power to existing\ntests. When testing for a single contrast of coefficients, an exact confidence\ninterval can be obtained by inverting the test. Furthermore, we provide a\nselective yet extensive literature review of the century-long efforts on this\nproblem, highlighting the novelty of our test.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 21:26:17 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 18:29:15 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lei", "Lihua", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1907.06145", "submitter": "Olanrewaju Akande", "authors": "Olanrewaju Akande, Gabriel Madson, D. Sunshine Hillygus and Jerome P.\n  Reiter", "title": "Leveraging Auxiliary Information on Marginal Distributions in\n  Nonignorable Models for Item and Unit Nonresponse", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, government agencies and survey organizations know the population\ncounts or percentages for some of the variables in a survey. These may be\navailable from auxiliary sources, for example, administrative databases or\nother high quality surveys. We present and illustrate a model-based framework\nfor leveraging such auxiliary marginal information when handling unit and item\nnonresponse. We show how one can use the margins to specify different\nmissingness mechanisms for each type of nonresponse. We use the framework to\nimpute missing values in voter turnout in a subset of data from the U.S.\\\nCurrent Population Survey (CPS). In doing so, we examine the sensitivity of\nresults to different assumptions about the unit and item nonresponse.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 23:02:47 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 19:57:14 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 23:56:32 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Akande", "Olanrewaju", ""], ["Madson", "Gabriel", ""], ["Hillygus", "D. Sunshine", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1907.06244", "submitter": "Daniel McDonald", "authors": "Daniel J. McDonald and Michael McBride and Yupeng Gu and Christopher\n  Raphael", "title": "Markov-switching State Space Models for Uncovering Musical\n  Interpretation", "comments": "33 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For concertgoers, musical interpretation is the most important factor in\ndetermining whether or not we enjoy a classical performance. Every performance\nincludes mistakes---intonation issues, a lost note, an unpleasant sound---but\nthese are all easily forgotten (or unnoticed) when a performer engages her\naudience, imbuing a piece with novel emotional content beyond the vague\ninstructions inscribed on the printed page. While music teachers use imagery or\nheuristic guidelines to motivate interpretive decisions, combining these vague\ninstructions to create a convincing performance remains the domain of the\nperformer, subject to the whims of the moment, technical fluency, and taste. In\nthis research, we use data from the CHARM Mazurka Project---forty-six\nprofessional recordings of Chopin's Mazurka Op. 63 No. 3 by consumate\nartists---with the goal of elucidating musically interpretable performance\ndecisions. Using information on the inter-onset intervals of the note attacks\nin the recordings, we apply functional data analysis techniques enriched with\nprior information gained from music theory to discover relevant features and\nperform hierarchical clustering. The resulting clusters suggest methods for\ninforming music instruction, discovering listening preferences, and analyzing\nperformances.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 16:25:08 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["McDonald", "Daniel J.", ""], ["McBride", "Michael", ""], ["Gu", "Yupeng", ""], ["Raphael", "Christopher", ""]]}, {"id": "1907.06336", "submitter": "Luke Prendergast", "authors": "Dilanka S. Dedduwakumara, Luke A. Prendergast and Robert G. Staudte", "title": "An efficient estimator of the parameters of the Generalized Lambda\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the four generalized lambda distribution parameters is not\nstraightforward, and available estimators that perform best have large\ncomputation times. In this paper, we introduce a simple two-step estimator of\nthe parameters that is comparatively very quick to compute and performs well\nwhen compared with other methods. This computational efficiency makes the use\nof bootstrapping to obtain interval estimators for the parameters possible.\nSimulations are used to assess the performance of the new estimators and\napplications to several data sets are included.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 05:33:38 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 06:58:20 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Dedduwakumara", "Dilanka S.", ""], ["Prendergast", "Luke A.", ""], ["Staudte", "Robert G.", ""]]}, {"id": "1907.06455", "submitter": "Radu Stoica", "authors": "R. Stoica (Universit\\'e de Lorraine), Madalina Deaconu\n  (TOSCA-NGE-POST), Anne Philippe (UN), Lluis Hurtado", "title": "Shadow Simulated Annealing algorithm: a new tool for global optimisation\n  and statistical inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new global optimisation method that applies to a family\nof criteria that are not entirely known. This family includes the criteria\nobtained from the class of posteriors that have nor-malising constants that are\nanalytically not tractable. The procedure applies to posterior probability\ndensities that are continuously differen-tiable with respect to their\nparameters. The proposed approach avoids the re-sampling needed for the\nclassical Monte Carlo maximum likelihood inference, while providing the missing\nconvergence properties of the ABC based methods. Results on simulated data and\nreal data are presented. The real data application fits an inhomogeneous area\ninteraction point process to cosmological data. The obtained results validate\ntwo important aspects of the galaxies distribution in our Universe : proximity\nof the galaxies from the cosmic filament network together with territorial\nclustering at given range of interactions. Finally, conclusions and\nperspectives are depicted.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 11:58:33 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Stoica", "R.", "", "Universit\u00e9 de Lorraine"], ["Deaconu", "Madalina", "", "TOSCA-NGE-POST"], ["Philippe", "Anne", "", "UN"], ["Hurtado", "Lluis", ""]]}, {"id": "1907.06477", "submitter": "Ray Bai", "authors": "Ray Bai, Mary R. Boland, Yong Chen", "title": "Fast Algorithms and Theory for High-Dimensional Bayesian Varying\n  Coefficient Models", "comments": "71 pages, 7 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric varying coefficient (NVC) models are useful for modeling\ntime-varying effects on responses that are measured repeatedly. In this paper,\nwe introduce the nonparametric varying coefficient spike-and-slab lasso\n(NVC-SSL) for Bayesian estimation and variable selection in NVC models. The\nNVC-SSL simultaneously selects and estimates the significant varying\ncoefficients, while also accounting for temporal correlations. Our model can be\nimplemented using a computationally efficient expectation-maximization (EM)\nalgorithm. We also employ a simple method to make our model robust to\nmisspecification of the temporal correlation structure. In contrast to\nfrequentist approaches, little is known about the large-sample properties for\nBayesian NVC models when the dimension of the covariates $p$ grows much faster\nthan sample size $n$. In this paper, we derive posterior contraction rates for\nthe NVC-SSL model when $p \\gg n$ under both correct specification and\nmisspecification of the temporal correlation structure. Thus, our results are\nderived under weaker assumptions than those seen in other high-dimensional NVC\nmodels which assume independent and identically distributed (iid) random\nerrors. Finally, we illustrate our methodology through simulation studies and\ndata analysis. Our method is implemented in the publicly available R package\nNVCSSL.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 12:52:27 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 11:45:41 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 18:01:38 GMT"}, {"version": "v4", "created": "Thu, 28 May 2020 06:55:38 GMT"}, {"version": "v5", "created": "Tue, 7 Jul 2020 19:56:59 GMT"}, {"version": "v6", "created": "Sun, 26 Jul 2020 18:36:31 GMT"}, {"version": "v7", "created": "Wed, 29 Jul 2020 03:21:51 GMT"}, {"version": "v8", "created": "Tue, 22 Dec 2020 22:30:58 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Bai", "Ray", ""], ["Boland", "Mary R.", ""], ["Chen", "Yong", ""]]}, {"id": "1907.06560", "submitter": "Brady West PhD", "authors": "Brady T. West, James Wagner, Stephanie Coffey, Michael R. Elliott", "title": "Deriving Priors for Bayesian Prediction of Daily Response Propensity in\n  Responsive Survey Design: Historical Data Analysis vs. Literature Review", "comments": "42 pages, 9 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Responsive Survey Design (RSD) aims to increase the efficiency of survey data\ncollection via live monitoring of paradata and the introduction of protocol\nchanges when survey errors and increased costs seem imminent. Daily predictions\nof response propensity for all active sampled cases are among the most\nimportant quantities for live monitoring of data collection outcomes, making\nsound predictions of these propensities essential for the success of RSD.\nBecause it relies on real-time updates of prior beliefs about key design\nquantities, such as predicted response propensities, RSD stands to benefit from\nBayesian approaches. However, empirical evidence of the merits of these\napproaches is lacking in the literature, and the derivation of informative\nprior distributions is required for these approaches to be effective. In this\npaper, we evaluate the ability of two approaches to deriving prior\ndistributions for the coefficients defining daily response propensity models to\nimprove predictions of daily response propensity in a real data collection\nemploying RSD. The first approach involves analyses of historical data from the\nsame survey, and the second approach involves literature review. We find that\nBayesian methods based on these two approaches result in higher-quality\npredictions of response propensity than more standard approaches ignoring prior\ninformation. This is especially true during the early-to-middle periods of data\ncollection when interventions are often considered in an RSD framework.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:05:00 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 22:16:08 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 20:54:11 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 13:17:42 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["West", "Brady T.", ""], ["Wagner", "James", ""], ["Coffey", "Stephanie", ""], ["Elliott", "Michael R.", ""]]}, {"id": "1907.06567", "submitter": "Shirley Liao", "authors": "Shirley Liao, Lucas Henneman, Corwin Zigler", "title": "Posterior Predictive Treatment Assignment Methods for Causal Inference\n  in the Context of Time-Varying Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models (MSM) with inverse probability weighting (IPW) are\nused to estimate causal effects of time-varying treatments, but can result in\nerratic finite-sample performance when there is low overlap in covariate\ndistributions across different treatment patterns. Modifications to IPW which\ntarget the average treatment effect (ATE) estimand either introduce bias or\nrely on unverifiable parametric assumptions and extrapolation. This paper\nextends an alternate estimand, the average treatment effect on the overlap\npopulation (ATO) which is estimated on a sub-population with a reasonable\nprobability of receiving alternate treatment patterns in time-varying treatment\nsettings. To estimate the ATO within a MSM framework, this paper extends a\nstochastic pruning method based on the posterior predictive treatment\nassignment (PPTA) as well as a weighting analogue to the time-varying treatment\nsetting. Simulations demonstrate the performance of these extensions compared\nagainst IPW and stabilized weighting with regard to bias, efficiency and\ncoverage. Finally, an analysis using these methods is performed on Medicare\nbeneficiaries residing across 18,480 zip codes in the U.S. to evaluate the\neffect of coal-fired power plant emissions exposure on ischemic heart disease\nhospitalization, accounting for seasonal patterns that lead to change in\ntreatment over time.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:17:13 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Liao", "Shirley", ""], ["Henneman", "Lucas", ""], ["Zigler", "Corwin", ""]]}, {"id": "1907.06606", "submitter": "Alex Rodrigo dos Santos Sousa", "authors": "Alex Rodrigo dos Santos Sousa, Nancy Lopes Garcia, Branislav Vidakovic", "title": "Bayesian Wavelet Shrinkage with Beta Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In wavelet shrinkage and thresholding, most of the standard techniques do not\nconsider information that wavelet coefficients might be bounded, although\ninformation about bounded energy in signals can be readily available. To\naddress this, we present a Bayesian approach for shrinkage of bounded wavelet\ncoefficients in the context of non-parametric regression. We propose the use of\na zero-contaminated beta distribution with a support symmetric around zero as\nthe prior distribution for the location parameter in the wavelet domain in\nmodels with additive gaussian errors. The hyperparameters of the proposed model\nare closely related to the shrinkage level, which facilitates their elicitation\nand interpretation. For signals with a low signal-to-noise ratio, the\nassociated Bayesian shrinkage rules provide significant improvement in\nperformance in simulation studies when compared with standard techniques.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 17:08:48 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 18:14:58 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 17:56:09 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Sousa", "Alex Rodrigo dos Santos", ""], ["Garcia", "Nancy Lopes", ""], ["Vidakovic", "Branislav", ""]]}, {"id": "1907.06622", "submitter": "Christopher Walters", "authors": "Patrick Kline and Christopher Walters", "title": "Audits as Evidence: Experiments, Ensembles, and Enforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop tools for utilizing correspondence experiments to detect illegal\ndiscrimination by individual employers. Employers violate US employment law if\ntheir propensity to contact applicants depends on protected characteristics\nsuch as race or sex. We establish identification of higher moments of the\ncausal effects of protected characteristics on callback rates as a function of\nthe number of fictitious applications sent to each job ad. These moments are\nused to bound the fraction of jobs that illegally discriminate. Applying our\nresults to three experimental datasets, we find evidence of significant\nemployer heterogeneity in discriminatory behavior, with the standard deviation\nof gaps in job-specific callback probabilities across protected groups\naveraging roughly twice the mean gap. In a recent experiment manipulating\nracially distinctive names, we estimate that at least 85% of jobs that contact\nboth of two white applications and neither of two black applications are\nengaged in illegal discrimination. To assess the tradeoff between type I and II\nerrors presented by these patterns, we consider the performance of a series of\ndecision rules for investigating suspicious callback behavior under a simple\ntwo-type model that rationalizes the experimental data. Though, in our\npreferred specification, only 17% of employers are estimated to discriminate on\nthe basis of race, we find that an experiment sending 10 applications to each\njob would enable accurate detection of 7-10% of discriminators while falsely\naccusing fewer than 0.2% of non-discriminators. A minimax decision rule\nacknowledging partial identification of the joint distribution of callback\nrates yields higher error rates but more investigations than our baseline\ntwo-type model. Our results suggest illegal labor market discrimination can be\nreliably monitored with relatively small modifications to existing audit\ndesigns.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 17:43:02 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 16:26:53 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Kline", "Patrick", ""], ["Walters", "Christopher", ""]]}, {"id": "1907.06734", "submitter": "Margarita Moreno-Betancur", "authors": "Margarita Moreno-Betancur, Paul Moran, Denise Becker, George C Patton,\n  John B Carlin", "title": "Mediation effects that emulate a target randomised trial:\n  Simulation-based evaluation of ill-defined interventions on multiple\n  mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many epidemiological questions concern potential interventions to alter the\npathways presumed to mediate an association. For example, we consider a study\nthat investigates the benefit of interventions in young adulthood for\nameliorating the poorer mid-life psychosocial outcomes of adolescent\nself-harmers relative to their healthy peers. Two methodological challenges\narise. Firstly, mediation methods have hitherto mostly focused on the elusive\ntask of discovering pathways, rather than on the evaluation of mediator\ninterventions. Secondly, the complexity of such questions is invariably such\nthat there are no existing data on well-defined interventions (i.e. actual\ntreatments, programs, etc.) capturing the populations, outcomes and time-spans\nof interest. Instead, researchers must rely on exposure (non-intervention) data\nto address these questions, such as self-reported substance use and employment.\nWe address the resulting challenges by specifying a target trial addressing\nthree policy-relevant questions, regarding the impacts of hypothetical (rather\nthan actual) interventions that would shift the mediators' distributions\n(separately, jointly or sequentially) to user-specified distributions that can\nbe emulated with the observed data. We then define novel interventional effects\nthat map to this trial, emulating shifts by setting mediators to random draws\nfrom those distributions. We show that estimation using a g-computation method\nis possible under an expanded set of causal assumptions relative to inference\nwith well-defined interventions. These expanded assumptions reflect the lower\nlevel of evidence that is inevitable with ill-defined interventions.\nApplication to the self-harm example using data from the Victorian Adolescent\nHealth Cohort Study illustrates the value of our proposal for informing the\ndesign and evaluation of actual interventions in the future.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:26:50 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 05:09:42 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 08:49:20 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Moreno-Betancur", "Margarita", ""], ["Moran", "Paul", ""], ["Becker", "Denise", ""], ["Patton", "George C", ""], ["Carlin", "John B", ""]]}, {"id": "1907.06759", "submitter": "Trevor Harris", "authors": "Trevor Harris, James Derek Tucker, Bo Li, Lyndsay Shand", "title": "Elastic depths for detecting shape anomalies in functional data", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2020.1811156", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of depth measures called the elastic depths that can\nbe used to greatly improve shape anomaly detection in functional data. Shape\nanomalies are functions that have considerably different geometric forms or\nfeatures from the rest of the data. Identifying them is generally more\ndifficult than identifying magnitude anomalies because shape anomalies are\noften not distinguishable from the bulk of the data with visualization methods.\nThe proposed elastic depths use the recently developed elastic distances to\ndirectly measure the centrality of functions in the amplitude and phase spaces.\nMeasuring shape outlyingness in these spaces provides a rigorous quantification\nof shape, which gives the elastic depths a strong theoretical and practical\nadvantage over other methods in detecting shape anomalies. A simple boxplot and\nthresholding method is introduced to identify shape anomalies using the elastic\ndepths. We assess the elastic depth's detection skill on simulated shape\noutlier scenarios and compare them against popular shape anomaly detectors.\nFinally, we use hurricane trajectories to demonstrate the elastic depth\nmethodology on manifold valued functional data. Supplementary materials,\nincluding additional simulations, data examples, and an R-package are available\nonline.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 21:17:45 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 19:54:50 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 20:27:49 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Harris", "Trevor", ""], ["Tucker", "James Derek", ""], ["Li", "Bo", ""], ["Shand", "Lyndsay", ""]]}, {"id": "1907.06770", "submitter": "Siyu Heng", "authors": "Siyu Heng, Hyunseung Kang, Dylan S. Small, Colin B. Fogarty", "title": "Increasing Power for Observational Studies of Aberrant Response: An\n  Adaptive Approach", "comments": "83 pages, 1 figure, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many observational studies, the interest is in the effect of treatment on\nbad, aberrant outcomes rather than the average outcome. For such settings, the\ntraditional approach is to define a dichotomous outcome indicating aberration\nfrom a continuous score and use the Mantel-Haenszel test with matched data. For\nexample, studies of determinants of poor child growth use the World Health\nOrganization's definition of child stunting being height-for-age z-score $\\leq\n-2$. The traditional approach may lose power because it discards potentially\nuseful information about the severity of aberration. We develop an adaptive\napproach that makes use of this information and asymptotically dominates the\ntraditional approach. We develop our approach in two parts. First, we develop\nan aberrant rank approach in matched observational studies and prove a novel\ndesign sensitivity formula enabling its asymptotic comparison with the\nMantel-Haenszel test under various settings. Second, we develop a new, general\nadaptive approach, the two-stage programming method, and use it to adaptively\ncombine the aberrant rank test and the Mantel-Haenszel test. We apply our\napproach to a study of the effect of teenage pregnancy on stunting.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 22:07:52 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 03:46:42 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 21:02:54 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Heng", "Siyu", ""], ["Kang", "Hyunseung", ""], ["Small", "Dylan S.", ""], ["Fogarty", "Colin B.", ""]]}, {"id": "1907.06810", "submitter": "Zifeng Zhao", "authors": "Zifeng Zhao and Chun Yip Yau", "title": "Alternating Pruned Dynamic Programming for Multiple Epidemic\n  Change-Point Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of multiple change-point detection for a\nunivariate sequence under the epidemic setting, where the behavior of the\nsequence alternates between a common normal state and different epidemic\nstates. This is a non-trivial generalization of the classical (single) epidemic\nchange-point testing problem. To explicitly incorporate the alternating\nstructure of the problem, we propose a novel model selection based approach for\nsimultaneous inference on both change-points and alternating states. Using the\nsame spirit as profile likelihood, we develop a two-stage alternating pruned\ndynamic programming algorithm, which conducts efficient and exact optimization\nof the model selection criteria and has $O(n^2)$ as the worst case\ncomputational cost. As demonstrated by extensive numerical experiments,\ncompared to classical general-purpose multiple change-point detection\nprocedures, the proposed method improves accuracy for both change-point\nestimation and model parameter estimation. We further show promising\napplications of the proposed algorithm to multiple testing with locally\nclustered signals, and demonstrate its advantages over existing methods in\nlarge scale multiple testing, in DNA copy number variation detection, and in\noceanographic study.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 02:30:10 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 03:24:24 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 23:50:44 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Zhao", "Zifeng", ""], ["Yau", "Chun Yip", ""]]}, {"id": "1907.06856", "submitter": "Iman Jaljuli", "authors": "Iman Jaljuli, Yoav Benjamini, Liat Shenhav, Orestis Panagiotou, Ruth\n  Heller", "title": "Quantifying replicability and consistency in systematic reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic reviews of interventions are important tools for synthesizing\nevidence from multiple studies. They serve to increase power and improve\nprecision, in the same way that larger studies can do, but also to establish\nthe consistency of effects and replicability of results across studies which\nare not identical. In this work we suggest to incorporate replicability\nanalysis tools to quantify the consistency and conflict. These are offered both\nfor the fixed-effect and for the random-effects meta-analyses. We motivate and\ndemonstrate our approach and its implications by examples from systematic\nreviews from the Cochrane library, and offer a way to incorporate our\nsuggestions in their standard reporting system.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 06:16:16 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 15:03:15 GMT"}, {"version": "v3", "created": "Sun, 18 Apr 2021 22:35:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jaljuli", "Iman", ""], ["Benjamini", "Yoav", ""], ["Shenhav", "Liat", ""], ["Panagiotou", "Orestis", ""], ["Heller", "Ruth", ""]]}, {"id": "1907.06933", "submitter": "Eni Musta", "authors": "C\\'ecile Durot and Eni Musta", "title": "On the $L_p$-error of the Grenander-type estimator in the Cox model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Cox regression model and study the asymptotic global behavior\nof the Grenander-type estimator for a monotone baseline hazard function. This\nmodel is not included in the general setting of Durot (2007). However, we show\nthat a similar central limit theorem holds for $L_p$-error of the\nGrenander-type estimator. We also propose a test procedure for a Weibull\nbaseline distribution, based on the $L_p$-distance between the Grenander\nestimator and a parametric estimator of the baseline hazard. Simulation studies\nare performed to investigate the performance of this test.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 10:40:58 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Durot", "C\u00e9cile", ""], ["Musta", "Eni", ""]]}, {"id": "1907.06988", "submitter": "Vitalii Makogin", "authors": "Denis Dresvyanskiy and Tatiana Karaseva and Vitalii Makogin and Sergei\n  Mitrofanov and Claudia Redenbach and Evgeny Spodarev", "title": "Detecting anomalies in fibre systems using 3-dimensional image data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting anomalies in the directional\ndistribution of fibre materials observed in 3D images. We divide the image into\na set of scanning windows and classify them into two clusters: homogeneous\nmaterial and anomaly. Based on a sample of estimated local fibre directions,\nfor each scanning window we compute several classification attributes, namely\nthe coordinate wise means of local fibre directions, the entropy of the\ndirectional distribution, and a combination of them. We also propose a new\nspatial modification of the Stochastic Approximation Expectation-Maximization\n(SAEM) algorithm. Besides the clustering we also consider testing the\nsignificance of anomalies. To this end, we apply a change point technique for\nrandom fields and derive the exact inequalities for tail probabilities of a\ntest statistics. The proposed methodology is first validated on simulated\nimages. Finally, it is applied to a 3D image of a fibre reinforced polymer.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:36:10 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Dresvyanskiy", "Denis", ""], ["Karaseva", "Tatiana", ""], ["Makogin", "Vitalii", ""], ["Mitrofanov", "Sergei", ""], ["Redenbach", "Claudia", ""], ["Spodarev", "Evgeny", ""]]}, {"id": "1907.06994", "submitter": "Faicel Chamroukhi", "authors": "Bao Tuyen Huynh and Faicel Chamroukhi", "title": "Estimation and Feature Selection in Mixtures of Generalized Linear\n  Experts Models", "comments": "arXiv admin note: text overlap with arXiv:1810.12161", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures-of-Experts (MoE) are conditional mixture models that have shown\ntheir performance in modeling heterogeneity in data in many statistical\nlearning approaches for prediction, including regression and classification, as\nwell as for clustering. Their estimation in high-dimensional problems is still\nhowever challenging. We consider the problem of parameter estimation and\nfeature selection in MoE models with different generalized linear experts\nmodels, and propose a regularized maximum likelihood estimation that\nefficiently encourages sparse solutions for heterogeneous data with\nhigh-dimensional predictors. The developed proximal-Newton EM algorithm\nincludes proximal Newton-type procedures to update the model parameter by\nmonotonically maximizing the objective function and allows to perform efficient\nestimation and feature selection. An experimental study shows the good\nperformance of the algorithms in terms of recovering the actual sparse\nsolutions, parameter estimation, and clustering of heterogeneous regression\ndata, compared to the main state-of-the art competitors.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 10:58:31 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Huynh", "Bao Tuyen", ""], ["Chamroukhi", "Faicel", ""]]}, {"id": "1907.07085", "submitter": "Sarah Ouadah", "authors": "C\\'eline L\\'evy-Leduc, Sarah Ouadah and Laure Sansonnet", "title": "Variable selection in sparse high-dimensional GLARMA models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel variable selection approach in the\nframework of sparse high-dimensional GLARMA models. It consists in combining\nthe estimation of the autoregressive moving average (ARMA) coefficients of\nthese models with regularized methods designed for Generalized Linear Models\n(GLM). The properties of our approach are investigated both from a theoretical\nand a numerical point of view. More precisely, we establish in a specific case\nthe consistency of the ARMA part coefficient estimators. We explain how to\nimplement our approach and we show that it is very attractive since it benefits\nfrom a low computational load. We also assess the performance of our\nmethodology using synthetic data and compare it with alternative approaches.\nOur numerical experiments show that combining the estimation of the ARMA part\ncoefficients with regularized methods designed for GLM dramatically improves\nthe variable selection performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:52:56 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 08:04:06 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Ouadah", "Sarah", ""], ["Sansonnet", "Laure", ""]]}, {"id": "1907.07148", "submitter": "Ping Li", "authors": "Martin Slawski, Emanuel Ben-David, Ping Li", "title": "A Two-Stage Approach to Multivariate Linear Regression with Sparsely\n  Mismatched Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tacit assumption in linear regression is that (response, predictor)-pairs\ncorrespond to identical observational units. A series of recent works have\nstudied scenarios in which this assumption is violated under terms such as\n``Unlabeled Sensing and ``Regression with Unknown Permutation''. In this paper,\nwe study the setup of multiple response variables and a notion of mismatches\nthat generalizes permutations in order to allow for missing matches as well as\nfor one-to-many matches. A two-stage method is proposed under the assumption\nthat most pairs are correctly matched. In the first stage, the regression\nparameter is estimated by handling mismatches as contaminations, and\nsubsequently the generalized permutation is estimated by a basic variant of\nmatching. The approach is both computationally convenient and equipped with\nfavorable statistical guarantees. Specifically, it is shown that the conditions\nfor permutation recovery become considerably less stringent as the number of\nresponses $m$ per observation increase. Particularly, for $m = \\Omega(\\log n)$,\nthe required signal-to-noise ratio no longer depends on the sample size $n$.\nNumerical results on synthetic and real data are presented to support the main\nfindings of our analysis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:12:19 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 15:28:13 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Slawski", "Martin", ""], ["Ben-David", "Emanuel", ""], ["Li", "Ping", ""]]}, {"id": "1907.07271", "submitter": "Guido Imbens", "authors": "Guido W. Imbens", "title": "Potential Outcome and Directed Acyclic Graph Approaches to Causality:\n  Relevance for Empirical Practice in Economics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this essay I discuss potential outcome and graphical approaches to\ncausality, and their relevance for empirical work in economics. I review some\nof the work on directed acyclic graphs, including the recent \"The Book of Why,\"\nby Pearl and MacKenzie. I also discuss the potential outcome framework\ndeveloped by Rubin and coauthors, building on work by Neyman. I then discuss\nthe relative merits of these approaches for empirical work in economics,\nfocusing on the questions each answer well, and why much of the the work in\neconomics is closer in spirit to the potential outcome framework.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 21:50:22 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 23:31:55 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Imbens", "Guido W.", ""]]}, {"id": "1907.07309", "submitter": "Tao Zhang", "authors": "Tao Zhang, Yang Ning and David Ruppert", "title": "Optimal Sampling for Generalized Linear Models under Measurement\n  Constraints", "comments": "52 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under \"measurement constraints,\" responses are expensive to measure and\ninitially unavailable on most of records in the dataset, but the covariates are\navailable for the entire dataset. Our goal is to sample a relatively small\nportion of the dataset where the expensive responses will be measured and the\nresultant sampling estimator is statistically efficient. Measurement\nconstraints require the sampling probabilities can only depend on a very small\nset of the responses. A sampling procedure that uses responses at most only on\na small pilot sample will be called \"response-free.\" We propose a response-free\nsampling procedure \\mbox{(OSUMC)} for generalized linear models (GLMs). Using\nthe A-optimality criterion, i.e., the trace of the asymptotic variance, the\nresultant estimator is statistically efficient within a class of sampling\nestimators. We establish the unconditional asymptotic distribution of a general\nclass of response-free sampling estimators. This result is novel compared with\nthe existing conditional results obtained by conditioning on both covariates\nand responses. Under our unconditional framework, the subsamples are no longer\nindependent and new martingale techniques are developed for our asymptotic\ntheory. We further derive the A-optimal response-free sampling distribution.\nSince this distribution depends on population level quantities, we propose the\nOptimal Sampling Under Measurement Constraints (OSUMC) algorithm to approximate\nthe theoretical optimal sampling. Finally, we conduct an intensive empirical\nstudy to demonstrate the advantages of OSUMC algorithm over existing methods in\nboth statistical and computational perspectives.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 02:46:36 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 04:05:22 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 02:31:47 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhang", "Tao", ""], ["Ning", "Yang", ""], ["Ruppert", "David", ""]]}, {"id": "1907.07320", "submitter": "Sonja Petrovic", "authors": "Sonja Petrovi\\'c", "title": "What is... a Markov basis?", "comments": "AMS Notices piece", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short piece defines a Markov basis. The aim is to introduce the\nstatistical concept to mathematicians.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 03:35:36 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Petrovi\u0107", "Sonja", ""]]}, {"id": "1907.07395", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Sayed H. Kadhem and Aristidis K. Nikoloulopoulos", "title": "Factor copula models for mixed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop factor copula models for analysing the dependence among mixed\ncontinuous and discrete responses. Factor copula models are canonical vine\ncopulas that involve both observed and latent variables, hence they allow tail,\nasymmetric and non-linear dependence. They can be explained as conditional\nindependence models with latent variables that don't necessarily have an\nadditive latent structure. We focus on important issues that would interest the\nsocial data analyst, such as model selection and goodness-of-fit. Our general\nmethodology is demonstrated with an extensive simulation study and illustrated\nby re-analysing three mixed response datasets. Our study suggests that there\ncan be a substantial improvement over the standard factor model for mixed data\nand makes the argument for moving to factor copula models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 08:49:38 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 19:02:19 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Kadhem", "Sayed H.", ""], ["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1907.07523", "submitter": "Anne Sabourin", "authors": "Ma\\\"el Chiapino (LTCI), St\\'ephan Cl\\'emen\\c{c}on (LTCI), Vincent\n  Feuillard, Anne Sabourin (LTCI)", "title": "A Multivariate Extreme Value Theory Approach to Anomaly Clustering and\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide variety of situations, anomalies in the behaviour of a complex\nsystem, whose health is monitored through the observation of a random vector X\n= (X1,. .. , X d) valued in R d , correspond to the simultaneous occurrence of\nextreme values for certain subgroups $\\alpha$ $\\subset$ {1,. .. , d} of\nvariables Xj. Under the heavy-tail assumption, which is precisely appropriate\nfor modeling these phenomena, statistical methods relying on multivariate\nextreme value theory have been developed in the past few years for identifying\nsuch events/subgroups. This paper exploits this approach much further by means\nof a novel mixture model that permits to describe the distribution of extremal\nobservations and where the anomaly type $\\alpha$ is viewed as a latent\nvariable. One may then take advantage of the model by assigning to any extreme\npoint a posterior probability for each anomaly type $\\alpha$, defining\nimplicitly a similarity measure between anomalies. It is explained at length\nhow the latter permits to cluster extreme observations and obtain an\ninformative planar representation of anomalies using standard graph-mining\ntools. The relevance and usefulness of the clustering and 2-d visual display\nthus designed is illustrated on simulated datasets and on real observations as\nwell, in the aeronautics application domain.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:48:58 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Chiapino", "Ma\u00ebl", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"], ["Feuillard", "Vincent", "", "LTCI"], ["Sabourin", "Anne", "", "LTCI"]]}, {"id": "1907.07592", "submitter": "Spencer Woody", "authors": "Carlos Carvalho, Avi Feller, Jared Murray, Spencer Woody, and David\n  Yeager", "title": "Assessing Treatment Effect Variation in Observational Studies: Results\n  from a Data Challenge", "comments": "15 pages, 4 figures, 2018 Atlantic Causal Inference Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of methods aim to assess the challenging question of\ntreatment effect variation in observational studies. This special section of\n\"Observational Studies\" reports the results of a workshop conducted at the 2018\nAtlantic Causal Inference Conference designed to understand the similarities\nand differences across these methods. We invited eight groups of researchers to\nanalyze a synthetic observational data set that was generated using a recent\nlarge-scale randomized trial in education. Overall, participants employed a\ndiverse set of methods, ranging from matching and flexible outcome modeling to\nsemiparametric estimation and ensemble approaches. While there was broad\nconsensus on the topline estimate, there were also large differences in\nestimated treatment effect moderation. This highlights the fact that estimating\nvarying treatment effects in observational studies is often more challenging\nthan estimating the average treatment effect alone. We suggest several\ndirections for future work arising from this workshop.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:44:38 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 17:55:31 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 20:45:12 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Carvalho", "Carlos", ""], ["Feller", "Avi", ""], ["Murray", "Jared", ""], ["Woody", "Spencer", ""], ["Yeager", "David", ""]]}, {"id": "1907.07957", "submitter": "John Mbotwa", "authors": "John Mbotwa, Marc de Kamps, Paul D. Baxter, Mark S. Gilthorpe", "title": "Application of Cox Model to predict the survival of patients with\n  Chronic Heart Failure: A latent class regression approach", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prediction models that are used in medical research fail to accurately\npredict health outcomes due to methodological limitations. Using routinely\ncollected patient data, we explore the use of a Cox proportional hazard (PH)\nmodel within a latent class framework to model survival of patients with\nchronic heart failure (CHF). We identify subgroups of patients based on their\nrisk with the aid of available covariates. We allow each subgroup to have its\nown risk model.We choose an optimum number of classes based on the reported\nBayesian information criteria (BIC). We assess the discriminative ability of\nthe chosen model using an area under the receiver operating characteristic\ncurve (AUC) for all the cross-validated and bootstrapped samples.We conduct a\nsimulation study to compare the predictive performance of our models. Our\nproposed latent class model outperforms the standard one class Cox PH model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 09:50:37 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Mbotwa", "John", ""], ["de Kamps", "Marc", ""], ["Baxter", "Paul D.", ""], ["Gilthorpe", "Mark S.", ""]]}, {"id": "1907.08074", "submitter": "Stanislav Nagy", "authors": "Fr\\'ed\\'eric Ferraty and Stanislav Nagy", "title": "Scalar-on-function local linear regression and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regressing a scalar response on a random function is nowadays a common\nsituation. In the nonparametric setting, this paper paves the way for making\nthe local linear regression based on a projection approach a prominent method\nfor solving this regression problem. Our asymptotic results demonstrate that\nthe functional local linear regression outperforms its functional local\nconstant counterpart. Beyond the estimation of the regression operator itself,\nthe local linear regression is also a useful tool for predicting the functional\nderivative of the regression operator, a promising mathematical object on its\nown. The local linear estimator of the functional derivative is shown to be\nconsistent. On simulated datasets we illustrate good finite sample properties\nof both proposed methods. On a real data example of a single-functional index\nmodel we indicate how the functional derivative of the regression operator\nprovides an original and fast, widely applicable estimating method.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:28:18 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Ferraty", "Fr\u00e9d\u00e9ric", ""], ["Nagy", "Stanislav", ""]]}, {"id": "1907.08242", "submitter": "Zad Rafi", "authors": "Zad Rafi", "title": "Misplaced Confidence in Observed Power", "comments": "1 page; 0 figures; 7 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently published randomized controlled trial in JAMA investigated the\nimpact of the selective serotonin reuptake inhibitor, escitalopram, on the risk\nof major adverse events (MACE). The authors estimated a hazard ratio (HR) of\n0.69 (95% CI: 0.49, 0.96; $p$ = 0.03) and then attempted to calculate how much\nstatistical power their study (test) had attained, and used this measure to\nassess how reliable their results were. Here, we discuss why this approach,\nalong with other post-hoc power analyses, are highly misleading.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:46:24 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 23:17:45 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 06:55:15 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 01:17:11 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Rafi", "Zad", ""]]}, {"id": "1907.08245", "submitter": "Leonardo Bottolo", "authors": "Angelos Alexopoulos and Leonardo Bottolo", "title": "Bayesian Variable Selection for Gaussian copula regression models", "comments": "39 pages main paper and 21 pages Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a novel Bayesian method to select important predictors in\nregression models with multiple responses of diverse types. A sparse Gaussian\ncopula regression model is used to account for the multivariate dependencies\nbetween any combination of discrete and/or continuous responses and their\nassociation with a set of predictors. We utilize the parameter expansion for\ndata augmentation strategy to construct a Markov chain Monte Carlo algorithm\nfor the estimation of the parameters and the latent variables of the model.\nBased on a centered parametrization of the Gaussian latent variables, we design\na fixed-dimensional proposal distribution to update jointly the latent binary\nvectors of important predictors and the corresponding non-zero regression\ncoefficients. For Gaussian responses and for outcomes that can be modeled as a\ndependent version of a Gaussian response, this proposal leads to a\nMetropolis-Hastings step that allows an efficient exploration of the\npredictors' model space. The proposed strategy is tested on simulated data and\napplied to real data sets in which the responses consist of low-intensity\ncounts, binary, ordinal and continuous variables.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:55:21 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 17:23:00 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Alexopoulos", "Angelos", ""], ["Bottolo", "Leonardo", ""]]}, {"id": "1907.08264", "submitter": "\\'Alvaro Ignacio Riquelme", "authors": "Alvaro I. Riquelme and Julian M. Ortiz", "title": "A general approach to the assessment of uncertainty in volumes by using\n  the multi-Gaussian model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this research is to derive an approach to assess uncertainty in\nan arbitrary volume conditioned by sampling data, without using geostatistical\nsimulation. We have accomplished this goal by deriving an numerical tool\nsuitable for any probabilistic distribution of the sample data. For this, we\nhave worked with an extension of the traditional multi-Gaussian model, allowing\nus to obtain a formulation that makes explicit the dependence of the\nuncertainty in the arbitrary volume from the grades within the volume, the\nspatial correlation of the data and the conditioning values. A Kriging of the\nGaussian values is the only requirement to obtain not only conditional local\nmeans and variances but also the complete local distributions at any support,\nin an easy and straightforward way.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 20:06:19 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Riquelme", "Alvaro I.", ""], ["Ortiz", "Julian M.", ""]]}, {"id": "1907.08360", "submitter": "Zhonglei Wang", "authors": "Xiaojun Mao, Zhonglei Wang and Shu Yang", "title": "Matrix Completion for Survey Data Prediction with Multivariate\n  Missingness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The National Health and Nutrition Examination Survey (NHANES) studies the\nnutritional and health status over the whole U.S. population with comprehensive\nphysical examinations and questionnaires. However, survey data analyses become\nchallenging due to inevitable missingness in almost all variables. In this\npaper, we develop a new imputation method to deal with multivariate missingness\nat random using matrix completion. In contrast to existing imputation schemes\neither conducting row-wise or column-wise imputation, we treat the data matrix\nas a whole which allows exploiting both row and column patterns to impute the\nmissing values in the whole data matrix at one time. We adopt a\ncolumn-space-decomposition model for the population data matrix with\neasy-to-obtain demographic data as covariates and a low-rank structured\nresidual matrix. A unique challenge arises due to lack of identification of\nparameters in the sample data matrix. We propose a projection strategy to\nuniquely identify the parameters and corresponding penalized estimators, which\nare computationally efficient and possess desired statistical properties. The\nsimulation study shows that the doubly robust estimator using the proposed\nmatrix completion for imputation has smaller mean squared error than other\ncompetitors. To demonstrate practical relevance, we apply the proposed method\nto the 2015-2016 NHANES Questionnaire Data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 03:29:46 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 18:40:25 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Mao", "Xiaojun", ""], ["Wang", "Zhonglei", ""], ["Yang", "Shu", ""]]}, {"id": "1907.08372", "submitter": "David Stoffer", "authors": "Chen Gong and David S. Stoffer", "title": "An Approach to Efficient Fitting of Univariate and Multivariate\n  Stochastic Volatility Models", "comments": "Code here: https://github.com/nickpoison/Stochastic-Volatility-Models", "journal-ref": null, "doi": "10.13140/RG.2.2.29926.37440", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The stochastic volatility model is a popular tool for modeling the volatility\nof assets. The model is a nonlinear and non-Gaussian state space model, and\nconsequently is difficult to fit. Many approaches, both classical and Bayesian,\nhave been developed that rely on numerically intensive techniques such as\nquasi-maximum likelihood estimation and Markov chain Monte Carlo (MCMC).\nConvergence and mixing problems still plague MCMC algorithms when drawing\nsamples sequentially from the posterior distributions. While particle Gibbs\nmethods have been successful when applied to nonlinear or non-Gaussian state\nspace models in general, slow convergence still haunts the technique when\napplied specifically to stochastic volatility models. We present an approach\nthat couples particle Gibbs with ancestral sampling and joint parameter\nsampling that ameliorates the slow convergence and mixing problems when fitting\nboth univariate and multivariate stochastic volatility models. We demonstrate\nthe enhanced method on various numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 04:58:10 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Gong", "Chen", ""], ["Stoffer", "David S.", ""]]}, {"id": "1907.08414", "submitter": "Guo Yu", "authors": "Guo Yu, Jacob Bien, Ryan Tibshirani", "title": "Reluctant Interaction Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Including pairwise interactions between the predictors of a regression model\ncan produce better predicting models. However, to fit such interaction models\non typical data sets in biology and other fields can often require solving\nenormous variable selection problems with billions of interactions. The scale\nof such problems demands methods that are computationally cheap (both in time\nand memory) yet still have sound statistical properties. Motivated by these\nlarge-scale problem sizes, we adopt a very simple guiding principle: One should\nprefer a main effect over an interaction if all else is equal. This\n\"reluctance\" to interactions, while reminiscent of the hierarchy principle for\ninteractions, is much less restrictive. We design a computationally efficient\nmethod built upon this principle and provide theoretical results indicating\nfavorable statistical properties. Empirical results show dramatic computational\nimprovement without sacrificing statistical properties. For example, the\nproposed method can solve a problem with 10 billion interactions with 5-fold\ncross-validation in under 7 hours on a single CPU.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 08:56:07 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Yu", "Guo", ""], ["Bien", "Jacob", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1907.08415", "submitter": "Wen Wei Loh", "authors": "Wen Wei Loh, Beatrijs Moerkerke, Tom Loeys, Stijn Vansteelandt", "title": "Heterogeneous Indirect Effects for Multiple Mediators using\n  Interventional Effect Models", "comments": null, "journal-ref": null, "doi": "10.1515/em-2020-0023", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing an exposure effect on an outcome into separate natural indirect\neffects through multiple mediators requires strict assumptions, such as\ncorrectly postulating the causal structure of the mediators, and no unmeasured\nconfounding among the mediators. In contrast, interventional indirect effects\nfor multiple mediators can be identified even when - as often - the mediators\neither have an unknown causal structure, or share unmeasured common causes, or\nboth. Existing estimation methods for interventional indirect effects require\ncalculating each distinct indirect effect in turn. This can quickly become\nunwieldy or unfeasible, especially when investigating indirect effect measures\nthat may be modified by observed baseline characteristics. In this article, we\nintroduce simplified estimation procedures for such heterogeneous\ninterventional indirect effects using interventional effect models.\nInterventional effect models are a class of marginal structural models that\nencode the interventional indirect effects as causal model parameters, thus\nreadily permitting effect modification by baseline covariates using\n(statistical) interaction terms. The mediators and outcome can be continuous or\nnoncontinuous. We propose two estimation procedures: one using inverse\nweighting by the counterfactual mediator density or mass functions, and another\nusing Monte Carlo integration. The former has the advantage of not requiring an\noutcome model, but is susceptible to finite sample biases due to highly\nvariable weights. The latter has the advantage of consistent estimation under a\ncorrectly specified (parametric) outcome model, but is susceptible to biases\ndue to extrapolation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 08:56:51 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 13:10:53 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 06:55:10 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Loh", "Wen Wei", ""], ["Moerkerke", "Beatrijs", ""], ["Loeys", "Tom", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1907.08566", "submitter": "Peter Tait A", "authors": "Peter A. Tait, Paul D. McNicholas and Joyce Obeid", "title": "Clustering Higher Order Data: An Application to Pediatric Multi-variable\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical activity levels are an important predictor of cardiovascular health\nand increasingly being measured by sensors, like accelerometers. Accelerometers\nproduce rich multivariate data that can inform important clinical decisions\nrelated to individual patients and public health. The CHAMPION study, a study\nof youth with chronic inflammatory conditions, aims to determine the links\nbetween heart health, inflammation, physical activity, and fitness. The\naccelerometer data from CHAMPION is represented as 4-dimensional arrays, and a\nfinite mixture of multidimensional arrays model is developed for clustering.\nThe use of model-based clustering for multidimensional arrays has thus far been\nlimited to two-dimensional arrays, i.e., matrices or order-two tensors, and the\nwork in this paper can also be seen as an approach for clustering D-dimensional\narrays for D > 2 or, in other words, for clustering order-D tensors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 16:40:01 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 21:33:56 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 17:09:19 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Tait", "Peter A.", ""], ["McNicholas", "Paul D.", ""], ["Obeid", "Joyce", ""]]}, {"id": "1907.08627", "submitter": "Paula Saavedra-Nieves", "authors": "A. Rodr\\'iguez-Casal and P. Saavedra-Nieves", "title": "Extent of occurrence reconstruction using a new data-driven support\n  estimator", "comments": "arXiv admin note: text overlap with arXiv:1404.7397", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of points from some unknown distribution, we propose a\nnew data-driven method for estimating its probability support S. Under the mild\nassumption that S is r-convex, the smallest r-convex set which contains the\nsample points is the natural estimator. The main problem for using this\nestimator in practice is that r is an unknown geometric characteristic of the\nset S. A stochastic algorithm is proposed for determining an optimal estimate\nof r from the data under mild regularity assumptions on the density function.\nThe resulting data-driven reconstruction of S attains the same convergence\nrates as the convex hull for estimating convex sets, but under a much more\nflexible smoothness shape condition. The new support estimator will be used for\nreconstructing the extent of occurrence of an assemblage of invasive plant\nspecies in the Azores archipelago.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 16:43:12 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Rodr\u00edguez-Casal", "A.", ""], ["Saavedra-Nieves", "P.", ""]]}, {"id": "1907.08713", "submitter": "Yunxiao Chen", "authors": "Haoran Zhang, Yunxiao Chen, and Xiaoou Li", "title": "A Note on Exploratory Item Factor Analysis by Singular Value\n  Decomposition", "comments": "43 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a singular value decomposition (SVD) algorithm given in Chen et\nal. (2019b) for exploratory Item Factor Analysis (IFA). This algorithm\nestimates a multidimensional IFA model by SVD and was used to obtain a starting\npoint for joint maximum likelihood estimation in Chen et al. (2019b). Thanks to\nthe analytic and computational properties of SVD, this algorithm guarantees a\nunique solution and has computational advantage over other exploratory IFA\nmethods. Its computational advantage becomes significant when the numbers of\nrespondents, items, and factors are all large. This algorithm can be viewed as\na generalization of principal component analysis (PCA) to binary data. In this\nnote, we provide the statistical underpinning of the algorithm. In particular,\nwe show its statistical consistency under the same double asymptotic setting as\nin Chen et al. (2019b). We also demonstrate how this algorithm provides a scree\nplot for investigating the number of factors and provide its asymptotic theory.\nFurther extensions of the algorithm are discussed. Finally, simulation studies\nsuggest that the algorithm has good finite sample performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 22:28:02 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 10:23:05 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 09:19:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhang", "Haoran", ""], ["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""]]}, {"id": "1907.08733", "submitter": "Wenjie Zhao", "authors": "Wenjie Zhao, Raquel Prado", "title": "Efficient Bayesian PARCOR Approaches for Dynamic Modeling of\n  Multivariate Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian lattice filtering and smoothing approach is proposed for fast and\naccurate modeling and inference in multivariate non-stationary time series.\nThis approach offers computational feasibility and interpretable time-frequency\nanalysis in the multivariate context. The proposed framework allows us to\nobtain posterior estimates of the time-varying spectral densities of individual\ntime series components, as well as posterior measurements of the time-frequency\nrelationships across multiple components, such as time-varying coherence and\npartial coherence.\n  The proposed formulation considers multivariate dynamic linear models (MDLMs)\non the forward and backward time-varying partial autocorrelation coefficients\n(TV-VPARCOR). Computationally expensive schemes for posterior inference on the\nmultivariate dynamic PARCOR model are avoided using approximations in the MDLM\ncontext. Approximate inference on the corresponding time-varying vector\nautoregressive (TV-VAR) coefficients is obtained via Whittle's algorithm. A key\naspect of the proposed TV-VPARCOR representations is that they are of lower\ndimension, and therefore more efficient, than TV-VAR representations. The\nperformance of the TV-VPARCOR models is illustrated in simulation studies and\nin the analysis of multivariate non-stationary temporal data arising in\nneuroscience and environmental applications. Model performance is evaluated\nusing goodness-of-fit measurements in the time-frequency domain and also by\nassessing the quality of short-term forecasting.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 01:20:19 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhao", "Wenjie", ""], ["Prado", "Raquel", ""]]}, {"id": "1907.09053", "submitter": "Evan Rosenman", "authors": "Evan Rosenman", "title": "Some New Results for Poisson Binomial Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of ecological inference, in which individual-level\ncovariates are known, but labeled data is available only at the aggregate\nlevel. The intended application is modeling voter preferences in elections. In\nRosenman and Viswanathan (2018), we proposed modeling individual voter\nprobabilities via a logistic regression, and posing the problem as a maximum\nlikelihood estimation for the parameter vector beta. The likelihood is a\nPoisson binomial, the distribution of the sum of independent but not\nidentically distributed Bernoulli variables, though we approximate it with a\nheteroscedastic Gaussian for computational efficiency. Here, we extend the\nprior work by proving results about the existence of the MLE and the curvature\nof this likelihood, which is not log-concave in general. We further demonstrate\nthe utility of our method on a real data example. Using data on voters in\nMorris County, NJ, we demonstrate that our approach outperforms other\necological inference methods in predicting a related, but known outcome:\nwhether an individual votes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 23:42:07 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Rosenman", "Evan", ""]]}, {"id": "1907.09090", "submitter": "Taylor Brown", "authors": "Taylor R. Brown, Timothy L. McMurry, Alexander Langevin", "title": "A Pseudo-Marginal Metropolis-Hastings Algorithm for Estimating\n  Generalized Linear Models in the Presence of Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The missing data issue often complicates the task of estimating generalized\nlinear models (GLMs). We describe why the pseudo-marginal Metropolis-Hastings\nalgorithm, used in this setting, is an effective strategy for parameter\nestimation. This approach requires fewer assumptions, it provides joint\ninferences on the parameters in the likelihood, the covariate model, and the\nparameters of the missingness-mechanism, and there is no logical inconsistency\nof assuming that there are multiple posterior distributions. Moreover, this\napproach is asymptotically exact, just like most other Markov chain Monte Carlo\ntechniques. We discuss computing strategies, conduct a simulation study\ndemonstrating how standard errors change as a function of percent missingness,\nand we use our approach on a \"real-world\" data set to describe how a collection\nof variables influences the car crash outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 02:39:57 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Brown", "Taylor R.", ""], ["McMurry", "Timothy L.", ""], ["Langevin", "Alexander", ""]]}, {"id": "1907.09162", "submitter": "Martin Magris", "authors": "Martin Magris", "title": "On the simulation of the Hawkes process via Lambert-W functions", "comments": "A short discussion paper. 7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been developed for the simulation of the Hawkes process.\nThe oldest approach is the inverse sampling transform (ITS) suggested in\n\\citep{ozaki1979maximum}, but rapidly abandoned in favor of more efficient\nalternatives. This manuscript shows that the ITS approach can be conveniently\ndiscussed in terms of Lambert-W functions. An optimized and efficient\nimplementation suggests that this approach is computationally more performing\nthan more recent alternatives available for the simulation of the Hawkes\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 07:28:22 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Magris", "Martin", ""]]}, {"id": "1907.09164", "submitter": "Catherine Matias", "authors": "Tabea Rebafka (LPSM (UMR\\_8001)), Estelle Kuhn (MaIAGE), Catherine\n  Matias (LPSM (UMR\\_8001))", "title": "Properties of the Stochastic Approximation EM Algorithm with Mini-batch\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deal with very large datasets a mini-batch version of the Monte Carlo\nMarkov Chain Stochastic Approximation Expectation-Maximization algorithm for\ngeneral latent variable models is proposed. For exponential models the\nalgorithm is shown to be convergent under classicalconditions as the number of\niterations increases. Numerical experiments illustrate the performance of the\nmini-batch algorithm in various models.In particular, we highlight that\nmini-batch sampling results in an important speed-up of the convergence of the\nsequence of estimators generated by the algorithm. Moreover, insights on the\neffect of the mini-batch size on the limit distribution are presented. Finally,\nwe illustrate how to use mini-batch sampling in practice to improve results\nwhen a constraint on the computing time is given.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 07:29:55 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 13:43:00 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 16:26:19 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Rebafka", "Tabea", "", "LPSM"], ["Kuhn", "Estelle", "", "MaIAGE"], ["Matias", "Catherine", "", "LPSM"]]}, {"id": "1907.09424", "submitter": "Xuefei Lu", "authors": "Isadora Antoniano-Villalobos, Emanuele Borgonovo, Xuefei Lu", "title": "Bayesian estimation of probabilistic sensitivity measures", "comments": "An Updated Version of the Manuscript is Forthcoming in Statistics and\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer experiments are becoming increasingly important in scientific\ninvestigations. In the presence of uncertainty, analysts employ probabilistic\nsensitivity methods to identify the key-drivers of change in the quantities of\ninterest. Simulation complexity, large dimensionality and long running times\nmay force analysts to make statistical inference at small sample sizes. Methods\ndesigned to estimate probabilistic sensitivity measures at relatively low\ncomputational costs are attracting increasing interest. We propose a fully\nBayesian approach to the estimation of probabilistic sensitivity measures based\non a one-sample design. We discuss, first, new estimators based on placing\npiecewise constant priors on the conditional distributions of the output given\neach input, by partitioning the input space. We then present two alternatives,\nbased on Bayesian non-parametric density estimation, which bypass the need for\npredefined partitions. In all cases, the Bayesian paradigm guarantees the\nquantification of uncertainty in the estimation process through the posterior\ndistribution over the sensitivity measures, without requiring additional\nsimulator evaluations. The performance of the proposed methods is compared to\nthat of traditional point estimators in a series of numerical experiments\ncomprising synthetic but challenging simulators, as well as a realistic\napplication. $\\textit{An Updated Version of the Manuscript is Forthcoming in\nStatistics and Computing.}$\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:50:50 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 07:38:10 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Antoniano-Villalobos", "Isadora", ""], ["Borgonovo", "Emanuele", ""], ["Lu", "Xuefei", ""]]}, {"id": "1907.09435", "submitter": "L\\'aszl\\'o N\\'emeth", "authors": "L\\'aszl\\'o N\\'emeth, Zuzana H\\\"ubnerov\\'a, Andr\\'as Zempl\\'eni", "title": "Trend detection in GEV models", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent environmental studies extreme events have a great impact. The\nyearly and monthly maxima of environment related indices can be analysed by the\ntools of extreme value theory. For instance, the monthly maxima of the fire\nweather index in British Columbian forests might be modelled by GEV\ndistribution, but the stationarity of the time series is questionable. This\nproperty can lead us to different approaches to test if there is a significant\ntrend in past few years data or not. An approach is a likelihood ratio based\nprocedure which has favourable asymptotic properties, but for realistic sample\nsizes it might have a large error. In this paper we analyse the properties of\nthe likelihood ratio test for extremes by bootstrap simulations and aim to\ndetermine a minimal required sample size. With the theoretical results we\nre-asses the trends of fire weather index in British Columbian forests.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 17:20:10 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 09:38:20 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 07:00:05 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["N\u00e9meth", "L\u00e1szl\u00f3", ""], ["H\u00fcbnerov\u00e1", "Zuzana", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1907.09477", "submitter": "Nan Zou", "authors": "Nan Zou, Stanislav Volgushev, Axel B\\\"ucher", "title": "Multiple block sizes and overlapping blocks for multivariate time series\n  extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block maxima methods constitute a fundamental part of the statistical toolbox\nin extreme value analysis. However, most of the corresponding theory is derived\nunder the simplifying assumption that block maxima are independent observations\nfrom a genuine extreme value distribution. In practice however, block sizes are\nfinite and observations from different blocks are dependent. Theory respecting\nthe latter complications is not well developed, and, in the multivariate case,\nhas only recently been established for disjoint blocks of a single block size.\nWe show that using overlapping blocks instead of disjoint blocks leads to a\nuniform improvement in the asymptotic variance of the multivariate empirical\ndistribution function of rescaled block maxima and any smooth functionals\nthereof (such as the empirical copula), without any sacrifice in the asymptotic\nbias. We further derive functional central limit theorems for multivariate\nempirical distribution functions and empirical copulas that are uniform in the\nblock size parameter, which seems to be the first result of this kind for\nestimators based on block maxima in general. The theory allows for various\naggregation schemes over multiple block sizes, leading to substantial\nimprovements over the single block length case and opens the door to further\nmethodology developments. In particular, we consider bias correction procedures\nthat can improve the convergence rates of extreme-value estimators and shed\nsome new light on estimation of the second-order parameter when the main\npurpose is bias correction.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:21:59 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zou", "Nan", ""], ["Volgushev", "Stanislav", ""], ["B\u00fccher", "Axel", ""]]}, {"id": "1907.09522", "submitter": "Xialu Liu", "authors": "Xialu Liu and Ting Zhang", "title": "Factor Analysis for High-Dimensional Time Series with Change Point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider change-point latent factor models for high-dimensional time\nseries, where a structural break may exist in the underlying factor structure.\nIn particular, we propose consistent estimators for factor loading spaces\nbefore and after the change point, and the problem of estimating the\nchange-point location is also considered. Compared with existing results on\nchange-point factor analysis of high-dimensional time series, a distinguished\nfeature of the current paper is that our results allow strong cross-sectional\ndependence in the noise process. To accommodate the unknown degree of\ncross-sectional dependence strength, we propose to use self-normalization to\npivotalize the change-point test statistic. Numerical experiments including a\nMonte Carlo simulation study and a real data application are presented to\nillustrate the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 18:43:16 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Liu", "Xialu", ""], ["Zhang", "Ting", ""]]}, {"id": "1907.09553", "submitter": "Andrew Brown", "authors": "Carl Ehrett, D. Andrew Brown, Evan Chodora, Christopher Kitchens, and\n  Sez Atamturktur", "title": "Coupling material and mechanical design processes via computer model\n  calibration", "comments": "20 pages, 7 figures. Supplementary material and computer code\n  available from the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer model calibration typically operates by choosing parameter values in\na computer model so that the model output faithfully predicts reality. By using\nperformance targets in place of observed data, we show that calibration\ntechniques can be repurposed to wed engineering and material design, two\nprocesses that are traditionally carried out separately. This allows materials\nto be designed with specific engineering targets in mind while quantifying the\nassociated sources of uncertainty. We demonstrate our proposed approach by\n\"calibrating\" material design settings to performance targets for a wind\nturbine blade.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:16:55 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 16:44:59 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ehrett", "Carl", ""], ["Brown", "D. Andrew", ""], ["Chodora", "Evan", ""], ["Kitchens", "Christopher", ""], ["Atamturktur", "Sez", ""]]}, {"id": "1907.09565", "submitter": "Ranjan Maitra", "authors": "Geoffrey Z. Thompson and Ranjan Maitra and William Q. Meeker and\n  Ashraf Bastawros", "title": "Classification with the matrix-variate-$t$ distribution", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1080/10618600.2019.1696208", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-variate distributions can intuitively model the dependence structure\nof matrix-valued observations that arise in applications with multivariate time\nseries, spatio-temporal or repeated measures. This paper develops an\nExpectation-Maximization algorithm for discriminant analysis and classification\nwith matrix-variate $t$-distributions. The methodology shows promise on\nsimulated datasets or when applied to the forensic matching of fractured\nsurfaces or the classification of functional Magnetic Resonance, satellite or\nhand gestures images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:44:34 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 22:07:55 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Thompson", "Geoffrey Z.", ""], ["Maitra", "Ranjan", ""], ["Meeker", "William Q.", ""], ["Bastawros", "Ashraf", ""]]}, {"id": "1907.09617", "submitter": "Likun Zhang", "authors": "Likun Zhang, Benjamin A. Shaby, and Jennifer L. Wadsworth", "title": "Hierarchical Transformed Scale Mixtures for Flexible Modeling of Spatial\n  Extremes on Datasets with Many Locations", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1858838", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Flexible spatial models that allow transitions between tail dependence\nclasses have recently appeared in the literature. However, inference for these\nmodels is computationally prohibitive, even in moderate dimensions, due to the\nnecessity of repeatedly evaluating the multivariate Gaussian distribution\nfunction. In this work, we attempt to achieve truly high-dimensional inference\nfor extremes of spatial processes, while retaining the desirable flexibility in\nthe tail dependence structure, by modifying an established class of models\nbased on scale mixtures Gaussian processes. We show that the desired extremal\ndependence properties from the original models are preserved under the\nmodification, and demonstrate that the corresponding Bayesian hierarchical\nmodel does not involve the expensive computation of the multivariate Gaussian\ndistribution function. We fit our model to exceedances of a high threshold, and\nperform coverage analyses and cross-model checks to validate its ability to\ncapture different types of tail characteristics. We use a standard adaptive\nMetropolis algorithm for model fitting, and further accelerate the computation\nvia parallelization and Rcpp. Lastly, we apply the model to a dataset of a fire\nthreat index on the Great Plains region of the US, which is vulnerable to\nmassively destructive wildfires. We find that the joint tail of the fire threat\nindex exhibits a decaying dependence structure that cannot be captured by\nlimiting extreme value models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 22:44:08 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 20:09:32 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhang", "Likun", ""], ["Shaby", "Benjamin A.", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "1907.09716", "submitter": "Claudio Heinrich", "authors": "Claudio Heinrich, Kristoffer H. Hellton, Alex Lenkoski, Thordis L.\n  Thorarinsdottir", "title": "Multivariate postprocessing methods for high-dimensional seasonal\n  weather forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seasonal weather forecasts are crucial for long-term planning in many\npractical situations and skillful forecasts may have substantial economic and\nhumanitarian implications. Current seasonal forecasting models require\nstatistical postprocessing of the output to correct systematic biases and\nunrealistic uncertainty assessments. We propose a multivariate postprocessing\napproach utilizing covariance tapering, combined with a dimension reduction\nstep based on principal component analysis for efficient computation. Our\nproposed technique can correctly and efficiently handle non-stationary,\nnon-isotropic and negatively correlated spatial error patterns, and is\napplicable on a global scale. Further, a moving average approach to marginal\npostprocessing is shown to flexibly handle trends in biases caused by global\nwarming, and short training periods. In an application to global sea surface\ntemperature forecasts issued by the Norwegian Climate Prediction Model\n(NorCPM), our proposed methodology is shown to outperform known reference\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 06:53:24 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 15:15:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Heinrich", "Claudio", ""], ["Hellton", "Kristoffer H.", ""], ["Lenkoski", "Alex", ""], ["Thorarinsdottir", "Thordis L.", ""]]}, {"id": "1907.09756", "submitter": "Daniela Marella", "authors": "Giuseppe Bove and Pier Luigi Conti and Daniela Marella", "title": "An ordinal measure of interrater absolute agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A measure of interrater absolute agreement for ordinal scales is proposed\ncapitalizing on the dispersion index for ordinal variables proposed by Giuseppe\nLeti. The procedure allows to avoid the problem of restriction of variance that\nsometimes affect traditional measures of interrater agreement in different\nfields of application. An unbiased estimator of the proposed measure is\nintroduced and its sampling properties are investigated. In order to construct\nconfidence intervals for interrater absolute agreement both asymptotic results\nand bootstrapping methods are used and their performance is evaluated.\nSimulated data are employed to demonstrate the accuracy and practical utility\nof the new procedure for assessing agreement. Finally, an application to a real\ncase is provided.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 08:40:47 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Bove", "Giuseppe", ""], ["Conti", "Pier Luigi", ""], ["Marella", "Daniela", ""]]}, {"id": "1907.09771", "submitter": "Sophie Donnet Dr", "authors": "Sophie Donnet, St\\'ephane Robin", "title": "Bayesian inference for network Poisson models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the analysis of ecological interaction networks.\nPoisson stochastic blockmodels are widely used in this field to decipher the\nstructure that underlies a weighted network, while accounting for covariate\neffects. Efficient algorithms based on variational approximations exist for\nfrequentist inference, but without statistical guaranties as for the resulting\nestimates. In absence of variational Bayes estimates, we show that a good proxy\nof the posterior distribution can be straightforwardly derived from the\nfrequentist variational estimation procedure, using a Laplace approximation. We\nuse this proxy to sample from the true posterior distribution via a sequential\nMonte-Carlo algorithm. As shown in the simulation study, the efficiency of the\nposterior sampling is greatly improved by the accuracy of the approximate\nposterior distribution. The proposed procedure can be easily extended to other\nlatent variable models. We use this methodology to assess the influence of\navailable covariates on the organization of two ecological networks, as well as\nthe existence of a residual interaction structure.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 09:02:30 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Donnet", "Sophie", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1907.09808", "submitter": "Haiyan Liu", "authors": "Haiyan Liu, Georgios Aivaliotis, Jeanine Houwing-Duistermaat", "title": "On estimation of the effect lag of predictors and prediction in\n  functional linear model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a functional linear model to predict a response using multiple\nfunctional and longitudinal predictors and to estimate the effect lags of\npredictors. The coefficient functions are written as the expansion of a basis\nsystem (e.g. functional principal components, splines), and the coefficients of\nthe fixed basis functions are estimated via optimizing a penalization\ncriterion. Then time lags are determined by simultaneously searching on a prior\ngrid mesh based on minimization of prediction error criterion. Moreover,\nmathematical properties of the estimated parameters and predicted responses are\nstudied and performance of the method is evaluated by extensive simulations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 10:42:57 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Liu", "Haiyan", ""], ["Aivaliotis", "Georgios", ""], ["Houwing-Duistermaat", "Jeanine", ""]]}, {"id": "1907.09851", "submitter": "Samuel Wiqvist", "authors": "Samuel Wiqvist, Andrew Golightly, Ashleigh T. McLean, Umberto Picchini", "title": "Efficient inference for stochastic differential equation mixed-effects\n  models using correlated particle pseudo-marginal algorithms", "comments": "Accepted manuscript. DOI: https://doi.org/10.1016/j.csda.2020.107151.\n  31 pages, 15 Figures and . 31 pages, 15 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Stochastic differential equation mixed-effects models (SDEMEMs) are flexible\nhierarchical models that are able to account for random variability inherent in\nthe underlying time-dynamics, as well as the variability between experimental\nunits and, optionally, account for measurement error. Fully Bayesian inference\nfor state-space SDEMEMs is performed, using data at discrete times that may be\nincomplete and subject to measurement error. However, the inference problem is\ncomplicated by the typical intractability of the observed data likelihood which\nmotivates the use of sampling-based approaches such as Markov chain Monte\nCarlo. A Gibbs sampler is proposed to target the marginal posterior of all\nparameter values of interest. The algorithm is made computationally efficient\nthrough careful use of blocking strategies and correlated pseudo-marginal\nMetropolis-Hastings steps within the Gibbs scheme. The resulting methodology is\nflexible and is able to deal with a large class of SDEMEMs. The methodology is\ndemonstrated on three case studies, including tumor growth dynamics and\nneuronal data. The gains in terms of increased computational efficiency are\nmodel and data dependent, but unless bespoke sampling strategies requiring\nanalytical derivations are possible for a given model, we generally observe an\nefficiency increase of one order of magnitude when using correlated particle\nmethods together with our blocked-Gibbs strategy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 13:02:22 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 11:20:22 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 10:58:16 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 14:44:26 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Wiqvist", "Samuel", ""], ["Golightly", "Andrew", ""], ["McLean", "Ashleigh T.", ""], ["Picchini", "Umberto", ""]]}, {"id": "1907.09864", "submitter": "Nicola Kuczewski", "authors": "Marjorie Fonnesu and Nicola Kuczewski", "title": "Doubts on the efficacy of outliers correction methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the utilisation of different methods of outliers correction has been\nshown to counteract the inferential error produced by the presence of\ncontaminating data not belonging to the studied population; the effects\nproduced by their utilisation when samples do not contain contaminating\noutliers are less clear. Here a simulation approach shows that the most popular\nmethods of outliers correction (2 Sigma, 3 Sigma, MAD, IQR, Grubbs and\nwinsorizing) worsen the inferential evaluation of the studied population in\nthis condition, in particular producing an inflation of Type I error and\nincreasing the error committed in estimating the population mean and STD. We\nshow that those methods that have the highest efficacy in counteract the\ninflation of Type I and Type II errors in the presence of contaminating\noutliers also produce the stronger increase of false positive results in their\nabsence, suggesting that the systematic utilisation of methods for outliers\ncorrection risk to produce more harmful than beneficial effect on statistical\ninference. We finally propose that the safest way to deal with the presence of\noutliers for statistical comparisons is the utilisation of non-parametric tests\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 13:20:09 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 13:25:34 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Fonnesu", "Marjorie", ""], ["Kuczewski", "Nicola", ""]]}, {"id": "1907.10012", "submitter": "Chao Gao", "authors": "Haoyang Liu, Chao Gao, Richard J. Samworth", "title": "Minimax rates in sparse, high-dimensional changepoint detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the detection of a sparse change in a high-dimensional mean vector\nas a minimax testing problem. Our first main contribution is to derive the\nexact minimax testing rate across all parameter regimes for $n$ independent,\n$p$-variate Gaussian observations. This rate exhibits a phase transition when\nthe sparsity level is of order $\\sqrt{p \\log \\log (8n)}$ and has a very\ndelicate dependence on the sample size: in a certain sparsity regime it\ninvolves a triple iterated logarithmic factor in~$n$. Further, in a dense\nasymptotic regime, we identify the sharp leading constant, while in the\ncorresponding sparse asymptotic regime, this constant is determined to within a\nfactor of $\\sqrt{2}$. Extensions that cover spatial and temporal dependence,\nprimarily in the dense case, are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:16:05 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 18:34:23 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Liu", "Haoyang", ""], ["Gao", "Chao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1907.10109", "submitter": "Andrew Finley Dr.", "authors": "Shinichiro Shirota, Andrew O. Finley, Bruce D. Cook, Sudipto Banerjee", "title": "Conjugate Nearest Neighbor Gaussian Process Models for Efficient\n  Statistical Interpolation of Large Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in spatial statistics is the analysis for massive\nspatially-referenced data sets. Such analyses often proceed from Gaussian\nprocess specifications that can produce rich and robust inference, but involve\ndense covariance matrices that lack computationally exploitable structures. The\nmatrix computations required for fitting such models involve floating point\noperations in cubic order of the number of spatial locations and dynamic memory\nstorage in quadratic order. Recent developments in spatial statistics offer a\nvariety of massively scalable approaches. Bayesian inference and hierarchical\nmodels, in particular, have gained popularity due to their richness and\nflexibility in accommodating spatial processes. Our current contribution is to\nprovide computationally efficient exact algorithms for spatial interpolation of\nmassive data sets using scalable spatial processes. We combine low-rank\nGaussian processes with efficient sparse approximations. Following recent work\nby [1], we model the low-rank process using a Gaussian predictive process (GPP)\nand the residual process as a sparsity-inducing nearest-neighbor Gaussian\nprocess (NNGP). A key contribution here is to implement these models using\nexact conjugate Bayesian modeling to avoid expensive iterative algorithms.\nThrough the simulation studies, we evaluate performance of the proposed\napproach and the robustness of our models, especially for long range\nprediction. We implement our approaches for remotely sensed light detection and\nranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit\n(TIU) in a remote portion of Interior Alaska.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 19:36:27 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Finley", "Andrew O.", ""], ["Cook", "Bruce D.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1907.10115", "submitter": "Sofia Ruiz Suarez", "authors": "Sofia Ruiz-Suarez, Vianey Leos-Barajas, Ignacio Alvarez-Castro, Juan\n  M. Morales", "title": "Approximate Bayesian inference for a \"steps and turns\" continuous-time\n  random walk observed at regular time intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of animal movement is challenging because it is a process modulated\nby many factors acting at different spatial and temporal scales. Several models\nhave been proposed which differ primarily in the temporal conceptualization,\nnamely continuous and discrete time formulations. Naturally, animal movement\noccurs in continuous time but we tend to observe it at fixed time intervals. To\naccount for the temporal mismatch between observations and movement decisions,\nwe used a state-space model where movement decisions (steps and turns) are made\nin continuous time. The movement process is then observed at regular time\nintervals. As the likelihood function of this state-space model turned out to\nbe complex to calculate yet simulating data is straightforward, we conduct\ninference using a few variations of Approximate Bayesian Computation (ABC). We\nexplore the applicability of these methods as a function of the discrepancy\nbetween the temporal scale of the observations and that of the movement process\nin a simulation study. We demonstrate the application of this model to a real\ntrajectory of a sheep that was reconstructed in high resolution using\ninformation from magnetometer and GPS devices. Our results suggest that\naccurate estimates can be obtained when the observations are less than 5 times\nthe average time between changes in movement direction. The state-space model\nused here allowed us to connect the scales of the observations and movement\ndecisions in an intuitive and easy to interpret way. Our findings underscore\nthe idea that the time scale at which animal movement decisions are made needs\nto be considered when designing data collection protocols, and that sometimes\nhigh-frequency data may not be necessary to have good estimates of certain\nmovement processes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:01:02 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Ruiz-Suarez", "Sofia", ""], ["Leos-Barajas", "Vianey", ""], ["Alvarez-Castro", "Ignacio", ""], ["Morales", "Juan M.", ""]]}, {"id": "1907.10117", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek", "title": "Simulating an infinite mean waiting time", "comments": null, "journal-ref": "Mathematica Applicanda (Matematyka Stosowana) 47(1): 93-102, 2019", "doi": "10.14708/ma.v47i1.6476", "report-no": null, "categories": "stat.ME q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a hybrid method to simulate the return time to the initial state\nin a critical-case birth--death process. The expected value of this return time\nis infinite, but its distribution asymptotically follows a power-law. Hence,\nthe simulation approach is to directly simulate the process, unless the\nsimulated time exceeds some threshold and if it does, draw the return time from\nthe tail of the power law.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:04:11 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 06:55:49 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Bartoszek", "Krzysztof", ""]]}, {"id": "1907.10176", "submitter": "Etienne Roquain", "authors": "Tabea Rebafka, Etienne Roquain, Fanny Villers", "title": "Graph inference with clustering and false discovery rate control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a noisy version of the stochastic block model (NSBM) is\nintroduced and we investigate the three following statistical inferences in\nthis model: estimation of the model parameters, clustering of the nodes and\nidentification of the underlying graph. While the two first inferences are done\nby using a variational expectation-maximization (VEM) algorithm, the graph\ninference is done by controlling the false discovery rate (FDR), that is, the\naverage proportion of errors among the edges declared significant, and by\nmaximizing the true discovery rate (TDR), that is, the average proportion of\nedges declared significant among the true edges. Provided that the VEM\nalgorithm provides reliable parameter estimates and clustering, we\ntheoretically show that our procedure does control the FDR while satisfying an\noptimal TDR property, up to remainder terms that become small when the size of\nthe graph grows. Numerical experiments show that our method outperforms the\nclassical FDR controlling methods that ignore the underlying SBM topology. In\naddition, these simulations demonstrate that the FDR/TDR properties of our\nmethod are robust to model mis-specification, that is, are essentially\nmaintained outside our model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 23:21:30 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Rebafka", "Tabea", ""], ["Roquain", "Etienne", ""], ["Villers", "Fanny", ""]]}, {"id": "1907.10187", "submitter": "Boris Beranger", "authors": "B. Beranger, A. G. Stephenson, S. A. Sisson", "title": "High-dimensional inference using the extremal skew-$t$ process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes are a popular tool for the study of environmental\nextremes, and the extremal skew-$t$ process is a general model that allows for\na flexible extremal dependence structure. For inference on max-stable processes\nwith high-dimensional data, exact likelihood-based estimation is\ncomputationally intractable. Composite likelihoods, using lower dimensional\ncomponents, and Stephenson-Tawn likelihoods, using occurrence times of maxima,\nare both attractive methods to circumvent this issue for moderate dimensions.\nIn this article we establish the theoretical formulae for simulations of and\ninference for the extremal skew-$t$ process. We also incorporate the\nStephenson-Tawn concept into the composite likelihood framework, giving greater\nstatistical and computational efficiency for higher-order composite\nlikelihoods. We compare 2-way (pairwise), 3-way (triplewise), 4-way, 5-way and\n10-way composite likelihoods for models of up to 100 dimensions. Furthermore,\nwe propose cdf approximations for the Stephenson-Tawn likelihood function,\nleading to large computational gains, and enabling accurate fitting of models\nin large dimensions in only a few minutes. We illustrate our methodology with\nan application to a 90-dimensional temperature dataset from Melbourne,\nAustralia.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 00:38:45 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 00:27:04 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 07:41:51 GMT"}, {"version": "v4", "created": "Sat, 18 Apr 2020 06:10:19 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Beranger", "B.", ""], ["Stephenson", "A. G.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1907.10207", "submitter": "Mityl Biswas", "authors": "Mityl Biswas, Arnab Maity", "title": "Hypothesis Testing in Nonlinear Function on Scalar Regression with\n  Application to Child Growth Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a kernel machine based hypothesis testing procedure in nonlinear\nfunction-on-scalar regression model. Our research is motivated by the Newborn\nEpigenetic Study (NEST) where the question of interest is whether a\npre-specified group of toxic metals or methylation at any of 9 differentially\nmethylated regions (DMRs) is associated with child growth. We take the child\ngrowth trajectory as the functional response, and model the toxic metal\nmeasurements jointly using a nonlinear function. We use a kernel machine\napproach to model the unknown function and transform the hypothesis of no\neffect to an appropriate variance component test. We demonstrate our proposed\nmethodology using a simulation study and by applying it to analyze the NEST\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 02:06:59 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 05:48:13 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Biswas", "Mityl", ""], ["Maity", "Arnab", ""]]}, {"id": "1907.10287", "submitter": "Jiannan Lu", "authors": "Jiannan Lu, Yunshu Zhang and Peng Ding", "title": "Sharp bounds on the relative treatment effect for ordinal outcomes", "comments": "Accepted by Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For ordinal outcomes, the average treatment effect is often ill-defined and\nhard to interpret. Echoing Agresti and Kateri (2017), we argue that the\nrelative treatment effect can be a useful measure especially for ordinal\noutcomes, which is defined as $\\gamma = \\mathrm{pr}\\{ Y_i(1) > Y_i(0) \\} -\n\\mathrm{pr}\\{ Y_i(1) < Y_i(0) \\}$, with $Y_i(1)$ and $Y_i(0)$ being the\npotential outcomes of unit $i$ under treatment and control, respectively. Given\nthe marginal distributions of the potential outcomes, we derive the sharp\nbounds on $\\gamma,$ which are identifiable parameters based on the observed\ndata. Agresti and Kateri (2017) focused on modeling strategies under the\nassumption of independent potential outcomes, but we allow for arbitrary\ndependence.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:57:33 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 16:03:25 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Lu", "Jiannan", ""], ["Zhang", "Yunshu", ""], ["Ding", "Peng", ""]]}, {"id": "1907.10426", "submitter": "Janet van Niekerk Dr", "authors": "Janet van Niekerk, Haakon Bakka, Haavard Rue and Olaf Schenk", "title": "New frontiers in Bayesian modeling using the INLA package in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The INLA package provides a tool for computationally efficient Bayesian\nmodeling and inference for various widely used models, more formally the class\nof latent Gaussian models. It is a non-sampling based framework which provides\napproximate results for Bayesian inference, using sparse matrices. The swift\nuptake of this framework for Bayesian modeling is rooted in the computational\nefficiency of the approach and catalyzed by the demand presented by the big\ndata era. In this paper, we present new developments within the INLA package\nwith the aim to provide a computationally efficient mechanism for the Bayesian\ninference of relevant challenging situations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:11:06 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 05:52:01 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["van Niekerk", "Janet", ""], ["Bakka", "Haakon", ""], ["Rue", "Haavard", ""], ["Schenk", "Olaf", ""]]}, {"id": "1907.10448", "submitter": "Leo Duan", "authors": "Leo L. Duan", "title": "Transport Monte Carlo: High-Accuracy Posterior Approximation via Random\n  Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian applications, there is a huge interest in rapid and accurate\nestimation of the posterior distribution, particularly for high dimensional or\nhierarchical models. In this article, we propose to use optimization to solve\nfor a joint distribution (random transport plan) between two random variables,\n$\\theta$ from the posterior distribution and $\\beta$ from the simple\nmultivariate uniform. Specifically, we obtain an approximate estimate of the\nconditional distribution $\\Pi(\\beta\\mid \\theta)$ as an infinite mixture of\nsimple location-scale changes; applying the Bayes' theorem,\n$\\Pi(\\theta\\mid\\beta)$ can be sampled as one of the reversed transforms from\nthe uniform, with the weight proportional to the posterior density/mass\nfunction. This produces independent random samples with high approximation\naccuracy, as well as nice theoretic guarantees. Our method shows compelling\nadvantages in performance and accuracy, compared to the state-of-the-art Markov\nchain Monte Carlo and approximations such as variational Bayes and normalizing\nflow. We illustrate this approach via several challenging applications, such as\nsampling from multi-modal distribution, estimating sparse signals in high\ndimension, and soft-thresholding of a graph with a prior on the degrees.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:47:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 04:09:32 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 01:28:07 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 18:08:19 GMT"}, {"version": "v5", "created": "Thu, 30 Jul 2020 18:07:48 GMT"}, {"version": "v6", "created": "Wed, 10 Mar 2021 19:51:28 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Duan", "Leo L.", ""]]}, {"id": "1907.10565", "submitter": "Takeru Matsuda", "authors": "Takeru Matsuda, Yuto Miyatake", "title": "Estimation of ordinary differential equation models with discretization\n  error quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parameter estimation of ordinary differential equation (ODE)\nmodels from noisy observations. For this problem, one conventional approach is\nto fit numerical solutions (e.g., Euler, Runge--Kutta) of ODEs to data.\nHowever, such a method does not account for the discretization error in\nnumerical solutions and has limited estimation accuracy. In this study, we\ndevelop an estimation method that quantifies the discretization error based on\ndata. The key idea is to model the discretization error as random variables and\nestimate their variance simultaneously with the ODE parameter. The proposed\nmethod has the form of iteratively reweighted least squares, where the\ndiscretization error variance is updated with the isotonic regression algorithm\nand the ODE parameter is updated by solving a weighted least squares problem\nusing the adjoint system. Experimental results demonstrate that the proposed\nmethod attains robust estimation with at least comparable accuracy to the\nconventional method by successfully quantifying the reliability of the\nnumerical solutions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 17:10:21 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 04:59:30 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 03:01:35 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 03:28:17 GMT"}, {"version": "v5", "created": "Mon, 4 Jan 2021 02:49:42 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Matsuda", "Takeru", ""], ["Miyatake", "Yuto", ""]]}, {"id": "1907.10569", "submitter": "Tianyuan Guan", "authors": "Tianyuan Guan, M. Khorshed Alam and M. Bhaskara Rao", "title": "Sample Size Calculations in Simple Linear Regression: Trials and\n  Tribulations", "comments": "20 pages, 1 figure and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem tackled in this paper is the determination of sample size for a\ngiven level and power in the context of a simple linear regression model. At a\ntechnical level, the simple linear regression model is a five-parameter model.\nIt is natural to base sample size calculations on the least squares' estimator\nof the slope parameter of the model. Nuisance parameters such as the variance\nof the predictor X and conditional variance of the response Y create problems\nin the calculations. The current approaches in the literature are not\nilluminating. One approach is based on the conditional distribution of the\nestimator of the slope parameter given the data on the predictor X. Another\napproach is based on the sample correlation coefficient. We overcome the\nproblems by determining the exact unconditional distribution of the test\nstatistic built on the estimator of the slope parameter. The exact\nunconditional distribution alleviates difficulties to some extent in the\ncomputation of sample sizes. On the other hand, the test based on the sample\ncorrelation coefficient of X and Y avoids the problems besetting the test based\non the slope parameter. However, we lose intuitive interpretation that comes\nwith the slope parameter. Surprisingly, we see that the sample size that comes\nfrom the correlation test works in synchronization with the one that comes from\nthe test built upon the slope parameter in a broad array of settings.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 17:21:17 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Guan", "Tianyuan", ""], ["Alam", "M. Khorshed", ""], ["Rao", "M. Bhaskara", ""]]}, {"id": "1907.10597", "submitter": "Roy Schwartz", "authors": "Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni", "title": "Green AI", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computations required for deep learning research have been doubling every\nfew months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].\nThese computations have a surprisingly large carbon footprint [38]. Ironically,\ndeep learning was inspired by the human brain, which is remarkably energy\nefficient. Moreover, the financial cost of the computations can make it\ndifficult for academics, students, and researchers, in particular those from\nemerging economies, to engage in deep learning research.\n  This position paper advocates a practical solution by making efficiency an\nevaluation criterion for research alongside accuracy and related measures. In\naddition, we propose reporting the financial cost or \"price tag\" of developing,\ntraining, and running models to provide baselines for the investigation of\nincreasingly efficient methods. Our goal is to make AI both greener and more\ninclusive---enabling any inspired undergraduate with a laptop to write\nhigh-quality research papers. Green AI is an emerging focus at the Allen\nInstitute for AI.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 19:36:18 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 02:54:44 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 20:09:57 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Schwartz", "Roy", ""], ["Dodge", "Jesse", ""], ["Smith", "Noah A.", ""], ["Etzioni", "Oren", ""]]}, {"id": "1907.10722", "submitter": "Benjamin Ackerman", "authors": "Benjamin Ackerman, Juned Siddique, Elizabeth A. Stuart", "title": "Transportability of Outcome Measurement Error Correction: from\n  Validation Studies to Intervention Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many lifestyle intervention trials depend on collecting self-reported\noutcomes, like dietary intake, to assess the intervention's effectiveness.\nSelf-reported outcome measures are subject to measurement error, which could\nimpact treatment effect estimation. External validation studies measure both\nself-reported outcomes and an accompanying biomarker, and can therefore be used\nfor measurement error correction. Most validation data, though, are only\nrelevant for outcomes under control conditions. Statistical methods have been\ndeveloped to use external validation data to correct for outcome measurement\nerror under control, and then conduct sensitivity analyses around the error\nunder treatment to obtain estimates of the corrected average treatment effect.\nHowever, an assumption underlying this approach is that the measurement error\nstructure of the outcome is the same in both the validation sample and the\nintervention trial, so that the error correction is transportable to the trial.\nThis may not always be a valid assumption to make. In this paper, we propose an\napproach that adjusts the validation sample to better resemble the trial sample\nand thus leads to more transportable measurement error corrections. We also\nformally investigate when bias due to poor transportability may arise. Lastly,\nwe examine the method performance using simulation, and illustrate them using\nPREMIER, a multi-arm lifestyle intervention trial measuring self-reported\nsodium intake as an outcome, and OPEN, a validation study that measures both\nself-reported diet and urinary biomarkers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:58:53 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 16:11:55 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 19:04:59 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Ackerman", "Benjamin", ""], ["Siddique", "Juned", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "1907.10821", "submitter": "Keith Levin", "authors": "Keith Levin, Elizaveta Levina", "title": "Bootstrapping Networks with Latent Space Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core problem in statistical network analysis is to develop network\nanalogues of classical techniques. The problem of bootstrapping network data\nstands out as especially challenging, since typically one observes only a\nsingle network, rather than a sample. Here we propose two methods for obtaining\nbootstrap samples for networks drawn from latent space models. The first method\ngenerates bootstrap replicates of network statistics that can be represented as\nU-statistics in the latent positions, and avoids actually constructing new\nbootstrapped networks. The second method generates bootstrap replicates of\nwhole networks, and thus can be used for bootstrapping any network function.\nCommonly studied network quantities that can be represented as U-statistics\ninclude many popular summaries, such as average degree and subgraph counts, but\nother equally popular summaries, such as the clustering coefficient, are not\nexpressible as U-statistics and thus require the second bootstrap method. Under\nthe assumption of a random dot product graph, a type of latent space network\nmodel, we show consistency of the proposed bootstrap methods. We give\nmotivating examples throughout and demonstrate the effectiveness of our methods\non synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 03:40:27 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Levin", "Keith", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1907.10829", "submitter": "Paromita Dubey", "authors": "Paromita Dubey and Hans-Georg M\\\"uller", "title": "Functional Models for Time-Varying Random Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, samples of time-varying object data such as time-varying\nnetworks that are not in a vector space have been increasingly collected. These\ndata can be viewed as elements of a general metric space that lacks local or\nglobal linear structure and therefore common approaches that have been used\nwith great success for the analysis of functional data, such as functional\nprincipal component analysis, cannot be applied. In this paper we propose\nmetric covariance, a novel association measure for paired object data lying in\na metric space $(\\Omega,d)$ that we use to define a metric auto-covariance\nfunction for a sample of random $\\Omega$-valued curves, where $\\Omega$\ngenerally will not have a vector space or manifold structure. The proposed\nmetric auto-covariance function is non-negative definite when the squared\nsemimetric $d^2$ is of negative type. Then the eigenfunctions of the linear\noperator with the auto-covariance function as kernel can be used as building\nblocks for an object functional principal component analysis for\n$\\Omega$-valued functional data, including time-varying probability\ndistributions, covariance matrices and time-dynamic networks. Analogues of\nfunctional principal components for time-varying objects are obtained by\napplying Fr\\'echet means and projections of distance functions of the random\nobject trajectories in the directions of the eigenfunctions, leading to\nreal-valued Fr\\'echet scores. Using the notion of generalized Fr\\'echet\nintegrals, we construct object functional principal components that lie in the\nmetric space $\\Omega$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 04:06:21 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 16:56:53 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Dubey", "Paromita", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1907.10867", "submitter": "Nicole Erler", "authors": "Nicole S. Erler, Dimitris Rizopoulos and Emmanuel M. E. H. Lesaffre", "title": "JointAI: Joint Analysis and Imputation of Incomplete Data in R", "comments": "imputation, Bayesian, missing covariates, non-linear, interaction,\n  multi-level, survival, joint model R, JAGS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data occur in many types of studies and typically complicate the\nanalysis. Multiple imputation, either using joint modelling or the more\nflexible fully conditional specification approach, are popular and work well in\nstandard settings. In settings involving non-linear associations or\ninteractions, however, incompatibility of the imputation model with the\nanalysis model is an issue often resulting in bias. Similarly, complex outcomes\nsuch as longitudinal or survival outcomes cannot be adequately handled by\nstandard implementations. In this paper, we introduce the R package JointAI,\nwhich utilizes the Bayesian framework to perform simultaneous analysis and\nimputation in regression models with incomplete covariates. Using a fully\nBayesian joint modelling approach it overcomes the issue of uncongeniality\nwhile retaining the attractive flexibility of fully conditional specification\nmultiple imputation by specifying the joint distribution of analysis and\nimputation models as a sequence of univariate models that can be adapted to the\ntype of variable. JointAI provides functions for Bayesian inference with\ngeneralized linear and generalized linear mixed models and extensions thereof\nas well as survival models and joint models for longitudinal and survival data,\nthat take arguments analogous to corresponding well known functions for the\nanalysis of complete data from base R and other packages. Usage and features of\nJointAI are described and illustrated using various examples and the\ntheoretical background is outlined.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 07:19:34 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 10:19:14 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 08:08:46 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Erler", "Nicole S.", ""], ["Rizopoulos", "Dimitris", ""], ["Lesaffre", "Emmanuel M. E. H.", ""]]}, {"id": "1907.10976", "submitter": "Jordi Cort\\'es", "authors": "Jordi Cort\\'es Mart\\'inez and Mois\\`es G\\'omez Mateu and KyungMann Kim\n  and Guadalupe G\\'omez Melis", "title": "Non-constant hazard ratios in randomized controlled trials with\n  composite endpoints", "comments": "17 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The hazard ratio is routinely used as a summary measure to assess the\ntreatment effect in clinical trials with time-to-event endpoints. It is\nfrequently assumed as constant over time although this assumption often does\nnot hold. When the hazard ratio deviates considerably from being constant, the\naverage of its plausible values is not a valid measure of the treatment effect,\ncan be clinically misleading and common sample size formulas are not\nappropriate.\n  In this paper, we study the hazard ratio along time of a two-component\ncomposite endpoint under the assumption that the hazard ratio for each\ncomponent is constant.\n  This work considers two measures for quantifying the non-proportionality of\nthe hazard ratio: the difference $D$ between the maximum and minimum values of\nhazard ratio over time and the relative measure $R$ representing the ratio\nbetween the sample sizes for the minimum detectable and the average effects. We\nillustrate $D$ and $R$ by means of the ZODIAC trial where the primary endpoint\nwas progression-free survival.\n  We have run a simulation study deriving scenarios for different values of the\nhazard ratios, different event rates and different degrees of association\nbetween the components. We illustrate situations that yield non-constant hazard\nratios for the composite endpoints and consider the likely impact on sample\nsize.\n  Results show that the distance between the two component hazard ratios plays\nan important role, especially when they are close to 1. Furthermore, even when\nthe treatment effects for each component are similar, if the two-component\nhazards are markedly different, hazard ratio of the composite is often\nnon-constant.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 11:32:36 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Mart\u00ednez", "Jordi Cort\u00e9s", ""], ["Mateu", "Mois\u00e8s G\u00f3mez", ""], ["Kim", "KyungMann", ""], ["Melis", "Guadalupe G\u00f3mez", ""]]}, {"id": "1907.11017", "submitter": "Imke Botha", "authors": "Imke Botha, Robert Kohn, Christopher Drovandi", "title": "Particle Methods for Stochastic Differential Equation Mixed Effects\n  Models", "comments": "Minor revisions throughout, added a simple running example, some\n  updates to example and results (Sections 5-7), link to code on GitHub is\n  provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter inference for stochastic differential equation mixed effects models\n(SDEMEMs) is a challenging problem. Analytical solutions for these models are\nrarely available, which means that the likelihood is also intractable. In this\ncase, exact inference is possible using the pseudo-marginal method, where the\nintractable likelihood is replaced by its nonnegative unbiased estimate. A\nuseful application of this idea is particle MCMC, which uses a particle filter\nestimate of the likelihood. While the exact posterior is targeted by these\nmethods, a naive implementation for SDEMEMs can be highly inefficient. We\ndevelop three extensions to the naive approach which exploits specific aspects\nof SDEMEMs and other advances such as correlated pseudo-marginal methods. We\ncompare these methods on real and simulated data from a tumour xenography study\non mice.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:02:45 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 01:36:45 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Botha", "Imke", ""], ["Kohn", "Robert", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1907.11024", "submitter": "Denis Belomestny", "authors": "Denis Belomestny, Alexander Goldenshluger", "title": "Density deconvolution under general assumptions on the distribution of\n  measurement errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of density deconvolution under general\nassumptions on the measurement error distribution. Typically deconvolution\nestimators are constructed using Fourier transform techniques, and it is\nassumed that the characteristic function of the measurement errors does not\nhave zeros on the real line. This assumption is rather strong and is not\nfulfilled in many cases of interest. In this paper we develop a methodology for\nconstructing optimal density deconvolution estimators in the general setting\nthat covers vanishing and non--vanishing characteristic functions of the\nmeasurement errors. We derive upper bounds on the risk of the proposed\nestimators and provide sufficient conditions under which zeros of the\ncorresponding characteristic function have no effect on estimation accuracy.\nMoreover, we show that the derived conditions are also necessary in some\nspecific problem instances.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:21:29 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 14:05:16 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 12:03:25 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Belomestny", "Denis", ""], ["Goldenshluger", "Alexander", ""]]}, {"id": "1907.11033", "submitter": "Daniela De Canditiis", "authors": "Daniela De Canditiis", "title": "Learning binary undirected graph in low dimensional regime", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample extracted from a Multivariate Bernoulli Variable (MBV),\nwe consider the problem of estimating the structure of the undirected graph for\nwhich the distribution is pairwise Markov and the parameters' vector of its\nexponential form. We propose a simple method that provides a closed form\nestimator of the parameters' vector and through its support also provides an\nestimate of the undirected graph associated to the MBV distribution. The\nestimator is proved to be consistent but it is feasible only in low-dimensional\nregimes. Synthetic examples illustrates its performance compared with another\nmethod that represents the state of the art in literature. Finally, the\nproposed procedure is used for the analysis of a real data set in the pediatric\nallergology area showing its practical efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:28:05 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["De Canditiis", "Daniela", ""]]}, {"id": "1907.11090", "submitter": "Heyang Gong", "authors": "Gong Heyang and Zhu Ke", "title": "Info Intervention", "comments": "See more information on Causal AI:\n  https://sites.google.com/view/minituring/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal diagrams based on do intervention are useful tools to formalize,\nprocess and understand causal relationship among variables. However, the do\nintervention has controversial interpretation of causal questions for\nnon-manipulable variables, and it also lacks the power to check the conditions\nrelated to counterfactual variables. This paper introduces a new info\nintervention to tackle these two problems, and provides causal diagrams for\ncommunication and theoretical focus based on this info intervention. Our info\nintervention intervenes the input/output information of causal mechanisms,\nwhile the do intervention intervenes the causal mechanisms. Consequently, the\ncausality is viewed as information transfer in the info intervention framework.\nAs an extension, the generalized info intervention is also proposed and studied\nin this paper.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:31:14 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 02:59:06 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 15:33:59 GMT"}, {"version": "v4", "created": "Fri, 17 Apr 2020 03:22:07 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2020 08:11:52 GMT"}, {"version": "v6", "created": "Tue, 2 Jun 2020 03:25:10 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Heyang", "Gong", ""], ["Ke", "Zhu", ""]]}, {"id": "1907.11142", "submitter": "Marzia Cremona", "authors": "Jacopo Di Iorio, Francesca Chiaromonte and Marzia A. Cremona", "title": "On the bias of H-scores for comparing biclusters, and how to correct it", "comments": "12 pages, 3 figures", "journal-ref": "Bioinformatics 2020, 36(1): 2955-2957", "doi": "10.1093/bioinformatics/btaa060", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades several biclustering methods have been developed as\nnew unsupervised learning techniques to simultaneously cluster rows and columns\nof a data matrix. These algorithms play a central role in contemporary machine\nlearning and in many applications, e.g. to computational biology and\nbioinformatics. The H-score is the evaluation score underlying the seminal\nbiclustering algorithm by Cheng and Church, as well as many other subsequent\nbiclustering methods. In this paper, we characterize a potentially troublesome\nbias in this score, that can distort biclustering results. We prove, both\nanalytically and by simulation, that the average H-score increases with the\nnumber of rows/columns in a bicluster. This makes the H-score, and hence all\nalgorithms based on it, biased towards small clusters. Based on our analytical\nproof, we are able to provide a straightforward way to correct this bias,\nallowing users to accurately compare biclusters.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:24:27 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Di Iorio", "Jacopo", ""], ["Chiaromonte", "Francesca", ""], ["Cremona", "Marzia A.", ""]]}, {"id": "1907.11369", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Daniel A. Griffith", "title": "A memory-free spatial additive mixed modeling for big spatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops a spatial additive mixed modeling (AMM) approach\nestimating spatial and non-spatial effects from large samples, such as millions\nof observations. Although fast AMM approaches are already well-established,\nthey are restrictive in that they assume an known spatial dependence structure.\nTo overcome this limitation, this study develops a fast AMM with the estimation\nof spatial structure in residuals and regression coefficients together with\nnon-spatial effects. We rely on a Moran coefficient-based approach to estimate\nthe spatial structure. The proposed approach pre-compresses large matrices\nwhose size grows with respect to the sample size N before the model estimation;\nthus, the computational complexity for the estimation is independent of the\nsample size. Furthermore, the pre-compression is done through a block-wise\nprocedure that makes the memory consumption independent of N. Eventually, the\nspatial AMM is memory-free and fast even for millions of observations. The\ndeveloped approach is compared to alternatives through Monte Carlo simulation\nexperiments. The result confirms the accuracy and computational efficiency of\nthe developed approach. The developed approaches are implemented in an R\npackage spmoran.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 02:55:02 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 11:38:33 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Murakami", "Daisuke", ""], ["Griffith", "Daniel A.", ""]]}, {"id": "1907.11493", "submitter": "Maarten van Smeden", "authors": "Ben Van Calster, Maarten van Smeden, Ewout W. Steyerberg", "title": "On the variability of regression shrinkage methods for clinical\n  prediction models: simulation study on predictive performance", "comments": "138 pages (incl 114 supplementary pages). Main document: 5 figures\n  and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When developing risk prediction models, shrinkage methods are recommended,\nespecially when the sample size is limited. Several earlier studies have shown\nthat the shrinkage of model coefficients can reduce overfitting of the\nprediction model and subsequently result in better predictive performance on\naverage. In this simulation study, we aimed to investigate the variability of\nregression shrinkage on predictive performance for a binary outcome, with focus\non the calibration slope. The slope indicates whether risk predictions are too\nextreme (slope < 1) or not extreme enough (slope > 1). We investigated the\nfollowing shrinkage methods in comparison to standard maximum likelihood\nestimation: uniform shrinkage (likelihood-based and bootstrap-based), ridge\nregression, penalized maximum likelihood, LASSO regression, adaptive LASSO,\nnon-negative garrote, and Firth's correction. There were three main findings.\nFirst, shrinkage improved calibration slopes on average. Second, the\nbetween-sample variability of calibration slopes was often increased relative\nto maximum likelihood. Among the shrinkage methods, the bootstrap-based uniform\nshrinkage worked well overall. In contrast to other shrinkage approaches,\nFirth's correction had only a small shrinkage effect but did so with low\nvariability. Third, the correlation between the estimated shrinkage and the\noptimal shrinkage to remove overfitting was typically negative. Hence, although\nshrinkage improved predictions on average, it often worked poorly in individual\ndatasets, in particular when shrinkage was most needed. The observed\nvariability of shrinkage methods implies that these methods do not solve\nproblems associated with small sample size or low number of events per\nvariable.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:29:19 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Van Calster", "Ben", ""], ["van Smeden", "Maarten", ""], ["Steyerberg", "Ewout W.", ""]]}, {"id": "1907.11508", "submitter": "Nicholas Rowe", "authors": "Nicholas Rowe", "title": "Some examples of application for predicting of compressive sensing\n  method", "comments": "38 pages , 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers application of the SALSA algorithm as a method of\nforecasting and applies it to simulated electrical signal, temperature\nrecording from the Australian Bureau of Meteorology and stock prices from the\nAustralian stock exchange. It compares it to basic linear extrapolation and\ncasual smoothing extrapolation, in all cases SALSA extrapolation proves to be a\nbetter method of forecasting than linear extrapolation. However, it cannot be\nimperially stated that it is superior to Causal smoothing extrapolation in\ncomplex systems as it has a higher L2 euclidean in these experiments. while\nusually retaining more shape and statistical elements of the original function\nthan Causal smoothing extrapolation. Leading to the conclusion the Causal\nSmoothing extrapolation can provide a more conservative forecast for complex\nsystems while the SALSA algorithm more accurately predicts the range of\npossible events as well as being the superior forecasting method for electrical\nsignals, the physical process it is designed to forecast.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 12:17:27 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Rowe", "Nicholas", ""]]}, {"id": "1907.11541", "submitter": "Mucyo Karemera", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "Phase Transition Unbiased Estimation in High Dimensional Settings", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.04443", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important challenge in statistical analysis concerns the control of the\nfinite sample bias of estimators. For example, the maximum likelihood estimator\nhas a bias that can result in a significant inferential loss. This problem is\ntypically magnified in high-dimensional settings where the number of variables\n$p$ is allowed to diverge with the sample size $n$. However, it is generally\ndifficult to establish whether an estimator is unbiased and therefore its\nasymptotic order is a common approach used (in low-dimensional settings) to\nquantify the magnitude of the bias. As an alternative, we introduce a new and\nstronger property, possibly for high-dimensional settings, called phase\ntransition unbiasedness. An estimator satisfying this property is unbiased for\nall $n$ greater than a finite sample size $n^\\ast$. Moreover, we propose a\nphase transition unbiased estimator built upon the idea of matching an initial\nestimator computed on the sample and on simulated data. It is not required for\nthis initial estimator to be consistent and thus it can be chosen for its\ncomputational efficiency and/or for other desirable properties such as\nrobustness. This estimator can be computed using a suitable simulation based\nalgorithm, namely the iterative bootstrap, which is shown to converge\nexponentially fast. In addition, we demonstrate the consistency and the\nlimiting distribution of this estimator in high-dimensional settings. Finally,\nas an illustration, we use our approach to develop new estimators for the\nlogistic regression model, with and without random effects, that also enjoy\nother properties such as robustness to data contamination and are also not\naffected by the problem of separability. In a simulation exercise, the\ntheoretical results are confirmed in settings where the sample size is\nrelatively small compared to the model dimension.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:58:07 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 15:27:00 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 15:57:08 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1907.11659", "submitter": "Dylan Spicker", "authors": "Dylan Spicker and Michael Wallace", "title": "Measurement error and precision medicine: error-prone tailoring\n  covariates in dynamic treatment regimes", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8690", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision medicine incorporates patient-level covariates to tailor treatment\ndecisions, seeking to improve outcomes. In longitudinal studies with\ntime-varying covariates and sequential treatment decisions, precision medicine\ncan be formalized with dynamic treatment regimes (DTRs): sequences of\ncovariate-dependent treatment rules. To date, the precision medicine literature\nhas not addressed a ubiquitous concern in health research - measurement error -\nwhere observed data deviate from the truth. We discuss the consequences of\nignoring measurement error in the context of DTRs, focusing on challenges\nunique to precision medicine. We show - through simulation and theoretical\nresults - that relatively simple measurement error correction techniques can\nlead to substantial improvements over uncorrected analyses, and apply these\nfindings to the Sequenced Treatment Alternatives to Relieve Depression (STAR*D)\nstudy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 16:09:45 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 18:22:00 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 19:38:25 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Spicker", "Dylan", ""], ["Wallace", "Michael", ""]]}, {"id": "1907.11680", "submitter": "Dootika Vats", "authors": "Dootika Vats, Nathan Robertson, James M Flegal, Galin L Jones", "title": "Analyzing MCMC Output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is a sampling-based method for estimating\nfeatures of probability distributions. MCMC methods produce a serially\ncorrelated, yet representative, sample from the desired distribution. As such\nit can be difficult to know when the MCMC method is producing reliable results.\nWe introduce some fundamental methods for ensuring a trustworthy simulation\nexperiment. In particular, we present a workflow for output analysis in MCMC\nproviding estimators, approximate sampling distributions, stopping rules, and\nvisualization tools.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 17:17:02 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 16:34:12 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Vats", "Dootika", ""], ["Robertson", "Nathan", ""], ["Flegal", "James M", ""], ["Jones", "Galin L", ""]]}, {"id": "1907.11752", "submitter": "Mauricio Gonzalez-Soto", "authors": "Mauricio Gonzalez-Soto, L. Enrique Sucar, Hugo J. Escalante", "title": "von Neumann-Morgenstern and Savage Theorems for Causal Decision Making", "comments": "Submitted to Journal of Causal Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal thinking and decision making under uncertainty are fundamental aspects\nof intelligent reasoning. Decision making under uncertainty has been well\nstudied when information is considered at the associative (probabilistic)\nlevel. The classical Theorems of von Neumann-Morgenstern and Savage provide a\nformal criterion for rational choice using purely associative information.\nCausal inference often yields uncertainty about the exact causal structure, so\nwe consider what kinds of decisions are possible in those conditions. In this\nwork, we consider decision problems in which available actions and consequences\nare causally connected. After recalling a previous causal decision making\nresult, which relies on a known causal model, we consider the case in which the\ncausal mechanism that controls some environment is unknown to a rational\ndecision maker. In this setting we state and prove a causal version of Savage's\nTheorem, which we then use to develop a notion of causal games with its\nrespective causal Nash equilibrium. These results highlight the importance of\ncausal models in decision making and the variety of potential applications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 18:44:39 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 12:13:10 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 23:18:32 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 04:13:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gonzalez-Soto", "Mauricio", ""], ["Sucar", "L. Enrique", ""], ["Escalante", "Hugo J.", ""]]}, {"id": "1907.11767", "submitter": "ShengLi Tzeng", "authors": "ShengLi Tzeng, Jun Zhu, Amy Weisman, Tyler Bradshaw and Robert Jeraj", "title": "Spatial Process Decomposition for Quantitative Imaging Biomarkers Using\n  Multiple Images of Varying Shapes", "comments": "27 pages, 6 figures, 2 tables", "journal-ref": "Statistics in Medicine,40(2021),1243-1261", "doi": "10.1002/sim.8838", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative imaging biomarkers (QIB) are extracted from medical images in\nradiomics for a variety of purposes including noninvasive disease detection,\ncancer monitoring, and precision medicine. The existing methods for QIB\nextraction tend to be ad-hoc and not reproducible. In this paper, a general and\nflexible statistical approach is proposed for handling up to three-dimensional\nmedical images in an objective and principled way. In particular, a model-based\nspatial process decomposition is developed where the random weights are unique\nto individual patients for component functions common across patients. Model\nfitting and selection are based on maximum likelihood, while feature\nextractions are via optimal prediction of the underlying true image. A\nsimulation study evaluates the properties of the proposed methodology and for\nillustration, a cancer image data set is analyzed and QIBs are extracted in\nassociation with a clinical endpoint.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 19:39:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tzeng", "ShengLi", ""], ["Zhu", "Jun", ""], ["Weisman", "Amy", ""], ["Bradshaw", "Tyler", ""], ["Jeraj", "Robert", ""]]}, {"id": "1907.11882", "submitter": "BaoLuo Sun", "authors": "Baoluo Sun, Yifan Cui and Eric Tchetgen Tchetgen", "title": "Selective Machine Learning of the Average Treatment Effect with an\n  Invalid Instrumental Variable", "comments": "47 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods have been widely used to identify causal\neffects in the presence of unmeasured confounding. A key identification\ncondition known as the exclusion restriction states that the instrument cannot\nhave a direct effect on the outcome which is not mediated by the exposure in\nview. In the health and social sciences, such an assumption is often not\ncredible. To address this concern, we consider identification conditions of the\npopulation average treatment effect with an invalid instrumental variable which\ndoes not satisfy the exclusion restriction, and derive the efficient influence\nfunction targeting the identifying functional under a nonparametric observed\ndata model. We propose a novel multiply robust locally efficient estimator of\nthe average treatment effect that is consistent in the union of multiple\nparametric nuisance models, as well as a multiply debiased machine learning\nestimator for which the nuisance parameters are estimated using generic machine\nlearning methods, that effectively exploit various forms of linear or nonlinear\nstructured sparsity in the nuisance parameter space. When one cannot be\nconfident that any of these machine learners is consistent at sufficiently fast\nrates to ensure $\\surd{n}$-consistency for the average treatment effect, we\nintroduce a new criteria for selective machine learning which leverages the\nmultiple robustness property in order to ensure small bias. The proposed\nmethods are illustrated through extensive simulations and a data analysis\nevaluating the causal effect of 401(k) participation on savings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 09:44:48 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 07:22:06 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 17:09:55 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 01:15:50 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sun", "Baoluo", ""], ["Cui", "Yifan", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1907.11958", "submitter": "Michael Law", "authors": "Michael Law and Ya'acov Ritov", "title": "Estimating the Random Effect in Big Data Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider three problems in high-dimensional Gaussian linear mixed models.\nWithout any assumptions on the design for the fixed effects, we construct an\nasymptotic $F$-statistic for testing whether a collection of random effects is\nzero, derive an asymptotic confidence interval for a single random effect at\nthe parametric rate $\\sqrt{n}$, and propose an empirical Bayes estimator for a\npart of the mean vector in ANOVA type models that performs asymptotically as\nwell as the oracle Bayes estimator. We support our results with numerical\nsimulations and provide comparisons with oracle estimators. The procedures\ndeveloped are applied to the Trends in International Mathematics and Sciences\nStudy (TIMSS) data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 18:21:10 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Law", "Michael", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1907.11969", "submitter": "Birgir Hrafnkelsson", "authors": "Birgir Hrafnkelsson, Stefan Siegert, Rapha\\\"el Huser, Haakon Bakka,\n  \\'Arni V. J\\'ohannesson", "title": "Max-and-Smooth: a two-step approach for approximate Bayesian inference\n  in latent Gaussian models", "comments": "This is version 2 of the Max-and-Smooth paper. Minor changes have\n  been made to this version with respect to version 1 of the Max-and-Smooth\n  paper. The order of the authors was changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With modern high-dimensional data, complex statistical models are necessary,\nrequiring computationally feasible inference schemes. We introduce\nMax-and-Smooth, an approximate Bayesian inference scheme for a flexible class\nof latent Gaussian models (LGMs) where one or more of the likelihood parameters\nare modeled by latent additive Gaussian processes. Max-and-Smooth consists of\ntwo-steps. In the first step (Max), the likelihood function is approximated by\na Gaussian density with mean and covariance equal to either (a) the maximum\nlikelihood estimate and the inverse observed information, respectively, or (b)\nthe mean and covariance of the normalized likelihood function. In the second\nstep (Smooth), the latent parameters and hyperparameters are inferred and\nsmoothed with the approximated likelihood function. The proposed method ensures\nthat the uncertainty from the first step is correctly propagated to the second\nstep. Since the approximated likelihood function is Gaussian, the approximate\nposterior density of the latent parameters of the LGM (conditional on the\nhyperparameters) is also Gaussian, thus facilitating efficient posterior\ninference in high dimensions. Furthermore, the approximate marginal posterior\ndistribution of the hyperparameters is tractable, and as a result, the\nhyperparameters can be sampled independently of the latent parameters. In the\ncase of a large number of independent data replicates, sparse precision\nmatrices, and high-dimensional latent vectors, the speedup is substantial in\ncomparison to an MCMC scheme that infers the posterior density from the exact\nlikelihood function. The proposed inference scheme is demonstrated on one\nspatially referenced real dataset and on simulated data mimicking spatial,\ntemporal, and spatio-temporal inference problems. Our results show that\nMax-and-Smooth is accurate and fast.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 19:59:47 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 08:30:45 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Hrafnkelsson", "Birgir", ""], ["Siegert", "Stefan", ""], ["Huser", "Rapha\u00ebl", ""], ["Bakka", "Haakon", ""], ["J\u00f3hannesson", "\u00c1rni V.", ""]]}, {"id": "1907.11970", "submitter": "Ranjan Maitra", "authors": "Fan Dai, Somak Dutta and Ranjan Maitra", "title": "A Matrix--free Likelihood Method for Exploratory Factor Analysis of\n  High-dimensional Gaussian Data", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1080/10618600.2019.1704296", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel profile likelihood method for estimating the\ncovariance parameters in exploratory factor analysis of high-dimensional\nGaussian datasets with fewer observations than number of variables. An\nimplicitly restarted Lanczos algorithm and a limited-memory quasi-Newton method\nare implemented to develop a matrix-free framework for likelihood maximization.\nSimulation results show that our method is substantially faster than the\nexpectation-maximization solution without sacrificing accuracy. Our method is\napplied to fit factor models on data from suicide attempters, suicide ideators\nand a control group.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 20:04:55 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 19:25:43 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dai", "Fan", ""], ["Dutta", "Somak", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1907.12150", "submitter": "Georgia Papadogeorgou", "authors": "Patrick Schnell, Georgia Papadogeorgou", "title": "Mitigating Unobserved Spatial Confounding when Estimating the Effect of\n  Supermarket Access on Cardiovascular Disease Deaths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confounding by unmeasured spatial variables has received some attention in\nthe spatial statistics and causal inference literatures, but concepts and\napproaches have remained largely separated. In this paper, we aim to bridge\nthese distinct strands of statistics by considering unmeasured spatial\nconfounding within a causal inference framework, and estimating effects using\noutcome regression tools popular within the spatial literature. First, we show\nhow using spatially correlated random effects in the outcome model, an approach\ncommon among spatial statisticians, does not necessarily mitigate bias due to\nspatial confounding, a previously published but not universally known result.\nMotivated by the bias term of commonly-used estimators, we propose an affine\nestimator which addresses this deficiency. We discuss how unbiased estimation\nof causal parameters in the presence of unmeasured spatial confounding can only\nbe achieved under an untestable set of assumptions which will often be\napplication-specific. We provide a set of assumptions which describe how the\nexposure and outcome of interest relate to the unmeasured variables, and we\nshow that this set of assumptions is sufficient for identification of the\ncausal effect based on the observed data when spatial dependencies can be\nrepresented by a ring graph. We implement our method using a fully Bayesian\napproach applicable to any type of outcome variable. This work is motivated by\nand used to estimate the effect of county-level limited access to supermarkets\non the rate of cardiovascular disease deaths in the elderly across the whole\ncontinental United States. Even though standard approaches return null or\nprotective effects, our approach uncovers evidence of unobserved spatial\nconfounding, and indicates that limited supermarket access has a harmful effect\non cardiovascular mortality.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 22:25:30 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 00:04:37 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 21:27:01 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Schnell", "Patrick", ""], ["Papadogeorgou", "Georgia", ""]]}, {"id": "1907.12160", "submitter": "Soumya Mohanty", "authors": "Soumya D. Mohanty, Ethan Fahnestock", "title": "Adaptive spline fitting with particle swarm optimization", "comments": "Accepted version; Typo corrected in equation 3; Minor changes to text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fitting data with a spline, finding the optimal placement of knots can\nsignificantly improve the quality of the fit. However, the challenging\nhigh-dimensional and non-convex optimization problem associated with completely\nfree knot placement has been a major roadblock in using this approach. We\npresent a method that uses particle swarm optimization (PSO) combined with\nmodel selection to address this challenge. The problem of overfitting due to\nknot clustering that accompanies free knot placement is mitigated in this\nmethod by explicit regularization, resulting in a significantly improved\nperformance on highly noisy data. The principal design choices available in the\nmethod are delineated and a statistically rigorous study of their effect on\nperformance is carried out using simulated data and a wide variety of benchmark\nfunctions. Our results demonstrate that PSO-based free knot placement leads to\na viable and flexible adaptive spline fitting approach that allows the fitting\nof both smooth and non-smooth functions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 23:30:15 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 02:29:55 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 23:52:23 GMT"}, {"version": "v4", "created": "Sun, 28 Jun 2020 17:37:24 GMT"}, {"version": "v5", "created": "Sun, 26 Jul 2020 21:43:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mohanty", "Soumya D.", ""], ["Fahnestock", "Ethan", ""]]}, {"id": "1907.12268", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "Discovering Association with Copula Entropy", "comments": "Minor revision. The code is available at\n  https://github.com/majianthu/copent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT q-bio.QM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering associations is of central importance in scientific practices.\nCurrently, most researches consider only linear association measured by\ncorrelation coefficient, which has its theoretical limitations. In this paper,\nwe propose a new method for discovering association with copula entropy -- a\nuniversal applicable association measure for not only linear cases, but\nnonlinear cases. The advantage of the method based on copula entropy over\ntraditional method is demonstrated on the NHANES data by discovering more\nbiomedical meaningful associations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:23:21 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 06:26:38 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "1907.12283", "submitter": "Heidi S{\\o}gaard Christensen", "authors": "Heidi S. Christensen and Jesper M{\\o}ller", "title": "Modelling spine locations on dendrite trees using inhomogeneous Cox\n  point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dendritic spines, which are small protrusions on the dendrites of a neuron,\nare of interest in neuroscience as they are related to cognitive processes such\nas learning and memory. We analyse the distribution of spine locations on six\ndifferent dendrite trees from mouse neurons using point process theory for\nlinear networks. Besides some possible small-scale repulsion, { we find that\ntwo of the spine point pattern data sets may be described by inhomogeneous\nPoisson process models}, while the other point pattern data sets exhibit\nclustering between spines at a larger scale. To model this we propose an\ninhomogeneous Cox process model constructed by thinning a Poisson process on a\nlinear network with retention probabilities determined by a spatially\ncorrelated random field. For model checking we consider network analogues of\nthe empirical $F$-, $G$-, and $J$-functions originally introduced for\ninhomogeneous point processes on a Euclidean space. The fitted Cox process\nmodels seem to catch the clustering of spine locations between spines, but also\nposses a large variance in the number of points for some of the data sets\ncausing large confidence regions for the empirical $F$- and $G$-functions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:59:02 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 19:16:56 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 09:59:57 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Christensen", "Heidi S.", ""], ["M\u00f8ller", "Jesper", ""]]}, {"id": "1907.12528", "submitter": "Robert Lunde", "authors": "Robert Lunde and Purnamrita Sarkar", "title": "Subsampling Sparse Graphons Under Minimal Assumptions", "comments": "V2 is a streamlined/shorter version of V1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a general theory for subsampling network data generated by the\nsparse graphon model. In contrast to previous work for networks, we demonstrate\nvalidity under minimal assumptions; the main requirement is weak convergence of\nthe functional of interest. We study the properties of two procedures: vertex\nsubsampling and $p$-subsampling. For the first, we prove validity under the\nmild condition that the number of subsampled vertices is $o(n)$. For the\nsecond, we establish validity under analogous conditions on the expected\nsubsample size. For both procedures, we also establish conditions under which\nuniform validity holds. Furthermore, under appropriate sparsity conditions, we\nderive limiting distributions for the nonzero eigenvalues of the adjacency\nmatrix of a low rank sparse graphon. Our weak convergence result immediately\nyields the validity of subsampling for the nonzero eigenvalues under suitable\nassumptions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 17:06:32 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 23:56:16 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lunde", "Robert", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1907.12589", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Smaller $p$-values via indirect information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops $p$-values for evaluating means of normal populations\nthat make use of indirect or prior information. A $p$-value of this type is\nbased on a biased test statistic that is optimal on average with respect to a\nprobability distribution that encodes indirect information about the mean\nparameter, resulting in a smaller $p$-value if the indirect information is\naccurate. In a variety of multiparameter settings, we show how to adaptively\nestimate the indirect information for each mean parameter while still\nmaintaining uniformity of the $p$-values under their null hypotheses. This is\ndone using a linking model through which indirect information about the mean of\none population may be obtained from the data of other populations. Importantly,\nthe linking model does not need to be correct to maintain the uniformity of the\n$p$-values under their null hypotheses. This methodology is illustrated in\nseveral data analysis scenarios, including small area inference, spatially\narranged populations, interactions in linear regression, and generalized linear\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 18:12:39 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 20:01:41 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1907.12709", "submitter": "Trang Nguyen", "authors": "Trang Quynh Nguyen and Elizabeth A. Stuart", "title": "Propensity score analysis with latent covariates: Measurement error bias\n  correction using the covariate's posterior mean, aka the inclusive factor\n  score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address measurement error bias in propensity score (PS) analysis due to\ncovariates that are latent variables. In the setting where latent covariate $X$\nis measured via multiple error-prone items $\\mathbf{W}$, PS analysis using\nseveral proxies for $X$ -- the $\\mathbf{W}$ items themselves, a summary score\n(mean/sum of the items), or the conventional factor score (cFS , i.e.,\npredicted value of $X$ based on the measurement model) -- often results in\nbiased estimation of the causal effect, because balancing the proxy (between\nexposure conditions) does not balance $X$. We propose an improved proxy: the\nconditional mean of $X$ given the combination of $\\mathbf{W}$, the observed\ncovariates $Z$, and exposure $A$, denoted $X_{WZA}$. The theoretical support,\nwhich applies whether $X$ is latent or not (but is unobserved), is that\nbalancing $X_{WZA}$ (e.g., via weighting or matching) implies balancing the\nmean of $X$. For a latent $X$, we estimate $X_{WZA}$ by the inclusive factor\nscore (iFS) -- predicted value of $X$ from a structural equation model that\ncaptures the joint distribution of $(X,\\mathbf{W},A)$ given $Z$. Simulation\nshows that PS analysis using the iFS substantially improves balance on the\nfirst five moments of $X$ and reduces bias in the estimated causal effect.\nHence, within the proxy variables approach, we recommend this proxy over\nexisting ones. We connect this proxy method to known results about\nweighting/matching functions (Lockwood & McCaffrey, 2016; McCaffrey, Lockwood,\n& Setodji, 2013). We illustrate the method in handling latent covariates when\nestimating the effect of out-of-school suspension on risk of later police\narrests using Add Health data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 02:31:23 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 01:27:08 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Nguyen", "Trang Quynh", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "1907.12732", "submitter": "Zijian Guo", "authors": "Zijian Guo, Cun-Hui Zhang", "title": "Local Inference in Additive Models with Decorrelated Local Linear\n  Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive models, as a natural generalization of linear regression, have\nplayed an important role in studying nonlinear relationships. Despite of a rich\nliterature and many recent advances on the topic, the statistical inference\nproblem in additive models is still relatively poorly understood. Motivated by\nthe inference for the exposure effect and other applications, we tackle in this\npaper the statistical inference problem for $f_1'(x_0)$ in additive models,\nwhere $f_1$ denotes the univariate function of interest and $f_1'(x_0)$ denotes\nits first order derivative evaluated at a specific point $x_0$. The main\nchallenge for this local inference problem is the understanding and control of\nthe additional uncertainty due to the need of estimating other components in\nthe additive model as nuisance functions. To address this, we propose a\ndecorrelated local linear estimator, which is particularly useful in reducing\nthe effect of the nuisance function estimation error on the estimation accuracy\nof $f'_1(x_0)$. We establish the asymptotic limiting distribution for the\nproposed estimator and then construct confidence interval and hypothesis\ntesting procedures for $f_1'(x_0)$. The variance level of the proposed\nestimator is of the same order as that of the local least squares in\nnonparametric regression, or equivalently the additive model with one\ncomponent, while the bias of the proposed estimator is jointly determined by\nthe statistical accuracies in estimating the nuisance functions and the\nrelationship between the variable of interest and the nuisance variables. The\nmethod is developed for general additive models and is demonstrated in the\nhigh-dimensional sparse setting.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 04:12:51 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Guo", "Zijian", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1907.12797", "submitter": "Mathias Bourel", "authors": "Mathias Bourel (IMERL), Badih Ghattas (I2M), Meliza Gonz\\'alez", "title": "Comparing partitions through the Matching Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim to propose a non parametric hypothesis test, this paper carries\nout a study on the Matching Error (ME), a comparison index of two partitions\nobtained from the same data set, using for example two clustering methods. This\nindex is related to the misclassifica-tion error in supervised learning. Some\nproperties of the ME and, especially, its distribution function for the case of\ntwo independent partitions are analyzed. Extensive simulations show the\nefficiency of the ME and we propose a hypothesis test based on it.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 09:23:40 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bourel", "Mathias", "", "IMERL"], ["Ghattas", "Badih", "", "I2M"], ["Gonz\u00e1lez", "Meliza", ""]]}, {"id": "1907.12804", "submitter": "Daniel Commenges", "authors": "Daniel Commenges and M\\'elanie Prague", "title": "Effects of interventions and optimal strategies in the stochastic system\n  approach to causality", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of defining the effect of an intervention on a\ntime-varying risk factor or treatment for a disease or a physiological marker;\nwe develop here the latter case. So, the system considered is $(Y,A,C)$, where\n$Y=(Y_t)$, is the marker process of interest, $A=A_t$ the treatment. A\nrealistic case is that the treatment can be changed only at discrete times. In\nan observational study the treatment attribution law is unknown; however, the\nphysical law can be estimated without knowing the treatment attribution law,\nprovided a well-specified model is available. An intervention is specified by\nthe treatment attribution law, which is thus known. Simple interventions will\nsimply randomize the attribution of the treatment; interventions that take into\naccount the past history will be called \"strategies\". The effect of\ninterventions can be defined by a risk function $R^{\\intr}=\\Ee_{\\intr}[L(\\bar\nY_{t_J}, \\bar A_{t_{J}},C)]$, where $L(\\bar Y_{t_J}, \\bar A_{t_{J}},C)$ is a\nloss function, and contrasts between risk functions for different strategies\ncan be formed. Once we can compute effects for any strategy, we can search for\noptimal or sub-optimal strategies; in particular we can find optimal parametric\nstrategies. We present several ways for designing strategies. As an\nillustration, we consider the choice of a strategy for containing the HIV load\nbelow a certain level while limiting the treatment burden. A simulation study\ndemonstrates the possibility of finding optimal parametric strategies.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 09:35:55 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Commenges", "Daniel", ""], ["Prague", "M\u00e9lanie", ""]]}, {"id": "1907.12853", "submitter": "Souvik Banerjee", "authors": "Souvik Banerjee, Gajendra K. Vishwakarma and Atanu Bhattacharjee", "title": "Classification Algorithm for High Dimensional Protein Markers in\n  Time-course Data", "comments": "Require major revisions and refinements", "journal-ref": "Statistics in Medicine, 2020", "doi": "10.1002/sim.8720", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of biomarkers is an emerging area in Oncology. In this\narticle, we develop an efficient statistical procedure for classification of\nprotein markers according to their effect on cancer progression. A\nhigh-dimensional time-course dataset of protein markers for 80 patients\nmotivates us for developing the model. We obtain the optimal threshold values\nfor markers using Cox proportional hazard model. The optimal threshold value is\ndefined as a level of a marker having maximum impact on cancer progression. The\nclassification was validated by comparing random components using both\nproportional hazard and accelerated failure time frailty models. The study\nelucidates the application of two separate joint modeling techniques using auto\nregressive-type model and mixed effect model for time-course data and\nproportional hazard model for survival data with proper utilization of Bayesian\nmethodology. Also, a prognostic score has been developed on the basis of few\nselected genes with application on patients. The complete analysis is performed\nby R programming code. This study facilitates to identify relevant biomarkers\nfrom a set of markers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:14:43 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 12:02:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Banerjee", "Souvik", ""], ["Vishwakarma", "Gajendra K.", ""], ["Bhattacharjee", "Atanu", ""]]}, {"id": "1907.12912", "submitter": "Brice Ozenne", "authors": "Brice Maxime Hugues Ozenne, Thomas Harder Scheike, Laila St{\\ae}rk and\n  Thomas Alexander Gerds", "title": "On the estimation of average treatment effects with right-censored time\n  to event outcome and competing risks", "comments": null, "journal-ref": "Biometrical Journal 2020", "doi": "10.1002/bimj.201800298", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the estimation of average treatment effects based on\nright-censored data of an observational study. We focus on causal inference of\ndifferences between t-year absolute event risks in a situation with competing\nrisks. We derive doubly robust estimation equations and implement estimators\nfor the nuisance parameters based on working regression models for the outcome,\nthe censoring and the treatment distribution conditional on auxiliary baseline\ncovariates. We use the functional delta method to show that our estimators are\nregular asymptotically linear estimators and estimate their variances based on\nestimates of their influence functions. In empirical studies we assess the\nrobustness of the estimators and the coverage of confidence intervals. The\nmethods are further illustrated using data from a Danish registry study.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:42:05 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ozenne", "Brice Maxime Hugues", ""], ["Scheike", "Thomas Harder", ""], ["St\u00e6rk", "Laila", ""], ["Gerds", "Thomas Alexander", ""]]}, {"id": "1907.13093", "submitter": "Jean-Jacques Forneron", "authors": "Jean-Jacques Forneron", "title": "Detecting Identification Failure in Moment Condition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper develops an approach to detect identification failure in a large\nclass of moment condition models. This is achieved by introducing a\nquasi-Jacobian matrix which is asymptotically singular under weak or set\nidentification, and when local identification fails. In these settings,\nstandard asymptotics are not valid. Under (semi)-strong identification, where\nstandard asymptotics are valid, the quasi-Jacobian is asymptotically equivalent\nto the usual Jacobian and, after re-scaling, is asymptotically non-singular.\nHence, the eigenvalues of the quasi-Jacobian are informative about local and\nglobal identification failures and the directions in which identification\nfails. Building on these results, a simple test procedure with chi-squared\ndata-driven critical values is introduced. It yields uniformly valid subvector\ninferences under strong, semi-strong, and weak identification without a priori\nknowledge about the underlying identification structure. It is non-conservative\nunder strong identification. Monte-Carlo simulations and an application to the\nLong-Run Risks model illustrate the results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 17:26:06 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 01:07:30 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 18:39:44 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Forneron", "Jean-Jacques", ""]]}, {"id": "1907.13208", "submitter": "Donghui Yan", "authors": "Donghui Yan, Ying Xu", "title": "Learning over inherently distributed data", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent decades have seen a surge of interests in distributed computing.\nExisting work focus primarily on either distributed computing platforms, data\nquery tools, or, algorithms to divide big data and conquer at individual\nmachines etc. It is, however, increasingly often that the data of interest are\ninherently distributed, i.e., data are stored at multiple distributed sites due\nto diverse collection channels, business operations etc. We propose to enable\nlearning and inference in such a setting via a general framework based on the\ndistortion minimizing local transformations. This framework only requires a\nsmall amount of local signatures to be shared among distributed sites,\neliminating the need of having to transmitting big data. Computation can be\ndone very efficiently via parallel local computation. The error incurred due to\ndistributed computing vanishes when increasing the size of local signatures. As\nthe shared data need not be in their original form, data privacy may also be\npreserved. Experiments on linear (logistic) regression and Random Forests have\nshown promise of this approach. This framework is expected to apply to a\ngeneral class of tools in learning and inference with the continuity property.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 20:11:19 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Yan", "Donghui", ""], ["Xu", "Ying", ""]]}, {"id": "1907.13258", "submitter": "Dominik Rothenh\\\"ausler", "authors": "Dominik Rothenh\\\"ausler and Bin Yu", "title": "Incremental causal effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal evidence is needed to act and it is often enough for the evidence to\npoint towards a direction of the effect of an action. For example, policymakers\nmight be interested in estimating the effect of slightly increasing taxes on\nprivate spending across the whole population. We study identifiability and\nestimation of causal effects, where a continuous treatment is slightly shifted\nacross the whole population (termed average partial effect or incremental\ncausal effect). We show that incremental effects are identified under local\nignorability and local overlap assumptions, where exchangeability and\npositivity only hold in a neighborhood of units. Average treatment effects are\nnot identified under these assumptions. In this case, and under a smoothness\ncondition, the incremental effect can be estimated via the average derivative.\nMoreover, we prove that in certain finite-sample observational settings,\nestimating the incremental effect is easier than estimating the average\ntreatment effect in terms of asymptotic variance. For high-dimensional\nsettings, we develop a simple feature transformation that allows for\ndoubly-robust estimation and inference of incremental causal effects. Finally,\nwe compare the behaviour of estimators of the incremental treatment effect and\naverage treatment effect in experiments including data-inspired simulations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 23:33:48 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 03:47:55 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 23:55:15 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 19:18:22 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Rothenh\u00e4usler", "Dominik", ""], ["Yu", "Bin", ""]]}, {"id": "1907.13323", "submitter": "Linbo Wang", "authors": "Dehan Kong, Shu Yang and Linbo Wang", "title": "Identifiability of causal effects with multiple causes and a binary\n  outcome", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unobserved confounding presents a major threat to causal inference from\nobservational studies. Recently, several authors suggest that this problem may\nbe overcome in a shared confounding setting where multiple treatments are\nindependent given a common latent confounder. It has been shown that under a\nlinear Gaussian model for the treatments, the causal effect is not identifiable\nwithout parametric assumptions on the outcome model. In this paper, we show\nthat the causal effect is indeed identifiable if we assume a general binary\nchoice model for the outcome with a non-probit link. Our identification\napproach is based on the incongruence between Gaussianity of the treatments and\nlatent confounder, and non-Gaussianity of a latent outcome variable. We further\ndevelop a two-step likelihood-based estimation procedure.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 06:26:19 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 07:08:49 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kong", "Dehan", ""], ["Yang", "Shu", ""], ["Wang", "Linbo", ""]]}, {"id": "1907.13421", "submitter": "Jinguo Xian", "authors": "Dong Han, Fugee Tsung and Jinguo Xian", "title": "Optimal Sequential Tests for Monitoring Changes in the Distribution of\n  Finite Observation Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a method to construct the optimal sequential test for\nmonitoring the changes in the distribution of finite observation sequences with\na general dependence structure. This method allows us to prove that different\noptimal sequential tests can be constructed for different performance measures\nof detection delay times. We also provide a formula to calculate the value of\nthe generalized out-of-control average run length for every optimal sequential\ntest. Moreover, we show that there is an equivalent optimal control limit which\ndoes not depend on the test statistic directly when the post-change conditional\ndensities (probabilities) of the observation sequences do not depend on the\nchange time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 11:23:40 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Han", "Dong", ""], ["Tsung", "Fugee", ""], ["Xian", "Jinguo", ""]]}, {"id": "1907.13489", "submitter": "Jean Abi Rizk", "authors": "Jean Rizk, Kevin Burke, Cathal Walsh", "title": "An Alternative Formulation of Coxian Phase-type Distributions with\n  Covariates: Application to Emergency Department Length of Stay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new methodology to model patient transitions and\nlength of stay in the emergency department using a series of conditional Coxian\nphase-type distributions, with covariates. We reformulate the Coxian models\n(standard Coxian, Coxian with multiple absorbing states, joint Coxian, and\nconditional Coxian) to take into account heterogeneity in patient\ncharacteristics such as arrival mode, time of admission and age. The approach\ndiffers from previous research in that it reduces the computational time, and\nit allows the inclusion of patient covariate information directly into the\nmodel. The model is applied to emergency department data from University\nHospital Limerick in Ireland.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:22:46 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Rizk", "Jean", ""], ["Burke", "Kevin", ""], ["Walsh", "Cathal", ""]]}, {"id": "1907.13493", "submitter": "Yuguang Wang", "authors": "Yu Guang Wang and Robert S. Womersley and Hau-Tieng Wu and Wei-Hsuan\n  Yu", "title": "Numerical computation of triangular complex spherical designs with small\n  mesh ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides triangular spherical designs for the complex unit sphere\n$\\Omega^d$ by exploiting the natural correspondence between the complex unit\nsphere in $d$ dimensions and the real unit sphere in $2d-1$. The existence of\ntriangular and square complex spherical $t$-designs with the optimal order\nnumber of points is established. A variational characterization of triangular\ncomplex designs is provided, with particular emphasis on numerical computation\nof efficient triangular complex designs with good geometric properties as\nmeasured by their mesh ratio. We give numerical examples of triangular\nspherical $t$-designs on complex unit spheres of dimension $d=2$ to $6$.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:29:06 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 15:24:53 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 06:30:07 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Wang", "Yu Guang", ""], ["Womersley", "Robert S.", ""], ["Wu", "Hau-Tieng", ""], ["Yu", "Wei-Hsuan", ""]]}, {"id": "1907.13501", "submitter": "John Kent", "authors": "John T. Kent and Shambo Bhattacharjee and Weston R. Faber and Islam I.\n  Hussein", "title": "Observation-centered Kalman filters", "comments": "17 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various methods have been proposed for the nonlinear filtering problem,\nincluding the extended Kalman filter (EKF), iterated extended Kalman filter\n(IEKF), unscented Kalman filter (UKF) and iterated unscented Kalman filter\n(IUKF). In this paper two new nonlinear Kalman filters are proposed and\ninvestigated, namely the observation-centered extended Kalman filter (OCEKF)\nand observation-centered unscented Kalman filter (OCUKF). Although the UKF and\nEKF are common default choices for nonlinear filtering, there are situations\nwhere they are bad choices. Examples are given where the EKF and UKF perform\nvery poorly, and the IEKF and OCEKF perform well. In addition the IUKF and\nOCUKF are generally similar to the IEKF and OCEKF, and also perform well,\nthough care is needed in the choice of tuning parameters when the observation\nerror is small. The reasons for this behaviour are explored in detail.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:45:37 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:27:41 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 16:25:26 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Kent", "John T.", ""], ["Bhattacharjee", "Shambo", ""], ["Faber", "Weston R.", ""], ["Hussein", "Islam I.", ""]]}, {"id": "1907.13554", "submitter": "Won Chang", "authors": "Won Chang, Bledar A. Konomi, Georgios Karagiannis, Yawen Guan, Murali\n  Haran", "title": "Ice Model Calibration Using Semi-continuous Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid changes in Earth's cryosphere caused by human activity can lead to\nsignificant environmental impacts. Computer models provide a useful tool for\nunderstanding the behavior and projecting the future of Arctic and Antarctic\nice sheets. However, these models are typically subject to large parametric\nuncertainties due to poorly constrained model input parameters that govern the\nbehavior of simulated ice sheets. Computer model calibration provides a formal\nstatistical framework to infer parameters using observational data, and to\nquantify the uncertainty in projections due to the uncertainty in these\nparameters. Calibration of ice sheet models is often challenging because the\nrelevant model output and observational data take the form of semi-continuous\nspatial data, with a point mass at zero and a right-skewed continuous\ndistribution for positive values. Current calibration approaches cannot handle\nsuch data. Here we introduce a hierarchical latent variable model that handles\nbinary spatial patterns and positive continuous spatial patterns as separate\ncomponents. To overcome challenges due to high-dimensionality we use\nlikelihood-based generalized principal component analysis to impose\nlow-dimensional structures on the latent variables for spatial dependence. We\napply our methodology to calibrate a physical model for the Antarctic ice sheet\nand demonstrate that we can overcome the aforementioned modeling and\ncomputational challenges. As a result of our calibration, we obtain improved\nfuture ice-volume change projections.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:30:36 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chang", "Won", ""], ["Konomi", "Bledar A.", ""], ["Karagiannis", "Georgios", ""], ["Guan", "Yawen", ""], ["Haran", "Murali", ""]]}, {"id": "1907.13563", "submitter": "Francisco Javier Rubio", "authors": "David Rossell and Francisco Javier Rubio", "title": "Additive Bayesian variable selection under censoring and\n  misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the role of misspecification and censoring on Bayesian model\nselection in the contexts of right-censored survival and concave log-likelihood\nregression. Misspecification includes wrongly assuming the censoring mechanism\nto be non-informative. Emphasis is placed on additive accelerated failure time,\nCox proportional hazards and probit models. We offer a theoretical treatment\nthat includes local and non-local priors, and a general non-linear effect\ndecomposition to improve power-sparsity trade-offs. We discuss a fundamental\nquestion: what solution can one hope to obtain when (inevitably) models are\nmisspecified, and how to interpret it? Asymptotically, covariates that do not\nhave predictive power for neither the outcome nor (for survival data) censoring\ntimes, in the sense of reducing a likelihood-associated loss, are discarded.\nMisspecification and censoring have an asymptotically negligible effect on\nfalse positives, but their impact on power is exponential. We show that it can\nbe advantageous to consider simple models that are computationally practical\nyet attain good power to detect potentially complex effects, including the use\nof finite-dimensional basis to detect truly non-parametric effects. We also\ndiscuss algorithms to capitalize on sufficient statistics and fast likelihood\napproximations for Gaussian-based survival and binary models.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:43:40 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 22:19:24 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 18:39:21 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 15:18:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rossell", "David", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "1907.13570", "submitter": "Chris Sherlock Dr.", "authors": "Matthew Ludkin and Chris Sherlock", "title": "Hug and Hop: a discrete-time, non-reversible Markov chain Monte-Carlo\n  algorithm", "comments": "7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduced the Hug and Hop Markov chain Monte Carlo algorithm for\nestimating expectations with respect to an intractable distribution. The\nalgorithm alternates between two kernels: Hug and Hop. Hug is a non-reversible\nkernel that repeatedly applies the bounce mechanism from the recently proposed\nBouncy Particle Sampler to produce a proposal point far from the current\nposition, yet on almost the same contour of the target density, leading to a\nhigh acceptance probability. Hug is complemented by Hop, which deliberately\nproposes jumps between contours and has an efficiency that degrades very slowly\nwith increasing dimension. There are many parallels between Hug and Hamiltonian\nMonte Carlo (HMC) using a leapfrog integrator, including the order of the\nintegration scheme, however Hug is also able to make use of local Hessian\ninformation without requiring implicit numerical integration steps, and its\nperformance is not terminally affected by unbounded gradients of the\nlog-posterior. We test Hug and Hop empirically on a variety of toy targets and\nreal statistical models and find that it can, and often does, outperform HMC.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 12:57:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:36:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ludkin", "Matthew", ""], ["Sherlock", "Chris", ""]]}, {"id": "1907.13620", "submitter": "Haiyan Zheng", "authors": "Haiyan Zheng, Lisa V. Hampson", "title": "A Bayesian decision-theoretic approach to incorporate preclinical\n  information into phase I oncology trials", "comments": "26 pages, 6 figures; accepted by Biometrical Journal", "journal-ref": "Biometrical Journal 2020; 1-19", "doi": "10.1002/bimj.201900161", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Leveraging preclinical animal data for a phase I first-in-man trial is\nappealing yet challenging. A prior based on animal data may place large\nprobability mass on values of the dose-toxicity model parameter(s), which\nappear infeasible in light of data accrued from the ongoing phase I clinical\ntrial. In this paper, we seek to use animal data to improve decision making in\na model-based dose-escalation procedure for phase I oncology trials.\nSpecifically, animal data are incorporated via a robust mixture prior for the\nparameters of the dose-toxicity relationship. This prior changes dynamically as\nthe trial progresses. After completion of treatment for each cohort, the weight\nallocated to the informative component, obtained based on animal data alone, is\nupdated using a decision-theoretic approach to assess the commensurability of\nthe animal data with the human toxicity data observed thus far. In particular,\nwe measure commensurability as a function of the utility of optimal prior\npredictions for the human responses (toxicity or no toxicity) on each\nadministered dose. The proposed methodology is illustrated through several\nexamples and an extensive simulation study. Results show that our proposal can\naddress difficulties in coping with prior-data conflict commencing in\nsequential trials with a small sample size.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:50:54 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 15:29:11 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Zheng", "Haiyan", ""], ["Hampson", "Lisa V.", ""]]}, {"id": "1907.13629", "submitter": "Jeremy Koster", "authors": "Jeremy Koster, George Leckie, Brandy Aven, Christopher Charlton", "title": "Methods and Software for the Multilevel Social Relations Model: A\n  Tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This tutorial demonstrates the estimation and interpretation of the\nMultilevel Social Relations Model for dyadic data. The Social Relations Model\nis appropriate for data structures in which individuals appear multiple times\nas both the source and recipient of dyadic outcomes. Estimated using Stat-JR\nstatistical software, the models are fitted to multiple outcome types:\ncontinuous, count, and binary outcomes. In addition, models are demonstrated\nfor dyadic data from a single group and from multiple groups. The modeling\napproaches are illustrated via a series of case studies, and the data and\nsoftware to replicate these analyses are available as supplemental files.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 13:32:09 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Koster", "Jeremy", ""], ["Leckie", "George", ""], ["Aven", "Brandy", ""], ["Charlton", "Christopher", ""]]}]