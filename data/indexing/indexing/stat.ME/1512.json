[{"id": "1512.00209", "submitter": "Christiane G\\\"orgen", "authors": "Christiane G\\\"orgen and Jim Q. Smith", "title": "Equivalence Classes of Staged Trees", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a complete characterization of the statistical\nequivalence classes of CEGs and of staged trees. We are able to show that all\ngraphical representations of the same model share a common polynomial\ndescription. Then, simple transformations on that polynomial enable us to\ntraverse the corresponding class of graphs. We illustrate our results with a\nreal analysis of the implicit dependence relationships within a previously\nstudied dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 10:33:53 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 10:20:49 GMT"}, {"version": "v3", "created": "Mon, 12 Sep 2016 14:46:47 GMT"}, {"version": "v4", "created": "Fri, 26 May 2017 07:45:44 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["G\u00f6rgen", "Christiane", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1512.00218", "submitter": "Johannes Schmidt-Hieber", "authors": "Kolyan Ray and Johannes Schmidt-Hieber", "title": "Minimax theory for a class of non-linear statistical inverse problems", "comments": "37 pages", "journal-ref": "Inverse Problems 32 (2016) 065003", "doi": "10.1088/0266-5611/32/6/065003", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of statistical inverse problems with non-linear pointwise\noperators motivated by concrete statistical applications. A two-step procedure\nis proposed, where the first step smoothes the data and inverts the\nnon-linearity. This reduces the initial non-linear problem to a linear inverse\nproblem with deterministic noise, which is then solved in a second step. The\nnoise reduction step is based on wavelet thresholding and is shown to be\nminimax optimal (up to logarithmic factors) in a pointwise function-dependent\nsense. Our analysis is based on a modified notion of H\\\"older smoothness scales\nthat are natural in this setting.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 10:53:59 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 12:33:32 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Ray", "Kolyan", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1512.00245", "submitter": "Panayiota Constantinou", "authors": "Panayiota Constantinou and A. Philip Dawid", "title": "Extended Conditional Independence and Applications in Causal Inference", "comments": null, "journal-ref": "Annals of Statistics 45 (2017), 2618-2653", "doi": "10.1214/16-AOS153", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to integrate the notions of stochastic conditional\nindependence and variation conditional independence under a more general notion\nof extended conditional independence. We show that under appropriate\nassumptions the calculus that applies for the two cases separately (axioms of a\nseparoid) still applies for the extended case. These results provide a rigorous\nbasis for a wide range of statistical concepts, including ancillarity and\nsufficiency, and, in particular, the Decision Theoretic framework for\nstatistical causality, which uses the language and calculus of conditional\nindependence in order to express causal properties and make causal inferences.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 13:06:01 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Constantinou", "Panayiota", ""], ["Dawid", "A. Philip", ""]]}, {"id": "1512.00377", "submitter": "Fatma Zehra Do\\u{g}ru", "authors": "Fatma Zehra Do\\u{g}ru and Olcay Arslan", "title": "Robust mixture regression based on the skew t distribution", "comments": "15", "journal-ref": null, "doi": "10.15446/rce.v40n1.53580", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a robust mixture regression procedure based on the\nskew t distribution to model heavy-tailed and/or skewed errors in a mixture\nregression setting. Using the scale mixture representation of the skew t\ndistribution, we give an Expectation Maximization (EM) algorithm to compute the\nmaximum likelihood (ML) estimates for the paramaters of interest. The\nperformance of proposed estimators is demonstrated by a simulation study and a\nreal data example.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 18:34:00 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Do\u011fru", "Fatma Zehra", ""], ["Arslan", "Olcay", ""]]}, {"id": "1512.00475", "submitter": "Claudio Fuentes", "authors": "Luis Leon-Novelo, Claudio Fuentes, Sarah Emerson", "title": "Bayesian Estimation of Negative Binomial Parameters with Applications to\n  RNA-Seq Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA-Seq data characteristically exhibits large variances, which need to be\nappropriately accounted for in the model. We first explore the effects of this\nvariability on the maximum likelihood estimator (MLE) of the overdispersion\nparameter of the negative binomial distribution, and propose instead the use an\nestimator obtained via maximization of the marginal likelihood in a conjugate\nBayesian framework. We show, via simulation studies, that the marginal MLE can\nbetter control this variation and produce a more stable and reliable estimator.\nWe then formulate a conjugate Bayesian hierarchical model, in which the\nestimate of overdispersion is a marginalized estimate and use this estimator to\npropose a Bayesian test to detect differentially expressed genes with RNA-Seq\ndata. We use numerical studies to show that our much simpler approach is\ncompetitive with other negative binomial based procedures, and we use a real\ndata set to illustrate the implementation and flexibility of the procedure.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 21:02:05 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Leon-Novelo", "Luis", ""], ["Fuentes", "Claudio", ""], ["Emerson", "Sarah", ""]]}, {"id": "1512.00538", "submitter": "Bruce Desmarais", "authors": "Skyler J. Cranmer and Bruce A. Desmarais", "title": "A Critique of Dyadic Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyadic research designs concern data that comprises interactions among\nactors. Dyadic approaches unambiguously constitute the most frequent designs\nemployed in the empirical study of international politics, but what do such\ndesigns cary with them in terms of theoretical claims and statistical problems?\nThese two issues are closely intertwined. When testing hypotheses empirically,\nthe statistical model must be a careful operationalization of the theory being\ntested. Given that the theoretical and statistical cannot be separated, we\ndiscuss dyadic research designs from these two perspectives; highlighting model\nmisspecification, erroneous assumptions about independence of events,\nartificial levels of analysis, and the incoherent treatment of\nmultilateral/multiparty events on the theoretical side and difficult-to-escape\nchallenges to valid inference on the statistical side.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 01:11:12 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Cranmer", "Skyler J.", ""], ["Desmarais", "Bruce A.", ""]]}, {"id": "1512.00725", "submitter": "Chee Chun Gan", "authors": "Chee Chun Gan, Gerard Learmonth", "title": "Comparing entropy with tests for randomness as a measure of complexity\n  in time series", "comments": "21 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy measures have become increasingly popular as an evaluation metric for\ncomplexity in the analysis of time series data, especially in physiology and\nmedicine. Entropy measures the rate of information gain, or degree of\nregularity in a time series e.g. heartbeat. Ideally, entropy should be able to\nquantify the complexity of any underlying structure in the series, as well as\ndetermine if the variation arises from a random process. Unfortunately current\nentropy measures mostly are unable to perform the latter differentiation. Thus,\na high entropy score indicates a random or chaotic series, whereas a low score\nindicates a high degree of regularity. This leads to the observation that\ncurrent entropy measures are equivalent to evaluating how random a series is,\nor conversely the degree of regularity in a time series. This raises the\npossibility that existing tests for randomness, such as the runs test or\npermutation test, may have similar utility in diagnosing certain conditions.\nThis paper compares various tests for randomness with existing entropy-based\nmeasurements such as sample entropy, permutation entropy and multi-scale\nentropy. Our experimental results indicate that the test statistics of the runs\ntest and permutation test are often highly correlated with entropy scores and\nmay be able to provide further information regarding the complexity of time\nseries.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 15:00:50 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Gan", "Chee Chun", ""], ["Learmonth", "Gerard", ""]]}, {"id": "1512.00736", "submitter": "B{\\l}a\\.zej Miasojedow", "authors": "B{\\l}a\\.zej Miasojedow, Wojciech Niemiro", "title": "Geometric ergodicity of Rao and Teh's algorithm for Markov jump\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rao and Teh (2013) introduced an efficient MCMC algorithm for sampling from\nthe posterior distribution of a hidden Markov jump process. The algorithm is\nbased on the idea of sampling virtual jumps. In the present paper we show that\nthe Markov chain generated by Rao and Teh's algorithm is geometrically ergodic.\nTo this end we establish a geometric drift condition towards a small set.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 15:35:14 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Miasojedow", "B\u0142a\u017cej", ""], ["Niemiro", "Wojciech", ""]]}, {"id": "1512.00792", "submitter": "Rebecca Steorts", "authors": "Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, and\n  Rebecca C. Steorts", "title": "Microclustering: When the Cluster Sizes Grow Sublinearly with the Size\n  of the Data Set", "comments": "8 pages, 3 figures, NIPS Bayesian Nonparametrics: The Next Generation\n  Workshop Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some tasks, this\nassumption is undesirable. For example, when performing entity resolution, the\nsize of each cluster is often unrelated to the size of the data set.\nConsequently, each cluster contains a negligible fraction of the total number\nof data points. Such tasks therefore require models that yield clusters whose\nsizes grow sublinearly with the size of the data set. We address this\nrequirement by defining the \\emph{microclustering property} and introducing a\nnew model that exhibits this property. We compare this model to several\ncommonly used clustering models by checking model fit using real and simulated\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 18:08:48 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Miller", "Jeffrey", ""], ["Betancourt", "Brenda", ""], ["Zaidi", "Abbas", ""], ["Wallach", "Hanna", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1512.00809", "submitter": "Korbinian Strimmer", "authors": "Agnan Kessy, Alex Lewin, and Korbinian Strimmer", "title": "Optimal whitening and decorrelation", "comments": "14 pages, 2 tables", "journal-ref": "The American Statistician 2018, Vol. 72, No. 4, pp. 309-314", "doi": "10.1080/00031305.2016.1277159", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whitening, or sphering, is a common preprocessing step in statistical\nanalysis to transform random variables to orthogonality. However, due to\nrotational freedom there are infinitely many possible whitening procedures.\nConsequently, there is a diverse range of sphering methods in use, for example\nbased on principal component analysis (PCA), Cholesky matrix decomposition and\nzero-phase component analysis (ZCA), among others.\n  Here we provide an overview of the underlying theory and discuss five natural\nwhitening procedures. Subsequently, we demonstrate that investigating the\ncross-covariance and the cross-correlation matrix between sphered and original\nvariables allows to break the rotational invariance and to identify optimal\nwhitening transformations. As a result we recommend two particular approaches:\nZCA-cor whitening to produce sphered variables that are maximally similar to\nthe original variables, and PCA-cor whitening to obtain sphered variables that\nmaximally compress the original variables.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 18:54:53 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 16:36:44 GMT"}, {"version": "v3", "created": "Thu, 15 Dec 2016 11:27:22 GMT"}, {"version": "v4", "created": "Sun, 18 Dec 2016 00:17:54 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Kessy", "Agnan", ""], ["Lewin", "Alex", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1512.00825", "submitter": "Anne van Delft Dr.", "authors": "Anne van Delft and Michael Eichler", "title": "Data-adaptive estimation of time-varying spectral densities", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 28:2, 244-255,\n  2019", "doi": "10.1080/10618600.2018.1512866", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a data-adaptive non-parametric approach for the\nestimation of time-varying spectral densities from nonstationary time series.\nTime-varying spectral densities are commonly estimated by local kernel\nsmoothing. The performance of these nonparametric estimators, however, depends\ncrucially on the smoothing bandwidths that need to be specified in both time\nand frequency direction. As an alternative and extension to traditional\nbandwidth selection methods, we propose an iterative algorithm for constructing\nlocalized smoothing kernels data-adaptively. The main idea, inspired by the\nconcept of propagation-separation (Polzehl and Spokoiny 2006), is to determine\nfor a point in the time-frequency plane the largest local vicinity over which\nsmoothing is justified by the data. By shaping the smoothing kernels\nnonparametrically, our method not only avoids the problem of bandwidth\nselection in the strict sense but also becomes more flexible. It not only\nadapts to changing curvature in smoothly varying spectra but also adjusts for\nstructural breaks in the time-varying spectrum.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 19:56:38 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 16:18:22 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 22:32:45 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["van Delft", "Anne", ""], ["Eichler", "Michael", ""]]}, {"id": "1512.00877", "submitter": "Jonathan Tuke", "authors": "Jonathan Tuke and Matthew Roughan", "title": "All networks look the same to me: Testing for homogeneity in networks", "comments": "19 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can researchers test for heterogeneity in the local structure of a\nnetwork? In this paper, we present a framework that utilizes random sampling to\ngive subgraphs which are then used in a goodness of fit test to test for\nheterogeneity. We illustrate how to use the goodness of fit test for an\nanalytically derived distribution as well as an empirical distribution. To\ndemonstrate our framework, we consider the simple case of testing for edge\nprobability heterogeneity. We examine the significance level, power and\ncomputation time for this case with appropriate examples. Finally we outline\nhow to apply our framework to other heterogeneity problems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 21:42:15 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Tuke", "Jonathan", ""], ["Roughan", "Matthew", ""]]}, {"id": "1512.00890", "submitter": "Gordon Hiscott", "authors": "Gordon Hiscott, Colin Fox, Matthew Parry, David Bryant", "title": "Efficient recycled algorithms for quantitative trait models on\n  phylogenies", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient and flexible method for computing likelihoods of\nphenotypic traits on a phylogeny. The method does not resort to Monte-Carlo\ncomputation but instead blends Felsenstein's discrete character pruning\nalgorithm with methods for numerical quadrature. It is not limited to Gaussian\nmodels and adapts readily to model uncertainty in the observed trait values. We\ndemonstrate the framework by developing efficient algorithms for likelihood\ncalculation and ancestral state reconstruction under Wright's threshold model,\napplying our methods to a dataset of trait data for extrafloral nectaries\n(EFNs) across a phylogeny of 839 Labales species.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 22:39:32 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Hiscott", "Gordon", ""], ["Fox", "Colin", ""], ["Parry", "Matthew", ""], ["Bryant", "David", ""]]}, {"id": "1512.00905", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela and Philip B. Stark", "title": "Shape-constrained uncertainty quantification in unfolding steeply\n  falling elementary particle spectra", "comments": "44 pages, 9 figures, to appear in the Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP hep-ex physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high energy physics unfolding problem is an important statistical inverse\nproblem in data analysis at the Large Hadron Collider (LHC) at CERN. The goal\nof unfolding is to make nonparametric inferences about a particle spectrum from\nmeasurements smeared by the finite resolution of the particle detectors.\nPrevious unfolding methods use ad hoc discretization and regularization,\nresulting in confidence intervals that can have significantly lower coverage\nthan their nominal level. Instead of regularizing using a roughness penalty or\nstopping iterative methods early, we impose physically motivated shape\nconstraints: positivity, monotonicity, and convexity. We quantify the\nuncertainty by constructing a nonparametric confidence set for the true\nspectrum, consisting of all those spectra that satisfy the shape constraints\nand that predict the observations within an appropriately calibrated level of\nfit. Projecting that set produces simultaneous confidence intervals for all\nfunctionals of the spectrum, including averages within bins. The confidence\nintervals have guaranteed conservative frequentist finite-sample coverage in\nthe important and challenging class of unfolding problems for steeply falling\nparticle spectra. We demonstrate the method using simulations that mimic\nunfolding the inclusive jet transverse momentum spectrum at the LHC. The\nshape-constrained intervals provide usefully tight conservative inferences,\nwhile the conventional methods suffer from severe undercoverage.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 23:46:05 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 01:38:00 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2016 16:00:56 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 01:43:03 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Kuusela", "Mikael", ""], ["Stark", "Philip B.", ""]]}, {"id": "1512.00947", "submitter": "Momiao Xiong", "authors": "Panpan Wang, Mohammad Rahman, Li Jin and Momiao Xiong", "title": "A New Statistical Framework for Genetic Pleiotropic Analysis of High\n  Dimensional Phenotype Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely used genetic pleiotropic analysis of multiple phenotypes are often\ndesigned for examining the relationship between common variants and a few\nphenotypes. They are not suited for both high dimensional phenotypes and high\ndimensional genotype (next-generation sequencing) data. To overcome these\nlimitations, we develop sparse structural equation models (SEMs) as a general\nframework for a new paradigm of genetic analysis of multiple phenotypes. To\nincorporate both common and rare variants into the analysis, we extend the\ntraditional multivariate SEMs to sparse functional SEMs. To deal with high\ndimensional phenotype and genotype data, we employ functional data analysis and\nthe alternative direction methods of multiplier (ADMM) techniques to reduce\ndata dimension and improve computational efficiency. Using large scale\nsimulations we showed that the proposed methods have higher power to detect\ntrue causal genetic pleiotropic structure than other existing methods.\nSimulations also demonstrate that the gene-based pleiotropic analysis has\nhigher power than the single variant-based pleiotropic analysis. The proposed\nmethod is applied to exome sequence data from the NHLBI Exome Sequencing\nProject (ESP) with 11 phenotypes, which identifies a network with 137 genes\nconnected to 11 phenotypes and 341 edges. Among them, 114 genes showed\npleiotropic genetic effects and 45 genes were reported to be associated with\nphenotypes in the analysis or other cardiovascular disease (CVD) related\nphenotypes in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 04:49:42 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Wang", "Panpan", ""], ["Rahman", "Mohammad", ""], ["Jin", "Li", ""], ["Xiong", "Momiao", ""]]}, {"id": "1512.00969", "submitter": "Daniel Williamson", "authors": "Daniel Williamson, Michael Goldstein", "title": "Posterior Belief Assessment: Extracting Meaningful Subjective Judgements\n  from Bayesian Analyses with Complex Statistical Models", "comments": "Published at http://dx.doi.org/10.1214/15-BA966SI in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 4, 877-908", "doi": "10.1214/15-BA966SI", "report-no": "VTeX-BA-BA966SI", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with attributing meaning to the results of a\nBayesian analysis for a problem which is sufficiently complex that we are\nunable to assert a precise correspondence between the expert probabilistic\njudgements of the analyst and the particular forms chosen for the prior\nspecification and the likelihood for the analysis. In order to do this, we\npropose performing a finite collection of additional Bayesian analyses under\nalternative collections of prior and likelihood modelling judgements that we\nmay also view as representative of our prior knowledge and the problem\nstructure, and use these to compute posterior belief assessments for key\nquantities of interest. We show that these assessments are closer to our true\nunderlying beliefs than the original Bayesian analysis and use the temporal\nsure preference principle to establish a probabilistic relationship between our\ntrue posterior judgements, our posterior belief assessment and our original\nBayesian analysis to make this precise. We exploit second order exchangeability\nin order to generalise our approach to situations where there are infinitely\nmany alternative Bayesian analyses we might consider as informative for our\ntrue judgements so that the method remains tractable even in these cases. We\nargue that posterior belief assessment is a tractable and powerful alternative\nto robust Bayesian analysis. We describe a methodology for computing posterior\nbelief assessments in even the most complex of statistical models and\nillustrate with an example of calibrating an expensive ocean model in order to\nquantify uncertainty about global mean temperature in the real ocean.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 07:10:07 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Williamson", "Daniel", ""], ["Goldstein", "Michael", ""]]}, {"id": "1512.00976", "submitter": "Lutz Gruber", "authors": "Lutz Gruber, Claudia Czado", "title": "Sequential Bayesian Model Selection of Regular Vine Copulas", "comments": "Published at http://dx.doi.org/10.1214/14-BA930 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 4, 937-963", "doi": "10.1214/14-BA930", "report-no": "VTeX-BA-BA930", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular vine copulas can describe a wider array of dependency patterns than\nthe multivariate Gaussian copula or the multivariate Student's t copula. This\npaper presents two contributions related to model selection of regular vine\ncopulas. First, our pair copula family selection procedure extends existing\nBayesian family selection methods by allowing pair families to be chosen from\nan arbitrary set of candidate families. Second, our method represents the first\nBayesian model selection approach to include the regular vine density\nconstruction in its scope of inference. The merits of our approach are\nestablished in a simulation study that benchmarks against methods suggested in\ncurrent literature. A real data example about forecasting of portfolio asset\nreturns for risk measurement and investment allocation illustrates the\nviability and relevance of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 07:39:13 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Gruber", "Lutz", ""], ["Czado", "Claudia", ""]]}, {"id": "1512.00982", "submitter": "Jere Koskela", "authors": "Jere Koskela and Paul A. Jenkins and Dario Span\\`o", "title": "Bayesian non-parametric inference for $\\Lambda$-coalescents: consistency\n  and a parametric method", "comments": "28 pages, 3 figures", "journal-ref": "Bernoulli 24(3):2122-2153, 2018", "doi": "10.3150/16-BEJ923", "report-no": null, "categories": "stat.ME math.PR math.ST q-bio.PE stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Bayesian non-parametric inference of the $\\Lambda$-measure of\n$\\Lambda$-coalescent processes with recurrent mutation, parametrised by\nprobability measures on the unit interval. We give verifiable criteria on the\nprior for posterior consistency when observations form a time series, and prove\nthat any non-trivial prior is inconsistent when all observations are\ncontemporaneous. We then show that the likelihood given a data set of size $n\n\\in \\mathbb{N}$ is constant across $\\Lambda$-measures whose leading $n - 2$\nmoments agree, and focus on inferring truncated sequences of moments. We\nprovide a large class of functionals which can be extremised using finite\ncomputation given a credible region of posterior truncated moment sequences,\nand a pseudo-marginal Metropolis-Hastings algorithm for sampling the posterior.\nFinally, we compare the efficiency of the exact and noisy pseudo-marginal\nalgorithms with and without delayed acceptance acceleration using a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 08:03:36 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 15:32:37 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 11:03:21 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 16:19:58 GMT"}, {"version": "v5", "created": "Mon, 23 Jan 2017 18:21:10 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Koskela", "Jere", ""], ["Jenkins", "Paul A.", ""], ["Span\u00f2", "Dario", ""]]}, {"id": "1512.01065", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer and Leonhard Held", "title": "Incorporating social contact data in spatio-temporal models for\n  infectious disease spread", "comments": "accepted manuscript; 14 pages, including 4 figures and 1 table", "journal-ref": "Biostatistics (2017); 18(2):338-351", "doi": "10.1093/biostatistics/kxw051", "report-no": null, "categories": "stat.ME physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routine public health surveillance of notifiable infectious diseases gives\nrise to weekly counts of reported cases -- possibly stratified by region and/or\nage group. We investigate how an age-structured social contact matrix can be\nincorporated into a spatio-temporal endemic-epidemic model for infectious\ndisease counts. To illustrate the approach, we analyze the spread of norovirus\ngastroenteritis over 6 age groups within the 12 districts of Berlin, 2011-2015,\nusing contact data from the POLYMOD study. The proposed age-structured model\noutperforms alternative scenarios with homogeneous or no mixing between age\ngroups. An extended contact model suggests a power transformation of the\nsurvey-based contact matrix towards more within-group transmission.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 13:02:57 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 15:50:16 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Meyer", "Sebastian", ""], ["Held", "Leonhard", ""]]}, {"id": "1512.01169", "submitter": "Thomas Lugrin", "authors": "Thomas Lugrin, Anthony C. Davison and Jonathan A. Tawn", "title": "Bayesian Uncertainty Management in Temporal Dependence of Extremes", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both marginal and dependence features must be described when modelling the\nextremes of a stationary time series. There are standard approaches to marginal\nmodelling, but long- and short-range dependence of extremes may both appear. In\napplications, an assumption of long-range independence often seems reasonable,\nbut short-range dependence, i.e., the clustering of extremes, needs attention.\nThe extremal index $0<\\theta\\le 1$ is a natural limiting measure of clustering,\nbut for wide classes of dependent processes, including all stationary Gaussian\nprocesses, it cannot distinguish dependent processes from independent processes\nwith $\\theta=1$. Eastoe and Tawn (2012) exploit methods from multivariate\nextremes to treat the subasymptotic extremal dependence structure of stationary\ntime series, covering both $0<\\theta<1$ and $\\theta=1$, through the\nintroduction of a threshold-based extremal index. Inference for their\ndependence models uses an inefficient stepwise procedure that has various\nweaknesses and has no reliable assessment of uncertainty. We overcome these\nissues using a Bayesian semiparametric approach. Simulations and the analysis\nof a UK daily river flow time series show that the new approach provides\nimproved efficiency for estimating properties of functionals of clusters.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 17:33:25 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 14:11:36 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Lugrin", "Thomas", ""], ["Davison", "Anthony C.", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1512.01241", "submitter": "Nikhil Padmanabhan", "authors": "Nikhil Padmanabhan, Martin White, Harrison H. Zhou, Ross O'Connell", "title": "Estimating sparse precision matrices", "comments": "11 pages, 14 figures, submitted to MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stw1042", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a method recently introduced to the statistical literature to\ndirectly estimate the precision matrix from an ensemble of samples drawn from a\ncorresponding Gaussian distribution. Motivated by the observation that\ncosmological precision matrices are often approximately sparse, the method\nallows one to exploit this sparsity of the precision matrix to more quickly\nconverge to an asymptotic 1/sqrt(Nsim) rate while simultaneously providing an\nerror model for all of the terms. Such an estimate can be used as the starting\npoint for further regularization efforts which can improve upon the\n1/sqrt(Nsim) limit above, and incorporating such additional steps is\nstraightforward within this framework. We demonstrate the technique with toy\nmodels and with an example motivated by large-scale structure two-point\nanalysis, showing significant improvements in the rate of convergence.For the\nlarge-scale structure example we find errors on the precision matrix which are\nfactors of 5 smaller than for the sample precision matrix for thousands of\nsimulations or, alternatively, convergence to the same error level with more\nthan an order of magnitude fewer simulations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 21:00:14 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Padmanabhan", "Nikhil", ""], ["White", "Martin", ""], ["Zhou", "Harrison H.", ""], ["O'Connell", "Ross", ""]]}, {"id": "1512.01255", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Moritz Grosse-Wentrup, Arthur Gretton", "title": "MERLiN: Mixture Effect Recovery in Linear Networks", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 10(7),\n  1254-1266, 2016", "doi": "10.1109/JSTSP.2016.2601144", "report-no": null, "categories": "stat.ME q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference concerns the identification of cause-effect relationships\nbetween variables, e.g. establishing whether a stimulus affects activity in a\ncertain brain region. The observed variables themselves often do not constitute\nmeaningful causal variables, however, and linear combinations need to be\nconsidered. In electroencephalographic studies, for example, one is not\ninterested in establishing cause-effect relationships between electrode signals\n(the observed variables), but rather between cortical signals (the causal\nvariables) which can be recovered as linear combinations of electrode signals.\n  We introduce MERLiN (Mixture Effect Recovery in Linear Networks), a family of\ncausal inference algorithms that implement a novel means of constructing causal\nvariables from non-causal variables. We demonstrate through application to EEG\ndata how the basic MERLiN algorithm can be extended for application to\ndifferent (neuroimaging) data modalities. Given an observed linear mixture, the\nalgorithms can recover a causal variable that is a linear effect of another\ngiven variable. That is, MERLiN allows us to recover a cortical signal that is\naffected by activity in a certain brain region, while not being a direct effect\nof the stimulus. The Python/Matlab implementation for all presented algorithms\nis available on https://github.com/sweichwald/MERLiN\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 21:29:30 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 17:43:18 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 21:27:57 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Grosse-Wentrup", "Moritz", ""], ["Gretton", "Arthur", ""]]}, {"id": "1512.01257", "submitter": "Milan Stehlik", "authors": "Milan Stehlik, Christian Helpersdorfer, Philipp Hermann", "title": "Optimal design, financial and risk modelling with stochastic processes\n  having semicontinuous covariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A.N. Kolmogorov proposed several problems on stochastic processes, which has\nbeen rarely addressed later on. One of the open problems are stochastic\nprocesses with discontinuous covariance function. For example, semicontinuous\ncovariance functions have been used in regression and kriging by many authors\nin statistics recently. In this paper we introduce purely topologically defined\nregularity conditions on covariance kernels which are still applicable for\nincreasing and infill domain asymptotics for regression problems, kriging and\nfinance. These conditions are related to semicontinuous maps of Ornstein\nUhlenbeck (OU) processes. Beside this new regularity conditions relax the\ncontinuity of covariance function by consideration of semicontinuous\ncovariance. We provide several novel applications of the introduced class for\noptimal design of random fields, random walks in finance and probabilities of\nruins related to shocks, e.g. by earthquakes. In particular we construct a\nrandom walk model with semicontinuous covariance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 21:32:26 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Stehlik", "Milan", ""], ["Helpersdorfer", "Christian", ""], ["Hermann", "Philipp", ""]]}, {"id": "1512.01473", "submitter": "Claudio Agostinelli", "authors": "Claudio Agostinelli, Isabella Locatelli, Alfio Marazzi and Victor J.\n  Yohai", "title": "Robust estimators of accelerated failure time regression with\n  generalized log-gamma errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized log-gamma (GLG) model is a very flexible family of\ndistributions to analyze datasets in many different areas of science and\ntechnology. In this paper, we propose estimators which are simultaneously\nhighly robust and highly efficient for the parameters of a GLG distribution in\nthe presence of censoring. We also introduced estimators with the same\nproperties for accelerated failure time models with censored observations and\nerror distribution belonging to the GLG family. We prove that the proposed\nestimators are asymptotically fully efficient and examine the maximum mean\nsquare error using Monte Carlo simulations. The simulations confirm that the\nproposed estimators are highly robust and highly efficient for finite sample\nsize. Finally, we illustrate the good behavior of the proposed estimators with\ntwo real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 16:33:57 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Agostinelli", "Claudio", ""], ["Locatelli", "Isabella", ""], ["Marazzi", "Alfio", ""], ["Yohai", "Victor J.", ""]]}, {"id": "1512.01524", "submitter": "Rebecca Barter", "authors": "Rebecca L Barter and Bin Yu", "title": "Superheat: An R package for creating beautiful and extendable heatmaps\n  for visualizing complex data", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technological advancements of the modern era have enabled the collection\nof huge amounts of data in science and beyond. Extracting useful information\nfrom such massive datasets is an ongoing challenge as traditional data\nvisualization tools typically do not scale well in high-dimensional settings.\nAn existing visualization technique that is particularly well suited to\nvisualizing large datasets is the heatmap. Although heatmaps are extremely\npopular in fields such as bioinformatics for visualizing large gene expression\ndatasets, they remain a severely underutilized visualization tool in modern\ndata analysis. In this paper we introduce superheat, a new R package that\nprovides an extremely flexible and customizable platform for visualizing large\ndatasets using extendable heatmaps. Superheat enhances the traditional heatmap\nby providing a platform to visualize a wide range of data types simultaneously,\nadding to the heatmap a response variable as a scatterplot, model results as\nboxplots, correlation information as barplots, text information, and more.\nSuperheat allows the user to explore their data to greater depths and to take\nadvantage of the heterogeneity present in the data to inform analysis\ndecisions. The goal of this paper is two-fold: (1) to demonstrate the potential\nof the heatmap as a default visualization method for a wide range of data types\nusing reproducible examples, and (2) to highlight the customizability and ease\nof implementation of the superheat package in R for creating beautiful and\nextendable heatmaps. The capabilities and fundamental applicability of the\nsuperheat package will be explored via three case studies, each based on\npublicly available data sources and accompanied by a file outlining the\nstep-by-step analytic pipeline (with code).\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 19:56:48 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 22:59:05 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Barter", "Rebecca L", ""], ["Yu", "Bin", ""]]}, {"id": "1512.01616", "submitter": "Barbara Engelhardt", "authors": "Bianca Dumitrascu, Gregory Darnell, Julien Ayroles, Barbara E\n  Engelhardt", "title": "A Bayesian test to identify variance effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying genetic variants that regulate quantitative traits, or QTLs, is\nthe primary focus of the field of statistical genetics. Most current methods\nare limited to identifying mean effects, or associations between genotype and\nthe mean value of a quantitative trait. It is possible, however, that a genetic\nvariant may affect the variance of the quantitative trait in lieu of, or in\naddition to, affecting the trait mean. Here, we develop a general\nmethodological approach to identifying covariates with variance effects on a\nquantitative trait using a Bayesian heteroskedastic linear regression model. We\nshow that our Bayesian test for heteroskedasticity (BTH) outperforms classical\ntests for differences in variation across a large range of simulations drawn\nfrom scenarios common to the analysis of quantitative traits. We apply BTH to\nmethylation QTL study data and expression QTL study data to identify variance\nQTLs. When compared with three tests for heteroskedasticity used in genomics,\nwe illustrate the benefits of using our approach, including avoiding\noverfitting by incorporating uncertainty and flexibly identifying\nheteroskedastic effects.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 03:41:00 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Dumitrascu", "Bianca", ""], ["Darnell", "Gregory", ""], ["Ayroles", "Julien", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1512.01617", "submitter": "Wen-Xin Zhou", "authors": "Jianqing Fan, Wen-Xin Zhou", "title": "Guarding against Spurious Discoveries in High Dimensions", "comments": "49 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data mining and statistical machine learning algorithms have been\ndeveloped to select a subset of covariates to associate with a response\nvariable. Spurious discoveries can easily arise in high-dimensional data\nanalysis due to enormous possibilities of such selections. How can we know\nstatistically our discoveries better than those by chance? In this paper, we\ndefine a measure of goodness of spurious fit, which shows how good a response\nvariable can be fitted by an optimally selected subset of covariates under the\nnull model, and propose a simple and effective LAMM algorithm to compute it. It\ncoincides with the maximum spurious correlation for linear models and can be\nregarded as a generalized maximum spurious correlation. We derive the\nasymptotic distribution of such goodness of spurious fit for generalized linear\nmodels and $L_1$ regression. Such an asymptotic distribution depends on the\nsample size, ambient dimension, the number of variables used in the fit, and\nthe covariance information. It can be consistently estimated by multiplier\nbootstrapping and used as a benchmark to guard against spurious discoveries. It\ncan also be applied to model selection, which considers only candidate models\nwith goodness of fits better than those by spurious fits. The theory and method\nare convincingly illustrated by simulated examples and an application to the\nbinary outcomes from German Neuroblastoma Trials.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 03:48:46 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 18:44:34 GMT"}, {"version": "v3", "created": "Sun, 23 Oct 2016 15:54:59 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Fan", "Jianqing", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1512.01631", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan and Jacob Bien", "title": "Hierarchical Sparse Modeling: A Choice of Two Group Lasso Formulations", "comments": "30 pages, 13 figures", "journal-ref": "Statist. Sci. 32 (2017), no. 4, 531--560", "doi": "10.1214/17-STS622", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demanding sparsity in estimated models has become a routine practice in\nstatistics. In many situations, we wish to require that the sparsity patterns\nattained honor certain problem-specific constraints. Hierarchical sparse\nmodeling (HSM) refers to situations in which these constraints specify that one\nset of parameters be set to zero whenever another is set to zero. In recent\nyears, numerous papers have developed convex regularizers for this form of\nsparsity structure, which arises in many areas of statistics including\ninteraction modeling, time series analysis, and covariance estimation. In this\npaper, we observe that these methods fall into two frameworks, the group lasso\n(GL) and latent overlapping group lasso (LOG), which have not been\nsystematically compared in the context of HSM. The purpose of this paper is to\nprovide a side-by-side comparison of these two frameworks for HSM in terms of\ntheir statistical properties and computational efficiency. We call special\nattention to GL's more aggressive shrinkage of parameters deep in the\nhierarchy, a property not shared by LOG. In terms of computation, we introduce\na finite-step algorithm that exactly solves the proximal operator of LOG for a\ncertain simple HSM structure; we later exploit this to develop a novel\npath-based block coordinate descent scheme for general HSM structures. Both\nalgorithms greatly improve the computational performance of LOG. Finally, we\ncompare the two methods in the context of covariance estimation, where we\nintroduce a new sparsely-banded estimator using LOG, which we show achieves the\nstatistical advantages of an existing GL-based method but is simpler to express\nand more efficient to compute.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 07:00:54 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 02:13:49 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 04:03:45 GMT"}, {"version": "v4", "created": "Wed, 29 Nov 2017 20:05:56 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bien", "Jacob", ""]]}, {"id": "1512.01633", "submitter": "Claudio Agostinelli", "authors": "Claudio Agostinelli, Alfio Marazzi, Victor J. Yohai and Alex\n  Randriamiharisoa", "title": "Robust Estimation of the Generalized Loggamma Model. The R Package\n  robustloggamma", "comments": "Accepted in Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  robustloggamma is an R package for robust estimation and inference in the\ngeneralized loggamma model. We briefly introduce the model, the estimation\nprocedures and the computational algorithms. Then, we illustrate the use of the\npackage with the help of a real data set.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 07:11:26 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Agostinelli", "Claudio", ""], ["Marazzi", "Alfio", ""], ["Yohai", "Victor J.", ""], ["Randriamiharisoa", "Alex", ""]]}, {"id": "1512.01779", "submitter": "Zaid Sawlan", "authors": "Ivo Babuska, Zaid Sawlan, Marco Scavino, Barna Szab\\'o, Ra\\'ul Tempone", "title": "Bayesian inference and model comparison for metallic fatigue data", "comments": null, "journal-ref": "Computer Methods in Applied Mechanics and Engineering 304 (2016)\n  171--196", "doi": "10.1016/j.cma.2016.02.013", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a statistical treatment of stress-life (S-N) data\ndrawn from a collection of records of fatigue experiments that were performed\non 75S-T6 aluminum alloys. Our main objective is to predict the fatigue life of\nmaterials by providing a systematic approach to model calibration, model\nselection and model ranking with reference to S-N data. To this purpose, we\nconsider fatigue-limit models and random fatigue-limit models that are\nspecially designed to allow the treatment of the run-outs (right-censored\ndata). We first fit the models to the data by maximum likelihood methods and\nestimate the quantiles of the life distribution of the alloy specimen. To\nassess the robustness of the estimation of the quantile functions, we obtain\nbootstrap confidence bands by stratified resampling with respect to the cycle\nratio. We then compare and rank the models by classical measures of fit based\non information criteria. We also consider a Bayesian approach that provides,\nunder the prior distribution of the model parameters selected by the user,\ntheir simulation-based posterior distributions. We implement and apply Bayesian\nmodel comparison methods, such as Bayes factor ranking and predictive\ninformation criteria based on cross-validation techniques under various a\npriori scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 11:30:34 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Babuska", "Ivo", ""], ["Sawlan", "Zaid", ""], ["Scavino", "Marco", ""], ["Szab\u00f3", "Barna", ""], ["Tempone", "Ra\u00fal", ""]]}, {"id": "1512.02097", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based\n  Parameter-Insensitive Clustering Method", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most density-based clustering methods largely rely on how well the underlying\ndensity is estimated. However, density estimation itself is also a challenging\nproblem, especially the determination of the kernel bandwidth. A large\nbandwidth could lead to the over-smoothed density estimation in which the\nnumber of density peaks could be less than the true clusters, while a small\nbandwidth could lead to the under-smoothed density estimation in which spurious\ndensity peaks, or called the \"ripple noise\", would be generated in the\nestimated density. In this paper, we propose a density-based hierarchical\nclustering method, called the Deep Nearest Neighbor Descent (D-NND), which\ncould learn the underlying density structure layer by layer and capture the\ncluster structure at the same time. The over-smoothed density estimation could\nbe largely avoided and the negative effect of the under-estimated cases could\nbe also largely reduced. Overall, D-NND presents not only the strong capability\nof discovering the underlying cluster structure but also the remarkable\nreliability due to its insensitivity to parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 15:47:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1512.02149", "submitter": "Arnab Hazra", "authors": "Arnab Hazra", "title": "A Time-varying Parameter Based Seasonally-adjusted Bayesian State-space\n  Model for Forecasting", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a time-varying parameter based seasonally-adjusted\nBayesian state-space model for non-stationary time series datasets where both\nthe trend and seasonal components are present and it is the general scenario\nfor most of the real datasets in various scientific disciplines. In spite of\nremoving such terms using some do-and-check procedure to make the data\nstationary, our model directly fits a dataset and forecasts a number of future\nobservations. For a specific prior construction we have considered, every\nparameter update is one-dimensional so that we don't need to invert any matrix\nand also we overcome the difficulty of Metropolis-Hastings steps simply by\nGibbs sampling which is another advantage of this model. It can handle missing\ndata as well which occurs very often in time series contexts. We implement it\non the sufficiently large (24 years of monthly average temperature series, i.e.\nthe number of observations =288) for 57 meteorological stations across India\nand show that for most of the cases, our method forecasts quite accurately for\nthe months of the 25-th year.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 18:13:32 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Hazra", "Arnab", ""]]}, {"id": "1512.02174", "submitter": "Eric Tchetgen Tchetgen", "authors": "James Robins, Lingling Li, Rajarshi Mukherjee, Eric Tchetgen Tchetgen,\n  Aad van der Vaart", "title": "Higher Order Estimating Equations for High-dimensional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method of estimation of parameters in semiparametric and\nnonparametric models. The method is based on estimating equations that are\n$U$-statistics in the observations. The $U$-statistics are based on higher\norder influence functions that extend ordinary linear influence functions of\nthe parameter of interest, and represent higher derivatives of this parameter.\nFor parameters for which the representation cannot be perfect the method leads\nto a bias-variance trade-off, and results in estimators that converge at a\nslower than $\\sqrt n$-rate. In a number of examples the resulting rate can be\nshown to be optimal. We are particularly interested in estimating parameters in\nmodels with a nuisance parameter of high dimension or low regularity, where the\nparameter of interest cannot be estimated at $\\sqrt n$-rate, but we also\nconsider efficient $\\sqrt n$-estimation using novel nonlinear estimators. The\ngeneral approach is applied in detail to the example of estimating a mean\nresponse when the response is not always observed.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 19:13:25 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 02:08:59 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Robins", "James", ""], ["Li", "Lingling", ""], ["Mukherjee", "Rajarshi", ""], ["Tchetgen", "Eric Tchetgen", ""], ["van der Vaart", "Aad", ""]]}, {"id": "1512.02280", "submitter": "Eric Tchetgen Tchetgen", "authors": "James Robins, Lingling Li, Eric Tchetgen Tchetgen, Aad van der Vaart", "title": "Asymptotic Normality of Quadratic Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove conditional asymptotic normality of a class of quadratic\nU-statistics that are dominated by their degenerate second order part and have\nkernels that change with the number of observations. These statistics arise in\nthe construction of estimators in high-dimensional semi- and non-parametric\nmodels, and in the construction of nonparametric confidence sets. This is\nillustrated by estimation of the integral of a square of a density or\nregression function, and estimation of the mean response with missing data. We\nshow that estimators are asymptotically normal even in the case that the rate\nis slower than the square root of the observations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 23:10:01 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Robins", "James", ""], ["Li", "Lingling", ""], ["Tchetgen", "Eric Tchetgen", ""], ["van der Vaart", "Aad", ""]]}, {"id": "1512.02487", "submitter": "Linjun Zhang", "authors": "T. Tony Cai and Linjun Zhang", "title": "High-Dimensional Gaussian Copula Regression: Adaptive Estimation and\n  Statistical Inference", "comments": "41 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop adaptive estimation and inference methods for high-dimensional\nGaussian copula regression that achieve the same performance without the\nknowledge of the marginal transformations as that for high-dimensional linear\nregression. Using a Kendall's tau based covariance matrix estimator, an\n$\\ell_1$ regularized estimator is proposed and a corresponding de-biased\nestimator is developed for the construction of the confidence intervals and\nhypothesis tests. Theoretical properties of the procedures are studied and the\nproposed estimation and inference methods are shown to be adaptive to the\nunknown monotone marginal transformations. Prediction of the response for a\ngiven value of the covariates is also considered. The procedures are easy to\nimplement and perform well numerically. The methods are also applied to analyze\nthe Communities and Crime Unnormalized Data from the UCI Machine Learning\nRepository.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 14:40:55 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Linjun", ""]]}, {"id": "1512.02565", "submitter": "William Fithian", "authors": "William Fithian, Jonathan Taylor, Robert Tibshirani, and Ryan\n  Tibshirani", "title": "Selective Sequential Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many model selection algorithms produce a path of fits specifying a sequence\nof increasingly complex models. Given such a sequence and the data used to\nproduce them, we consider the problem of choosing the least complex model that\nis not falsified by the data. Extending the selected-model tests of Fithian et\nal. (2014), we construct p-values for each step in the path which account for\nthe adaptive selection of the model path using the data. In the case of linear\nregression, we propose two specific tests, the max-t test for forward stepwise\nregression (generalizing a proposal of Buja and Brown (2014)), and the\nnext-entry test for the lasso. These tests improve on the power of the\nsaturated-model test of Tibshirani et al. (2014), sometimes dramatically. In\naddition, our framework extends beyond linear regression to a much more general\nclass of parametric and nonparametric model selection problems.\n  To select a model, we can feed our single-step p-values as inputs into\nsequential stopping rules such as those proposed by G'Sell et al. (2013) and Li\nand Barber (2015), achieving control of the familywise error rate or false\ndiscovery rate (FDR) as desired. The FDR-controlling rules require the null\np-values to be independent of each other and of the non-null p-values, a\ncondition not satisfied by the saturated-model p-values of Tibshirani et al.\n(2014). We derive intuitive and general sufficient conditions for independence,\nand show that our proposed constructions yield independent p-values.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 17:52:16 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Fithian", "William", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1512.02578", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick, Michael Jordan", "title": "Robust Inference with Variational Bayes", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian analysis, the posterior follows from the data and a choice of a\nprior and a likelihood. One hopes that the posterior is robust to reasonable\nvariation in the choice of prior and likelihood, since this choice is made by\nthe modeler and is necessarily somewhat subjective. Despite the fundamental\nimportance of the problem and a considerable body of literature, the tools of\nrobust Bayes are not commonly used in practice. This is in large part due to\nthe difficulty of calculating robustness measures from MCMC draws. Although\nmethods for computing robustness measures from MCMC draws exist, they lack\ngenerality and often require additional coding or computation.\n  In contrast to MCMC, variational Bayes (VB) techniques are readily amenable\nto robustness analysis. The derivative of a posterior expectation with respect\nto a prior or data perturbation is a measure of local robustness to the prior\nor likelihood. Because VB casts posterior inference as an optimization problem,\nits methodology is built on the ability to calculate derivatives of posterior\nquantities with respect to model parameters, even in very complex models. In\nthe present work, we develop local prior robustness measures for mean-field\nvariational Bayes(MFVB), a VB technique which imposes a particular\nfactorization assumption on the variational posterior approximation. We start\nby outlining existing local prior measures of robustness. Next, we use these\nresults to derive closed-form measures of the sensitivity of mean-field\nvariational posterior approximation to prior specification. We demonstrate our\nmethod on a meta-analysis of randomized controlled interventions in access to\nmicrocredit in developing countries.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 18:33:05 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""], ["Jordan", "Michael", ""]]}, {"id": "1512.02666", "submitter": "Damian Kozbur", "authors": "Damian Kozbur", "title": "Analysis of Testing-Based Forward Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and analyzes a procedure called Testing-based forward\nmodel selection (TBFMS) in linear regression problems. This procedure\ninductively selects covariates that add predictive power into a working\nstatistical model before estimating a final regression. The criterion for\ndeciding which covariate to include next and when to stop including covariates\nis derived from a profile of traditional statistical hypothesis tests. This\npaper proves probabilistic bounds, which depend on the quality of the tests,\nfor prediction error and the number of selected covariates. As an example, the\nbounds are then specialized to a case with heteroskedastic data, with tests\nconstructed with the help of Huber-Eicker-White standard errors. Under the\nassumed regularity conditions, these tests lead to estimation convergence rates\nmatching other common high-dimensional estimators including Lasso.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 21:19:10 GMT"}, {"version": "v10", "created": "Tue, 25 Feb 2020 14:58:09 GMT"}, {"version": "v11", "created": "Thu, 27 Feb 2020 17:11:27 GMT"}, {"version": "v12", "created": "Tue, 3 Mar 2020 17:51:26 GMT"}, {"version": "v13", "created": "Mon, 6 Apr 2020 17:01:39 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 17:07:58 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 17:30:26 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 11:33:01 GMT"}, {"version": "v5", "created": "Tue, 17 Apr 2018 12:02:20 GMT"}, {"version": "v6", "created": "Thu, 7 Jun 2018 12:21:07 GMT"}, {"version": "v7", "created": "Fri, 4 Oct 2019 12:49:29 GMT"}, {"version": "v8", "created": "Fri, 3 Jan 2020 14:03:44 GMT"}, {"version": "v9", "created": "Mon, 13 Jan 2020 21:50:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kozbur", "Damian", ""]]}, {"id": "1512.02764", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Alan H. Welsh and Rheanna Mainzer", "title": "The Performance of the Turek-Fletcher Model Averaged Confidence Interval", "comments": null, "journal-ref": "The performance of model averaged tail area confidence intervals.\n  Communications in Statistics - Theory and Methods (2017)", "doi": "10.1080/03610926.2016.1242741", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the model averaged tail area (MATA) confidence interval proposed\nby Turek and Fletcher, CSDA, 2012, in the simple situation in which we average\nover two nested linear regression models. We prove that the MATA for any\nreasonable weight function belongs to the class of confidence intervals defined\nby Kabaila and Giri, JSPI, 2009. Each confidence interval in this class is\nspecified by two functions b and s. Kabaila and Giri show how to compute these\nfunctions so as to optimize these intervals in terms of satisfying the coverage\nconstraint and minimizing the expected length for the simpler model, while\nensuring that the expected length has desirable properties for the full model.\nThese Kabaila and Giri \"optimized\" intervals provide an upper bound on the\nperformance of the MATA for an arbitrary weight function. This fact is used to\nevaluate the MATA for a broad class of weights based on exponentiating a\ncriterion related to Mallows' C_P. Our results show that, while far from ideal,\nthis MATA performs surprisingly well, provided that we choose a member of this\nclass that does not put too much weight on the simpler model.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 06:11:58 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Welsh", "Alan H.", ""], ["Mainzer", "Rheanna", ""]]}, {"id": "1512.02773", "submitter": "Yasin Asar", "authors": "Yasin Asar and A\\c{s}{\\i}r Gen\\c{c}", "title": "On Some New Modifications of Ridge Estimators", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge estimator is an alternative to ordinary least square estimator when\nthere is multicollinearity problem. There are many proposed estimators in\nliterature. In this paper, we propose new estimators which are modifications of\nthe estimator suggested by Lawless and Wang (1976). A Monte Carlo experiment\nhas been conducted for the comparison of the performances of the estimators.\nMean squared error (MSE) is used as a performance criterion. The benefits of\nnew estimators are illustrated using two real datasets. According to both\nsimulation results and applications, our new estimators have better\nperformances in the sense of MSE in most of the situations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 07:19:22 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Asar", "Yasin", ""], ["Gen\u00e7", "A\u015f\u0131r", ""]]}, {"id": "1512.03036", "submitter": "Yimeng Xie", "authors": "Yimeng Xie, Caleb B. King, Yili Hong and Qingyu Yang", "title": "Semi-parametric Models for Accelerated Destructive Degradation Test Data\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated destructive degradation tests (ADDT) are widely used in industry\nto evaluate materials' long term properties. Even though there has been\ntremendous statistical research in nonparametric methods, the current\nindustrial practice is still to use application-specific parametric models to\ndescribe ADDT data. The challenge of using a nonparametric approach comes from\nthe need to retain the physical meaning of degradation mechanisms and also\nperform extrapolation for predictions at the use condition. Motivated by this\nchallenge, we propose a semi-parametric model to describe ADDT data. We use\nmonotonic B-splines to model the degradation path, which not only provides\nflexible models with few assumptions, but also retains the physical meaning of\ndegradation mechanisms (e.g., the degradation path is monotonically\ndecreasing). Parametric models, such as the Arrhenius model, are used for\nmodeling the relationship between the degradation and accelerating variable,\nallowing for extrapolation to the use conditions. We develop an efficient\nprocedure to estimate model parameters. We also use simulation to validate the\ndeveloped procedures and demonstrate the robustness of the semi-parametric\nmodel under model misspecification. Finally, the proposed method is illustrated\nby multiple industrial applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 20:38:19 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Xie", "Yimeng", ""], ["King", "Caleb B.", ""], ["Hong", "Yili", ""], ["Yang", "Qingyu", ""]]}, {"id": "1512.03081", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Yulai Cong, Bo Chen", "title": "Gamma Belief Networks", "comments": "44 pages, 24 figures", "journal-ref": "Journal of Machine Learning Research, 17(163):1-44, September 2016", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To infer multilayer deep representations of high-dimensional discrete and\nnonnegative real vectors, we propose an augmentable gamma belief network (GBN)\nthat factorizes each of its hidden layers into the product of a sparse\nconnection weight matrix and the nonnegative real hidden units of the next\nlayer. The GBN's hidden layers are jointly trained with an upward-downward\nGibbs sampler that solves each layer with the same subroutine. The\ngamma-negative binomial process combined with a layer-wise training strategy\nallows inferring the width of each layer given a fixed budget on the width of\nthe first layer. Example results illustrate interesting relationships between\nthe width of the first layer and the inferred network structure, and\ndemonstrate that the GBN can add more layers to improve its performance in both\nunsupervisedly extracting features and predicting heldout data. For exploratory\ndata analysis, we extract trees and subnetworks from the learned deep network\nto visualize how the very specific factors discovered at the first hidden layer\nand the increasingly more general factors discovered at deeper hidden layers\nare related to each other, and we generate synthetic data by propagating random\nvariables through the deep network from the top hidden layer back to the bottom\ndata layer.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 21:21:09 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 04:25:18 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Cong", "Yulai", ""], ["Chen", "Bo", ""]]}, {"id": "1512.03188", "submitter": "Till Hoffmann", "authors": "Till Hoffmann and Nick S. Jones", "title": "Unified treatment of the asymptotics of asymmetric kernel density\n  estimators", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend balloon and sample-smoothing estimators, two types of\nvariable-bandwidth kernel density estimators, by a shift parameter and derive\ntheir asymptotic properties. Our approach facilitates the unified study of a\nwide range of density estimators which are subsumed under these two general\nclasses of kernel density estimators. We demonstrate our method by deriving the\nasymptotic bias, variance, and mean (integrated) squared error of density\nestimators with gamma, log-normal, Birnbaum-Saunders, inverse Gaussian and\nreciprocal inverse Gaussian kernels. We propose two new density estimators for\npositive random variables that yield properly-normalised density estimates.\nPlugin expressions for bandwidth estimation are provided to facilitate easy\nexploratory data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 09:28:57 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Hoffmann", "Till", ""], ["Jones", "Nick S.", ""]]}, {"id": "1512.03216", "submitter": "J. S. Marron", "authors": "J. S. Marron, James O. Ramsay, Laura M. Sangalli, Anuj Srivastava", "title": "Functional Data Analysis of Amplitude and Phase Variation", "comments": "Published at http://dx.doi.org/10.1214/15-STS524 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 468-484", "doi": "10.1214/15-STS524", "report-no": "IMS-STS-STS524", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of functional observations in scientific endeavors has led to a\nsignificant development in tools for functional data analysis (FDA). This kind\nof data comes with several challenges: infinite-dimensionality of function\nspaces, observation noise, and so on. However, there is another interesting\nphenomena that creates problems in FDA. The functional data often comes with\nlateral displacements/deformations in curves, a phenomenon which is different\nfrom the height or amplitude variability and is termed phase variation. The\npresence of phase variability artificially often inflates data variance, blurs\nunderlying data structures, and distorts principal components. While the\nseparation and/or removal of phase from amplitude data is desirable, this is a\ndifficult problem. In particular, a commonly used alignment procedure, based on\nminimizing the $\\mathbb{L}^2$ norm between functions, does not provide\nsatisfactory results. In this paper we motivate the importance of dealing with\nthe phase variability and summarize several current ideas for separating phase\nand amplitude components. These approaches differ in the following: (1) the\ndefinition and mathematical representation of phase variability, (2) the\nobjective functions that are used in functional data alignment, and (3) the\nalgorithmic tools for solving estimation/optimization problems. We use simple\nexamples to illustrate various approaches and to provide useful contrast\nbetween them.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 11:16:53 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Marron", "J. S.", ""], ["Ramsay", "James O.", ""], ["Sangalli", "Laura M.", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1512.03223", "submitter": "Thijs Van Ommen", "authors": "Thijs van Ommen, Wouter M. Koolen, Thijs E. Feenstra, Peter D.\n  Gr\\\"unwald", "title": "Robust Probability Updating", "comments": "47 pages, 4 figures. This second version is the accepted manuscript:\n  it incorporates reviewer comments and has a new title", "journal-ref": "International Journal of Approximate Reasoning 74 (2016) 30-57", "doi": "10.1016/j.ijar.2016.03.001", "report-no": null, "categories": "stat.ME cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses an alternative to conditioning that may be used when the\nprobability distribution is not fully specified. It does not require any\nassumptions (such as CAR: coarsening at random) on the unknown distribution.\nThe well-known Monty Hall problem is the simplest scenario where neither naive\nconditioning nor the CAR assumption suffice to determine an updated probability\ndistribution. This paper thus addresses a generalization of that problem to\narbitrary distributions on finite outcome spaces, arbitrary sets of `messages',\nand (almost) arbitrary loss functions, and provides existence and\ncharacterization theorems for robust probability updating strategies. We find\nthat for logarithmic loss, optimality is characterized by an elegant condition,\nwhich we call RCAR (reverse coarsening at random). Under certain conditions,\nthe same condition also characterizes optimality for a much larger class of\nloss functions, and we obtain an objective and general answer to how one should\nupdate probabilities in the light of new information.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 11:51:52 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 14:47:47 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["van Ommen", "Thijs", ""], ["Koolen", "Wouter M.", ""], ["Feenstra", "Thijs E.", ""], ["Gr\u00fcnwald", "Peter D.", ""]]}, {"id": "1512.03232", "submitter": "Ruodu Wang", "authors": "Giovanni Puccetti, Ruodu Wang", "title": "Extremal Dependence Concepts", "comments": "Published at http://dx.doi.org/10.1214/15-STS525 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 485-517", "doi": "10.1214/15-STS525", "report-no": "IMS-STS-STS525", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probabilistic characterization of the relationship between two or more\nrandom variables calls for a notion of dependence. Dependence modeling leads to\nmathematical and statistical challenges, and recent developments in extremal\ndependence concepts have drawn a lot of attention to probability and its\napplications in several disciplines. The aim of this paper is to review various\nconcepts of extremal positive and negative dependence, including several\nrecently established results, reconstruct their history, link them to\nprobabilistic optimization problems, and provide a list of open questions in\nthis area. While the concept of extremal positive dependence is agreed upon for\nrandom vectors of arbitrary dimensions, various notions of extremal negative\ndependence arise when more than two random variables are involved. We review\nexisting popular concepts of extremal negative dependence given in literature\nand introduce a novel notion, which in a general sense includes the existing\nones as particular cases. Even if much of the literature on dependence is\nfocused on positive dependence, we show that negative dependence plays an\nequally important role in the solution of many optimization problems. While the\nmost popular tool used nowadays to model dependence is that of a copula\nfunction, in this paper we use the equivalent concept of a set of\nrearrangements. This is not only for historical reasons. Rearrangement\nfunctions describe the relationship between random variables in a completely\ndeterministic way, allow a deeper understanding of dependence itself, and have\nseveral advantages on the approximation of solutions in a broad class of\noptimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 12:36:31 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 14:48:35 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Puccetti", "Giovanni", ""], ["Wang", "Ruodu", ""]]}, {"id": "1512.03307", "submitter": "Nicolas Jung", "authors": "Jung Nicolas and Fr\\'ed\\'eric Bertrand and Myriam Maumy-Bertrand", "title": "AcSel: selecting variables with accuracy in correlated datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of high-throughput technologies, it is possible to measure\nlarge amounts of data relatively at low cost. Such situations arise in many\nfields from sciences to humanities, and variable selection may be of great help\nto answer challenges that are specific to each of them. Variable selection may\nallow to know, among all measured variables, which are of interest and which\nare not. A lot of methods have been proposed to handle this issue, with the\nLasso and other penalized regression as special cases. These methods fail in\nsome cases and linear correlation between explanatory variables is the most\ncommon of these, especially in big datasets. In this article, we introduce\nAcSel, a wrapping algorithm able to enhance the accuracy of any variable\nselection method. To achieve this result, we use intensive computational\nsimulations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 16:15:16 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Nicolas", "Jung", ""], ["Bertrand", "Fr\u00e9d\u00e9ric", ""], ["Maumy-Bertrand", "Myriam", ""]]}, {"id": "1512.03350", "submitter": "Michael Fop", "authors": "Michael Fop, Keith Smart, Thomas Brendan Murphy", "title": "Variable Selection for Latent Class Analysis with Application to Low\n  Back Pain Diagnosis", "comments": "Published in The Annals of Applied Statistics by the Institute of\n  Mathematical Statistics", "journal-ref": "The Annals of Applied Statistics 2017, Vol. 11, No. 4, 2085-2115", "doi": "10.1214/17-AOAS1061", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of most relevant clinical criteria related to low back\npain disorders may aid the evaluation of the nature of pain suffered in a way\nthat usefully informs patient assessment and treatment. Data concerning low\nback pain can be of categorical nature, in the form of a check-list in which\neach item denotes presence or absence of a clinical condition. Latent class\nanalysis is a model-based clustering method for multivariate categorical\nresponses, which can be applied to such data for a preliminary diagnosis of the\ntype of pain. In this work, we propose a variable selection method for latent\nclass analysis applied to the selection of the most useful variables in\ndetecting the group structure in the data. The method is based on the\ncomparison of two different models and allows the discarding of those variables\nwith no group information and those variables carrying the same information as\nthe already selected ones. We consider a swap-stepwise algorithm where at each\nstep the models are compared through an approximation to their Bayes factor.\nThe method is applied to the selection of the clinical criteria most useful for\nthe clustering of patients in different classes. It is shown to perform a\nparsimonious variable selection and to give a clustering performance comparable\nto the expert-based classification of patients into three classes of pain.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 18:03:54 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 18:38:52 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Fop", "Michael", ""], ["Smart", "Keith", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1512.03397", "submitter": "Rina Foygel Barber", "authors": "Rina Foygel Barber and Aaditya Ramdas", "title": "The p-filter: multi-layer FDR control for grouped hypotheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications of multiple hypothesis testing using the False\nDiscovery Rate (FDR), the given hypotheses can be naturally partitioned into\ngroups, and one may not only want to control the number of false discoveries\n(wrongly rejected null hypotheses), but also the number of falsely discovered\ngroups of hypotheses (we say a group is falsely discovered if at least one\nhypothesis within that group is rejected, when in reality the group contains\nonly nulls). In this paper, we introduce the p-filter, a procedure which\nunifies and generalizes the standard FDR procedure by Benjamini and Hochberg\nand global null testing procedure by Simes. We first prove that our proposed\nmethod can simultaneously control the overall FDR at the finest level\n(individual hypotheses treated separately) and the group FDR at coarser levels\n(when such groups are user-specified). We then generalize the p-filter\nprocedure even further to handle multiple partitions of hypotheses, since that\nmight be natural in many applications. For example, in neuroscience\nexperiments, we may have a hypothesis for every (discretized) location in the\nbrain, and at every (discretized) timepoint: does the stimulus correlate with\nactivity in location x at time t after the stimulus was presented? In this\nsetting, one might want to group hypotheses by location and by time.\nImportantly, our procedure can handle multiple partitions which are\nnonhierarchical (i.e. one partition may arrange p-values by voxel, and another\npartition arranges them by time point; neither one is nested inside the other).\nWe prove that our procedure controls FDR simultaneously across these multiple\nlay- ers, under assumptions that are standard in the literature: we do not need\nthe hypotheses to be independent, but require a nonnegative dependence\ncondition known as PRDS.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 20:23:16 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 14:57:01 GMT"}, {"version": "v3", "created": "Sat, 29 Oct 2016 01:53:16 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1512.03500", "submitter": "Baisuo Jin", "authors": "Jialiang Li and Baisuo Jin", "title": "Multi-threshold Accelerate Failure Time Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-stage procedure for simultaneously detecting multiple thresholds and\nachieving model selection in the segmented accelerate failure time (AFT) model\nis developed in this paper. In the first stage, we formulate the threshold\nproblem as a group model selection problem so that a concave 2-norm group\nselection method can be applied. In the second stage, the thresholds are\nfinalized via a refining method. We establish the strong consistency of the\nthreshold estimates and regression coefficient estimates under some mild\ntechnical conditions. The proposed procedure performs satisfactorily in our\nextensive simulation studies. Its real world applicability is demonstrated via\nanalyzing a follicular lymphoma data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 01:31:11 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 08:00:46 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Li", "Jialiang", ""], ["Jin", "Baisuo", ""]]}, {"id": "1512.03533", "submitter": "Louise Ryan", "authors": "Louise Ryan", "title": "A Conversation with Nan Laird", "comments": "Published at http://dx.doi.org/10.1214/15-STS528 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org), Report No IMS-STS-STS528", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 582-596", "doi": "10.1214/15-STS528", "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nan McKenzie Laird is the Harvey V. Fineberg Professor of Biostatistics at\nthe Harvard T. H. Chan School of Public Health. She has made fundamental\ncontributions to statistical methods for longitudinal data analysis, missing\ndata and meta-analysis. In addition, she is widely known for her work in\nstatistical genetics and in statistical methods for psychiatric epidemiology.\nHer 1977 paper with Dempster and Rubin on the EM algorithm is among the top 100\nmost highly cited papers in science [Nature 524 (2014) 550-553]. Her applied\nwork on medical practice errors is widely cited among the medical malpractice\ncommunity. Nan was born in Gainesville, Florida, in 1943. Shortly thereafter,\nher parents Angus McKenzie Laird and Myra Adelia Doyle, moved to Tallahassee,\nFlorida, with Nan and her sister Victoria Mell. Nan started college at Rice\nUniversity in 1961, but then transferred to the University of Georgia where she\nreceived a B.S. in Statistics in 1969 and was elected to Phi Beta Kappa. After\ngraduation Nan worked at the Massachusetts Institute of Technology Draper\nLaboratories where she worked on Kalman filtering for the Apollo Man to the\nMoon Program. She enrolled in the Statistics Department at Harvard University\nin 1971 and received her Ph.D. in 1975. She joined the faculty of Harvard\nSchool of Public Health upon receiving her Ph.D., and remains there as research\nprofessor, after her retirement in 2015. The interview was conducted in Boston,\nMassachusetts, in July 2014. A link to Nan's full CV can be found at\n\\surlwww.hsph.harvard.edu/nan-laird/.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 05:53:11 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Ryan", "Louise", ""]]}, {"id": "1512.03769", "submitter": "Andrew Brown", "authors": "D. Andrew Brown, Gauri S. Datta, Nicole A. Lazar", "title": "A Bayesian Generalized CAR Model for Correlated Signal Detection", "comments": "Statistica Sinica, 2017", "journal-ref": null, "doi": "10.5705/ss.202015.0382", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, large-scale multiple testing has found itself at the\nforefront of modern data analysis. In many applications data are correlated, so\nthat the observed test statistic used for detecting a non-null case, or signal,\nat each location in a dataset carries some information about the chances of a\ntrue signal at other locations. Brown, Lazar, Datta, Jang, and McDowell (2014)\nproposed in the neuroimaging context a Bayesian multiple testing model that\naccounts for the dependence of each volume element on the behavior of its\nneighbors through a conditional autoregressive (CAR) model. Here, we propose a\ngeneralized CAR model that allows for inclusion of points with no neighbors at\nall, something that is not possible under conventional CAR models. We consider\nalso neighborhoods based on criteria other than physical location, such as\ngenetic pathways in microarray determined from existing biological knowledge.\nThis generalization provides a unified framework for the simultaneous modeling\nof dependent and independent cases, resulting in stronger Bayesian learning in\nthe posterior and increased precision in the estimates of interesting signals.\nWe justify the selected prior distribution and prove that the resulting\nposterior distribution is proper. We illustrate the effectiveness and\napplicability of our proposed model by using it to analyze both simulated and\nreal microarray data in which the genes exhibit dependence that is determined\nby physical adjacency on a chromosome or predefined gene pathways.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 19:40:28 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 21:04:47 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 21:50:16 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Brown", "D. Andrew", ""], ["Datta", "Gauri S.", ""], ["Lazar", "Nicole A.", ""]]}, {"id": "1512.04060", "submitter": "Qing Feng", "authors": "Qing Feng, Jan Hannig, J.S.Marron", "title": "Non-iterative Joint and Individual Variation Explained", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrative analysis of disparate data blocks measured on a common set of\nexperimental subjects is one major challenge in modern data analysis. This data\nstructure naturally motivates the simultaneous exploration of the joint and\nindividual variation within each data block resulting in new insights. For\ninstance, there is a strong desire to integrate the multiple genomic data sets\nin The Cancer Genome Atlas (TCGA) to characterize the common and also the\nunique aspects of cancer genetics and cell biology for each source. In this\npaper we introduce Non-iterative Joint and Individual Variation Explained\n(Non-iterative JIVE), capturing both joint and individual variation within each\ndata block. This is a major improvement over earlier approaches to this\nchallenge in terms of a new conceptual understanding, much better adaption to\ndata heterogeneity and a fast linear algebra computation. Important\nmathematical contributions are the use of score subspaces as the principal\ndescriptors of variation structure and the use of perturbation theory as the\nguide for variation segmentation. This leads to a method which is robust\nagainst the heterogeneity among data blocks without a need for normalization.\nAn application to TCGA data reveals different behaviors of each type of signal\nin characterizing tumor subtypes. An application to a mortality data set\nreveals interesting historical lessons.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 14:46:13 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 14:11:44 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Feng", "Qing", ""], ["Hannig", "Jan", ""], ["Marron", "J. S.", ""]]}, {"id": "1512.04068", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and D\\'ora Nemoda", "title": "Censored and shifted gamma distribution based EMOS model for\n  probabilistic quantitative precipitation forecasting", "comments": "20 pages, 5 figures, 8 tables", "journal-ref": "Environmetrics, 27, 280-292 (2016)", "doi": "10.1002/env.2391", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently all major weather prediction centres provide forecast ensembles of\ndifferent weather quantities which are obtained from multiple runs of numerical\nweather prediction models with various initial conditions and model\nparametrizations. However, ensemble forecasts often show an underdispersive\ncharacter and may also be biased, so that some post-processing is needed to\naccount for these deficiencies. Probably the most popular modern\npost-processing techniques are the ensemble model output statistics (EMOS) and\nthe Bayesian model averaging (BMA) which provide estimates of the density of\nthe predictable weather quantity.\n  In the present work an EMOS method for calibrating ensemble forecasts of\nprecipitation accumulation is proposed, where the predictive distribution\nfollows a censored and shifted gamma (CSG) law with parameters depending on the\nensemble members. The CSG EMOS model is tested on ensemble forecasts of 24 h\nprecipitation accumulation of the eight-member University of Washington\nmesoscale ensemble and on the 11 member ensemble produced by the operational\nLimited Area Model Ensemble Prediction System of the Hungarian Meteorological\nService. The predictive performance of the new EMOS approach is compared with\nthe fit of the raw ensemble, the generalized extreme value (GEV) distribution\nbased EMOS model and the gamma BMA method. According to the results, the\nproposed CSG EMOS model slightly outperforms the GEV EMOS approach in terms of\ncalibration of probabilistic and accuracy of point forecasts and shows\nsignificantly better predictive skill that the raw ensemble and the BMA model.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 15:34:35 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 16:23:19 GMT"}, {"version": "v3", "created": "Fri, 1 Apr 2016 07:18:49 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Nemoda", "D\u00f3ra", ""]]}, {"id": "1512.04093", "submitter": "Ning Hao", "authors": "Yue S. Niu, Ning Hao, and Heping Zhang", "title": "Multiple Change-point Detection: a Selective Overview", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very long and noisy sequence data arise from biological sciences to social\nscience including high throughput data in genomics and stock prices in\neconometrics. Often such data are collected in order to identify and understand\nshifts in trend, e.g., from a bull market to a bear market in finance or from a\nnormal number of chromosome copies to an excessive number of chromosome copies\nin genetics. Thus, identifying multiple change points in a long, possibly very\nlong, sequence is an important problem. In this article, we review both\nclassical and new multiple change-point detection strategies. Considering the\nlong history and the extensive literature on the change-point detection, we\nprovide an in-depth discussion on a normal mean change-point model from aspects\nof regression analysis, hypothesis testing, consistency and inference. In\nparticular, we present a strategy to gather and aggregate local information for\nchange-point detection that has become the cornerstone of several emerging\nmethods because of its attractiveness in both computational and theoretical\nproperties.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 18:09:24 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 19:17:07 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Niu", "Yue S.", ""], ["Hao", "Ning", ""], ["Zhang", "Heping", ""]]}, {"id": "1512.04273", "submitter": "David van Dyk", "authors": "Nathan M Stein, David A van Dyk, and Vinay L Kashyap", "title": "Preprocessing Solar Images while Preserving their Latent Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telescopes such as the Atmospheric Imaging Assembly aboard the Solar Dynamics\nObservatory, a NASA satellite, collect massive streams of high resolution\nimages of the Sun through multiple wavelength filters. Reconstructing\npixel-by-pixel thermal properties based on these images can be framed as an\nill-posed inverse problem with Poisson noise, but this reconstruction is\ncomputationally expensive and there is disagreement among researchers about\nwhat regularization or prior assumptions are most appropriate. This article\npresents an image segmentation framework for preprocessing such images in order\nto reduce the data volume while preserving as much thermal information as\npossible for later downstream analyses. The resulting segmented images reflect\nthermal properties but do not depend on solving the ill-posed inverse problem.\nThis allows users to avoid the Poisson inverse problem altogether or to tackle\nit on each of $\\sim$10 segments rather than on each of $\\sim$10$^7$ pixels,\nreducing computing time by a factor of $\\sim$10$^6$. We employ a parametric\nclass of dissimilarities that can be expressed as cosine dissimilarity\nfunctions or Hellinger distances between nonlinearly transformed vectors of\nmulti-passband observations in each pixel. We develop a decision theoretic\nframework for choosing the dissimilarity that minimizes the expected loss that\narises when estimating identifiable thermal properties based on segmented\nimages rather than on a pixel-by-pixel basis. We also examine the efficacy of\ndifferent dissimilarities for recovering clusters in the underlying thermal\nproperties. The expected losses are computed under scientifically motivated\nprior distributions. Two simulation studies guide our choices of dissimilarity\nfunction. We illustrate our method by segmenting images of a coronal hole\nobserved on 26 February 2015.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 12:18:18 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Stein", "Nathan M", ""], ["van Dyk", "David A", ""], ["Kashyap", "Vinay L", ""]]}, {"id": "1512.04395", "submitter": "Claudio Agostinelli", "authors": "Claudio Agostinelli", "title": "Local Half-Region Depth for Functional Data", "comments": "46 pages. 19 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data depth proves successful in the analysis of multivariate data sets, in\nparticular deriving an overall center and assigning ranks to the observed\nunits. Two key features are: the directions of the ordering, from the center\ntowards the outside, and the recognition of a unique center irrespective of the\ndistribution being unimodal or multimodal. This behaviour is a consequence of\nthe monotonicity of the ranks that decrease along any ray from the deepest\npoint. Recently, a wider framework allowing identification of partial centers\nwas suggested in Agostinelli and Romanazzi [2011]. The corresponding\ngeneralized depth functions, called local depth functions are able to record\nlocal fluctuations and can be used in mode detection, identification of\ncomponents in mixture models and in cluster analysis. Functional data [Ramsay\nand Silverman, 2006] are become common nowadays. Recently, Lopez-Pintado and\nRomo [2011] has proposed the half-region depth suited for functional data and\nfor high dimensional data. Here, following their work we propose a local\nversion of this data depth, we study its theoretical properties and illustrate\nits behaviour with examples based on real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:29:22 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 09:15:06 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Agostinelli", "Claudio", ""]]}, {"id": "1512.04499", "submitter": "Sihai Zhao", "authors": "Sihai Dave Zhao and Yet Tien Nguyen", "title": "Nonparametric false discovery rate control for identifying simultaneous\n  signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is frequently of interest to jointly analyze multiple sequences of\nmultiple tests in order to identify simultaneous signals, defined as features\ntested in multiple studies whose test statistics are non-null in each. In many\nproblems, however, the null distributions of the test statistics may be\ncomplicated or even unknown, and there do not currently exist any procedures\nthat can be employed in these cases. This paper proposes a new nonparametric\nprocedure that can identify simultaneous signals across multiple studies even\nwithout knowing the null distributions of the test statistics. The method is\nshown to asymptotically control the false discovery rate, and in simulations\nhad excellent power and error control. In an analysis of gene expression and\nhistone acetylation patterns in the brains of mice exposed to a conspecific\nintruder, it identified genes that were both differentially expressed and next\nto differentially accessible chromatin. The proposed method is available in the\nR package github.com/sdzhao/ssa.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 20:07:48 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 19:35:20 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 16:03:20 GMT"}, {"version": "v4", "created": "Tue, 15 Jan 2019 18:51:03 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Zhao", "Sihai Dave", ""], ["Nguyen", "Yet Tien", ""]]}, {"id": "1512.04636", "submitter": "Alexander Wong", "authors": "Ameneh Boroomand, Mohammad Javad Shafiee, Farzad Khalvati, Masoom A.\n  Haider, and Alexander Wong", "title": "Noise-Compensated, Bias-Corrected Diffusion Weighted Endorectal Magnetic\n  Resonance Imaging via a Stochastically Fully-Connected Joint Conditional\n  Random Field Model", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion weighted magnetic resonance imaging (DW-MR) is a powerful tool in\nimaging-based prostate cancer screening and detection. Endorectal coils are\ncommonly used in DW-MR imaging to improve the signal-to-noise ratio (SNR) of\nthe acquisition, at the expense of significant intensity inhomogeneities (bias\nfield) that worsens as we move away from the endorectal coil. The presence of\nbias field can have a significant negative impact on the accuracy of different\nimage analysis tasks, as well as prostate tumor localization, thus leading to\nincreased inter- and intra-observer variability. Retrospective bias correction\napproaches are introduced as a more efficient way of bias correction compared\nto the prospective methods such that they correct for both of the scanner and\nanatomy-related bias fields in MR imaging. Previously proposed retrospective\nbias field correction methods suffer from undesired noise amplification that\ncan reduce the quality of bias-corrected DW-MR image. Here, we propose a\nunified data reconstruction approach that enables joint compensation of bias\nfield as well as data noise in DW-MR imaging. The proposed noise-compensated,\nbias-corrected (NCBC) data reconstruction method takes advantage of a novel\nstochastically fully connected joint conditional random field (SFC-JCRF) model\nto mitigate the effects of data noise and bias field in the reconstructed MR\ndata. The proposed NCBC reconstruction method was tested on synthetic DW-MR\ndata, physical DW-phantom as well as real DW-MR data all acquired using\nendorectal MR coil. Both qualitative and quantitative analysis illustrated that\nthe proposed NCBC method can achieve improved image quality when compared to\nother tested bias correction methods. As such, the proposed NCBC method may\nhave potential as a useful retrospective approach for improving the consistency\nof image interpretations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 03:44:28 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 16:47:37 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Boroomand", "Ameneh", ""], ["Shafiee", "Mohammad Javad", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.04808", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Bernhard Sch\\\"olkopf, Tonio Ball, Moritz\n  Grosse-Wentrup", "title": "Causal and anti-causal learning in pattern recognition for neuroimaging", "comments": "accepted manuscript", "journal-ref": "Pattern Recognition in Neuroimaging, 2014 International Workshop\n  on, 1-4, 2014", "doi": "10.1109/PRNI.2014.6858551", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition in neuroimaging distinguishes between two types of\nmodels: encoding- and decoding models. This distinction is based on the insight\nthat brain state features, that are found to be relevant in an experimental\nparadigm, carry a different meaning in encoding- than in decoding models. In\nthis paper, we argue that this distinction is not sufficient: Relevant features\nin encoding- and decoding models carry a different meaning depending on whether\nthey represent causal- or anti-causal relations. We provide a theoretical\njustification for this argument and conclude that causal inference is essential\nfor interpretation in neuroimaging.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:05:00 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Ball", "Tonio", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1512.04809", "submitter": "Alexander Keil", "authors": "Alexander P. Keil, Eric J. Daza, Stephanie M. Engel, Jessie P.\n  Buckley, Jessie K. Edwards", "title": "A Bayesian approach to the g-formula", "comments": "24 pages", "journal-ref": "Stat Methods Med Res. 2017 Jan 1:962280217694665", "doi": "10.1177/0962280217694665", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiologists often wish to estimate quantities that are easy to\ncommunicate and correspond to the results of realistic public health scenarios.\nMethods from causal inference can answer these questions. We adopt the language\nof potential outcomes under Rubin's original Bayesian framework and show that\nthe parametric g-formula is easily amenable to a Bayesian approach. We show\nthat the frequentist properties of the Bayesian g-formula suggest it improves\nthe accuracy of estimates of causal effects in small samples or when data may\nbe sparse. We demonstrate our approach to estimate the effect of environmental\ntobacco smoke on body mass index z-scores among children aged 4-9 years who\nwere enrolled in a longitudinal birth cohort in New York, USA. We give a\ngeneral algorithm and supply SAS and Stan code that can be adopted to implement\nour computational approach in both time-fixed and longitudinal data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:10:27 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Keil", "Alexander P.", ""], ["Daza", "Eric J.", ""], ["Engel", "Stephanie M.", ""], ["Buckley", "Jessie P.", ""], ["Edwards", "Jessie K.", ""]]}, {"id": "1512.04823", "submitter": "Philipp Wacker", "authors": "Miguel de Benito Delgado, Philipp Wacker", "title": "Bayesian model selection for linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we introduce linear regression with basis functions in order to\napply Bayesian model selection. The goal is to incorporate Occam's razor as\nprovided by Bayes analysis in order to automatically pick the model optimally\nable to explain the data without overfitting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:46:11 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Delgado", "Miguel de Benito", ""], ["Wacker", "Philipp", ""]]}, {"id": "1512.04831", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Adeline Samson", "title": "Coupling stochastic EM and Approximate Bayesian Computation for\n  parameter inference in state-space models", "comments": "29 pages. Made a small fix to equation (8). Added the doi of the\n  published version, doi: 10.1007/s00180-017-0770-y", "journal-ref": null, "doi": "10.1007/s00180-017-0770-y", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the class of state-space models and perform maximum likelihood\nestimation for the model parameters. We consider a stochastic approximation\nexpectation-maximization (SAEM) algorithm to maximize the likelihood function\nwith the novelty of using approximate Bayesian computation (ABC) within SAEM.\nThe task is to provide each iteration of SAEM with a filtered state of the\nsystem, and this is achieved using an ABC sampler for the hidden state, based\non sequential Monte Carlo (SMC) methodology. It is shown that the resulting\nSAEM-ABC algorithm can be calibrated to return accurate inference, and in some\nsituations it can outperform a version of SAEM incorporating the bootstrap\nfilter. Two simulation studies are presented, first a nonlinear Gaussian\nstate-space model then a state-space model having dynamics expressed by a\nstochastic differential equation. Comparisons with iterated filtering for\nmaximum likelihood inference, and Gibbs sampling and particle marginal methods\nfor Bayesian inference are presented.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:59:56 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 23:52:58 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 21:12:50 GMT"}, {"version": "v4", "created": "Thu, 28 Sep 2017 11:59:45 GMT"}, {"version": "v5", "created": "Mon, 16 Oct 2017 13:07:32 GMT"}, {"version": "v6", "created": "Tue, 24 Oct 2017 09:30:57 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Picchini", "Umberto", ""], ["Samson", "Adeline", ""]]}, {"id": "1512.04922", "submitter": "Ramesh Johari", "authors": "Ramesh Johari, Leo Pekelis, David J. Walsh", "title": "Always Valid Inference: Bringing Sequential Analysis to A/B Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B tests are typically analyzed via frequentist p-values and confidence\nintervals; but these inferences are wholly unreliable if users endogenously\nchoose samples sizes by *continuously monitoring* their tests. We define\n*always valid* p-values and confidence intervals that let users try to take\nadvantage of data as fast as it becomes available, providing valid statistical\ninference whenever they make their decision. Always valid inference can be\ninterpreted as a natural interface for a sequential hypothesis test, which\nempowers users to implement a modified test tailored to them. In particular, we\nshow in an appropriate sense that the measures we develop tradeoff sample size\nand power efficiently, despite a lack of prior knowledge of the user's relative\npreference between these two goals. We also use always valid p-values to obtain\nmultiple hypothesis testing control in the sequential context. Our methodology\nhas been implemented in a large scale commercial A/B testing platform to\nanalyze hundreds of thousands of experiments to date.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 20:33:31 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 07:12:05 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 19:42:42 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Johari", "Ramesh", ""], ["Pekelis", "Leo", ""], ["Walsh", "David J.", ""]]}, {"id": "1512.05153", "submitter": "Ines Wilms", "authors": "Ines Wilms and Christophe Croux", "title": "An algorithm for the multivariate group lasso with covariance estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a group lasso estimator for the multivariate linear regression model\nthat accounts for correlated error terms. A block coordinate descent algorithm\nis used to compute this estimator. We perform a simulation study with\ncategorical data and multivariate time series data, typical settings with a\nnatural grouping among the predictor variables. Our simulation studies show the\ngood performance of the proposed group lasso estimator compared to alternative\nestimators. We illustrate the method on a time series data set of gene\nexpressions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 12:50:30 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Wilms", "Ines", ""], ["Croux", "Christophe", ""]]}, {"id": "1512.05169", "submitter": "Moritz  Berger", "authors": "Moritz Berger and Gerhard Tutz", "title": "Tree-Structured Clustering in Fixed Effects Models", "comments": "31 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed effects models are very flexible because they do not make assumptions\non the distribution of effects and can also be used if the heterogeneity\ncomponent is correlated with explanatory variables. A disadvantage is the large\nnumber of effects that have to be estimated. A recursive partitioning (or tree\nbased) method is proposed that identifies clusters of units that share the same\neffect. The approach reduces the number of parameters to be estimated and is\nuseful in particular if one is interested in identifying clusters with the same\neffect on a response variable. It is shown that the method performs well and\noutperforms competitors like the finite mixture model in particular if the\nheterogeneity component is correlated with explanatory variables. In two\napplications the usefulness of the approach to identify clusters that share the\nsame effect is illustrated.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 13:32:47 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Berger", "Moritz", ""], ["Tutz", "Gerhard", ""]]}, {"id": "1512.05171", "submitter": "Giovanni Mana", "authors": "Giovanni Mana, Carlo Palmisano", "title": "Covariant priors and model uncertainty", "comments": "preprint, 27 pages, 8 figures, submitted to Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the application of Bayesian methods to metrology, pre-data probabilities\nplay a critical role in the estimation of the model uncertainty. Following the\nobservation that distributions form Riemann's manifolds, methods of\ndifferential geometry can be applied to ensure covariant priors and\nuncertainties independent of parameterization. Paradoxes were found in\nmulti-parameter problems and alternatives were developed; but, when different\nparameters are of interest, covariance may be lost. This paper overviews\ninformation geometry, investigates some key paradoxes, and proposes solutions\nthat preserve covariance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 13:33:39 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Mana", "Giovanni", ""], ["Palmisano", "Carlo", ""]]}, {"id": "1512.05225", "submitter": "Denis Allard", "authors": "Denis Allard and Thierry Marchant", "title": "Means and covariance functions for geostatistical compositional data: an\n  axiomatic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the characterization of the central tendency of a sample\nof compositional data. It provides new results about theoretical properties of\nmeans and covariance functions for compositional data, with an axiomatic\nperspective. Original results that shed new light on the geostatistical\nmodeling of compositional data are presented. As a first result, it is shown\nthat the weighted arithmetic mean is the only central tendency characteristic\nsatisfying a small set of axioms, namely continuity, reflexivity and marginal\nstability. Moreover, this set of axioms also implies that the weights must be\nidentical for all parts of the composition. This result has deep consequences\non the spatial multivariate covariance modeling of compositional data. In a\ngeostatistical setting, it is shown as a second result that the proportional\nmodel of covariance functions (i.e., the product of a covariance matrix and a\nsingle correlation function) is the only model that provides identical kriging\nweights for all components of the compositional data. As a consequence of these\ntwo results, the proportional model of covariance function is the only\ncovariance model compatible with reflexivity and marginal stability.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:13:33 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 14:18:43 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 12:31:50 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Allard", "Denis", ""], ["Marchant", "Thierry", ""]]}, {"id": "1512.05446", "submitter": "Mohammad Jafari Jozani", "authors": "Mohammad Nourmohammadi, Mohammad Jafari Jozani and Brad Johnson", "title": "Parametric inference for proportional (reverse) hazard rate models with\n  nomination sampling", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\noindent Randomized nomination sampling (RNS) is a rank-based sampling\ntechnique which has been shown to be effective in several nonparametric studies\ninvolving environmental and ecological applications. In this paper, we\ninvestigate parametric inference using RNS design for estimating the unknown\nvector of parameters $\\boldsymbol{\\theta}$ in the proportional hazard rate and\nproportional reverse hazard rate models. We examine both maximum likelihood\n(ML) and method of moments (MM) methods and investigate the relative precision\nof our proposed RNS-based estimators compared with those based on simple random\nsampling (SRS). We introduce four types of RNS-based data as well as necessary\nEM algorithms for the ML estimation, and evaluate the performance of\ncorresponding estimators in estimating $\\boldsymbol{\\theta}$. We show that\nthere are always values of the design parameters on which RNS-based estimators\nare more efficient than those based on SRS. Inference based on imperfect\nranking is also explored and it is shown that the improvement holds even when\nthe ranking is imperfect. Theoretical results are augmented with numerical\nevaluations and a case study.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 02:51:06 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Nourmohammadi", "Mohammad", ""], ["Jozani", "Mohammad Jafari", ""], ["Johnson", "Brad", ""]]}, {"id": "1512.05566", "submitter": "Roman Schefzik", "authors": "Roman Schefzik", "title": "Combining low-dimensional ensemble postprocessing with reordering\n  methods", "comments": null, "journal-ref": null, "doi": "10.1002/qj.2839", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art weather forecasts usually rely on ensemble prediction\nsystems, accounting for the different sources of uncertainty. As ensembles are\ntypically uncalibrated, they should get statistically postprocessed. Several\nmultivariate ensemble postprocessing techniques, which additionally consider\nspatial, inter-variable and/or temporal dependencies, have been developed.\nThese can be roughly divided into two groups. The first group comprises\nparametric, mainly low-dimensional approaches that are tailored to specific\nsettings. The second group involves non-parametric reordering methods that\nimpose a specific dependence template on univariately postprocesed forecasts\nand are suitable in any dimension. In this paper, these different strategies\nare combined, with the aim to exploit the benefits of both concepts.\nSpecifically, a high-dimensional postprocessing problem is divided into\nmultiple low-dimensional instances, each of which is postprocessed via a\nsuitable multivariate parametric method. From each postprocessed\nlow-dimensional distribution, a sample is drawn, which is then reordered\naccording to the corresponding multidimensional rank structure of an\nappropriately chosen dependence template. In this context, different ranking\nconcepts for multivariate settings are discussed. Finally, all reordered\nsamples are aggregated to obtain the overall postprocessed ensemble. The new\napproach is applied to ensemble forecasts for temperature and wind speed at\nseveral locations from the European Centre for Medium-Range Weather Forecasts,\nusing a recent bivariate ensemble model output statistics postprocessing\ntechnique and a reordering based on the raw ensemble forecasts similar to the\nensemble copula coupling method. It shows good predictive skill and outperforms\nreference ensembles.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 12:56:11 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Schefzik", "Roman", ""]]}, {"id": "1512.05629", "submitter": "Roman Schefzik", "authors": "Roman Schefzik", "title": "Multivariate discrete copulas, with applications in probabilistic\n  weather forecasting", "comments": "arXiv admin note: text overlap with arXiv:1305.3445", "journal-ref": "ANNALES DE L'I.S.U.P. - Publications de l'Institut de Statistique\n  de l'Universit\\'{e} de Paris, vol. 59, fasc. 1-2, pp. 87-116, 2015", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In probability and statistics, copulas play important roles theoretically as\nwell as to address a wide range of problems in various application areas. We\nintroduce the concept of multivariate discrete copulas, discuss their\nequivalence to stochastic arrays, and prove a multivariate discrete version of\nSklar's theorem. These results provide the theoretical frame for multivariate\nstatistical methods to postprocess weather forecasts made by ensemble systems,\nincluding the ensemble copula coupling approach and the Schaake shuffle.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 15:29:05 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Schefzik", "Roman", ""]]}, {"id": "1512.05633", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "Summary Statistics in Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is due to appear as a chapter of the forthcoming Handbook of\nApproximate Bayesian Computation (ABC) edited by S. Sisson, Y. Fan, and M.\nBeaumont.\n  Since the earliest work on ABC, it has been recognised that using summary\nstatistics is essential to produce useful inference results. This is because\nABC suffers from a curse of dimensionality effect, whereby using high\ndimensional inputs causes large approximation errors in the output. It is\ntherefore crucial to find low dimensional summaries which are informative about\nthe parameter inference or model choice task at hand. This chapter reviews the\nmethods which have been proposed to select such summaries, extending the\nprevious review paper of Blum et al. (2013) with recent developments. Related\ntheoretical results on the ABC curse of dimensionality and sufficiency are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 15:38:34 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Prangle", "Dennis", ""]]}, {"id": "1512.05635", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Ivan Fernandez-Val, and Ye Luo", "title": "The Sorted Effects Method: Discovering Heterogeneous Effects Beyond\n  Their Averages", "comments": "62 pages, 9 figures, 8 tables, includes appendix with supplementary\n  materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial (ceteris paribus) effects of interest in nonlinear and\ninteractive linear models are heterogeneous as they can vary dramatically with\nthe underlying observed or unobserved covariates. Despite the apparent\nimportance of heterogeneity, a common practice in modern empirical work is to\nlargely ignore it by reporting average partial effects (or, at best, average\neffects for some groups). While average effects provide very convenient scalar\nsummaries of typical effects, by definition they fail to reflect the entire\nvariety of the heterogeneous effects. In order to discover these effects much\nmore fully, we propose to estimate and report sorted effects -- a collection of\nestimated partial effects sorted in increasing order and indexed by\npercentiles. By construction the sorted effect curves completely represent and\nhelp visualize the range of the heterogeneous effects in one plot. They are as\nconvenient and easy to report in practice as the conventional average partial\neffects. They also serve as a basis for classification analysis, where we\ndivide the observational units into most or least affected groups and summarize\ntheir characteristics. We provide a quantification of uncertainty (standard\nerrors and confidence bands) for the estimated sorted effects and related\nclassification analysis, and provide confidence sets for the most and least\naffected groups. The derived statistical results rely on establishing key, new\nmathematical results on Hadamard differentiability of a multivariate sorting\noperator and a related classification operator, which are of independent\ninterest. We apply the sorted effects method and classification analysis to\ndemonstrate several striking patterns in the gender wage gap.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 15:41:01 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 10:46:29 GMT"}, {"version": "v3", "created": "Wed, 1 Nov 2017 15:05:20 GMT"}, {"version": "v4", "created": "Fri, 25 May 2018 20:06:19 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fernandez-Val", "Ivan", ""], ["Luo", "Ye", ""]]}, {"id": "1512.06086", "submitter": "Nicolas Dobigeon", "authors": "Cl\\'ement Elvira, Pierre Chainais and Nicolas Dobigeon", "title": "Bayesian anti-sparse coding", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2645543", "report-no": null, "categories": "stat.ML physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations have proven their efficiency in solving a wide class\nof inverse problems encountered in signal and image processing. Conversely,\nenforcing the information to be spread uniformly over representation\ncoefficients exhibits relevant properties in various applications such as\ndigital communications. Anti-sparse regularization can be naturally expressed\nthrough an $\\ell_{\\infty}$-norm penalty. This paper derives a probabilistic\nformulation of such representations. A new probability distribution, referred\nto as the democratic prior, is first introduced. Its main properties as well as\nthree random variate generators for this distribution are derived. Then this\nprobability distribution is used as a prior to promote anti-sparsity in a\nGaussian linear inverse problem, yielding a fully Bayesian formulation of\nanti-sparse coding. Two Markov chain Monte Carlo (MCMC) algorithms are proposed\nto generate samples according to the posterior distribution. The first one is a\nstandard Gibbs sampler. The second one uses Metropolis-Hastings moves that\nexploit the proximity mapping of the log-posterior distribution. These samples\nare used to approximate maximum a posteriori and minimum mean square error\nestimators of both parameters and hyperparameters. Simulations on synthetic\ndata illustrate the performances of the two proposed samplers, for both\ncomplete and over-complete dictionaries. All results are compared to the recent\ndeterministic variational FITRA algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 19:37:24 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Elvira", "Cl\u00e9ment", ""], ["Chainais", "Pierre", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1512.06158", "submitter": "Dandan Jiang", "authors": "Dandan Jiang", "title": "Likelihood-based tests on linear hypotheses of large dimensional mean\n  vectors with unequal covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers testing linear hypotheses of a set of mean vectors with\nunequal covariance matrices in large dimensional setting. The problem of\ntesting the hypothesis $H_0 : \\sum_{i=1}^q \\beta_i \\bmu_i =\\bmu_0 $ for a given\nvector $\\bmu_0$ is studied from the view of likelihood, which makes the\nproposed tests more powerful. We use the CLT for linear spectral statistics of\na large dimensional $F$-matrix in Zheng(2012) [21] to establish the new test\nstatistics in large dimensional framework, so that the proposed tests can be\napplicable for large dimensional non-Gaussian variables in a wider range.\nFurthermore, our new tests provide more optimal empirical powers due to the\nlikelihood-based statistics, meanwhile their empirical sizes are closer to the\nsignificant level. Finally, the simulation study is provided to compare the\nproposed tests with other high dimensional mean vectors tests for evaluation of\ntheir performances.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 22:53:31 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Jiang", "Dandan", ""]]}, {"id": "1512.06159", "submitter": "Richard Chen", "authors": "Richard Y. Chen, Per A. Mykland", "title": "Model-Free Approaches to Discern Non-Stationary Microstructure Noise and\n  Time-Varying Liquidity in High-Frequency Data", "comments": null, "journal-ref": "Journal of Econometrics, Volume 200, Issue 1, September 2017,\n  Pages 79-103", "doi": "10.1016/j.jeconom.2017.05.015", "report-no": null, "categories": "q-fin.ST math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide non-parametric statistical tools to test\nstationarity of microstructure noise in general hidden Ito semimartingales, and\ndiscuss how to measure liquidity risk using high frequency financial data. In\nparticular, we investigate the impact of non-stationary microstructure noise on\nsome volatility estimators, and design three complementary tests by exploiting\nedge effects, information aggregation of local estimates and high-frequency\nasymptotic approximation. The asymptotic distributions of these tests are\navailable under both stationary and non-stationary assumptions, thereby enable\nus to conservatively control type-I errors and meanwhile ensure the proposed\ntests enjoy the asymptotically optimal statistical power. Besides it also\nenables us to empirically measure aggregate liquidity risks by these test\nstatistics. As byproducts, functional dependence and endogenous microstructure\nnoise are briefly discussed. Simulation with a realistic configuration\ncorroborates our theoretical results, and our empirical study indicates the\nprevalence of non-stationary microstructure noise in New York Stock Exchange.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 22:57:28 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2016 00:25:42 GMT"}, {"version": "v3", "created": "Sun, 15 Jan 2017 17:48:02 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 20:13:04 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Chen", "Richard Y.", ""], ["Mykland", "Per A.", ""]]}, {"id": "1512.06171", "submitter": "Alex Gibberd Mr", "authors": "Alexander J. Gibberd and James D. B. Nelson", "title": "Regularized Estimation of Piecewise Constant Gaussian Graphical Models:\n  The Group-Fused Graphical Lasso", "comments": "32 pages, 9 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 2017, Volume\n  26, Number 3, pp 623--634", "doi": "10.1080/10618600.2017.1302340", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time-evolving precision matrix of a piecewise-constant Gaussian graphical\nmodel encodes the dynamic conditional dependency structure of a multivariate\ntime-series. Traditionally, graphical models are estimated under the assumption\nthat data is drawn identically from a generating distribution. Introducing\nsparsity and sparse-difference inducing priors we relax these assumptions and\npropose a novel regularized M-estimator to jointly estimate both the graph and\nchangepoint structure. The resulting estimator possesses the ability to\ntherefore favor sparse dependency structures and/or smoothly evolving graph\nstructures, as required. Moreover, our approach extends current methods to\nallow estimation of changepoints that are grouped across multiple dependencies\nin a system. An efficient algorithm for estimating structure is proposed. We\nstudy the empirical recovery properties in a synthetic setting. The qualitative\neffect of grouped changepoint estimation is then demonstrated by applying the\nmethod on two real-world data-sets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 00:53:58 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:28:38 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Gibberd", "Alexander J.", ""], ["Nelson", "James D. B.", ""]]}, {"id": "1512.06217", "submitter": "Jingyi Guo", "authors": "Jingyi Guo and H{\\aa}vard Rue and Andrea Riebler", "title": "Bayesian bivariate meta-analysis of diagnostic test studies with\n  interpretable priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a bivariate meta-analysis the number of diagnostic studies involved is\noften very low so that frequentist methods may result in problems. Bayesian\ninference is attractive as informative priors that add small amount of\ninformation can stabilise the analysis without overwhelming the data. However,\nBayesian analysis is often computationally demanding and the selection of the\nprior for the covariance matrix of the bivariate structure is crucial with\nlittle data. The integrated nested Laplace approximations (INLA) method\nprovides an efficient solution to the computational issues by avoiding any\nsampling, but the important question of priors remain. We explore the penalised\ncomplexity (PC) prior framework for specifying informative priors for the\nvariance parameters and the correlation parameter. PC priors facilitate model\ninterpretation and hyperparameter specification as expert knowledge can be\nincorporated intuitively. We conduct a simulation study to compare the\nproperties and behaviour of differently defined PC priors to currently used\npriors in the field. The simulation study shows that the use of PC priors\nresults in more precise estimates when specified in a sensible neighbourhood\naround the truth. To investigate the usage of PC priors in practice we\nreanalyse a meta-analysis using the telomerase marker for the diagnosis of\nbladder cancer.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 10:00:01 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Guo", "Jingyi", ""], ["Rue", "H\u00e5vard", ""], ["Riebler", "Andrea", ""]]}, {"id": "1512.06220", "submitter": "Jingyi Guo", "authors": "Jingyi Guo and Andrea Riebler", "title": "meta4diag: Bayesian Bivariate Meta-analysis of Diagnostic Test Studies\n  for Routine Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the \\proglang{R} package \\pkg{meta4diag} for\nimplementing Bayesian bivariate meta-analyses of diagnostic test studies. Our\npackage \\pkg{meta4diag} is a purpose-built front end of the \\proglang{R}\npackage \\pkg{INLA}. While \\pkg{INLA} offers full Bayesian inference for the\nlarge set of latent Gaussian models using integrated nested Laplace\napproximations, \\pkg{meta4diag} extracts the features needed for bivariate\nmeta-analysis and presents them in an intuitive way. It allows the user a\nstraightforward model-specification and offers user-specific prior\ndistributions. Further, the newly proposed penalised complexity prior framework\nis supported, which builds on prior intuitions about the behaviours of the\nvariance and correlation parameters. Accurate posterior marginal distributions\nfor sensitivity and specificity as well as all hyperparameters, and covariates\nare directly obtained without Markov chain Monte Carlo sampling. Further,\nunivariate estimates of interest, such as odds ratios, as well as the SROC\ncurve and other common graphics are directly available for interpretation. An\ninteractive graphical user interface provides the user with the full\nfunctionality of the package without requiring any \\proglang{R} programming.\nThe package is available through CRAN\n\\url{https://cran.r-project.org/web/packages/meta4diag/} and its usage will be\nillustrated using three real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 10:11:14 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 13:55:05 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Guo", "Jingyi", ""], ["Riebler", "Andrea", ""]]}, {"id": "1512.06412", "submitter": "Benjamin Frot", "authors": "Benjamin Frot, Luke Jostins, Gil McVean", "title": "Latent variable model selection for Gaussian conditional random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a conditional Gaussian graphical model in\nthe presence of latent variables. Building on recent advances in this field, we\nsuggest a method that decomposes the parameters of a conditional Markov random\nfield into the sum of a sparse and a low-rank matrix. We derive convergence\nbounds for this estimator and show that it is well-behaved in the\nhigh-dimensional regime as well as \"sparsistent\" (i.e. capable of recovering\nthe graph structure). We then show how proximal gradient algorithms and\nsemi-definite programming techniques can be employed to fit the model to\nthousands of variables. Through extensive simulations, we illustrate the\nconditions required for identifiability and show that there is a wide range of\nsituations in which this model performs significantly better than its\ncounterparts, for example, by accommodating more latent variables. Finally, the\nsuggested method is applied to two datasets comprising individual level data on\ngenetic variants and metabolites levels. We show our results replicate better\nthan alternative approaches and show enriched biological signal.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 17:44:49 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 00:24:33 GMT"}, {"version": "v3", "created": "Sat, 4 Mar 2017 21:49:35 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Frot", "Benjamin", ""], ["Jostins", "Luke", ""], ["McVean", "Gil", ""]]}, {"id": "1512.06505", "submitter": "Vladimir Minin", "authors": "James R. Faulkner and Vladimir N. Minin", "title": "Locally adaptive smoothing with Markov random fields and shrinkage\n  priors", "comments": "38 pages, to appear in Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a locally adaptive nonparametric curve fitting method that\noperates within a fully Bayesian framework. This method uses shrinkage priors\nto induce sparsity in order-k differences in the latent trend function,\nproviding a combination of local adaptation and global control. Using a scale\nmixture of normals representation of shrinkage priors, we make explicit\nconnections between our method and kth order Gaussian Markov random field\nsmoothing. We call the resulting processes shrinkage prior Markov random fields\n(SPMRFs). We use Hamiltonian Monte Carlo to approximate the posterior\ndistribution of model parameters because this method provides superior\nperformance in the presence of the high dimensionality and strong parameter\ncorrelations exhibited by our models. We compare the performance of three prior\nformulations using simulated data and find the horseshoe prior provides the\nbest compromise between bias and precision. We apply SPMRF models to two\nbenchmark data examples frequently used to test nonparametric methods. We find\nthat this method is flexible enough to accommodate a variety of data generating\nmodels and offers the adaptive properties and computational tractability to\nmake it a useful addition to the Bayesian nonparametric toolbox.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 06:34:27 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 05:54:05 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Faulkner", "James R.", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1512.06830", "submitter": "Daniel Takahashi", "authors": "Andr\\'e Fujita, Daniel Yasumasa Takahashi, Joana Bisol Balardin and\n  Jo\\~ao Ricardo Sato", "title": "Correlation between graphs with an application to brain networks\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global functional brain network (graph) is more suitable for\ncharacterizing brain states than local analysis of the connectivity of brain\nregions. Therefore, graph-theoretic approaches are the natural methods to study\nthe brain. However, conventional graph theoretical analyses are limited due to\nthe lack of formal statistical methods for estimation and inference for random\ngraphs. For example, the concept of correlation between two vectors of graphs\nis yet not defined. The aim of this article to introduce a notion of\ncorrelation between graphs. In order to develop a framework to infer\ncorrelation between graphs, we assume that they are generated by mathematical\nmodels and that the parameters of the models are our random variables. Then, we\ndefine that two vectors of graphs are independent whether their parameters are\nindependent. The problem is that, in real world, the model is rarely known, and\nconsequently, the parameters cannot be estimated. By analyzing the graph\nspectrum, we showed that the spectral radius is highly associated with the\nparameters of the graph model. Based on it, we constructed a framework for\ncorrelation inference between graphs and illustrate our approach in a\nfunctional magnetic resonance imaging data composed of 814 subjects comprising\n529 controls and 285 individuals diagnosed with autism spectrum disorder (ASD).\nResults show that correlations between default-mode and control, default-mode\nand somatomotor, and default-mode and visual sub-networks are higher ($p<0.05$)\nin ASD than in controls.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 20:54:52 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Fujita", "Andr\u00e9", ""], ["Takahashi", "Daniel Yasumasa", ""], ["Balardin", "Joana Bisol", ""], ["Sato", "Jo\u00e3o Ricardo", ""]]}, {"id": "1512.06881", "submitter": "Katrin Haeussler D", "authors": "Katrin Haeussler, Ardo van den Hout, Gianluca Baio", "title": "A dynamic Bayesian Markov model for health economic evaluations of\n  interventions in infectious disease", "comments": null, "journal-ref": "BMC Medical Research Methodology(2018) 18:82", "doi": "doi.org/10.1186/s12874-018-0541-7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Health economic evaluations of interventions against infectious\ndiseases are commonly based on the predictions of ordinary differential\nequation (ODE) systems or Markov models (MMs). Standard MMs are static, whereas\nODE systems are usually dynamic and account for herd immunity which is crucial\nto prevent overestimation of infection prevalence. Complex ODE systems\nincluding probabilistic model parameters are computationally intensive. Thus,\nmainly ODE-based models including deterministic parameters are presented in the\nliterature. These do not account for parameter uncertainty. As a consequence,\nprobabilistic sensitivity analysis (PSA), a crucial component of health\neconomic evaluations, cannot be conducted straightforwardly.\n  Methods. We present a dynamic MM under a Bayesian framework. We extend a\nstatic MM by incorporating the force of infection into the state allocation\nalgorithm. The corresponding output is based on dynamic changes in prevalence\nand thus accounts for herd immunity. In contrast to deterministic ODE-based\nmodels, PSA can be conducted straightforwardly. We introduce a case study of a\nfictional sexually transmitted infection and compare our dynamic Bayesian MM to\na deterministic and a Bayesian ODE system. The models are calibrated to time\nseries data.\n  Results. By means of the case study, we show that our methodology produces\noutcome which is comparable to the \"gold standard\" of the Bayesian ODE system.\n  Conclusions. In contrast to ODE systems in the literature, the setting of the\ndynamic MM is probabilistic at manageable computational effort (including\ncalibration). The run time of the Bayesian ODE system is 44 times longer.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 21:34:01 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 21:08:29 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 16:41:34 GMT"}, {"version": "v4", "created": "Fri, 3 Feb 2017 03:37:33 GMT"}, {"version": "v5", "created": "Sat, 1 Sep 2018 10:03:15 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Haeussler", "Katrin", ""], ["Hout", "Ardo van den", ""], ["Baio", "Gianluca", ""]]}, {"id": "1512.07008", "submitter": "Debasish Roy", "authors": "Mamatha Venugopal, Ram Mohan Vasu and Debasish Roy", "title": "A Stochastically Evolving Non-local Search and Solutions to Inverse\n  Problems with Sparse Data", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building upon our earlier work of a martingale approach to global\noptimization, a powerful stochastic search scheme for the global optimum of\ncost functions is proposed on the basis of change of measures on the states\nthat evolve as diffusion processes and splitting of the state-space along the\nlines of a Bayesian game. To begin with, the efficacy of the optimizer, when\ncontrasted with one of the most efficient existing schemes, is assessed against\na family of Np-hard benchmark problems. Then, using both simulated- and\nexperimental data, potentialities of the new proposal are further explored in\nthe context of an inverse problem of significance in medical imaging, wherein\nthe superior reconstruction features of a global search vis-\\`a-vis the\ncommonly adopted local or quasi-local schemes are brought into relief.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 09:59:33 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Venugopal", "Mamatha", ""], ["Vasu", "Ram Mohan", ""], ["Roy", "Debasish", ""]]}, {"id": "1512.07060", "submitter": "Bertrand Iooss", "authors": "Thomas Browne (UPD5), Bertrand Iooss (IMT, GdR MASCOT-NUM), Lo\\\"ic Le\n  Gratiet, J\\'er\\^ome Lonchampt, Emmanuel Remy", "title": "Stochastic simulators based optimization by Gaussian process metamodels\n  -- Application to maintenance investments planning issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the optimization of industrial asset management\nstrategies, whose profitability is characterized by the Net Present Value (NPV)\nindicator which is assessed by a Monte Carlo simulator. The developed method\nconsists in building a metamodel of this stochastic simulator, allowing to get,\nfor a given model input, the NPV probability distribution without running the\nsimulator. The present work is concentrated on the emulation of the quantile\nfunction of the stochastic simulator by interpolating well chosen basis\nfunctions and metamodeling their coefficients (using the Gaussian process\nmetamodel). This quantile function metamodel is then used to treat a problem of\nstrategy maintenance optimization (four systems installed on different plants),\nin order to optimize an NPV quantile. Using the Gaussian process framework, an\nadaptive design method (called QFEI) is defined by extending in our case the\nwell known EGO algorithm. This allows to obtain an \"optimal\" solution using a\nsmall number of simulator runs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 12:42:47 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 14:32:06 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Browne", "Thomas", "", "UPD5"], ["Iooss", "Bertrand", "", "IMT, GdR MASCOT-NUM"], ["Gratiet", "Lo\u00efc Le", ""], ["Lonchampt", "J\u00e9r\u00f4me", ""], ["Remy", "Emmanuel", ""]]}, {"id": "1512.07075", "submitter": "Catherine Matias", "authors": "Catherine Matias (1), Tabea Rebafka (1), Fanny Villers (1) ((1) LPMA)", "title": "A semiparametric extension of the stochastic block model for\n  longitudinal networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model recurrent interaction events in continuous time, an extension of the\nstochastic block model is proposed where every individual belongs to a latent\ngroup and interactions between two individuals follow a conditional\ninhomogeneous Poisson process with intensity driven by the individuals' latent\ngroups. The model is shown to be identifiable and its estimation is based on a\nsemiparametric variational expectation-maximization algorithm. Two versions of\nthe method are developed, using either a nonparametric histogram approach (with\nan adaptive choice of the partition size) or kernel intensity estimators. The\nnumber of latent groups can be selected by an integrated classification\nlikelihood criterion. Finally, we demonstrate the performance of our procedure\non synthetic experiments, analyse two datasets to illustrate the utility of our\napproach and comment on competing methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 13:22:08 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 11:35:19 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 13:12:56 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Matias", "Catherine", "", "LPMA"], ["Rebafka", "Tabea", "", "LPMA"], ["Villers", "Fanny", "", "LPMA"]]}, {"id": "1512.07082", "submitter": "Xing He", "authors": "Xing He, Robert C. Qiu (IEEE Fellow), Qian Ai (IEEE Member), Lei Chu,\n  Xinyi Xu, Zenan Ling", "title": "Designing for Situation Awareness of Future Power Grids: An Indicator\n  System Based on Linear Eigenvalue Statistics of Large Random Matrices", "comments": "8 pages, 8 figures, 3 tables", "journal-ref": "IEEE Access , vol.4, pp.3557-3568, 2016", "doi": "10.1109/ACCESS.2016.2581838", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future power grids are fundamentally different from current ones, both in\nsize and in complexity; this trend imposes challenges for situation awareness\n(SA) based on classical indicators, which are usually model-based and\ndeterministic. As an alternative, this paper proposes a statistical indicator\nsystem based on linear eigenvalue statistics (LESs) of large random matrices:\n1) from a data modeling viewpoint, we build, starting from power flows\nequations, the random matrix models (RMMs) only using the real-time data flow\nin a statistical manner; 2) for a data analysis that is fully driven from RMMs,\nwe put forward the high-dimensional indicators, called LESs that have some\nunique statistical features such as Gaussian properties; and 3) we develop a\nthree-dimensional (3D) power-map to visualize the system, respectively, from a\nhigh-dimensional viewpoint and a low-dimensional one. Therefore, a statistical\nmethodology of SA is employed; it conducts SA with a model-free and data-driven\nprocedure, requiring no knowledge of system topologies, units operation/control\nmodels, causal relationship, etc. This methodology has numerous advantages,\nsuch as sensitivity, universality, speed, and flexibility. In particular, its\nrobustness against bad data is highlighted, with potential advantages in cyber\nsecurity. The theory of big data based stability for on-line operations may\nprove feasible along with this line of work, although this critical development\nwill be reported elsewhere.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 13:42:44 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 12:42:19 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["He", "Xing", "", "IEEE Fellow"], ["Qiu", "Robert C.", "", "IEEE Fellow"], ["Ai", "Qian", "", "IEEE Member"], ["Chu", "Lei", ""], ["Xu", "Xinyi", ""], ["Ling", "Zenan", ""]]}, {"id": "1512.07139", "submitter": "Deepesh Bhati Mr.", "authors": "Deepesh Bhati and Savitri Joshi", "title": "Weighted geometric distribution with a new characterisation of geometric\n  distribution", "comments": "17 pages 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new generalization of geometric distribution\nwhich can also viewed as discrete analogue of weighted exponential distribution\nintroduced by Gupta and Kundu(2009). We study some basic distributional\nproperties like moments, generating functions, hazard function followed by\ndifferent methods of estimation of the parameters. Characterization of\nGeometric distribution have also been presented. Finally, we examine the model\nwith real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 16:00:49 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 18:36:07 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Bhati", "Deepesh", ""], ["Joshi", "Savitri", ""]]}, {"id": "1512.07273", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley, Scott H. Holan, Christopher K. Wikle", "title": "Computationally Efficient Distribution Theory for Bayesian Inference of\n  High-Dimensional Dependent Count-Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian approach for multivariate spatio-temporal prediction\nfor high-dimensional count-valued data. Our primary interest is when there are\npossibly millions of data points referenced over different variables,\ngeographic regions, and times. This problem requires extensive methodological\nadvancements, as jointly modeling correlated data of this size leads to the\nso-called \"big n problem.\" The computational complexity of prediction in this\nsetting is further exacerbated by acknowledging that count-valued data are\nnaturally non-Gaussian. Thus, we develop a new computationally efficient\ndistribution theory for this setting. In particular, we introduce a\nmultivariate log-gamma distribution and provide substantial theoretical\ndevelopment including: results regarding conditional distributions, marginal\ndistributions, an asymptotic relationship with the multivariate normal\ndistribution, and full-conditional distributions for a Gibbs sampler. To\nincorporate dependence between variables, regions, and time points, a\nmultivariate spatio-temporal mixed effects model (MSTM) is used. The results in\nthis manuscript are extremely general, and can be used for data that exhibit\nfewer sources of dependency than what we consider (e.g., multivariate,\nspatial-only, or spatio-temporal-only data). Hence, the implications of our\nmodeling framework may have a large impact on the general problem of jointly\nmodeling correlated count-valued data. We show the effectiveness of our\napproach through a simulation study. Additionally, we demonstrate our proposed\nmethodology with an important application analyzing data obtained from the\nLongitudinal Employer-Household Dynamics (LEHD) program, which is administered\nby the U.S. Census Bureau.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 21:48:33 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1512.07328", "submitter": "Shirin Golchi", "authors": "Shirin Golchi and Jason L. Loeppky", "title": "Monte Carlo based Designs for Constrained Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space filling designs are central to studying complex systems in various\nareas of science. They are used for obtaining an overall understanding of the\nbehaviour of the response over the input space, model construction and\nuncertainty quantification. In many applications a set of constraints are\nimposed over the inputs that result in a non-rectangular and sometimes\nnon-convex input space. Many of the existing design construction techniques in\nthe literature rely on a set of candidate points on the target space.\nGenerating a sample on highly constrained regions can be a challenging task. We\npropose a sampling algorithm based on sequential Monte Carlo that is\nspecifically designed to sample uniformly over constrained regions. In\naddition, a review of Monte Carlo based design algorithms is provided and the\nperformance of the sampling algorithm as well as selected design methodology is\nillustrated via examples.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 01:50:08 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 23:13:41 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Golchi", "Shirin", ""], ["Loeppky", "Jason L.", ""]]}, {"id": "1512.07385", "submitter": "Michael Creel", "authors": "Michael Creel, Jiti Gao, Han Hong, Dennis Kristensen", "title": "Bayesian Indirect Inference and the ABC of GMM", "comments": "Further work has led us to conflicting results, we are still working\n  on resolving the differences. This version is likely to have some incorrect\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study local linear and polynomial based\nestimators for implementing Approximate Bayesian Computation (ABC) style\nindirect inference and GMM estimators. This method makes use of nonparametric\nregression in the computation of GMM and Indirect Inference models. We provide\nformal conditions under which frequentist inference is asymptotically valid and\ndemonstrate the validity of the estimated posterior quantiles for confidence\ninterval construction. We also show that in this setting, local linear kernel\nregression methods have theoretical advantages over local constant kernel\nmethods that are also reflected in finite sample simulation results. Our\nresults also apply to both exactly and over identified models. These estimators\ndo not need to rely on numerical optimization or Markov Chain Monte Carlo\n(MCMC) simulations. They provide an effective complement to the classical\nM-estimators and to MCMC methods, and can be applied to both likelihood based\nmodels and method of moment based models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 08:14:16 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 09:36:22 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Creel", "Michael", ""], ["Gao", "Jiti", ""], ["Hong", "Han", ""], ["Kristensen", "Dennis", ""]]}, {"id": "1512.07441", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Viatcheslav B. Melas, Petr Shpilev", "title": "$T$-optimal discriminating designs for Fourier regression models", "comments": "Keywords and Phrases: T-optimal design; model discrimination; linear\n  optimality criteria; Chebyshev polynomial, trigonometric models AMS subject\n  classification: 62K05", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of constructing $T$-optimal\ndiscriminating designs for Fourier regression models. We provide explicit\nsolutions of the optimal design problem for discriminating between two Fourier\nregression models, which differ by at most three trigonometric functions. In\ngeneral, the $T$-optimal discriminating design depends in a complicated way on\nthe parameters of the larger model, and for special configurations of the\nparameters $T$-optimal discriminating designs can be found analytically.\nMoreover, we also study this dependence in the remaining cases by calculating\nthe optimal designs numerically. In particular, it is demonstrated that $D$-\nand $D_s$-optimal designs have rather low efficiencies with respect to the\n$T$-optimality criterion.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 11:49:46 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Dette", "Holger", ""], ["Melas", "Viatcheslav B.", ""], ["Shpilev", "Petr", ""]]}, {"id": "1512.07451", "submitter": "David Woods", "authors": "Veronica E. Bowman and David C. Woods", "title": "Emulation of multivariate simulators using thin-plate splines with\n  application to atmospheric dispersion", "comments": null, "journal-ref": "SIAM/ASA Journal of Uncertainty Quantification, 4, 1323-1344, 2016", "doi": "10.1137/140970148", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often desirable to build a statistical emulator of a complex computer\nsimulator in order to perform analysis which would otherwise be computationally\ninfeasible. We propose methodology to model multivariate output from a computer\nsimulator taking into account output structure in the responses. The utility of\nthis approach is demonstrated by applying it to a chemical and biological\nhazard prediction model. Predicting the hazard area that results from an\naccidental or deliberate chemical or biological release is imperative in civil\nand military planning and also in emergency response. The hazard area resulting\nfrom such a release is highly structured in space and we therefore propose the\nuse of a thin-plate spline to capture the spatial structure and fit a Gaussian\nprocess emulator to the coefficients of the resultant basis functions. We\ncompare and contrast four different techniques for emulating multivariate\noutput: dimension-reduction using (i) a fully Bayesian approach with a\nprincipal component basis, (ii) a fully Bayesian approach with a thin-plate\nspline basis, assuming that the basis coefficients are independent, and (iii) a\n\"plug-in\" Bayesian approach with a thin-plate spline basis and a separable\ncovariance structure; and (iv) a functional data modeling approach using a\ntensor-product (separable) Gaussian process. We develop methodology for the two\nthin-plate spline emulators and demonstrate that these emulators significantly\noutperform the principal component emulator. Further, the separable thin-plate\nspline emulator, which accounts for the dependence between basis coefficients,\nprovides substantially more realistic quantification of uncertainty, and is\nalso computationally more tractable, allowing fast emulation. For high\nresolution output data, it also offers substantial predictive and computational\nadvantages over the tensor-product Gaussian process emulator.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 12:34:17 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 09:04:04 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 16:58:29 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Bowman", "Veronica E.", ""], ["Woods", "David C.", ""]]}, {"id": "1512.07568", "submitter": "Jingjing Yang", "authors": "Jingjing Yang, Dennis D. Cox, Jong Soo Lee, Peng Ren, Taeryon Choi", "title": "Efficient Bayesian hierarchical functional data analysis with basis\n  function approximations using Gaussian-Wishart processes", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data are defined as realizations of random functions (mostly\nsmooth functions) varying over a continuum, which are usually collected with\nmeasurement errors on discretized grids. In order to accurately smooth noisy\nfunctional observations and deal with the issue of high-dimensional observation\ngrids, we propose a novel Bayesian method based on the Bayesian hierarchical\nmodel with a Gaussian-Wishart process prior and basis function representations.\nWe first derive an induced model for the basis-function coefficients of the\nfunctional data, and then use this model to conduct posterior inference through\nMarkov chain Monte Carlo. Compared to the standard Bayesian inference that\nsuffers serious computational burden and unstableness for analyzing\nhigh-dimensional functional data, our method greatly improves the computational\nscalability and stability, while inheriting the advantage of simultaneously\nsmoothing raw observations and estimating the mean-covariance functions in a\nnonparametric way. In addition, our method can naturally handle functional data\nobserved on random or uncommon grids. Simulation and real studies demonstrate\nthat our method produces similar results as the standard Bayesian inference\nwith low-dimensional common grids, while efficiently smoothing and estimating\nfunctional data with random and high-dimensional observation grids where the\nstandard Bayesian inference fails. In conclusion, our method can efficiently\nsmooth and estimate high-dimensional functional data, providing one way to\nresolve the curse of dimensionality for Bayesian functional data analysis with\nGaussian-Wishart processes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 18:09:34 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 17:01:49 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 18:52:03 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Yang", "Jingjing", ""], ["Cox", "Dennis D.", ""], ["Lee", "Jong Soo", ""], ["Ren", "Peng", ""], ["Choi", "Taeryon", ""]]}, {"id": "1512.07607", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Mevin B. Hooten, Bailey K. Fosdick, Devin S. Johnson,\n  Josh M. London, John W. Durban", "title": "Dynamic social networks based on movement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network modeling techniques provide a means for quantifying social structure\nin populations of individuals. Data used to define social connectivity are\noften expensive to collect and based on case-specific, ad hoc criteria.\nMoreover, in applications involving animal social networks, collection of these\ndata is often opportunistic and can be invasive. Frequently, the social network\nof interest for a given population is closely related to the way individuals\nmove. Thus telemetry data, which are minimally-invasive and relatively\ninexpensive to collect, present an alternative source of information. We\ndevelop a framework for using telemetry data to infer social relationships\namong animals. To achieve this, we propose a Bayesian hierarchical model with\nan underlying dynamic social network controlling movement of individuals via\ntwo mechanisms: an attractive effect, and an aligning effect. We demonstrate\nthe model and its ability to accurately identify complex social behavior in\nsimulation, and apply our model to telemetry data arising from killer whales.\nUsing auxiliary information about the study population, we investigate model\nvalidity and find the inferred dynamic social network is consistent with killer\nwhale ecology and expert knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 20:03:13 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 00:36:45 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Scharf", "Henry R.", ""], ["Hooten", "Mevin B.", ""], ["Fosdick", "Bailey K.", ""], ["Johnson", "Devin S.", ""], ["London", "Josh M.", ""], ["Durban", "John W.", ""]]}, {"id": "1512.07619", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Victor Chernozhukov, Denis Chetverikov, and Ying\n  Wei", "title": "Uniformly Valid Post-Regularization Confidence Regions for Many\n  Functional Parameters in Z-Estimation Framework", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop procedures to construct simultaneous confidence\nbands for $\\tilde p$ potentially infinite-dimensional parameters after model\nselection for general moment condition models where $\\tilde p$ is potentially\nmuch larger than the sample size of available data, $n$. This allows us to\ncover settings with functional response data where each of the $\\tilde p$\nparameters is a function. The procedure is based on the construction of score\nfunctions that satisfy certain orthogonality condition. The proposed\nsimultaneous confidence bands rely on uniform central limit theorems for\nhigh-dimensional vectors (and not on Donsker arguments as we allow for $\\tilde\np \\gg n$). To construct the bands, we employ a multiplier bootstrap procedure\nwhich is computationally efficient as it only involves resampling the estimated\nscore functions (and does not require resolving the high-dimensional\noptimization problems). We formally apply the general theory to inference on\nregression coefficient process in the distribution regression model with a\nlogistic link, where two implementations are analyzed in detail. Simulations\nand an application to real data are provided to help illustrate the\napplicability of the results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 20:33:45 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 21:44:27 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 18:33:38 GMT"}, {"version": "v4", "created": "Sun, 3 Feb 2019 22:48:04 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Wei", "Ying", ""]]}, {"id": "1512.07621", "submitter": "Jean-David Fermanian", "authors": "Jean-David Fermanian and Olivier Lopez", "title": "Single-index copulae", "comments": "Revised version: correction of Assumption 3 and some minor induced\n  modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce so-called \"single-index copulae\". They are semi-parametric\nconditional copulae whose parameter is an unknown \"link\" function of a\nunivariate index only. We provide estimates of this link function and of the\nfinite dimensional unknown parameter. The asymptotic properties of the latter\nestimates are stated. Thanks to some properties of conditional Kendall's tau,\nwe illustrate our technical conditions with several usual copula families.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 20:43:23 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 09:17:27 GMT"}, {"version": "v3", "created": "Tue, 4 Jul 2017 18:42:06 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Fermanian", "Jean-David", ""], ["Lopez", "Olivier", ""]]}, {"id": "1512.07678", "submitter": "Alexis Roche", "authors": "Alexis Roche", "title": "Composite Bayesian inference", "comments": "Working paper: 4th version (v4), significantly improved wrt previous\n  version (v3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit and generalize the concept of composite likelihood as a method to\nmake a probabilistic inference by aggregation of multiple Bayesian agents,\nthereby defining a class of predictive models which we call composite Bayesian.\nThis perspective gives insight to choose the weights associated with composite\nlikelihood, either a priori or via learning; in the latter case, they may be\ntuned so as to minimize prediction cross-entropy, yielding an easy-to-solve\nconvex problem. We argue that composite Bayesian inference is a middle way\nbetween generative and discriminative models that trades off between\ninterpretability and prediction performance, both of which are crucial to many\nartificial intelligence tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 01:28:30 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 08:40:37 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 09:31:17 GMT"}, {"version": "v4", "created": "Wed, 17 Apr 2019 15:34:30 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Roche", "Alexis", ""]]}, {"id": "1512.07848", "submitter": "James Johndrow", "authors": "James E. Johndrow and Robert L. Wolpert", "title": "Model-free inference on extreme dependence via waiting times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of methods have been proposed for inference about extreme\ndependence for multivariate or spatially-indexed stochastic processes and time\nseries. Most of these proceed by first transforming data to some specific\nextreme value marginal distribution, often the unit Fr\\'echet, then fitting a\nfamily of max-stable processes to the transformed data and exploring dependence\nwithin the framework of that model. The marginal transformation, model\nselection, and model fitting are all possible sources of misspecification in\nthis approach.\n  We propose an alternative model-free approach, based on the idea that\nsubstantial information on the strength of tail dependence and its temporal\nstructure are encoded in the distribution of the waiting times between\nexceedances of high thresholds at different locations. We propose quantifying\nthe strength of extremal dependence and assessing uncertainty by using\nstatistics based on these waiting times. The method does not rely on any\nspecific underlying model for the process, nor on asymptotic distribution\ntheory. The method is illustrated by applications to climatological, financial,\nand electrophysiology data.\n  To put the proposed approach within the context of the existing literature,\nwe construct a class of spacetime-indexed stochastic processes whose waiting\ntime distributions are available in closed form by endowing the support points\nin de Haan's spectral representation of max-stable processes with random birth\ntimes, velocities, and lifetimes, and applying Smith's model to these\nprocesses. We show that waiting times in this model are stochatically\ndecreasing in mean speed, and the sample mean of the waiting times obeys a\ncentral limit theorem with a uniform convergence rate under mild conditions.\nThis indicates that our procedure can be implemented in this setting using\nstandard $t$ statistics and associated hypothesis tests.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 16:21:39 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 23:01:54 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 17:32:01 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Johndrow", "James E.", ""], ["Wolpert", "Robert L.", ""]]}, {"id": "1512.07945", "submitter": "Hui Li", "authors": "Hui Li", "title": "Nonsymmetric Dependence Measures: the Discrete Case", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following our previous work on copula-based nonsymmetric dependence measures,\nwe introduce similar measures for discrete random variables. The measures cover\nthe range between two extremes: independence and complete dependence, which\ntake minimum value exactly on independence and take maximum value exactly on\ncomplete dependence. We find that the * product on copulas in the continuous\ncase reduces to matrix product of transition matrices in the discrete case and\nwe use it to prove the DPI condition. The measures can also be extended to\ndetect dependence between groups of discrete random variables or conditional\ndependence. Unlike the continuous case, one drawback is that the value of the\nmeasures depends on marginal distributions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 02:31:58 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Li", "Hui", ""]]}, {"id": "1512.07947", "submitter": "Alexander Wong", "authors": "Edward Li, Farzad Khalvati, Mohammad Javad Shafiee, Masoom A. Haider,\n  Alexander Wong", "title": "Sparse Reconstruction of Compressive Sensing MRI using Cross-Domain\n  Stochastically Fully Connected Conditional Random Fields", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a crucial medical imaging technology for\nthe screening and diagnosis of frequently occurring cancers. However image\nquality may suffer by long acquisition times for MRIs due to patient motion, as\nwell as result in great patient discomfort. Reducing MRI acquisition time can\nreduce patient discomfort and as a result reduces motion artifacts from the\nacquisition process. Compressive sensing strategies, when applied to MRI, have\nbeen demonstrated to be effective at decreasing acquisition times significantly\nby sparsely sampling the \\emph{k}-space during the acquisition process.\nHowever, such a strategy requires advanced reconstruction algorithms to produce\nhigh quality and reliable images from compressive sensing MRI. This paper\nproposes a new reconstruction approach based on cross-domain stochastically\nfully connected conditional random fields (CD-SFCRF) for compressive sensing\nMRI. The CD-SFCRF introduces constraints in both \\emph{k}-space and spatial\ndomains within a stochastically fully connected graphical model to produce\nimproved MRI reconstruction. Experimental results using T2-weighted (T2w)\nimaging and diffusion-weighted imaging (DWI) of the prostate show strong\nperformance in preserving fine details and tissue structures in the\nreconstructed images when compared to other tested methods even at low sampling\nrates.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 02:58:53 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Li", "Edward", ""], ["Khalvati", "Farzad", ""], ["Shafiee", "Mohammad Javad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.07948", "submitter": "Mandev Gill", "authors": "Mandev S. Gill, Lam Si Tung Ho, Guy Baele, Philippe Lemey, and Marc A.\n  Suchard", "title": "A Relaxed Drift Diffusion Model for Phylogenetic Trait Evolution", "comments": "35 pages, 3 figures, 5 tables. Changed from double-spaced to\n  single-spaced", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the processes that give rise to quantitative measurements\nassociated with molecular sequence data remains an important issue in\nstatistical phylogenetics. Examples of such measurements include geographic\ncoordinates in the context of phylogeography and phenotypic traits in the\ncontext of comparative studies. A popular approach is to model the evolution of\ncontinuously varying traits as a Brownian diffusion process. However, standard\nBrownian diffusion is quite restrictive and may not accurately characterize\ncertain trait evolutionary processes. Here, we relax one of the major\nrestrictions of standard Brownian diffusion by incorporating a nontrivial\nestimable drift into the process. We introduce a relaxed drift diffusion model\nfor the evolution of multivariate continuously varying traits along a\nphylogenetic tree via Brownian diffusion with drift. Notably, the relaxed drift\nmodel accommodates branch-specific variation of drift rates while preserving\nmodel identifiability. We implement the relaxed drift model in a Bayesian\ninference framework to simultaneously reconstruct the evolutionary histories of\nmolecular sequence data and associated multivariate continuous trait data, and\nprovide tools to visualize evolutionary reconstructions. We illustrate our\napproach in three viral examples. In the first two, we examine the\nspatiotemporal spread of HIV-1 in central Africa and West Nile virus in North\nAmerica and show that a relaxed drift approach uncovers a clearer, more\ndetailed picture of the dynamics of viral dispersal than standard Brownian\ndiffusion. Finally, we study antigenic evolution in the context of HIV-1\nresistance to three broadly neutralizing antibodies. Our analysis reveals\nevidence of a continuous drift at the HIV-1 population level towards enhanced\nresistance to neutralization by the VRC01 monoclonal antibody over the course\nof the epidemic.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 03:04:42 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 08:36:57 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Gill", "Mandev S.", ""], ["Ho", "Lam Si Tung", ""], ["Baele", "Guy", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1512.08077", "submitter": "Jeong Eun  Lee Dr", "authors": "Cristiano Villa, Jeong Eun Lee", "title": "Model Prior Distribution for Variable Selection in Linear Regression\n  Models", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we discuss a novel model prior probability for variable\nselection in linear regression. The idea is to determine the prior mass in an\nobjective sense, by considering the worth of each of the possible regression\nmodels, given the number of covariates under consideration. Through a\nsimulation study, we show that the proposed prior outperforms the uniform prior\nand the Scott \\& Berger prior in a scenario of no prior knowledge about the\nsize of the true regression models. We illustrate the use of the prior using\ntwo well-known data sets with, respectively, 15 and 4 covariates.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 05:53:49 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Villa", "Cristiano", ""], ["Lee", "Jeong Eun", ""]]}, {"id": "1512.08110", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz", "title": "Efficient Estimation of Quantiles in Missing Data Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel targeted maximum likelihood estimator (TMLE) for quantiles\nin semiparametric missing data models. Our proposed estimator is locally\nefficient, $\\sqrt{n}$-consistent, asymptotically normal, and doubly robust,\nunder regularity conditions. We use Monte Carlo simulation to compare our\nproposed method to existing estimators. The TMLE is superior to all\ncompetitors, with relative efficiency up to three times smaller than the\ninverse probability weighted estimator (IPW), and up to two times smaller than\nthe augmented IPW. This research is motivated by a causal inference research\nquestion with highly variable treatment assignment probabilities, and a heavy\ntailed, highly variable outcome. Estimation of causal effects on the mean is a\nhard problem in such scenarios because the information bound is generally\nsmall. In our application, the efficiency bound for estimating the effect on\nthe mean is possibly infinite. This rules out $\\sqrt{n}$-consistent inference\nand reduces the power for testing hypothesis of no treatment effect on the\nmean. In our simulations, using the effect on the median allows us to test a\nlocation-shift hypothesis with 30\\% more power. This allows us to make claims\nabout the effectiveness of treatment that would have hard to make for the\neffect on the mean. We provide R code to implement the proposed estimators.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 13:17:53 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 15:02:57 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""]]}, {"id": "1512.08191", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB)", "title": "Estimation of Kullback-Leibler losses for noisy recovery problems within\n  the exponential family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the question of estimating Kullback-Leibler losses rather than\nsquared losses in recovery problems where the noise is distributed within the\nexponential family. Inspired by Stein unbiased risk estimator (SURE), we\nexhibit conditions under which these losses can be unbiasedly estimated or\nestimated with a controlled bias. Simulations on parameter selection problems\nin applications to image denoising and variable selection with Gamma and\nPoisson noises illustrate the interest of Kullback-Leibler losses and the\nproposed estimators.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 09:46:32 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 11:13:05 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 12:37:29 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB"]]}, {"id": "1512.08309", "submitter": "Rakesh Malladi", "authors": "Rakesh Malladi, Giridhar Kalamangalam, Nitin Tandon and Behnaam\n  Aazhang", "title": "Identifying Seizure Onset Zone from the Causal Connectivity Inferred\n  Using Directed Information", "comments": "This paper is accepted for publication in IEEE Journal of Selected\n  Topics in Signal Processing, special issue on Advanced Signal Processing in\n  Brain Networks, October 2016. 16 pages, 11 figures and 2 tables", "journal-ref": null, "doi": "10.1109/JSTSP.2016.2601485", "report-no": null, "categories": "q-bio.NC cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we developed a model-based and a data-driven estimator for\ndirected information (DI) to infer the causal connectivity graph between\nelectrocorticographic (ECoG) signals recorded from brain and to identify the\nseizure onset zone (SOZ) in epileptic patients. Directed information, an\ninformation theoretic quantity, is a general metric to infer causal\nconnectivity between time-series and is not restricted to a particular class of\nmodels unlike the popular metrics based on Granger causality or transfer\nentropy. The proposed estimators are shown to be almost surely convergent.\nCausal connectivity between ECoG electrodes in five epileptic patients is\ninferred using the proposed DI estimators, after validating their performance\non simulated data. We then proposed a model-based and a data-driven SOZ\nidentification algorithm to identify SOZ from the causal connectivity inferred\nusing model-based and data-driven DI estimators respectively. The data-driven\nSOZ identification outperforms the model-based SOZ identification algorithm\nwhen benchmarked against visual analysis by neurologist, the current clinical\ngold standard. The causal connectivity analysis presented here is the first\nstep towards developing novel non-surgical treatments for epilepsy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 02:53:24 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 19:21:32 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Malladi", "Rakesh", ""], ["Kalamangalam", "Giridhar", ""], ["Tandon", "Nitin", ""], ["Aazhang", "Behnaam", ""]]}, {"id": "1512.08552", "submitter": "Daniel Benjamin", "authors": "M.J. Bayarri, Daniel J. Benjamin, James O. Berger, Thomas M. Sellke", "title": "Rejection Odds and Rejection Ratios: A Proposal for Statistical Practice\n  in Testing Hypotheses", "comments": "62-00, 62-01, 62A01", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of science is (rightly or wrongly) driven by hypothesis testing. Even in\nsituations where the hypothesis testing paradigm is correct, the common\npractice of basing inferences solely on p-values has been under intense\ncriticism for over 50 years. We propose, as an alternative, the use of the odds\nof a correct rejection of the null hypothesis to incorrect rejection. Both\npre-experimental versions (involving the power and Type I error) and\npost-experimental versions (depending on the actual data) are considered.\nImplementations are provided that range from depending only on the p-value to\nconsideration of full Bayesian analysis. A surprise is that all implementations\n-- even the full Bayesian analysis -- have complete frequentist justification.\nVersions of our proposal can be implemented that require only minor\nmodifications to existing practices yet overcome some of their most severe\nshortcomings.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 23:05:26 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Bayarri", "M. J.", ""], ["Benjamin", "Daniel J.", ""], ["Berger", "James O.", ""], ["Sellke", "Thomas M.", ""]]}, {"id": "1512.08560", "submitter": "Cameron Bracken", "authors": "Cameron Bracken, Balaji Rajagopalan, Linyin Cheng, Will Kleiber and\n  Subhrendu Gangopadhyay", "title": "Spatial Bayesian hierarchical modeling of precipitation extremes over a\n  large domain", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": "10.1002/2016WR018768", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian hierarchical model for spatial extremes on a large\ndomain. In the data layer a Gaussian elliptical copula having generalized\nextreme value (GEV) marginals is applied. Spatial dependence in the GEV\nparameters are captured with a latent spatial regression with spatially varying\ncoefficients. Using a composite likelihood approach, we are able to efficiently\nincorporate a large precipitation dataset, which includes stations with missing\ndata. The model is demonstrated by application to fall precipitation extremes\nat approximately 2600 stations covering the western United States, -125E to\n-100E longitude and 30N to 50N latitude. The hierarchical model provides GEV\nparameters on a $1/8$th degree grid and consequently maps of return levels and\nassociated uncertainty. The model results indicate that return levels vary\ncoherently both spatially and across seasons, providing information about the\nspace-time variations of risk of extreme precipitation in the western US,\nhelpful for infrastructure planning.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 23:31:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 18:39:26 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Bracken", "Cameron", ""], ["Rajagopalan", "Balaji", ""], ["Cheng", "Linyin", ""], ["Kleiber", "Will", ""], ["Gangopadhyay", "Subhrendu", ""]]}, {"id": "1512.08731", "submitter": "Y. Samuel Wang", "authors": "Y. Samuel Wang, Ross Matsueda, Elena A. Erosheva", "title": "A Variational EM Method for Mixed Membership Models with Multivariate\n  Rank Data: an Analysis of Public Policy Preferences", "comments": "24 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider modeling ranked responses from a heterogeneous\npopulation. Specifically, we analyze data from the Eurobarometer 34.1 survey\nregarding public policy preferences towards drugs, alcohol and AIDS. Such\npolicy preferences are likely to exhibit substantial differences within as well\nas across European nations reflecting a wide variety of cultures, political\naffiliations, ideological perspectives and common practices. We use a mixed\nmembership model to account for multiple subgroups with differing preferences\nand to allow each individual to possess partial membership in more than one\nsubgroup. Previous methods for fitting mixed membership models to rank data in\na univariate setting have utilized an MCMC approach and do not estimate the\nrelative frequency of each subgroup. We propose a variational EM approach for\nfitting mixed membership models with multivariate rank data. Our method allows\nfor fast approximate inference and explicitly estimates the subgroup sizes.\nAnalyzing the Eurobarometer 34.1 data, we find interpretable subgroups which\ngenerally agree with the \"left vs right\" classification of political\nideologies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 17:13:48 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 16:24:10 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 17:08:09 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Wang", "Y. Samuel", ""], ["Matsueda", "Ross", ""], ["Erosheva", "Elena A.", ""]]}, {"id": "1512.08815", "submitter": "Peter Hoff", "authors": "James W. Harmon and Peter D. Hoff", "title": "A Pivot-Based Improvement to Sandwich-Based Confidence Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current standard for confidence interval construction in the context of a\npossibly misspecified model is to use an interval based on the sandwich\nestimate of variance. These intervals provide asymptotically correct coverage,\nbut small-sample coverage is known to be poor. By eliminating a plug-in\nassumption, we derive a pivot-based method for confidence interval construction\nunder possibly misspecified models. When compared against confidence intervals\ngenerated by the sandwich estimate of variance, this method provides more\naccurate coverage of the pseudo-true parameter at small sample sizes. This is\nshown in the results of several simulation studies. Asymptotic results show\nthat our pivot-based intervals have large sample efficiency equal to that of\nintervals based on the sandwich estimate of variance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 23:52:17 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Harmon", "James W.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1512.08819", "submitter": "Lingzhou Xue", "authors": "Danning Li and Lingzhou Xue", "title": "Joint limiting laws for high-dimensional independence tests", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing independence is of significant interest in many important areas of\nlarge-scale inference. Using extreme-value form statistics to test against\nsparse alternatives and using quadratic form statistics to test against dense\nalternatives are two important testing procedures for high-dimensional\nindependence. However, quadratic form statistics suffer from low power against\nsparse alternatives, and extreme-value form statistics suffer from low power\nagainst dense alternatives with small disturbances and may have size\ndistortions due to its slow convergence. For real-world applications, it is\nimportant to derive powerful testing procedures against more general\nalternatives. Based on intermediate limiting distributions, we derive\n(model-free) joint limiting laws of extreme-value form and quadratic form\nstatistics, and surprisingly, we prove that they are asymptotically\nindependent. Given such asymptotic independencies, we propose (model-free)\ntesting procedures to boost the power against general alternatives and also\nretain the correct asymptotic size. Under the high-dimensional setting, we\nderive the closed-form limiting null distributions, and obtain their explicit\nrates of uniform convergence. We prove their consistent statistical powers\nagainst general alternatives. We demonstrate the performance of our proposed\ntest statistics in simulation studies. Our work provides very helpful insights\nto high-dimensional independence tests, and fills an important gap.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 00:30:19 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Li", "Danning", ""], ["Xue", "Lingzhou", ""]]}, {"id": "1512.08996", "submitter": "Mingyuan Zhou", "authors": "Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou", "title": "Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices", "comments": "Appeared in Artificial Intelligence and Statistics (AISTATS), May\n  2015. The ArXiv version fixes a typo in (8), the equation right above Section\n  3.2 in Page 4 of http://www.jmlr.org/proceedings/papers/v38/acharya15.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A gamma process dynamic Poisson factor analysis model is proposed to\nfactorize a dynamic count matrix, whose columns are sequentially observed count\nvectors. The model builds a novel Markov chain that sends the latent gamma\nrandom variables at time $(t-1)$ as the shape parameters of those at time $t$,\nwhich are linked to observed or latent counts under the Poisson likelihood. The\nsignificant challenge of inferring the gamma shape parameters is fully\naddressed, using unique data augmentation and marginalization techniques for\nthe negative binomial distribution. The same nonparametric Bayesian model also\napplies to the factorization of a dynamic binary matrix, via a\nBernoulli-Poisson link that connects a binary observation to a latent count,\nwith closed-form conditional posteriors for the latent counts and efficient\ncomputation for sparse observations. We apply the model to text and music\nanalysis, with state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 16:28:55 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Acharya", "Ayan", ""], ["Ghosh", "Joydeep", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1512.09020", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Limitations on detecting row covariance in the presence of column\n  covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference techniques for multivariate data analysis assume that the rows\nof the data matrix are realizations of independent and identically distributed\nrandom vectors. Such an assumption will be met, for example, if the rows of the\ndata matrix are multivariate measurements on a set of independently sampled\nunits. In the absence of an independent random sample, a relevant question is\nwhether or not a statistical model that assumes such row exchangeability is\nplausible. One method for assessing this plausibility is a statistical test of\nrow covariation. Maintenance of a constant type I error rate regardless of the\ncolumn covariance or matrix mean can be accomplished with a test that is\ninvariant under an appropriate group of transformations. In the context of a\nclass of elliptically contoured matrix regression models (such as matrix normal\nmodels), I show that there are no non-trivial invariant tests if the number of\nrows is not sufficiently larger than the number of columns. Furthermore, I show\nthat even if the number of rows is large, there are no non-trivial invariant\ntests that have power to detect arbitrary row covariance in the presence of\narbitrary column covariance. However, we can construct biased tests that have\npower to detect certain types of row covariance that may be encountered in\npractice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 17:12:35 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1512.09052", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer, Ingeborg Warnke, Wulf R\\\"ossler, Leonhard Held", "title": "Model-based testing for space-time interaction using point processes: An\n  application to psychiatric hospital admissions in an urban area", "comments": "21 pages including 4 figures and 5 tables; methods are implemented in\n  the R package surveillance (https://CRAN.R-project.org/package=surveillance)", "journal-ref": "Spatial and Spatio-temporal Epidemiology 17, 15-25 (2016)", "doi": "10.1016/j.sste.2016.03.002", "report-no": null, "categories": "stat.ME physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal interaction is inherent to cases of infectious diseases and\noccurrences of earthquakes, whereas the spread of other events, such as cancer\nor crime, is less evident. Statistical significance tests of space-time\nclustering usually assess the correlation between the spatial and temporal\n(transformed) distances of the events. Although appealing through simplicity,\nthese classical tests do not adjust for the underlying population nor can they\naccount for a distance decay of interaction. We propose to use the framework of\nan endemic-epidemic point process model to jointly estimate a background event\nrate explained by seasonal and areal characteristics, as well as a superposed\nepidemic component representing the hypothesis of interest. We illustrate this\nnew model-based test for space-time interaction by analysing psychiatric\ninpatient admissions in Zurich, Switzerland (2007-2012). Several socio-economic\nfactors were found to be associated with the admission rate, but there was no\nevidence of general clustering of the cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 18:32:58 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 12:53:42 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Meyer", "Sebastian", ""], ["Warnke", "Ingeborg", ""], ["R\u00f6ssler", "Wulf", ""], ["Held", "Leonhard", ""]]}, {"id": "1512.09206", "submitter": "Kevin Lee", "authors": "Kevin Lee and Lingzhou Xue", "title": "Nonparametric mixture of Gaussian graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical model has been widely used to investigate the complex dependence\nstructure of high-dimensional data, and it is common to assume that observed\ndata follow a homogeneous graphical model. However, observations usually come\nfrom different resources and have heterogeneous hidden commonality in\nreal-world applications. Thus, it is of great importance to estimate\nheterogeneous dependencies and discover subpopulation with certain commonality\nacross the whole population. In this work, we introduce a novel regularized\nestimation scheme for learning nonparametric mixture of Gaussian graphical\nmodels, which extends the methodology and applicability of Gaussian graphical\nmodels and mixture models. We propose a unified penalized likelihood approach\nto effectively estimate nonparametric functional parameters and heterogeneous\ngraphical parameters. We further design an efficient generalized effective EM\nalgorithm to address three significant challenges: high-dimensionality,\nnon-convexity, and label switching. Theoretically, we study both the\nalgorithmic convergence of our proposed algorithm and the asymptotic properties\nof our proposed estimators. Numerically, we demonstrate the performance of our\nmethod in simulation studies and a real application to estimate human brain\nfunctional connectivity from ADHD imaging data, where two heterogeneous\nconditional dependencies are explained through profiling demographic variables\nand supported by existing scientific findings.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 03:41:38 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Lee", "Kevin", ""], ["Xue", "Lingzhou", ""]]}, {"id": "1512.09244", "submitter": "Sebastian Lerch", "authors": "Sebastian Lerch, Thordis L. Thorarinsdottir, Francesco Ravazzolo,\n  Tilmann Gneiting", "title": "Forecaster's Dilemma: Extreme Events and Forecast Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In public discussions of the quality of forecasts, attention typically\nfocuses on the predictive performance in cases of extreme events. However, the\nrestriction of conventional forecast evaluation methods to subsets of extreme\nobservations has unexpected and undesired effects, and is bound to discredit\nskillful forecasts when the signal-to-noise ratio in the data generating\nprocess is low. Conditioning on outcomes is incompatible with the theoretical\nassumptions of established forecast evaluation methods, thereby confronting\nforecasters with what we refer to as the forecaster's dilemma. For\nprobabilistic forecasts, proper weighted scoring rules have been proposed as\ndecision theoretically justifiable alternatives for forecast evaluation with an\nemphasis on extreme events. Using theoretical arguments, simulation\nexperiments, and a real data study on probabilistic forecasts of U.S. inflation\nand gross domestic product growth, we illustrate and discuss the forecaster's\ndilemma along with potential remedies.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 09:27:14 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Lerch", "Sebastian", ""], ["Thorarinsdottir", "Thordis L.", ""], ["Ravazzolo", "Francesco", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1512.09325", "submitter": "Roberto Molinari Mr", "authors": "Stephane Guerrier and Roberto Molinari", "title": "Robust Inference for Time Series Models: a Wavelet-based Framework", "comments": "This paper has been withdrawn since it has become obsolete due to new\n  papers which correct and improve over many of the results in this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for the robust estimation of latent time series\nmodels which is fairly general and, for example, covers models going from ARMA\nto state-space models. This approach provides estimators which are (i)\nconsistent and asymptotically normally distributed, (ii) applicable to various\nclasses of time series models, (iii) straightforward to implement and (iv)\ncomputationally efficient. The framework is based on the recently developed\nGeneralized Method of Wavelet Moments (GMWM) and a new robust estimator of the\nwavelet variance. Compared to existing methods, the latter directly estimates\nthe quantity of interest while performing better in finite samples and using\nmilder conditions for its asymptotic properties to hold. Moreover, results are\ngiven showing the identifiability of the GMWM for various classes of time\nseries models thereby allowing this method to consistently estimate many models\n(and combinations thereof) under mild conditions. Hence, not only does this\npaper provide an alternative estimator which allows to perform wavelet variance\nanalysis when data are contaminated but also a general approach to robustly\nestimate the parameters of a variety of (latent) time series models. The\nsimulation studies carried out confirm the better performance of the proposed\nestimators and the usefulness and broadness of the proposed methodology is\nshown using practical examples from the domains of economics and engineering\nwith sample sizes up to 900,000.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 17:05:12 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2016 11:39:39 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2016 14:22:12 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Guerrier", "Stephane", ""], ["Molinari", "Roberto", ""]]}]