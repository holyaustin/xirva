[{"id": "1205.0309", "submitter": "Joshua Vogelstein", "authors": "Donniell E. Fishkind, Daniel L. Sussman, Minh Tang, Joshua T.\n  Vogelstein, Carey E. Priebe", "title": "Consistent adjacency-spectral partitioning for the stochastic block\n  model when the model parameters are unknown", "comments": "26 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For random graphs distributed according to a stochastic block model, we\nconsider the inferential task of partioning vertices into blocks using spectral\ntechniques. Spectral partioning using the normalized Laplacian and the\nadjacency matrix have both been shown to be consistent as the number of\nvertices tend to infinity. Importantly, both procedures require that the number\nof blocks and the rank of the communication probability matrix are known, even\nas the rest of the parameters may be unknown. In this article, we prove that\nthe (suitably modified) adjacency-spectral partitioning procedure, requiring\nonly an upper bound on the rank of the communication probability matrix, is\nconsistent. Indeed, this result demonstrates a robustness to model\nmis-specification; an overestimate of the rank may impose a moderate\nperformance penalty, but the procedure is still consistent. Furthermore, we\nextend this procedure to the setting where adjacencies may have multiple\nmodalities and we allow for either directed or undirected graphs.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 02:51:31 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2012 23:38:43 GMT"}], "update_date": "2012-08-23", "authors_parsed": [["Fishkind", "Donniell E.", ""], ["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1205.0310", "submitter": "James Scott", "authors": "Nicholas G. Polson, James G. Scott, Jesse Windle", "title": "Bayesian inference for logistic models using Polya-Gamma latent\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new data-augmentation strategy for fully Bayesian inference in\nmodels with binomial likelihoods. The approach appeals to a new class of\nPolya-Gamma distributions, which are constructed in detail. A variety of\nexamples are presented to show the versatility of the method, including\nlogistic regression, negative binomial regression, nonlinear mixed-effects\nmodels, and spatial models for count data. In each case, our data-augmentation\nstrategy leads to simple, effective methods for posterior inference that: (1)\ncircumvent the need for analytic approximations, numerical integration, or\nMetropolis-Hastings; and (2) outperform other known data-augmentation\nstrategies, both in ease of use and in computational efficiency. All methods,\nincluding an efficient sampler for the Polya-Gamma distribution, are\nimplemented in the R package BayesLogit.\n  In the technical supplement appended to the end of the paper, we provide\nfurther details regarding the generation of Polya-Gamma random variables; the\nempirical benchmarks reported in the main manuscript; and the extension of the\nbasic data-augmentation framework to contingency tables and multinomial\noutcomes.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 02:52:37 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 20:20:15 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2013 16:19:08 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Windle", "Jesse", ""]]}, {"id": "1205.0411", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic, Arthur Gretton, Bharath Sriperumbudur, Kenji Fukumizu", "title": "Hypothesis testing using pairwise distances and associated kernels (with\n  Appendix)", "comments": "Appearing in Proceedings of the 29th International Conference on\n  Machine Learning, Edinburgh, Scotland, UK, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a unifying framework linking two classes of statistics used in\ntwo-sample and independence testing: on the one hand, the energy distances and\ndistance covariances from the statistics literature; on the other, distances\nbetween embeddings of distributions to reproducing kernel Hilbert spaces\n(RKHS), as established in machine learning. The equivalence holds when energy\ndistances are computed with semimetrics of negative type, in which case a\nkernel may be defined such that the RKHS distance between distributions\ncorresponds exactly to the energy distance. We determine the class of\nprobability distributions for which kernels induced by semimetrics are\ncharacteristic (that is, for which embeddings of the distributions to an RKHS\nare injective). Finally, we investigate the performance of this family of\nkernels in two-sample and independence tests: we show in particular that the\nenergy distance most commonly employed in statistics is just one member of a\nparametric family of kernels, and that other choices from this family can yield\nmore powerful tests.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 12:49:19 GMT"}, {"version": "v2", "created": "Mon, 21 May 2012 23:29:06 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Sejdinovic", "Dino", ""], ["Gretton", "Arthur", ""], ["Sriperumbudur", "Bharath", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1205.0482", "submitter": "Luca Martino", "authors": "Luca Martino, David Luengo, Joaqu\\'in M\\'iguez", "title": "On the Generalized Ratio of Uniforms as a Combination of Transformed\n  Rejection and Extended Inverse of Density Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the relationship among three classical sampling\ntechniques: the inverse of density (Khintchine's theorem), the transformed\nrejection (TR) and the generalized ratio of uniforms (GRoU). Given a monotonic\nprobability density function (PDF), we show that the transformed area obtained\nusing the generalized ratio of uniforms method can be found equivalently by\napplying the transformed rejection sampling approach to the inverse function of\nthe target density. Then we provide an extension of the classical inverse of\ndensity idea, showing that it is completely equivalent to the GRoU method for\nmonotonic densities. Although we concentrate on monotonic probability density\nfunctions (PDFs), we also discuss how the results presented here can be\nextended to any non-monotonic PDF that can be decomposed into a collection of\nintervals where it is monotonically increasing or decreasing. In this general\ncase, we show the connections with transformations of certain random variables\nand the generalized inverse PDF with the GRoU technique. Finally, we also\nintroduce a GRoU technique to handle unbounded target densities.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 16:20:31 GMT"}, {"version": "v2", "created": "Mon, 7 May 2012 20:02:00 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2012 13:52:34 GMT"}, {"version": "v4", "created": "Sun, 22 Jul 2012 18:32:26 GMT"}, {"version": "v5", "created": "Sat, 4 Aug 2012 14:57:00 GMT"}, {"version": "v6", "created": "Sat, 11 Aug 2012 16:42:07 GMT"}, {"version": "v7", "created": "Tue, 16 Jul 2013 15:48:18 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Martino", "Luca", ""], ["Luengo", "David", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1205.0498", "submitter": "Vladimir Spokoiny", "authors": "Vladimir Spokoiny", "title": "Penalized maximum likelihood estimation and effective dimension", "comments": "arXiv admin note: text overlap with arXiv:1111.3029", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends some prominent statistical results including \\emph{Fisher\nTheorem and Wilks phenomenon} to the penalized maximum likelihood estimation\nwith a quadratic penalization. It appears that sharp expansions for the\npenalized MLE \\(\\tilde{\\thetav}_{G} \\) and for the penalized maximum likelihood\ncan be obtained without involving any asymptotic arguments, the results only\nrely on smoothness and regularity properties of the of the considered\nlog-likelihood function. The error of estimation is specified in terms of the\neffective dimension \\(p_G \\) of the parameter set which can be much smaller\nthan the true parameter dimension and even allows an infinite dimensional\nfunctional parameter. In the i.i.d. case, the Fisher expansion for the\npenalized MLE can be established under the constraint \"\\(p_G^{2}/n\\) is small\"\nwhile the remainder in the Wilks result is of order \\(p_G^{3}/n \\).\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 17:41:00 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2013 15:51:00 GMT"}, {"version": "v3", "created": "Fri, 25 Apr 2014 12:07:35 GMT"}, {"version": "v4", "created": "Mon, 10 Aug 2015 08:26:49 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Spokoiny", "Vladimir", ""]]}, {"id": "1205.0534", "submitter": "Elif Fidan  Acar PhD", "authors": "Elif F. Acar and Lei Sun", "title": "A Generalized Kruskal-Wallis Test Incorporating Group Uncertainty with\n  Application to Genetic Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by genetic association studies of SNPs with genotype uncertainty,\nwe propose a generalization of the Kruskal-Wallis test that incorporates group\nuncertainty when comparing k samples. The extended test statistic is based on\nprobability-weighted rank-sums and follows an asymptotic chi-square\ndistribution with k-1 degrees of freedom under the null hypothesis. Simulation\nstudies confirm the validity and robustness of the proposed test in finite\nsamples. Application to a genome-wide association study of type 1 diabetic\ncomplications further demonstrates the utilities of this generalized\nKruskal-Wallis test for studies with group uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 19:38:39 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Acar", "Elif F.", ""], ["Sun", "Lei", ""]]}, {"id": "1205.0739", "submitter": "Caroline Uhler", "authors": "Caroline Uhler, Aleksandra B. Slavkovic and Stephen E. Fienberg", "title": "Privacy-Preserving Data Sharing for Genome-Wide Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional statistical methods for confidentiality protection of statistical\ndatabases do not scale well to deal with GWAS (genome-wide association studies)\ndatabases especially in terms of guarantees regarding protection from linkage\nto external information. The more recent concept of differential privacy,\nintroduced by the cryptographic community, is an approach which provides a\nrigorous definition of privacy with meaningful privacy guarantees in the\npresence of arbitrary external information, although the guarantees come at a\nserious price in terms of data utility. Building on such notions, we propose\nnew methods to release aggregate GWAS data without compromising an individual's\nprivacy. We present methods for releasing differentially private minor allele\nfrequencies, chi-square statistics and p-values. We compare these approaches on\nsimulated data and on a GWAS study of canine hair length involving 685 dogs. We\nalso propose a privacy-preserving method for finding genome-wide associations\nbased on a differentially-private approach to penalized logistic regression.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2012 15:42:46 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Uhler", "Caroline", ""], ["Slavkovic", "Aleksandra B.", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1205.1774", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "Variance components and generalized Sobol' indices", "comments": "24 pages, 0 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces generalized Sobol' indices, compares strategies for\ntheir estimation, and makes a systematic search for efficient estimators. Of\nparticular interest are contrasts, sums of squares and indices of bilinear form\nwhich allow a reduced number of function evaluations compared to alternatives.\nThe bilinear framework includes some efficient estimators from Saltelli (2002)\nand Mauntz (2002) as well as some new estimators for specific variance\ncomponents and mean dimensions. This paper also provides a bias corrected\nversion of the estimator of Janon et al.\\,(2012) and extends the bias\ncorrection to generalized Sobol' indices. Some numerical comparisons are given.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 19:04:23 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "1205.1839", "submitter": "Abhra Sarkar", "authors": "Abhra Sarkar, Anindya Bhadra and Bani K. Mallick", "title": "Nonparametric Bayesian Approaches to Non-homogeneous Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article a flexible Bayesian non-parametric model is proposed for\nnon-homogeneous hidden Markov models. The model is developed through the\namalgamation of the ideas of hidden Markov models and predictor dependent\nstick-breaking processes. Computation is carried out using auxiliary variable\nrepresentation of the model which enable us to perform exact MCMC sampling from\nthe posterior. Furthermore, the model is extended to the situation when the\npredictors can simultaneously in influence the transition dynamics of the\nhidden states as well as the emission distribution. Estimates of few steps\nahead conditional predictive distributions of the response have been used as\nperformance diagnostics for these models. The proposed methodology is\nillustrated through simulation experiments as well as analysis of a real data\nset concerned with the prediction of rainfall induced malaria epidemics.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 21:59:39 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Sarkar", "Abhra", ""], ["Bhadra", "Anindya", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1205.1937", "submitter": "F Lau Mr", "authors": "F. Din-Houn Lau and Axel Gandy", "title": "Optimality of Non-Restarting CUSUM charts", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show optimality, in a well-defined sense, using cumulative sum (CUSUM)\ncharts for detecting changes in distributions. We consider a setting with\nmultiple changes between two known distributions. This result advocates the use\nof non-restarting CUSUM charts with an upper boundary. Typically, after\nsignalling, a CUSUM chart is restarted by setting it to some value below the\nthreshold. A non-restarting CUSUM chart is not reset after signalling; thus is\nable to signal continuously. Imposing an upper boundary prevents the CUSUM\nchart rising too high, which facilitates detection in our setting. We discuss,\nvia simulations, how the choice of the upper boundary changes the signals made\nby the non-restarting CUSUM charts.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 11:01:32 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Lau", "F. Din-Houn", ""], ["Gandy", "Axel", ""]]}, {"id": "1205.1971", "submitter": "Xin Lu", "authors": "Xin Lu", "title": "Linked Ego Networks: Improving Estimate Reliability and Validity with\n  Respondent-driven Sampling", "comments": "22 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is currently widely used for the study of\nHIV/AIDS-related high risk populations. However, recent studies have shown that\ntraditional RDS methods are likely to generate large variances and may be\nseverely biased since the assumptions behind RDS are seldom fully met in real\nlife. To improve estimation in RDS studies, we propose a new method to generate\nestimates with ego network data, which is collected by asking RDS respondents\nabout the composition of their personal networks, such as \"what proportion of\nyour friends are married?\". By simulations on an extracted real-world social\nnetwork of gay men as well as on artificial networks with varying structural\nproperties, we show that the new estimator, RDSI^{ego} shows superior\nperformance over traditional RDS estimators. Importantly, RDSI^{ego} exhibits\nstrong robustness to the preference of peer recruitment and variations in\nnetwork structural properties, such as homophily, activity ratio, and community\nstructure. While the biases of traditional RDS estimators can sometimes be as\nlarge as 10%~20%, biases of all RDSI^{ego} estimates are well restrained to be\nless than 2%. The positive results henceforth encourage researchers to collect\nego network data for variables of interests by RDS, for both hard-to-access\npopulations and general populations when random sampling is not applicable. The\nlimitation of RDSI^{ego} is evaluated by simulating RDS assuming different\nlevel of reporting error.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 13:07:47 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2012 19:16:02 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Lu", "Xin", ""]]}, {"id": "1205.2106", "submitter": "Lingsong Zhang Lingsong Zhang", "authors": "Lingsong Zhang and Zhengyuan Zhu", "title": "Spatial Multiresolution Cluster Detection Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel multi-resolution cluster detection (MCD) method is proposed to\nidentify irregularly shaped clusters in space. Multi-scale test statistic on a\nsingle cell is derived based on likelihood ratio statistic for Bernoulli\nsequence, Poisson sequence and Normal sequence. A neighborhood variability\nmeasure is defined to select the optimal test threshold. The MCD method is\ncompared with single scale testing methods controlling for false discovery rate\nand the spatial scan statistics using simulation and f-MRI data. The MCD method\nis shown to be more effective for discovering irregularly shaped clusters, and\nthe implementation of this method does not require heavy computation, making it\nsuitable for cluster detection for large spatial data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 21:15:08 GMT"}], "update_date": "2012-05-11", "authors_parsed": [["Zhang", "Lingsong", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1205.2417", "submitter": "Giovanni Montana", "authors": "Christopher Minas and Giovanni Montana", "title": "Distance-based analysis of variance: approximate inference and an\n  application to genome-wide association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several modern applications, ranging from genetics to genomics and\nneuroimaging, there is a need to compare observations across different\npopulations, such as groups of healthy and diseased individuals. The interest\nis in detecting a group effect. When the observations are vectorial,\nreal-valued and follow a multivariate Normal distribution, multivariate\nanalysis of variance (MANOVA) tests are routinely applied. However, such\ntraditional procedures are not suitable when dealing with more complex data\nstructures such as functional (e.g. curves) or graph-structured (e.g. trees and\nnetworks) objects, where the required distributional assumptions may be\nviolated. In this paper we discuss a distance-based MANOVA-like approach, the\nDBF test, for detecting differences between groups for a wider range of data\ntypes. The test statistic, analogously to other distance-based statistics, only\nrelies on a suitably chosen distance measure that captures the pairwise\ndissimilarity among all available samples. An approximate null probability\ndistribution of the DBF statistic is proposed thus allowing inferences to be\ndrawn without the need for costly permutation procedures. Through extensive\nsimulations we provide evidence that the proposed methodology works well for a\nrange of data types and distances, and generalizes the traditional MANOVA\ntests. We also report on an application of the proposed methodology for the\nanalysis of a multi-locus genome-wide association study of Alzheimer's disease,\nwhich has been carried out using several genetic distance measures.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2012 01:59:28 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Minas", "Christopher", ""], ["Montana", "Giovanni", ""]]}, {"id": "1205.2615", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Judea Pearl", "title": "Effects of Treatment on the Treated: Identification and Generalization", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-514-521", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of causal analysis call for assessing, retrospectively, the\neffect of withholding an action that has in fact been implemented. This\ncounterfactual quantity, sometimes called \"effect of treatment on the treated,\"\n(ETT) have been used to to evaluate educational programs, critic public\npolicies, and justify individual decision making. In this paper we explore the\nconditions under which ETT can be estimated from (i.e., identified in)\nexperimental and/or observational studies. We show that, when the action\ninvokes a singleton variable, the conditions for ETT identification have simple\ncharacterizations in terms of causal diagrams. We further give a graphical\ncharacterization of the conditions under which the effects of multiple\ntreatments on the treated can be identified, as well as ways in which the ETT\nestimand can be constructed from both interventional and observational\ndistributions.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:29:08 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Shpitser", "Ilya", ""], ["Pearl", "Judea", ""]]}, {"id": "1205.2617", "submitter": "Mark Schmidt", "authors": "Mark Schmidt, Kevin Murphy", "title": "Modeling Discrete Interventional Data using Directed Cyclic Graphical\n  Models", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-487-495", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a representation for discrete multivariate distributions in terms\nof interventional potential functions that are globally normalized. This\nrepresentation can be used to model the effects of interventions, and the\nindependence properties encoded in this model can be represented as a directed\ngraph that allows cycles. In addition to discussing inference and sampling with\nthis representation, we give an exponential family parametrization that allows\nparameter estimation to be stated as a convex optimization problem; we also\ngive a convex relaxation of the task of simultaneous parameter and structure\nlearning using group l1-regularization. The model is evaluated on simulated\ndata and intracellular flow cytometry data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:26:23 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Schmidt", "Mark", ""], ["Murphy", "Kevin", ""]]}, {"id": "1205.2641", "submitter": "Patrik O. Hoyer", "authors": "Patrik O. Hoyer, Antti Hyttinen", "title": "Bayesian Discovery of Linear Acyclic Causal Models", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-240-248", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for automated discovery of causal relationships from\nnon-interventional data have received much attention recently. A widely used\nand well understood model family is given by linear acyclic causal models\n(recursive structural equation models). For Gaussian data both constraint-based\nmethods (Spirtes et al., 1993; Pearl, 2000) (which output a single equivalence\nclass) and Bayesian score-based methods (Geiger and Heckerman, 1994) (which\nassign relative scores to the equivalence classes) are available. On the\ncontrary, all current methods able to utilize non-Gaussianity in the data\n(Shimizu et al., 2006; Hoyer et al., 2008) always return only a single graph or\na single equivalence class, and so are fundamentally unable to express the\ndegree of certainty attached to that output. In this paper we develop a\nBayesian score-based approach able to take advantage of non-Gaussianity when\nestimating linear acyclic causal models, and we empirically demonstrate that,\nat least on very modest size networks, its accuracy is as good as or better\nthan existing methods. We provide a complete code package (in R) which\nimplements all algorithms and performs all of the analysis provided in the\npaper, and hope that this will further the application of these methods to\nsolving causal inference problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:30:07 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Hoyer", "Patrik O.", ""], ["Hyttinen", "Antti", ""]]}, {"id": "1205.2777", "submitter": "Antonino Abbruzzo AA", "authors": "Antonino Abbruzzo and Ernst Wit", "title": "Modelling slowly changing dynamic gene-regulatory networks", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic gene-regulatory networks are complex since the number of potential\ncomponents involved in the system is very large. Estimating dynamic networks is\nan important task because they compromise valuable information about\ninteractions among genes. Graphical models are a powerful class of models to\nestimate conditional independence among random variables, e.g. interactions in\ndynamic systems. Indeed, these interactions tend to vary over time. However,\nthe literature has been focused on static networks, which can only reveal\noverall structures. Time-course experiments are performed in order to tease out\nsignificant changes in networks. It is typically reasonable to assume that\nchanges in genomic networks are few because systems in biology tend to be\nstable. We introduce a new model for estimating slowly changes in dynamic\ngene-regulatory networks which is suitable for a high-dimensional dataset, e.g.\ntime-course genomic data. Our method is based on i) the penalized likelihood\nwith $\\ell_1$-norm, ii) the penalized differences between conditional\nindependence elements across time points and iii) the heuristic search strategy\nto find optimal smoothing parameters. We implement a set of linear constraints\nnecessary to estimate sparse graphs and penalized changing in dynamic networks.\nThese constraints are not in the linear form. For this reason, we introduce\nslack variables to re-write our problem into a standard convex optimization\nproblem subject to equality linear constraints. We show that GL$_\\Delta$\nperforms well in a simulation study. Finally, we apply the proposed model to a\ntime-course genetic dataset T-cell.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2012 12:36:09 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Abbruzzo", "Antonino", ""], ["Wit", "Ernst", ""]]}, {"id": "1205.2816", "submitter": "Tsuyoshi Kunihama", "authors": "Tsuyoshi Kunihama and David B. Dunson", "title": "Bayesian modeling of temporal dependence in large sparse contingency\n  tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, it is of interest to study trends over time in\nrelationships among categorical variables, such as age group, ethnicity,\nreligious affiliation, political party and preference for particular policies.\nAt each time point, a sample of individuals provide responses to a set of\nquestions, with different individuals sampled at each time. In such settings,\nthere tends to be abundant missing data and the variables being measured may\nchange over time. At each time point, one obtains a large sparse contingency\ntable, with the number of cells often much larger than the number of\nindividuals being surveyed. To borrow information across time in modeling large\nsparse contingency tables, we propose a Bayesian autoregressive tensor\nfactorization approach. The proposed model relies on a probabilistic Parafac\nfactorization of the joint pmf characterizing the categorical data distribution\nat each time point, with autocorrelation included across times. Efficient\ncomputational methods are developed relying on MCMC. The methods are evaluated\nthrough simulation examples and applied to social survey data.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2012 21:45:54 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Kunihama", "Tsuyoshi", ""], ["Dunson", "David B.", ""]]}, {"id": "1205.2903", "submitter": "Nickos Papadatos D", "authors": "Giorgos Afendras and Nickos Papadatos", "title": "Integrated Pearson family and orthogonality of the Rodrigues\n  polynomials: A review including new results and an alternative classification\n  of the Pearson system", "comments": "Dedicated to Professor V. Papathanasiou. 45 pages, including a Table\n  with all integrating Pearson densities (Table 2.1 on p.8)", "journal-ref": "Applicationes Mathematicae (2015), vol. 42(2-3), pp. 231-267", "doi": "10.4064/am42-2-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An alternative classification of the Pearson family of probability densities\nis related to the orthogonality of the corresponding Rodrigues polynomials.\nThis leads to a subset of the ordinary Pearson system, the Integrated Pearson\nFamily. Basic properties of this family are discussed and reviewed, and some\nnew results are presented. A detailed comparison between the integrated Pearson\nfamily and the ordinary Pearson system is presented, including an algorithm\nthat enables to decide whether a given Pearson density belongs to the\nintegrated system, or not. Recurrences between the derivatives of the\ncorresponding orthonormal polynomial systems are also given.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2012 19:16:18 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2012 10:35:07 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Afendras", "Giorgos", ""], ["Papadatos", "Nickos", ""]]}, {"id": "1205.2911", "submitter": "Antonino Abbruzzo AA", "authors": "E. C. Wit, A. Abbruzzo", "title": "Factorial graphical lasso for dynamic networks", "comments": "30 pp, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks models describe a growing number of important scientific\nprocesses, from cell biology and epidemiology to sociology and finance. There\nare many aspects of dynamical networks that require statistical considerations.\nIn this paper we focus on determining network structure. Estimating dynamic\nnetworks is a difficult task since the number of components involved in the\nsystem is very large. As a result, the number of parameters to be estimated is\nbigger than the number of observations. However, a characteristic of many\nnetworks is that they are sparse. For example, the molecular structure of genes\nmake interactions with other components a highly-structured and therefore\nsparse process.\n  Penalized Gaussian graphical models have been used to estimate sparse\nnetworks. However, the literature has focussed on static networks, which lack\nspecific temporal constraints. We propose a structured Gaussian dynamical\ngraphical model, where structures can consist of specific time dynamics, known\npresence or absence of links and block equality constraints on the parameters.\nThus, the number of parameters to be estimated is reduced and accuracy of the\nestimates, including the identification of the network, can be tuned up. Here,\nwe show that the constrained optimization problem can be solved by taking\nadvantage of an efficient solver, logdetPPA, developed in convex optimization.\nMoreover, model selection methods for checking the sensitivity of the inferred\nnetworks are described. Finally, synthetic and real data illustrate the\nproposed methodologies.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2012 22:02:28 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Wit", "E. C.", ""], ["Abbruzzo", "A.", ""]]}, {"id": "1205.3009", "submitter": "Alicia L. Carriquiry", "authors": "Alicia L. Carriquiry", "title": "Election Forensics and the 2004 Venezuelan Presidential Recall\n  Referendum as a Case Study", "comments": "Published in at http://dx.doi.org/10.1214/11-STS379 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 471-478", "doi": "10.1214/11-STS379", "report-no": "IMS-STS-STS379", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A referendum to recall President Hugo Ch\\'{a}vez was held in Venezuela in\nAugust of 2004. In the referendum, voters were to vote YES if they wished to\nrecall the President and NO if they wanted him to continue in office. The\nofficial results were 59% NO and 41% YES. Even though the election was\nmonitored by various international groups including the Organization of\nAmerican States and the Carter Center (both of which declared that the\nreferendum had been conducted in a free and transparent manner), the outcome of\nthe election was questioned by other groups both inside and outside of\nVenezuela. The collection of manuscripts that comprise this issue of\nStatistical Science discusses the general topic of election forensics but also\nfocuses on different statistical approaches to explore, post-election, whether\nirregularities in the voting, vote transmission or vote counting processes\ncould be detected in the 2004 presidential recall referendum. In this\nintroduction to the Venezuela issue, we discuss the more recent literature on\npost-election auditing, describe the institutional context for the 2004\nVenezuelan referendum, and briefly introduce each of the five contributions.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 13:01:17 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Carriquiry", "Alicia L.", ""]]}, {"id": "1205.3217", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Stephen E. Fienberg", "title": "A Generalized Fellegi-Sunter Framework for Multiple Record Linkage With\n  Application to Homicide Record Systems", "comments": "Several changes with respect to previous version. Accepted in the\n  Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic method for linking multiple datafiles. This task\nis not trivial in the absence of unique identifiers for the individuals\nrecorded. This is a common scenario when linking census data to coverage\nmeasurement surveys for census coverage evaluation, and in general when\nmultiple record-systems need to be integrated for posterior analysis. Our\nmethod generalizes the Fellegi-Sunter theory for linking records from two\ndatafiles and its modern implementations. The multiple record linkage goal is\nto classify the record K-tuples coming from K datafiles according to the\ndifferent matching patterns. Our method incorporates the transitivity of\nagreement in the computation of the data used to model matching probabilities.\nWe use a mixture model to fit matching probabilities via maximum likelihood\nusing the EM algorithm. We present a method to decide the record K-tuples\nmembership to the subsets of matching patterns and we prove its optimality. We\napply our method to the integration of three Colombian homicide record systems\nand we perform a simulation study in order to explore the performance of the\nmethod under measurement error and different scenarios. The proposed method\nworks well and opens some directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 23:01:28 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 18:58:00 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1205.3290", "submitter": "Luis Pericchi", "authors": "Luis Pericchi, David Torres", "title": "Quick Anomaly Detection by the Newcomb--Benford Law, with Applications\n  to Electoral Processes Data from the USA, Puerto Rico and Venezuela", "comments": "Published in at http://dx.doi.org/10.1214/09-STS296 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 502-516", "doi": "10.1214/09-STS296", "report-no": "IMS-STS-STS296", "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple and quick general test to screen for numerical anomalies is\npresented. It can be applied, for example, to electoral processes, both\nelectronic and manual. It uses vote counts in officially published voting\nunits, which are typically widely available and institutionally backed. The\ntest examines the frequencies of digits on voting counts and rests on the First\n(NBL1) and Second Digit Newcomb--Benford Law (NBL2), and in a novel\ngeneralization of the law under restrictions of the maximum number of voters\nper unit (RNBL2). We apply the test to the 2004 USA presidential elections, the\nPuerto Rico (1996, 2000 and 2004) governor elections, the 2004 Venezuelan\npresidential recall referendum (RRP) and the previous 2000 Venezuelan\nPresidential election. The NBL2 is compellingly rejected only in the Venezuelan\nreferendum and only for electronic voting units. Our original suggestion on the\nRRP (Pericchi and Torres, 2004) was criticized by The Carter Center report\n(2005). Acknowledging this, Mebane (2006) and The Economist (US) (2007)\npresented voting models and case studies in favor of NBL2. Further evidence is\npresented here. Moreover, under the RNBL2, Mebane's voting models are valid\nunder wider conditions. The adequacy of the law is assessed through Bayes\nFactors (and corrections of $p$-values) instead of significance testing, since\nfor large sample sizes and fixed $\\alpha$ levels the null hypothesis is over\nrejected. Our tests are extremely simple and can become a standard screening\nthat a fair electoral process should pass.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2012 08:35:27 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Pericchi", "Luis", ""], ["Torres", "David", ""]]}, {"id": "1205.3320", "submitter": "Raquel Prado", "authors": "Raquel Prado, Bruno Sans\\'o", "title": "The 2004 Venezuelan Presidential Recall Referendum: Discrepancies\n  Between Two Exit Polls and Official Results", "comments": "Published in at http://dx.doi.org/10.1214/09-STS295 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 517-527", "doi": "10.1214/09-STS295", "report-no": "IMS-STS-STS295", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simulation-based study in which the results of two major exit\npolls conducted during the recall referendum that took place in Venezuela on\nAugust 15, 2004, are compared to the official results of the Venezuelan\nNational Electoral Council \"Consejo Nacional Electoral\" (CNE). The two exit\npolls considered here were conducted independently by S\\'{u}mate, a\nnongovernmental organization, and Primero Justicia, a political party. We find\nsignificant discrepancies between the exit poll data and the official CNE\nresults in about 60% of the voting centers that were sampled in these polls. We\nshow that discrepancies between exit polls and official results are not due to\na biased selection of the voting centers or to problems related to the size of\nthe samples taken at each center. We found discrepancies in all the states\nwhere the polls were conducted. We do not have enough information on the exit\npoll data to determine whether the observed discrepancies are the consequence\nof systematic biases in the selection of the people interviewed by the\npollsters around the country. Neither do we have information to study the\npossibility of a high number of false or nonrespondents. We have limited data\nsuggesting that the discrepancies are not due to a drastic change in the voting\npatterns that occurred after the exit polls were conducted. We notice that the\ntwo exit polls were done independently and had few centers in common, yet their\noverall results were very similar.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2012 10:56:59 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Prado", "Raquel", ""], ["Sans\u00f3", "Bruno", ""]]}, {"id": "1205.3564", "submitter": "Isbelia Mart\\'{i}n", "authors": "Isbelia Mart\\'in", "title": "2004 Venezuelan Presidential Recall Referendum (2004 PRR): A Statistical\n  Analysis from the Point of View of Electronic Voting Data Transmissions", "comments": "Published in at http://dx.doi.org/10.1214/10-STS350 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 528-542", "doi": "10.1214/10-STS350", "report-no": "IMS-STS-STS350", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical comparisons of electoral variables are made between groups of\nelectronic voting machines and voting centers classified by types of\ntransmissions according to the volume of traffic in incoming and outgoing data\nof machines from and toward the National Electoral Council (CNE) totalizing\nservers. One unexpectedly finds two types of behavior in wire telephony data\ntransmissions and only one type where cellular telephony is employed,\ncontravening any reasonable electoral normative. Differentiation in data\ntransmissions arise when comparing number of incoming and outgoing data bytes\nper machine against total number of votes per machine reported officially by\nthe CNE. The respective distributions of electoral variables for each type of\ntransmission show that the groups classified by it do not correspond to random\nsets of the electoral universe. In particular, the distributions for the NO\npercentage of votes per machine differ statistically across groups. The\npresidential elections of 1998, 2000 and the 2004 Presidential Recall\nReferendum (2004 PRR) are compared according to the type of transmissions in\n2004 PRR. Statistically, the difference between the empirical distributions of\nthe 2004 PRR NO results and the 2000 Chavez votes results by voting centers is\nnot significant.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2012 06:34:41 GMT"}], "update_date": "2012-05-17", "authors_parsed": [["Mart\u00edn", "Isbelia", ""]]}, {"id": "1205.3645", "submitter": "Ra\\'{u}l Jim\\'{e}nez", "authors": "Ra\\'ul Jim\\'enez", "title": "Forensic Analysis of the Venezuelan Recall Referendum", "comments": "Published in at http://dx.doi.org/10.1214/11-STS375 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 564-583", "doi": "10.1214/11-STS375", "report-no": "IMS-STS-STS375", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best way to reconcile political actors in a controversial electoral\nprocess is a full audit. When this is not possible, statistical tools may be\nuseful for measuring the likelihood of the results. The Venezuelan recall\nreferendum (2004) provides a suitable dataset for thinking about this important\nproblem. The cost of errors in examining an allegation of electoral fraud can\nbe enormous. They can range from legitimizing an unfair election to supporting\nan unfounded accusation, with serious political implications. For this reason,\nwe must be very selective about data, hypotheses and test statistics that will\nbe used. This article offers a critical review of recent statistical literature\non the Venezuelan referendum. In addition, we propose a testing methodology,\nbased exclusively on vote counting, that is potentially useful in election\nforensics. The referendum is reexamined, offering new and intriguing aspects to\nprevious analyses. The main conclusion is that there were a significant number\nof irregularities in the vote counting that introduced a bias in favor of the\nwinning option. A plausible scenario in which the irregularities could overturn\nthe results is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2012 12:05:20 GMT"}], "update_date": "2012-05-17", "authors_parsed": [["Jim\u00e9nez", "Ra\u00fal", ""]]}, {"id": "1205.3845", "submitter": "Luke Bornn", "authors": "Luke Bornn, Marian Anghel, Ingo Steinwart", "title": "Forecasting with Historical Data or Process Knowledge under\n  Misspecification: A Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with the task of forecasting a dynamic system, practitioners often\nhave available historical data, knowledge of the system, or a combination of\nboth. While intuition dictates that perfect knowledge of the system should in\ntheory yield perfect forecasting, often knowledge of the system is only\npartially known, known up to parameters, or known incorrectly. In contrast,\nforecasting using previous data without any process knowledge might result in\naccurate prediction for simple systems, but will fail for highly nonlinear and\nchaotic systems. In this paper, the authors demonstrate how even in chaotic\nsystems, forecasting with historical data is preferable to using process\nknowledge if this knowledge exhibits certain forms of misspecification. Through\nan extensive simulation study, a range of misspecification and forecasting\nscenarios are examined with the goal of gaining an improved understanding of\nthe circumstances under which forecasting from historical data is to be\npreferred over using process knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2012 04:06:29 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Bornn", "Luke", ""], ["Anghel", "Marian", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1205.3906", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan, David J. Nott", "title": "Variational Inference for Generalized Linear Mixed Models Using\n  Partially Noncentered Parametrizations", "comments": "Published in at http://dx.doi.org/10.1214/13-STS418 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 168-188", "doi": "10.1214/13-STS418", "report-no": "IMS-STS-STS418", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effects of different parametrizations on the convergence of Bayesian\ncomputational algorithms for hierarchical models are well explored. Techniques\nsuch as centering, noncentering and partial noncentering can be used to\naccelerate convergence in MCMC and EM algorithms but are still not well studied\nfor variational Bayes (VB) methods. As a fast deterministic approach to\nposterior approximation, VB is attracting increasing interest due to its\nsuitability for large high-dimensional data. Use of different parametrizations\nfor VB has not only computational but also statistical implications, as\ndifferent parametrizations are associated with different factorized posterior\napproximations. We examine the use of partially noncentered parametrizations in\nVB for generalized linear mixed models (GLMMs). Our paper makes four\ncontributions. First, we show how to implement an algorithm called nonconjugate\nvariational message passing for GLMMs. Second, we show that the partially\nnoncentered parametrization can adapt to the quantity of information in the\ndata and determine a parametrization close to optimal. Third, we show that\npartial noncentering can accelerate convergence and produce more accurate\nposterior approximations than centering or noncentering. Finally, we\ndemonstrate how the variational lower bound, produced as part of the\ncomputation, can be useful for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2012 11:17:08 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 02:28:54 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2013 08:09:12 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Nott", "David J.", ""]]}, {"id": "1205.3918", "submitter": "Adrian Baddeley", "authors": "Adrian Baddeley, Ege Rubak, Jesper M{\\o}ller", "title": "Score, Pseudo-Score and Residual Diagnostics for Spatial Point Process\n  Models", "comments": "Published in at http://dx.doi.org/10.1214/11-STS367 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 613-646", "doi": "10.1214/11-STS367", "report-no": "IMS-STS-STS367", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new tools for formal inference and informal model validation in\nthe analysis of spatial point pattern data. The score test is generalized to a\n\"pseudo-score\" test derived from Besag's pseudo-likelihood, and to a class of\ndiagnostics based on point process residuals. The results lend theoretical\nsupport to the established practice of using functional summary statistics,\nsuch as Ripley's $K$-function, when testing for complete spatial randomness;\nand they provide new tools such as the compensator of the $K$-function for\ntesting other fitted models. The results also support localization methods such\nas the scan statistic and smoothed residual plots. Software for computing the\ndiagnostics is provided.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2012 12:43:36 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Baddeley", "Adrian", ""], ["Rubak", "Ege", ""], ["M\u00f8ller", "Jesper", ""]]}, {"id": "1205.3921", "submitter": "Roberto Rigobon", "authors": "Ricardo Hausmann, Roberto Rigobon", "title": "In Search of the Black Swan: Analysis of the Statistical Evidence of\n  Electoral Fraud in Venezuela", "comments": "Published in at http://dx.doi.org/10.1214/11-STS373 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 543-563", "doi": "10.1214/11-STS373", "report-no": "IMS-STS-STS373", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study analyzes diverse hypotheses of electronic fraud in the Recall\nReferendum celebrated in Venezuela on August 15, 2004. We define fraud as the\ndifference between the elector's intent, and the official vote tally. Our null\nhypothesis is that there was no fraud, and we attempt to search for evidence\nthat will allow us to reject this hypothesis. We find no evidence that fraud\nwas committed by applying numerical maximums to machines in some precincts.\nEqually, we discard any hypothesis that implies altering some machines and not\nothers, at each electoral precinct, because the variation patterns between\nmachines at each precinct are normal. However, the statistical evidence is\ncompatible with the occurrence of fraud that has affected every machine in a\nsingle precinct, but differentially more in some precincts than others. We find\nthat the deviation pattern between precincts, based on the relationship between\nthe signatures collected to request the referendum in November 2003 (the\nso-called, Reafirmazo), and the YES votes on August 15, is positive and\nsignificantly correlated with the deviation pattern in the relationship between\nexit polls and votes in those same precincts. In other words, those precincts\nin which, according to the number of signatures, there are an unusually low\nnumber of YES votes (i.e., votes to impeach the president), is also where,\naccording to the exit polls, the same thing occurs.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2012 12:52:40 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Hausmann", "Ricardo", ""], ["Rigobon", "Roberto", ""]]}, {"id": "1205.3989", "submitter": "Anna Varvak", "authors": "Anna Varvak", "title": "Mirror bootstrap method for testing hypotheses of one mean", "comments": "9 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general philosophy for bootstrap or permutation methods for testing\nhypotheses is to simulate the variation of the test statistic by generating the\nsampling distribution which assumes both that the null hypothesis is true, and\nthat the data in the sample is somehow representative of the population. This\nphilosophy is inapplicable for testing hypotheses for a single parameter like\nthe population mean, since the two assumptions are contradictory (e.g., how can\nwe assume both that the mean of the population is $\\mu_0,$ and that the\nindividuals in the sample with a mean $M \\ne \\mu_0$ are representative of the\npopulation?). The Mirror Bootstrap resolves that conundrum. The philosophy of\nthe Mirror Bootstrap method for testing hypotheses regarding one population\nparameter is that we assume both that the null hypothesis is true, and that the\nindividuals in our sample are as representative as they could be without\nassuming more extreme cases than observed. For example, the Mirror Bootstrap\nmethod for testing hypotheses of one mean uses a generated symmetric\ndistribution constructed by reflecting the original sample around the\nhypothesized population mean $\\mu_0$. Simulations of the performance of the\nMirror Bootstrap for testing hypotheses of one mean show that, while the method\nis slightly on the conservative side for very small samples, its validity and\npower quickly approach that of the widely used t-test. The philosophy of the\nMirror Bootstrap is sufficiently general to be adapted for testing hypotheses\nabout other parameters; this exploration is left for future research.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2012 17:33:00 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Varvak", "Anna", ""]]}, {"id": "1205.4079", "submitter": "Lei Sun", "authors": "Andriy Derkach, Jerry F. Lawless, Lei Sun", "title": "Pooled Association Tests for Rare Genetic Variants: A Review and Some\n  New Results", "comments": "Published in at http://dx.doi.org/10.1214/13-STS456 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 302-321", "doi": "10.1214/13-STS456", "report-no": "IMS-STS-STS456", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the search for genetic factors that are associated with complex heritable\nhuman traits, considerable attention is now being focused on rare variants that\nindividually have small effects. In response, numerous recent papers have\nproposed testing strategies to assess association between a group of rare\nvariants and a trait, with competing claims about the performance of various\ntests. The power of a given test in fact depends on the nature of any\nassociation and on the rareness of the variants in question. We review such\ntests within a general framework that covers a wide range of genetic models and\ntypes of data. We study the performance of specific tests through exact or\nasymptotic power formulas and through novel simulation studies of over 10,000\ndifferent models. The tests considered are also applied to real sequence data\nfrom the 1000 Genomes project and provided by the GAW17. We recommend a testing\nstrategy, but our results show that power to detect association in plausible\ngenetic scenarios is low for studies of medium size unless a high proportion of\nthe chosen variants are causal. Consequently, considerable attention must be\ngiven to relevant biological information that can guide the selection of\nvariants for testing.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 05:30:12 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 11:09:15 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Derkach", "Andriy", ""], ["Lawless", "Jerry F.", ""], ["Sun", "Lei", ""]]}, {"id": "1205.4163", "submitter": "Erin Schliep", "authors": "Erin M. Schliep and Jennifer A. Hoeting", "title": "Multilevel latent Gaussian process model for mixed discrete and\n  continuous multivariate response data", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": "10.1007/s13253-013-0136", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian model for mixed ordinal and continuous multivariate\ndata to evaluate a latent spatial Gaussian process. Our proposed model can be\nused in many contexts where mixed continuous and discrete multivariate\nresponses are observed in an effort to quantify an unobservable continuous\nmeasurement. In our example, the latent, or unobservable measurement is wetland\ncondition. While predicted values of the latent wetland condition variable\nproduced by the model at each location do not hold any intrinsic value, the\nrelative magnitudes of the wetland condition values are of interest. In\naddition, by including point-referenced covariates in the model, we are able to\nmake predictions at new locations for both the latent random variable and the\nmultivariate response. Lastly, the model produces ranks of the multivariate\nresponses in relation to the unobserved latent random field. This is an\nimportant result as it allows us to determine which response variables are most\nclosely correlated with the latent variable. Our approach offers an alternative\nto traditional indices based on best professional judgment that are frequently\nused in ecology. We apply our model to assess wetland condition in the North\nPlatte and Rio Grande River Basins in Colorado. The model facilitates a\ncomparison of wetland condition at multiple locations and ranks the importance\nof in-field measurements.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 14:15:50 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2013 19:37:42 GMT"}, {"version": "v3", "created": "Sat, 23 Mar 2013 16:05:34 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Schliep", "Erin M.", ""], ["Hoeting", "Jennifer A.", ""]]}, {"id": "1205.4174", "submitter": "Alain Hauser", "authors": "Alain Hauser and Peter B\\\"uhlmann", "title": "Two Optimal Strategies for Active Learning of Causal Models from\n  Interventional Data", "comments": null, "journal-ref": "International Journal of Approximate Reasoning, Volume 55, Issue\n  4, June 2014, Pages 926-939", "doi": "10.1016/j.ijar.2013.11.007", "report-no": null, "categories": "stat.ME cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From observational data alone, a causal DAG is only identifiable up to Markov\nequivalence. Interventional data generally improves identifiability; however,\nthe gain of an intervention strongly depends on the intervention target, that\nis, the intervened variables. We present active learning (that is, optimal\nexperimental design) strategies calculating optimal interventions for two\ndifferent learning goals. The first one is a greedy approach using\nsingle-vertex interventions that maximizes the number of edges that can be\noriented after each intervention. The second one yields in polynomial time a\nminimum set of targets of arbitrary size that guarantees full identifiability.\nThis second approach proves a conjecture of Eberhardt (2008) indicating the\nnumber of unbounded intervention targets which is sufficient and in the worst\ncase necessary for full identifiability. In a simulation study, we compare our\ntwo active learning approaches to random interventions and an existing\napproach, and analyze the influence of estimation errors on the overall\nperformance of active learning.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 15:12:01 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2012 11:27:30 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2013 13:42:56 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Hauser", "Alain", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1205.4181", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu, Vladislav B. Tadi\\'c, Matti Vihola", "title": "On the stability of some controlled Markov chains and its applications\n  to stochastic approximation with Markovian dynamic", "comments": "Published in at http://dx.doi.org/10.1214/13-AAP953 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 1, 1-45", "doi": "10.1214/13-AAP953", "report-no": "IMS-AAP-AAP953", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a practical approach to establish the stability, that is, the\nrecurrence in a given set, of a large class of controlled Markov chains. These\nprocesses arise in various areas of applied science and encompass important\nnumerical methods. We show in particular how individual Lyapunov functions and\nassociated drift conditions for the parametrized family of Markov transition\nprobabilities and the parameter update can be combined to form Lyapunov\nfunctions for the joint process, leading to the proof of the desired stability\nproperty. Of particular interest is the fact that the approach applies even in\nsituations where the two components of the process present a time-scale\nseparation, which is a crucial feature of practical situations. We then move on\nto show how such a recurrence property can be used in the context of stochastic\napproximation in order to prove the convergence of the parameter sequence,\nincluding in the situation where the so-called stepsize is adaptively tuned. We\nfinally show that the results apply to various algorithms of interest in\ncomputational statistics and cognate areas.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 15:25:55 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 14:13:11 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 09:28:17 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Andrieu", "Christophe", ""], ["Tadi\u0107", "Vladislav B.", ""], ["Vihola", "Matti", ""]]}, {"id": "1205.4306", "submitter": "Lorentz Jantschi", "authors": "Lorentz J\\\"antschi", "title": "Distribution fitting 16. How many colors are actually in the field?", "comments": "5 pages; 3 tables; 1 figure; prepared for Prospectus for the 3rd\n  Millennium Agriculture symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A study to compare different methods of estimation was conducted. The goal\nwas to provide an estimate for the number of petal colors existing in the field\nby using a random sample of Lycoris longituba flowers taken from the field. The\nstudy revealed that the estimation from observed sample agrees very well with\nthird order Jackknife method of estimation.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2012 08:38:32 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["J\u00e4ntschi", "Lorentz", ""]]}, {"id": "1205.4471", "submitter": "Zhilin Zhang", "authors": "Bhaskar D. Rao, Zhilin Zhang, Yuzhe Jin", "title": "Sparse Signal Recovery in the Presence of Intra-Vector and Inter-Vector\n  Correlation", "comments": "Invited review paper of 2012 International Conference on Signal\n  Processing and Communications (SPCOM 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work discusses the problem of sparse signal recovery when there is\ncorrelation among the values of non-zero entries. We examine intra-vector\ncorrelation in the context of the block sparse model and inter-vector\ncorrelation in the context of the multiple measurement vector model, as well as\ntheir combination. Algorithms based on the sparse Bayesian learning are\npresented and the benefits of incorporating correlation at the algorithm level\nare discussed. The impact of correlation on the limits of support recovery is\nalso discussed highlighting the different impact intra-vector and inter-vector\ncorrelations have on such limits.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2012 23:56:17 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Rao", "Bhaskar D.", ""], ["Zhang", "Zhilin", ""], ["Jin", "Yuzhe", ""]]}, {"id": "1205.4591", "submitter": "Georg M. Goerg", "authors": "Georg M. Goerg", "title": "Forecastable Component Analysis (ForeCA)", "comments": "10 pages, 4 figures; ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce Forecastable Component Analysis (ForeCA), a novel dimension\nreduction technique for temporally dependent signals. Based on a new\nforecastability measure, ForeCA finds an optimal transformation to separate a\nmultivariate time series into a forecastable and an orthogonal white noise\nspace. I present a converging algorithm with a fast eigenvector solution.\nApplications to financial and macro-economic time series show that ForeCA can\nsuccessfully discover informative structure, which can be used for forecasting\nas well as classification. The R package ForeCA\n(http://cran.r-project.org/web/packages/ForeCA/index.html) accompanies this\nwork and is publicly available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 13:17:46 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 04:35:24 GMT"}, {"version": "v3", "created": "Sat, 4 May 2013 19:33:05 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "1205.4645", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke, Jiashun Jin, Jianqing Fan", "title": "Covariate assisted screening and estimation", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1243 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2202-2242", "doi": "10.1214/14-AOS1243", "report-no": "IMS-AOS-AOS1243", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a linear model $Y=X\\beta+z$, where $X=X_{n,p}$ and $z\\sim N(0,I_n)$.\nThe vector $\\beta$ is unknown but is sparse in the sense that most of its\ncoordinates are $0$. The main interest is to separate its nonzero coordinates\nfrom the zero ones (i.e., variable selection). Motivated by examples in\nlong-memory time series (Fan and Yao [Nonlinear Time Series: Nonparametric and\nParametric Methods (2003) Springer]) and the change-point problem (Bhattacharya\n[In Change-Point Problems (South Hadley, MA, 1992) (1994) 28-56 IMS]), we are\nprimarily interested in the case where the Gram matrix $G=X'X$ is nonsparse but\nsparsifiable by a finite order linear filter. We focus on the regime where\nsignals are both rare and weak so that successful variable selection is very\nchallenging but is still possible. We approach this problem by a new procedure\ncalled the covariate assisted screening and estimation (CASE). CASE first uses\na linear filtering to reduce the original setting to a new regression model\nwhere the corresponding Gram (covariance) matrix is sparse. The new covariance\nmatrix induces a sparse graph, which guides us to conduct multivariate\nscreening without visiting all the submodels. By interacting with the signal\nsparsity, the graph enables us to decompose the original problem into many\nseparated small-size subproblems (if only we know where they are!). Linear\nfiltering also induces a so-called problem of information leakage, which can be\novercome by the newly introduced patching technique. Together, these give rise\nto CASE, which is a two-stage screen and clean [Fan and Song Ann. Statist. 38\n(2010) 3567-3604; Wasserman and Roeder Ann. Statist. 37 (2009) 2178-2201]\nprocedure, where we first identify candidates of these submodels by patching\nand screening, and then re-examine each candidate to remove false positives.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 16:01:34 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2012 03:26:00 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2013 19:12:52 GMT"}, {"version": "v4", "created": "Wed, 19 Nov 2014 12:22:55 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Jin", "Jiashun", ""], ["Fan", "Jianqing", ""]]}, {"id": "1205.4697", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa, Aleksandra Slavkovi\\'c", "title": "Inference using noisy degrees: Differentially private $\\beta$-model and\n  synthetic graphs", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1358 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 1, 87-112", "doi": "10.1214/15-AOS1358", "report-no": "IMS-AOS-AOS1358", "categories": "stat.ME cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\beta$-model of random graphs is an exponential family model with the\ndegree sequence as a sufficient statistic. In this paper, we contribute three\nkey results. First, we characterize conditions that lead to a quadratic time\nalgorithm to check for the existence of MLE of the $\\beta$-model, and show that\nthe MLE never exists for the degree partition $\\beta$-model. Second, motivated\nby privacy problems with network data, we derive a differentially private\nestimator of the parameters of $\\beta$-model, and show it is consistent and\nasymptotically normally distributed - it achieves the same rate of convergence\nas the nonprivate estimator. We present an efficient algorithm for the private\nestimator that can be used to release synthetic graphs. Our techniques can also\nbe used to release degree distributions and degree partitions accurately and\nprivately, and to perform inference from noisy degrees arising from contexts\nother than privacy. We evaluate the proposed estimator on real graphs and\ncompare it with a current algorithm for releasing degree distributions and find\nthat it does significantly better. Finally, our paper addresses shortcomings of\ncurrent approaches to a fundamental problem of how to perform valid statistical\ninference from data released by privacy mechanisms, and lays a foundational\ngroundwork on how to achieve optimal and private statistical inference in a\nprincipled manner by modeling the privacy mechanism; these principles should be\napplicable to a class of models beyond the $\\beta$-model.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 19:13:13 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2012 16:40:49 GMT"}, {"version": "v3", "created": "Fri, 8 Aug 2014 07:29:25 GMT"}, {"version": "v4", "created": "Thu, 22 Jan 2015 22:14:10 GMT"}, {"version": "v5", "created": "Tue, 12 Jan 2016 13:04:24 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Karwa", "Vishesh", ""], ["Slavkovi\u0107", "Aleksandra", ""]]}, {"id": "1205.4701", "submitter": "Wei  Zhong", "authors": "Runze Li, Wei Zhong and Liping Zhu", "title": "Feature Screening via Distance Correlation Learning", "comments": "32 pages, 5 tables and 1 figure. Wei Zhong is the corresponding\n  author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with screening features in ultrahigh dimensional data\nanalysis, which has become increasingly important in diverse scientific fields.\nWe develop a sure independence screening procedure based on the distance\ncorrelation (DC-SIS, for short). The DC-SIS can be implemented as easily as the\nsure independence screening procedure based on the Pearson correlation (SIS,\nfor short) proposed by Fan and Lv (2008). However, the DC-SIS can significantly\nimprove the SIS. Fan and Lv (2008) established the sure screening property for\nthe SIS based on linear models, but the sure screening property is valid for\nthe DC-SIS under more general settings including linear models. Furthermore,\nthe implementation of the DC-SIS does not require model specification (e.g.,\nlinear model or generalized linear model) for responses or predictors. This is\na very appealing property in ultrahigh dimensional data analysis. Moreover, the\nDC-SIS can be used directly to screen grouped predictor variables and for\nmultivariate response variables. We establish the sure screening property for\nthe DC-SIS, and conduct simulations to examine its finite sample performance.\nNumerical comparison indicates that the DC-SIS performs much better than the\nSIS in various models. We also illustrate the DC-SIS through a real data\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 19:20:43 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2012 04:43:09 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["Li", "Runze", ""], ["Zhong", "Wei", ""], ["Zhu", "Liping", ""]]}, {"id": "1205.4770", "submitter": "James Sharpnack", "authors": "Mladen Kolar, James Sharpnack", "title": "Variance function estimation in high-dimensions", "comments": "Appearing in Proceedings of the 29 th International Conference on\n  Machine Learning, Edinburgh, Scotland, UK, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the high-dimensional heteroscedastic regression model, where the\nmean and the log variance are modeled as a linear combination of input\nvariables. Existing literature on high-dimensional linear regres- sion models\nhas largely ignored non-constant error variances, even though they commonly\noccur in a variety of applications ranging from biostatis- tics to finance. In\nthis paper we study a class of non-convex penalized pseudolikelihood estimators\nfor both the mean and variance parameters. We show that the Heteroscedastic\nIterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracle\nproperty, that is, we prove that the rates of convergence are the same as if\nthe true model was known. We demonstrate numerical properties of the procedure\non a simulation study and real world data.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 23:35:49 GMT"}], "update_date": "2012-05-23", "authors_parsed": [["Kolar", "Mladen", ""], ["Sharpnack", "James", ""]]}, {"id": "1205.4807", "submitter": "Antonio Lijoi", "authors": "Antonio Lijoi, Igor Pr\\\"unster", "title": "A Conversation with Eugenio Regazzini", "comments": "Published in at http://dx.doi.org/10.1214/11-STS362 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 647-672", "doi": "10.1214/11-STS362", "report-no": "IMS-STS-STS362", "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eugenio Regazzini was born on August 12, 1946 in Cremona (Italy), and took\nhis degree in 1969 at the University \"L. Bocconi\" of Milano. He has held\npositions at the universities of Torino, Bologna and Milano, and at the\nUniversity \"L. Bocconi\" as assistant professor and lecturer from 1974 to 1980,\nand then professor since 1980. He is currently professor in probability and\nmathematical statistics at the University of Pavia. In the periods 1989-2001\nand 2006-2009 he was head of the Institute for Applications of Mathematics and\nComputer Science of the Italian National Research Council (C.N.R.) in Milano\nand head of the Department of Mathematics at the University of Pavia,\nrespectively. For twelve years between 1989 and 2006, he served as a member of\nthe Scientific Board of the Italian Mathematical Union (U.M.I.). In 2007, he\nwas elected Fellow of the IMS and, in 2001, Fellow of the \"Istituto\nLombardo---Accademia di Scienze e Lettere.\" His research activity in\nprobability and statistics has covered a wide spectrum of topics, including\nfinitely additive probabilities, foundations of the Bayesian paradigm,\nexchangeability and partial exchangeability, distribution of functionals of\nrandom probability measures, stochastic integration, history of probability and\nstatistics. Overall, he has been one of the most authoritative developers of de\nFinetti's legacy. In the last five years, he has extended his scientific\ninterests to probabilistic methods in mathematical physics; in particular, he\nhas studied the asymptotic behavior of the solutions of equations, which are of\ninterest for the kinetic theory of gases. The present interview was taken in\noccasion of his 65th birthday.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 05:36:49 GMT"}], "update_date": "2012-05-23", "authors_parsed": [["Lijoi", "Antonio", ""], ["Pr\u00fcnster", "Igor", ""]]}, {"id": "1205.4844", "submitter": "Jakob Stoeber", "authors": "Jakob St\\\"ober and Harry Joe and Claudia Czado", "title": "Simplified Pair Copula Constructions --- Limits and Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So called pair copula constructions (PCCs), specifying multivariate\ndistributions only in terms of bivariate building blocks (pair copulas),\nconstitute a flexible class of dependence models. To keep them tractable for\ninference and model selection, the simplifying assumption that copulas of\nconditional distributions do not depend on the values of the variables which\nthey are conditioned on is popular. In this paper, we show for which classes of\ndistributions such a simplification is applicable, significantly extending the\ndiscussion of Hob{\\ae}k Haff et al. (2010). In particular, we show that the\nonly Archimedean copula in dimension d \\geq 4 which is of the simplified type\nis that based on the gamma Laplace transform or its extension, while the\nStudent-t copula is the only one arising from a scale mixture of Normals.\nFurther, we illustrate how PCCs can be adapted for situations where conditional\ncopulas depend on values which are conditioned on.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 08:57:22 GMT"}], "update_date": "2012-05-23", "authors_parsed": [["St\u00f6ber", "Jakob", ""], ["Joe", "Harry", ""], ["Czado", "Claudia", ""]]}, {"id": "1205.5050", "submitter": "Jacob Bien", "authors": "Jacob Bien, Jonathan Taylor, Robert Tibshirani", "title": "A lasso for hierarchical interactions", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1096 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 3, 1111-1141", "doi": "10.1214/13-AOS1096", "report-no": "IMS-AOS-AOS1096", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We add a set of convex constraints to the lasso to produce sparse interaction\nmodels that honor the hierarchy restriction that an interaction only be\nincluded in a model if one or both variables are marginally important. We give\na precise characterization of the effect of this hierarchy constraint, prove\nthat hierarchy holds with probability one and derive an unbiased estimate for\nthe degrees of freedom of our estimator. A bound on this estimate reveals the\namount of fitting \"saved\" by the hierarchy constraint. We distinguish between\nparameter sparsity - the number of nonzero coefficients - and practical\nsparsity - the number of raw variables one must measure to make a new\nprediction. Hierarchy focuses on the latter, which is more closely tied to\nimportant data collection concerns such as cost, time and effort. We develop an\nalgorithm, available in the R package hierNet, and perform an empirical study\nof our method.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 20:33:35 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2013 20:26:31 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2013 10:13:53 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Bien", "Jacob", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1205.5074", "submitter": "Paul Kabaila", "authors": "Paul Kabaila", "title": "Note on a paradox in decision-theoretic interval estimation", "comments": null, "journal-ref": "Kabaila, P. (2013) Note on a paradox in decision-theoretic\n  interval estimation. Statistics and Probability Letters, 83, 123-126", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence intervals are assessed according to two criteria, namely expected\nlength and coverage probability. In an attempt to apply the decision-theoretic\nmethod to finding a good confidence interval, a loss function that is a linear\ncombination of the interval length and the indicator function that the interval\nincludes the parameter of interest has been proposed. We consider the\nparticular case that the parameter of interest is the normal mean, when the\nvariance is unknown. Casella, Hwang and Robert, Statistica Sinica, 1993, have\nshown that this loss function, combined with the standard noninformative prior,\nleads to a generalized Bayes rule that is a confidence interval for this\nparameter which has \"paradoxical behaviour\". We show that a simple modification\nof this loss function, combined with the same prior, leads to a generalized\nBayes rule that is the usual confidence interval i.e. the \"paradoxical\nbehaviour\" is removed.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 23:55:44 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""]]}, {"id": "1205.5082", "submitter": "Dominic Lee", "authors": "Dominic S. Lee and Carey E. Priebe", "title": "Bayesian Vertex Nomination", "comments": "25 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an attributed graph whose vertices are colored green or red, but\nonly a few are observed to be red. The color of the other vertices is\nunobserved. Typically, the unknown total number of red vertices is small. The\nvertex nomination problem is to nominate one of the unobserved vertices as\nbeing red. The edge set of the graph is a subset of the set of unordered pairs\nof vertices. Suppose that each edge is also colored green or red and this is\nobserved for all edges. The context statistic of a vertex is defined as the\nnumber of observed red vertices connected to it, and its content statistic is\nthe number of red edges incident to it. Assuming that these statistics are\nindependent between vertices and that red edges are more likely between red\nvertices, Coppersmith and Priebe (2012) proposed a likelihood model based on\nthese statistics. Here, we formulate a Bayesian model using the proposed\nlikelihood together with prior distributions chosen for the unknown parameters\nand unobserved vertex colors. From the resulting posterior distribution, the\nnominated vertex is the one with the highest posterior probability of being\nred. Inference is conducted using a Metropolis-within-Gibbs algorithm, and\nperformance is illustrated by a simulation study. Results show that (i) the\nBayesian model performs significantly better than chance; (ii) the probability\nof correct nomination increases with increasing posterior probability that the\nnominated vertex is red; and (iii) the Bayesian model either matches or\nperforms better than the method in Coppersmith and Priebe. An application\nexample is provided using the Enron email corpus, where vertices represent\nEnron employees and their associates, observed red vertices are known\nfraudsters, red edges represent email communications perceived as fraudulent,\nand we wish to identify one of the latent vertices as most likely to be a\nfraudster.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2012 01:19:37 GMT"}], "update_date": "2012-05-24", "authors_parsed": [["Lee", "Dominic S.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1205.5108", "submitter": "Gustavo Delfino", "authors": "Gustavo Delfino, Guillermo Salas", "title": "Analysis of the 2004 Venezuela Referendum: The Official Results Versus\n  the Petition Signatures", "comments": "Published in at http://dx.doi.org/10.1214/08-STS263 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 4, 479-501", "doi": "10.1214/08-STS263", "report-no": "IMS-STS-STS263", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On August 15th, 2004, Venezuelans had the opportunity to vote in a\nPresidential Recall Referendum to decide whether or not President Hugo\nCh\\'{a}vez should be removed from office. The process was largely computerized\nusing a touch-screen system. In general the ballots were not manually counted.\nThe significance of the high linear correlation (0.99) between the number of\nrequesting signatures for the recall petition and the number of opposition\nvotes in computerized centers is analyzed. The same-day audit was found to be\nnot only ineffective but a source of suspicion. Official results were compared\nwith the 1998 presidential election and other electoral events and distortions\nwere found.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2012 07:22:20 GMT"}], "update_date": "2012-05-24", "authors_parsed": [["Delfino", "Gustavo", ""], ["Salas", "Guillermo", ""]]}, {"id": "1205.5314", "submitter": "Ethan Anderes", "authors": "Ethan Anderes and Marc Coram", "title": "A general spline representation for nonparametric and semiparametric\n  density estimates using diffeomorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theorem of McCann shows that for any two absolutely continuous probability\nmeasures on R^d there exists a monotone transformation sending one probability\nmeasure to the other. A consequence of this theorem, relevant to statistics, is\nthat density estimation can be recast in terms of transformations. In\nparticular, one can fix any absolutely continuous probability measure, call it\nP, and then reparameterize the whole class of absolutely continuous probability\nmeasures as monotone transformations from P. In this paper we utilize this\nreparameterization of densities, as monotone transformations from some P, to\nconstruct semiparametric and nonparametric density estimates. We focus our\nattention on classes of transformations, developed in the image processing and\ncomputational anatomy literature, which are smooth, invertible and which have\nattractive computational properties. The techniques developed for this class of\ntransformations allow us to show that a penalized maximum likelihood estimate\n(PMLE) of a smooth transformation from P exists and has a finite dimensional\ncharacterization, similar to those results found in the spline literature.\nThese results are derived utilizing an Euler-Lagrange characterization of the\nPMLE which also establishes a surprising connection to a generalization of\nStein's lemma for characterizing the normal distribution.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2012 00:52:33 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Anderes", "Ethan", ""], ["Coram", "Marc", ""]]}, {"id": "1205.5467", "submitter": "Jun Yan", "authors": "Marcos O. Prates, Dipak K. Dey, Michael R. Willig, and Jun Yan", "title": "Transformed Gaussian Markov Random Fields and Spatial Modeling", "comments": "19 pages, 2 figures, 6 tables", "journal-ref": "Spatial Statistics 14 (2015): 382-399", "doi": "10.1016/j.spasta.2015.07.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian random field (GRF) and the Gaussian Markov random field (GMRF)\nhave been widely used to accommodate spatial dependence under the generalized\nlinear mixed model framework. These models have limitations rooted in the\nsymmetry and thin tail of the Gaussian distribution. We introduce a new class\nof random fields, termed transformed GRF (TGRF), and a new class of Markov\nrandom fields, termed transformed GMRF (TGMRF). They are constructed by\ntransforming the margins of GRFs and GMRFs, respectively, to desired marginal\ndistributions to accommodate asymmetry and heavy tail as needed in practice.\nThe Gaussian copula that characterizes the dependence structure facilitates\ninferences and applications in modeling spatial dependence. This construction\nleads to new models such as gamma or beta Markov fields with Gaussian copulas,\nwhich can be used to model Poisson intensity or Bernoulli rate in a spatial\ngeneralized linear mixed model. The method is naturally implemented in a\nBayesian framework. We illustrate the utility of the methodology in an\necological application with spatial count data and spatial presence/absence\ndata of some snail species, where the new models are shown to outperform the\ntraditional spatial models. The validity of Bayesian inferences and model\nselection are assessed through simulation studies for both spatial Poisson\nregression and spatial Bernoulli regression.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2012 14:38:52 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Prates", "Marcos O.", ""], ["Dey", "Dipak K.", ""], ["Willig", "Michael R.", ""], ["Yan", "Jun", ""]]}, {"id": "1205.5494", "submitter": "Luca Martino", "authors": "Luca Martino, Jesse Read, David Luengo", "title": "Improved Adaptive Rejection Metropolis Sampling Algorithms", "comments": "Matlab code provided in http://a2rms.sourceforge.net/", "journal-ref": "Independent Doubly Adaptive Rejection Metropolis Sampling Within\n  Gibbs Sampling, IEEE Transactions on Signal Processing, Volume 63, Issue 12,\n  Pages 3123-3138, 2015", "doi": "10.1109/TSP.2015.2420537", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods, such as the Metropolis-Hastings (MH)\nalgorithm, are widely used for Bayesian inference. One of the most important\nissues for any MCMC method is the convergence of the Markov chain, which\ndepends crucially on a suitable choice of the proposal density. Adaptive\nRejection Metropolis Sampling (ARMS) is a well-known MH scheme that generates\nsamples from one-dimensional target densities making use of adaptive piecewise\nproposals constructed using support points taken from rejected samples. In this\nwork we pinpoint a crucial drawback in the adaptive procedure in ARMS: support\npoints might never be added inside regions where the proposal is below the\ntarget. When this happens in many regions it leads to a poor performance of\nARMS, with the proposal never converging to the target. In order to overcome\nthis limitation we propose two improved adaptive schemes for constructing the\nproposal. The first one is a direct modification of the ARMS procedure that\nincorporates support points inside regions where the proposal is below the\ntarget, while satisfying the diminishing adaptation property, one of the\nrequired conditions to assure the convergence of the Markov chain. The second\none is an adaptive independent MH algorithm with the ability to learn from all\nprevious samples except for the current state of the chain, thus also\nguaranteeing the convergence to the invariant density. These two new schemes\nimprove the adaptive strategy of ARMS, thus simplifying the complexity in the\nconstruction of the proposals. Numerical results show that the new techniques\nprovide better performance w.r.t. the standard ARMS.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2012 16:21:50 GMT"}, {"version": "v2", "created": "Sun, 27 May 2012 00:39:12 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2012 10:39:51 GMT"}, {"version": "v4", "created": "Mon, 8 Oct 2012 17:05:05 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Martino", "Luca", ""], ["Read", "Jesse", ""], ["Luengo", "David", ""]]}, {"id": "1205.5658", "submitter": "Christian P. Robert", "authors": "K. L. Mengersen (QUT, Brisbane), P. Pudlo (Universite Montpellier 2),\n  and C. P. Robert (Universite Paris-Dauphine)", "title": "Bayesian computation via empirical likelihood", "comments": "21 pages, 12 figures, revised version of the previous version with a\n  new title", "journal-ref": null, "doi": "10.1073/pnas.1208827110", "report-no": null, "categories": "stat.CO q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) has become an essential tool for the\nanalysis of complex stochastic models when the likelihood function is\nnumerically unavailable. However, the well-established statistical method of\nempirical likelihood provides another route to such settings that bypasses\nsimulations from the model and the choices of the ABC parameters (summary\nstatistics, distance, tolerance), while being convergent in the number of\nobservations. Furthermore, bypassing model simulations may lead to significant\ntime savings in complex models, for instance those found in population\ngenetics. The BCel algorithm we develop in this paper also provides an\nevaluation of its own performance through an associated effective sample size.\nThe method is illustrated using several examples, including estimation of\nstandard distributions, time series, and population genetics models.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2012 10:42:35 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2012 14:46:58 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2012 21:27:42 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Mengersen", "K. L.", "", "QUT, Brisbane"], ["Pudlo", "P.", "", "Universite Montpellier 2"], ["Robert", "C. P.", "", "Universite Paris-Dauphine"]]}, {"id": "1205.5865", "submitter": "Branden Fitelson", "authors": "Branden Fitelson and Daniel Osherson", "title": "Remarks on \"Random Sequences\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we show that classical statistical tests for randomness are\nlanguage dependent.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 09:11:18 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Fitelson", "Branden", ""], ["Osherson", "Daniel", ""]]}, {"id": "1205.5868", "submitter": "Kei Hirose", "authors": "Kei Hirose, Michio Yamamoto", "title": "Sparse estimation via nonconcave penalized likelihood in a factor\n  analysis model", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse estimation in a factor analysis model. A\ntraditional estimation procedure in use is the following two-step approach: the\nmodel is estimated by maximum likelihood method and then a rotation technique\nis utilized to find sparse factor loadings. However, the maximum likelihood\nestimates cannot be obtained when the number of variables is much larger than\nthe number of observations. Furthermore, even if the maximum likelihood\nestimates are available, the rotation technique does not often produce a\nsufficiently sparse solution. In order to handle these problems, this paper\nintroduces a penalized likelihood procedure that imposes a nonconvex penalty on\nthe factor loadings. We show that the penalized likelihood procedure can be\nviewed as a generalization of the traditional two-step approach, and the\nproposed methodology can produce sparser solutions than the rotation technique.\nA new algorithm via the EM algorithm along with coordinate descent is\nintroduced to compute the entire solution path, which permits the application\nto a wide variety of convex and nonconvex penalties. Monte Carlo simulations\nare conducted to investigate the performance of our modeling strategy. A real\ndata example is also given to illustrate our procedure.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 10:04:27 GMT"}, {"version": "v2", "created": "Tue, 29 May 2012 14:28:46 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 11:41:15 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Hirose", "Kei", ""], ["Yamamoto", "Michio", ""]]}, {"id": "1205.5920", "submitter": "Nam Lee", "authors": "Nam H. Lee and Jordan Yoder and Minh Tang and Carey E Priebe", "title": "On latent position inference from doubly stochastic messaging activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model messaging activities as a hierarchical doubly stochastic point\nprocess with three main levels, and develop an iterative algorithm for\ninferring actors' relative latent positions from a stream of messaging activity\ndata. Each of the message-exchanging actors is modeled as a process in a latent\nspace. The actors' latent positions are assumed to be influenced by the\ndistribution of a much larger population over the latent space. Each actor's\nmovement in the latent space is modeled as being governed by two parameters\nthat we call confidence and visibility, in addition to dependence on the\npopulation distribution. The messaging frequency between a pair of actors is\nassumed to be inversely proportional to the distance between their latent\npositions. Our inference algorithm is based on a projection approach to an\nonline filtering problem. The algorithm associates each actor with a\nprobability density-valued process, and each probability density is assumed to\nbe a mixture of basis functions. For efficient numerical experiments, we\nfurther develop our algorithm for the case where the basis functions are\nobtained by translating and scaling a standard Gaussian density.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 22:30:58 GMT"}, {"version": "v2", "created": "Tue, 29 May 2012 00:52:45 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2013 21:04:11 GMT"}, {"version": "v4", "created": "Thu, 14 Feb 2013 03:19:13 GMT"}, {"version": "v5", "created": "Thu, 25 Apr 2013 15:02:17 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Lee", "Nam H.", ""], ["Yoder", "Jordan", ""], ["Tang", "Minh", ""], ["Priebe", "Carey E", ""]]}, {"id": "1205.6167", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Wenceslao Gonz\\'alez-Manteiga, Manuel\n  Febrero-Bande", "title": "A goodness-of-fit test for the functional linear model with scalar\n  response", "comments": "Paper: 17 pages, 2 figures, 3 tables. Supplementary material: 8\n  pages, 6 figures, 10 tables", "journal-ref": "Journal of Computational and Graphical Statistics, 23(3):761-778,\n  2014", "doi": "10.1080/10618600.2013.812519", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, a goodness-of-fit test for the null hypothesis of a functional\nlinear model with scalar response is proposed. The test is based on a\ngeneralization to the functional framework of a previous one, designed for the\ngoodness-of-fit of regression models with multivariate covariates using random\nprojections. The test statistic is easy to compute using geometrical and matrix\narguments, and simple to calibrate in its distribution by a wild bootstrap on\nthe residuals. The finite sample properties of the test are illustrated by a\nsimulation study for several types of basis and under different alternatives.\nFinally, the test is applied to two datasets for checking the assumption of the\nfunctional linear model and a graphical tool is introduced. Supplementary\nmaterials are available online.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2012 17:42:53 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2012 18:38:34 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2012 10:30:50 GMT"}, {"version": "v4", "created": "Mon, 11 Feb 2013 12:53:49 GMT"}, {"version": "v5", "created": "Wed, 26 Feb 2014 11:24:36 GMT"}, {"version": "v6", "created": "Sun, 28 Sep 2014 03:04:50 GMT"}, {"version": "v7", "created": "Sun, 20 Sep 2020 22:51:32 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Febrero-Bande", "Manuel", ""]]}, {"id": "1205.6244", "submitter": "Weiping Zhu", "authors": "Weiping Zhu", "title": "Explicit Estimators for Loss Tomography", "comments": "11 pages, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full likelihood has been widely used in loss tomography because most believe\nit can produce accurate estimates although the full likelihood estimators\nproposed so far are complex in structure and expensive in execution. We in this\npaper advocate a different likelihood called composite likelihood to replace\nthe full likelihood in loss tomography for simplicity and accuracy. Using the\nproposed likelihood, we propose a number of explicit estimators with\nstatistical analysis. The analysis shows all of the explicit estimators perform\nalmost as good as the full likelihood one in terms of accuracy and better than\nthe full likelihood one in computational complexity. Although the discussion is\nrestricted to the tree topology, the methodology proposed here is also\napplicable to a network of a general topology.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 01:31:33 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2012 05:11:20 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2012 22:43:42 GMT"}, {"version": "v4", "created": "Mon, 3 Dec 2012 01:59:41 GMT"}, {"version": "v5", "created": "Thu, 13 Jun 2013 01:31:27 GMT"}, {"version": "v6", "created": "Tue, 13 Aug 2013 03:45:31 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Zhu", "Weiping", ""]]}, {"id": "1205.6310", "submitter": "Alberto Sorrentino", "authors": "Alberto Sorrentino, Adam M. Johansen, John A. D. Aston, Thomas E.\n  Nichols, Wilfrid S. Kendall", "title": "Dynamic filtering of static dipoles in magnetoencephalography", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS611 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 955-988", "doi": "10.1214/12-AOAS611", "report-no": "IMS-AOAS-AOAS611", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating neural activity from measurements of\nthe magnetic fields recorded by magnetoencephalography. We exploit the temporal\nstructure of the problem and model the neural current as a collection of\nevolving current dipoles, which appear and disappear, but whose locations are\nconstant throughout their lifetime. This fully reflects the physiological\ninterpretation of the model. In order to conduct inference under this proposed\nmodel, it was necessary to develop an algorithm based around state-of-the-art\nsequential Monte Carlo methods employing carefully designed importance\ndistributions. Previous work employed a bootstrap filter and an artificial\ndynamic structure where dipoles performed a random walk in space, yielding\nnonphysical artefacts in the reconstructions; such artefacts are not observed\nwhen using the proposed model. The algorithm is validated with simulated data,\nin which it provided an average localisation error which is approximately half\nthat of the bootstrap filter. An application to complex real data derived from\na somatosensory experiment is presented. Assessment of model fit via marginal\nlikelihood showed a clear preference for the proposed model and the associated\nreconstructions show better localisation.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 09:29:51 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2012 12:20:42 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2013 12:01:18 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Sorrentino", "Alberto", ""], ["Johansen", "Adam M.", ""], ["Aston", "John A. D.", ""], ["Nichols", "Thomas E.", ""], ["Kendall", "Wilfrid S.", ""]]}, {"id": "1205.6653", "submitter": "Bernd  Klaus", "authors": "Bernd Klaus", "title": "Effect Size Estimation and Misclassification Rate Based Variable\n  Selection in Linear Discriminant Analysis", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised classifying of biological samples based on genetic information,\n(e.g. gene expression profiles) is an important problem in biostatistics. In\norder to find both accurate and interpretable classification rules variable\nselection is indispensable. This article explores how an assessment of the\nindividual importance of variables (effect size estimation) can be used to\nperform variable selection. I review recent effect size estimation approaches\nin the context of linear discriminant analysis (LDA) and propose a new\nconceptually simple effect size estimation method which is at the same time\ncomputationally efficient. I then show how to use effect sizes to perform\nvariable selection based on the misclassification rate which is the data\nindependent expectation of the prediction error. Simulation studies and real\ndata analyses illustrate that the proposed effect size estimation and variable\nselection methods are competitive. Particularly, they lead to both compact and\ninterpretable feature sets.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 12:59:26 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2012 16:18:13 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Klaus", "Bernd", ""]]}, {"id": "1205.6761", "submitter": "Adriano Zambom", "authors": "Adriano Zanin Zambom and Michael Akritas", "title": "Nonparametric Model Checking and Variable Selection", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let X be a d dimensional vector of covariates and Y be the response variable.\nUnder the nonparametric model Y = m(X) + {\\sigma}(X) \\in we develop an\nANOVA-type test for the null hypothesis that a particular coordinate of X has\nno influence on the regression function. The asymptotic distribution of the\ntest statistic, using residuals based on Nadaraya-Watson type kernel estimator\nand d \\leq 4, is established under the null hypothesis and local alternatives.\nSimulations suggest that under a sparse model, the applicability of the test\nextends to arbitrary d through sufficient dimension reduction. Using p-values\nfrom this test, a variable selection method based on multiple testing ideas is\nproposed. The proposed test outperforms existing procedures, while additional\nsimulations reveal that the proposed variable selection method performs\ncompetitively against well established procedures. A real data set is analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 17:32:18 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Zambom", "Adriano Zanin", ""], ["Akritas", "Michael", ""]]}, {"id": "1205.6843", "submitter": "Adriano Zambom", "authors": "Adriano Zanin Zambom and Michael G. Akritas", "title": "Significance Testing and Group Variable Selection", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let X; Z be r and s-dimensional covariates, respectively, used to model the\nresponse variable Y as Y = m(X;Z) + \\sigma(X;Z)\\epsilon. We develop an\nANOVA-type test for the null hypothesis that Z has no influence on the\nregression function, based on residuals obtained from local polynomial fitting\nof the null model. Using p-values from this test, a group variable selection\nmethod based on multiple testing ideas is proposed. Simulations studies suggest\nthat the proposed test procedure outperforms the generalized likelihood ratio\ntest when the alternative is non-additive or there is heteroscedasticity.\nAdditional simulation studies, with data generated from linear, non-linear and\nlogistic regression, reveal that the proposed group variable selection\nprocedure performs competitively against Group Lasso, and outperforms it in\nselecting groups having nonlinear effects. The proposed group variable\nselection procedure is illustrated on a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 21:47:40 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Zambom", "Adriano Zanin", ""], ["Akritas", "Michael G.", ""]]}, {"id": "1205.6920", "submitter": "Chris Sherlock Dr.", "authors": "Paul Fearnhead, Vasileios Giagos and Chris Sherlock", "title": "Inference for reaction networks using the Linear Noise Approximation", "comments": null, "journal-ref": null, "doi": "10.1111/biom.12152", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference for the reaction rates in discretely observed networks\nsuch as those found in models for systems biology, population ecology and\nepidemics. Most such networks are neither slow enough nor small enough for\ninference via the true state-dependent Markov jump process to be feasible.\nTypically, inference is conducted by approximating the dynamics through an\nordinary differential equation (ODE), or a stochastic differential equation\n(SDE). The former ignores the stochasticity in the true model, and can lead to\ninaccurate inferences. The latter is more accurate but is harder to implement\nas the transition density of the SDE model is generally unknown. The Linear\nNoise Approximation (LNA) is a first order Taylor expansion of the\napproximating SDE about a deterministic solution and can be viewed as a\ncompromise between the ODE and SDE models. It is a stochastic model, but\ndiscrete time transition probabilities for the LNA are available through the\nsolution of a series of ordinary differential equations. We describe how a\nrestarting LNA can be efficiently used to perform inference for a general class\nof reaction networks; evaluate the accuracy of such an approach; and show how\nand when this approach is either statistically or computationally more\nefficient than ODE or SDE methods. We apply the LNA to analyse Google Flu\nTrends data from the North and South Islands of New Zealand, and are able to\nobtain more accurate short-term forecasts of new flu cases than another\nrecently proposed method, although at a greater computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2012 08:51:16 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2014 16:57:13 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Fearnhead", "Paul", ""], ["Giagos", "Vasileios", ""], ["Sherlock", "Chris", ""]]}]