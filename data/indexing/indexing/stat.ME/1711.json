[{"id": "1711.00031", "submitter": "Hojin Yang", "authors": "Hojin Yang, Veerabhadran Baladandayuthapani, Jeffrey S. Morris", "title": "Quantile Functional Regression using Quantlets", "comments": "41 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a quantile functional regression modeling framework\nthat models the distribution of a set of common repeated observations from a\nsubject through the quantile function, which is regressed on a set of\ncovariates to determine how these factors affect various aspects of the\nunderlying subject-specific distribution. To account for smoothness in the\nquantile functions, we introduce custom basis functions we call\n\\textit{quantlets} that are sparse, regularized, near-lossless, and empirically\ndefined, adapting to the features of a given data set and containing a Gaussian\nsubspace so {non-Gaussianness} can be assessed. While these quantlets could be\nused within various functional regression frameworks, we build a Bayesian\nframework that uses nonlinear shrinkage of quantlet coefficients to regularize\nthe functional regression coefficients and allows fully Bayesian inferences\nafter fitting a Markov chain Monte Carlo. Specifically, we apply global tests\nto assess which covariates have any effect on the distribution at all, followed\nby local tests to identify at which specific quantiles the differences lie\nwhile adjusting for multiple testing, and to assess whether the covariate\naffects certain major aspects of the distribution, including location, scale,\nskewness, Gaussianness, or tails. If the difference lies in these commonly-used\nsummaries, our approach can still detect them, but our systematic modeling\nstrategy can also detect effects on other aspects of the distribution that\nmight be missed if one restricted attention to pre-chosen summaries. We\ndemonstrate the benefit of the basis space modeling through simulation studies,\nand illustrate the method using a biomedical imaging data set in which we\nrelate the distribution of pixel intensities from a tumor image to various\ndemographic, clinical, and genetic characteristics.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 18:11:11 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Yang", "Hojin", ""], ["Baladandayuthapani", "Veerabhadran", ""], ["Morris", "Jeffrey S.", ""]]}, {"id": "1711.00052", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Yue Zhang", "title": "Empirical likelihood inference for partial functional linear regression\n  models based on B spline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply empirical likelihood method to inference for the\nregression parameters in the partial functional linear regression models based\non B spline. We prove that the empirical log likelihood ratio for the\nregression parameters converges in law to a weighted sum of independent chi\nsquare distributions and run simulations to assess the finite sample\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 18:48:58 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Yuan", "Mingao", ""], ["Zhang", "Yue", ""]]}, {"id": "1711.00064", "submitter": "Chandler Zuo", "authors": "Chandler Zuo", "title": "Calibration for Stratified Classification Models", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classification problems, sampling bias between training data and testing\ndata is critical to the ranking performance of classification scores. Such bias\ncan be both unintentionally introduced by data collection and intentionally\nintroduced by the algorithm, such as under-sampling or weighting techniques\napplied to imbalanced data. When such sampling bias exists, using the raw\nclassification score to rank observations in the testing data can lead to\nsuboptimal results. In this paper, I investigate the optimal calibration\nstrategy in general settings, and develop a practical solution for one specific\nsampling bias case, where the sampling bias is introduced by stratified\nsampling. The optimal solution is developed by analytically solving the problem\nof optimizing the ROC curve. For practical data, I propose a ranking algorithm\nfor general classification models with stratified data. Numerical experiments\ndemonstrate that the proposed algorithm effectively addresses the stratified\nsampling bias issue. Interestingly, the proposed method shows its potential\napplicability in two other machine learning areas: unsupervised learning and\nmodel ensembling, which can be future research topics.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 19:23:57 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Zuo", "Chandler", ""]]}, {"id": "1711.00069", "submitter": "Shulei Wang", "authors": "Shulei Wang, Ellen T. Arena, Jordan T. Becker, William M. Bement,\n  Nathan M. Sherer, Kevin W. Eliceiri, Ming Yuan", "title": "Spatially Adaptive Colocalization Analysis in Dual-Color Fluorescence\n  Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colocalization analysis aims to study complex spatial associations between\nbio-molecules via optical imaging techniques. However, existing colocalization\nanalysis workflows only assess an average degree of colocalization within a\ncertain region of interest and ignore the unique and valuable spatial\ninformation offered by microscopy. In the current work, we introduce a new\nframework for colocalization analysis that allows us to quantify colocalization\nlevels at each individual location and automatically identify pixels or regions\nwhere colocalization occurs. The framework, referred to as spatially adaptive\ncolocalization analysis (SACA), integrates a pixel-wise local kernel model for\ncolocalization quantification and a multi-scale adaptive propagation-separation\nstrategy for utilizing spatial information to detect colocalization in a\nspatially adaptive fashion. Applications to simulated and real biological\ndatasets demonstrate the practical merits of SACA in what we hope to be an\neasily applicable and robust colocalization analysis method. In addition,\ntheoretical properties of SACA are investigated to provide rigorous statistical\njustification.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 19:40:29 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 12:57:08 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Wang", "Shulei", ""], ["Arena", "Ellen T.", ""], ["Becker", "Jordan T.", ""], ["Bement", "William M.", ""], ["Sherer", "Nathan M.", ""], ["Eliceiri", "Kevin W.", ""], ["Yuan", "Ming", ""]]}, {"id": "1711.00097", "submitter": "Matteo Iacopini", "authors": "Monica Billio, Roberto Casarin, Matteo Iacopini", "title": "Bayesian Markov Switching Tensor Regression for Time-varying Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Bayesian Markov switching regression model for\nmultidimensional arrays (tensors) of binary time series. We assume a\nzero-inflated logit regression with time-varying parameters and apply it to\nmultilayer temporal networks. The original contribution is threefold. First, to\navoid over-fitting we propose a parsimonious parametrization based on a\nlow-rank decomposition of the tensor of regression coefficients. Second, we\nassume the parameters are driven by a hidden Markov chain, thus allowing for\nstructural changes in the network topology. We follow a Bayesian approach to\ninference and provide an efficient Gibbs sampler for posterior approximation.\nWe apply the methodology to a real dataset of financial networks to study the\nimpact of several risk factors on the edge probability. Supplementary materials\nfor this article are available online.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:37:07 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 14:49:53 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 21:37:27 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Billio", "Monica", ""], ["Casarin", "Roberto", ""], ["Iacopini", "Matteo", ""]]}, {"id": "1711.00101", "submitter": "Anru R. Zhang", "authors": "Anru R. Zhang and Kehui Chen", "title": "Nonparametric covariance estimation for mixed longitudinal studies, with\n  applications in midlife women's health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In mixed longitudinal studies, a group of subjects enter the study at\ndifferent ages (cross-sectional) and are followed for successive years\n(longitudinal). In the context of such studies, we consider nonparametric\ncovariance estimation with samples of noisy and partially observed functional\ntrajectories. The proposed algorithm is based on a noniterative\nsequential-aggregation scheme with only basic matrix operations and closed-form\nsolutions in each step. The good performance of the proposed method is\nsupported by both theory and numerical experiments. We also apply the proposed\nprocedure to a study on the working memory of midlife women, based on data from\nthe Study of Women's Health Across the Nation (SWAN).\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:42:02 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 14:30:28 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 22:17:09 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 15:07:54 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Zhang", "Anru R.", ""], ["Chen", "Kehui", ""]]}, {"id": "1711.00136", "submitter": "Stephane Shao", "authors": "Stephane Shao, Pierre E. Jacob, Jie Ding, Vahid Tarokh", "title": "Bayesian model comparison with the Hyv\\\"arinen score: computation and\n  consistency", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": "10.1080/01621459.2018.1518237", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayes factor is a widely used criterion in model comparison and its\nlogarithm is a difference of out-of-sample predictive scores under the\nlogarithmic scoring rule. However, when some of the candidate models involve\nvague priors on their parameters, the log-Bayes factor features an arbitrary\nadditive constant that hinders its interpretation. As an alternative, we\nconsider model comparison using the Hyv\\\"arinen score. We propose a method to\nconsistently estimate this score for parametric models, using sequential Monte\nCarlo methods. We show that this score can be estimated for models with\ntractable likelihoods as well as nonlinear non-Gaussian state-space models with\nintractable likelihoods. We prove the asymptotic consistency of this new model\nselection criterion under strong regularity assumptions in the case of\nnon-nested models, and we provide qualitative insights for the nested case. We\nalso use existing characterizations of proper scoring rules on discrete spaces\nto extend the Hyv\\\"arinen score to discrete observations. Our numerical\nillustrations include L\\'evy-driven stochastic volatility models and diffusion\nmodels for population dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 22:50:32 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 02:48:08 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Shao", "Stephane", ""], ["Jacob", "Pierre E.", ""], ["Ding", "Jie", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1711.00149", "submitter": "Indranil Ghosh", "authors": "Indranil Ghosh", "title": "Statistical Inference of Kumaraswamy distribution under imprecise\n  information", "comments": "This is a short communication type article with 12 pages only", "journal-ref": "Journal of Biometrics and Biostatistics (2017)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional statistical approaches for estimating the parameters of the\nKumaraswamy distribution have dealt with precise information. However, in real\nworld situations, some information about an underlying experimental process\nmight be imprecise and might be represented in the form of fuzzy information.\nIn this paper, we consider the problem of estimating the parameters of a\nunivariate Kumaraswamy distribution with two parameters when the available\nobservations are described by means of fuzzy information. We derive the maximum\nlikelihood estimate of the parameters by using Newton Raphson as well as EM\nalgorithm method. The estimation procedures are discussed in details and\ncompared via Markov Chain Monte Carlo simulations in terms of their average\nbiases and mean squared errors.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 00:21:21 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ghosh", "Indranil", ""]]}, {"id": "1711.00158", "submitter": "Indranil Ghosh", "authors": "Indranil Ghosh, GG Hamedani", "title": "On the Ristic-Balakrishnan distribution: bivariate extension and\n  characterizations", "comments": "This is a short communication type article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, a significant development has been made towards\nthe augmentation of some well-known lifetime distributions by various\nstrategies. These newly developed models have enjoyed a considerable amount of\nsuccess in modeling various real life phenomena. Motivated by this, Ristic &\nBalakrishnan (2012) developed a special class of univariate distributions (see\nRistic- Balakrishnan (2012)). Henceforth we call this family of distribution as\nRB-G family of distributions. The RB-G family has the same parameters of the G\ndistribution plus an additional positive shape parameter a. Several RB-G\ndistribution can be obtained from a specified G distribution. For a = 1, the\nbaseline G distribution is a basic exemplar of the RB-G family with a\ncontinuous crossover towards cases with various shapes. In this article we\nfocus our attention on the characterization of this family and discuss some\nstructural properties of the bivariate RB-G family of distributions which are\nnot discussed in detail in Ristic and Balakrishnan (2012).\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 01:53:46 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ghosh", "Indranil", ""], ["Hamedani", "GG", ""]]}, {"id": "1711.00162", "submitter": "Kelly Cristina Mota Goncalves", "authors": "Kelly C. M. Gon\\c{c}alves, Helio S. Migon and Leonardo S. Bastos", "title": "Dynamic quantile linear models: a Bayesian approach", "comments": "38 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of models, named dynamic quantile linear models, is presented. It\ncombines dynamic linear models with distribution free quantile regression\nproducing a robust statistical method. Bayesian inference for dynamic quantile\nlinear models can be performed using an efficient Markov chain Monte Carlo\nalgorithm. A fast sequential procedure suited for high-dimensional predictive\nmodeling applications with massive data, in which the generating process is\nitself changing overtime, is also proposed. The proposed model is evaluated\nusing synthetic and well-known time series data. The model is also applied to\npredict annual incidence of tuberculosis in Rio de Janeiro state for future\nyears and compared with global strategy targets set by the World Health\nOrganization.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 02:01:04 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 18:43:18 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Gon\u00e7alves", "Kelly C. M.", ""], ["Migon", "Helio S.", ""], ["Bastos", "Leonardo S.", ""]]}, {"id": "1711.00177", "submitter": "Haiming Zhou", "authors": "Haiming Zhou and Xianzheng Huang", "title": "Bandwidth selection for nonparametric modal regression", "comments": "To appear in Communications in Statistics - Simulation and\n  Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of estimating local modes of a conditional density based on\nkernel density estimators, we show that existing bandwidth selection methods\ndeveloped for kernel density estimation are unsuitable for mode estimation. We\npropose two methods to select bandwidths tailored for mode estimation in the\nregression setting. Numerical studies using synthetic data and a real-life data\nset are carried out to demonstrate the performance of the proposed methods in\ncomparison with several well received bandwidth selection methods for density\nestimation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 02:52:56 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Zhou", "Haiming", ""], ["Huang", "Xianzheng", ""]]}, {"id": "1711.00235", "submitter": "Jun Lu", "authors": "Lu Lin, Jun Lu, Chen Lin", "title": "Group-Average and Convex Clustering for Partially Heterogeneous Linear\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a subgroup least squares and a convex clustering are\nintroduced for inferring a partially heterogenous linear regression that has\npotential application in the areas of precision marketing and precision\nmedicine. The homogenous parameter and the subgroup-average of the heterogenous\nparameters can be consistently estimated by the subgroup least squares, without\nneed of the sparsity assumption on the heterogenous parameters. The\nheterogenous parameters can be consistently clustered via the convex\nclustering. Unlike the existing methods for regression clustering, our\nclustering procedure is a standard mean clustering, although the model under\nstudy is a type of regression, and the corresponding algorithm only involves\nlow dimensional parameters. Thus, it is simple and stable even if the sample\nsize is large. The advantage of the method is further illustrated via\nsimulation studies and the analysis of car sales data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 07:44:01 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Lin", "Lu", ""], ["Lu", "Jun", ""], ["Lin", "Chen", ""]]}, {"id": "1711.00432", "submitter": "Zhaoxia Yu", "authors": "Zhaoxia Yu, Dustin Pluta, Tong Shen, Chuansheng Chen, Gui Xue,\n  Hernando Ombao", "title": "Statistical Challenges in Modeling Big Brain Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain signal data are inherently big: massive in amount, complex in\nstructure, and high in dimensions. These characteristics impose great\nchallenges for statistical inference and learning. Here we review several key\nchallenges, discuss possible solutions, and highlight future research\ndirections.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:40:58 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Yu", "Zhaoxia", ""], ["Pluta", "Dustin", ""], ["Shen", "Tong", ""], ["Chen", "Chuansheng", ""], ["Xue", "Gui", ""], ["Ombao", "Hernando", ""]]}, {"id": "1711.00437", "submitter": "Claudio Fronterr\\`e", "authors": "Claudio Fronterr\\`e, Emanuele Giorgi, Peter J. Diggle", "title": "Geostatistical inference in the presence of geomasking: a\n  composite-likelihood approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In almost any geostatistical analysis, one of the underlying, often implicit,\nmodelling assump- tions is that the spatial locations, where measurements are\ntaken, are recorded without error. In this study we develop geostatistical\ninference when this assumption is not valid. This is often the case when, for\nexample, individual address information is randomly altered to provide pri-\nvacy protection or imprecisions are induced by geocoding processes and\nmeasurement devices. Our objective is to develop a method of inference based on\nthe composite likelihood that over- comes the inherent computational limits of\nthe full likelihood method as set out in Fanshawe and Diggle (2011). Through a\nsimulation study, we then compare the performance of our proposed approach with\nan N-weighted least squares estimation procedure, based on a corrected version\nof the empirical variogram. Our results indicate that the composite-likelihood\napproach outper- forms the latter, leading to smaller root-mean-square-errors\nin the parameter estimates. Finally, we illustrate an application of our method\nto analyse data on malnutrition from a Demographic and Health Survey conducted\nin Senegal in 2011, where locations were randomly perturbed to protect the\nprivacy of respondents.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:47:15 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Fronterr\u00e8", "Claudio", ""], ["Giorgi", "Emanuele", ""], ["Diggle", "Peter J.", ""]]}, {"id": "1711.00460", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela and Michael L. Stein", "title": "Locally stationary spatio-temporal interpolation of Argo profiling float\n  data", "comments": "28 pages, 8 figures, changes to the presentation throughout, some\n  material moved to the supplement, author version of the published paper", "journal-ref": "Proceedings of the Royal Society A 474: 20180400 (2018)", "doi": "10.1098/rspa.2018.0400", "report-no": null, "categories": "stat.AP physics.ao-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argo floats measure seawater temperature and salinity in the upper 2,000 m of\nthe global ocean. Statistical analysis of the resulting spatio-temporal dataset\nis challenging due to its nonstationary structure and large size. We propose\nmapping these data using locally stationary Gaussian process regression where\ncovariance parameter estimation and spatio-temporal prediction are carried out\nin a moving-window fashion. This yields computationally tractable nonstationary\nanomaly fields without the need to explicitly model the nonstationary\ncovariance structure. We also investigate Student-$t$ distributed fine-scale\nvariation as a means to account for non-Gaussian heavy tails in ocean\ntemperature data. Cross-validation studies comparing the proposed approach with\nthe existing state-of-the-art demonstrate clear improvements in point\npredictions and show that accounting for the nonstationarity and\nnon-Gaussianity is crucial for obtaining well-calibrated uncertainties. This\napproach also provides data-driven local estimates of the spatial and temporal\ndependence scales for the global ocean which are of scientific interest in\ntheir own right.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 17:49:33 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 19:32:27 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 18:54:41 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Kuusela", "Mikael", ""], ["Stein", "Michael L.", ""]]}, {"id": "1711.00484", "submitter": "Pulong Ma", "authors": "Pulong Ma, Emily L. Kang, Amy Braverman, and Hai Nguyen", "title": "Spatial Statistical Downscaling for Constructing High-Resolution Nature\n  Runs in Global Observing System Simulation Experiments", "comments": "Accepted version in Technometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing system simulation experiments (OSSEs) have been widely used as a\nrigorous and cost-effective way to guide development of new observing systems,\nand to evaluate the performance of new data assimilation algorithms. Nature\nruns (NRs), which are outputs from deterministic models, play an essential role\nin building OSSE systems for global atmospheric processes because they are used\nboth to create synthetic observations at high spatial resolution, and to\nrepresent the \"true\" atmosphere against which the forecasts are verified.\nHowever, most NRs are generated at resolutions coarser than actual\nobservations. Here, we propose a principled statistical downscaling framework\nto construct high-resolution NRs via conditional simulation from\ncoarse-resolution numerical model output. We use nonstationary spatial\ncovariance function models that have basis function representations. This\napproach not only explicitly addresses the change-of-support problem, but also\nallows fast computation with large volumes of numerical model output. We also\npropose a data-driven algorithm to select the required basis functions\nadaptively, in order to increase the flexibility of our nonstationary\ncovariance function models. In this article we demonstrate these techniques by\ndownscaling a coarse-resolution physical NR at a native resolution of\n$1^{\\circ} \\text{ latitude} \\times 1.25^{\\circ} \\text{ longitude}$ of global\nsurface $\\text{CO}_2$ concentrations to 655,362 equal-area hexagons.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:01:26 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 18:43:29 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ma", "Pulong", ""], ["Kang", "Emily L.", ""], ["Braverman", "Amy", ""], ["Nguyen", "Hai", ""]]}, {"id": "1711.00497", "submitter": "Amit Meir", "authors": "Ruth Heller, Amit Meir, Nilanjan Chatterjee", "title": "Post-selection estimation and testing following aggregated association\n  tests", "comments": "33 pages, 9 figures", "journal-ref": "J. R. Stat. Soc. B, 81 (2019): 547-573", "doi": "10.1111/rssb.12318", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practice of pooling several individual test statistics to form aggregate\ntests is common in many statistical application where individual tests may be\nunderpowered. While selection by aggregate tests can serve to increase power,\nthe selection process invalidates the individual test-statistics, making it\ndifficult to identify the ones that drive the signal in follow-up inference.\nHere, we develop a general approach for valid inference following selection by\naggregate testing. We present novel powerful post-selection tests for the\nindividual null hypotheses which are exact for the normal model and\nasymptotically justified otherwise. Our approach relies on the ability to\ncharacterize the distribution of the individual test statistics after\nconditioning on the event of selection. We provide efficient algorithms for\nestimation of the post-selection maximum-likelihood estimates and suggest\nconfidence intervals which rely on a novel switching regime for good coverage\nguarantees. We validate our methods via comprehensive simulation studies and\napply them to data from the Dallas Heart Study, demonstrating that single\nvariant association discovery following selection by an aggregated test is\nindeed possible in practice.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:21:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Heller", "Ruth", ""], ["Meir", "Amit", ""], ["Chatterjee", "Nilanjan", ""]]}, {"id": "1711.00555", "submitter": "Qi Dong", "authors": "Jon Wakefield, Tracy Qi Dong, Vladimir N. Minin", "title": "Spatio-Temporal Analysis of Surveillance Data", "comments": "31 pages, 7 figures. This is an author-created preprint of a book\n  chapter to appear in the Handbook of Infectious Disease Data Analysis edited\n  by Leonhard Held, Niel Hens, Philip D O'Neill and Jacco Wallinga, Chapman and\n  Hall/CRC", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we consider space-time analysis of surveillance count data.\nSuch data are ubiquitous and a number of approaches have been proposed for\ntheir analysis. We first describe the aims of a surveillance endeavor, before\nreviewing and critiquing a number of common models. We focus on models in which\ntime is discretized to the time scale of the latent and infectious periods of\nthe disease under study. In particular, we focus on the time series SIR (TSIR)\nmodels originally described by Finkenstadt and Grenfell in their 2000 paper and\nthe epidemic/endemic models first proposed by Held, Hohle, and Hofmann in their\n2005 paper. We implement both of these models in the Stan software and\nillustrate their performance via analyses of measles data collected over a\n2-year period in 17 regions in the Weser-Ems region of Lower Saxony, Germany.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 22:43:35 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Wakefield", "Jon", ""], ["Dong", "Tracy Qi", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1711.00562", "submitter": "Jacopo Soriano", "authors": "Jacopo Soriano", "title": "Percent Change Estimation in Large Scale Online Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online experiments are a fundamental component of the development of\nweb-facing products. Given their large user-bases, even small product\nimprovements can have a large impact on user engagement or profits on an\nabsolute scale. As a result, accurately estimating the relative impact of these\nchanges is extremely important. I propose an approach based on an objective\nBayesian model to improve the sensitivity of percent change estimation in A/B\nexperiments. Leveraging pre-period information, this approach produces more\nrobust and accurate point estimates and up to 50% tighter credible intervals\nthan traditional methods. The R package abpackage provides an implementation of\nthe approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 23:28:46 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 04:35:38 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Soriano", "Jacopo", ""]]}, {"id": "1711.00564", "submitter": "Gregor Kastner", "authors": "Martin Feldkircher, Florian Huber, Gregor Kastner", "title": "Sophisticated and small versus simple and sizeable: When does it pay off\n  to introduce drifting coefficients in Bayesian VARs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the relationship between model size and complexity in the\ntime-varying parameter VAR framework via thorough predictive exercises for the\nEuro Area, the United Kingdom and the United States. It turns out that\nsophisticated dynamics through drifting coefficients are important in small\ndata sets while simpler models tend to perform better in sizeable data sets. To\ncombine best of both worlds, novel shrinkage priors help to mitigate the curse\nof dimensionality, resulting in competitive forecasts for all scenarios\nconsidered. Furthermore, we discuss dynamic model selection to improve upon the\nbest performing individual model for each point in time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 23:34:11 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 16:45:40 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Feldkircher", "Martin", ""], ["Huber", "Florian", ""], ["Kastner", "Gregor", ""]]}, {"id": "1711.00572", "submitter": "Saptarshi Chakraborty", "authors": "Saptarshi Chakraborty and Kshitij Khare", "title": "Consistent estimation of the spectrum of trace class data augmentation\n  algorithms", "comments": "43 pages (including Appendix), 3 figures; final version", "journal-ref": "Bernoulli, Volume 25, Number 4B (2019), 3832-3863", "doi": "10.3150/19-BEJ1112", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo is widely used in a variety of scientific\napplications to generate approximate samples from intractable distributions. A\nthorough understanding of the convergence and mixing properties of these Markov\nchains can be obtained by studying the spectrum of the associated Markov\noperator. While several methods to bound/estimate the second largest eigenvalue\nare available in the literature, very few general techniques for consistent\nestimation of the entire spectrum have been proposed. Existing methods for this\npurpose require the Markov transition density to be available in closed form,\nwhich is often not true in practice, especially in modern statistical\napplications. In this paper, we propose a novel method to consistently estimate\nthe entire spectrum of a general class of Markov chains arising from a popular\nand widely used statistical approach known as Data Augmentation. The transition\ndensities of these Markov chains can often only be expressed as intractable\nintegrals. We illustrate the applicability of our method using real and\nsimulated data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 00:20:41 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 02:40:06 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 21:57:26 GMT"}, {"version": "v4", "created": "Thu, 3 Oct 2019 04:27:52 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Chakraborty", "Saptarshi", ""], ["Khare", "Kshitij", ""]]}, {"id": "1711.00573", "submitter": "Clementine Mottet", "authors": "Clementine Mottet and Henry Lam", "title": "On Optimization over Tail Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of optimization to compute bounds for extremal\nperformance measures. This approach takes a non-parametric viewpoint that aims\nto alleviate the issue of model misspecification possibly encountered by\nconventional methods in extreme event analysis. We make two contributions\ntowards solving these formulations, paying especial attention to the arising\ntail issues. First, we provide a technique in parallel to Choquet's theory, via\na combination of integration by parts and change of measures, to transform\nshape constrained problems (e.g., monotonicity of derivatives) into families of\nmoment problems. Second, we show how a moment problem cast over infinite\nsupport can be reformulated into a problem over compact support with an\nadditional slack variable. In the context of optimization over tail\ndistributions, the latter helps resolve the issue of non-convergence of\nsolutions when using algorithms such as generalized linear programming. We\nfurther demonstrate the applicability of this result to problems with\ninfinite-value constraints, which can arise in modeling heavy tails.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 00:21:48 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Mottet", "Clementine", ""], ["Lam", "Henry", ""]]}, {"id": "1711.00636", "submitter": "Patrick McDermott", "authors": "Patrick L. McDermott and Christopher K. Wikle", "title": "Bayesian Recurrent Neural Network Models for Forecasting and Quantifying\n  Uncertainty in Spatial-Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are nonlinear dynamical models commonly used\nin the machine learning and dynamical systems literature to represent complex\ndynamical or sequential relationships between variables. More recently, as deep\nlearning models have become more common, RNNs have been used to forecast\nincreasingly complicated systems. Dynamical spatio-temporal processes represent\na class of complex systems that can potentially benefit from these types of\nmodels. Although the RNN literature is expansive and highly developed,\nuncertainty quantification is often ignored. Even when considered, the\nuncertainty is generally quantified without the use of a rigorous framework,\nsuch as a fully Bayesian setting. Here we attempt to quantify uncertainty in a\nmore formal framework while maintaining the forecast accuracy that makes these\nmodels appealing, by presenting a Bayesian RNN model for nonlinear\nspatio-temporal forecasting. Additionally, we make simple modifications to the\nbasic RNN to help accommodate the unique nature of nonlinear spatio-temporal\ndata. The proposed model is applied to a Lorenz simulation and two real-world\nnonlinear spatio-temporal forecasting applications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 07:27:19 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 02:30:21 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["McDermott", "Patrick L.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1711.00748", "submitter": "Warren Lord", "authors": "Warren M. Lord, Jie Sun and Erik M. Bollt", "title": "Geometric k-nearest neighbor estimation of entropy and mutual\n  information", "comments": null, "journal-ref": null, "doi": "10.1063/1.5011683", "report-no": null, "categories": "math.ST cs.IT math.DS math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of mutual information is used in a wide range of\nscientific problems to quantify dependence between variables. The k-nearest\nneighbor (knn) methods are consistent, and therefore expected to work well for\nlarge sample size. These methods use geometrically regular local volume\nelements. This practice allows maximum localization of the volume elements, but\ncan also induce a bias due to a poor description of the local geometry of the\nunderlying probability measure. We introduce a new class of knn estimators that\nwe call geometric knn estimators (g-knn), which use more complex local volume\nelements to better model the local geometry of the probability measures. As an\nexample of this class of estimators, we develop a g-knn estimator of entropy\nand mutual information based on elliptical volume elements, capturing the local\nstretching and compression common to a wide range of dynamical systems\nattractors. A series of numerical examples in which the thickness of the\nunderlying distribution and the sample sizes are varied suggest that local\ngeometry is a source of problems for knn methods such as the\nKraskov-St\\\"{o}gbauer-Grassberger (KSG) estimator when local geometric effects\ncannot be removed by global preprocessing of the data. The g-knn method\nperforms well despite the manipulation of the local geometry. In addition, the\nexamples suggest that the g-knn estimators can be of particular relevance to\napplications in which the system is large, but data size is limited.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 14:03:37 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 19:50:44 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 22:11:36 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Lord", "Warren M.", ""], ["Sun", "Jie", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1711.00789", "submitter": "Li Ma", "authors": "Meng Li and Li Ma", "title": "Learning Asymmetric and Local Features in Multi-Dimensional Data through\n  Wavelets with Recursive Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective learning of asymmetric and local features in images and other data\nobserved on multi-dimensional grids is a challenging objective critical for a\nwide range of image processing applications involving biomedical and natural\nimages. It requires methods that are sensitive to local details while fast\nenough to handle massive numbers of images of ever increasing sizes. We\nintroduce a probabilistic model-based framework that achieves these objectives\nby incorporating adaptivity into discrete wavelet transforms (DWT) through\nBayesian hierarchical modeling, thereby allowing wavelet bases to adapt to the\ngeometric structure of the data while maintaining the high computational\nscalability of wavelet methods---linear in the sample size (e.g., the\nresolution of an image). We derive a recursive representation of the Bayesian\nposterior model which leads to an exact message passing algorithm to complete\nlearning and inference. While our framework is applicable to a range of\nproblems including multi-dimensional signal processing, compression, and\nstructural learning, we illustrate its work and evaluate its performance in the\ncontext of image reconstruction using real images from the ImageNet database,\ntwo widely used benchmark datasets, and a dataset from retinal optical\ncoherence tomography and compare its performance to state-of-the-art methods\nbased on basis transforms and deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 15:51:16 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 12:22:56 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 02:05:25 GMT"}, {"version": "v4", "created": "Mon, 29 Apr 2019 22:53:35 GMT"}, {"version": "v5", "created": "Fri, 6 Nov 2020 19:23:52 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Meng", ""], ["Ma", "Li", ""]]}, {"id": "1711.00800", "submitter": "Katherine Wilson", "authors": "Jon Wakefield, Geir-Arne Fuglstad, Andrea Riebler, Jessica Godwin,\n  Katie Wilson, and Samuel J. Clark", "title": "Estimating Under Five Mortality in Space and Time in a Developing World\n  Context", "comments": "36 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimates of the under-5 mortality rate (U5MR) in a developing world\ncontext are a key barometer of the health of a nation. This paper describes new\nmodels to analyze survey data on mortality in this context. We are interested\nin both spatial and temporal description, that is, wishing to estimate U5MR\nacross regions and years, and to investigate the association between the U5MR\nand spatially-varying covariate surfaces. We illustrate the methodology by\nproducing yearly estimates for subnational areas in Kenya over the period 1980\n- 2014 using data from demographic health surveys (DHS). We use a binomial\nlikelihood with fixed effects for the urban/rural stratification to account for\nthe complex survey design. We carry out smoothing using Bayesian hierarchical\nmodels with continuous spatial and temporally discrete components. A key\ncomponent of the model is an offset to adjust for bias due to the effects of\nHIV epidemics. Substantively, there has been a sharp decline in U5MR in the\nperiod 1980 - 2014, but large variability in estimated subnational rates\nremains. A priority for future research is understanding this variability.\nTemperature, precipitation and a measure of malaria infection prevalence were\ncandidates for inclusion in the covariate model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 16:20:43 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Wakefield", "Jon", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""], ["Godwin", "Jessica", ""], ["Wilson", "Katie", ""], ["Clark", "Samuel J.", ""]]}, {"id": "1711.00813", "submitter": "Alden Green", "authors": "Alden Green and Cosma Rohilla Shalizi", "title": "Bootstrapping Exchangeable Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two new bootstraps for exchangeable random graphs. One, the\n\"empirical graphon\", is based purely on resampling, while the other, the\n\"histogram stochastic block model\", is a model-based \"sieve\" bootstrap. We show\nthat both of them accurately approximate the sampling distributions of motif\ndensities, i.e., of the normalized counts of the number of times fixed\nsubgraphs appear in the network. These densities characterize the distribution\nof (infinite) exchangeable networks. Our bootstraps therefore give, for the\nfirst time, a valid quantification of uncertainty in inferences about\nfundamental network statistics, and so of parameters identifiable from them.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 16:52:15 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Green", "Alden", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1711.00949", "submitter": "Yoshikazu Terada", "authors": "Yoshikazu Terada and Hidetoshi Shimodaira", "title": "Selective inference for the problem of regions via multiscale bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general approach to selective inference is considered for hypothesis\ntesting of the null hypothesis represented as an arbitrary shaped region in the\nparameter space of multivariate normal model. This approach is useful for\nhierarchical clustering where confidence levels of clusters are calculated only\nfor those appeared in the dendrogram, thus subject to heavy selection bias. Our\ncomputation is based on a raw confidence measure, called bootstrap probability,\nwhich is easily obtained by counting how many times the same cluster appears in\nbootstrap replicates of the dendrogram. We adjust the bias of the bootstrap\nprobability by utilizing the scaling-law in terms of geometric quantities of\nthe region in the abstract parameter space, namely, signed distance and mean\ncurvature. Although this idea has been used for non-selective inference of\nhierarchical clustering, its selective inference version has not been discussed\nin the literature. Our bias-corrected $p$-values are asymptotically\nsecond-order accurate in the large sample theory of smooth boundary surfaces of\nregions, and they are also justified for nonsmooth surfaces such as polyhedral\ncones. The $p$-values are asymptotically equivalent to those of the iterated\nbootstrap but with less computation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 21:39:27 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 03:10:24 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Terada", "Yoshikazu", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1711.01241", "submitter": "Boyu Ren", "authors": "Boyu Ren, Sergio Bacallado, Stefano Favaro, Tommi Vatanen, Curtis\n  Huttenhower and Lorenzo Trippa", "title": "Bayesian Mixed Effects Models for Zero-inflated Compositions in\n  Microbiome Data Analysis", "comments": null, "journal-ref": null, "doi": "10.1214/19-AOAS1295", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting associations between microbial compositions and sample\ncharacteristics is one of the most important tasks in microbiome studies. Most\nof the existing methods apply univariate models to single microbial species\nseparately, with adjustments for multiple hypothesis testing. We propose a\nBayesian analysis for a generalized mixed effects linear model tailored to this\napplication. The marginal prior on each microbial composition is a Dirichlet\nProcess, and dependence across compositions is induced through a linear\ncombination of individual covariates, such as disease biomarkers or the\nsubject's age, and latent factors. The latent factors capture residual\nvariability and their dimensionality is learned from the data in a fully\nBayesian procedure. The proposed model is tested in data analyses and\nsimulation studies with zero-inflated compositions. In these settings, within\neach sample, a large proportion of counts per microbial species are equal to\nzero. In our Bayesian model a priori the probability of compositions with\nabsent microbial species is strictly positive. We propose an efficient\nalgorithm to sample from the posterior and visualizations of model parameters\nwhich reveal associations between covariates and microbial compositions. We\nevaluate the proposed method in simulation studies, and then analyze a\nmicrobiome dataset for infants with type 1 diabetes which contains a large\nproportion of zeros in the sample-specific microbial compositions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:08:39 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 23:16:10 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ren", "Boyu", ""], ["Bacallado", "Sergio", ""], ["Favaro", "Stefano", ""], ["Vatanen", "Tommi", ""], ["Huttenhower", "Curtis", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "1711.01280", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou, Fabrizia Mealli, Corwin M. Zigler", "title": "Causal inference for interfering units with cluster and population level\n  treatment allocation programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interference arises when an individual's potential outcome depends on the\nindividual treatment level, but also on the treatment level of others. A common\nassumption in the causal inference literature in the presence of interference\nis partial interference, implying that the population can be partitioned in\nclusters of individuals whose potential outcomes only depend on the treatment\nof units within the same cluster. Previous literature has defined average\npotential outcomes under counterfactual scenarios where treatments are randomly\nallocated to units within a cluster. However, within clusters there may be\nunits that are more or less likely to receive treatment based on covariates or\nneighbors' treatment. We define new estimands that describe average potential\noutcomes for realistic counterfactual treatment allocation programs, extending\nexisting estimands to take into consideration the units' covariates and\ndependence between units' treatment assignment. We further propose entirely new\nestimands for population-level interventions over the collection of clusters,\nwhich correspond in the motivating setting to regulations at the federal (vs.\ncluster or regional) level. We discuss these estimands, propose unbiased\nestimators and derive asymptotic results as the number of clusters grows.\nFinally, we estimate effects in a comparative effectiveness study of power\nplant emission reduction technologies on ambient ozone pollution.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:01:22 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 14:46:47 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Papadogeorgou", "Georgia", ""], ["Mealli", "Fabrizia", ""], ["Zigler", "Corwin M.", ""]]}, {"id": "1711.01312", "submitter": "Martin Zhang", "authors": "Fei Xia, Martin J. Zhang, James Zou, David Tse", "title": "NeuralFDR: Learning Discovery Thresholds from Hypothesis Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As datasets grow richer, an important challenge is to leverage the full\nfeatures in the data to maximize the number of useful discoveries while\ncontrolling for false positives. We address this problem in the context of\nmultiple hypotheses testing, where for each hypothesis, we observe a p-value\nalong with a set of features specific to that hypothesis. For example, in\ngenetic association studies, each hypothesis tests the correlation between a\nvariant and the trait. We have a rich set of features for each variant (e.g.\nits location, conservation, epigenetics etc.) which could inform how likely the\nvariant is to have a true association. However popular testing approaches, such\nas Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting\n(IHW), either ignore these features or assume that the features are categorical\nor uni-variate. We propose a new algorithm, NeuralFDR, which automatically\nlearns a discovery threshold as a function of all the hypothesis features. We\nparametrize the discovery threshold as a neural network, which enables flexible\nhandling of multi-dimensional discrete and continuous features as well as\nefficient end-to-end optimization. We prove that NeuralFDR has strong false\ndiscovery rate (FDR) guarantees, and show that it makes substantially more\ndiscoveries in synthetic and real datasets. Moreover, we demonstrate that the\nlearned discovery threshold is directly interpretable.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 19:27:11 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 08:22:26 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 06:01:26 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 20:44:38 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Xia", "Fei", ""], ["Zhang", "Martin J.", ""], ["Zou", "James", ""], ["Tse", "David", ""]]}, {"id": "1711.01341", "submitter": "Jason Xu", "authors": "Jason Xu, Eric C. Chi, Kenneth Lange", "title": "Generalized Linear Model Regression under Distance-to-set Penalties", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation in generalized linear models (GLM) is complicated by the presence\nof constraints. One can handle constraints by maximizing a penalized\nlog-likelihood. Penalties such as the lasso are effective in high dimensions,\nbut often lead to unwanted shrinkage. This paper explores instead penalizing\nthe squared distance to constraint sets. Distance penalties are more flexible\nthan algebraic and regularization penalties, and avoid the drawback of\nshrinkage. To optimize distance penalized objectives, we make use of the\nmajorization-minimization principle. Resulting algorithms constructed within\nthis framework are amenable to acceleration and come with global convergence\nguarantees. Applications to shape constraints, sparse regression, and\nrank-restricted matrix regression on synthetic and real data showcase strong\nempirical performance, even under non-convex constraints.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 21:37:23 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Xu", "Jason", ""], ["Chi", "Eric C.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1711.01504", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Ryan P. Browne and Paul D. McNicholas", "title": "Mixtures of Hidden Truncation Hyperbolic Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of factor analyzers model was first introduced over 20 years ago\nand, in the meantime, has been extended to several non-Gaussian analogues. In\ngeneral, these analogues account for situations with heavy tailed and/or skewed\nclusters. An approach is introduced that unifies many of these approaches into\none very general model: the mixture of hidden truncation hyperbolic factor\nanalyzers (MHTHFA) model. In the process of doing this, a hidden truncation\nhyperbolic factor analysis model is also introduced. The MHTHFA model is\nillustrated for clustering as well as semi-supervised classification using two\nreal datasets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 22:41:41 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 19:24:42 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Murray", "Paula M.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1711.01527", "submitter": "Jeffrey Blume", "authors": "Jeffrey D Blume, Leena Choi", "title": "Likelihood Based Study Designs for Time-to-Event Endpoints", "comments": "21 pages; 1 graph; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood methods for measuring statistical evidence obey the likelihood\nprinciple while maintaining bounded and well-controlled frequency properties.\nThese methods lend themselves to sequential study designs because they measure\nthe strength of statistical evidence in accumulating data without needing\nadjustments for the number of planned or unplanned examinations of data.\nHowever, sample size projections have, to date, only been developed for fixed\nsample size designs. In this paper, we consider sequential study designs for\ntime-to-event outcomes assuming likelihood methods will be used to monitor the\nstrength of statistical evidence for efficacy and futility. We develop sample\nsize projections with the aim of controlling the probability of observing\nmisleading evidence under the null and alternative hypotheses, and we show how\nefficacy and futility considerations are managed in this context. We also\nconsider relaxing the requirement of specifying the simple alternative\nhypothesis in advance of the study. Finally, we end with a comparative\nillustration of these methods in a phase II cancer clinical trial that\npreviously was designed within a Bayesian framework.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 03:49:16 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Blume", "Jeffrey D", ""], ["Choi", "Leena", ""]]}, {"id": "1711.01598", "submitter": "Xuan Bi", "authors": "Xuan Bi, Annie Qu and Xiaotong Shen", "title": "Multilayer tensor factorization with applications to recommender systems", "comments": "Accepted by the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have been widely adopted by electronic commerce and\nentertainment industries for individualized prediction and recommendation,\nwhich benefit consumers and improve business intelligence. In this article, we\npropose an innovative method, namely the recommendation engine of multilayers\n(REM), for tensor recommender systems. The proposed method utilizes the\nstructure of a tensor response to integrate information from multiple modes,\nand creates an additional layer of nested latent factors to accommodate\nbetween-subjects dependency. One major advantage is that the proposed method is\nable to address the \"cold-start\" issue in the absence of information from new\ncustomers, new products or new contexts. Specifically, it provides more\neffective recommendations through sub-group information. To achieve scalable\ncomputation, we develop a new algorithm for the proposed method, which\nincorporates a maximum block improvement strategy into the cyclic\nblockwise-coordinate-descent algorithm. In theory, we investigate both\nalgorithmic properties for global and local convergence, along with the\nasymptotic consistency of estimated parameters. Finally, the proposed method is\napplied in simulations and IRI marketing data with 116 million observations of\nproduct sales. Numerical studies demonstrate that the proposed method\noutperforms existing competitors in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 14:40:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Bi", "Xuan", ""], ["Qu", "Annie", ""], ["Shen", "Xiaotong", ""]]}, {"id": "1711.01667", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn, Knut Are Aastveit, Jouchi Nakajima, Mike West", "title": "Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2019.1660171", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the methodology and a detailed case study in use of a class of\nBayesian predictive synthesis (BPS) models for multivariate time series\nforecasting. This extends the recently introduced foundational framework of BPS\nto the multivariate setting, with detailed application in the topical and\nchallenging context of multi-step macroeconomic forecasting in a monetary\npolicy setting. BPS evaluates-- sequentially and adaptively over time-- varying\nforecast biases and facets of miscalibration of individual forecast densities,\nand-- critically-- of time-varying inter-dependencies among them over multiple\nseries. We develop new BPS methodology for a specific subclass of the dynamic\nmultivariate latent factor models implied by BPS theory. Structured dynamic\nlatent factor BPS is here motivated by the application context-- sequential\nforecasting of multiple US macroeconomic time series with forecasts generated\nfrom several traditional econometric time series models. The case study\nhighlights the potential of BPS to improve of forecasts of multiple series at\nmultiple forecast horizons, and its use in learning dynamic relationships among\nforecasting models or agents.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 22:03:43 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 05:17:50 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 00:19:36 GMT"}, {"version": "v4", "created": "Mon, 13 Aug 2018 06:55:01 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["McAlinn", "Kenichiro", ""], ["Aastveit", "Knut Are", ""], ["Nakajima", "Jouchi", ""], ["West", "Mike", ""]]}, {"id": "1711.01674", "submitter": "Hung Hung", "authors": "Hung Hung", "title": "A robust RUV-testing procedure via gamma-divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of differentially expressed genes (DE-genes) is commonly\nconducted in modern biomedical researches. However, unwanted variation\ninevitably arises during the data collection process, which could make the\ndetection results heavily biased. It is suggested to remove the unwanted\nvariation while keeping the biological variation to ensure a reliable analysis\nresult. Removing Unwanted Variation (RUV) is recently proposed for this purpose\nby the virtue of negative control genes. On the other hand, outliers are\nfrequently appear in modern high-throughput genetic data that can heavily\naffect the performances of RUV and its downstream analysis. In this work, we\npropose a robust RUV-testing procedure via gamma-divergence. The advantages of\nour method are twofold: (1) it does not involve any modeling for the outlier\ndistribution, which is applicable to various situations, (2) it is easy to\nimplement in the sense that its robustness is controlled by a single tuning\nparameter gamma of gamma-divergence, and a data-driven criterion is developed\nto select $\\gamma$. In the Gender Study, our method can successfully remove\nunwanted variation, and is able to identify more DE-genes than conventional\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 23:24:53 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hung", "Hung", ""]]}, {"id": "1711.01739", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Rheanna Mainzer", "title": "Two sources of poor coverage of confidence intervals after model\n  selection", "comments": null, "journal-ref": "Statistics and Probability Letters 2018", "doi": "10.1016/j.spl.2018.05.001", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the following two sources of poor coverage of post-model-selection\nconfidence intervals: the preliminary data-based model selection sometimes\nchooses the wrong model and the data used to choose the model is re-used for\nthe construction of the confidence interval.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 06:19:20 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kabaila", "Paul", ""], ["Mainzer", "Rheanna", ""]]}, {"id": "1711.01762", "submitter": "Pietro Coretto", "authors": "Francesco Giordano and Pietro Coretto", "title": "A fast subsampling method for estimating the distribution of\n  signal-to-noise ratio statistics in nonparametric time series regression\n  models", "comments": null, "journal-ref": "Statistical Methods and Applications, 29(3):48-514, 2020", "doi": "10.1007/s10260-019-00487-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal-to-noise ratio (SNR) statistics play a central role in many\napplications. A common situation where SNR is studied is when a continuous time\nsignal is sampled at a fixed frequency with some noise in the background. While\nestimation methods exist, little is known about its distribution when the noise\nis not weakly stationary. In this paper we develop a nonparametric method to\nestimate the distribution of an SNR statistic when the noise belongs to a\nfairly general class of stochastic processes that encompasses both short and\nlong-range dependence, as well as nonlinearities. The method is based on a\ncombination of smoothing and subsampling techniques. Computations are only\noperated at the subsample level, and this allows to manage the typical enormous\nsample size produced by modern data acquisition technologies. We derive\nasymptotic guarantees for the proposed method, and we show the finite sample\nperformance based on numerical experiments. Finally, we propose an application\nto electroencephalography (EEG) data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 07:56:27 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 19:58:12 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 13:09:06 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 07:43:30 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Giordano", "Francesco", ""], ["Coretto", "Pietro", ""]]}, {"id": "1711.01878", "submitter": "Cl\\'ement Chevalier", "authors": "Cl\\'ement Chevalier, David Ginsbourger and Olivia Martius", "title": "Modeling non-stationary extreme dependence with stationary max-stable\n  processes and multidimensional scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the joint distribution of extreme weather events in multiple\nlocations is a challenging task with important applications. In this study, we\nuse max-stable models to study extreme daily precipitation events in\nSwitzerland. The non-stationarity of the spatial process at hand involves\nimportant challenges, which are often dealt with by using a stationary model in\na so-called climate space, with well-chosen covariates. Here, we instead chose\nto warp the weather stations under study in a latent space of higher dimension\nusing multidimensional scaling (MDS). The advantage of this approach is its\nimproved flexibility to reproduce highly non-stationary phenomena, while\nkeeping a tractable stationary spatial model in the latent space. Two model\nfitting approaches, which both use MDS, are presented and compared to a\nclassical approach that relies on composite likelihood maximization in a\nclimate space. Results suggest that the proposed methods better reproduce the\nobserved extremal coefficients and their complex spatial dependence.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 13:30:11 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 11:03:52 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Chevalier", "Cl\u00e9ment", ""], ["Ginsbourger", "David", ""], ["Martius", "Olivia", ""]]}, {"id": "1711.02064", "submitter": "Bo Henry Lindqvist", "authors": "Bo H. Lindqvist and Gunnar Taraldsen", "title": "On the proper treatment of improper distributions", "comments": "Journal of Statistical Planning and Inference, 2017", "journal-ref": null, "doi": "10.1016/j.jspi.2017.09.008", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The axiomatic foundation of probability theory presented by Kolmogorov has\nbeen the basis of modern theory for probability and statistics. In certain\napplications it is, however, necessary or convenient to allow improper\n(unbounded) distributions, which is often done without a theoretical\nfoundation. The paper reviews a recent theory which includes improper\ndistributions, and which is related to Renyi's theory of conditional\nprobability spaces. It is in particular demonstrated how the theory leads to\nsimple explanations of apparent paradoxes known from the Bayesian literature.\nSeveral examples from statistical practice with improper distributions are\ndiscussed in light of the given theoretical results, which also include a\nrecent theory of convergence of proper distributions to improper ones.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:20:08 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lindqvist", "Bo H.", ""], ["Taraldsen", "Gunnar", ""]]}, {"id": "1711.02141", "submitter": "Yanjun Han", "authors": "Yanjun Han, Jiantao Jiao, Tsachy Weissman, Yihong Wu", "title": "Optimal rates of entropy estimation over Lipschitz balls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimax estimation of the entropy of a density\nover Lipschitz balls. Dropping the usual assumption that the density is bounded\naway from zero, we obtain the minimax rates $(n\\ln n)^{-s/(s+d)} + n^{-1/2}$\nfor $0<s\\leq 2$ for densities supported on $[0,1]^d$, where $s$ is the\nsmoothness parameter and $n$ is the number of independent samples. We\ngeneralize the results to densities with unbounded support: given an Orlicz\nfunctions $\\Psi$ of rapid growth (such as the sub-exponential and sub-Gaussian\nclasses), the minimax rates for densities with bounded $\\Psi$-Orlicz norm\nincrease to $(n\\ln n)^{-s/(s+d)} (\\Psi^{-1}(n))^{d(1-d/p(s+d))} + n^{-1/2}$,\nwhere $p$ is the norm parameter in the Lipschitz ball. We also show that the\nintegral-form plug-in estimators with kernel density estimates fail to achieve\nthe minimax rates, and characterize their worst case performances over the\nLipschitz ball.\n  One of the key steps in analyzing the bias relies on a novel application of\nthe Hardy-Littlewood maximal inequality, which also leads to a new inequality\non the Fisher information that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:56:46 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 09:23:01 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 21:25:16 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2019 00:19:33 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Han", "Yanjun", ""], ["Jiao", "Jiantao", ""], ["Weissman", "Tsachy", ""], ["Wu", "Yihong", ""]]}, {"id": "1711.02184", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, Whitney Newey, Sami\n  Stouli and Francis Vella", "title": "Semiparametric Estimation of Structural Functions in Nonseparable\n  Triangular Models", "comments": "45 pages, 4 figures, 1 table, we have added grant funding\n  acknowledgement to v3", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangular systems with nonadditively separable unobserved heterogeneity\nprovide a theoretically appealing framework for the modelling of complex\nstructural relationships. However, they are not commonly used in practice due\nto the need for exogenous variables with large support for identification, the\ncurse of dimensionality in estimation, and the lack of inferential tools. This\npaper introduces two classes of semiparametric nonseparable triangular models\nthat address these limitations. They are based on distribution and quantile\nregression modelling of the reduced form conditional distributions of the\nendogenous variables. We show that average, distribution and quantile\nstructural functions are identified in these systems through a control function\napproach that does not require a large support condition. We propose a\ncomputationally attractive three-stage procedure to estimate the structural\nfunctions where the first two stages consist of quantile or distribution\nregressions. We provide asymptotic theory and uniform inference methods for\neach stage. In particular, we derive functional central limit theorems and\nbootstrap functional central limit theorems for the distribution regression\nestimators of the structural functions. These results establish the validity of\nthe bootstrap for three-stage estimators of structural functions, and lead to\nsimple inference algorithms. We illustrate the implementation and applicability\nof all our methods with numerical simulations and an empirical application to\ndemand analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 21:37:43 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 13:27:23 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 15:48:08 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Newey", "Whitney", ""], ["Stouli", "Sami", ""], ["Vella", "Francis", ""]]}, {"id": "1711.02288", "submitter": "Wei Gao", "authors": "Jun Wang, Wei Gao and Man-Lai Tang", "title": "Estimation of Treatment Effects for Heterogeneous Matched Pairs Data\n  with Probit Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Estimating the effect of medical treatments on subject responses is one of\nthe crucial problems in medical research. Matched-pairs designs are commonly\nimplemented in the field of medical research to eliminate confounding and\nimprove efficiency. In this article, new estimators of treatment effects for\nheterogeneous matched pairs data are proposed. Asymptotic properties of the\nproposed estimators are derived. Simulation studies show that the proposed\nestimators have some advantages over the famous Heckman's estimator and inverse\nprobability weighted (IPW) estimator. We apply the proposed methodologies to a\nblood lead level data set and an acute leukaemia data set.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 05:17:47 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Wang", "Jun", ""], ["Gao", "Wei", ""], ["Tang", "Man-Lai", ""]]}, {"id": "1711.02623", "submitter": "Adrian Dobra", "authors": "Adrian Dobra and Abdolreza Mohammadi", "title": "Loglinear model selection and human mobility", "comments": "30 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for selecting loglinear models were among Steve Fienberg's research\ninterests since the start of his long and fruitful career. After we dwell upon\nthe string of papers focusing on loglinear models that can be partly attributed\nto Steve's contributions and influential ideas, we develop a new algorithm for\nselecting graphical loglinear models that is suitable for analyzing\nhyper-sparse contingency tables. We show how multi-way contingency tables can\nbe used to represent patterns of human mobility. We analyze a dataset of\ngeolocated tweets from South Africa that comprises 46 million\nlatitude/longitude locations of 476,601 Twitter users that is summarized as a\ncontingency table with 214 variables.\n  KEYWORDS: contingency tables, model selection, human mobility, graphical\nmodels, Bayesian structural learning, birth-death processes, pseudo-likelihood\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:38:08 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Dobra", "Adrian", ""], ["Mohammadi", "Abdolreza", ""]]}, {"id": "1711.02725", "submitter": "Pablo Martinez-CAmblor", "authors": "Pablo Martinez-Camblor, Todd A. MacKenzie, Douglas O. Staiger, Philip\n  P. Goodney, A. James O'Malley", "title": "Adjusting for bias introduced by instrumental variable estimation in the\n  Cox Proportional Hazards Model", "comments": "27 pages, 8 figures, 4 tables", "journal-ref": "Biostatistics, 2017", "doi": "10.1093/biostatistics/kxx062", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable (IV) methods are widely used for estimating average\ntreatment effects in the presence of unmeasured confounders. However, the\ncapability of existing IV procedures, and most notably the two-stage residual\ninclusion (2SRI) procedure recommended for use in nonlinear contexts, to\naccount for unmeasured confounders in the Cox proportional hazard model is\nunclear. We show that instrumenting an endogenous treatment induces an\nunmeasured covariate, referred to as an individual frailty in survival analysis\nparlance, which if not accounted for leads to bias. We propose a new procedure\nthat augments 2SRI with an individual frailty and prove that it is consistent\nunder certain conditions. The finite sample-size behavior is studied across a\nbroad set of conditions via Monte Carlo simulations. Finally, the proposed\nmethodology is used to estimate the average effect of carotid endarterectomy\nversus carotid artery stenting on the mortality of patients suffering from\ncarotid artery disease. Results suggest that the 2SRI-frailty estimator\ngenerally reduces the bias of both point and interval estimators compared to\ntraditional 2SRI.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 20:54:56 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Martinez-Camblor", "Pablo", ""], ["MacKenzie", "Todd A.", ""], ["Staiger", "Douglas O.", ""], ["Goodney", "Philip P.", ""], ["O'Malley", "A. James", ""]]}, {"id": "1711.02753", "submitter": "Abdollah Safari", "authors": "Abdollah Safari, Rachel MacKay Altman, Brian Leroux", "title": "Parameter-driven models for time series of count data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a general class of parameter-driven models for time\nseries of counts. A comprehensive simulation study is conducted to evaluate the\naccuracy and efficiency of three estimators: the maximum likelihood estimators\nof the generalized linear model, 2-state finite mixture model, and 2-state\nhidden Markov model. Standard errors for these estimators are derived. Our\nresults show that except in extreme cases, the maximum likelihood estimator of\nthe generalized linear model is an efficient, consistent and robust estimator\nwith a well-behaved estimated standard error. The maximum likelihood estimator\nof the 2-state hidden Markov model is appropriate only when the true model is\nextreme relative to the generalized linear model. Our results are applied to\nproblems concerning polio incidence and daily numbers of epileptic seizures.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 22:33:48 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Safari", "Abdollah", ""], ["Altman", "Rachel MacKay", ""], ["Leroux", "Brian", ""]]}, {"id": "1711.02774", "submitter": "Chibueze Ogbonnaya", "authors": "Chibueze E. Ogbonnaya, Simon P. Preston, Andrew T. A. Wood", "title": "The extended power distribution: A new distribution on $(0, 1)$", "comments": "22 pages, 19 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-parameter bounded probability distribution called the\nextended power distribution. This distribution on $(0, 1)$ is similar to the\nbeta distribution, however there are some advantages which we explore. We\ndefine the moments and quantiles of this distribution and show that it is\npossible to give an $r$-parameter extension of this distribution ($r>2$). We\nalso consider its complementary distribution and show that it has some\nflexibility advantages over the Kumaraswamy and beta distributions. This\ndistribution can be used as an alternative to the Kumaraswamy distribution\nsince it has a closed form for its cumulative function. However, it can be\nfitted to data where there are some samples that are exactly equal to 1, unlike\nthe Kumaraswamy and beta distributions which cannot be fitted to such data or\nmay require some censoring. Applications considered show the extended power\ndistribution performs favourably against the Kumaraswamy distribution in most\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 00:05:39 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Ogbonnaya", "Chibueze E.", ""], ["Preston", "Simon P.", ""], ["Wood", "Andrew T. A.", ""]]}, {"id": "1711.02836", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau, Ajay Jasra and Sumeetpal S. Singh", "title": "Multilevel Monte Carlo for Smoothing via Transport Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider recursive approximations of the smoothing\ndistribution associated to partially observed stochastic differential equations\n(SDEs), which are observed discretely in time. Such models appear in a wide\nvariety of applications including econometrics, finance and engineering. This\nproblem is notoriously challenging, as the smoother is not available\nanalytically and hence require numerical approximation. This usually consists\nby applying a time-discretization to the SDE, for instance the Euler method,\nand then applying a numerical (e.g. Monte Carlo) method to approximate the\nsmoother. This has lead to a vast literature on methodology for solving such\nproblems, perhaps the most popular of which is based upon the particle filter\n(PF) e.g. [9]. In the context of filtering for this class of problems, it is\nwell-known that the particle filter can be improved upon in terms of cost to\nachieve a given mean squared error (MSE) for estimates. This in the sense that\nthe computational effort can be reduced to achieve this target MSE, by using\nmultilevel (ML) methods [12, 13, 18], via the multilevel particle filter (MLPF)\n[16, 20, 21]. For instance, to obtain a MSE of $\\mathcal{O}(\\epsilon^2)$ for\nsome $\\epsilon > 0$ when approximating filtering distributions associated with\nEuler-discretized diffusions with constant diffusion coefficients, the cost of\nthe PF is $\\mathcal{O}(\\epsilon^{-3})$ while the cost of the MLPF is\n$\\mathcal{O}(\\epsilon^{-2}\\log(\\epsilon)^2)$. In this article we consider a new\napproach to replace the particle filter, using transport methods in [27]. In\nthe context of filtering, one expects that the proposed method improves upon\nthe MLPF by yielding, under assumptions, a MSE of $\\mathcal{O}(\\epsilon^2)$ for\na cost of $\\mathcal{O}(\\epsilon^{-2})$. This is established theoretically in an\n\"ideal\" example and numerically in numerous examples.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 05:21:26 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 06:45:04 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Jasra", "Ajay", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1711.02869", "submitter": "Shiwei Lan", "authors": "Shiwei Lan, Andrew Holbrook, Gabriel A. Elias, Norbert J. Fortin,\n  Hernando Ombao and Babak Shahbaba", "title": "Flexible Bayesian Dynamic Modeling of Correlation and Covariance\n  Matrices", "comments": "49 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling correlation (and covariance) matrices can be challenging due to the\npositive-definiteness constraint and potential high-dimensionality. Our\napproach is to decompose the covariance matrix into the correlation and\nvariance matrices and propose a novel Bayesian framework based on modeling the\ncorrelations as products of unit vectors. By specifying a wide range of\ndistributions on a sphere (e.g. the squared-Dirichlet distribution), the\nproposed approach induces flexible prior distributions for covariance matrices\n(that go beyond the commonly used inverse-Wishart prior). For modeling\nreal-life spatio-temporal processes with complex dependence structures, we\nextend our method to dynamic cases and introduce unit-vector Gaussian process\npriors in order to capture the evolution of correlation among components of a\nmultivariate time series. To handle the intractability of the resulting\nposterior, we introduce the adaptive $\\Delta$-Spherical Hamiltonian Monte\nCarlo. We demonstrate the validity and flexibility of our proposed framework in\na simulation study of periodic processes and an analysis of rat's local field\npotential activity in a complex sequence memory task.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 08:17:03 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 02:21:36 GMT"}, {"version": "v3", "created": "Wed, 27 Dec 2017 05:11:16 GMT"}, {"version": "v4", "created": "Sat, 15 Sep 2018 08:21:36 GMT"}, {"version": "v5", "created": "Wed, 16 Jan 2019 05:26:25 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Lan", "Shiwei", ""], ["Holbrook", "Andrew", ""], ["Elias", "Gabriel A.", ""], ["Fortin", "Norbert J.", ""], ["Ombao", "Hernando", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1711.02955", "submitter": "Jakob Knollm\\\"uller", "authors": "Jakob Knollm\\\"uller, Theo Steininger and Torsten A. En{\\ss}lin", "title": "Inference of signals with unknown correlation structure from nonlinear\n  measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to reconstruct autocorrelated signals together with their\nautocorrelation structure from nonlinear, noisy measurements for arbitrary\nmonotonous nonlinear instrument response. In the presented formulation the\nalgorithm provides a significant speedup compared to prior implementations,\nallowing for a wider range of application. The nonlinearity can be used to\nmodel instrument characteristics or to enforce properties on the underlying\nsignal, such as positivity. Uncertainties on any posterior quantities can be\nprovided due to independent samples from an approximate posterior distribution.\nWe demonstrate the methods applicability via simulated and real measurements,\nusing different measurement instruments, nonlinearities and dimensionality.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 14:22:14 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 09:19:25 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Knollm\u00fcller", "Jakob", ""], ["Steininger", "Theo", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1711.03150", "submitter": "Dexter Cahoy", "authors": "Dexter Cahoy and Joseph Sedransk", "title": "Inverse stable prior for exponential models", "comments": null, "journal-ref": "Journal of Statistical Theory and Practice 2019", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of non-conjugate priors as a mixing family of\ndistributions for a parameter (e.g., Poisson or gamma rate, inverse scale or\nprecision of an inverse-gamma, inverse variance of a normal distribution) of an\nexponential subclass of discrete and continuous data distributions. The prior\nclass is proper, nonzero at the origin (unlike the gamma and inverted beta\npriors with shape parameter less than one and Jeffreys prior for a Poisson\nrate), and is easy to generate random numbers from. The prior class also\nprovides flexibility in capturing a wide array of prior beliefs (right-skewed\nand left-skewed) as modulated by a bounded parameter $\\alpha \\in (0, 1).$ The\nresulting posterior family in the single-parameter case can be expressed in\nclosed-form and is proper, making calibration unnecessary. The mixing induced\nby the inverse stable family results to a marginal prior distribution in the\nform of a generalized Mittag-Leffler function, which covers a broad array of\ndistributional shapes. We derive closed-form expressions of some properties\nlike the moment generating function and moments. We propose algorithms to\ngenerate samples from the posterior distribution and calculate the Bayes\nestimators for real data analysis. We formulate the predictive prior and\nposterior distributions. We test the proposed Bayes estimators using Monte\nCarlo simulations. The extension to hierarchical modeling and inverse variance\ncomponents models is straightforward. We can find $\\alpha$ (which acts like a\nsmoothing parameter) values for which the inverse stable can provide better\nshrinkage than the inverted beta prior in many cases. We illustrate the\nmethodology using a real data set, introduce a hyperprior density for the\nhyperparameters, and extend the model to a heavy-tailed distribution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:35:56 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 03:58:52 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 21:51:25 GMT"}, {"version": "v4", "created": "Mon, 29 Jan 2018 03:00:40 GMT"}, {"version": "v5", "created": "Mon, 26 Mar 2018 16:58:50 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Cahoy", "Dexter", ""], ["Sedransk", "Joseph", ""]]}, {"id": "1711.03170", "submitter": "Sungkyu Jung", "authors": "Sungkyu Jung, Jeongyoun Ahn, Yongho Jeon", "title": "Penalized Orthogonal Iteration for Sparse Estimation of Generalized\n  Eigenvalue Problem", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2019.1568014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for sparse estimation of eigenvectors in\ngeneralized eigenvalue problems (GEP). The GEP arises in a number of modern\ndata-analytic situations and statistical methods, including principal component\nanalysis (PCA), multiclass linear discriminant analysis (LDA), canonical\ncorrelation analysis (CCA), sufficient dimension reduction (SDR) and invariant\nco-ordinate selection. We propose to modify the standard generalized orthogonal\niteration with a sparsity-inducing penalty for the eigenvectors. To achieve\nthis goal, we generalize the equation-solving step of orthogonal iteration to a\npenalized convex optimization problem. The resulting algorithm, called\npenalized orthogonal iteration, provides accurate estimation of the true\neigenspace, when it is sparse. Also proposed is a computationally more\nefficient alternative, which works well for PCA and LDA problems. Numerical\nstudies reveal that the proposed algorithms are competitive, and that our\ntuning procedure works well. We demonstrate applications of the proposed\nalgorithm to obtain sparse estimates for PCA, multiclass LDA, CCA and SDR.\nSupplementary materials are available online.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 21:17:39 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 19:15:30 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Jung", "Sungkyu", ""], ["Ahn", "Jeongyoun", ""], ["Jeon", "Yongho", ""]]}, {"id": "1711.03174", "submitter": "Yuqi Gu", "authors": "Yuqi Gu and Gongjun Xu", "title": "The Sufficient and Necessary Condition for the Identifiability and\n  Estimability of the DINA Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Diagnosis Models (CDMs) are useful statistical tools in cognitive\ndiagnosis assessment. However, as many other latent variable models, the CDMs\noften suffer from the non-identifiability issue. This work gives the sufficient\nand necessary condition for the identifiability of the basic DINA model, which\nnot only addresses the open problem in Xu and Zhang (2016, Psychomatrika,\n81:625-649) on the minimal requirement for the identifiability, but also sheds\nlight on the study of more general CDMs, which often cover the DINA as a\nsubmodel. Moreover, we show the identifiability condition ensures the\nconsistent estimation of the model parameters. From a practical perspective,\nthe identifiability condition only depends on the Q-matrix structure and is\neasy to verify, which would provide a guideline for designing statistically\nvalid and estimable cognitive diagnosis tests.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 21:28:46 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 19:48:17 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "1711.03259", "submitter": "Yinqiu He", "authors": "Yinqiu He, Gongjun Xu", "title": "Estimating Tail Probabilities of the Ratio of the Largest Eigenvalue to\n  the Trace of a Wishart Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an efficient Monte Carlo method to estimate the tail\nprobabilities of the ratio of the largest eigenvalue to the trace of the\nWishart matrix, which plays an important role in multivariate data analysis.\nThe estimator is constructed based on a change-of-measure technique and it is\nproved to be asymptotically efficient for both the real and complex Wishart\nmatrices. Simulation studies further show the outperformance of the proposed\nmethod over existing approaches based on asymptotic approximations, especially\nwhen estimating probabilities of rare events.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 05:35:48 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 11:36:17 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["He", "Yinqiu", ""], ["Xu", "Gongjun", ""]]}, {"id": "1711.03342", "submitter": "Yuta Koike", "authors": "Yuta Koike, Yuta Tanoue", "title": "Oracle inequalities for sign constrained generalized linear models", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data have recently been analyzed because of data collection\ntechnology evolution. Although many methods have been developed to gain sparse\nrecovery in the past two decades, most of these methods require selection of\ntuning parameters. As a consequence of this feature, results obtained with\nthese methods heavily depend on the tuning. In this paper we study the\ntheoretical properties of sign-constrained generalized linear models with\nconvex loss function, which is one of the sparse regression methods without\ntuning parameters. Recent studies on this topic have shown that, in the case of\nlinear regression, sign-constrains alone could be as efficient as the oracle\nmethod if the design matrix enjoys a suitable assumption in addition to a\ntraditional compatibility condition. We generalize this kind of result to a\nmuch more general model which encompasses the logistic and quantile\nregressions. We also perform some numerical experiments to confirm theoretical\nfindings obtained in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:05:36 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Koike", "Yuta", ""], ["Tanoue", "Yuta", ""]]}, {"id": "1711.03596", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald and Ambuj Tewari and Predrag Klasnja and Susan\n  Murphy", "title": "Action Centered Contextual Bandits", "comments": "to appear at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandits have become popular as they offer a middle ground between\nvery simple approaches based on multi-armed bandits and very complex approaches\nusing the full power of reinforcement learning. They have demonstrated success\nin web applications and have a rich body of associated theoretical guarantees.\nLinear models are well understood theoretically and preferred by practitioners\nbecause they are not only easily interpretable but also simple to implement and\ndebug. Furthermore, if the linear model is true, we get very strong performance\nguarantees. Unfortunately, in emerging applications in mobile health, the\ntime-invariant linear model assumption is untenable. We provide an extension of\nthe linear model for contextual bandits that has two parts: baseline reward and\ntreatment effect. We allow the former to be complex but keep the latter simple.\nWe argue that this model is plausible for mobile health applications. At the\nsame time, it leads to algorithms with strong performance guarantees as in the\nlinear model setting, while still allowing for complex nonlinear baseline\nmodeling. Our theory is supported by experiments on data gathered in a recently\nconcluded mobile health study.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 20:48:19 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Tewari", "Ambuj", ""], ["Klasnja", "Predrag", ""], ["Murphy", "Susan", ""]]}, {"id": "1711.03611", "submitter": "Isabel Fulcher", "authors": "Isabel R. Fulcher, Ilya Shpitser, Stella Marealle, Eric J. Tchetgen\n  Tchetgen", "title": "Robust inference on population indirect causal effects: the generalized\n  front-door criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard methods for inference about direct and indirect effects require\nstringent no unmeasured confounding assumptions which often fail to hold in\npractice, particularly in observational studies. The goal of this paper is to\nintroduce a new form of indirect effect, the population intervention indirect\neffect (PIIE), that can be nonparametrically identified in the presence of an\nunmeasured common cause of exposure and outcome. This new type of indirect\neffect captures the extent to which the effect of exposure is mediated by an\nintermediate variable under an intervention that holds the component of\nexposure directly influencing the outcome at its observed value. The PIIE is in\nfact the indirect component of the population intervention effect, introduced\nby Hubbard and Van der Laan (2008). Interestingly, our identification criterion\ngeneralizes Judea Pearl's front-door criterion as it does not require no direct\neffect of exposure not mediated by the intermediate variable. For inference, we\ndevelop both parametric and semiparametric methods, including a novel doubly\nrobust semiparametric locally efficient estimator, that perform very well in\nsimulation studies. Finally, the proposed methods are used to measure the\neffectiveness of monetary saving recommendations among women enrolled in a\nmaternal health program in Tanzania.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 21:40:27 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 14:05:14 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 21:13:57 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Fulcher", "Isabel R.", ""], ["Shpitser", "Ilya", ""], ["Marealle", "Stella", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1711.03662", "submitter": "Juan Sosa", "authors": "Juan Sosa and Abel Rodriguez", "title": "A Latent Space Model for Cognitive Social Structures Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for modeling a set of directed, binary\nnetworks in the context of cognitive social structures (CSSs) data. We adopt a\nrelativist approach in which no assumption is made about the existence of an\nunderlying true network. More specifically, we rely on a generalized linear\nmodel that incorporates a bilinear structure to model transitivity effects\nwithin networks, and a hierarchical specification on the bilinear effects to\nborrow information across networks. This is a spatial model, in which the\nperception of each individual about the strength of the relationships can be\nexplained by the perceived position of the actors (themselves and others) on a\nlatent social space. A key goal of the model is to provide a mechanism to\nformally assess the agreement between each actors' perception of their own\nsocial roles with that of the rest of the group. Our experiments with both real\nand simulated data show that the capabilities of our model are comparable with\nor, even superior to, other models for CSS data reported in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 01:26:36 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 01:31:07 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 22:07:40 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Sosa", "Juan", ""], ["Rodriguez", "Abel", ""]]}, {"id": "1711.03701", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald and Seyoung Park and Shuheng Zhou and Alexander\n  Giessing", "title": "Time-dependent spatially varying graphical models, with application to\n  brain fMRI data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an additive model for space-time data that splits\nthe data into a temporally correlated component and a spatially correlated\ncomponent. We model the spatially correlated portion using a time-varying\nGaussian graphical model. Under assumptions on the smoothness of changes in\ncovariance matrices, we derive strong single sample convergence results,\nconfirming our ability to estimate meaningful graphical structures as they\nevolve over time. We apply our methodology to the discovery of time-varying\nspatial structures in human brain fMRI signals.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 05:40:11 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Park", "Seyoung", ""], ["Zhou", "Shuheng", ""], ["Giessing", "Alexander", ""]]}, {"id": "1711.03744", "submitter": "Chuan-Ju Wang", "authors": "Cheng-Der Fuh, Chuan-Ju Wang", "title": "Efficient Exponential Tilting for Portfolio Credit Risk", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of measuring the credit risk in portfolios\nof loans, bonds, and other instruments subject to possible default under\nmulti-factor models. Due to the amount of the portfolio, the heterogeneous\neffect of obligors, and the phenomena that default events are rare and mutually\ndependent, it is difficult to calculate portfolio credit risk either by means\nof direct analysis or crude Monte Carlo under such models. To capture the\nextreme dependence among obligors, we provide an efficient simulation method\nfor multi-factor models with a normal mixture copula that allows the\nmultivariate defaults to have an asymmetric distribution, while most of the\nliterature focuses on simulating one-dimensional cases. To this end, we first\npropose a general account of an importance sampling algorithm based on an\nunconventional exponential embedding, which is related to the classical\nsufficient statistic. Note that this innovative tilting device is more suitable\nfor the multivariate normal mixture model than traditional one-parameter\ntilting methods and is of independent interest. Next, by utilizing a fast\ncomputational method for how the rare event occurs and the proposed importance\nsampling method, we provide an efficient simulation algorithm to estimate the\nprobability that the portfolio incurs large losses under the normal mixture\ncopula. Here the proposed simulation device is based on importance sampling for\na joint probability other than the conditional probability used in previous\nstudies. Theoretical investigations and simulation studies, which include an\nempirical example, are given to illustrate the method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 09:52:49 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 04:14:09 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 14:47:47 GMT"}, {"version": "v4", "created": "Wed, 13 Dec 2017 12:58:59 GMT"}, {"version": "v5", "created": "Sat, 2 Jun 2018 18:08:58 GMT"}, {"version": "v6", "created": "Tue, 9 Apr 2019 00:50:43 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Fuh", "Cheng-Der", ""], ["Wang", "Chuan-Ju", ""]]}, {"id": "1711.03758", "submitter": "Sourabh Bhattacharya", "authors": "Noirrit Kiran Chandra, Richa Singh and Sourabh Bhattacharya", "title": "A Novel Bayesian Multiple Testing Approach to Deregulated miRNA\n  Discovery Harnessing Positional Clustering", "comments": "An updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MicroRNAs (miRNAs) are small non-coding RNAs that function as regulators of\ngene expression. In recent years, there has been a tremendous and growing\ninterest among researchers to investigate the role of miRNAs in normal cellular\nas well as in disease processes. Thus to investigate the role of miRNAs in oral\ncancer, we analyse the expression levels of miRNAs to identify miRNAs with\nstatistically significant differential expression in cancer tissues.\n  In this article, we propose a novel Bayesian hierarchical model of miRNA\nexpression data. Compelling evidences have demonstrated that the transcription\nprocess of miRNAs in human genome is a latent process instrumental for the\nobserved expression levels. We take into account positional clustering of the\nmiRNAs in the analysis and model the latent transcription phenomenon\nnonparametrically by an appropriate Gaussian process.\n  For the testing purpose we employ a novel Bayesian multiple testing method\nwhere we mainly focus on utilizing the dependence structure between the\nhypotheses for better results, while also ensuring optimality in many respects.\nIndeed, our non-marginal method yielded results in accordance with the\nunderlying scientific knowledge which are found to be missed by the very\npopular Benjamini-Hochberg method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 10:17:09 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 18:00:47 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Chandra", "Noirrit Kiran", ""], ["Singh", "Richa", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1711.03838", "submitter": "Shahryar Minhas", "authors": "Arturas Rozenas, Shahryar Minhas, John Ahlquist", "title": "Modeling Asymmetric Relationships from Symmetric Networks", "comments": null, "journal-ref": "Polit. Anal. 27 (2019) 231-236", "doi": "10.1017/pan.2018.41", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many relationships requiring mutual agreement between pairs of actors produce\nobservable networks that are symmetric and undirected. Nevertheless the\nunobserved, asymmetric network is often of primary scientific interest. We\npropose a method that probabilistically reconstructs the unobserved, asymmetric\nnetwork from the observed, symmetric graph using a regression-based framework\nthat allows for inference on predictors of actors' decisions. We apply this\nmodel to the bilateral investment treaty network. Our approach extracts\npolitically relevant information about the network structure that is\ninaccessible to alternative approaches and has superior predictive performance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 14:51:27 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 12:25:06 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Rozenas", "Arturas", ""], ["Minhas", "Shahryar", ""], ["Ahlquist", "John", ""]]}, {"id": "1711.03884", "submitter": "Briana Stephenson", "authors": "Briana Stephenson, Amy Herring, Andrew Olshan", "title": "Robust Clustering with Subpopulation-specific Deviations", "comments": "45 pages, 6 figures", "journal-ref": "Journal of the American Statistical Association (2019)", "doi": "10.1080/01621459.2019.1611583", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The National Birth Defects Prevention Study (NBDPS) is a case-control study\nof birth defects conducted across 10 U.S. states. Researchers are interested in\ncharacterizing the etiologic role of maternal diet, collected using a food\nfrequency questionnaire. Because diet is multi-dimensional, dimension reduction\nmethods such as cluster analysis are often used to summarize dietary patterns.\nIn a large, heterogeneous population, traditional clustering methods, such as\nlatent class analysis, used to estimate dietary patterns can produce a large\nnumber of clusters due to a variety of factors, including study size and\nregional diversity. These factors result in a loss of interpretability of\npatterns that may differ due to minor consumption changes.\n  Based on adaptation of the local partition process, we propose a new method,\nRobust Profile Clustering, to handle these data complexities. Here,\nparticipants may be clustered at two levels: (1) globally, where women are\nassigned to an overall population-level cluster via an overfitted finite\nmixture model, and (2) locally, where regional variations in diet are\naccommodated via a beta-Bernoulli process dependent on subpopulation\ndifferences. We use our method to analyze the NBDPS data, deriving\npre-pregnancy dietary patterns for women in the NBDPS while accounting for\nregional variability.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:40:36 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 05:11:01 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2018 14:36:15 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Stephenson", "Briana", ""], ["Herring", "Amy", ""], ["Olshan", "Andrew", ""]]}, {"id": "1711.03890", "submitter": "Filip Elvander", "authors": "Filip Elvander, Andreas Jakobsson, and Johan Karlsson", "title": "Interpolation and Extrapolation of Toeplitz Matrices via Optimal Mass\n  Transport", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 66, no. 20, (2018),\n  pp. 5285 - 5298", "doi": "10.1109/TSP.2018.2866432", "report-no": null, "categories": "eess.SP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel method for quantifying distances between\nToeplitz structured covariance matrices. By exploiting the spectral\nrepresentation of Toeplitz matrices, the proposed distance measure is defined\nbased on an optimal mass transport problem in the spectral domain. This may\nthen be interpreted in the covariance domain, suggesting a natural way of\ninterpolating and extrapolating Toeplitz matrices, such that the positive\nsemi-definiteness and the Toeplitz structure of these matrices are preserved.\nThe proposed distance measure is also shown to be contractive with respect to\nboth additive and multiplicative noise, and thereby allows for a quantification\nof the decreased distance between signals when these are corrupted by noise.\nFinally, we illustrate how this approach can be used for several applications\nin signal processing. In particular, we consider interpolation and\nextrapolation of Toeplitz matrices, as well as clustering problems and tracking\nof slowly varying stochastic processes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:46:36 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 18:08:41 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Elvander", "Filip", ""], ["Jakobsson", "Andreas", ""], ["Karlsson", "Johan", ""]]}, {"id": "1711.03918", "submitter": "Zachary del Rosario", "authors": "Zachary del Rosario and Minyong Lee and Gianluca Iaccarino", "title": "Lurking Variable Detection via Dimensional Analysis", "comments": "28 pages; full simulation codes provided in ancillary document for\n  reproducibility", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lurking variables represent hidden information, and preclude a full\nunderstanding of phenomena of interest. Detection is usually based on\nserendipity -- visual detection of unexplained, systematic variation. However,\nthese approaches are doomed to fail if the lurking variables do not vary. In\nthis article, we address these challenges by introducing formal hypothesis\ntests for the presence of lurking variables, based on Dimensional Analysis.\nThese procedures utilize a modified form of the Buckingham Pi theorem to\nprovide structure for a suitable null hypothesis. We present analytic tools for\nreasoning about lurking variables in physical phenomena, construct procedures\nto handle cases of increasing complexity, and present examples of their\napplication to engineering problems. The results of this work enable\nalgorithm-driven lurking variable detection, complementing a traditionally\ninspection-based approach.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 16:56:52 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 15:37:42 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["del Rosario", "Zachary", ""], ["Lee", "Minyong", ""], ["Iaccarino", "Gianluca", ""]]}, {"id": "1711.03959", "submitter": "Mika Meitz", "authors": "Mika Meitz and Pentti Saikkonen", "title": "Testing for observation-dependent regime switching in mixture\n  autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for regime switching when the regime switching probabilities are\nspecified either as constants (`mixture models') or are governed by a\nfinite-state Markov chain (`Markov switching models') are long-standing\nproblems that have also attracted recent interest. This paper considers testing\nfor regime switching when the regime switching probabilities are time-varying\nand depend on observed data (`observation-dependent regime switching').\nSpecifically, we consider the likelihood ratio test for observation-dependent\nregime switching in mixture autoregressive models. The testing problem is\nhighly nonstandard, involving unidentified nuisance parameters under the null,\nparameters on the boundary, singular information matrices, and higher-order\napproximations of the log-likelihood. We derive the asymptotic null\ndistribution of the likelihood ratio test statistic in a general mixture\nautoregressive setting using high-level conditions that allow for various forms\nof dependence of the regime switching probabilities on past observations, and\nwe illustrate the theory using two particular mixture autoregressive models.\nThe likelihood ratio test has a nonstandard asymptotic distribution that can\neasily be simulated, and Monte Carlo studies show the test to have satisfactory\nfinite sample size and power properties.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:40:36 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Meitz", "Mika", ""], ["Saikkonen", "Pentti", ""]]}, {"id": "1711.03962", "submitter": "Brian Vegetabile", "authors": "Brian Vegetabile, Jenny Molet, Tallie Z. Baram, and Hal Stern", "title": "Estimating the Entropy Rate of Finite Markov Chains with Application to\n  Behavior Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictability of behavior has emerged an an important characteristic in many\nfields including biology, medicine, and marketing. Behavior can be recorded as\na sequence of actions performed by an individual over a given time period. This\nsequence of actions can often be modeled as a stationary time-homogeneous\nMarkov chain and the predictability of the individual's behavior can be\nquantified by the entropy rate of the process. This paper provides a\ncomprehensive investigation of three estimators of the entropy rate of finite\nMarkov processes and a bootstrap procedure for providing standard errors. The\nfirst two methods directly estimate the entropy rate through estimates of the\ntransition matrix and stationary distribution of the process; the methods\ndiffer in the technique used to estimate the stationary distribution. The third\nmethod is related to the sliding-window Lempel-Ziv (SWLZ) compression\nalgorithm. The first two methods achieve consistent estimates of the true\nentropy rate for reasonably short observed sequences, but are limited by\nrequiring a priori specification of the order of the process. The method based\non the SWLZ algorithm does not require specifying the order of the process and\nis optimal in the limit of an infinite sequence, but is biased for short\nsequences. When used together, the methods can provide a clear picture of the\nentropy rate of an individual's behavior.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:51:17 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Vegetabile", "Brian", ""], ["Molet", "Jenny", ""], ["Baram", "Tallie Z.", ""], ["Stern", "Hal", ""]]}, {"id": "1711.04092", "submitter": "Indranil Sahoo", "authors": "Indranil Sahoo, Joseph Guinness and Brian J. Reich", "title": "A Test for Isotropy on a Sphere using Spherical Harmonic Functions", "comments": null, "journal-ref": "Statistica Sinica 29 (2019): 1253-1276", "doi": "10.5705/ss.202017.0475", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of geostatistical data is often based on the assumption that the\nspatial random field is isotropic. This assumption, if erroneous, can adversely\naffect model predictions and statistical inference. Nowadays many applications\nconsider data over the entire globe and hence it is necessary to check the\nassumption of isotropy on a sphere. In this paper, a test for spatial isotropy\non a sphere is proposed. The data are first projected onto the set of spherical\nharmonic functions. Under isotropy, the spherical harmonic coefficients are\nuncorrelated whereas they are correlated if the underlying fields are not\nisotropic. This motivates a test based on the sample correlation matrix of the\nspherical harmonic coefficients. In particular, we use the largest eigenvalue\nof the sample correlation matrix as the test statistic. Extensive simulations\nare conducted to assess the Type I errors of the test under different\nscenarios. We show how temporal correlation affects the test and provide a\nmethod for handling temporal correlation. We also gauge the power of the test\nas we move away from isotropy. The method is applied to the near-surface air\ntemperature data which is part of the HadCM3 model output. Although we do not\nexpect global temperature fields to be isotropic, we propose several\nanisotropic models with increasing complexity, each of which has an isotropic\nprocess as model component and we apply the test to the isotropic component in\na sequence of such models as a method of determining how well the models\ncapture the anisotropy in the fields.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 07:58:01 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 03:13:29 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Sahoo", "Indranil", ""], ["Guinness", "Joseph", ""], ["Reich", "Brian J.", ""]]}, {"id": "1711.04155", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Art B. Owen", "title": "Deterministic parallel analysis: An improved method for selecting\n  factors and principal components", "comments": "Made title consistent with published version", "journal-ref": "JRSS-B, Volume 81, Issue 1, February 2019, Pages 163-183", "doi": "10.1111/rssb.12301", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis and principal component analysis (PCA) are used in many\napplication areas. The first step, choosing the number of components, remains a\nserious challenge. Our work proposes improved methods for this important\nproblem. One of the most popular state-of-the-art methods is Parallel Analysis\n(PA), which compares the observed factor strengths to simulated ones under a\nnoise-only model. This paper proposes improvements to PA. We first de-randomize\nit, proposing Deterministic Parallel Analysis (DPA), which is faster and more\nreproducible than PA. Both PA and DPA are prone to a shadowing phenomenon in\nwhich a strong factor makes it hard to detect smaller but more interesting\nfactors. We propose deflation to counter shadowing. We also propose to raise\nthe decision threshold to improve estimation accuracy. We prove several\nconsistency results for our methods, and test them in simulations. We also\nillustrate our methods on data from the Human Genome Diversity Project, where\nthey significantly improve the accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 15:35:32 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 21:56:58 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 21:09:56 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 00:52:55 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Dobriban", "Edgar", ""], ["Owen", "Art B.", ""]]}, {"id": "1711.04268", "submitter": "Ali Tajer", "authors": "Javad Heydari, Ali Tajer, H. Vincent Poor", "title": "Active Sampling for the Quickest Detection of Markov Networks", "comments": "50 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider $n$ random variables forming a Markov random field (MRF). The true\nmodel of the MRF is unknown, and it is assumed to belong to a binary set. The\nobjective is to sequentially sample the random variables (one-at-a-time) such\nthat the true MRF model can be detected with the fewest number of samples,\nwhile in parallel, the decision reliability is controlled. The core element of\nan optimal decision process is a rule for selecting and sampling the random\nvariables over time. Such a process, at every time instant and adaptively to\nthe collected data, selects the random variable that is expected to be most\ninformative about the model, rendering an overall minimized number of samples\nrequired for reaching a reliable decision. The existing studies on detecting\nMRF structures generally sample the entire network at the same time and focus\non designing optimal detection rules without regard to the data-acquisition\nprocess. This paper characterizes the sampling process for general MRFs, which,\nin conjunction with the sequential probability ratio test, is shown to be\noptimal in the asymptote of large $n$. The critical insight in designing the\nsampling process is devising an information measure that captures the\ndecisions' inherent statistical dependence over time. Furthermore, when the\nMRFs can be modeled by acyclic probabilistic graphical models, the sampling\nrule is shown to take a computationally simple form. Performance analysis for\nthe general case is provided, and the results are interpreted in several\nspecial cases: Gaussian MRFs, non-asymptotic regimes, connection to Chernoff's\nrule to controlled (active) sensing, and the problem of cluster detection.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 10:42:01 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 06:37:15 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 01:30:24 GMT"}, {"version": "v4", "created": "Sun, 2 Aug 2020 20:36:23 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Heydari", "Javad", ""], ["Tajer", "Ali", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1711.04333", "submitter": "David Bolin", "authors": "David Bolin and Kristin Kirchner", "title": "The rational SPDE approach for Gaussian random fields with general\n  smoothness", "comments": "28 pages, 4 figures", "journal-ref": "J. Comput. Graph. Statist. (2019)", "doi": "10.1080/10618600.2019.1665537", "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach for modeling and inference in spatial statistics is to\nrepresent Gaussian random fields as solutions to stochastic partial\ndifferential equations (SPDEs) of the form $L^{\\beta}u = \\mathcal{W}$, where\n$\\mathcal{W}$ is Gaussian white noise, $L$ is a second-order differential\noperator, and $\\beta>0$ is a parameter that determines the smoothness of $u$.\nHowever, this approach has been limited to the case $2\\beta\\in\\mathbb{N}$,\nwhich excludes several important models and makes it necessary to keep $\\beta$\nfixed during inference.\n  We propose a new method, the rational SPDE approach, which in spatial\ndimension $d\\in\\mathbb{N}$ is applicable for any $\\beta>d/4$, and thus remedies\nthe mentioned limitation. The presented scheme combines a finite element\ndiscretization with a rational approximation of the function $x^{-\\beta}$ to\napproximate $u$. For the resulting approximation, an explicit rate of\nconvergence to $u$ in mean-square sense is derived. Furthermore, we show that\nour method has the same computational benefits as in the restricted case\n$2\\beta\\in\\mathbb{N}$. Several numerical experiments and a statistical\napplication are used to illustrate the accuracy of the method, and to show that\nit facilitates likelihood-based inference for all model parameters including\n$\\beta$.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 18:15:20 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 10:38:11 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 16:45:57 GMT"}, {"version": "v4", "created": "Sun, 1 Dec 2019 14:20:28 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bolin", "David", ""], ["Kirchner", "Kristin", ""]]}, {"id": "1711.04349", "submitter": "Hao Chen", "authors": "Jingru Zhang and Hao Chen", "title": "Graph-Based Two-Sample Tests for Data with Repeated Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the regime of two-sample comparison, tests based on a graph constructed on\nobservations by utilizing similarity information among them is gaining\nattention due to their flexibility and good performances for\nhigh-dimensional/non-Euclidean data. However, when there are repeated\nobservations, these graph-based tests could be problematic as they are\nversatile to the choice of the similarity graph. We propose extended\ngraph-based test statistics to resolve this problem. The analytic p-value\napproximations to these extended graph-based tests are derived to facilitate\nthe application of these tests to large datasets. The new tests are illustrated\nin the analysis of a phone-call network dataset. All tests are implemented in\nan R package gTests.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 20:35:59 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 05:44:41 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Zhang", "Jingru", ""], ["Chen", "Hao", ""]]}, {"id": "1711.04359", "submitter": "Maria Rizzo", "authors": "Songzi Li and Maria L. Rizzo", "title": "K-groups: A Generalization of K-means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of distribution-based clustering algorithms, called\nk-groups, based on energy distance between samples. The energy distance\nclustering criterion assigns observations to clusters according to a\nmulti-sample energy statistic that measures the distance between distributions.\nThe energy distance determines a consistent test for equality of distributions,\nand it is based on a population distance that characterizes equality of\ndistributions. The k-groups procedure therefore generalizes the k-means method,\nwhich separates clusters that have different means. We propose two k-groups\nalgorithms: k-groups by first variation; and k-groups by second variation. The\nimplementation of k-groups is partly based on Hartigan and Wong's algorithm for\nk-means. The algorithm is generalized from moving one point on each iteration\n(first variation) to moving $m$ $(m > 1)$ points. For univariate data, we prove\nthat Hartigan and Wong's k-means algorithm is a special case of k-groups by\nfirst variation. The simulation results from univariate and multivariate cases\nshow that our k-groups algorithms perform as well as Hartigan and Wong's\nk-means algorithm when clusters are well-separated and normally distributed.\nMoreover, both k-groups algorithms perform better than k-means when data does\nnot have a finite first moment or data has strong skewness and heavy tails. For\nnon--spherical clusters, both k-groups algorithms performed better than k-means\nin high dimension, and k-groups by first variation is consistent as dimension\nincreases. In a case study on dermatology data with 34 features, both k-groups\nalgorithms performed better than k-means.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 21:18:12 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Li", "Songzi", ""], ["Rizzo", "Maria L.", ""]]}, {"id": "1711.04376", "submitter": "Fl\\'avio Gon\\c{c}alves", "authors": "N\\'ivea B. da Silva, Marcos O. Prates, Fl\\'avio B. Gon\\c{c}alves", "title": "Bayesian linear regression models with flexible error distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel methodology based on finite mixtures of\nStudent-t distributions to model the errors' distribution in linear regression\nmodels. The novelty lies on a particular hierarchical structure for the mixture\ndistribution in which the first level models the number of modes, responsible\nto accommodate multimodality and skewness features, and the second level models\ntail behavior. Moreover, the latter is specified in a way that no degrees of\nfreedom parameters are estimated and, therefore, the known statistical\ndifficulties when dealing with those parameters is mitigated, and yet model\nflexibility is not compromised. Inference is performed via Markov chain Monte\nCarlo and simulation studies are conducted to evaluate the performance of the\nproposed methodology. The analysis of two real data sets are also presented.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 22:47:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["da Silva", "N\u00edvea B.", ""], ["Prates", "Marcos O.", ""], ["Gon\u00e7alves", "Fl\u00e1vio B.", ""]]}, {"id": "1711.04392", "submitter": "Yuan Liao", "authors": "Yuan Liao, Xiye Yang", "title": "Uniform Inference for Characteristic Effects of Large Continuous-Time\n  Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider continuous-time models with a large panel of moment conditions,\nwhere the structural parameter depends on a set of characteristics, whose\neffects are of interest. The leading example is the linear factor model in\nfinancial economics where factor betas depend on observed characteristics such\nas firm specific instruments and macroeconomic variables, and their effects\npick up long-run time-varying beta fluctuations. We specify the factor betas as\nthe sum of characteristic effects and an orthogonal idiosyncratic parameter\nthat captures high-frequency movements. It is often the case that researchers\ndo not know whether or not the latter exists, or its strengths, and thus the\ninference about the characteristic effects should be valid uniformly over a\nbroad class of data generating processes for idiosyncratic parameters. We\nconstruct our estimation and inference in a two-step continuous-time GMM\nframework. It is found that the limiting distribution of the estimated\ncharacteristic effects has a discontinuity when the variance of the\nidiosyncratic parameter is near the boundary (zero), which makes the usual\n\"plug-in\" method using the estimated asymptotic variance only valid pointwise\nand may produce either over- or under- coveraging probabilities. We show that\nthe uniformity can be achieved by cross-sectional bootstrap. Our procedure\nallows both known and estimated factors, and also features a bias correction\nfor the effect of estimating unknown factors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 02:25:33 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 20:27:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liao", "Yuan", ""], ["Yang", "Xiye", ""]]}, {"id": "1711.04432", "submitter": "Jiannan Lu", "authors": "Jiannan Lu", "title": "Sharpening randomization-based causal inference for $2^2$ factorial\n  designs with binary outcomes", "comments": "Accepted by Statistical Methods in Medical Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical research, a scenario often entertained is randomized controlled\n$2^2$ factorial design with a binary outcome. By utilizing the concept of\npotential outcomes, Dasgupta et al. (2015) proposed a randomization-based\ncausal inference framework, allowing flexible and simultaneous estimations and\ninferences of the factorial effects. However, a fundamental challenge that\nDasgupta et al. (2015)'s proposed methodology faces is that the sampling\nvariance of the randomization-based factorial effect estimator is\nunidentifiable, rendering the corresponding classic \"Neymanian\" variance\nestimator suffering from over-estimation. To address this issue, for randomized\ncontrolled $2^2$ factorial designs with binary outcomes, we derive the sharp\nlower bound of the sampling variance of the factorial effect estimator, which\nleads to a new variance estimator that sharpens the finite-population Neymanian\ncausal inference. We demonstrate the advantages of the new variance estimator\nthrough a series of simulation studies, and apply our newly proposed\nmethodology to two real-life datasets from randomized clinical trials, where we\ngain new insights.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 06:21:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Lu", "Jiannan", ""]]}, {"id": "1711.04441", "submitter": "Mostafa Reisi", "authors": "Mostafa Reisi Gahrooei and Kamran Paynabar", "title": "Change Detection in a Dynamic Stream of Attributed Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While anomaly detection in static networks has been extensively studied, only\nrecently, researchers have focused on dynamic networks. This trend is mainly\ndue to the capacity of dynamic networks in representing complex physical,\nbiological, cyber, and social systems. This paper proposes a new methodology\nfor modeling and monitoring of dynamic attributed networks for quick detection\nof temporal changes in network structures. In this methodology, the generalized\nlinear model (GLM) is used to model static attributed networks. This model is\nthen combined with a state transition equation to capture the dynamic behavior\nof the system. Extended Kalman filter (EKF) is used as an online, recursive\ninference procedure to predict and update network parameters over time. In\norder to detect changes in the underlying mechanism of edge formation,\nprediction residuals are monitored through an Exponentially Weighted Moving\nAverage (EWMA) control chart. The proposed modeling and monitoring procedure is\nexamined through simulations for attributed binary and weighted networks. The\nemail communication data from the Enron corporation is used as a case study to\nshow how the method can be applied in real-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 06:58:07 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Gahrooei", "Mostafa Reisi", ""], ["Paynabar", "Kamran", ""]]}, {"id": "1711.04462", "submitter": "Shogo Nakakita", "authors": "Shogo H. Nakakita, Masayuki Uchida", "title": "Adaptive estimation and noise detection for an ergodic diffusion with\n  observation noises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We research adaptive maximum likelihood-type estimation for an ergodic\ndiffusion process where the observation is contaminated by noise. This\nmethodology leads to the asymptotic independence of the estimators for the\nvariance of observation noise, the diffusion parameter and the drift one of the\nlatent diffusion process. Moreover, it can lessen the computational burden\ncompared to simultaneous maximum likelihood-type estimation. In addition to\nadaptive estimation, we propose a test to see if noise exists or not, and\nanalyse real data as the example such that data contains observation noise with\nstatistical significance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 08:00:32 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 11:53:10 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1711.04632", "submitter": "Daniel W. Meyer", "authors": "Daniel W. Meyer", "title": "(Un)Conditional Sample Generation Based on Distribution Element Trees", "comments": "published online in the Journal of Computational and Graphical\n  Statistics", "journal-ref": null, "doi": "10.1080/10618600.2018.1482768", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, distribution element trees (DETs) were introduced as an accurate\nand computationally efficient method for density estimation. In this work, we\ndemonstrate that the DET formulation promotes an easy and inexpensive way to\ngenerate random samples similar to a smooth bootstrap. These samples can be\ngenerated unconditionally, but also, without further complications,\nconditionally utilizing available information about certain probability-space\ncomponents.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 15:22:45 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 06:35:42 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Meyer", "Daniel W.", ""]]}, {"id": "1711.04702", "submitter": "Deisy Morselli Gysi", "authors": "Deisy Morselli Gysi, Andre Voigt, Tiago de Miranda Fragoso, Eivind\n  Almaas and Katja Nowick", "title": "wTO: an R package for computing weighted topological overlap and\n  consensus networks with an integrated visualization tool", "comments": null, "journal-ref": null, "doi": "10.1186/s12859-018-2351-7", "report-no": null, "categories": "q-bio.MN stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network analyses, such as of gene co-expression networks, metabolic networks\nand ecological networks have become a central approach for the systems-level\nstudy of biological data. Several software packages exist for generating and\nanalyzing such networks, either from correlation scores or the absolute value\nof a transformed score called weighted topological overlap (wTO). However,\nsince gene regulatory processes can up- or down-regulate genes, it is of great\ninterest to explicitly consider both positive and negative correlations when\nconstructing a gene co-expression network. Here, we present an R package for\ncalculating the wTO, that, in contrast to existing packages, explicitly\naddresses the sign of the wTO values, and is thus especially valuable for the\nanalysis of gene regulatory networks. The package includes the calculation of\np-values (raw and adjusted) for each pairwise gene score. Our package also\nallows the calculation of networks from time series (without replicates). Since\nnetworks from independent datasets (biological repeats or related studies) are\nnot the same due to technical and biological noise in the data, we\nadditionally, incorporated a novel method for calculating a consensus network\n(CN) from two or more networks into our R package. We compare our new wTO\npackage to state of art packages and demonstrate the application of the wTO and\nCN functions using 3 independently derived datasets from healthy human\npre-frontal cortex samples. To showcase an example for the time series\napplication we utilized a metagenomics data set. In this work, we developed a\nsoftware package that allows the computation of wTO networks, CNs and a\nvisualization tool in the R statistical environment. It is publicly available\non CRAN repositories under the GPL-2 Open Source License\n(https://cran.r-project.org/web/packages/wTO/).\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:54:57 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 19:44:12 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gysi", "Deisy Morselli", ""], ["Voigt", "Andre", ""], ["Fragoso", "Tiago de Miranda", ""], ["Almaas", "Eivind", ""], ["Nowick", "Katja", ""]]}, {"id": "1711.04749", "submitter": "Cristian Oliva", "authors": "Cristian Oliva-Aviles, Mary C. Meyer, Jean D. Opsomer", "title": "Checking validity of monotone domain mean estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimates of population characteristics such as domain means are often\nexpected to follow monotonicity assumptions. Recently, a method to adaptively\npool neighboring domains was proposed, which ensures that the resulting domain\nmean estimates follow monotone constraints. The method leads to asymptotically\nvalid estimation and inference, and can lead to substantial improvements in\nefficiency, in comparison with unconstrained domain estimators. However,\nassuming incorrect shape constraints could lead to biased estimators. Here, we\ndevelop the Cone Information Criterion for Survey Data (CICs) as a diagnostic\nmethod to measure monotonicity departures on population domain means. We show\nthat the criterion leads to a consistent methodology that makes an\nasymptotically correct decision choosing between unconstrained and constrained\ndomain mean estimators.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 18:37:50 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 22:58:02 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Oliva-Aviles", "Cristian", ""], ["Meyer", "Mary C.", ""], ["Opsomer", "Jean D.", ""]]}, {"id": "1711.04761", "submitter": "Jian Shi", "authors": "Pengcheng Zeng, Jian Qing Shi, and Won-Seok Kim", "title": "Simultaneous Registration and Clustering for Multi-dimensional\n  Functional Data", "comments": "36 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering for functional data with misaligned problems has drawn much\nattention in the last decade. Most methods do the clustering after those\nfunctional data being registered and there has been little research using both\nfunctional and scalar variables. In this paper, we propose a simultaneous\nregistration and clustering (SRC) model via two-level models, allowing the use\nof both types of variables and also allowing simultaneous registration and\nclustering. For the data collected from subjects in different unknown groups, a\nGaussian process functional regression model with time warping is used as the\nfirst level model; an allocation model depending on scalar variables is used as\nthe second level model providing further information over the groups. The\nformer carries out registration and modeling for the multi-dimensional\nfunctional data (2D or 3D curves) at the same time. This methodology is\nimplemented using an EM algorithm, and is examined on both simulated data and\nreal data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 18:57:07 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Zeng", "Pengcheng", ""], ["Shi", "Jian Qing", ""], ["Kim", "Won-Seok", ""]]}, {"id": "1711.04793", "submitter": "Vitaliy Oryshchenko", "authors": "Vitaliy Oryshchenko and Richard J. Smith", "title": "Improved Density and Distribution Function Estimation", "comments": "32 pages, 3 figures, 3 tables", "journal-ref": "Electron. J. Statist. 13 (2019), no. 2, 3943--3984", "doi": "10.1214/19-EJS1619", "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given additional distributional information in the form of moment\nrestrictions, kernel density and distribution function estimators with implied\ngeneralised empirical likelihood probabilities as weights achieve a reduction\nin variance due to the systematic use of this extra information. The particular\ninterest here is the estimation of densities or distributions of (generalised)\nresiduals in semi-parametric models defined by a finite number of moment\nrestrictions. Such estimates are of great practical interest, being potentially\nof use for diagnostic purposes, including tests of parametric assumptions on an\nerror distribution, goodness-of-fit tests or tests of overidentifying moment\nrestrictions. The paper gives conditions for the consistency and describes the\nasymptotic mean squared error properties of the kernel density and distribution\nestimators proposed in the paper. A simulation study evaluates the small sample\nperformance of these estimators. Supplements provide analytic examples to\nillustrate situations where kernel weighting provides a reduction in variance\ntogether with proofs of the results in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 19:02:26 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 15:40:05 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Oryshchenko", "Vitaliy", ""], ["Smith", "Richard J.", ""]]}, {"id": "1711.04817", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova and Tianying Wang", "title": "Sparse quadratic classification rules via linear dimension reduction", "comments": null, "journal-ref": "Journal of Multivariate Analysis 2019, Vol. 169, 278-299", "doi": "10.1016/j.jmva.2018.09.011", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of high-dimensional classification between the two\ngroups with unequal covariance matrices. Rather than estimating the full\nquadratic discriminant rule, we propose to perform simultaneous variable\nselection and linear dimension reduction on original data, with the subsequent\napplication of quadratic discriminant analysis on the reduced space. In\ncontrast to quadratic discriminant analysis, the proposed framework doesn't\nrequire estimation of precision matrices and scales linearly with the number of\nmeasurements, making it especially attractive for the use on high-dimensional\ndatasets. We support the methodology with theoretical guarantees on variable\nselection consistency, and empirical comparison with competing approaches. We\napply the method to gene expression data of breast cancer patients, and confirm\nthe crucial importance of ESR1 gene in differentiating estrogen receptor\nstatus.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 19:52:06 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 22:09:21 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Wang", "Tianying", ""]]}, {"id": "1711.04818", "submitter": "Xiaohao Cai", "authors": "Xiaohao Cai, Marcelo Pereyra, Jason D. McEwen", "title": "Uncertainty quantification for radio interferometric imaging: I.\n  proximal MCMC methods", "comments": "16 pages, 7 figures, see companion article in this arXiv listing", "journal-ref": "Monthly Notices of the Royal Astronomical Society, Volume 480,\n  Issue 3, 1 November 2018, Pages 4154--4169", "doi": "10.1093/mnras/sty2004", "report-no": null, "categories": "astro-ph.IM cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification is a critical missing component in radio\ninterferometric imaging that will only become increasingly important as the\nbig-data era of radio interferometry emerges. Since radio interferometric\nimaging requires solving a high-dimensional, ill-posed inverse problem,\nuncertainty quantification is difficult but also critical to the accurate\nscientific interpretation of radio observations. Statistical sampling\napproaches to perform Bayesian inference, like Markov Chain Monte Carlo (MCMC)\nsampling, can in principle recover the full posterior distribution of the\nimage, from which uncertainties can then be quantified. However, traditional\nhigh-dimensional sampling methods are generally limited to smooth (e.g.\nGaussian) priors and cannot be used with sparsity-promoting priors. Sparse\npriors, motivated by the theory of compressive sensing, have been shown to be\nhighly effective for radio interferometric imaging. In this article proximal\nMCMC methods are developed for radio interferometric imaging, leveraging\nproximal calculus to support non-differential priors, such as sparse priors, in\na Bayesian framework. Furthermore, three strategies to quantify uncertainties\nusing the recovered posterior distribution are developed: (i) local\n(pixel-wise) credible intervals to provide error bars for each individual\npixel; (ii) highest posterior density credible regions; and (iii) hypothesis\ntesting of image structure. These forms of uncertainty quantification provide\nrich information for analysing radio interferometric observations in a\nstatistically robust manner.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 19:52:51 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 09:19:28 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Cai", "Xiaohao", ""], ["Pereyra", "Marcelo", ""], ["McEwen", "Jason D.", ""]]}, {"id": "1711.04819", "submitter": "Xiaohao Cai", "authors": "Xiaohao Cai, Marcelo Pereyra, Jason D. McEwen", "title": "Uncertainty quantification for radio interferometric imaging: II. MAP\n  estimation", "comments": "13 pages, 10 figures, see companion article in this arXiv listing", "journal-ref": "Monthly Notices of the Royal Astronomical Society, Volume 480,\n  Issue 3, 1 November 2018, Pages 4170--4182", "doi": "10.1093/mnras/sty2015", "report-no": null, "categories": "astro-ph.IM cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification is a critical missing component in radio\ninterferometric imaging that will only become increasingly important as the\nbig-data era of radio interferometry emerges. Statistical sampling approaches\nto perform Bayesian inference, like Markov Chain Monte Carlo (MCMC) sampling,\ncan in principle recover the full posterior distribution of the image, from\nwhich uncertainties can then be quantified. However, for massive data sizes,\nlike those anticipated from the Square Kilometre Array (SKA), it will be\ndifficult if not impossible to apply any MCMC technique due to its inherent\ncomputational cost. We formulate Bayesian inference problems with\nsparsity-promoting priors (motivated by compressive sensing), for which we\nrecover maximum a posteriori (MAP) point estimators of radio interferometric\nimages by convex optimisation. Exploiting recent developments in the theory of\nprobability concentration, we quantify uncertainties by post-processing the\nrecovered MAP estimate. Three strategies to quantify uncertainties are\ndeveloped: (i) highest posterior density credible regions; (ii) local credible\nintervals (cf. error bars) for individual pixels and superpixels; and (iii)\nhypothesis testing of image structure. These forms of uncertainty\nquantification provide rich information for analysing radio interferometric\nobservations in a statistically robust manner. Our MAP-based methods are\napproximately $10^5$ times faster computationally than state-of-the-art MCMC\nmethods and, in addition, support highly distributed and parallelised\nalgorithmic structures. For the first time, our MAP-based techniques provide a\nmeans of quantifying uncertainties for radio interferometric imaging for\nrealistic data volumes and practical use, and scale to the emerging big-data\nera of radio astronomy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 19:52:59 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 08:44:58 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Cai", "Xiaohao", ""], ["Pereyra", "Marcelo", ""], ["McEwen", "Jason D.", ""]]}, {"id": "1711.04834", "submitter": "Brian Barkley", "authors": "Brian G. Barkley, Michael G. Hudgens, John D. Clemens, Mohammad Ali,\n  Michael E. Emch", "title": "Causal Inference from Observational Studies with Clustered Interference", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": "10.1214/19-AOAS1314", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal effects from an observational study is challenging because\nparticipants are not randomized to treatment. Observational studies in\ninfectious disease research present the additional challenge that one\nparticipant's treatment may affect another participant's outcome, i.e., there\nmay be interference. In this paper recent approaches to defining causal effects\nin the presence of interference are considered, and new causal estimands\ndesigned specifically for use with observational studies are proposed.\nPreviously defined estimands target counterfactual scenarios in which\nindividuals independently select treatment with equal probability. However, in\nsettings where there is interference between individuals within clusters, it\nmay be unlikely that treatment selection is independent between individuals in\nthe same cluster. The proposed causal estimands instead describe counterfactual\nscenarios in which the treatment selection correlation structure is the same as\nin the observed data distribution, allowing for within-cluster dependence in\nthe individual treatment selections. These estimands may be more relevant for\npolicy-makers or public health officials who desire to quantify the effect of\nincreasing the proportion of treated individuals in a population. Inverse\nprobability-weighted estimators for these estimands are proposed. The\nlarge-sample properties of the estimators are derived, and a simulation study\ndemonstrating the finite-sample performance of the estimators is presented. The\nproposed methods are illustrated by analyzing data from a study of cholera\nvaccination in over 100,000 individuals in Bangladesh.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 20:19:29 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Barkley", "Brian G.", ""], ["Hudgens", "Michael G.", ""], ["Clemens", "John D.", ""], ["Ali", "Mohammad", ""], ["Emch", "Michael E.", ""]]}, {"id": "1711.04854", "submitter": "Shojaeddin Chenouri", "authors": "Behdad Mostafaiy, MohammadReza FaridRohani, Shojaeddin Chenouri", "title": "Optimal estimation in functional linear regression for sparse\n  noise-contaminated data", "comments": "42 pages, 6 figures,4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to fit a functional linear\nregression in which both the response and the predictor are functions of a\ncommon variable such as time. We consider the case that the response and the\npredictor processes are both sparsely sampled on random time points and are\ncontaminated with random errors. In addition, the random times are allowed to\nbe different for the measurements of the predictor and the response functions.\nThe aforementioned situation often occurs in the longitudinal data settings. To\nestimate the covariance and the cross-covariance functions we use a\nregularization method over a reproducing kernel Hilbert space. The estimate of\nthe cross-covarinace function is used to obtain an estimate of the regression\ncoefficient function and also functional singular components. We derive the\nconvergence rates of the proposed cross-covariance, the regression coefficient\nand the singular component function estimators. Furthermore, we show that,\nunder some regularity conditions, the estimator of the coefficient function has\na minimax optimal rate. We conduct a simulation study and demonstrate merits of\nthe proposed method by comparing it to some other existing methods in the\nliterature. We illustrate the method by an example of an application to a well\nknown multicenter AIDS Cohort Study.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 21:12:22 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mostafaiy", "Behdad", ""], ["FaridRohani", "MohammadReza", ""], ["Chenouri", "Shojaeddin", ""]]}, {"id": "1711.04877", "submitter": "Andrew Holbrook", "authors": "Andrew Holbrook, Thomas Lumley, Daniel Gillen", "title": "Estimating prediction error for complex samples", "comments": "To appear in the Canadian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a growing interest in using non-representative samples to train\nprediction models for numerous outcomes it is necessary to account for the\nsampling design that gives rise to the data in order to assess the generalized\npredictive utility of a proposed prediction rule. After learning a prediction\nrule based on a non-uniform sample, it is of interest to estimate the rule's\nerror rate when applied to unobserved members of the population. Efron (1986)\nproposed a general class of covariance penalty inflated prediction error\nestimators that assume the available training data are representative of the\ntarget population for which the prediction rule is to be applied. We extend\nEfron's estimator to the complex sample context by incorporating\nHorvitz-Thompson sampling weights and show that it is consistent for the true\ngeneralization error rate when applied to the underlying superpopulation. The\nresulting Horvitz-Thompson-Efron (HTE) estimator is equivalent to dAIC, a\nrecent extension of AIC to survey sampling data, but is more widely applicable.\nThe proposed methodology is assessed with simulations and is applied to models\npredicting renal function obtained from the large-scale NHANES survey.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 22:30:47 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 05:46:58 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 21:04:15 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Holbrook", "Andrew", ""], ["Lumley", "Thomas", ""], ["Gillen", "Daniel", ""]]}, {"id": "1711.04934", "submitter": "Dong Xia", "authors": "Dong Xia and Ming Yuan and Cun-Hui Zhang", "title": "Statistically Optimal and Computationally Efficient Low Rank Tensor\n  Completion from Noisy Entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop methods for estimating a low rank tensor from\nnoisy observations on a subset of its entries to achieve both statistical and\ncomputational efficiencies. There have been a lot of recent interests in this\nproblem of noisy tensor completion. Much of the attention has been focused on\nthe fundamental computational challenges often associated with problems\ninvolving higher order tensors, yet very little is known about their\nstatistical performance. To fill in this void, in this article, we characterize\nthe fundamental statistical limits of noisy tensor completion by establishing\nminimax optimal rates of convergence for estimating a $k$th order low rank\ntensor under the general $\\ell_p$ ($1\\le p\\le 2$) norm which suggest\nsignificant room for improvement over the existing approaches. Furthermore, we\npropose a polynomial-time computable estimating procedure based upon power\niteration and a second-order spectral initialization that achieves the optimal\nrates of convergence. Our method is fairly easy to implement and numerical\nexperiments are presented to further demonstrate the practical merits of our\nestimator.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 03:46:05 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 20:09:57 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Xia", "Dong", ""], ["Yuan", "Ming", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1711.05188", "submitter": "Kristin Kirchner", "authors": "David Bolin, Kristin Kirchner, Mih\\'aly Kov\\'acs", "title": "Weak convergence of Galerkin approximations for fractional elliptic\n  stochastic PDEs with spatial white noise", "comments": "22 pages, 1 figure", "journal-ref": "BIT Numer. Math. 58 (2018) pp. 881-906", "doi": "10.1007/s10543-018-0719-8", "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical approximation of the solution to a stochastic partial\ndifferential equation with additive spatial white noise on a bounded domain is\nconsidered. The differential operator is assumed to be a fractional power of an\ninteger order elliptic differential operator. The solution is approximated by\nmeans of a finite element discretization in space and a quadrature\napproximation of an integral representation of the fractional inverse from the\nDunford-Taylor calculus.\n  For the resulting approximation, a concise analysis of the weak error is\nperformed. Specifically, for the class of twice continuously Fr\\'echet\ndifferentiable functionals with second derivatives of polynomial growth, an\nexplicit rate of weak convergence is derived, and it is shown that the\ncomponent of the convergence rate stemming from the stochasticity is doubled\ncompared to the corresponding strong rate. Numerical experiments for different\nfunctionals validate the theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:53:27 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 20:41:22 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2018 11:07:30 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Bolin", "David", ""], ["Kirchner", "Kristin", ""], ["Kov\u00e1cs", "Mih\u00e1ly", ""]]}, {"id": "1711.05337", "submitter": "Nawaf Bou-Rabee", "authors": "Nawaf Bou-Rabee and Jes\\'us Mar\\'ia Sanz-Serna", "title": "Geometric integrators and the Hamiltonian Monte Carlo method", "comments": "Final version will appear in Acta Numerica 2018", "journal-ref": "Acta Numerica, Vol. 27, pp. 113-206, 2018", "doi": "10.1017/S0962492917000101", "report-no": null, "categories": "math.PR math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys in detail the relations between numerical integration and\nthe Hamiltonian (or hybrid) Monte Carlo method (HMC). Since the computational\ncost of HMC mainly lies in the numerical integrations, these should be\nperformed as efficiently as possible. However, HMC requires methods that have\nthe geometric properties of being volume-preserving and reversible, and this\nlimits the number of integrators that may be used. On the other hand, these\ngeometric properties have important quantitative implications on the\nintegration error, which in turn have an impact on the acceptance rate of the\nproposal. While at present the velocity Verlet algorithm is the method of\nchoice for good reasons, we argue that Verlet can be improved upon. We also\ndiscuss in detail the behavior of HMC as the dimensionality of the target\ndistribution increases.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 22:38:46 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bou-Rabee", "Nawaf", ""], ["Sanz-Serna", "Jes\u00fas Mar\u00eda", ""]]}, {"id": "1711.05360", "submitter": "Alexander Shkolnik", "authors": "Lisa Goldberg, Alex Papanicolaou and Alex Shkolnik", "title": "The Dispersion Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation error has plagued quantitative finance since Harry Markowitz\nlaunched modern portfolio theory in 1952. Using random matrix theory, we\ncharacterize a source of bias in the sample eigenvectors of financial\ncovariance matrices. Unchecked, the bias distorts weights of minimum variance\nportfolios and leads to risk forecasts that are severely biased downward. To\naddress these issues, we develop an eigenvector bias correction. Our approach\nis distinct from the regularization and eigenvalue shrinkage methods found in\nthe literature. We provide theoretical guarantees on the improvement our\ncorrection provides as well as estimation methods for computing the optimal\ncorrection from data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 00:03:58 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 00:59:40 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 06:03:18 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 17:16:12 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Goldberg", "Lisa", ""], ["Papanicolaou", "Alex", ""], ["Shkolnik", "Alex", ""]]}, {"id": "1711.05381", "submitter": "Wen-Xin Zhou", "authors": "Wen-Xin Zhou, Koushiki Bose, Jianqing Fan and Han Liu", "title": "A New Perspective on Robust $M$-Estimation: Finite Sample Theory and\n  Applications to Dependence-Adjusted Multiple Testing", "comments": "Ann. Statist. (in press)", "journal-ref": null, "doi": "10.1214/17-AOS1606", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed errors impair the accuracy of the least squares estimate, which\ncan be spoiled by a single grossly outlying observation. As argued in the\nseminal work of Peter Huber in 1973 [{\\it Ann. Statist.} {\\bf 1} (1973)\n799--821], robust alternatives to the method of least squares are sorely\nneeded. To achieve robustness against heavy-tailed sampling distributions, we\nrevisit the Huber estimator from a new perspective by letting the tuning\nparameter involved diverge with the sample size. In this paper, we develop\nnonasymptotic concentration results for such an adaptive Huber estimator,\nnamely, the Huber estimator with the tuning parameter adapted to sample size,\ndimension, and the variance of the noise. Specifically, we obtain a\nsub-Gaussian-type deviation inequality and a nonasymptotic Bahadur\nrepresentation when noise variables only have finite second moments. The\nnonasymptotic results further yield two conventional normal approximation\nresults that are of independent interest, the Berry-Esseen inequality and\nCram\\'er-type moderate deviation. As an important application to large-scale\nsimultaneous inference, we apply these robust normal approximation results to\nanalyze a dependence-adjusted multiple testing procedure for moderately\nheavy-tailed data. It is shown that the robust dependence-adjusted procedure\nasymptotically controls the overall false discovery proportion at the nominal\nlevel under mild moment conditions. Thorough numerical results on both\nsimulated and real datasets are also provided to back up our theory.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 02:15:03 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Zhou", "Wen-Xin", ""], ["Bose", "Koushiki", ""], ["Fan", "Jianqing", ""], ["Liu", "Han", ""]]}, {"id": "1711.05386", "submitter": "Wen-Xin Zhou", "authors": "Jianqing Fan, Yuan Ke, Qiang Sun and Wen-Xin Zhou", "title": "FarmTest: Factor-Adjusted Robust Multiple Testing with Approximate False\n  Discovery Control", "comments": "52 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale multiple testing with correlated and heavy-tailed data arises in\na wide range of research areas from genomics, medical imaging to finance.\nConventional methods for estimating the false discovery proportion (FDP) often\nignore the effect of heavy-tailedness and the dependence structure among test\nstatistics, and thus may lead to inefficient or even inconsistent estimation.\nAlso, the commonly imposed joint normality assumption is arguably too stringent\nfor many applications. To address these challenges, in this paper we propose a\nFactor-Adjusted Robust Multiple Testing (FarmTest) procedure for large-scale\nsimultaneous inference with control of the false discovery proportion. We\ndemonstrate that robust factor adjustments are extremely important in both\ncontrolling the FDP and improving the power. We identify general conditions\nunder which the proposed method produces consistent estimate of the FDP. As a\nbyproduct that is of independent interest, we establish an exponential-type\ndeviation inequality for a robust $U$-type covariance estimator under the\nspectral norm. Extensive numerical experiments demonstrate the advantage of the\nproposed method over several state-of-the-art methods especially when the data\nare generated from heavy-tailed distributions. The proposed procedures are\nimplemented in the R-package FarmTest.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 02:41:08 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 09:14:21 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 00:35:02 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Fan", "Jianqing", ""], ["Ke", "Yuan", ""], ["Sun", "Qiang", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1711.05466", "submitter": "Xu Gao", "authors": "Xu Gao, Babak Shahbaba, Hernando Ombao", "title": "Modeling Binary Time Series Using Gaussian Processes with Application to\n  Predicting Sleep States", "comments": "Journal of Classification (2018)", "journal-ref": null, "doi": "10.1007/s00357-018-9268-8", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of predicting sleep states, we develop a mixed\neffects model for binary time series with a stochastic component represented by\na Gaussian process. The fixed component captures the effects of covariates on\nthe binary-valued response. The Gaussian process captures the residual\nvariations in the binary response that are not explained by covariates and past\nrealizations. We develop a frequentist modeling framework that provides\nefficient inference and more accurate predictions. Results demonstrate the\nadvantages of improved prediction rates over existing approaches such as\nlogistic regression, generalized additive mixed model, models for ordinal data,\ngradient boosting, decision tree and random forest. Using our proposed model,\nwe show that previous sleep state and heart rates are significant predictors\nfor future sleep states. Simulation studies also show that our proposed method\nis promising and robust. To handle computational complexity, we utilize Laplace\napproximation, golden section search and successive parabolic interpolation.\nWith this paper, we also submit an R-package (HIBITS) that implements the\nproposed procedure.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 09:16:12 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 21:11:19 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Gao", "Xu", ""], ["Shahbaba", "Babak", ""], ["Ombao", "Hernando", ""]]}, {"id": "1711.05570", "submitter": "Raiden Hasegawa", "authors": "Colin B. Fogarty and Raiden B. Hasegawa", "title": "Extended Sensitivity Analysis for Heterogeneous Unmeasured Confounding\n  with An Application to Sibling Studies of Returns to Education", "comments": "Both authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional model for assessing insensitivity to hidden bias in paired\nobservational studies constructs a worst-case distribution for treatment\nassignments subject to bounds on the maximal bias to which any given pair is\nsubjected. In studies where rare cases of extreme hidden bias are suspected,\nthe maximal bias may be substantially larger than the typical bias across\npairs, such that a correctly specified bound on the maximal bias would yield an\nunduly pessimistic perception of the study's robustness to hidden bias. We\npresent an extended sensitivity analysis which allows researchers to\nsimultaneously bound the maximal and typical bias perturbing the pairs under\ninvestigation while maintaining the desired Type I error rate. We motivate and\nillustrate our method with two sibling studies on the impact of schooling on\nearnings, one containing information of cognitive ability of siblings and the\nother not. Cognitive ability, clearly influential of both earnings and degree\nof schooling, is likely similar between members of most sibling pairs yet\ncould, conceivably, vary drastically for some siblings. The method is\nstraightforward to implement, simply requiring the solution to a quadratic\nprogram.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 13:53:18 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 03:23:55 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 20:57:23 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Fogarty", "Colin B.", ""], ["Hasegawa", "Raiden B.", ""]]}, {"id": "1711.05618", "submitter": "Massimo Ventrucci", "authors": "Fedele Greco, Massimo Ventrucci, Elisa Castelli", "title": "P-spline smoothing for spatial data collected worldwide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial data collected worldwide at a huge number of locations are frequently\nused in environmental and climate studies. Spatial modelling for this type of\ndata presents both methodological and computational challenges. In this work we\nillustrate a computationally efficient non parametric framework to model and\nestimate the spatial field while accounting for geodesic distances between\nlocations. The spatial field is modelled via penalized splines (P-splines)\nusing intrinsic Gaussian Markov Random Field (GMRF) priors for the spline\ncoefficients. The key idea is to use the sphere as a surrogate for the Globe,\nthen build the basis of B-spline functions on a geodesic grid system. The basis\nmatrix is sparse and so is the precision matrix of the GMRF prior, thus\ncomputational efficiency is gained by construction. We illustrate the approach\non a real climate study, where the goal is to identify the Intertropical\nConvergence Zone using high-resolution remote sensing data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 15:13:14 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Greco", "Fedele", ""], ["Ventrucci", "Massimo", ""], ["Castelli", "Elisa", ""]]}, {"id": "1711.05632", "submitter": "Keefe Murphy", "authors": "Keefe Murphy and Thomas Brendan Murphy", "title": "Gaussian Parsimonious Clustering Models with Covariates and a Noise\n  Component", "comments": "Published in Advances in Data Analysis and Classification", "journal-ref": "Advances in Data Analysis and Classification, 14(2): 293-325\n  (2020)", "doi": "10.1007/s11634-019-00373-8", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider model-based clustering methods for continuous, correlated data\nthat account for external information available in the presence of mixed-type\nfixed covariates by proposing the MoEClust suite of models. These models allow\ndifferent subsets of covariates to influence the component weights and/or\ncomponent densities by modelling the parameters of the mixture as functions of\nthe covariates. A familiar range of constrained eigen-decomposition\nparameterisations of the component covariance matrices are also accommodated.\nThis paper thus addresses the equivalent aims of including covariates in\nGaussian parsimonious clustering models and incorporating parsimonious\ncovariance structures into all special cases of the Gaussian mixture of experts\nframework. The MoEClust models demonstrate significant improvement from both\nperspectives in applications to both univariate and multivariate data sets.\nNovel extensions to include a uniform noise component for capturing outliers\nand to address initialisation of the EM algorithm, model selection, and the\nvisualisation of results are also proposed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 15:41:00 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 17:40:43 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 20:09:54 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Murphy", "Keefe", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1711.05646", "submitter": "Shinichiro Shirota Dr", "authors": "Shinichiro Shirota, Alan E. Gelfand, Sudipto Banerjee", "title": "Spatial Joint Species Distribution Modeling using Dirichlet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Species distribution models usually attempt to explain presence-absence or\nabundance of a species at a site in terms of the environmental features\n(socalled abiotic features) present at the site. Historically, such models have\nconsidered species individually. However, it is well-established that species\ninteract to influence presence-absence and abundance (envisioned as biotic\nfactors). As a result, there has been substantial recent interest in joint\nspecies distribution models with various types of response, e.g.,\npresence-absence, continuous and ordinal data. Such models incorporate\ndependence between species response as a surrogate for interaction.\n  The challenge we focus on here is how to address such modeling in the context\nof a large number of species (e.g., order 102) across sites numbering in the\norder of 102 or 103 when, in practice, only a few species are found at any\nobserved site. Again, there is some recent literature to address this; we adopt\na dimension reduction approach. The novel wrinkle we add here is spatial\ndependence. That is, we have a collection of sites over a relatively small\nspatial region so it is anticipated that species distribution at a given site\nwould be similar to that at a nearby site. Specifically, we handle dimension\nreduction through Dirichlet processes joined with spatial dependence through\nGaussian processes.\n  We use both simulated data and a plant communities dataset for the Cape\nFloristic Region (CFR) of South Africa to demonstrate our approach. The latter\nconsists of presence-absence measurements for 639 tree species on 662\nlocations. Through both data examples we are able to demonstrate improved\npredictive performance using the foregoing specification.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 16:13:01 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 04:57:33 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 22:27:33 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Gelfand", "Alan E.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1711.05663", "submitter": "Jessica Gronsbell", "authors": "Jessica Gronsbell and Tianxi Cai", "title": "Semi-Supervised Approaches to Efficient Evaluation of Model Prediction\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern machine learning applications, the outcome is expensive or\ntime-consuming to collect while the predictor information is easy to obtain.\nSemi-supervised learning (SSL) aims at utilizing large amounts of `unlabeled'\ndata along with small amounts of `labeled' data to improve the efficiency of a\nclassical supervised approach. Though numerous SSL classification and\nprediction procedures have been proposed in recent years, no methods currently\nexist to evaluate the prediction performance of a working regression model. In\nthe context of developing phenotyping algorithms derived from electronic\nmedical records (EMR), we present an efficient two-step estimation procedure\nfor evaluating a binary classifier based on various prediction performance\nmeasures in the semi-supervised (SS) setting. In step I, the labeled data is\nused to obtain a non-parametrically calibrated estimate of the conditional risk\nfunction. In step II, SS estimates of the prediction accuracy parameters are\nconstructed based on the estimated conditional risk function and the unlabeled\ndata. We demonstrate that under mild regularity conditions the proposed\nestimators are consistent and asymptotically normal. Importantly, the\nasymptotic variance of the SS estimators is always smaller than that of the\nsupervised counterparts under correct model specification. We also correct for\npotential overfitting bias in the SS estimators in finite sample with\ncross-validation and develop a perturbation resampling procedure to approximate\ntheir distributions. Our proposals are evaluated through extensive simulation\nstudies and illustrated with two real EMR studies aiming to develop phenotyping\nalgorithms for rheumatoid arthritis and multiple sclerosis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 16:50:43 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Gronsbell", "Jessica", ""], ["Cai", "Tianxi", ""]]}, {"id": "1711.05686", "submitter": "Hormuzd Katki", "authors": "Hormuzd A. Katki", "title": "Novel decision-theoretic and risk-stratification metrics of predictive\n  performance: Application to deciding who should undergo genetic testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, women are referred for BRCA1/2 mutation-testing only if their\nfamily-history of breast/ovarian cancer implies that their risk of carrying a\nmutation exceeds 10\\%. However, as mutation-testing costs fall, prominent\nvoices have called for testing all women, which would strain clinical resources\nby testing millions of women, almost all of whom will test negative. To better\nevaluate risk-thresholds for BRCA1/2 testing, we introduce two broadly\napplicable, linked metrics: Mean Risk Stratification (MRS) and a\ndecision-theoretic metric, Net Benefit of Information (NBI). MRS and NBI\nprovide a range of risk thresholds at which a marker/model is \"optimally\ninformative\", in the sense of maximizing both MRS and NBI. NBI is a function of\nonly MRS and the risk-threshold for action, connecting decision-theory to\nrisk-stratification and providing a decision-theoretic rationale for MRS. AUC\nand Youden's index reflect on both the fraction of maximum MRS, and of maximum\nNBI, attained by the marker/model, providing AUC and Youden's index with\nlong-sought decision-theoretic and risk-stratification rationale. To evaluate\nrisk-thresholds for BRCA1/2 testing, we propose an eclectic approach\nconsidering AUC, Net Benefit, and MRS/NBI. MRS/NBI interpret AUC in the context\nof mutation-prevalence and provide a range of risk thresholds for which the\nrisk model is optimally informative.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:34:11 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Katki", "Hormuzd A.", ""]]}, {"id": "1711.05704", "submitter": "Kirsten Schorning", "authors": "Kirsten Schorning, Maria Konstantinou", "title": "Bayesian optimal designs for dose-response curves with common parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of determining not only an adequate dose but also a dosing\nfrequency of a drug arises frequently in Phase II clinical trials. This results\nin the comparison of models which have some parameters in common. Planning such\nstudies based on Bayesian optimal designs offers robustness to our conclusions\nsince these designs, unlike locally optimal designs, are efficient even if the\nparameters are misspecified. In this paper we develop approximate design theory\nfor Bayesian $D$-optimality for nonlinear regression models with common\nparameters and investigate the cases of common location or common location and\nscale parameters separately. Analytical characterisations of saturated Bayesian\n$D$-optimal designs are derived for frequently used dose-response models and\nthe advantages of our results are illustrated via a numerical investigation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:59:02 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Schorning", "Kirsten", ""], ["Konstantinou", "Maria", ""]]}, {"id": "1711.05825", "submitter": "Richard Everitt", "authors": "Richard G. Everitt", "title": "Bootstrapped synthetic likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) and synthetic likelihood (SL)\ntechniques have enabled the use of Bayesian inference for models that may be\nsimulated, but for which the likelihood cannot be evaluated pointwise at values\nof an unknown parameter $\\theta$. The main idea in ABC and SL is to, for\ndifferent values of $\\theta$ (usually chosen using a Monte Carlo algorithm),\nbuild estimates of the likelihood based on simulations from the model\nconditional on $\\theta$. The quality of these estimates determines the\nefficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to\nimprove an estimated likelihood at $\\theta$ is to simulate more times from the\nmodel conditional on $\\theta$, which is infeasible in cases where the simulator\nis computationally expensive. In this paper we describe how to use\nbootstrapping as a means for improving SL estimates whilst using fewer\nsimulations from the model, and also investigate its use in ABC. Further, we\ninvestigate the use of the bag of little bootstraps as a means for applying\nthis approach to large datasets, yielding Monte Carlo algorithms that\naccurately approximate posterior distributions whilst only simulating\nsubsamples of the full data. Examples of the approach applied to i.i.d.,\ntemporal and spatial data are given.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:13:48 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 23:16:04 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Everitt", "Richard G.", ""]]}, {"id": "1711.05863", "submitter": "Adriano Polpo", "authors": "Renault Caron, Debajyoti Sinha, Dipak Dey, Adriano Polpo", "title": "Categorical data analysis using a skewed Weibull regression model", "comments": null, "journal-ref": null, "doi": "10.3390/e20030176", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Weibull link (skewed) model for categorical\nresponse data arising from binomial as well as multinomial model. We show that,\nfor such types of categorical data, the most commonly used models (logit,\nprobit and complementary log-log) can be obtained as limiting cases. We further\ncompare the proposed model with some other asymmetrical models. The Bayesian as\nwell as frequentist estimation procedures for binomial and multinomial data\nresponses are presented in details. The analysis of two data sets to show the\nefficiency of the proposed model is performed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 00:14:38 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Caron", "Renault", ""], ["Sinha", "Debajyoti", ""], ["Dey", "Dipak", ""], ["Polpo", "Adriano", ""]]}, {"id": "1711.05869", "submitter": "Franz J. Kir\\'aly", "authors": "Samuel Burkart and Franz J Kir\\'aly", "title": "Predictive Independence Testing, Predictive Conditional Independence\n  Testing, and Predictive Graphical Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing (conditional) independence of multivariate random variables is a task\ncentral to statistical inference and modelling in general - though\nunfortunately one for which to date there does not exist a practicable\nworkflow. State-of-art workflows suffer from the need for heuristic or\nsubjective manual choices, high computational complexity, or strong parametric\nassumptions.\n  We address these problems by establishing a theoretical link between\nmultivariate/conditional independence testing, and model comparison in the\nmultivariate predictive modelling aka supervised learning task. This link\nallows advances in the extensively studied supervised learning workflow to be\ndirectly transferred to independence testing workflows - including automated\ntuning of machine learning type which addresses the need for a heuristic\nchoice, the ability to quantitatively trade-off computational demand with\naccuracy, and the modern black-box philosophy for checking and interfacing.\n  As a practical implementation of this link between the two workflows, we\npresent a python package 'pcit', which implements our novel multivariate and\nconditional independence tests, interfacing the supervised learning API of the\nscikit-learn package. Theory and package also allow for straightforward\nindependence test based learning of graphical model structure.\n  We empirically show that our proposed predictive independence test outperform\nor are on par to current practice, and the derived graphical model structure\nlearning algorithms asymptotically recover the 'true' graph. This paper, and\nthe 'pcit' package accompanying it, thus provide powerful, scalable,\ngeneralizable, and easy-to-use methods for multivariate and conditional\nindependence testing, as well as for graphical model structure learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 00:37:34 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 20:32:52 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Burkart", "Samuel", ""], ["Kir\u00e1ly", "Franz J", ""]]}, {"id": "1711.05895", "submitter": "Jie Chen", "authors": "Jie Chen and Michael L. Stein", "title": "Linear-Cost Covariance Functions for Gaussian Random Fields", "comments": "Code is available at https://github.com/jiechenjiechen/RLCM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields (GRF) are a fundamental stochastic model for\nspatiotemporal data analysis. An essential ingredient of GRF is the covariance\nfunction that characterizes the joint Gaussian distribution of the field.\nCommonly used covariance functions give rise to fully dense and unstructured\ncovariance matrices, for which required calculations are notoriously expensive\nto carry out for large data. In this work, we propose a construction of\ncovariance functions that result in matrices with a hierarchical structure.\nEmpowered by matrix algorithms that scale linearly with the matrix dimension,\nthe hierarchical structure is proved to be efficient for a variety of random\nfield computations, including sampling, kriging, and likelihood evaluation.\nSpecifically, with $n$ scattered sites, sampling and likelihood evaluation has\nan $O(n)$ cost and kriging has an $O(\\log n)$ cost after preprocessing,\nparticularly favorable for the kriging of an extremely large number of sites\n(e.g., predicting on more sites than observed). We demonstrate comprehensive\nnumerical experiments to show the use of the constructed covariance functions\nand their appealing computation time. Numerical examples on a laptop include\nsimulated data of size up to one million, as well as a climate data product\nwith over two million observations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 02:20:47 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 03:52:18 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 20:09:22 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chen", "Jie", ""], ["Stein", "Michael L.", ""]]}, {"id": "1711.06219", "submitter": "Mar\\'ia Egl\\'ee P\\'erez", "authors": "Luis R. Pericchi and Maria-Eglee Perez", "title": "Converting P-Values in Adaptive Robust Lower Bounds of Posterior\n  Probabilities to increase the reproducible Scientific \"Findings\"", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We put forward a novel calibration of p values, the \"Adaptive Robust Lower\nBound\" (ARLB) which maps p values into approximations of posterior\nprobabilities taking into account the effect of sample sizes. We build on the\nRobust Lower Bound proposed by Sellke, Bayarri and Berger (2001), but we\nincorporate a simple power of the sample size to make it adaptive to different\namounts of data.\n  We present several illustrations from where it is apparent that the ARLB\nclosely approximates exact Bayes Factors. In particular, it has the same\nasymptotics as posterior probabilities but avoiding the problems of \"Bayesian\nInformation Criterion\" (BIC) for small samples relative to the number of\nparameters.\n  We prove that the ARLB is consistent as the sample size grows, and that it is\ninformation consistent (Berger and Pericchi, 2001) for the canonical Normal\ncase, but with methods that are keen to be generalized. So ARLB also avoids the\nproblems of certain conjugate priors as g-priors.\n  In summary, this is a novel criterion easy to apply, as it only requires a\nreal p value, a sample size and parameter dimensionality. This method is\nintended to aid the practitioners, who are increasingly aware of the lack of\nreproducibility of traditional hypothesis testing \"findings\" but at the same\ntime, lack of concrete simple alternatives. Here is one.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:00:17 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Pericchi", "Luis R.", ""], ["Perez", "Maria-Eglee", ""]]}, {"id": "1711.06252", "submitter": "Shojaeddin Chenouri", "authors": "Jiaxi Liang, Shojaeddin Chenouri and Christopher G. Small", "title": "A New Method for Performance Analysis in Nonlinear Dimensionality\n  Reduction", "comments": "20 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a local rank correlation measure which quantifies\nthe performance of dimension reduction methods. The local rank correlation is\neasily interpretable, and robust against the extreme skewness of nearest\nneighbor distributions in high dimensions. Some benchmark datasets are studied.\nWe find that the local rank correlation closely corresponds to our visual\ninterpretation of the quality of the output. In addition, we demonstrate that\nthe local rank correlation is useful in estimating the intrinsic dimensionality\nof the original data, and in selecting a suitable value of tuning parameters\nused in some algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:52:17 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Liang", "Jiaxi", ""], ["Chenouri", "Shojaeddin", ""], ["Small", "Christopher G.", ""]]}, {"id": "1711.06341", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon, Myl\\`ene B\\'edard and Alain Desgagn\\'e", "title": "An automatic robust Bayesian approach to principal component regression", "comments": "To appear in Journal of Applied Statistics", "journal-ref": null, "doi": "10.1080/02664763.2019.1710478", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component regression uses principal components as regressors. It is\nparticularly useful in prediction settings with high-dimensional covariates.\nThe existing literature treating of Bayesian approaches is relatively sparse.\nWe introduce a Bayesian approach that is robust to outliers in both the\ndependent variable and the covariates. Outliers can be thought of as\nobservations that are not in line with the general trend. The proposed approach\nautomatically penalises these observations so that their impact on the\nposterior gradually vanishes as they move further and further away from the\ngeneral trend, corresponding to a concept in Bayesian statistics called whole\nrobustness. The predictions produced are thus consistent with the bulk of the\ndata. The approach also exploits the geometry of principal components to\nefficiently identify those that are significant. Individual predictions\nobtained from the resulting models are consolidated according to\nmodel-averaging mechanisms to account for model uncertainty. The approach is\nevaluated on real data and compared to its nonrobust Bayesian counterpart, the\ntraditional frequentist approach, and a commonly employed robust frequentist\nmethod. Detailed guidelines to automate the entire statistical procedure are\nprovided. All required code is made available, see ArXiv:1711.06341.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 22:54:02 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 11:58:37 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 04:03:55 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Gagnon", "Philippe", ""], ["B\u00e9dard", "Myl\u00e8ne", ""], ["Desgagn\u00e9", "Alain", ""]]}, {"id": "1711.06393", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Hisashi Noma", "title": "A Unified Method for Improved Inference in Random-effects Meta-analysis", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects meta-analyses have been widely applied in evidence synthesis\nfor various types of medical studies. However, standard inference methods (e.g.\nrestricted maximum likelihood estimation) usually underestimate statistical\nerrors and possibly provide highly overconfident results under realistic\nsituations; for instance, coverage probabilities of confidence intervals can be\nsubstantially below the nominal level. The main reason is that these inference\nmethods rely on large sample approximations even though the number of\nsynthesized studies is usually small or moderate in practice. In this article\nwe solve this problem using a unified inference method based on Monte Carlo\nconditioning for broad application to random-effects meta-analysis. The\ndeveloped method provides improved confidence intervals with coverage\nprobabilities that are closer to the nominal level than standard methods. As\nspecific applications, we provide new inference procedures for three types of\nmeta-analysis: conventional univariate meta-analysis for pairwise treatment\ncomparisons, meta-analysis of diagnostic test accuracy, and multiple treatment\ncomparisons via network meta-analysis. We also illustrate the practical\neffectiveness of these methods via real data applications and simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 04:14:00 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 05:35:48 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 14:38:08 GMT"}, {"version": "v4", "created": "Mon, 18 Mar 2019 12:54:22 GMT"}, {"version": "v5", "created": "Fri, 10 May 2019 00:42:39 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Noma", "Hisashi", ""]]}, {"id": "1711.06477", "submitter": "William Weimin Yoo", "authors": "William Weimin Yoo", "title": "Discussion on Computationally Efficient Multivariate Spatio-Temporal\n  Models for High-Dimensional Count-Valued Data by Bradley et al", "comments": null, "journal-ref": "Bayesian Anal. Volume 13, Number 1 (2018), 284-285", "doi": "10.1214/17-BA1069", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I begin my discussion by summarizing the methodology proposed and new\ndistributional results on multivariate log-Gamma derived in the paper. Then, I\ndraw an interesting connection between their work with mean field variational\nBayes. Lastly, I make some comments on the simulation results and the\nperformance of the proposed Poisson multivariate spatio-temporal mixed effects\nmodel (P-MSTM).\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 10:01:17 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 22:17:04 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Yoo", "William Weimin", ""]]}, {"id": "1711.06642", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "Nonparametric independence testing via mutual information", "comments": "46 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a test of independence of two multivariate random vectors, given a\nsample from the underlying population. Our approach, which we call MINT, is\nbased on the estimation of mutual information, whose decomposition into joint\nand marginal entropies facilitates the use of recently-developed efficient\nentropy estimators derived from nearest neighbour distances. The proposed\ncritical values, which may be obtained from simulation (in the case where one\nmarginal is known) or resampling, guarantee that the test has nominal size, and\nwe provide local power analyses, uniformly over classes of densities whose\nmutual information satisfies a lower bound. Our ideas may be extended to\nprovide a new goodness-of-fit tests of normal linear models based on assessing\nthe independence of our vector of covariates and an appropriately-defined\nnotion of an error vector. The theory is supported by numerical studies on both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 17:38:50 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1711.06705", "submitter": "Zhigang Yao", "authors": "Zhigang Yao and Zhenyue Zhang", "title": "Principal Boundary on Riemannian Manifolds", "comments": "31 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classification problem and focus on nonlinear methods for\nclassification on manifolds. For multivariate datasets lying on an embedded\nnonlinear Riemannian manifold within the higher-dimensional ambient space, we\naim to acquire a classification boundary for the classes with labels, using the\nintrinsic metric on the manifolds. Motivated by finding an optimal boundary\nbetween the two classes, we invent a novel approach -- the principal boundary.\nFrom the perspective of classification, the principal boundary is defined as an\noptimal curve that moves in between the principal flows traced out from two\nclasses of data, and at any point on the boundary, it maximizes the margin\nbetween the two classes. We estimate the boundary in quality with its\ndirection, supervised by the two principal flows. We show that the principal\nboundary yields the usual decision boundary found by the support vector machine\nin the sense that locally, the two boundaries coincide. Some optimality and\nconvergence properties of the random principal boundary and its population\ncounterpart are also shown. We illustrate how to find, use and interpret the\nprincipal boundary with an application in real data.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 14:35:45 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 17:34:40 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Yao", "Zhigang", ""], ["Zhang", "Zhenyue", ""]]}, {"id": "1711.06746", "submitter": "Kun Meng", "authors": "Kun Meng and Ani Eloyan", "title": "Principal manifold estimation via model complexity selection", "comments": "40 pages, 9 figures", "journal-ref": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology), 2021", "doi": "10.1111/rssb.12416", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework of principal manifolds to model high-dimensional data.\nThis framework is based on Sobolev spaces and designed to model data of any\nintrinsic dimension. It includes principal component analysis and principal\ncurve algorithm as special cases. We propose a novel method for model\ncomplexity selection to avoid overfitting, eliminate the effects of outliers,\nand improve the computation speed. Additionally, we propose a method for\nidentifying the interiors of circle-like curves and cylinder/ball-like\nsurfaces. The proposed approach is compared to existing methods by simulations\nand applied to estimate tumor surfaces and interiors in a lung cancer study.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 22:09:31 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 19:56:14 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 03:58:20 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 04:28:18 GMT"}, {"version": "v5", "created": "Tue, 25 Aug 2020 04:09:15 GMT"}, {"version": "v6", "created": "Sun, 28 Mar 2021 15:40:46 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Meng", "Kun", ""], ["Eloyan", "Ani", ""]]}, {"id": "1711.06912", "submitter": "Tony Yaacoub", "authors": "Tony Yaacoub, George V. Moustakides, Yajun Mei", "title": "Optimal Stopping for Interval Estimation in Bernoulli Trials", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimal sequential methodology for obtaining confidence\nintervals for a binomial proportion $\\theta$. Assuming that an i.i.d. random\nsequence of Benoulli($\\theta$) trials is observed sequentially, we are\ninterested in designing a)~a stopping time $T$ that will decide when is the\nbest time to stop sampling the process, and b)~an optimum estimator\n$\\hat{\\theta}_{T}$ that will provide the optimum center of the interval\nestimate of $\\theta$. We follow a semi-Bayesian approach, where we assume that\nthere exists a prior distribution for $\\theta$, and our goal is to minimize the\naverage number of samples while we guarantee a minimal coverage probability\nlevel. The solution is obtained by applying standard optimal stopping theory\nand computing the optimum pair $(T,\\hat{\\theta}_{T})$ numerically. Regarding\nthe optimum stopping time component $T$, we demonstrate that it enjoys certain\nvery uncommon characteristics not encountered in solutions of other classical\noptimal stopping problems. Finally, we compare our method with the optimum\nfixed-sample-size procedure but also with existing alternative sequential\nschemes.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 18:26:00 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Yaacoub", "Tony", ""], ["Moustakides", "George V.", ""], ["Mei", "Yajun", ""]]}, {"id": "1711.06926", "submitter": "William Weimin Yoo", "authors": "William Weimin Yoo and Aad W. van der Vaart", "title": "The Bayes Lepski's Method and Credible Bands through Volume of Tubular\n  Neighborhoods", "comments": "42 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a general class of priors based on random series basis expansion, we\ndevelop the Bayes Lepski's method to estimate unknown regression function. In\nthis approach, the series truncation point is determined based on a stopping\nrule that balances the posterior mean bias and the posterior standard\ndeviation. Equipped with this mechanism, we present a method to construct\nadaptive Bayesian credible bands, where this statistical task is reformulated\ninto a problem in geometry, and the band's radius is computed based on finding\nthe volume of certain tubular neighborhood embedded on a unit sphere. We\nconsider two special cases involving B-splines and wavelets, and discuss some\ninteresting consequences such as the uncertainty principle and self-similarity.\nLastly, we show how to program the Bayes Lepski stopping rule on a computer,\nand numerical simulations in conjunction with our theoretical investigations\nconcur that this is a promising Bayesian uncertainty quantification procedure.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 21:09:13 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Yoo", "William Weimin", ""], ["van der Vaart", "Aad W.", ""]]}, {"id": "1711.06999", "submitter": "Daniele Durante", "authors": "Daniele Durante and Tommaso Rigon", "title": "Conditionally conjugate mean-field variational Bayes for logistic models", "comments": null, "journal-ref": "Statistical Science (2019). 34, 472-485", "doi": "10.1214/19-STS712", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is a common strategy for approximate Bayesian\ninference, but simple methods are only available for specific classes of models\nincluding, in particular, representations having conditionally conjugate\nconstructions within an exponential family. Models with logit components are an\napparently notable exception to this class, due to the absence of conjugacy\nbetween the logistic likelihood and the Gaussian priors for the coefficients in\nthe linear predictor. To facilitate approximate inference within this widely\nused class of models, Jaakkola and Jordan (2000) proposed a simple variational\napproach which relies on a family of tangent quadratic lower bounds of logistic\nlog-likelihoods, thus restoring conjugacy between these approximate bounds and\nthe Gaussian priors. This strategy is still implemented successfully, but less\nattempts have been made to formally understand the reasons underlying its\nexcellent performance. To cover this key gap, we provide a formal connection\nbetween the above bound and a recent P\\'olya-gamma data augmentation for\nlogistic regression. Such a result places the computational methods associated\nwith the aforementioned bounds within the framework of variational inference\nfor conditionally conjugate exponential family models, thereby allowing recent\nadvances for this class to be inherited also by the methods relying on Jaakkola\nand Jordan (2000).\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 10:50:24 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 06:59:24 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Durante", "Daniele", ""], ["Rigon", "Tommaso", ""]]}, {"id": "1711.07137", "submitter": "Ashley Naimi", "authors": "Ashley I Naimi and Alan E Mishler and Edward H Kennedy", "title": "Challenges in Obtaining Valid Causal Effect Estimates with Machine\n  Learning Algorithms", "comments": "21 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike parametric regression, machine learning (ML) methods do not generally\nrequire precise knowledge of the true data generating mechanisms. As such,\nnumerous authors have advocated for ML methods to estimate causal effects.\nUnfortunately, ML algorithms can perform worse than parametric regression. We\ndemonstrate the performance of ML-based single- and double-robust estimators.\nWe use 100 Monte Carlo samples with sample sizes of 200, 1200, and 5000 to\ninvestigate bias and confidence interval coverage under several scenarios. In a\nsimple confounding scenario, confounders were related to the treatment and the\noutcome via parametric models. In a complex confounding scenario, the simple\nconfounders were transformed to induce complicated nonlinear relationships. In\nthe simple scenario, when ML algorithms were used, double-robust estimators\nwere superior to single-robust estimators. In the complex scenario,\nsingle-robust estimators with ML algorithms were at least as biased as\nestimators using misspecified parametric models. Double-robust estimators were\nless biased, but coverage was well below nominal. The use of sample splitting,\ninclusion of confounder interactions, reliance on a richly specified ML\nalgorithm, and use of doubly robust estimators was the only explored approach\nthat yielded negligible bias and nominal coverage. Our results suggest that ML\nbased singly robust methods should be avoided.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 04:27:18 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 14:52:34 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Naimi", "Ashley I", ""], ["Mishler", "Alan E", ""], ["Kennedy", "Edward H", ""]]}, {"id": "1711.07287", "submitter": "Giuseppe Di Benedetto", "authors": "Giuseppe Di Benedetto, Fran\\c{c}ois Caron, Yee Whye Teh", "title": "Non-exchangeable random partition models for microclustering", "comments": "20 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular random partition models, such as the Chinese restaurant process\nand its two-parameter extension, fall in the class of exchangeable random\npartitions, and have found wide applicability in model-based clustering,\npopulation genetics, ecology or network analysis. While the exchangeability\nassumption is sensible in many cases, it has some strong implications. In\nparticular, Kingman's representation theorem implies that the size of the\nclusters necessarily grows linearly with the sample size; this feature may be\nundesirable for some applications, as recently pointed out by Miller et al.\n(2015). We present here a flexible class of non-exchangeable random partition\nmodels which are able to generate partitions whose cluster sizes grow\nsublinearly with the sample size, and where the growth rate is controlled by\none parameter. Along with this result, we provide the asymptotic behaviour of\nthe number of clusters of a given size, and show that the model can exhibit a\npower-law behavior, controlled by another parameter. The construction is based\non completely random measures and a Poisson embedding of the random partition,\nand inference is performed using a Sequential Monte Carlo algorithm.\nAdditionally, we show how the model can also be directly used to generate\nsparse multigraphs with power-law degree distributions and degree sequences\nwith sublinear growth. Finally, experiments on real datasets emphasize the\nusefulness of the approach compared to a two-parameter Chinese restaurant\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:45:36 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Di Benedetto", "Giuseppe", ""], ["Caron", "Fran\u00e7ois", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1711.07357", "submitter": "Abolfazl Safikhani", "authors": "Abolfazl Safikhani, Ali Shojaie", "title": "Joint Structural Break Detection and Parameter Estimation in\n  High-Dimensional Non-Stationary VAR Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.02736", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming stationarity is unrealistic in many time series applications. A more\nrealistic alternative is to allow for piecewise stationarity, where the model\nis allowed to change at given time points. We propose a three-stage procedure\nfor consistent estimation of both structural change points and parameters of\nhigh-dimensional piecewise vector autoregressive (VAR) models. In the first\nstep, we reformulate the change point detection problem as a high-dimensional\nvariable selection one, and propose a penalized least square estimator using a\ntotal variation penalty. We show that the proposed penalized estimation method\nover-estimates the number of change points. We then propose a backward\nselection criterion in conjunction with a penalized least square estimator to\ntackle this issue. In the last step of our procedure, we estimate the VAR\nparameters in each of the segments. We prove that the proposed procedure\nconsistently detects the number of change points and their locations. We also\nshow that the procedure consistently estimates the VAR parameters. The\nperformance of the method is illustrated through several simulation scenarios\nand real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 01:38:46 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 06:25:52 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Safikhani", "Abolfazl", ""], ["Shojaie", "Ali", ""]]}, {"id": "1711.07516", "submitter": "Yuan Yan", "authors": "Yuan Yan and Marc Genton", "title": "Non-Gaussian Autoregressive Processes with Tukey g-and-h Transformations", "comments": null, "journal-ref": null, "doi": "10.1002/env.2503", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing a time series analysis of continuous data, for example from\nclimate or environmental problems, the assumption that the process is Gaussian\nis often violated. Therefore, we introduce two non-Gaussian autoregressive time\nseries models that are able to fit skewed and heavy-tailed time series data.\nOur two models are based on the Tukey g-and-h transformation. We discuss\nparameter estimation, order selection, and forecasting procedures for our\nmodels and examine their performances in a simulation study. We demonstrate the\nusefulness of our models by applying them to two sets of wind speed data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 19:49:44 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 08:13:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yan", "Yuan", ""], ["Genton", "Marc", ""]]}, {"id": "1711.07518", "submitter": "Kaspar Rufibach", "authors": "Kaspar Rufibach", "title": "Treatment Effect Quantification for Time-to-event Endpoints --\n  Estimands, Analysis Strategies, and beyond", "comments": "37 pages", "journal-ref": "Pharm Stat, 2019, 18, 144-164", "doi": "10.1002/pst.1917", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A draft addendum to ICH E9 has been released for public consultation in\nAugust 2017. The addendum focuses on two topics particularly relevant for\nrandomized confirmatory clinical trials: estimands and sensitivity analyses.\nThe need to amend ICH E9 grew out of the realization of a lack of alignment\nbetween the objectives of a clinical trial stated in the protocol and the\naccompanying quantification of the \"treatment effect\" reported in a regulatory\nsubmission. We embed time-to-event endpoints in the estimand framework, and\ndiscuss how the four estimand attributes described in the addendum apply to\ntime-to-event endpoints. We point out that if the proportional hazards\nassumption is not met, the estimand targeted by the most prevalent methods used\nto analyze time-to-event endpoints, logrank test and Cox regression, depends on\nthe censoring distribution. We discuss for a large randomized clinical trial\nhow the analyses for the primary and secondary endpoints as well as the\nsensitivity analyses actually performed in the trial can be seen in the context\nof the addendum. To the best of our knowledge, this is the first attempt to do\nso for a trial with a time-to-event endpoint. Questions that remain open with\nthe addendum for time-to-event endpoints and beyond are formulated, and\nrecommendations for planning of future trials are given. We hope that this will\nprovide a contribution to developing a common framework based on the final\nversion of the addendum that can be applied to design, protocols, statistical\nanalysis plans, and clinical study reports in the future.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 19:53:23 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 15:05:52 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2018 20:55:54 GMT"}, {"version": "v4", "created": "Fri, 5 Oct 2018 10:50:47 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Rufibach", "Kaspar", ""]]}, {"id": "1711.07592", "submitter": "Jean Feng", "authors": "Jean Feng, Noah Simon", "title": "Sparse-Input Neural Networks for High-dimensional Nonparametric\n  Regression and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are usually not the tool of choice for nonparametric\nhigh-dimensional problems where the number of input features is much larger\nthan the number of observations. Though neural networks can approximate complex\nmultivariate functions, they generally require a large number of training\nobservations to obtain reasonable fits, unless one can learn the appropriate\nnetwork structure. In this manuscript, we show that neural networks can be\napplied successfully to high-dimensional settings if the true function falls in\na low dimensional subspace, and proper regularization is used. We propose\nfitting a neural network with a sparse group lasso penalty on the first-layer\ninput weights. This results in a neural net that only uses a small subset of\nthe original features. In addition, we characterize the statistical convergence\nof the penalized empirical risk minimizer to the optimal neural network: we\nshow that the excess risk of this penalized estimator only grows with the\nlogarithm of the number of input features; and we show that the weights of\nirrelevant features converge to zero. Via simulation studies and data analyses,\nwe show that these sparse-input neural networks outperform existing\nnonparametric high-dimensional estimation methods when the data has complex\nhigher-order interactions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 01:11:00 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 18:23:48 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Feng", "Jean", ""], ["Simon", "Noah", ""]]}, {"id": "1711.07635", "submitter": "Ray Bai", "authors": "Ray Bai, Malay Ghosh", "title": "High-Dimensional Multivariate Posterior Consistency Under Global-Local\n  Shrinkage Priors", "comments": "18 pages, 3 tables, 1 figure. More technical details of computation\n  added to Section 4.2, proofs moved to separate online supplement", "journal-ref": "J.Multivariate Anal. 167 (2018) 157-170", "doi": "10.1016/j.jmva.2018.04.010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sparse Bayesian estimation in the classical multivariate linear\nregression model with $p$ regressors and $q$ response variables. In univariate\nBayesian linear regression with a single response $y$, shrinkage priors which\ncan be expressed as scale mixtures of normal densities are popular for\nobtaining sparse estimates of the coefficients. In this paper, we extend the\nuse of these priors to the multivariate case to estimate a $p \\times q$\ncoefficients matrix $\\mathbf{B}$. We derive sufficient conditions for posterior\nconsistency under the Bayesian multivariate linear regression framework and\nprove that our method achieves posterior consistency even when $p>n$ and even\nwhen $p$ grows at nearly exponential rate with the sample size. We derive an\nefficient Gibbs sampling algorithm and provide the implementation in a\ncomprehensive R package called MBSP. Finally, we demonstrate through\nsimulations and data analysis that our model has excellent finite sample\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 05:27:58 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 22:25:12 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Bai", "Ray", ""], ["Ghosh", "Malay", ""]]}, {"id": "1711.07715", "submitter": "Dominik Liebl", "authors": "Dominik Liebl, Stefan Rameseder", "title": "Partially Observed Functional Data: The Case of Systematically Missing\n  Parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New estimators for the mean and the covariance function for partially\nobserved functional data are proposed using a detour via the fundamental\ntheorem of calculus. The new estimators allow for a consistent estimation of\nthe mean and covariance function under specific violations of the\nmissing-completely-at-random assumption. The requirements of the estimation\nprocedure can be tested using a sequential multiple hypothesis test procedure.\nAn extensive simulation study compares the new estimators with the classical\nestimators from the literature in different missing data scenarios. The\nproposed methodology is motivated by the practical problem of estimating the\nmean price curve in the German Control Reserve Market. In this auction market,\nprice curves are only partially observable and the underlying missing data\nmechanism depends on systematic trading strategies which clearly violate the\nmissing-completely-at-random assumption. In contrast to the classical\nestimators, the new estimators lead to useful estimates of the mean and\ncovariance functions. Supplementary materials are provided online.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 11:06:06 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 21:02:00 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2018 21:43:04 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Liebl", "Dominik", ""], ["Rameseder", "Stefan", ""]]}, {"id": "1711.07748", "submitter": "Michael Fop", "authors": "Michael Fop, Thomas Brendan Murphy, Luca Scrucca", "title": "Model-based Clustering with Sparse Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite Gaussian mixture models are widely used for model-based clustering of\ncontinuous data. Nevertheless, since the number of model parameters scales\nquadratically with the number of variables, these models can be easily\nover-parameterized. For this reason, parsimonious models have been developed\nvia covariance matrix decompositions or assuming local independence. However,\nthese remedies do not allow for direct estimation of sparse covariance matrices\nnor do they take into account that the structure of association among the\nvariables can vary from one cluster to the other. To this end, we introduce\nmixtures of Gaussian covariance graph models for model-based clustering with\nsparse covariance matrices. A penalized likelihood approach is employed for\nestimation and a general penalty term on the graph configurations can be used\nto induce different levels of sparsity and incorporate prior knowledge. Model\nestimation is carried out using a structural-EM algorithm for parameters and\ngraph structure estimation, where two alternative strategies based on a genetic\nalgorithm and an efficient stepwise search are proposed for inference. With\nthis approach, sparse component covariance matrices are directly obtained. The\nframework results in a parsimonious model-based clustering of the data via a\nflexible model for the within-group joint distribution of the variables.\nExtensive simulated data experiments and application to illustrative datasets\nshow that the method attains good classification performance and model quality.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 12:36:08 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 12:21:14 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Fop", "Michael", ""], ["Murphy", "Thomas Brendan", ""], ["Scrucca", "Luca", ""]]}, {"id": "1711.07801", "submitter": "Harry Crane", "authors": "Harry Crane", "title": "Why \"Redefining Statistical Significance\" Will Not Improve\n  Reproducibility and Could Make the Replication Crisis Worse", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent proposal to \"redefine statistical significance\" (Benjamin, et al.\nNature Human Behaviour, 2017) claims that false positive rates \"would\nimmediately improve\" by factors greater than two and replication rates would\ndouble simply by changing the conventional cutoff for 'statistical\nsignificance' from P<0.05 to P<0.005. I analyze the veracity of these claims,\nfocusing especially on how Benjamin, et al neglect the effects of P-hacking in\nassessing the impact of their proposal. My analysis shows that once P-hacking\nis accounted for the perceived benefits of the lower threshold all but\ndisappear, prompting two main conclusions: (i) The claimed improvements to\nfalse positive rate and replication rate in Benjamin, et al (2017) are\nexaggerated and misleading. (ii) There are plausible scenarios under which the\nlower cutoff will make the replication crisis worse.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:26:42 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Crane", "Harry", ""]]}, {"id": "1711.08077", "submitter": "Douglas Nychka", "authors": "Douglas Nychka, Dorit Hammerling, Mitchell Krock, and Ashton Wiens", "title": "Modeling and emulation of nonstationary Gaussian fields", "comments": "32 pages total, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geophysical and other natural processes often exhibit non-stationary\ncovariances and this feature is important to take into account for statistical\nmodels that attempt to emulate the physical process. A convolution-based model\nis used to represent non-stationary Gaussian processes that allows for\nvariation in the correlation range and vari- ance of the process across space.\nApplication of this model has two steps: windowed estimates of the covariance\nfunction under the as- sumption of local stationary and encoding the local\nestimates into a single spatial process model that allows for efficient\nsimulation. Specifically we give evidence to show that non-stationary\ncovariance functions based on the Mat`ern family can be reproduced by the Lat-\nticeKrig model, a flexible, multi-resolution representation of Gaussian\nprocesses. We propose to fit locally stationary models based on the Mat`ern\ncovariance and then assemble these estimates into a single, global LatticeKrig\nmodel. One advantage of the LatticeKrig model is that it is efficient for\nsimulating non-stationary fields even at 105 locations. This work is motivated\nby the interest in emulating spatial fields derived from numerical model\nsimulations such as Earth system models. We successfully apply these ideas to\nemulate fields that de- scribe the uncertainty in the pattern scaling of mean\nsummer (JJA) surface temperature from a series of climate model experiments.\nThis example is significant because it emulates tens of thousands of loca-\ntions, typical in geophysical model fields, and leverages embarrassing parallel\ncomputation to speed up the local covariance fitting\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 22:57:02 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Nychka", "Douglas", ""], ["Hammerling", "Dorit", ""], ["Krock", "Mitchell", ""], ["Wiens", "Ashton", ""]]}, {"id": "1711.08129", "submitter": "Hyebin Song", "authors": "Hyebin Song, Garvesh Raskutti", "title": "PULasso: High-dimensional variable selection with presence-only data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various real-world problems, we are presented with classification problems\nwith positive and unlabeled data, referred to as presence-only responses. In\nthis paper, we study variable selection in the context of presence only\nresponses where the number of features or covariates p is large. The\ncombination of presence-only responses and high dimensionality presents both\nstatistical and computational challenges. In this paper, we develop the PUlasso\nalgorithm for variable selection and classification with positive and unlabeled\nresponses. Our algorithm involves using the majorization-minimization (MM)\nframework which is a generalization of the well-known expectation-maximization\n(EM) algorithm. In particular to make our algorithm scalable, we provide two\ncomputational speed-ups to the standard EM algorithm. We provide a theoretical\nguarantee where we first show that our algorithm converges to a stationary\npoint, and then prove that any stationary point within a local neighborhood of\nthe true parameter achieves the minimax optimal mean-squared error under both\nstrict sparsity and group sparsity assumptions. We also demonstrate through\nsimulations that our algorithm out-performs state-of-the-art algorithms in the\nmoderate p settings in terms of classification performance. Finally, we\ndemonstrate that our PUlasso algorithm performs well on a biochemistry example.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 04:59:12 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 06:55:12 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 20:06:29 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Song", "Hyebin", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1711.08147", "submitter": "Yalin Zhu", "authors": "Yalin Zhu, Wenge Guo", "title": "Familywise Error Rate Controlling Procedures for Discrete Data", "comments": "27 pages, 4 figures", "journal-ref": "Statistics in Biopharmaceutical Research 2019", "doi": "10.1080/19466315.2019.1654912", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as clinical safety analysis, the data of the experiments\nusually consists of frequency counts. In the analysis of such data, researchers\noften face the problem of multiple testing based on discrete test statistics,\naimed at controlling family-wise error rate (FWER). Most existing FWER\ncontrolling procedures are developed for continuous data, which are often\nconservative when analyzing discrete data. By using minimal attainable\n$p$-values, several FWER controlling procedures have been specifically\ndeveloped for discrete data in the literature. In this paper, by utilizing\nknown marginal distributions of true null $p$-values, three more powerful\nstepwise procedures are developed, which are modified versions of the\nconventional Bonferroni, Holm and Hochberg procedures, respectively. It is\nshown that the first two procedures strongly control the FWER under arbitrary\ndependence and are more powerful than the existing Tarone-type procedures,\nwhile the last one only ensures control of the FWER in special settings.\nThrough extensive simulation studies, we provide numerical evidence of superior\nperformance of the proposed procedures in terms of the FWER control and minimal\npower. A real clinical safety data is used to demonstrate applications of our\nproposed procedures. An R package \"MHTdiscrete\" and a web application are\ndeveloped for implementing the proposed procedures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 06:28:39 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 18:53:12 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 16:17:36 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Zhu", "Yalin", ""], ["Guo", "Wenge", ""]]}, {"id": "1711.08265", "submitter": "Xiang Liu", "authors": "Xiang Liu, Haohan Wang, Wenting Ye, Eric P. Xing", "title": "Sparse Variable Selection on High Dimensional Heterogeneous Data with\n  Tree Structured Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse variable selection on high dimension\nheterogeneous data sets, which has been taken on renewed interest recently due\nto the growth of biological and medical data sets with complex, non-i.i.d.\nstructures and prolific response variables. The heterogeneity is likely to\nconfound the association between explanatory variables and responses, resulting\nin a wealth of false discoveries when Lasso or its variants are na\\\"ively\napplied. Therefore, the research interest of developing effective confounder\ncorrection methods is growing. However, ordinarily employing recent confounder\ncorrection methods will result in undesirable performance due to the ignorance\nof the convoluted interdependency among the prolific response variables. To\nfully improve current variable selection methods, we introduce a model that can\nutilize the dependency information from multiple responses to select the active\nvariables from heterogeneous data. Through extensive experiments on synthetic\nand real data sets, we show that our proposed model outperforms the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 13:18:37 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Liu", "Xiang", ""], ["Wang", "Haohan", ""], ["Ye", "Wenting", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.08360", "submitter": "Sanjay Pant", "authors": "Sanjay Pant", "title": "Information sensitivity functions to assess parameter information gain\n  and identifiability of dynamical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of functions, called the `Information sensitivity functions'\n(ISFs), which quantify the information gain about the parameters through the\nmeasurements/observables of a dynamical system are presented. These functions\ncan be easily computed through classical sensitivity functions alone and are\nbased on Bayesian and information-theoretic approaches. While marginal\ninformation gain is quantified by decrease in differential entropy,\ncorrelations between arbitrary sets of parameters are assessed through mutual\ninformation. For individual parameters these information gains are also\npresented as marginal posterior variances, and, to assess the effect of\ncorrelations, as conditional variances when other parameters are given. The\neasy to interpret ISFs can be used to a) identify time-intervals or regions in\ndynamical system behaviour where information about the parameters is\nconcentrated; b) assess the effect of measurement noise on the information gain\nfor the parameters; c) assess whether sufficient information in an experimental\nprotocol (input, measurements, and their frequency) is available to identify\nthe parameters; d) assess correlation in the posterior distribution of the\nparameters to identify the sets of parameters that are likely to be\nindistinguishable; and e) assess identifiability problems for particular sets\nof parameters.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 12:53:16 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 20:16:14 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Pant", "Sanjay", ""]]}, {"id": "1711.08411", "submitter": "Samprit Banerjee", "authors": "Samprit Banerjee and Stefano Monni", "title": "An Orthogonally Equivariant Estimator of the Covariance Matrix in High\n  Dimensions and for Small Sample Sizes", "comments": "Journal of Statistical Planning and Inference (2020)", "journal-ref": null, "doi": "10.1016/j.jspi.2020.10.006", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an estimation method of covariance matrices in a\nhigh-dimensional setting, i.e., when the dimension of the matrix, , is larger\nthan the sample size . Specifically, we propose an orthogonally equivariant\nestimator. The eigenvectors of such estimator are the same as those of the\nsample covariance matrix. The eigenvalue estimates are obtained from an\nadjusted profile likelihood function derived by approximating the integral of\nthe density function of the sample covariance matrix over its eigenvectors,\nwhich is a challenging problem in its own right. Exact solutions to the\napproximate likelihood equations are obtained and employed to construct\nestimates that involve a tuning parameter. Bootstrap and cross-validation based\nalgorithms are proposed to choose this tuning parameter under various loss\nfunctions. Finally, comparisons with two well-known orthogonally equivariant\nestimators of the covariance matrix are given, which are based on Monte-Carlo\nrisk estimates for simulated data and misclassification errors in real data\nanalyses. In addition, Monte-Carlo risk estimates are also provided to compare\nour estimates of eigenvalues to those of a consistent estimator of population\neigenvalues.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 17:38:36 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 18:46:56 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 07:17:54 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Banerjee", "Samprit", ""], ["Monni", "Stefano", ""]]}, {"id": "1711.08695", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist and Christoph Hirnschall", "title": "Grabit: Gradient Tree-Boosted Tobit Models for Default Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A frequent problem in binary classification is class imbalance between a\nminority and a majority class such as defaults and non-defaults in default\nprediction. In this article, we introduce a novel binary classification model,\nthe Grabit model, which is obtained by applying gradient tree boosting to the\nTobit model. We show how this model can leverage auxiliary data to obtain\nincreased predictive accuracy for imbalanced data. We apply the Grabit model to\npredicting defaults on loans made to Swiss small and medium-sized enterprises\n(SME) and obtain a large and significant improvement in predictive performance\ncompared to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:59:48 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 15:54:01 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 17:21:58 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 16:33:29 GMT"}, {"version": "v5", "created": "Wed, 15 Aug 2018 15:19:54 GMT"}, {"version": "v6", "created": "Thu, 14 Feb 2019 13:49:06 GMT"}, {"version": "v7", "created": "Fri, 1 Mar 2019 13:42:54 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Sigrist", "Fabio", ""], ["Hirnschall", "Christoph", ""]]}, {"id": "1711.08747", "submitter": "Mengjia Yu", "authors": "Mengjia Yu, Xiaohui Chen", "title": "Finite sample change point inference and identification for\n  high-dimensional mean vectors", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12406", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cumulative sum (CUSUM) statistics are widely used in the change point\ninference and identification. For the problem of testing for existence of a\nchange point in an independent sample generated from the mean-shift model, we\nintroduce a Gaussian multiplier bootstrap to calibrate critical values of the\nCUSUM test statistics in high dimensions. The proposed bootstrap CUSUM test is\nfully data-dependent and it has strong theoretical guarantees under arbitrary\ndependence structures and mild moment conditions. Specifically, we show that\nwith a boundary removal parameter the bootstrap CUSUM test enjoys the uniform\nvalidity in size under the null and it achieves the minimax separation rate\nunder the sparse alternatives when the dimension $p$ can be larger than the\nsample size $n$.\n  Once a change point is detected, we estimate the change point location by\nmaximizing the $\\ell^{\\infty}$-norm of the generalized CUSUM statistics at two\ndifferent weighting scales corresponding to covariance stationary and\nnon-stationary CUSUM statistics. For both estimators, we derive their rates of\nconvergence and show that dimension impacts the rates only through logarithmic\nfactors, which implies that consistency of the CUSUM estimators is possible\nwhen $p$ is much larger than $n$. In the presence of multiple change points, we\npropose a principled bootstrap-assisted binary segmentation (BABS) algorithm to\ndynamically adjust the change point detection rule and recursively estimate\ntheir locations. We derive its rate of convergence under suitable signal\nseparation and strength conditions.\n  The results derived in this paper are non-asymptotic and we provide extensive\nsimulation studies to assess the finite sample performance. The empirical\nevidence shows an encouraging agreement with our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 15:55:38 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 01:17:53 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 04:03:14 GMT"}, {"version": "v4", "created": "Sun, 6 Sep 2020 03:47:55 GMT"}, {"version": "v5", "created": "Sat, 2 Jan 2021 05:16:07 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yu", "Mengjia", ""], ["Chen", "Xiaohui", ""]]}, {"id": "1711.08822", "submitter": "Kin Wai Chan", "authors": "Kin Wai Chan and Xiao-Li Meng", "title": "Multiple Improvements of Multiple Imputation Likelihood Ratio Tests", "comments": "45 pages, 9 tables, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) inference handles missing data by first properly\nimputing the missing values $m$ times, and then combining the $m$ analysis\nresults from applying a complete-data procedure to each of the completed\ndatasets. However, the existing method for combining likelihood ratio tests has\nmultiple defects: (i) the combined test statistic can be negative in practice\nwhen the reference null distribution is a standard $F$ distribution; (ii) it is\nnot invariant to re-parametrization; (iii) it fails to ensure monotonic power\ndue to its use of an inconsistent estimator of the fraction of missing\ninformation (FMI) under the alternative hypothesis; and (iv) it requires\nnon-trivial access to the likelihood ratio test statistic as a function of\nestimated parameters instead of datasets. This paper shows, via both\ntheoretical derivations and empirical investigations, that essentially all of\nthese problems can be straightforwardly addressed if we are willing to perform\nan additional likelihood ratio test by stacking the $m$ completed datasets as\none big completed dataset. A particularly intriguing finding is that the FMI\nitself can be estimated consistently by a likelihood ratio statistic for\ntesting whether the $m$ completed datasets produced by MI can be regarded\neffectively as samples coming from a common model. Practical guidelines are\nprovided based on an extensive comparison of existing MI tests.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 20:18:45 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 03:48:21 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Chan", "Kin Wai", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1711.08876", "submitter": "Harlan Campbell", "authors": "Harlan Campbell", "title": "Is it even rainier in North Vancouver? A non-parametric rank-based test\n  for semicontinuous longitudinal data", "comments": "21 pages with SAS and R code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the outcome of interest is semicontinuous and collected longitudinally,\nefficient testing can be difficult. Daily rainfall data is an excellent example\nwhich we use to illustrate the various challenges. Even under the simplest\nscenario, the popular 'two-part model', which uses correlated random-effects to\naccount for both the semicontinuous and longitudinal characteristics of the\ndata, often requires prohibitively intensive numerical integration and\ndifficult interpretation. Reducing data to binary (truncating continuous\npositive values to equal one), while relatively straightforward, leads to a\npotentially substantial loss in power. We propose an alternative: using a\nnon-parametric rank test recently proposed for joint longitudinal survival\ndata. We investigate the benefits of such a test for the analysis of\nsemicontinuous longitudinal data with regards to power and computational\nfeasibility.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 06:16:22 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 18:19:35 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Campbell", "Harlan", ""]]}, {"id": "1711.08950", "submitter": "Matteo Farn\\`e Dr.", "authors": "Matteo Farn\\`e and Angela Montanari", "title": "A large covariance matrix estimator under intermediate spikiness regimes", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2019.104577", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper concerns large covariance matrix estimation via composite\nminimization under the assumption of low rank plus sparse structure. In this\napproach, the low rank plus sparse decomposition of the covariance matrix is\nrecovered by least squares minimization under nuclear norm plus $l_1$ norm\npenalization. This paper proposes a new estimator of that family based on an\nadditional least-squares re-optimization step aimed at un-shrinking the\neigenvalues of the low rank component estimated at the first step. We prove\nthat such un-shrinkage causes the final estimate to approach the target as\nclosely as possible in Frobenius norm while recovering exactly the underlying\nlow rank and sparsity pattern. Consistency is guaranteed when $n$ is at least\n$O(p^{\\frac{3}{2}\\delta})$, provided that the maximum number of non-zeros per\nrow in the sparse component is $O(p^{\\delta})$ with $\\delta \\leq \\frac{1}{2}$.\nConsistent recovery is ensured if the latent eigenvalues scale to $p^{\\alpha}$,\n$\\alpha \\in[0,1]$, while rank consistency is ensured if $\\delta \\leq \\alpha$.\nThe resulting estimator is called UNALCE (UNshrunk ALgebraic Covariance\nEstimator) and is shown to outperform state of the art estimators, especially\nfor what concerns fitting properties and sparsity pattern detection. The\neffectiveness of UNALCE is highlighted on a real example regarding ECB banking\nsupervisory data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 12:57:34 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 21:25:47 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 09:44:43 GMT"}, {"version": "v4", "created": "Sat, 20 Oct 2018 17:59:56 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Farn\u00e8", "Matteo", ""], ["Montanari", "Angela", ""]]}, {"id": "1711.08960", "submitter": "Benjamin All\\'evius", "authors": "Benjamin All\\'evius and Michael H\\\"ohle", "title": "Prospective Detection of Outbreaks", "comments": "This manuscript is a preprint of a chapter to appear in the Handbook\n  of Infectious Disease Data Analysis, Held, L., Hens, N., O'Neill, P.D. and\n  Wallinga, J. (Eds.). Chapman \\& Hall/CRC, 2018. Please use the book for\n  possible citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter surveys univariate and multivariate methods for infectious\ndisease outbreak detection. The setting considered is a prospective one: data\narrives sequentially as part of the surveillance systems maintained by public\nhealth authorities, and the task is to determine whether to 'sound the alarm'\nor not, given the recent history of data. The chapter begins by describing two\npopular detection methods for univariate time series data: the EARS algorithm\nof the CDC, and the Farrington algorithm more popular at European public health\ninstitutions. This is followed by a discussion of methods that extend some of\nthe univariate methods to a multivariate setting. This may enable the detection\nof outbreaks whose signal is only weakly present in any single data stream\nconsidered on its own. The chapter ends with a longer discussion of methods for\noutbreak detection in spatio-temporal data. These methods are not only tasked\nwith determining if and when an outbreak started to emerge, but also where. In\nparticular, the scan statistics methodology for outbreak cluster detection in\ndiscrete-time area-referenced data is discussed, as well as similar methods for\ncontinuous-time, continuous-space data. As a running example to illustrate the\nmethods covered in the chapter, a dataset on invasive meningococcal disease in\nGermany in the years 2002-2008 is used. This data and the methods covered are\navailable through the R packages surveillance and scanstatistics.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 13:48:04 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["All\u00e9vius", "Benjamin", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "1711.09179", "submitter": "Shubhadeep Chakraborty", "authors": "Shubhadeep Chakraborty and Xianyang Zhang", "title": "Distance Metrics for Measuring Joint Dependence with Application to\n  Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical applications require the quantification of joint dependence\namong more than two random vectors. In this work, we generalize the notion of\ndistance covariance to quantify joint dependence among d >= 2 random vectors.\nWe introduce the high order distance covariance to measure the so-called\nLancaster interaction dependence. The joint distance covariance is then defined\nas a linear combination of pairwise distance covariances and their higher order\ncounterparts which together completely characterize mutual independence. We\nfurther introduce some related concepts including the distance cumulant,\ndistance characteristic function, and rank-based distance covariance. Empirical\nestimators are constructed based on certain Euclidean distances between sample\nelements. We study the large sample properties of the estimators and propose a\nbootstrap procedure to approximate their sampling distributions. The asymptotic\nvalidity of the bootstrap procedure is justified under both the null and\nalternative hypotheses. The new metrics are employed to perform model selection\nin causal inference, which is based on the joint independence testing of the\nresiduals from the fitted structural equation models. The effectiveness of the\nmethod is illustrated via both simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 02:07:19 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 08:17:26 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Chakraborty", "Shubhadeep", ""], ["Zhang", "Xianyang", ""]]}, {"id": "1711.09317", "submitter": "Yanan Fan Dr", "authors": "T. Rodrigues, J.-L. Dortet-Bernadet and Y. Fan", "title": "Noncrossing simultaneous Bayesian quantile curve fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian simultaneous estimation of nonparametric quantile curves is a\nchallenging problem, requiring a flexible and robust data model whilst\nsatisfying the monotonicity or noncrossing constraints on the quantiles. This\npaper presents the use of the pyramid quantile regression method in the spline\nregression setting. In high dimensional problems, the choice of the pyramid\nlocations becomes crucial for a robust parameter estimation. In this work we\nderive the optimal {pyramid locations which then allows us to propose an\nefficient} adaptive block-update MCMC scheme for posterior computation.\nSimulation studies show the proposed method provides estimates with\nsignificantly smaller errors and better empirical coverage probability when\ncompared to existing alternative approaches. We illustrate the method with\nthree real applications.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 01:48:41 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rodrigues", "T.", ""], ["Dortet-Bernadet", "J. -L.", ""], ["Fan", "Y.", ""]]}, {"id": "1711.09388", "submitter": "Ingeborg Waernbaum PhD", "authors": "Ingeborg Waernbaum and Laura Pazzagli", "title": "Model misspecification and bias for inverse probability weighting and\n  doubly robust estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the causal inference literature an estimator belonging to a class of\nsemi-parametric estimators is called robust if it has desirable properties\nunder the assumption that at least one of the working models is correctly\nspecified. In this paper we propose a crude analytical approach to study the\nlarge sample bias of semi-parameteric estimators of the average causal effect\nwhen all working models are misspecified. We apply our approach to three\nprototypical estimators, two inverse probability weighting (IPW) estimators,\nusing a misspecified propensity score model, and a doubly robust (DR)\nestimator, using misspecified models for the outcome regression and the\npropensity score. To analyze the question of when the use of two misspecified\nmodels are better than one we derive necessary and sufficient conditions for\nwhen the DR estimator has a smaller bias than a simple IPW estimator and when\nit has a smaller bias than an IPW estimator with normalized weights. If the\nmisspecificiation of the outcome model is moderate the comparisons of the\nbiases of the IPW and DR estimators suggest that the DR estimator has a smaller\nbias than the IPW estimators. However, all biases include the PS-model error\nand we suggest that a researcher is careful when modeling the PS whenever such\na model is involved.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 13:49:14 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 14:54:48 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Waernbaum", "Ingeborg", ""], ["Pazzagli", "Laura", ""]]}, {"id": "1711.09492", "submitter": "Praneeth Narayanamurthy", "authors": "Namrata Vaswani, Thierry Bouwmans, Sajid Javed, and Praneeth\n  Narayanamurthy", "title": "Robust Subspace Learning: Robust PCA, Robust Subspace Tracking, and\n  Robust Subspace Recovery", "comments": "To appear, IEEE Signal Processing Magazine, July 2018", "journal-ref": "IEEE Signal Processing Magazine (Volume: 35, Issue: 4, July 2018)", "doi": "10.1109/MSP.2018.2826566", "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PCA is one of the most widely used dimension reduction techniques. A related\neasier problem is \"subspace learning\" or \"subspace estimation\". Given\nrelatively clean data, both are easily solved via singular value decomposition\n(SVD). The problem of subspace learning or PCA in the presence of outliers is\ncalled robust subspace learning or robust PCA (RPCA). For long data sequences,\nif one tries to use a single lower dimensional subspace to represent the data,\nthe required subspace dimension may end up being quite large. For such data, a\nbetter model is to assume that it lies in a low-dimensional subspace that can\nchange over time, albeit gradually. The problem of tracking such data (and the\nsubspaces) while being robust to outliers is called robust subspace tracking\n(RST). This article provides a magazine-style overview of the entire field of\nrobust subspace learning and tracking. In particular solutions for three\nproblems are discussed in detail: RPCA via sparse+low-rank matrix decomposition\n(S+LR), RST via S+LR, and \"robust subspace recovery (RSR)\". RSR assumes that an\nentire data vector is either an outlier or an inlier. The S+LR formulation\ninstead assumes that outliers occur on only a few data vector indices and hence\nare well modeled as sparse corruptions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 23:52:53 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 23:33:54 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 21:49:47 GMT"}, {"version": "v4", "created": "Thu, 5 Jul 2018 22:46:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Vaswani", "Namrata", ""], ["Bouwmans", "Thierry", ""], ["Javed", "Sajid", ""], ["Narayanamurthy", "Praneeth", ""]]}, {"id": "1711.09533", "submitter": "Wei Ning", "authors": "Ramadha D. Piyadi Gamage and Wei Ning", "title": "Empirical Likelihood for Change Point Detection in Autoregressive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point analysis has become an important research topic in many fields\nof applications. Several research work has been carried out to detect changes\nand its locations in time series data. In this paper, a nonparametric method\nbased on the empirical likelihood is proposed to detect the structural changes\nof the parameters in autoregressive (AR) models . Under certain conditions, the\nasymptotic null distribution of the empirical likelihood ratio test statistic\nis proved to be the extreme value distribution. Further, the consistency of the\ntest statistic has been proved. Simulations have been carried out to show that\nthe power of the proposed test statistic is significant. The proposed method is\napplied to real world data set to further illustrate the testing procedure.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 04:46:16 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gamage", "Ramadha D. Piyadi", ""], ["Ning", "Wei", ""]]}, {"id": "1711.09548", "submitter": "Behdad Mostafaiy", "authors": "Behdad Mostafaiy", "title": "On estimation in varying coefficient models for sparse and irregularly\n  sampled functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a smoothness regularization method for a varying\ncoefficient model based on sparse and irregularly sampled functional data which\nis contaminated with some measurement errors. We estimate the one-dimensional\ncovariance and cross-covariance functions of the underlying stochastic\nprocesses based on a reproducing kernel Hilbert space approach. We then obtain\nleast squares estimates of the coefficient functions. Simulation studies\ndemonstrate that the proposed method has good performance. We illustrate our\nmethod by an analysis of longitudinal primary biliary liver cirrhosis data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 06:03:43 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Mostafaiy", "Behdad", ""]]}, {"id": "1711.09586", "submitter": "Yixin Wang", "authors": "Yixin Wang and Stefan Van Aelst", "title": "Robust variable screening for regression using factor profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sure Independence Screening is a fast procedure for variable selection in\nultra-high dimensional regression analysis. Unfortunately, its performance\ngreatly deteriorates with increasing dependence among the predictors. To solve\nthis issue, Factor Profiled Sure Independence Screening (FPSIS) models the\ncorrelation structure of the predictor variables, assuming that it can be\nrepresented by a few latent factors. The correlations can then be profiled out\nby projecting the data onto the orthogonal complement of the subspace spanned\nby these factors. However, neither of these methods can handle the presence of\noutliers in the data. Therefore, we propose a robust screening method which\nuses a least trimmed squares method to estimate the latent factors and the\nfactor profiled variables. Variable screening is then performed on factor\nprofiled variables by using regression MM-estimators. Different types of\noutliers in this model and their roles in variable screening are studied. Both\nsimulation studies and a real data analysis show that the proposed robust\nprocedure has good performance on clean data and outperforms the two nonrobust\nmethods on contaminated data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 08:58:01 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 08:27:44 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Wang", "Yixin", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "1711.09677", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic, Ljubomir Buturovic, Simon Thomas, David E Leahy", "title": "Binary classification models with \"Uncertain\" predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary classification models which can assign probabilities to categories\nsuch as \"the tissue is 75% likely to be tumorous\" or \"the chemical is 25%\nlikely to be toxic\" are well understood statistically, but their utility as an\ninput to decision making is less well explored. We argue that users need to\nknow which is the most probable outcome, how likely that is to be true and, in\naddition, whether the model is capable enough to provide an answer. It is the\nlast case, where the potential outcomes of the model explicitly include \"don't\nknow\" that is addressed in this paper. Including this outcome would better\nseparate those predictions that can lead directly to a decision from those\nwhere more data is needed. Where models produce an \"Uncertain\" answer similar\nto a human reply of \"don't know\" or \"50:50\" in the examples we refer to\nearlier, this would translate to actions such as \"operate on tumour\" or \"remove\ncompound from use\" where the models give a \"more true than not\" answer. Where\nthe models judge the result \"Uncertain\" the practical decision might be \"carry\nout more detailed laboratory testing of compound\" or \"commission new tissue\nanalyses\". The paper presents several examples where we first analyse the\neffect of its introduction, then present a methodology for separating\n\"Uncertain\" from binary predictions and finally, we provide arguments for its\nuse in practice.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:29:42 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 10:49:25 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 15:10:52 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Krstajic", "Damjan", ""], ["Buturovic", "Ljubomir", ""], ["Thomas", "Simon", ""], ["Leahy", "David E", ""]]}, {"id": "1711.10016", "submitter": "Merlin Keller", "authors": "Merlin Keller and Kaniav Kamary", "title": "Bayesian model averaging via mixture model estimation", "comments": "20 pages, 5 figures, submission in preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for Bayesian model averaging (BMA) and selection is proposed,\nbased on the mixture model approach for hypothesis testing in Kaniav et al.,\n2014. Inheriting from the good properties of this approach, it extends BMA to\ncases where improper priors are chosen for parameters that are common to all\ncandidate models.\n  From an algorithmic point of view, our approach consists in sampling from the\nposterior distribution of the single-datum mixture of all candidate models,\nweighted by their prior probabilities. We show that this posterior distribution\nis equal to the 'Bayesian-model averaged' posterior distribution over all\ncandidate models, weighted by their posterior probability. From this BMA\nposterior sample, a simple Monte-Carlo estimate of each model's posterior\nprobability is derived, as well as importance sampling estimates for\nexpectations under each model's posterior distribution.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 21:56:37 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 14:36:09 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Keller", "Merlin", ""], ["Kamary", "Kaniav", ""]]}, {"id": "1711.10028", "submitter": "William Fithian", "authors": "William Fithian and Daniel Ting", "title": "Family learning: nonparametric statistical inference with parametric\n  efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing and other statistical inference procedures are most\nefficient when a reliable low-dimensional parametric family can be specified.\nWe propose a method that learns such a family when one exists but its form is\nnot known a priori, by examining samples from related populations and fitting a\nlow-dimensional exponential family that approximates all the samples as well as\npossible. We propose a computationally efficient spectral method that allows us\nto carry out hypothesis tests that are valid whether or not the fit is good,\nand recover asymptotically optimal power if it is. Our method is\ncomputationally efficient and can produce substantial power gains in simulation\nand real-world A/B testing data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 22:43:30 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Fithian", "William", ""], ["Ting", "Daniel", ""]]}, {"id": "1711.10199", "submitter": "Michael Grayling", "authors": "Michael Grayling, Adrian Mander, James Wason", "title": "A two-stage Fisher exact test for multi-arm studies with binary outcome\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In small sample studies with binary outcome data, use of a normal\napproximation for hypothesis testing can lead to substantial inflation of the\ntype-I error-rate. Consequently, exact statistical methods are necessitated,\nand accordingly, much research has been conducted to facilitate this. Recently,\nthis has included methodology for the design of two-stage multi-arm studies\nutilising exact binomial tests. These designs were demonstrated to carry\nsubstantial efficiency advantages over a fixed sample design, but generally\nsuffered from strong conservatism. An alternative classical means of small\nsample inference with dichotomous data is Fisher's exact test. However, this\nmethod is limited to single-stage designs when there are multiple arms.\nTherefore, here, we propose a two-stage version of Fisher's exact test, with\nthe potential to stop early to accept or reject null hypotheses, which is\napplicable to multi-arm studies. In particular, we provide precise formulae\ndescribing the requirements for achieving weak or strong control of the\nfamilywise error-rate with this design. Following this, we describe how the\ndesign parameters may be optimised to confer desirable operating\ncharacteristics. For a motivating example based on a phase II clinical trial,\nwe demonstrate that on average our approach is less conservative than\ncorresponding optimal designs based on exact binomial tests.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:31:00 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Grayling", "Michael", ""], ["Mander", "Adrian", ""], ["Wason", "James", ""]]}, {"id": "1711.10411", "submitter": "Yang Feng", "authors": "Yang Feng, Yichao Wu, Leonard Stefanski", "title": "Nonparametric Independence Screening via Favored Smoothing Bandwidth", "comments": "22 pages", "journal-ref": "Journal of Statistical Planning and Inference Volume 197, December\n  2018, Pages 1-14", "doi": "10.1016/j.jspi.2017.11.006", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible nonparametric regression method for\nultrahigh-dimensional data. As a first step, we propose a fast screening method\nbased on the favored smoothing bandwidth of the marginal local constant\nregression. Then, an iterative procedure is developed to recover both the\nimportant covariates and the regression function. Theoretically, we prove that\nthe favored smoothing bandwidth based screening possesses the model selection\nconsistency property. Simulation studies as well as real data analysis show the\ncompetitive performance of the new procedure.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 17:11:20 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Feng", "Yang", ""], ["Wu", "Yichao", ""], ["Stefanski", "Leonard", ""]]}, {"id": "1711.10420", "submitter": "Zenon Gniazdowski", "authors": "Zenon Gniazdowski", "title": "New Interpretation of Principal Components Analysis", "comments": "ISSN 1896-396X", "journal-ref": "Zeszyty Naukowe WWSI, No 16, Vol. 11, 2017, pp. 43-65", "doi": "10.26348/znwwsi.16.43", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new look on the principal component analysis has been presented. Firstly, a\ngeometric interpretation of determination coefficient was shown. In turn, the\nability to represent the analyzed data and their interdependencies in the form\nof easy-to-understand basic geometric structures was shown. As a result of the\nanalysis of these structures it was proposed to enrich the classical PCA. In\nparticular, it was proposed a new criterion for the selection of important\nprincipal components and a new algorithm for clustering primary variables by\ntheir level of similarity to the principal components. Virtual and real data\nspaces, as well as tensor operations on data, have also been identified.The\nanisotropy of the data was identified too.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 13:41:29 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Gniazdowski", "Zenon", ""]]}, {"id": "1711.10421", "submitter": "Xiaoyue Niu", "authors": "Bomin Kim, Kevin Lee, Lingzhou Xue, and Xiaoyue Niu", "title": "A Review of Dynamic Network Models with Latent Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a selective review of statistical modeling of dynamic networks. We\nfocus on models with latent variables, specifically, the latent space models\nand the latent class models (or stochastic blockmodels), which investigate both\nthe observed features and the unobserved structure of networks. We begin with\nan overview of the static models, and then we introduce the dynamic extensions.\nFor each dynamic model, we also discuss its applications that have been studied\nin the literature, with the data source listed in Appendix. Based on the\nreview, we summarize a list of open problems and challenges in dynamic network\nmodeling with latent variables.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:31:21 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 15:38:06 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Kim", "Bomin", ""], ["Lee", "Kevin", ""], ["Xue", "Lingzhou", ""], ["Niu", "Xiaoyue", ""]]}, {"id": "1711.10427", "submitter": "Kelly Bodwin", "authors": "Carson Mosso, Kelly Bodwin, Suman Chakraborty, Kai Zhang, and Andrew\n  B. Nobel", "title": "Latent Association Mining in Binary Data", "comments": "29 pages, 2 tables, 4 figures 54 page appendix/supplemental figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying stable sets of mutually associated\nfeatures in moderate or high-dimensional binary data. In this context we\ndevelop and investigate a method called Latent Association Mining for Binary\nData (LAMB). The LAMB method is based on a simple threshold model in which the\nobserved binary values represent a random thresholding of a latent continuous\nvector that may have a complex association structure. We consider a measure of\nlatent association that quantifies association in the latent continuous vector\nwithout bias due to the random thresholding. The LAMB method uses an iterative\ntesting based search procedure to identify stable sets of mutually associated\nfeatures. We compare the LAMB method with several competing methods on\nartificial binary-valued datasets and two real count-valued datasets. The LAMB\nmethod detects meaningful associations in these datasets. In the case of the\ncount-valued datasets, associations detected by the LAMB method are based only\non information about whether the counts are zero or non-zero, and is\ncompetitive with methods that have access to the full count data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 17:38:53 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 06:38:22 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Mosso", "Carson", ""], ["Bodwin", "Kelly", ""], ["Chakraborty", "Suman", ""], ["Zhang", "Kai", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1711.10440", "submitter": "Michail Papathomas Dr", "authors": "Wei Jing and Michail Papathomas", "title": "On the correspondence of deviances and maximum likelihood and interval\n  estimates from log-linear to logistic regression modelling", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set of categorical variables $\\mathcal{P}$ where at least one,\ndenoted by $Y$, is binary. The log-linear model that describes the counts in\nthe resulting contingency table implies a specific logistic regression model,\nwith the binary variable as the outcome. Extending results in Christensen\n(1997), by also considering the case where factors present in the contingency\ntable disappear from the logistic regression model, we prove that the Maximum\nLikelihood Estimate (MLE) for the parameters of the logistic regression equals\nthe MLE for the corresponding parameters of the log-linear model. We prove\nthat, asymptotically, standard errors for the two sets of parameters are also\nequal. Subsequently, Wald confidence intervals are asymptotically equal. These\nresults demonstrate the extent to which inferences from the log-linear\nframework can be translated to inferences within the logistic regression\nframework, on the magnitude of main effects and interactions. Finally, we prove\nthat the deviance of the log-linear model is equal to the deviance of the\ncorresponding logistic regression, provided that the latter is fitted to a\ndataset where no cell observations are merged when one or more factors in\n$\\mathcal{P} \\setminus \\{ Y \\}$ become obsolete. We illustrate the derived\nresults with the analysis of a real dataset.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:01:06 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 11:02:09 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 12:34:13 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Jing", "Wei", ""], ["Papathomas", "Michail", ""]]}, {"id": "1711.10463", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio", "title": "The joint projected normal and skew-normal: a distribution for\n  poly-cylindrical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of this work is the introduction of a multivariate\ncircular-linear (or poly- cylindrical) distribution obtained by combining the\nprojected and the skew-normal. We show the flexibility of our proposal, its\nproperty of closure under marginalization and how to quantify multivariate\ndependence. Due to a non-identifiability issue that our proposal inherits from\nthe projected normal, a compu- tational problem arises. We overcome it in a\nBayesian framework, adding suitable latent variables and showing that posterior\nsamples can be obtained with a post-processing of the estimation algo- rithm\noutput. Under specific prior choices, this approach enables us to implement a\nMarkov chain Monte Carlo algorithm relying only on Gibbs steps, where the\nupdates of the parameters are done as if we were working with a multivariate\nnormal likelihood. The proposed approach can be also used with the projected\nnormal. As a proof of concept, on simulated examples we show the ability of our\nalgorithm in recovering the parameters values and to solve the identification\nproblem. Then the proposal is used in a real data example, where the\nturning-angles (circular variables) and the logarithm of the step-lengths\n(linear variables) of four zebras are jointly modelled.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 09:31:14 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Mastrantonio", "Gianluca", ""]]}, {"id": "1711.10635", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Jacob Bien", "title": "Valid Inference Corrected for Outlier Removal", "comments": "21 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least square (OLS) estimation of a linear regression model is\nwell-known to be highly sensitive to outliers. It is common practice to (1)\nidentify and remove outliers by looking at the data and (2) to fit OLS and form\nconfidence intervals and p-values on the remaining data as if this were the\noriginal data collected. This standard \"detect-and-forget\" approach has been\nshown to be problematic, and in this paper we highlight the fact that it can\nlead to invalid inference and show how recently developed tools in selective\ninference can be used to properly account for outlier detection and removal.\nOur inferential procedures apply to a general class of outlier removal\nprocedures that includes several of the most commonly used approaches. We\nconduct simulations to corroborate the theoretical results, and we apply our\nmethod to three real data sets to illustrate how our inferential results can\ndiffer from the traditional detect-and-forget strategy. A companion R package,\noutference, implements these new procedures with an interface that matches the\nfunctions commonly used for inference with lm in R.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 01:18:56 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 01:13:13 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2019 04:41:56 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Chen", "Shuxiao", ""], ["Bien", "Jacob", ""]]}, {"id": "1711.10645", "submitter": "Manoel Santos Neto Santos-Neto", "authors": "Josemar Rodrigues, Marcelo Bourguignon, Manoel Santos-Neto and N.\n  Balakrishnan", "title": "Fractional approaches for the distribution of innovation sequence of\n  INAR(1) processes", "comments": "19 pages", "journal-ref": null, "doi": "10.1080/03610926.2019.1568492", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a fractional decomposition of the probability\ngenerating function of the innovation process of the first-order non-negative\ninteger-valued autoregressive [INAR(1)] process to obtain the corresponding\nprobability mass function. We also provide a comprehensive review of\ninteger-valued time series models, based on the concept of thinning operators,\nwith geometric-type marginals. In particular, we develop four fractional\napproaches to obtain the distribution of innovation processes of the INAR(1)\nmodel and show that the distribution of the innovations sequence has\ngeometric-type distribution. These approaches are discussed in detail and\nillustrated through a few examples. Finally, using the methods presented here,\nwe develop four new first-order non-negative integer-valued autoregressive\nprocess for autocorrelated counts with overdispersion with known marginals, and\nderive some properties of these models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 02:03:11 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Rodrigues", "Josemar", ""], ["Bourguignon", "Marcelo", ""], ["Santos-Neto", "Manoel", ""], ["Balakrishnan", "N.", ""]]}, {"id": "1711.10654", "submitter": "Xin Zhou", "authors": "Xin Zhou and Michael R. Kosorok", "title": "Augmented Outcome-weighted Learning for Optimal Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision medicine is of considerable interest in clinical, academic and\nregulatory parties. The key to precision medicine is the optimal treatment\nregime. Recently, Zhou et al. (2017) developed residual weighted learning (RWL)\nto construct the optimal regime that directly optimize the clinical outcome.\nHowever, this method involves computationally intensive non-convex\noptimization, which cannot guarantee a global solution. Furthermore, this\nmethod does not possess fully semiparametrical efficiency. In this article, we\npropose augmented outcome-weighted learning (AOL). The method is built on a\ndoubly robust augmented inverse probability weighted estimator, and hence\nconstructs semiparametrically efficient regimes. Our proposed AOL is closely\nrelated to RWL. The weights are obtained from counterfactual residuals, where\nnegative residuals are reflected to positive and accordingly their treatment\nassignments are switched to opposites. Convex loss functions are thus applied\nto guarantee a global solution and to reduce computations. We show that AOL is\nuniversally consistent, i.e., the estimated regime of AOL converges the Bayes\nregime when the sample size approaches infinity, without knowing any specifics\nof the distribution of the data. We also propose variable selection methods for\nlinear and nonlinear regimes, respectively, to further improve performance. The\nperformance of the proposed AOL methods is illustrated in simulation studies\nand in an analysis of the Nefazodone-CBASP clinical trial data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 02:57:39 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Zhou", "Xin", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1711.10819", "submitter": "Erlis Ruli", "authors": "Federica Giummol\\`e, Valentina Mameli, Erlis Ruli and Laura Ventura", "title": "Objective Bayesian inference with proper scoring rules", "comments": "29 pages and 9 figures", "journal-ref": "Test 2019", "doi": "10.1007/s11749-018-0597-z", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Bayesian analyses can be difficult to perform when the full\nlikelihood, and consequently the full posterior distribution, is too complex\nand difficult to specify or if robustness with respect to data or to model\nmisspecifications is required. In these situations, we suggest to resort to a\nposterior distribution for the parameter of interest based on proper scoring\nrules. Scoring rules are loss functions designed to measure the quality of a\nprobability distribution for a random variable, given its observed value.\nImportant examples are the Tsallis score and the Hyv\\\"arinen score, which allow\nus to deal with model misspecifications or with complex models. Also the full\nand the composite likelihoods are both special instances of scoring rules.\n  The aim of this paper is twofold. Firstly, we discuss the use of scoring\nrules in the Bayes formula in order to compute a posterior distribution, named\nSR-posterior distribution, and we derive its asymptotic normality. Secondly, we\npropose a procedure for building default priors for the unknown parameter of\ninterest that can be used to update the information provided by the scoring\nrule in the SR-posterior distribution. In particular, a reference prior is\nobtained by maximizing the average $\\alpha-$divergence from the SR-posterior\ndistribution. For $0 \\leq |\\alpha|<1$, the result is a Jeffreys-type prior that\nis proportional to the square root of the determinant of the Godambe\ninformation matrix associated to the scoring rule. Some examples are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 12:40:53 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 15:10:30 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Giummol\u00e8", "Federica", ""], ["Mameli", "Valentina", ""], ["Ruli", "Erlis", ""], ["Ventura", "Laura", ""]]}, {"id": "1711.10940", "submitter": "Manoel Santos Neto Santos-Neto", "authors": "Marcelo Bourguignon, Josemar Rodrigues and Manoel Santos-Neto", "title": "Extended Poisson INAR(1) processes with equidispersion, underdispersion\n  and overdispersion", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": "10.1080/02664763.2018.1458216", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real count data time series often show the phenomenon of the underdispersion\nand overdispersion. In this paper, we develop two extensions of the first-order\ninteger-valued autoregressive process with Poisson innovations, based on\nbinomial thinning, for modeling integer-valued time series with equidispersion,\nunderdispersion and overdispersion. The main properties of the models are\nderived. The methods of conditional maximum likelihood, Yule-Walker and\nconditional least squares are used for estimating the parameters, and their\nasymptotic properties are established. We also use a test based on our\nprocesses for checking if the count time series considered is overdispersed or\nunderdispersed. The proposed models are fitted to time series of number of\nweekly sales and of cases of family violence illustrating its capabilities in\nchallenging cases of overdispersed and underdispersed count data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:22:37 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Bourguignon", "Marcelo", ""], ["Rodrigues", "Josemar", ""], ["Santos-Neto", "Manoel", ""]]}, {"id": "1711.10967", "submitter": "Kevin Xu", "authors": "Ruthwik R. Junuthula, Maysam Haghdan, Kevin S. Xu, and Vijay K.\n  Devabhaktuni", "title": "The Block Point Process Model for Continuous-Time Event-Based Dynamic\n  Networks", "comments": "To appear at The Web Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of analyzing timestamped relational events between a\nset of entities, such as messages between users of an on-line social network.\nSuch data are often analyzed using static or discrete-time network models,\nwhich discard a significant amount of information by aggregating events over\ntime to form network snapshots. In this paper, we introduce a block point\nprocess model (BPPM) for continuous-time event-based dynamic networks. The BPPM\nis inspired by the well-known stochastic block model (SBM) for static networks.\nWe show that networks generated by the BPPM follow an SBM in the limit of a\ngrowing number of nodes. We use this property to develop principled and\nefficient local search and variational inference procedures initialized by\nregularized spectral clustering. We fit BPPMs with exponential Hawkes processes\nto analyze several real network data sets, including a Facebook wall post\nnetwork with over 3,500 nodes and 130,000 events.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 17:11:28 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 01:56:18 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Junuthula", "Ruthwik R.", ""], ["Haghdan", "Maysam", ""], ["Xu", "Kevin S.", ""], ["Devabhaktuni", "Vijay K.", ""]]}, {"id": "1711.10982", "submitter": "Simon Shaw", "authors": "Simon C. Shaw and Michael Goldstein", "title": "Bayesian analysis of finite population sampling in multivariate\n  co-exchangeable structures with separable covariance matric", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the effect of finite population sampling in design problems with\nmany variables cross-classified in many ways. In particular, we investigate\ndesigns where we wish to sample individuals belonging to different groups for\nwhich the underlying covariance matrices are separable between groups and\nvariables. We exploit the generalised conditional independence structure of the\nmodel to show how the analysis of the full model can be reduced to an\ninterpretable series of lower dimensional problems. The types of information we\ngain by sampling are identified with the orthogonal canonical directions. We\nfirst solve a variable problem, which utilises the powerful properties of the\nadjustment of second-order exchangeable vectors, which has the same qualitative\nfeatures, represented by the underlying canonical variable directions,\nirrespective of chosen group, population size or sample size. We then solve a\nseries of group problems which in a balanced design reduce to the sampling of\nsecond-order exchangeable vectors. If the population sizes are finite then the\nqualitative and quantitative features of each group problem will depend upon\nthe sampling fractions in each group, mimicking the infinite problem when the\nsampling fractions in each group are the same.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:01:28 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Shaw", "Simon C.", ""], ["Goldstein", "Michael", ""]]}, {"id": "1711.11034", "submitter": "Lingfei Wang", "authors": "Lingfei Wang and Tom Michoel", "title": "Wisdom of the crowd from unsupervised dimension reduction", "comments": "12 pages, 4 figures. Supplementary in sup folder of source files. 5\n  sup figures, 2 sup tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wisdom of the crowd, the collective intelligence derived from responses of\nmultiple human or machine individuals to the same questions, can be more\naccurate than each individual, and improve social decision-making and\nprediction accuracy. This can also integrate multiple programs or datasets,\neach as an individual, for the same predictive questions. Crowd wisdom\nestimates each individual's independent error level arising from their limited\nknowledge, and finds the crowd consensus that minimizes the overall error.\nHowever, previous studies have merely built isolated, problem-specific models\nwith limited generalizability, and mainly for binary (yes/no) responses. Here\nwe show with simulation and real-world data that the crowd wisdom problem is\nanalogous to one-dimensional unsupervised dimension reduction in machine\nlearning. This provides a natural class of crowd wisdom solutions, such as\nprincipal component analysis and Isomap, which can handle binary and also\ncontinuous responses, like confidence levels, and consequently can be more\naccurate than existing solutions. They can even outperform\nsupervised-learning-based collective intelligence that is calibrated on\nhistorical performance of individuals, e.g. penalized linear regression and\nrandom forest. This study unifies crowd wisdom and unsupervised dimension\nreduction, and thereupon introduces a broad range of highly-performing and\nwidely-applicable crowd wisdom methods. As the costs for data acquisition and\nprocessing rapidly decrease, this study will promote and guide crowd wisdom\napplications in the social and natural sciences, including data fusion,\nmeta-analysis, crowd-sourcing, and committee decision making.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:40:49 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Wang", "Lingfei", ""], ["Michoel", "Tom", ""]]}, {"id": "1711.11057", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Y. Samuel Wang, Elena A. Erosheva", "title": "On the use of bootstrap with variational inference: Theory,\n  interpretation, and a two-sample test example", "comments": "Accepted to the Annals of Applied Statistics; 34 pages, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a general approach for approximating complex density\nfunctions, such as those arising in latent variable models, popular in machine\nlearning. It has been applied to approximate the maximum likelihood estimator\nand to carry out Bayesian inference, however, quantification of uncertainty\nwith variational inference remains challenging from both theoretical and\npractical perspectives. This paper is concerned with developing uncertainty\nmeasures for variational inference by using bootstrap procedures. We first\ndevelop two general bootstrap approaches for assessing the uncertainty of a\nvariational estimate and the study the underlying bootstrap theory in both\nfixed- and increasing-dimension settings. We then use the bootstrap approach\nand our theoretical results in the context of mixed membership modeling with\nmultivariate binary data on functional disability from the National Long Term\nCare Survey. We carry out a two-sample approach to test for changes in the\nrepeated measures of functional disability for the subset of individuals\npresent in 1989 and 1994 waves.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:08:07 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 03:35:00 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Wang", "Y. Samuel", ""], ["Erosheva", "Elena A.", ""]]}, {"id": "1711.11190", "submitter": "Paul McNicholas", "authors": "Anjali Silva, Steven J. Rothstein, Paul D. McNicholas and Sanjeena\n  Subedi", "title": "A Multivariate Poisson-Log Normal Mixture Model for Clustering\n  Transcriptome Sequencing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data of discrete and skewed nature is commonly encountered\nin high-throughput sequencing studies. Analyzing the network itself or the\ninterplay between genes in this type of data continues to present many\nchallenges. As data visualization techniques become cumbersome for higher\ndimensions and unconvincing when there is no clear separation between\nhomogeneous subgroups within the data, cluster analysis provides an intuitive\nalternative. The aim of applying mixture model-based clustering in this context\nis to discover groups of co-expressed genes, which can shed light on biological\nfunctions and pathways of gene products. A mixture of multivariate Poisson-Log\nNormal (MPLN) model is proposed for clustering of high-throughput transcriptome\nsequencing data. The MPLN model is able to fit a wide range of correlation and\noverdispersion situations, and is ideal for modeling multivariate count data\nfrom RNA sequencing studies. Parameter estimation is carried out via a Markov\nchain Monte Carlo expectation-maximization algorithm (MCMC-EM), and information\ncriteria are used for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 02:04:15 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Silva", "Anjali", ""], ["Rothstein", "Steven J.", ""], ["McNicholas", "Paul D.", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "1711.11239", "submitter": "Joseph Antonelli", "authors": "Joseph Antonelli, Maitreyi Mazumdar, David Bellinger, David C.\n  Christiani, Robert Wright, Brent A. Coull", "title": "Estimating the health effects of environmental mixtures using Bayesian\n  semiparametric regression and sparsity inducing priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are routinely exposed to mixtures of chemical and other environmental\nfactors, making the quantification of health effects associated with\nenvironmental mixtures a critical goal for establishing environmental policy\nsufficiently protective of human health. The quantification of the effects of\nexposure to an environmental mixture poses several statistical challenges. It\nis often the case that exposure to multiple pollutants interact with each other\nto affect an outcome. Further, the exposure-response relationship between an\noutcome and some exposures, such as some metals, can exhibit complex, nonlinear\nforms, since some exposures can be beneficial and detrimental at different\nranges of exposure. To estimate the health effects of complex mixtures we\npropose a flexible Bayesian approach that allows exposures to interact with\neach other and have nonlinear relationships with the outcome. We induce\nsparsity using multivariate spike and slab priors to determine which exposures\nare associated with the outcome, and which exposures interact with each other.\nThe proposed approach is interpretable, as we can use the posterior\nprobabilities of inclusion into the model to identify pollutants that interact\nwith each other. We illustrate our approach's ability to estimate complex\nfunctions using simulated data, and apply our method to two studies to\ndetermine which environmental pollutants adversely affect health.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 05:36:17 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 23:16:31 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 19:08:58 GMT"}, {"version": "v4", "created": "Fri, 21 Jun 2019 21:12:08 GMT"}, {"version": "v5", "created": "Tue, 29 Oct 2019 22:33:47 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Antonelli", "Joseph", ""], ["Mazumdar", "Maitreyi", ""], ["Bellinger", "David", ""], ["Christiani", "David C.", ""], ["Wright", "Robert", ""], ["Coull", "Brent A.", ""]]}, {"id": "1711.11286", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao and Dylan S. Small and Bhaswar B. Bhattacharya", "title": "Sensitivity analysis for inverse probability weighting estimators via\n  the percentile bootstrap", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the estimand in missing data problems and observational studies,\nit is common to base the statistical estimation on the \"missing at random\" and\n\"no unmeasured confounder\" assumptions. However, these assumptions are\nunverifiable using empirical data and pose serious threats to the validity of\nthe qualitative conclusions of the statistical inference. A sensitivity\nanalysis asks how the conclusions may change if the unverifiable assumptions\nare violated to a certain degree. In this paper we consider a marginal\nsensitivity model which is a natural extension of Rosenbaum's sensitivity model\nthat is widely used for matched observational studies. We aim to construct\nconfidence intervals based on inverse probability weighting estimators, such\nthat asymptotically the intervals have at least nominal coverage of the\nestimand whenever the data generating distribution is in the collection of\nmarginal sensitivity models. We use a percentile bootstrap and a generalized\nminimax/maximin inequality to transform this intractable problem to a linear\nfractional programming problem, which can be solved very efficiently. We\nillustrate our method using a real dataset to estimate the causal effect of\nfish consumption on blood mercury level.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 09:44:09 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 14:52:28 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Small", "Dylan S.", ""], ["Bhattacharya", "Bhaswar B.", ""]]}, {"id": "1711.11426", "submitter": "Xia Cui", "authors": "Lu Lin and Lili Liu and Xia Cui", "title": "A simple and efficient profile likelihood for semiparametric exponential\n  family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semiparametric exponential family proposed by Ning et al. (2017) is an\nextension of the parametric exponential family to the case with a nonparametric\nbase measure function. Such a distribution family has potential application in\nsome areas such as high dimensional data analysis. However, the methodology for\nachieving the semiparametric efficiency has not been proposed in the existing\nliterature. In this paper, we propose a profile likelihood to efficiently\nestimate both parameter and nonparametric function. Due to the use of the least\nfavorable curve in the procedure of profile likelihood, the semiparametric\nefficiency is achieved successfully and the estimation bias is reduced\nsignificantly. Moreover, by making the most of the structure information of the\nsemiparametric exponential family, the estimator of the least favorable curve\nhas an explicit expression. It ensures that the newly proposed profile\nlikelihood can be implemented and is computationally simple. Simulation studies\ncan illustrate that our proposal is much better than the existing methodology\nfor most cases under study, and is robust to the different model conditions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 14:32:26 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Lin", "Lu", ""], ["Liu", "Lili", ""], ["Cui", "Xia", ""]]}, {"id": "1711.11488", "submitter": "David Woods", "authors": "Frederick Kin Hing Phoa and Yi-Hua Liao and David C. Woods and\n  Shah-Kae Chou", "title": "Summary of effect aliasing structure (SEAS): new descriptive statistics\n  for factorial and supersaturated designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the assessment and selection of supersaturated designs, the aliasing\nstructure of interaction effects is usually ignored by traditional criteria\nsuch as $E(s^2)$-optimality. We introduce the Summary of Effect Aliasing\nStructure (SEAS) for assessing the aliasing structure of supersaturated\ndesigns, and other non-regular fractional factorial designs, that takes account\nof interaction terms and provides more detail than usual summaries such as\n(generalized) resolution and wordlength patterns. The new summary consists of\nthree criteria, abbreviated as MAP: (1) Maximum dependency aliasing pattern;\n(2) Average square aliasing pattern; and (3) Pairwise dependency ratio. These\ncriteria provided insight when traditional criteria fail to differentiate\nbetween designs. We theoretically study the relationship between the MAP\ncriteria and traditional quantities, and demonstrate the use of SEAS for\ncomparing some example supersaturated designs, including designs suggested in\nthe literature. We also propose a variant of SEAS to measure the aliasing\nstructure for individual columns of a design, and use it to choose assignments\nof factors to columns for an $E(s^2)$-optimal design.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:12:15 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 09:01:18 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 15:29:37 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Phoa", "Frederick Kin Hing", ""], ["Liao", "Yi-Hua", ""], ["Woods", "David C.", ""], ["Chou", "Shah-Kae", ""]]}, {"id": "1711.11501", "submitter": "Mengyang Gu", "authors": "Mengyang Gu and Yanxun Xu", "title": "Nonseparable Gaussian Stochastic Process: A Unified View and\n  Computational Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian stochastic process (GaSP) has been widely used as a prior over\nfunctions due to its flexibility and tractability in modeling. However, the\ncomputational cost in evaluating the likelihood is $O(n^3)$, where $n$ is the\nnumber of observed points in the process, as it requires to invert the\ncovariance matrix. This bottleneck prevents GaSP being widely used in\nlarge-scale data. We propose a general class of nonseparable GaSP models for\nmultiple functional observations with a fast and exact algorithm, in which the\ncomputation is linear ($O(n)$) and exact, requiring no approximation to compute\nthe likelihood. We show that the commonly used linear regression and separable\nmodels are special cases of the proposed nonseparable GaSP model. Through the\nstudy of an epigenetic application, the proposed nonseparable GaSP model can\naccurately predict the genome-wide DNA methylation levels and compares\nfavorably to alternative methods, such as linear regression, random forests and\nlocalized Kriging method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:42:27 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 19:12:27 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Gu", "Mengyang", ""], ["Xu", "Yanxun", ""]]}]