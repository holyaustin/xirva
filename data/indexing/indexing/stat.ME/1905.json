[{"id": "1905.00008", "submitter": "Christopher Jackson", "authors": "Rob Johnson, James Woodcock, Audrey de Nazelle, Thiago de Sa, Rahul\n  Goel, Marko Tainio, Christopher Jackson", "title": "A guide to Value of Information methods for prioritising research in\n  health impact modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health impact simulation models are used to predict how a proposed\nintervention or scenario will affect public health outcomes, based on available\ndata and knowledge of the process. The outputs of these models are uncertain\ndue to uncertainty in the structure and inputs to the model. In order to assess\nthe extent of uncertainty in the outcome we must quantify all potentially\nrelevant uncertainties. Then to reduce uncertainty we should obtain and analyse\nnew data, but it may be unclear which parts of the model would benefit from\nsuch extra research.\n  This paper presents methods for uncertainty quantification and research\nprioritisation in health impact models based on Value of Information (VoI)\nanalysis. Specifically, we\n  1. discuss statistical methods for quantifying uncertainty in this type of\nmodel, given the typical kinds of data that are available, which are often\nweaker than the ideal data that are desired;\n  2. show how the expected value of partial perfect information (EVPPI) can be\ncalculated to compare how uncertainty in each model parameter influences\nuncertainty in the output;\n  3. show how research time can be prioritised efficiently, in the light of\nwhich components contribute most to outcome uncertainty.\n  The same methods can be used whether the purpose of the model is to estimate\nquantities of interest to a policy maker, or to explicitly decide between\npolicies. We demonstrate how these methods might be used in a model of the\nimpact of air pollution on health outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 09:47:41 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Johnson", "Rob", ""], ["Woodcock", "James", ""], ["de Nazelle", "Audrey", ""], ["de Sa", "Thiago", ""], ["Goel", "Rahul", ""], ["Tainio", "Marko", ""], ["Jackson", "Christopher", ""]]}, {"id": "1905.00048", "submitter": "Halley Brantley", "authors": "Halley Brantley, Montserrat Fuentes, Joseph Guinness, Eben Thoma", "title": "Smooth Density Spatial Quantile Regression", "comments": "Submitted to Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the properties and demonstrate the desirability of a model-based\nmethod for estimating the spatially-varying effects of covariates on the\nquantile function. By modeling the quantile function as a combination of\nI-spline basis functions and Pareto tail distributions, we allow for flexible\nparametric modeling of the extremes while preserving non-parametric flexibility\nin the center of the distribution. We further establish that the model\nguarantees the desired degree of differentiability in the density function and\nenables the estimation of non-stationary covariance functions dependent on the\npredictors. We demonstrate through a simulation study that the proposed method\nproduces more efficient estimates of the effects of predictors than other\nmethods, particularly in distributions with heavy tails. To illustrate the\nutility of the model we apply it to measurements of benzene collected around an\noil refinery to determine the effect of an emission source within the refinery\non the distribution of the fence line measurements.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 18:15:55 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Brantley", "Halley", ""], ["Fuentes", "Montserrat", ""], ["Guinness", "Joseph", ""], ["Thoma", "Eben", ""]]}, {"id": "1905.00105", "submitter": "Christian Staerk", "authors": "Christian Staerk, Maria Kateri, Ioannis Ntzoufras", "title": "High-dimensional variable selection via low-dimensional adaptive\n  learning", "comments": null, "journal-ref": "Electronic Journal of Statistics, 15(1), 830-879 (2021)", "doi": "10.1214/21-EJS1797", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stochastic search method, the so-called Adaptive Subspace (AdaSub) method,\nis proposed for variable selection in high-dimensional linear regression\nmodels. The method aims at finding the best model with respect to a certain\nmodel selection criterion and is based on the idea of adaptively solving\nlow-dimensional sub-problems in order to provide a solution to the original\nhigh-dimensional problem. Any of the usual $\\ell_0$-type model selection\ncriteria can be used, such as Akaike's Information Criterion (AIC), the\nBayesian Information Criterion (BIC) or the Extended BIC (EBIC), with the last\nbeing particularly suitable for high-dimensional cases. The limiting properties\nof the new algorithm are analysed and it is shown that, under certain\nconditions, AdaSub converges to the best model according to the considered\ncriterion. In a simulation study, the performance of AdaSub is investigated in\ncomparison to alternative methods. The effectiveness of the proposed method is\nillustrated via various simulated datasets and a high-dimensional real data\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:32:23 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 10:03:19 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 12:41:23 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Staerk", "Christian", ""], ["Kateri", "Maria", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "1905.00177", "submitter": "Jay Bartroff", "authors": "Xinrui He and Jay Bartroff", "title": "Asymptotically optimal sequential FDR and pFDR control with (or without)\n  prior information on the number of signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate asymptotically optimal multiple testing procedures for streams\nof sequential data in the context of prior information on the number of false\nnull hypotheses (\"signals\"). We show that the \"gap\" and \"gap-intersection\"\nprocedures, recently proposed and shown by Song and Fellouris (2017, Electron.\nJ. Statist.) to be asymptotically optimal for controlling type 1 and 2\nfamilywise error rates (FWEs), are also asymptotically optimal for controlling\nFDR/FNR when their critical values are appropriately adjusted. Generalizing\nthis result, we show that these procedures, again with appropriately adjusted\ncritical values, are asymptotically optimal for controlling any multiple\ntesting error metric that is bounded between multiples of FWE in a certain\nsense. This class of metrics includes FDR/FNR but also pFDR/pFNR, the\nper-comparison and per-family error rates, and the false positive rate. Our\nanalysis includes asymptotic regimes in which the number of null hypotheses\napproaches $\\infty$ as the type 1 and 2 error metrics approach $0$.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 04:01:17 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 18:30:56 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 03:28:47 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["He", "Xinrui", ""], ["Bartroff", "Jay", ""]]}, {"id": "1905.00241", "submitter": "Matteo Quartagno", "authors": "Matteo Quartagno, A. Sarah Walker, Abdel G. Babiker, Rebecca M.\n  Turner, Mahesh K.B. Parmar, Andrew Copas, Ian R. White", "title": "Handling an uncertain control group event risk in non-inferiority\n  trials: non-inferiority frontiers and the power-stabilising transformation", "comments": "~4000 words + 5 figures + 1 table + additional material (2\n  appendices, 1 table, 4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Non-inferiority (NI) trials are increasingly used to evaluate new\ntreatments expected to have secondary advantages over standard of care, but\nsimilar efficacy on the primary outcome. When designing a NI trial with a\nbinary primary outcome, the choice of effect measure for the NI margin has an\nimportant effect on sample size calculations; furthermore, if the control event\nrisk observed is markedly different from that assumed, the trial can quickly\nlose power or the results become difficult to interpret. Methods. We propose a\nnew way of designing NI trials to overcome the issues raised by unexpected\ncontrol event risks by specifying a NI frontier, i.e. a curve defining the most\nappropriate non-inferiority margin for each possible value of control event\nrisk. We propose a fixed arcsine difference frontier, the power-stabilising\ntransformation for binary outcomes. We propose and compare three ways of\ndesigning a trial using this frontier. Results. Testing and reporting on the\narcsine scale leads to results which are challenging to interpret clinically.\nWorking on the arcsine scale generally requires a larger sample size compared\nto the risk difference scale. Therefore, working on the risk difference scale,\nmodifying the margin after observing the control event risk, might be\npreferable, as it requires a smaller sample size. However, this approach tends\nto slightly inflate type I error rate; a solution is to use a lower\nsignificance level for testing. When working on the risk ratio scale, the same\napproach leads to power levels above the nominal one, maintaining type I error\nunder control. Conclusions. Our proposed methods of designing NI trials using\npower-stabilising frontiers make trial design more resilient to unexpected\nvalues of the control event risk, at the only cost of requiring larger sample\nsizes when the goal is to report results on the risk difference scale.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 10:00:14 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Quartagno", "Matteo", ""], ["Walker", "A. Sarah", ""], ["Babiker", "Abdel G.", ""], ["Turner", "Rebecca M.", ""], ["Parmar", "Mahesh K. B.", ""], ["Copas", "Andrew", ""], ["White", "Ian R.", ""]]}, {"id": "1905.00266", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Narumasa Tsutsumida, Takahiro Yoshida, Tomoki\n  Nakaya, Binbin Lu", "title": "Scalable GWR: A linear-time algorithm for large-scale geographically\n  weighted regression with polynomial kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a number of studies have developed fast geographically weighted\nregression (GWR) algorithms for large samples, none of them has achieved\nlinear-time estimation, which is considered a requisite for big data analysis\nin machine learning, geostatistics, and related domains. Against this backdrop,\nthis study proposes a scalable GWR (ScaGWR) for large datasets. The key\nimprovement is the calibration of the model through a pre-compression of the\nmatrices and vectors whose size depends on the sample size, prior to the\nleave-one-out cross-validation, which is the heaviest computational step in\nconventional GWR. This pre-compression allows us to run the proposed GWR\nextension so that its computation time increases linearly with the sample size.\nWith this improvement, the ScaGWR can be calibrated with one million\nobservations without parallelization. Moreover, the ScaGWR estimator can be\nregarded as an empirical Bayesian estimator that is more stable than the\nconventional GWR estimator. We compare the ScaGWR with the conventional GWR in\nterms of estimation accuracy and computational efficiency using a Monte Carlo\nsimulation. Then, we apply these methods to a US income analysis. The code for\nScaGWR is available in the R package scgwr. The code is embedded into C++ code\nand implemented in another R package, GWmodel.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 11:17:47 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 06:34:26 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 13:39:17 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Murakami", "Daisuke", ""], ["Tsutsumida", "Narumasa", ""], ["Yoshida", "Takahiro", ""], ["Nakaya", "Tomoki", ""], ["Lu", "Binbin", ""]]}, {"id": "1905.00353", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene, Cesar Cristancho, Mariana Ospina, Domingo Morales", "title": "Prevalence of international migration: an alternative for small area\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an alternative procedure for estimating the prevalence\nof international migration at the municipal level in Colombia. The new\nmethodology uses the empirical best linear unbiased predictor based on a\nFay-Herriot model with target and auxiliary variables available from census\nstudies and from the Demographic and Health Survey. The proposed alternative\nproduces prevalence estimates which are consistent with sample sizes and\ndemographic dynamics in Colombia. Additionally, the estimated coefficients of\nvariation are lower than 20% for municipalities and large\ndemographically-relevant capital cities and therefore estimates may be\nconsidered as reliable.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 20:15:26 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Fuquene", "Jairo", ""], ["Cristancho", "Cesar", ""], ["Ospina", "Mariana", ""], ["Morales", "Domingo", ""]]}, {"id": "1905.00365", "submitter": "Srikanth Namuduri", "authors": "Colleen M. Farrelly, Srikanth Namuduri, Uchenna Chukwu", "title": "Quantum Generalized Linear Models", "comments": "10 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG quant-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Generalized linear models (GLM) are link function based statistical models.\nMany supervised learning algorithms are extensions of GLMs and have link\nfunctions built into the algorithm to model different outcome distributions.\nThere are two major drawbacks when using this approach in applications using\nreal world datasets. One is that none of the link functions available in the\npopular packages is a good fit for the data. Second, it is computationally\ninefficient and impractical to test all the possible distributions to find the\noptimum one. In addition, many GLMs and their machine learning extensions\nstruggle on problems of overdispersion in Tweedie distributions. In this paper\nwe propose a quantum extension to GLM that overcomes these drawbacks. A quantum\ngate with non-Gaussian transformation can be used to continuously deform the\noutcome distribution from known results. In doing so, we eliminate the need for\na link function. Further, by using an algorithm that superposes all possible\ndistributions to collapse to fit a dataset, we optimize the model in a\ncomputationally efficient way. We provide an initial proof-of-concept by\ntesting this approach on both a simulation of overdispersed data and then on a\nbenchmark dataset, which is quite overdispersed, and achieved state of the art\nresults. This is a game changer in several applied fields, such as part failure\nmodeling, medical research, actuarial science, finance and many other fields\nwhere Tweedie regression and overdispersion are ubiquitous.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 16:29:04 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Farrelly", "Colleen M.", ""], ["Namuduri", "Srikanth", ""], ["Chukwu", "Uchenna", ""]]}, {"id": "1905.00419", "submitter": "Rico Krueger", "authors": "Rico Krueger, Prateek Bansal, Michel Bierlaire, Ricardo A. Daziano,\n  Taha H. Rashidi", "title": "Variational Bayesian Inference for Mixed Logit Models with Unobserved\n  Inter- and Intra-Individual Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB), a method originating from machine learning, enables\nfast and scalable estimation of complex probabilistic models. Thus far,\napplications of VB in discrete choice analysis have been limited to mixed logit\nmodels with unobserved inter-individual taste heterogeneity. However, such a\nmodel formulation may be too restrictive in panel data settings, since tastes\nmay vary both between individuals as well as across choice tasks encountered by\nthe same individual. In this paper, we derive a VB method for posterior\ninference in mixed logit models with unobserved inter- and intra-individual\nheterogeneity. In a simulation study, we benchmark the performance of the\nproposed VB method against maximum simulated likelihood (MSL) and Markov chain\nMonte Carlo (MCMC) methods in terms of parameter recovery, predictive accuracy\nand computational efficiency. The simulation study shows that VB can be a fast,\nscalable and accurate alternative to MSL and MCMC estimation, especially in\napplications in which fast predictions are paramount. VB is observed to be\nbetween 2.8 and 17.7 times faster than the two competing methods, while\naffording comparable or superior accuracy. Besides, the simulation study\ndemonstrates that a parallelised implementation of the MSL estimator with\nanalytical gradients is a viable alternative to MCMC in terms of both\nestimation accuracy and computational efficiency, as the MSL estimator is\nobserved to be between 0.9 and 2.1 times faster than MCMC.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 11:58:13 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 10:54:25 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 06:07:25 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Krueger", "Rico", ""], ["Bansal", "Prateek", ""], ["Bierlaire", "Michel", ""], ["Daziano", "Ricardo A.", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1905.00466", "submitter": "Byol Kim", "authors": "Byol Kim, Song Liu, Mladen Kolar", "title": "Two-sample inference for high-dimensional Markov networks", "comments": "81 pages, 18 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are frequently used in sciences to represent conditional\nindependence relationships underlying observed variables arising from a complex\nsystem. It is often of interest to understand how an underlying network differs\nbetween two conditions. In this paper, we develop methods for comparing a pair\nof high-dimensional Markov networks where we allow the number of observed\nvariables to increase with the sample sizes. By taking the density ratio\napproach, we are able to learn the network difference directly and avoid\nestimating the individual graphs. Our methods are thus applicable even when the\nindividual networks are dense as long as their difference is sparse. We prove\nfinite-sample Gaussian approximation error bounds for the estimator we\nconstruct under significantly weaker assumptions than are typically required\nfor model selection consistency. Furthermore, we propose bootstrap procedures\nfor estimating quantiles of a max-type statistics based on our estimator, and\nshow how they can be used to test the equality of two Markov networks or\nconstruct simultaneous confidence intervals. The performance of our methods is\ndemonstrated through extensive simulations. The scientific usefulness is\nillustrated with an analysis of a new fMRI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 19:39:48 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 22:00:03 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Kim", "Byol", ""], ["Liu", "Song", ""], ["Kolar", "Mladen", ""]]}, {"id": "1905.00516", "submitter": "Steffen Lauritzen", "authors": "Steffen Lauritzen, Caroline Uhler, Piotr Zwiernik", "title": "Total positivity in exponential families with application to binary\n  variables", "comments": null, "journal-ref": "Annals of Statistics 2021, Vol. 49, 1436-1459", "doi": "10.1214/20-AOS2007", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exponential families of distributions that are multivariate totally\npositive of order 2 (MTP2), show that these are convex exponential families,\nand derive conditions for existence of the MLE. Quadratic exponential familes\nof MTP2 distributions contain attractive Gaussian graphical models and\nferromagnetic Ising models as special examples. We show that these are defined\nby intersecting the space of canonical parameters with a polyhedral cone whose\nfaces correspond to conditional independence relations. Hence MTP2 serves as an\nimplicit regularizer for quadratic exponential families and leads to sparsity\nin the estimated graphical model. We prove that the maximum likelihood\nestimator (MLE) in an MTP2 binary exponential family exists if and only if both\nof the sign patterns $(1,-1)$ and $(-1,1)$ are represented in the sample for\nevery pair of variables; in particular, this implies that the MLE may exist\nwith $n=d$ observations, in stark contrast to unrestricted binary exponential\nfamilies where $2^d$ observations are required. Finally, we provide a novel and\nglobally convergent algorithm for computing the MLE for MTP2 Ising models\nsimilar to iterative proportional scaling and apply it to the analysis of data\nfrom two psychological disorders.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 22:03:50 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 10:19:53 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2020 08:39:25 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lauritzen", "Steffen", ""], ["Uhler", "Caroline", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1905.00630", "submitter": "Juergen Lerner", "authors": "J\\\"urgen Lerner and Alessandro Lomi", "title": "Reliability of relational event model estimates under sampling: how to\n  fit a relational event model to 360 million dyadic events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the reliability of relational event model parameters estimated\nunder two sampling schemes: (1) uniform sampling from the observed events and\n(2) case-control sampling which samples non-events, or null dyads (\"controls\"),\nfrom a suitably defined risk set. We experimentally determine the variability\nof estimated parameters as a function of the number of sampled events and\ncontrols per event, respectively. Results suggest that relational event models\ncan be reliably fitted to networks with more than 12 million nodes connected by\nmore than 360 million dyadic events by analyzing a sample of some tens of\nthousands of events and a small number of controls per event. Using data that\nwe collected on the Wikipedia editing network, we illustrate how network\neffects commonly included in empirical studies based on relational event models\nneed widely different sample sizes to be estimated reliably. For our analysis\nwe use an open-source software which implements the two sampling schemes,\nallowing analysts to fit and analyze relational event models to the same or\nother data that may be collected in different empirical settings, varying\nsample parameters or model specification.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:13:12 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 16:00:30 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Lerner", "J\u00fcrgen", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1905.00744", "submitter": "Yinchu Zhu", "authors": "Jelena Bradic, Stefan Wager, Yinchu Zhu", "title": "Sparsity Double Robust Inference of Average Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular methods for building confidence intervals on causal effects\nunder high-dimensional confounding require strong \"ultra-sparsity\" assumptions\nthat may be difficult to validate in practice. To alleviate this difficulty, we\nhere study a new method for average treatment effect estimation that yields\nasymptotically exact confidence intervals assuming that either the conditional\nresponse surface or the conditional probability of treatment allows for an\nultra-sparse representation (but not necessarily both). This guarantee allows\nus to provide valid inference for average treatment effect in high dimensions\nunder considerably more generality than available baselines. In addition, we\nshowcase that our results are semi-parametrically efficient.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 13:47:15 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Bradic", "Jelena", ""], ["Wager", "Stefan", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1905.00803", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri and Mark S. Handcock", "title": "A Conditional Empirical Likelihood Based Method for Model Parameter\n  Estimation from Complex survey Datasets", "comments": null, "journal-ref": "Statistics and Applications, Volume 16, No. 1, 2018 (New Series),\n  pp 245-268", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an empirical likelihood framework for inference for a statistical\nmodel based on an informative sampling design. Covariate information is\nincorporated both through the weights and the estimating equations. The\nestimator is based on conditional weights. We show that under usual conditions,\nwith population size increasing unbounded, the estimates are strongly\nconsistent, asymptotically unbiased and normally distributed. Our framework\nprovides additional justification for inverse probability weighted score\nestimators in terms of conditional empirical likelihood. In doing so, it\nbridges the gap between design-based and model-based modes of inference in\nsurvey sampling settings. We illustrate these ideas with an application to an\nelectoral survey.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:24:36 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Chaudhuri", "Sanjay", ""], ["Handcock", "Mark S.", ""]]}, {"id": "1905.00959", "submitter": "R\\'emy Garnier", "authors": "Pierre Alquier, Karine Bertin, Paul Doukhan, R\\'emy Garnier", "title": "High dimensional VAR with low rank transition", "comments": null, "journal-ref": "Statistics and Computing, 2020, vol. 30, pp. 1139-1153", "doi": "10.1007/s11222-020-09929-7", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vector auto-regressive (VAR) model with a low-rank constraint on\nthe transition matrix. This new model is well suited to predict\nhigh-dimensional series that are highly correlated, or that are driven by a\nsmall number of hidden factors. We study estimation, prediction, and rank\nselection for this model in a very general setting. Our method shows excellent\nperformances on a wide variety of simulated datasets. On macro-economic data\nfrom Giannone et al. (2015), our method is competitive with state-of-the-art\nmethods in small dimension, and even improves on them in high dimension.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 21:00:58 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 13:10:52 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Alquier", "Pierre", ""], ["Bertin", "Karine", ""], ["Doukhan", "Paul", ""], ["Garnier", "R\u00e9my", ""]]}, {"id": "1905.00977", "submitter": "Adriano Zambom", "authors": "Adriano Zanin Zambom, Julian A. Collazos, Ronaldo Dias", "title": "Selection of the Number of Clusters in Functional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the number $K$ of clusters in a dataset is one of the most\ndifficult problems in clustering analysis. A choice of $K$ that correctly\ncharacterizes the features of the data is essential for building meaningful\nclusters. In this paper we tackle the problem of estimating the number of\nclusters in functional data analysis by introducing a new measure that can be\nused with different procedures in selecting the optimal $K$. The main idea is\nto use a combination of two test statistics, which measure the lack of\nparallelism and the mean distance between curves, to compute criteria such as\nthe within and between cluster sum of squares. Simulations in challenging\nscenarios suggest that procedures using this measure can detect the correct\nnumber of clusters more frequently than existing methods in the literature. The\napplication of the proposed method is illustrated on several real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 21:57:01 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Zambom", "Adriano Zanin", ""], ["Collazos", "Julian A.", ""], ["Dias", "Ronaldo", ""]]}, {"id": "1905.01036", "submitter": "Sijia Xiang", "authors": "Sijia Xiang, Weixin Yao", "title": "Robust Model Selection for Finite Mixture of Regression Models Through\n  Trimming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a new variable selection technique through\ntrimming for finite mixture of regression models. Compared to the traditional\nvariable selection techniques, the new method is robust and not sensitive to\noutliers. The estimation algorithm is introduced and numerical studies are\nconducted to examine the finite sample performance of the proposed procedure\nand to compare it with other existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 05:55:18 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Xiang", "Sijia", ""], ["Yao", "Weixin", ""]]}, {"id": "1905.01059", "submitter": "Asaf Weinstein", "authors": "Asaf Weinstein, Aaditya Ramdas", "title": "Online Control of the False Coverage Rate and False Sign Rate", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The false coverage rate (FCR) is the expected ratio of number of constructed\nconfidence intervals (CIs) that fail to cover their respective parameters to\nthe total number of constructed CIs. Procedures for FCR control exist in the\noffline setting, but none so far have been designed with the online setting in\nmind. In the online setting, there is an infinite sequence of fixed unknown\nparameters $\\theta_t$ ordered by time. At each step, we see independent data\nthat is informative about $\\theta_t$, and must immediately make a decision\nwhether to report a CI for $\\theta_t$ or not. If $\\theta_t$ is selected for\ncoverage, the task is to determine how to construct a CI for $\\theta_t$ such\nthat $\\text{FCR} \\leq \\alpha$ for any $T\\in \\mathbb{N}$. A straightforward\nsolution is to construct at each step a $(1-\\alpha)$ level conditional CI. In\nthis paper, we present a novel solution to the problem inspired by online false\ndiscovery rate (FDR) algorithms, which only requires the statistician to be\nable to construct a marginal CI at any given level. Apart from the fact that\nmarginal CIs are usually simpler to construct than conditional ones, the\nmarginal procedure has an important qualitative advantage over the conditional\nsolution, namely, it allows selection to be determined by the candidate CI\nitself. We take advantage of this to offer solutions to some online problems\nwhich have not been addressed before. For example, we show that our general CI\nprocedure can be used to devise online sign-classification procedures that\ncontrol the false sign rate (FSR). In terms of power and length of the\nconstructed CIs, we demonstrate that the two approaches have complementary\nstrengths and weaknesses using simulations. Last, all of our methodology\napplies equally well to online FCR control for prediction intervals, having\nparticular implications for assumption-free selective conformal inference.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 08:00:52 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Weinstein", "Asaf", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1905.01195", "submitter": "Daniel Commenges", "authors": "Daniel Commenges", "title": "Causality without potential outcomes and the dynamic approach", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approaches to causal inference from observational studies have been\nproposed. Since the proposal of Rubin (1974) many works have developed a\ncounterfactual approach to causality, statistically formalized by potential\noutcomes. Pearl (2000) has put forward a theory of structural causal models\nwhich gives an important role to graphical models and do not necessarily use\npotential outcomes. On the other hand, several authors have developed a\ndynamical approach in line with Granger (1969). We analyze prospective and\nretrospective causal questions and their different modalities. Following Dawid\n(2000) we develop criticisms about the potential outcome approach and we show\nthat causal effects can be estimated without potential outcomes: in particular\ndirect computation of the marginal effect can be done by a change of\nprobability measure. Finally, we highlight the need to adopt a dynamic approach\nto causality through two examples, \"truncation by death\" and the \"obesity\nparadox\".\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 14:11:04 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Commenges", "Daniel", ""]]}, {"id": "1905.01243", "submitter": "Elena Kulinskaya", "authors": "Ilyas Bakbergenuly, David C. Hoaglin, and Elena Kulinskaya", "title": "Simulation study of estimating between-study variance and overall effect\n  in meta-analyses of log-response-ratio for lognormal data", "comments": "17 pages and full simulation results, comprising 160 figures, each\n  presenting 12 combinations of sample sizes and numbers of studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for random-effects meta-analysis require an estimate of the\nbetween-study variance, $\\tau^2$. The performance of estimators of $\\tau^2$\n(measured by bias and coverage) affects their usefulness in assessing\nheterogeneity of study-level effects, and also the performance of related\nestimators of the overall effect. For the effect measure log-response-ratio\n(LRR, also known as the logarithm of the ratio of means, RoM), we review four\npoint estimators of $\\tau^2$ (the popular methods of DerSimonian-Laird (DL),\nrestricted maximum likelihood, and Mandel and Paule (MP), and the less-familiar\nmethod of Jackson), four interval estimators for $\\tau^2$ (profile likelihood,\nQ-profile, Biggerstaff and Jackson, and Jackson), five point estimators of the\noverall effect (the four related to the point estimators of $\\tau^2$ and an\nestimator whose weights use only study-level sample sizes), and seven interval\nestimators for the overall effect (four based on the point estimators for\n$\\tau^2$, the Hartung-Knapp-Sidik-Jonkman (HKSJ) interval, a modification of\nHKSJ that uses the MP estimator of $\\tau^2$ instead of the DL estimator, and an\ninterval based on the sample-size-weighted estimator). We obtain empirical\nevidence from extensive simulations of data from lognormal distributions.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:01:54 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 19:33:47 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Bakbergenuly", "Ilyas", ""], ["Hoaglin", "David C.", ""], ["Kulinskaya", "Elena", ""]]}, {"id": "1905.01252", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Michael Gutmann, Aki Vehtari, Pekka Marttinen", "title": "Parallel Gaussian process surrogate Bayesian inference with noisy\n  likelihood evaluations", "comments": "Minor changes to the text. 37 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference when only a limited number of noisy\nlog-likelihood evaluations can be obtained. This occurs for example when\ncomplex simulator-based statistical models are fitted to data, and synthetic\nlikelihood (SL) method is used to form the noisy log-likelihood estimates using\ncomputationally costly forward simulations. We frame the inference task as a\nsequential Bayesian experimental design problem, where the log-likelihood\nfunction is modelled with a hierarchical Gaussian process (GP) surrogate model,\nwhich is used to efficiently select additional log-likelihood evaluation\nlocations. Motivated by recent progress in the related problem of batch\nBayesian optimisation, we develop various batch-sequential design strategies\nwhich allow to run some of the potentially costly simulations in parallel. We\nanalyse the properties of the resulting method theoretically and empirically.\nExperiments with several toy problems and simulation models suggest that our\nmethod is robust, highly parallelisable, and sample-efficient.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:18:50 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 11:31:22 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 15:19:26 GMT"}, {"version": "v4", "created": "Fri, 6 Mar 2020 13:35:20 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Gutmann", "Michael", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1905.01402", "submitter": "Pengfei Li", "authors": "Jiahua Chen, Pengfei Li, Jing Qin, and Tao Yu", "title": "Test for homogeneity with unordered paired observations", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some applications, an experimental unit is composed of two distinct but\nrelated subunits. The response from such a unit is $(X_{1}, X_{2})$ but we\nobserve only $Y_1 = \\min\\{X_{1},X_{2}\\}$ and $Y_2 = \\max\\{X_{1},X_{2}\\}$, i.e.,\nthe subunit identities are not observed. We call $(Y_1, Y_2)$ unordered paired\nobservations. Based on unordered paired observations $\\{(Y_{1i},\nY_{2i})\\}_{i=1}^n$, we are interested in whether the marginal distributions for\n$X_1$ and $X_2$ are identical. Testing methods are available in the literature\nunder the assumptions that $Var(X_1) = Var(X_2)$ and $Cov(X_1, X_2) = 0$.\nHowever, by extensive simulation studies, we observe that when one or both\nassumptions are violated, these methods have inflated type I errors or much\nlower powers. In this paper, we study the likelihood ratio test statistics for\nvarious scenarios and explore their limiting distributions without these\nrestrictive assumptions. Furthermore, we develop Bartlett correction formulae\nfor these statistics to enhance their precision when the sample size is not\nlarge. Simulation studies and real-data examples are used to illustrate the\nefficacy of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 01:18:01 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Chen", "Jiahua", ""], ["Li", "Pengfei", ""], ["Qin", "Jing", ""], ["Yu", "Tao", ""]]}, {"id": "1905.01413", "submitter": "Mingyuan Zhou", "authors": "Mingzhang Yin, Yuguang Yue, Mingyuan Zhou", "title": "ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient\n  Backpropagation Through Categorical Variables", "comments": "Published in ICML 2019. We have updated Section 4.2 and the Appendix\n  to reflect the improvements brought by fixing some bugs hidden in our\n  original code. Please find the Errata in the authors' websites and check the\n  updated code in Github", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the challenge of backpropagating the gradient through categorical\nvariables, we propose the augment-REINFORCE-swap-merge (ARSM) gradient\nestimator that is unbiased and has low variance. ARSM first uses variable\naugmentation, REINFORCE, and Rao-Blackwellization to re-express the gradient as\nan expectation under the Dirichlet distribution, then uses variable swapping to\nconstruct differently expressed but equivalent expectations, and finally shares\ncommon random numbers between these expectations to achieve significant\nvariance reduction. Experimental results show ARSM closely resembles the\nperformance of the true gradient for optimization in univariate settings;\noutperforms existing estimators by a large margin when applied to categorical\nvariational auto-encoders; and provides a \"try-and-see self-critic\" variance\nreduction method for discrete-action policy gradient, which removes the need of\nestimating baselines by generating a random number of pseudo actions and\nestimating their action-value functions.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 02:26:09 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 16:43:00 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Yin", "Mingzhang", ""], ["Yue", "Yuguang", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1905.01455", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin and Francisco Cuevas-Pacheco and Jean-Fran\\c{c}ois\n  Coeurjolly and Rasmus Waagepetersen", "title": "Regularized estimation for highly multivariate log Gaussian Cox\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference for highly multivariate point pattern data is\nchallenging due to complex models with large numbers of parameters. In this\npaper, we develop numerically stable and efficient parameter estimation and\nmodel selection algorithms for a class of multivariate log Gaussian Cox\nprocesses. The methodology is applied to a highly multivariate point pattern\ndata set from tropical rain forest ecology.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 08:18:44 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Cuevas-Pacheco", "Francisco", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "1905.01480", "submitter": "Haotian Xu", "authors": "Haotian Xu, St\\'ephane Guerrier, Roberto Molinari, Mucyo Karemera", "title": "Multivariate Signal Modelling with Applications to Inertial Sensor\n  Calibration", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2935902", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common approach to inertial sensor calibration for navigation purposes\nhas been to model the stochastic error signals of individual sensors\nindependently, whether as components of a single inertial measurement unit\n(IMU) in different directions or arrayed in the same direction for redundancy.\nThese signals usually have an extremely complex spectral structure that is\noften described using latent (or composite) models composed by a sum of\nunderlying models. A large amount of research in this domain has been focused\non the latter aspect through the proposal of various methods that have been\nable to improve the estimation of these models both from a computational and a\nstatistical point of view. However, the separate calibration of the individual\nsensors is still unable to take into account the dependence between each of\nthem which can have an important impact on the precision of the navigation\nsystems. In this paper, we develop a new approach to simultaneously model both\nthe individual signals and the dependence between them by studying the quantity\ncalled Wavelet Cross-Covariance and using it to extend the application of the\nGeneralized Method of Wavelet Moments. This new method can be used in other\nsettings for time series modeling, especially in cases where the dependence\namong signals may be hard to detect. Moreover, in the field of inertial sensor\ncalibration, this approach can deliver important contributions among which the\npossibility to test dependence between sensors, integrate their dependence\nwithin the navigation filter and construct an optimal virtual sensor that can\nbe used to simplify and improve navigation accuracy. The advantages of this\nmethod and its usefulness for inertial sensor calibration are highlighted\nthrough a simulation study and an applied example with a small array of XSens\nMTi-G IMUs.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 12:06:30 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 19:30:50 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 12:49:25 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Xu", "Haotian", ""], ["Guerrier", "St\u00e9phane", ""], ["Molinari", "Roberto", ""], ["Karemera", "Mucyo", ""]]}, {"id": "1905.01582", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Hisashi Noma", "title": "Efficient screening of predictive biomarkers for individual treatment\n  selection", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of molecular diagnostic tools to achieve individualized\nmedicine requires identifying predictive biomarkers associated with subgroups\nof individuals who might receive beneficial or harmful effects from different\navailable treatments. However, due to the large number of candidate biomarkers\nin the large-scale genetic and molecular studies, and complex relationships\namong clinical outcome, biomarkers and treatments, the ordinary statistical\ntests for the interactions between treatments and covariates have difficulties\nfrom their limited statistical powers. In this paper, we propose an efficient\nmethod for detecting predictive biomarkers. We employ weighted loss functions\nof Chen et al. (2017) to directly estimate individual treatment scores and\npropose synthetic posterior inference for effect sizes of biomarkers. We\ndevelop an empirical Bayes approach, namely, we estimate unknown\nhyperparameters in the prior distribution based on data. We then provide\nefficient screening methods for the candidate biomarkers via optimal discovery\nprocedure with adequate control of false discovery rate. The proposed method is\ndemonstrated in simulation studies and an application to a breast cancer\nclinical study in which the proposed method was shown to detect the much larger\nnumbers of significant biomarkers than existing standard methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 02:15:56 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 12:35:13 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Noma", "Hisashi", ""]]}, {"id": "1905.01798", "submitter": "Ke Zhu", "authors": "Feiyu Jiang, Dong Li, Ke Zhu", "title": "Non-standard inference for augmented double autoregressive models with\n  null volatility coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers an augmented double autoregressive (DAR) model, which\nallows null volatility coefficients to circumvent the over-parameterization\nproblem in the DAR model. Since the volatility coefficients might be on the\nboundary, the statistical inference methods based on the Gaussian quasi-maximum\nlikelihood estimation (GQMLE) become non-standard, and their asymptotics\nrequire the data to have a finite sixth moment, which narrows applicable scope\nin studying heavy-tailed data. To overcome this deficiency, this paper develops\na systematic statistical inference procedure based on the self-weighted GQMLE\nfor the augmented DAR model. Except for the Lagrange multiplier test statistic,\nthe Wald, quasi-likelihood ratio and portmanteau test statistics are all shown\nto have non-standard asymptotics. The entire procedure is valid as long as the\ndata is stationary, and its usefulness is illustrated by simulation studies and\none real example.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 02:39:20 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Jiang", "Feiyu", ""], ["Li", "Dong", ""], ["Zhu", "Ke", ""]]}, {"id": "1905.01832", "submitter": "Patricio Maturana-Russel Dr", "authors": "Patricio Maturana-Russel and Renate Meyer", "title": "Bayesian spectral density estimation using P-splines with quantile-based\n  knot placement", "comments": "5 figures, 4 tables", "journal-ref": null, "doi": "10.1007/s00180-021-01066-7", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article proposes a Bayesian approach to estimating the spectral density\nof a stationary time series using a prior based on a mixture of P-spline\ndistributions. Our proposal is motivated by the B-spline Dirichlet process\nprior of Edwards et al. (2019) in combination with Whittle's likelihood and\naims at reducing the high computational complexity of its posterior\ncomputations. The strength of the B-spline Dirichlet process prior over the\nBernstein-Dirichlet process prior of Choudhuri et al. (2004) lies in its\nability to estimate spectral densities with sharp peaks and abrupt changes due\nto the flexibility of B-splines with variable number and location of knots.\nHere, we suggest to use P-splines of Eilers and Marx (1996) that combine a\nB-spline basis with a discrete penalty on the basis coefficients. In addition\nto equidistant knots, a novel strategy for a more expedient placement of knots\nis proposed that makes use of the information provided by the periodogram about\nthe steepness of the spectral power distribution. We demonstrate in a\nsimulation study and two real case studies that this approach retains the\nflexibility of the B-splines, achieves similar ability to accurately estimate\npeaks due to the new data-driven knot allocation scheme but significantly\nreduces the computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 06:10:21 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 20:43:58 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 02:59:20 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Maturana-Russel", "Patricio", ""], ["Meyer", "Renate", ""]]}, {"id": "1905.02034", "submitter": "Dami\\'an G. Hern\\'andez", "authors": "Dami\\'an G. Hern\\'andez and In\\'es Samengo", "title": "Estimating the Mutual Information between two Discrete, Asymmetric\n  Variables with Limited Samples", "comments": null, "journal-ref": null, "doi": "10.3390/e21060623", "report-no": null, "categories": "physics.data-an q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the strength of non-linear statistical dependencies between two\nvariables is a crucial matter in many research fields. The established measure\nfor quantifying such relations is the mutual information. However, estimating\nmutual information from limited samples is a challenging task. Since the mutual\ninformation is the difference of two entropies, the existing Bayesian\nestimators of entropy may be used to estimate information. This procedure,\nhowever, is still biased in the severely under-sampled regime. Here we propose\nan alternative estimator that is applicable to those cases in which the\nmarginal distribution of one of the two variables---the one with minimal\nentropy---is well sampled. The other variable, as well as the joint and\nconditional distributions, can be severely undersampled. We obtain an estimator\nthat presents very low bias, outperforming previous methods even when the\nsampled data contain few coincidences. As with other Bayesian estimators, our\nproposal focuses on the strength of the interaction between two discrete\nvariables, without seeking to model the specific way in which the variables are\nrelated. A distinctive property of our method is that the main data statistics\ndetermining the amount of mutual information is the inhomogeneity of the\nconditional distribution of the low-entropy variable in those states (typically\nfew) in which the large-entropy variable registers coincidences.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 13:29:21 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Hern\u00e1ndez", "Dami\u00e1n G.", ""], ["Samengo", "In\u00e9s", ""]]}, {"id": "1905.02065", "submitter": "Qi Long", "authors": "Pallavi S. Mishra-Kalyani and Brent A. Johnson and Qi Long", "title": "Propensity Process: a Balancing Functional", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational clinic registries, time to treatment is often of interest,\nbut treatment can be given at any time during follow-up and there is no\nstructure or intervention to ensure regular clinic visits for data collection.\nTo address these challenges, we introduce the time-dependent propensity process\nas a generalization of the propensity score. We show that the propensity\nprocess balances the entire time-varying covariate history which cannot be\nachieved by existing propensity score methods and that treatment assignment is\nstrongly ignorable conditional on the propensity process. We develop methods\nfor estimating the propensity process using observed data and for matching\nbased on the propensity process. We illustrate the propensity process method\nusing the Emory Amyotrophic Lateral Sclerosis (ALS) Registry data.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 14:38:08 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mishra-Kalyani", "Pallavi S.", ""], ["Johnson", "Brent A.", ""], ["Long", "Qi", ""]]}, {"id": "1905.02535", "submitter": "Masaaki Okabe", "authors": "Masaaki Okabe, Jun Tsuchida, Hiroshi Yadohisa", "title": "F-measure Maximizing Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is a widely used method in several fields. When applying\nlogistic regression to imbalanced data, for which majority classes dominate\nover minority classes, all class labels are estimated as `majority class.' In\nthis article, we use an F-measure optimization method to improve the\nperformance of logistic regression applied to imbalanced data. While many\nF-measure optimization methods adopt a ratio of the estimators to approximate\nthe F-measure, the ratio of the estimators tends to have more bias than when\nthe ratio is directly approximated. Therefore, we employ an approximate\nF-measure for estimating the relative density ratio. In addition, we define a\nrelative F-measure and approximate the relative F-measure. We show an algorithm\nfor a logistic regression weighted approximated relative to the F-measure. The\nexperimental results using real world data demonstrated that our proposed\nmethod is an efficient algorithm to improve the performance of logistic\nregression applied to imbalanced data.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 13:18:22 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Okabe", "Masaaki", ""], ["Tsuchida", "Jun", ""], ["Yadohisa", "Hiroshi", ""]]}, {"id": "1905.02659", "submitter": "Isabella Gollini", "authors": "Isabella Gollini", "title": "A mixture model approach for clustering bipartite networks", "comments": "To appear in \"Challenges in Social Network Research\" Volume in the\n  Lecture Notes in Social Networks (LNSN - Series of Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter investigates the latent structure of bipartite networks via a\nmodel-based clustering approach which is able to capture both latent groups of\nsending nodes and latent variability of the propensity of sending nodes to\ncreate links with receiving nodes within each group. This modelling approach is\nvery flexible and can be estimated by using fast inferential approaches such as\nvariational inference. We apply this model to the analysis of a terrorist\nnetwork in order to identify the main latent groups of terrorists and their\nlatent trait scores based on their attendance to some events.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:05:20 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 16:46:20 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Gollini", "Isabella", ""]]}, {"id": "1905.02700", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, Snigdhansu Chatterjee", "title": "On Weighted Multivariate Sign Functions", "comments": "Keywords: Multivariate sign, Principal component analysis, Data\n  depth, Sufficient dimension reduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate sign functions are often used for robust estimation and\ninference. We propose using data dependent weights in association with such\nfunctions. The proposed weighted sign functions retain desirable robustness\nproperties, while significantly improving efficiency in estimation and\ninference compared to unweighted multivariate sign-based methods. Using\nweighted signs, we demonstrate methods of robust location estimation and robust\nprincipal component analysis. We extend the scope of using robust multivariate\nmethods to include robust sufficient dimension reduction and functional outlier\ndetection. Several numerical studies and real data applications demonstrate the\nefficacy of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 17:28:15 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 20:23:43 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 16:33:48 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "1905.02810", "submitter": "Jingyuan Wang", "authors": "Kai Feng, Han Hong, Ke Tang, Jingyuan Wang", "title": "Decision Making with Machine Learning and ROC Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI econ.GN q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Receiver Operating Characteristic (ROC) curve is a representation of the\nstatistical information discovered in binary classification problems and is a\nkey concept in machine learning and data science. This paper studies the\nstatistical properties of ROC curves and its implication on model selection. We\nanalyze the implications of different models of incentive heterogeneity and\ninformation asymmetry on the relation between human decisions and the ROC\ncurves. Our theoretical discussion is illustrated in the context of a large\ndata set of pregnancy outcomes and doctor diagnosis from the Pre-Pregnancy\nCheckups of reproductive age couples in Henan Province provided by the Chinese\nMinistry of Health.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 08:01:23 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Feng", "Kai", ""], ["Hong", "Han", ""], ["Tang", "Ke", ""], ["Wang", "Jingyuan", ""]]}, {"id": "1905.02897", "submitter": "Paula Saavedra-Nieves", "authors": "Alberto Rodr\\'iguez-Casal and Paula Saavedra-Nieves", "title": "Minimax Hausdorff estimation of density level sets", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.7687", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of points from some unknown density, we propose a\ndata-driven method for estimating density level sets under the r-convexity\nassumption. This shape condition generalizes the convexity property. However,\nthe main problem in practice is that r is an unknown geometric characteristic\nof the set related to its curvature. A stochastic algorithm is proposed for\nselecting its optimal value from the data. The resulting reconstruction of the\nlevel set is able to achieve minimax rates for Hausdorff metric and distance in\nmeasure, up to log factors, uniformly on the level of the set.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 15:35:40 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Rodr\u00edguez-Casal", "Alberto", ""], ["Saavedra-Nieves", "Paula", ""]]}, {"id": "1905.02928", "submitter": "Rina Barber", "authors": "Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, Ryan J.\n  Tibshirani", "title": "Predictive inference with the jackknife+", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the jackknife+, which is a novel method for\nconstructing predictive confidence intervals. Whereas the jackknife outputs an\ninterval centered at the predicted response of a test point, with the width of\nthe interval determined by the quantiles of leave-one-out residuals, the\njackknife+ also uses the leave-one-out predictions at the test point to account\nfor the variability in the fitted regression function. Assuming exchangeable\ntraining samples, we prove that this crucial modification permits rigorous\ncoverage guarantees regardless of the distribution of the data points, for any\nalgorithm that treats the training points symmetrically. Such guarantees are\nnot possible for the original jackknife and we demonstrate examples where the\ncoverage rate may actually vanish. Our theoretical and empirical analysis\nreveals that the jackknife and the jackknife+ intervals achieve nearly exact\ncoverage and have similar lengths whenever the fitting algorithm obeys some\nform of stability. Further, we extend the jackknife+ to K-fold cross validation\nand similarly establish rigorous coverage properties. Our methods are related\nto cross-conformal prediction proposed by Vovk [2015] and we discuss\nconnections.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 06:16:01 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 02:38:30 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 22:30:33 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Candes", "Emmanuel J.", ""], ["Ramdas", "Aaditya", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1905.02962", "submitter": "Elisa Cabana Garceran Del Vall", "authors": "Elisa Cabana, Rosa E. Lillo, Henry Laniado", "title": "Robust regression based on shrinkage estimators", "comments": null, "journal-ref": null, "doi": "10.1007/s00477-020-01774-4", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A robust estimator is proposed for the parameters that characterize the\nlinear regression problem. It is based on the notion of shrinkages, often used\nin Finance and previously studied for outlier detection in multivariate data. A\nthorough simulation study is conducted to investigate: the efficiency with\nnormal and heavy-tailed errors, the robustness under contamination, the\ncomputational times, the affine equivariance and breakdown value of the\nregression estimator. Two classical data-sets often used in the literature and\na real socio-economic data-set about the Living Environment Deprivation of\nareas in Liverpool (UK), are studied. The results from the simulations and the\nreal data examples show the advantages of the proposed robust estimator in\nregression.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 08:48:20 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Cabana", "Elisa", ""], ["Lillo", "Rosa E.", ""], ["Laniado", "Henry", ""]]}, {"id": "1905.02971", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh and Magne Thoresen", "title": "Consistent Fixed-Effects Selection in Ultra-high dimensional Linear\n  Mixed Models with Error-Covariate Endogeneity", "comments": "To appear in Statistica Sinica (2020)", "journal-ref": null, "doi": "10.5705/ss.202019.0421", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, applied sciences, including longitudinal and clustered studies in\nbiomedicine require the analysis of ultra-high dimensional linear mixed effects\nmodels where we need to select important fixed effect variables from a vast\npool of available candidates. However, all existing literature assume that all\nthe available covariates and random effect components are independent of the\nmodel error which is often violated (endogeneity) in practice. In this paper,\nwe first investigate this important issue in ultra-high dimensional linear\nmixed effects models with particular focus on the fixed effects selection. We\nstudy the effects of different types of endogeneity on existing regularization\nmethods and prove their inconsistencies. Then, we propose a new profiled\nfocused generalized method of moments (PFGMM) approach to consistently select\nfixed effects under 'error-covariate' endogeneity, i.e., in the presence of\ncorrelation between the model error and covariates. Our proposal is proved to\nbe oracle consistent with probability tending to one and works well under most\nother type of endogeneity too. Additionally, we also propose and illustrate a\nfew consistent parameter estimators, including those of the variance\ncomponents, along with variable selection through PFGMM. Empirical simulations\nand an interesting real data example further support the claimed utility of our\nproposal.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 09:16:54 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 18:43:26 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ghosh", "Abhik", ""], ["Thoresen", "Magne", ""]]}, {"id": "1905.02990", "submitter": "Giona Casiraghi", "authors": "Laurence Brandenberger, Giona Casiraghi, Vahan Nanumyan, Frank\n  Schweitzer", "title": "Quantifying Triadic Closure in Multi-Edge Social Networks", "comments": "19 pages, 5 figures, 6 tables", "journal-ref": "Proceedings of the 2019 IEEE/ACM International Conference on\n  Advances in Social Networks Analysis and Mining (2019) 307-310", "doi": "10.1145/3341161.3342926", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-edge networks capture repeated interactions between individuals. In\nsocial networks, such edges often form closed triangles, or triads. Standard\napproaches to measure this triadic closure, however, fail for multi-edge\nnetworks, because they do not consider that triads can be formed by edges of\ndifferent multiplicity. We propose a novel measure of triadic closure for\nmulti-edge networks of social interactions based on a shared partner statistic.\nWe demonstrate that our operalization is able to detect meaningful closure in\nsynthetic and empirical multi-edge networks, where common approaches fail. This\nis a cornerstone in driving inferential network analyses from the analysis of\nbinary networks towards the analyses of multi-edge and weighted networks, which\noffer a more realistic representation of social interactions and relations.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 10:09:21 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Brandenberger", "Laurence", ""], ["Casiraghi", "Giona", ""], ["Nanumyan", "Vahan", ""], ["Schweitzer", "Frank", ""]]}, {"id": "1905.03151", "submitter": "Giles Hooker", "authors": "Giles Hooker and Lucas Mentch", "title": "Please Stop Permuting Features: An Explanation and Alternatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates against permute-and-predict (PaP) methods for\ninterpreting black box functions. Methods such as the variable importance\nmeasures proposed for random forests, partial dependence plots, and individual\nconditional expectation plots remain popular because of their ability to\nprovide model-agnostic measures that depend only on the pre-trained model\noutput. However, numerous studies have found that these tools can produce\ndiagnostics that are highly misleading, particularly when there is strong\ndependence among features. Rather than simply add to this growing literature by\nfurther demonstrating such issues, here we seek to provide an explanation for\nthe observed behavior. In particular, we argue that breaking dependencies\nbetween features in hold-out data places undue emphasis on sparse regions of\nthe feature space by forcing the original model to extrapolate to regions where\nthere is little to no data. We explore these effects through various settings\nwhere a ground-truth is understood and find support for previous claims in the\nliterature that PaP metrics tend to over-emphasize correlated features both in\nvariable importance and partial dependence plots, even though applying\npermutation methods to the ground-truth models do not. As an alternative, we\nrecommend more direct approaches that have proven successful in other settings:\nexplicitly removing features, conditional permutations, or model distillation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 22:37:47 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Hooker", "Giles", ""], ["Mentch", "Lucas", ""]]}, {"id": "1905.03222", "submitter": "Yaniv Romano", "authors": "Yaniv Romano, Evan Patterson, Emmanuel J. Cand\\`es", "title": "Conformalized Quantile Regression", "comments": "19 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a technique for constructing prediction intervals\nthat attain valid coverage in finite samples, without making distributional\nassumptions. Despite this appeal, existing conformal methods can be\nunnecessarily conservative because they form intervals of constant or weakly\nvarying length across the input space. In this paper we propose a new method\nthat is fully adaptive to heteroscedasticity. It combines conformal prediction\nwith classical quantile regression, inheriting the advantages of both. We\nestablish a theoretical guarantee of valid coverage, supplemented by extensive\nexperiments on popular regression datasets. We compare the efficiency of\nconformalized quantile regression to other conformal methods, showing that our\nmethod tends to produce shorter intervals.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 17:21:11 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Romano", "Yaniv", ""], ["Patterson", "Evan", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1905.03278", "submitter": "David N. Levin", "authors": "David N. Levin", "title": "On the representation of speech and music", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most automatic speech recognition (ASR) systems, the audio signal is\nprocessed to produce a time series of sensor measurements (e.g., filterbank\noutputs). This time series encodes semantic information in a speaker-dependent\nway. An earlier paper showed how to use the sequence of sensor measurements to\nderive an \"inner\" time series that is unaffected by any previous invertible\ntransformation of the sensor measurements. The current paper considers two or\nmore speakers, who mimic one another in the following sense: when they say the\nsame words, they produce sensor states that are invertibly mapped onto one\nanother. It follows that the inner time series of their utterances must be the\nsame when they say the same words. In other words, the inner time series\nencodes their speech in a manner that is speaker-independent. Consequently, the\nASR training process can be simplified by collecting and labelling the inner\ntime series of the utterances of just one speaker, instead of training on the\nsensor time series of the utterances of a large variety of speakers. A similar\nargument suggests that the inner time series of music is\ninstrument-independent. This is demonstrated in experiments on monophonic\nelectronic music.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 18:11:50 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Levin", "David N.", ""]]}, {"id": "1905.03337", "submitter": "Adam Kapelner", "authors": "Adam Kapelner, Abba M. Krieger, Michael Sklar, David Azriel", "title": "Optimal Rerandomization via a Criterion that Provides Insurance Against\n  Failed Experiments", "comments": "27 pages, 5 figures, 2 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimized rerandomization design procedure for a non-sequential\ntreatment-control experiment. Randomized experiments are the gold standard for\nfinding causal effects in nature. But sometimes random assignments result in\nunequal partitions of the treatment and control group visibly seen as imbalance\nin observed covariates. There can additionally be imbalance on unobserved\ncovariates. Imbalance in either observed or unobserved covariates increases\ntreatment effect estimator error inflating the width of confidence regions and\nreducing experimental power. \"Rerandomization\" is a strategy that omits poor\nimbalance assignments by limiting imbalance in the observed covariates to a\nprespecified threshold. However, limiting this threshold too much can increase\nthe risk of contracting error from unobserved covariates. We introduce a\ncriterion that combines observed imbalance while factoring in the risk of\ninadvertently imbalancing unobserved covariates. We then use this criterion to\nlocate the optimal rerandomization threshold based on the practitioner's level\nof desired insurance against high estimator error. We demonstrate the gains of\nour designs in simulation and in a dataset from a large randomized experiment\nin education. We provide an open source R package available on CRAN named\nOptimalRerandExpDesigns which generates designs according to our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 21:04:46 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 15:52:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kapelner", "Adam", ""], ["Krieger", "Abba M.", ""], ["Sklar", "Michael", ""], ["Azriel", "David", ""]]}, {"id": "1905.03380", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Mohammad ahsanullah", "title": "Asymptotic laws for upper and strong record values in the extreme domain\n  of attraction and beyond", "comments": "!7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic laws of records values have usually been investigated as limits in\ntype. In this paper, we use functional representations of the tail of\ncumulative distribution functions in the extreme value domain of attraction to\ndirectly establish asymptotic laws of records value, not necessarily as limits\nin type. Results beyond the extreme value value domain are provided. Explicit\nasymptotic laws concerning very usual laws are listed as well. Some of these\nlaws are expected to be used in fitting distribution\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 22:23:58 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 02:35:05 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Lo", "Gane Samb", ""], ["ahsanullah", "Mohammad", ""]]}, {"id": "1905.03426", "submitter": "Nan Shen", "authors": "Nan Shen, B\\'arbara Gonz\\'alez and Luis Ra\\'ul Pericchi", "title": "Comparison Between Bayesian and Frequentist Tail Probability Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the reasons that the Bayesian estimator of the\ntail probability is always higher than the frequentist estimator. Sufficient\nconditions for this phenomenon are established both by using Jensen's\nInequality and by looking at Taylor series approximations, both of which point\nto the convexity of the distribution function.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 03:13:54 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Shen", "Nan", ""], ["Gonz\u00e1lez", "B\u00e1rbara", ""], ["Pericchi", "Luis Ra\u00fal", ""]]}, {"id": "1905.03628", "submitter": "Lorenz Gilch", "authors": "Lorenz A. Gilch", "title": "Prediction Model for the Africa Cup of Nations 2019 via Nested Poisson\n  Regression", "comments": "14 pages, 3 figures, 15 tables. arXiv admin note: substantial text\n  overlap with arXiv:1806.01930", "journal-ref": "http://dx.doi.org/10.16929/ajas/2019.599.233", "doi": "10.16929/ajas/2019.599.233", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to the forecast of the Africa Cup of Nations 2019\nfootball tournament. It is based on a Poisson regression model that includes\nthe Elo points of the participating teams as covariates and incorporates\ndifferences of team-specific skills. The proposed model allows predictions in\nterms of probabilities in order to quantify the chances for each team to reach\na certain stage of the tournament. Monte Carlo simulations are used to estimate\nthe outcome of each single match of the tournament and hence to simulate the\nwhole tournament itself. The model is fitted on all football games on neutral\nground of the participating teams since 2010.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 13:10:40 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Gilch", "Lorenz A.", ""]]}, {"id": "1905.03657", "submitter": "Daniel Eck", "authors": "Daniel J. Eck and Forrest W. Crawford", "title": "Efficient and minimal length parametric conformal prediction regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction methods construct prediction regions for iid data that\nare valid in finite samples. We provide two parametric conformal prediction\nregions that are applicable for a wide class of continuous statistical models.\nThis class of statistical models includes generalized linear models (GLMs) with\ncontinuous outcomes. Our parametric conformal prediction regions possesses\nfinite sample validity, even when the model is misspecified, and are\nasymptotically of minimal length when the model is correctly specified. The\nfirst parametric conformal prediction region is constructed through binning of\nthe predictor space, guarantees finite-sample local validity and is\nasymptotically minimal at the $\\sqrt{\\log(n)/n}$ rate when the dimension $d$ of\nthe predictor space is one or two, and converges at the\n$O\\{(\\log(n)/n)^{1/d}\\}$ rate when $d > 2$. The second parametric conformal\nprediction region is constructed by transforming the outcome variable to a\ncommon distribution via the probability integral transform, guarantees\nfinite-sample marginal validity, and is asymptotically minimal at the\n$\\sqrt{\\log(n)/n}$ rate. We develop a novel concentration inequality for\nmaximum likelihood estimation that induces these convergence rates. We analyze\nprediction region coverage properties, large-sample efficiency, and robustness\nproperties of four methods for constructing conformal prediction intervals for\nGLMs: fully nonparametric kernel-based conformal, residual based conformal,\nnormalized residual based conformal, and parametric conformal which uses the\nassumed GLM density as a conformity measure. Extensive simulations compare\nthese approaches to standard asymptotic prediction regions. The utility of the\nparametric conformal prediction region is demonstrated in an application to\ninterval prediction of glycosylated hemoglobin levels, a blood measurement used\nto diagnose diabetes.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:31:29 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 15:52:10 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 18:08:51 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Eck", "Daniel J.", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1905.03673", "submitter": "Chris Oates", "authors": "Wilson Ye Chen, Alessandro Barp, Fran\\c{c}ois-Xavier Briol, Jackson\n  Gorham, Mark Girolami, Lester Mackey, Chris. J. Oates", "title": "Stein Point Markov Chain Monte Carlo", "comments": "Minor bug fixed in Theorem 4 (result unchanged)", "journal-ref": "ICML 2019", "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in machine learning and statistics is the approximation of\na probability measure by an empirical measure supported on a discrete point\nset. Stein Points are a class of algorithms for this task, which proceed by\nsequentially minimising a Stein discrepancy between the empirical measure and\nthe target and, hence, require the solution of a non-convex optimisation\nproblem to obtain each new point. This paper removes the need to solve this\noptimisation problem by, instead, selecting each new point based on a Markov\nchain sample path. This significantly reduces the computational cost of Stein\nPoints and leads to a suite of algorithms that are straightforward to\nimplement. The new algorithms are illustrated on a set of challenging Bayesian\ninference problems, and rigorous theoretical guarantees of consistency are\nestablished.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:57:02 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 08:16:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chen", "Wilson Ye", ""], ["Barp", "Alessandro", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Gorham", "Jackson", ""], ["Girolami", "Mark", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""]]}, {"id": "1905.03747", "submitter": "Espen Bernton", "authors": "Espen Bernton (Harvard University), Pierre E. Jacob (Harvard\n  University), Mathieu Gerber (University of Bristol), Christian P. Robert\n  (Universit\\'e Paris-Dauphine, PSL and University of Warwick)", "title": "Approximate Bayesian computation with the Wasserstein distance", "comments": "42 pages, 10 figures. Supplementary materials can be found on the\n  first author's webpage. Portions of this work previously appeared as\n  arXiv:1701.05146", "journal-ref": "Journal of the Royal Statistical Society: Series B, Volume 81,\n  Issue 2, pages 235-269 (April 2019)", "doi": "10.1111/rssb.12312", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of generative statistical models do not permit the numerical\nevaluation of their likelihood functions. Approximate Bayesian computation\n(ABC) has become a popular approach to overcome this issue, in which one\nsimulates synthetic data sets given parameters and compares summaries of these\ndata sets with the corresponding observed values. We propose to avoid the use\nof summaries and the ensuing loss of information by instead using the\nWasserstein distance between the empirical distributions of the observed and\nsynthetic data. This generalizes the well-known approach of using order\nstatistics within ABC to arbitrary dimensions. We describe how recently\ndeveloped approximations of the Wasserstein distance allow the method to scale\nto realistic data sizes, and propose a new distance based on the Hilbert\nspace-filling curve. We provide a theoretical study of the proposed method,\ndescribing consistency as the threshold goes to zero while the observations are\nkept fixed, and concentration properties as the number of observations grows.\nVarious extensions to time series data are discussed. The approach is\nillustrated on various examples, including univariate and multivariate g-and-k\ndistributions, a toggle switch model from systems biology, a queueing model,\nand a L\\'evy-driven stochastic volatility model.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 16:48:23 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Bernton", "Espen", "", "Harvard University"], ["Jacob", "Pierre E.", "", "Harvard\n  University"], ["Gerber", "Mathieu", "", "University of Bristol"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, PSL and University of Warwick"]]}, {"id": "1905.03950", "submitter": "Marie-Therese Wolfram", "authors": "Andrew M. Stuart and Marie-Therese Wolfram", "title": "Inverse optimal transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete optimal transportation problems arise in various contexts in\nengineering, the sciences and the social sciences. Often the underlying cost\ncriterion is unknown, or only partly known, and the observed optimal solutions\nare corrupted by noise. In this paper we propose a systematic approach to infer\nunknown costs from noisy observations of optimal transportation plans. The\nalgorithm requires only the ability to solve the forward optimal transport\nproblem, which is a linear program, and to generate random numbers. It has a\nBayesian interpretation, and may also be viewed as a form of stochastic\noptimization.\n  We illustrate the developed methodologies using the example of international\nmigration flows. Reported migration flow data captures (noisily) the number of\nindividuals moving from one country to another in a given period of time. It\ncan be interpreted as a noisy observation of an optimal transportation map,\nwith costs related to the geographical position of countries. We use a\ngraph-based formulation of the problem, with countries at the nodes of graphs\nand non-zero weighted adjacencies only on edges between countries which share a\nborder. We use the proposed algorithm to estimate the weights, which represent\ncost of transition, and to quantify uncertainty in these weights.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 05:51:08 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Stuart", "Andrew M.", ""], ["Wolfram", "Marie-Therese", ""]]}, {"id": "1905.04022", "submitter": "Maxime Taillardat", "authors": "Maxime Taillardat (CNRM), Anne-Laure Foug\\`eres (PSPM), Philippe\n  Naveau (LSCE), Rapha\\\"el de Fondeville (EPFL)", "title": "Extreme events evaluation using CRPS distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of ensemble forecasts for extreme events remains a challenging\nquestion. The general public as well as the media naturely pay particular\nattention on extreme events and conclude about the global predictive\nperformance of ensembles, which are often unskillful when they are needed.\nAshing classical verification tools to focus on such events can lead to\nunexpected behaviors. To square up these effects, thresholded and weighted\nscoring rules have been developed. Most of them use derivations of the\nContinuous Ranked Probability Score (CRPS). However, some properties of the\nCRPS for extreme events generate undesirable effects on the quality of\nverification. Using theoretical arguments and simulation examples, we\nillustrate some pitfalls of conventional verification tools and propose a\ndifferent direction to assess ensemble forecasts using extreme value theory,\nconsidering proper scores as random variables.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:15:38 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Taillardat", "Maxime", "", "CNRM"], ["Foug\u00e8res", "Anne-Laure", "", "PSPM"], ["Naveau", "Philippe", "", "LSCE"], ["de Fondeville", "Rapha\u00ebl", "", "EPFL"]]}, {"id": "1905.04389", "submitter": "Deborah Kunkel", "authors": "Deborah Kunkel and Mario Peruggia", "title": "Statistical inference with anchored Bayesian mixture of regressions\n  models: A case study analysis of allometric data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case study in which we use a mixture of regressions model to\nimprove on an ill-fitting simple linear regression model relating log brain\nmass to log body mass for 100 placental mammalian species. The slope of this\nregression model is of particular scientific interest because it corresponds to\na constant that governs a hypothesized allometric power law relating brain mass\nto body mass. A specific line of investigation is to determine whether the\nregression parameters vary across subgroups of related species.\n  We model these data using an anchored Bayesian mixture of regressions model,\nwhich modifies the standard Bayesian Gaussian mixture by pre-assigning small\nsubsets of observations to given mixture components with probability one. These\nobservations (called anchor points) break the relabeling invariance typical of\nexchangeable model specifications (the so-called label-switching problem). A\ncareful choice of which observations to pre-classify to which mixture\ncomponents is key to the specification of a well-fitting anchor model.\n  In the article we compare three strategies for the selection of anchor\npoints. The first assumes that the underlying mixture of regressions model\nholds and assigns anchor points to different components to maximize the\ninformation about their labeling. The second makes no assumption about the\nrelationship between x and y and instead identifies anchor points using a\nbivariate Gaussian mixture model. The third strategy begins with the assumption\nthat there is only one mixture regression component and identifies anchor\npoints that are representative of a clustering structure based on case-deletion\nimportance sampling weights. We compare the performance of the three strategies\non the allometric data set and use auxiliary taxonomic information about the\nspecies to evaluate the model-based classifications estimated from these\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 22:06:56 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Kunkel", "Deborah", ""], ["Peruggia", "Mario", ""]]}, {"id": "1905.04396", "submitter": "Leying Guan", "authors": "Leying Guan, Rob Tibshirani", "title": "Prediction and outlier detection in classification problems", "comments": "22 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-class classification problem when the training data and\nthe out-of-sample test data may have different distributions and propose a\nmethod called BCOPS (balanced and conformal optimized prediction sets). BCOPS\nconstructs a prediction set $C(x)$ as a subset of class labels, possibly empty.\nIt tries to optimize the out-of-sample performance, aiming to include the\ncorrect class as often as possible, but also detecting outliers $x$, for which\nthe method returns no prediction (corresponding to $C(x)$ equal to the empty\nset). The proposed method combines supervised-learning algorithms with the\nmethod of conformal prediction to minimize a misclassification loss averaged\nover the out-of-sample distribution. The constructed prediction sets have a\nfinite-sample coverage guarantee without distributional assumptions.\n  We also propose a method to estimate the outlier detection rate of a given\nmethod. We prove asymptotic consistency and optimality of our proposals under\nsuitable assumptions and illustrate our methods on real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 22:56:39 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 05:00:56 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 16:23:07 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Guan", "Leying", ""], ["Tibshirani", "Rob", ""]]}, {"id": "1905.04492", "submitter": "Erik-Jan van Kesteren", "authors": "Erik-Jan van Kesteren and Daniel L. Oberski", "title": "Structural Equation Models as Computation Graphs", "comments": "R code and package are available online as supplementary material at\n  https://github.com/vankesteren/sem-computationgraphs and\n  https://github.com/vankesteren/tensorsem/tree/computationgraph, respectively", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structural equation modeling (SEM) is a popular tool in the social and\nbehavioural sciences, where it is being applied to ever more complex data\ntypes. The high-dimensional data produced by modern sensors, brain images, or\n(epi)genetic measurements require variable selection using parameter\npenalization; experimental models combining disparate data sources benefit from\nregularization to obtain a stable result; and genomic SEM or network models\nlead to alternative objective functions. With each proposed extension,\nresearchers currently have to completely reformulate SEM and its optimization\nalgorithm -- a challenging and time-consuming task.\n  In this paper, we consider each SEM as a computation graph, a flexible method\nof specifying objective functions borrowed from the field of deep learning.\nWhen combined with state-of-the-art optimizers, our computation graph approach\ncan extend SEM without the need for bespoke software development. We show that\nboth existing and novel SEM improvements follow naturally from our approach. To\ndemonstrate, we discuss least absolute deviation estimation and penalized\nregression models. We also introduce spike-and-slab SEM, which may perform\nbetter when shrinkage of large factor loadings is not desired. By applying\ncomputation graphs to SEM, we hope to greatly accelerate the process of\ndeveloping SEM techniques, paving the way for new applications. We provide an\naccompanying R package tensorsem.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 10:23:06 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 12:38:59 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["van Kesteren", "Erik-Jan", ""], ["Oberski", "Daniel L.", ""]]}, {"id": "1905.04667", "submitter": "Nadezhda Gribkova Dr.", "authors": "Nadezhda Gribkova and Ri\\v{c}ardas Zitikis", "title": "Functional Correlations in the Pursuit of Performance Assessment of\n  Classifiers", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical classification and machine learning, as well as in social and\nother sciences, a number of measures of association have been proposed for\nassessing and comparing individual classifiers, raters, as well as their\ngroups. In this paper, we introduce, justify, and explore several new measures\nof association, which we call CO-, ANTI- and COANTI-correlation coefficients,\nthat we demonstrate to be powerful tools for classifying confusion matrices. We\nillustrate the performance of these new coefficients using a number of\nexamples, from which we also conclude that the coefficients are new objects in\nthe sense that they differ from those already in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 08:43:06 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 17:18:24 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 08:18:20 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Gribkova", "Nadezhda", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "1905.04735", "submitter": "Zhen Li", "authors": "Tao Hu, Eric B. Laber, Zhen Li, Nick J. Meyer, Krishna Pacifici", "title": "Note on Thompson sampling for large decision problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing interest in using streaming data to inform decision\nmaking across a wide range of application domains including mobile health, food\nsafety, security, and resource management. A decision support system formalizes\nonline decision making as a map from up-to-date information to a recommended\ndecision. Online estimation of an optimal decision strategy from streaming data\nrequires simultaneous estimation of components of the underlying system\ndynamics as well as the optimal decision strategy given these dynamics; thus,\nthere is an inherent trade-off between choosing decisions that lead to improved\nestimates and choosing decisions that appear to be optimal based on current\nestimates. Thompson (1933) was among the first to formalize this trade-off in\nthe context of choosing between two treatments for a stream of patients; he\nproposed a simple heuristic wherein a treatment is selected randomly at each\ntime point with selection probability proportional to the posterior probability\nthat it is optimal. We consider a variant of Thompson sampling that is simple\nto implement and can be applied to large and complex decision problems. We show\nthat the proposed Thompson sampling estimator is consistent for the optimal\ndecision support system and provide rates of convergence and finite sample\nerror bounds. The proposed algorithm is illustrated using an agent-based model\nof the spread of influenza on a network and management of mallard populations\nin the United States.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 15:29:52 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Hu", "Tao", ""], ["Laber", "Eric B.", ""], ["Li", "Zhen", ""], ["Meyer", "Nick J.", ""], ["Pacifici", "Krishna", ""]]}, {"id": "1905.05016", "submitter": "Aaron Ellison", "authors": "Ronny Vallejos, Javier P\\'erez, Aaron M. Ellison, Andrew D. Richardson", "title": "A Spatial Concordance Correlation Coefficient with an Application to\n  Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we define a spatial concordance coefficient for second-order\nstationary processes. This problem has been widely addressed in a non-spatial\ncontext, but here we consider a coefficient that for a fixed spatial lag allows\none to compare two spatial sequences along a 45-degree line. The proposed\ncoefficient was explored for the bivariate Mat\\'ern and Wendland covariance\nfunctions. The asymptotic normality of a sample version of the spatial\nconcordance coefficient for an increasing domain sampling framework was\nestablished for the Wendland covariance function. To work with large digital\nimages, we developed a local approach for estimating the concordance that uses\nlocal spatial models on non-overlapping windows. Monte Carlo simulations were\nused to gain additional insights into the asymptotic properties for finite\nsample sizes. As an illustrative example, we applied this methodology to two\nsimilar images of a deciduous forest canopy. The images were recorded with\ndifferent cameras but similar fields-of-view and within minutes of each other.\nOur analysis showed that the local approach helped to explain a percentage of\nthe non-spatial concordance and to provided additional information about its\ndecay as a function of the spatial lag.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 12:46:21 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Vallejos", "Ronny", ""], ["P\u00e9rez", "Javier", ""], ["Ellison", "Aaron M.", ""], ["Richardson", "Andrew D.", ""]]}, {"id": "1905.05056", "submitter": "Raphael Huser", "authors": "Yan Gong and Rapha\\\"el Huser", "title": "Asymmetric tail dependence modeling, with application to cryptocurrency\n  market data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the inception of Bitcoin in 2008, cryptocurrencies have played an\nincreasing role in the world of e-commerce, but the recent turbulence in the\ncryptocurrency market in 2018 has raised some concerns about their stability\nand associated risks. For investors, it is crucial to uncover the dependence\nrelationships between cryptocurrencies for a more resilient portfolio\ndiversification. Moreover, the stochastic behavior in both tails is important,\nas long positions are sensitive to a decrease in prices (lower tail), while\nshort positions are sensitive to an increase in prices (upper tail). In order\nto assess both risk types, we develop in this paper a flexible copula model\nwhich is able to distinctively capture asymptotic dependence or independence in\nits lower and upper tails simultaneously. Our proposed model is parsimonious\nand smoothly bridges (in each tail) both extremal dependence classes in the\ninterior of the parameter space. Inference is performed using a full or\ncensored likelihood approach, and we investigate by simulation the estimators'\nefficiency under three different censoring schemes which reduce the impact of\nnon-extreme observations. We also develop a local likelihood approach to\ncapture the temporal dynamics of extremal dependence among two leading\ncryptocurrencies. We here apply our model to historical closing prices of five\nleading cryotocurrencies, which share most of the cryptocurrency market\ncapitalizations. The results show that our proposed copula model outperforms\nalternative copula models and that the lower tail dependence level between most\npairs of leading cryptocurrencies -- and in particular Bitcoin and Ethereum --\nhas become stronger over time, smoothly transitioning from an asymptotic\nindependence regime to an asymptotic dependence regime in recent years, whilst\nthe upper tail has been relatively more stable overall at a weaker dependence\nlevel.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 14:31:01 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 10:37:03 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gong", "Yan", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "1905.05074", "submitter": "Wenqian Wang", "authors": "Wenqian Wang, Beth Andrews", "title": "Partially Specified Space Time Autoregressive Model with Artificial\n  Neural Network", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.07822", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The space time autoregressive model has been widely applied in science, in\nareas such as economics, public finance, political science, agricultural\neconomics, environmental studies and transportation analyses. The classical\nspace time autoregressive model is a linear model for describing spatial\ncorrelation. In this work, we expand the classical model to include related\nexogenous variables, possibly non-Gaussian, high volatility errors, and a\nnonlinear neural network component. The nonlinear neural network component\nallows for more model flexibility, the ability to learn and model nonlinear and\ncomplex relationships. We use a maximum likelihood approach for model parameter\nestimation. We establish consistency and asymptotic normality for these\nestimators under some standard conditions on the space time model and neural\nnetwork component. We investigate the quality of the asymptotic approximations\nfor finite samples by means of numerical simulation studies. For illustration,\nwe include a real world application.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 15:05:08 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wang", "Wenqian", ""], ["Andrews", "Beth", ""]]}, {"id": "1905.05242", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Xinyi Lu, Perry J. Williams, and Mevin B. Hooten", "title": "Hierarchical approaches for flexible and interpretable binary regression\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary regression models are ubiquitous in virtually every scientific field.\nFrequently, traditional generalized linear models fail to capture the\nvariability in the probability surface that gives rise to the binary\nobservations and novel methodology is required. This has generated a\nsubstantial literature comprised of binary regression models motivated by\nvarious applications. We describe a novel organization of generalizations to\ntraditional binary regression methods based on the familiar three-part\nstructure of generalized linear models (random component, systematic component,\nlink function). This new perspective facilitates both the comparison of\nexisting approaches, and the development of novel, flexible models with\ninterpretable parameters that capture application-specific data generating\nmechanisms. We use our proposed organizational structure to discuss some\nconcerns with certain existing models for binary data based on quantile\nregression. We then use the framework to develop several new binary regression\nmodels tailored to occupancy data for European red squirrels (Sciurus\nvulgaris).\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 18:56:36 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Scharf", "Henry R.", ""], ["Lu", "Xinyi", ""], ["Williams", "Perry J.", ""], ["Hooten", "Mevin B.", ""]]}, {"id": "1905.05274", "submitter": "Qi Long", "authors": "Domonique W. Hodge and Sandra E. Safo and Qi Long", "title": "Multiple imputation using dimension reduction techniques for\n  high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data present challenges in data analysis. Naive analyses such as\ncomplete-case and available-case analysis may introduce bias and loss of\nefficiency, and produce unreliable results. Multiple imputation (MI) is one of\nthe most widely used methods for handling missing data which can be partly\nattributed to its ease of use. However, existing MI methods implemented in most\nstatistical software are not applicable to or do not perform well in\nhigh-dimensional settings where the number of predictors is large relative to\nthe sample size. To remedy this issue, we develop an MI approach that uses\ndimension reduction techniques. Specifically, in constructing imputation models\nin the presence of high-dimensional data our approach uses sure independent\nscreening followed by either sparse principal component analysis (sPCA) or\nsufficient dimension reduction (SDR) techniques. Our simulation studies,\nconducted for high-dimensional data, demonstrate that using SIS followed by\nsPCA to perform MI achieves better performance than the other imputation\nmethods including several existing imputation approaches. We apply our approach\nto analysis of gene expression data from a prostate cancer study.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 20:18:54 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Hodge", "Domonique W.", ""], ["Safo", "Sandra E.", ""], ["Long", "Qi", ""]]}, {"id": "1905.05284", "submitter": "Ryan Martin", "authors": "Yue Yang and Ryan Martin and Howard Bondell", "title": "Variational approximations using Fisher divergence", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications of Bayesian inference involve models that are\nsufficiently complex that the corresponding posterior distributions are\nintractable and must be approximated. The most common approximation is based on\nMarkov chain Monte Carlo, but these can be expensive when the data set is large\nand/or the model is complex, so more efficient variational approximations have\nrecently received considerable attention. The traditional variational methods,\nthat seek to minimize the Kullback--Leibler divergence between the posterior\nand a relatively simple parametric family, provide accurate and efficient\nestimation of the posterior mean, but often does not capture other moments, and\nhave limitations in terms of the models to which they can be applied. Here we\npropose the construction of variational approximations based on minimizing the\nFisher divergence, and develop an efficient computational algorithm that can be\napplied to a wide range of models without conjugacy or potentially unrealistic\nmean-field assumptions. We demonstrate the superior performance of the proposed\nmethod for the benchmark case of logistic regression.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 20:58:34 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Yang", "Yue", ""], ["Martin", "Ryan", ""], ["Bondell", "Howard", ""]]}, {"id": "1905.05337", "submitter": "Brendan McVeigh", "authors": "Brendan S. McVeigh, Bradley T. Spahn, and Jared S. Murray", "title": "Scaling Bayesian Probabilistic Record Linkage with Post-Hoc Blocking: An\n  Application to the California Great Registers", "comments": "42 pages with appendices, 7 figures, 20 page supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic record linkage (PRL) is the process of determining which\nrecords in two databases correspond to the same underlying entity in the\nabsence of a unique identifier. Bayesian solutions to this problem provide a\npowerful mechanism for propagating uncertainty due to uncertain links between\nrecords (via the posterior distribution). However, computational considerations\nseverely limit the practical applicability of existing Bayesian approaches. We\npropose a new computational approach, providing both a fast algorithm for\nderiving point estimates of the linkage structure that properly account for\none-to-one matching and a restricted MCMC algorithm that samples from an\napproximate posterior distribution. Our advances make it possible to perform\nBayesian PRL for larger problems, and to assess the sensitivity of results to\nvarying prior specifications. We demonstrate the methods on a subset of an\nOCR'd dataset, the California Great Registers, a collection of 57 million voter\nregistrations from 1900 to 1968 that comprise the only panel data set of party\nregistration collected before the advent of scientific surveys.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 01:20:36 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 00:39:15 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["McVeigh", "Brendan S.", ""], ["Spahn", "Bradley T.", ""], ["Murray", "Jared S.", ""]]}, {"id": "1905.05389", "submitter": "Michael Lingzhi Li", "authors": "Kosuke Imai, Michael Lingzhi Li", "title": "Experimental Evaluation of Individualized Treatment Rules", "comments": "Accepted at JASA", "journal-ref": null, "doi": "10.1080/01621459.2021.1923511", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of individual-level data has led to numerous\napplications of individualized (or personalized) treatment rules (ITRs). Policy\nmakers often wish to empirically evaluate ITRs and compare their relative\nperformance before implementing them in a target population. We propose a new\nevaluation metric, the population average prescriptive effect (PAPE). The PAPE\ncompares the performance of ITR with that of non-individualized treatment rule,\nwhich randomly treats the same proportion of units. Averaging the PAPE over a\nrange of budget constraints yields our second evaluation metric, the area under\nthe prescriptive effect curve (AUPEC). The AUPEC represents an overall\nperformance measure for evaluation, like the area under the receiver and\noperating characteristic curve (AUROC) does for classification, and is a\ngeneralization of the QINI coefficient utilized in uplift modeling. We use\nNeyman's repeated sampling framework to estimate the PAPE and AUPEC and derive\ntheir exact finite-sample variances based on random sampling of units and\nrandom assignment of treatment. We extend our methodology to a common setting,\nin which the same experimental data is used to both estimate and evaluate ITRs.\nIn this case, our variance calculation incorporates the additional uncertainty\ndue to random splits of data used for cross-validation. The proposed evaluation\nmetrics can be estimated without requiring modeling assumptions, asymptotic\napproximation, or resampling methods. As a result, it is applicable to any ITR\nincluding those based on complex machine learning algorithms. The open-source\nsoftware package is available for implementing the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 04:40:26 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 17:06:52 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 06:09:48 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 02:01:16 GMT"}, {"version": "v5", "created": "Mon, 25 Jan 2021 19:50:07 GMT"}, {"version": "v6", "created": "Wed, 5 May 2021 17:18:32 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Imai", "Kosuke", ""], ["Li", "Michael Lingzhi", ""]]}, {"id": "1905.05394", "submitter": "Mingyuan Zhou", "authors": "Chaojie Wang, Bo Chen, Sucheng Xiao, Mingyuan Zhou", "title": "Convolutional Poisson Gamma Belief Network", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For text analysis, one often resorts to a lossy representation that either\ncompletely ignores word order or embeds each word as a low-dimensional dense\nfeature vector. In this paper, we propose convolutional Poisson factor analysis\n(CPFA) that directly operates on a lossless representation that processes the\nwords in each document as a sequence of high-dimensional one-hot vectors. To\nboost its performance, we further propose the convolutional Poisson gamma\nbelief network (CPGBN) that couples CPFA with the gamma belief network via a\nnovel probabilistic pooling layer. CPFA forms words into phrases and captures\nvery specific phrase-level topics, and CPGBN further builds a hierarchy of\nincreasingly more general phrase-level topics. For efficient inference, we\ndevelop both a Gibbs sampler and a Weibull distribution based convolutional\nvariational auto-encoder. Experimental results demonstrate that CPGBN can\nextract high-quality text latent representations that capture the word order\ninformation, and hence can be leveraged as a building block to enrich a wide\nvariety of existing latent variable models that ignore word order.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 05:08:47 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Wang", "Chaojie", ""], ["Chen", "Bo", ""], ["Xiao", "Sucheng", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1905.05569", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "Estimating Bayes factors from minimal ANOVA summaries for\n  repeated-measures designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, I develop a formula for estimating Bayes factors directly from\nminimal summary statistics produced in repeated measures analysis of variance\ndesigns. The formula, which requires knowing only the $F$-statistic, the number\nof subjects, and the number of repeated measurements per subject, is based on\nthe BIC approximation of the Bayes factor, a common default method for Bayesian\ncomputation with linear models. In addition to providing computational\nexamples, I report a simulation study in which I demonstrate that the formula\ncompares favorably to a recently developed, more complex method that accounts\nfor correlation between repeated measurements. The minimal BIC method provides\na simple way for researchers to estimate Bayes factors from a minimal set of\nsummary statistics, giving users a powerful index for estimating the evidential\nvalue of not only their own data, but also the data reported in published\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 12:55:08 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 11:44:43 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 10:49:34 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "1905.05760", "submitter": "Marie B\\\"ohnstedt", "authors": "Marie B\\\"ohnstedt, Hein Putter, Nadine Ouellette, Gerda Claeskens and\n  Jutta Gampe", "title": "Shifting attention to old age: Detecting mortality deceleration using\n  focused model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decrease in the increase in death rates at old ages is a phenomenon that\nhas repeatedly been discussed in demographic research. While mortality\ndeceleration can be explained in the gamma-Gompertz model as an effect of\nselection in heterogeneous populations, this phenomenon can be difficult to\nassess statistically because it relates to the tail of the distribution of the\nages at death. By using a focused information criterion (FIC) for model\nselection, we can directly target model performance at those advanced ages. The\ngamma-Gompertz model is reduced to the competing Gompertz model without\nmortality deceleration if the variance parameter lies on the boundary of the\nparameter space. We develop a new version of the FIC that is adapted to this\nnon-standard condition. In a simulation study, the new FIC is shown to\noutperform other methods in detecting mortality deceleration. The application\nof the FIC to extinct French-Canadian birth cohorts demonstrates that focused\nmodel selection can be used to rebut previous assertions about mortality\ndeceleration.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 12:55:13 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["B\u00f6hnstedt", "Marie", ""], ["Putter", "Hein", ""], ["Ouellette", "Nadine", ""], ["Claeskens", "Gerda", ""], ["Gampe", "Jutta", ""]]}, {"id": "1905.05876", "submitter": "Wojciech Rejchel", "authors": "Wojciech Rejchel and Malgorzata Bogdan", "title": "Rank-based Lasso -- efficient methods for high-dimensional robust model\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying significant predictors in large data\nbases, where the response variable depends on the linear combination of\nexplanatory variables through an unknown link function, corrupted with the\nnoise from the unknown distribution. We utilize the natural, robust and\nefficient approach, which relies on replacing values of the response variables\nby their ranks and then identifying significant predictors by using well known\nLasso. We provide new consistency results for the proposed procedure (called\n,,RankLasso\") and extend the scope of its applications by proposing its\nthresholded and adaptive versions. Our theoretical results show that these\nmodifications can identify the set of relevant predictors under much wider\nrange of data generating scenarios than regular RankLasso. Theoretical results\nare supported by the simulation study and the real data analysis, which show\nthat our methods can properly identify relevant predictors, even when the error\nterms come from the Cauchy distribution and the link function is nonlinear.\nThey also demonstrate the superiority of the modified versions of RankLasso in\nthe case when predictors are substantially correlated. The numerical study\nshows also that RankLasso performs substantially better in model selection than\nLADLasso, which is a well established methodology for robust model selection.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 23:11:45 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 11:49:03 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Rejchel", "Wojciech", ""], ["Bogdan", "Malgorzata", ""]]}, {"id": "1905.05884", "submitter": "Julyan Arbel", "authors": "Hien D. Nguyen, Julyan Arbel, Hongliang L\\\"u, Florence Forbes", "title": "Approximate Bayesian computation via the energy statistic", "comments": "25 pages, 6 figures, 5 tables", "journal-ref": "IEEE Access (2020)", "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) has become an essential part of the\nBayesian toolbox for addressing problems in which the likelihood is\nprohibitively expensive or entirely unknown, making it intractable. ABC defines\na pseudo-posterior by comparing observed data with simulated data,\ntraditionally based on some summary statistics, the elicitation of which is\nregarded as a key difficulty. Recently, using data discrepancy measures has\nbeen proposed in order to bypass the construction of summary statistics. Here\nwe propose to use the importance-sampling ABC (IS-ABC) algorithm relying on the\nso-called two-sample energy statistic. We establish a new asymptotic result for\nthe case where both the observed sample size and the simulated data sample size\nincrease to infinity, which highlights to what extent the data discrepancy\nmeasure impacts the asymptotic pseudo-posterior. The result holds in the broad\nsetting of IS-ABC methodologies, thus generalizing previous results that have\nbeen established only for rejection ABC algorithms. Furthermore, we propose a\nconsistent V-statistic estimator of the energy statistic, under which we show\nthat the large sample result holds, and prove that the rejection ABC algorithm,\nbased on the energy statistic, generates pseudo-posterior distributions that\nachieves convergence to the correct limits, when implemented with rejection\nthresholds that converge to zero, in the finite sample setting. Our proposed\nenergy statistic based ABC algorithm is demonstrated on a variety of models,\nincluding a Gaussian mixture, a moving-average model of order two, a bivariate\nbeta and a multivariate $g$-and-$k$ distribution. We find that our proposed\nmethod compares well with alternative discrepancy measures.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 23:41:45 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 10:17:29 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Arbel", "Julyan", ""], ["L\u00fc", "Hongliang", ""], ["Forbes", "Florence", ""]]}, {"id": "1905.05935", "submitter": "Ruobin Gong", "authors": "Ruobin Gong", "title": "Simultaneous Inference Under the Vacuous Orientation Assumption", "comments": "10 pages, 3 figures, ISIPTA 2019", "journal-ref": "PMLR 103:225-234, 2019", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a novel approach to simultaneous inference that alleviates the need\nto specify a correlational structure among marginal errors. The vacuous\norientation assumption retains what the normal i.i.d. assumption implies about\nthe distribution of error configuration, but relaxes the implication that the\nerror orientation is isotropic. When a large number of highly dependent\nhypotheses are tested simultaneously, the proposed model produces calibrated\nposterior inference by leveraging the logical relationship among them. This\nstands in contrast to the conservative performance of the Bonferroni\ncorrection, even if neither approaches makes assumptions about error\ndependence. The proposed model employs the Dempster-Shafer Extended Calculus of\nProbability, and delivers posterior inference in the form of stochastic\nthree-valued logic.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 03:53:59 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gong", "Ruobin", ""]]}, {"id": "1905.05963", "submitter": "Souvik Roy", "authors": "Suvra Pal and Souvik Roy", "title": "A New Estimation Algorithm for Box-Cox Transformation Cure Rate Model\n  and Comparison With EM Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new estimation procedure based on the non-linear\nconjugate gradient (NCG) algorithm for the Box-Cox transformation cure rate\nmodel. We compare the performance of the NCG algorithm with the well-known\nexpectation maximization (EM) algorithm through a simulation study and show the\nadvantages of the NCG algorithm over the EM algorithm. In particular, we show\nthat the NCG algorithm allows simultaneous maximization of all model parameters\nwhen the likelihood surface is flat with respect to a Box-Cox model parameter.\nThis is a big advantage over the EM algorithm, where a profile likelihood\napproach has been proposed in the literature that may not provide satisfactory\nresults. We finally use the NCG algorithm to analyze a well-known melanoma data\nand show that it results in a better fit.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 06:16:59 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Pal", "Suvra", ""], ["Roy", "Souvik", ""]]}, {"id": "1905.06097", "submitter": "Zhou Fan", "authors": "Sheng Xu and Zhou Fan", "title": "Iterative Alpha Expansion for estimating gradient-sparse signals from\n  linear measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating a piecewise-constant image, or a gradient-sparse\nsignal on a general graph, from noisy linear measurements. We propose and study\nan iterative algorithm to minimize a penalized least-squares objective, with a\npenalty given by the \"l_0-norm\" of the signal's discrete graph gradient. The\nmethod proceeds by approximate proximal descent, applying the alpha-expansion\nprocedure to minimize a proximal gradient in each iteration, and using a\ngeometric decay of the penalty parameter across iterations. Under a\ncut-restricted isometry property for the measurement design, we prove global\nrecovery guarantees for the estimated signal. For standard Gaussian designs,\nthe required number of measurements is independent of the graph structure, and\nimproves upon worst-case guarantees for total-variation (TV) compressed sensing\non the 1-D and 2-D lattice graphs by polynomial and logarithmic factors,\nrespectively. The method empirically yields lower mean-squared recovery error\ncompared with TV regularization in regimes of moderate undersampling and\nmoderate to high signal-to-noise, for several examples of changepoint signals\nand gradient-sparse phantom images.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 11:40:35 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Xu", "Sheng", ""], ["Fan", "Zhou", ""]]}, {"id": "1905.06201", "submitter": "Alexander D\\\"urre", "authors": "Alexander D\\\"urre, Roland Fried", "title": "Robust change point tests by bounded transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical moment based change point tests like the cusum test are very\npowerful in case of Gaussian time series with one change point but behave\npoorly under heavy tailed distributions and corrupted data. A new class of\nrobust change point tests based on cusum statistics of robustly transformed\nobservations is proposed. This framework is quite flexible, depending on the\nused transformation one can detect for instance changes in the mean, scale or\ndependence of a possibly multivariate time series. Simulations indicate that\nthis approach is very powerful in detecting changes in the marginal variance of\nARCH processes and outperforms existing proposals for detecting structural\nbreaks in the dependence structure of heavy tailed multivariate time series.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 14:14:12 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["D\u00fcrre", "Alexander", ""], ["Fried", "Roland", ""]]}, {"id": "1905.06261", "submitter": "Ming Yu", "authors": "Ming Yu, Varun Gupta, Mladen Kolar", "title": "Simultaneous Inference for Pairwise Graphical Models with Generalized\n  Score Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models provide a flexible yet parsimonious framework\nfor modeling dependencies among nodes in networks. There is a vast literature\non parameter estimation and consistent model selection for graphical models.\nHowever, in many of the applications, scientists are also interested in\nquantifying the uncertainty associated with the estimated parameters and\nselected models, which current literature has not addressed thoroughly. In this\npaper, we propose a novel estimator for statistical inference on edge\nparameters in pairwise graphical models based on generalized Hyv\\\"arinen\nscoring rule. Hyv\\\"arinen scoring rule is especially useful in cases where the\nnormalizing constant cannot be obtained efficiently in a closed form, which is\na common problem for graphical models, including Ising models and truncated\nGaussian graphical models. Our estimator allows us to perform statistical\ninference for general graphical models whereas the existing works mostly focus\non statistical inference for Gaussian graphical models where finding\nnormalizing constant is computationally tractable. Under mild conditions that\nare typically assumed in the literature for consistent estimation, we prove\nthat our proposed estimator is $\\sqrt{n}$-consistent and asymptotically normal,\nwhich allows us to construct confidence intervals and build hypothesis tests\nfor edge parameters. Moreover, we show how our proposed method can be applied\nto test hypotheses that involve a large number of model parameters\nsimultaneously. We illustrate validity of our estimator through extensive\nsimulation studies on a diverse collection of data-generating processes.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 16:03:36 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 19:48:56 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yu", "Ming", ""], ["Gupta", "Varun", ""], ["Kolar", "Mladen", ""]]}, {"id": "1905.06268", "submitter": "Hsin-Cheng Huang", "authors": "Hsin-Cheng Huang, Noel Cressie, Andrew Zammit-Mangion, Guowen Huang", "title": "False Discovery Rates to Detect Signals from Incomplete Spatially\n  Aggregated Data", "comments": "45 pages, 23 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a number of ways to test for the absence/presence of a spatial\nsignal in a completely observed fine-resolution image. One of these is a\npowerful nonparametric procedure called Enhanced False Discovery Rate (EFDR). A\ndrawback of EFDR is that it requires the data to be defined on regular pixels\nin a rectangular spatial domain. Here, we develop an EFDR procedure for\npossibly incomplete data defined on irregular small areas. Motivated by\nstatistical learning, we use conditional simulation (CS) to condition on the\navailable data and simulate the full rectangular image at its finest resolution\nmany times (M, say). EFDR is then applied to each of these simulations\nresulting in M estimates of the signal and M statistically dependent p-values.\nAveraging over these estimates yields a single, combined estimate of a possible\nsignal, but inference is needed to determine whether there really is a signal\npresent. We test the original null hypothesis of no signal by combining the M\np-values into a single p-value using copulas and a composite likelihood. If the\nnull hypothesis of no signal is rejected, we use the combined estimate. We call\nthis new procedure EFDR-CS and, to demonstrate its effectiveness, we show\nresults from a simulation study; an experiment where we introduce aggregation\nand incompleteness into temperature-change data in the Asia-Pacific; and an\napplication to total-column carbon dioxide from satellite remote sensing data\nover a region of the Middle East, Afghanistan, and the western part of\nPakistan.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 16:11:26 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 05:16:14 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 02:41:42 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Huang", "Hsin-Cheng", ""], ["Cressie", "Noel", ""], ["Zammit-Mangion", "Andrew", ""], ["Huang", "Guowen", ""]]}, {"id": "1905.06306", "submitter": "Sumanta Kumar Das Dr", "authors": "Sumanta Kumar Das and Randhir Singh", "title": "A multiple-frame approach of crop yield estimation from satellite\n  remotely sensed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have recently explored the information from the\nsatellite-remotely sensed data (SRSD) for estimating the crop production\nstatistics. The value of this information depends on the aerial and spatial\nresolutions of SRSD. The SRSD with fine spatial resolution is costly and the\naerial coverage is less. Use of multiple frames of SRSD in the estimation\nprocess of crop production can increase the precision. We propose an estimator\nfor the average yield of wheat for the state of Haryana, India. This estimator\nuses the information from the Wide Field Sensor (WiFS) and the Linear Imaging\nSelf Scanner (LISS-III) data from the Indian Remote Sensing satellite (IRS-1D)\nand the crop cutting experiment data collected by probability sampling design\nfrom a list frame of villages. We find that the relative efficiencies of the\nmultiple-frame estimators are high in comparison to the single frame\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 11:57:48 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Das", "Sumanta Kumar", ""], ["Singh", "Randhir", ""]]}, {"id": "1905.06310", "submitter": "Vinny Davies", "authors": "Vinny Davies, Umberto No\\`e, Alan Lazarus, Hao Gao, Benn Macdonald,\n  Colin Berry, Xiaoyu Luo, Dirk Husmeier", "title": "Fast Parameter Inference in a Biomechanical Model of the Left Ventricle\n  using Statistical Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in biomechanical studies of personalised human left\nventricular (LV) modelling is estimating the material properties and\nbiophysical parameters from in-vivo clinical measurements in a time frame\nsuitable for use within a clinic. Understanding these properties can provide\ninsight into heart function or dysfunction and help inform personalised\nmedicine. However, finding a solution to the differential equations which\nmathematically describe the kinematics and dynamics of the myocardium through\nnumerical integration can be computationally expensive. To circumvent this\nissue, we use the concept of emulation to infer the myocardial properties of a\nhealthy volunteer in a viable clinical time frame using in-vivo magnetic\nresonance image (MRI) data. Emulation methods avoid computationally expensive\nsimulations from the LV model by replacing the biomechanical model, which is\ndefined in terms of explicit partial differential equations, with a surrogate\nmodel inferred from simulations generated before the arrival of a patient,\nvastly improving computational efficiency at the clinic. We compare and\ncontrast two emulation strategies: (i) emulation of the computational model\noutputs and (ii) emulation of the loss between the observed patient data and\nthe computational model outputs. These strategies are tested with two different\ninterpolation methods, as well as two different loss functions...\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 14:10:44 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Davies", "Vinny", ""], ["No\u00e8", "Umberto", ""], ["Lazarus", "Alan", ""], ["Gao", "Hao", ""], ["Macdonald", "Benn", ""], ["Berry", "Colin", ""], ["Luo", "Xiaoyu", ""], ["Husmeier", "Dirk", ""]]}, {"id": "1905.06391", "submitter": "Eky Febrianto", "authors": "Mark Girolami, Eky Febrianto, Ge Yin, Fehmi Cirak", "title": "The statistical finite element method (statFEM) for coherent synthesis\n  of observation data and model predictions", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.113533", "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of observation data from engineering systems in\noperation poses the question of how to incorporate this data into finite\nelement models. To this end, we propose a novel statistical construction of the\nfinite element method that provides the means of synthesising measurement data\nand finite element models. The Bayesian statistical framework is adopted to\ntreat all the uncertainties present in the data, the mathematical model and its\nfinite element discretisation. From the outset, we postulate a data-generating\nmodel which additively decomposes data into a finite element, a model\nmisspecification and a noise component. Each of the components may be uncertain\nand is considered as a random variable with a respective prior probability\ndensity. The prior of the finite element component is given by a conventional\nstochastic forward problem. The prior probabilities of the model\nmisspecification and measurement noise, without loss of generality, are assumed\nto have zero-mean and known covariance structure. Our proposed statistical\nmodel is hierarchical in the sense that each of the three random components may\ndepend on non-observable random hyperparameters. Because of the hierarchical\nstructure of the statistical model, Bayes rule is applied on three different\nlevels in turn to infer the posterior densities of the three random components\nand hyperparameters. On level one, we determine the posterior densities of the\nfinite element component and the true system response using the prior finite\nelement density given by the forward problem and the data likelihood. On the\nnext level, we infer the hyperparameter posterior densities from their\nrespective priors and the marginal likelihood of the first inference problem.\nFinally, on level three we use Bayes rule to choose the most suitable finite\nelement model in light of the observed data by computing the model posteriors.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 19:00:06 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 11:35:01 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 12:30:22 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Girolami", "Mark", ""], ["Febrianto", "Eky", ""], ["Yin", "Ge", ""], ["Cirak", "Fehmi", ""]]}, {"id": "1905.06400", "submitter": "Dennis Shen", "authors": "Muhummad Amjad, Vishal Misra, Devavrat Shah, Dennis Shen", "title": "mRSC: Multi-dimensional Robust Synthetic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When evaluating the impact of a policy on a metric of interest, it may not be\npossible to conduct a randomized control trial. In settings where only\nobservational data is available, Synthetic Control (SC) methods provide a\npopular data-driven approach to estimate a \"synthetic\" control by combining\nmeasurements of \"similar\" units (donors). Recently, Robust SC (RSC) was\nproposed as a generalization of SC to overcome the challenges of missing data\nhigh levels of noise, while removing the reliance on domain knowledge for\nselecting donors. However, SC, RSC, and their variants, suffer from poor\nestimation when the pre-intervention period is too short. As the main\ncontribution, we propose a generalization of unidimensional RSC to\nmulti-dimensional RSC, mRSC. Our proposed mechanism incorporates multiple\nmetrics to estimate a synthetic control, thus overcoming the challenge of poor\ninference from limited pre-intervention data. We show that the mRSC algorithm\nwith $K$ metrics leads to a consistent estimator of the synthetic control for\nthe target unit under any metric. Our finite-sample analysis suggests that the\nprediction error decays to zero at a rate faster than the RSC algorithm by a\nfactor of $K$ and $\\sqrt{K}$ for the training and testing periods (pre- and\npost-intervention), respectively. Additionally, we provide a diagnostic test\nthat evaluates the utility of including additional metrics. Moreover, we\nintroduce a mechanism to validate the performance of mRSC: time series\nprediction. That is, we propose a method to predict the future evolution of a\ntime series based on limited data when the notion of time is relative and not\nabsolute, i.e., we have access to a donor pool that has undergone the desired\nfuture evolution. Finally, we conduct experimentation to establish the efficacy\nof mRSC on synthetic data and two real-world case studies (retail and Cricket).\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 19:20:56 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 18:40:03 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 12:17:13 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Amjad", "Muhummad", ""], ["Misra", "Vishal", ""], ["Shah", "Devavrat", ""], ["Shen", "Dennis", ""]]}, {"id": "1905.06463", "submitter": "Alimire Nabijiang", "authors": "Alimire Nabijiang, Supratik Mukhopadhyay, Yimin Zhu, Ravindra\n  Gudishala, Sanaz Saeidi, Qun Liu", "title": "Why do you take that route?", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to determine whether a particular context factor\namong the variables that a researcher is interested in causally affects the\nroute choice behavior of drivers. To our knowledge, there is limited literature\nthat consider the effects of various factors on route choice based on causal\ninference.Yet, collecting data sets that are sensitive to the aforementioned\nfactors are challenging and the existing approaches usually take into account\nonly the general factors motivating drivers route choice behavior. To fill\nthese gaps, we carried out a study using Immersive Virtual Environment (IVE)\ntools to elicit drivers' route choice behavioral data, covering drivers'\nnetwork familiarity, educationlevel, financial concern, etc, apart from\nconventional measurement variables. Having context-aware, high-fidelity\nproperties, IVE data affords the opportunity to incorporate the impacts of\nhuman related factors into the route choice causal analysis and advance a more\ncustomizable research tool for investigating causal factors on path selection\nin network routing. This causal analysis provides quantitative evidence to\nsupport drivers' diversion decision.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 22:57:51 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Nabijiang", "Alimire", ""], ["Mukhopadhyay", "Supratik", ""], ["Zhu", "Yimin", ""], ["Gudishala", "Ravindra", ""], ["Saeidi", "Sanaz", ""], ["Liu", "Qun", ""]]}, {"id": "1905.06467", "submitter": "Claus Ekstr{\\o}m", "authors": "Claus Thorn Ekstr{\\o}m and Christian Bressen Pipper (Section of\n  Biostatistics, Department of Public Health, University of Copenhagen)", "title": "Moment-based Estimation of Mixtures of Regression Models", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of regression models provide a flexible modeling framework\nfor many phenomena. Using moment-based estimation of the regression parameters,\nwe develop unbiased estimators with a minimum of assumptions on the mixture\ncomponents. In particular, only the average regression model for one of the\ncomponents in the mixture model is needed and no requirements on the\ndistributions. The consistency and asymptotic distribution of the estimators is\nderived and the proposed method is validated through a series of simulation\nstudies and is shown to be highly accurate. We illustrate the use of the\nmoment-based mixture of regression models with an application to wine quality\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 23:11:48 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ekstr\u00f8m", "Claus Thorn", "", "Section of\n  Biostatistics, Department of Public Health, University of Copenhagen"], ["Pipper", "Christian Bressen", "", "Section of\n  Biostatistics, Department of Public Health, University of Copenhagen"]]}, {"id": "1905.06491", "submitter": "Sokbae Lee", "authors": "Joel L. Horowitz, Sokbae Lee", "title": "Inference in a class of optimization problems: Confidence regions and\n  finite sample bounds on errors in coverage probabilities", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for carrying out inference on partially\nidentified parameters that are solutions to a class of optimization problems.\nThe optimization problems arise in applications in which grouped data are used\nfor estimation of a model's structural parameters. The parameters are\ncharacterized by restrictions that involve the unknown population means of\nobserved random variables in addition to the structural parameters of interest.\nInference consists of finding confidence intervals for the structural\nparameters. Our theory provides a finite-sample bound on the difference between\nthe true and nominal probabilities with which a confidence interval contains\nthe true but unknown value of a parameter. We contrast our method with an\nalternative inference method based on the median-of-means estimator of Minsker\n(2015). The results of Monte Carlo experiments and empirical examples\nillustrate the usefulness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 01:40:51 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 14:48:07 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 17:51:33 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Horowitz", "Joel L.", ""], ["Lee", "Sokbae", ""]]}, {"id": "1905.06501", "submitter": "Raj Agrawal", "authors": "Raj Agrawal, Jonathan H. Huggins, Brian Trippe, Tamara Broderick", "title": "The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise\n  Interactions in High Dimensions", "comments": "Accepted at ICML 2019. 20 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering interaction effects on a response of interest is a fundamental\nproblem faced in biology, medicine, economics, and many other scientific\ndisciplines. In theory, Bayesian methods for discovering pairwise interactions\nenjoy many benefits such as coherent uncertainty quantification, the ability to\nincorporate background knowledge, and desirable shrinkage properties. In\npractice, however, Bayesian methods are often computationally intractable for\neven moderate-dimensional problems. Our key insight is that many hierarchical\nmodels of practical interest admit a particular Gaussian process (GP)\nrepresentation; the GP allows us to capture the posterior with a vector of O(p)\nkernel hyper-parameters rather than O(p^2) interactions and main effects. With\nthe implicit representation, we can run Markov chain Monte Carlo (MCMC) over\nmodel hyper-parameters in time and memory linear in p per iteration. We focus\non sparsity-inducing models and show on datasets with a variety of covariate\nbehaviors that our method: (1) reduces runtime by orders of magnitude over\nnaive applications of MCMC, (2) provides lower Type I and Type II error\nrelative to state-of-the-art LASSO-based approaches, and (3) offers improved\ncomputational scaling in high dimensions relative to existing Bayesian and\nLASSO-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 02:19:10 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 03:46:06 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Agrawal", "Raj", ""], ["Huggins", "Jonathan H.", ""], ["Trippe", "Brian", ""], ["Broderick", "Tamara", ""]]}, {"id": "1905.07065", "submitter": "Li Chen", "authors": "Li Chen", "title": "Privacy Preserving Adjacency Spectral Embedding on Stochastic\n  Blockmodels", "comments": "Accepted at Learning and Reasoning with Graph-Structured\n  Representations at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For graphs generated from stochastic blockmodels, adjacency spectral\nembedding is asymptotically consistent. Further, adjacency spectral embedding\ncomposed with universally consistent classifiers is universally consistent to\nachieve the Bayes error. However when the graph contains private or sensitive\ninformation, treating the data as non-private can potentially leak privacy and\nincur disclosure risks. In this paper, we propose a differentially private\nadjacency spectral embedding algorithm for stochastic blockmodels. We\ndemonstrate that our proposed methodology can estimate the latent positions\nclose to, in Frobenius norm, the latent positions by adjacency spectral\nembedding and achieve comparable accuracy at desired privacy parameters in\nsimulated and real world networks.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 23:43:45 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Chen", "Li", ""]]}, {"id": "1905.07067", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin, Jane-Ling Wang and Qixian Zhong", "title": "Basis Expansions for Functional Snippets", "comments": "51 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of mean and covariance functions is fundamental for functional\ndata analysis. While this topic has been studied extensively in the literature,\na key assumption is that there are enough data in the domain of interest to\nestimate both the mean and covariance functions. In this paper, we investigate\nmean and covariance estimation for functional snippets in which observations\nfrom a subject are available only in an interval of length strictly (and often\nmuch) shorter than the length of the whole interval of interest. For such a\nsampling plan, no data is available for direct estimation of the off-diagonal\nregion of the covariance function. We tackle this challenge via a basis\nrepresentation of the covariance function. The proposed approach allows one to\nconsistently estimate an infinite-rank covariance function from functional\nsnippets. We establish the convergence rates for the proposed estimators and\nillustrate their finite-sample performance via simulation studies and two data\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 23:49:04 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 22:54:39 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 03:19:17 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2020 12:48:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lin", "Zhenhua", ""], ["Wang", "Jane-Ling", ""], ["Zhong", "Qixian", ""]]}, {"id": "1905.07103", "submitter": "Spencer Woody", "authors": "Spencer Woody, Carlos M. Carvalho and Jared S. Murray", "title": "Model interpretation through lower-dimensional posterior summarization", "comments": "40 pages, 16 figures", "journal-ref": null, "doi": "10.1080/10618600.2020.1796684", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric regression models have recently surged in their power and\npopularity, accompanying the trend of increasing dataset size and complexity.\nWhile these models have proven their predictive ability in empirical settings,\nthey are often difficult to interpret and do not address the underlying\ninferential goals of the analyst or decision maker. In this paper, we propose a\nmodular two-stage approach for creating parsimonious, interpretable summaries\nof complex models which allow freedom in the choice of modeling technique and\nthe inferential target. In the first stage a flexible model is fit which is\nbelieved to be as accurate as possible. In the second stage, lower-dimensional\nsummaries are constructed by projecting draws from the distribution onto\nsimpler structures. These summaries naturally come with valid Bayesian\nuncertainty estimates. Further, since we use the data only once to move from\nprior to posterior, these uncertainty estimates remain valid across multiple\nsummaries and after iteratively refining a summary. We apply our method and\ndemonstrate its strengths across a range of simulated and real datasets. Code\nto reproduce the examples shown is avaiable at github.com/spencerwoody/ghost\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 03:41:30 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 16:35:39 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 14:38:24 GMT"}, {"version": "v4", "created": "Fri, 28 Feb 2020 18:34:38 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Woody", "Spencer", ""], ["Carvalho", "Carlos M.", ""], ["Murray", "Jared S.", ""]]}, {"id": "1905.07157", "submitter": "Veronique Maume-Deschamps", "authors": "Weihong Ni, Corina Constantinescu, Alfredo Eg\\'idio dos Reis,\n  V\\'eronique Maume-Deschamps (ICJ, PSPM)", "title": "Estimation of foreseeable and unforeseeable risks in motor insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project works with the risk model developed by Li et al. (2015) and\nquests modelling, estimating and pricing insurance for risks brought in by\ninnovative technologies, or other emerging or latent risks. The model considers\ntwo different risk streams that arise together, however not clearly separated\nor observed. Specifically, we consider a risk surplus process where premia are\nadjusted according to past claim frequencies, like in a Bonus-Malus (BM)\nsystem, when we consider a classical or historical risk stream and an\nunforeseeable risk one. These are unknown risks which can be of high\nuncertainty that, when pricing insurance (ratemaking and experience rating),\nsuggest a sensitive premium adjustment strategy. It is not clear for the\nactuary to observe which claim comes from one or the other stream. When\nmodelling such risks it is crucial to estimate the behaviour of such claims,\noccurrence and their severity. Premium calculation must fairly reflect the\nnature of these two kinds of risk streams. We start proposing a model,\nseparating claim counts and severities, then propose a premium calculation\nmethod, and finally a parameter estimation procedure. In the modelling we\nassume a Bayesian approach as used in credibility theory, a credibility\napproach for premium calculation and the use of the Expectation-Maximization\n(EM) algorithm in the estimation procedure.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 08:04:32 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Ni", "Weihong", "", "ICJ, PSPM"], ["Constantinescu", "Corina", "", "ICJ, PSPM"], ["Reis", "Alfredo Eg\u00eddio dos", "", "ICJ, PSPM"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ, PSPM"]]}, {"id": "1905.07194", "submitter": "Anastasios Papanikos Tasos", "authors": "Tasos Papanikos, John Thompson, Keith Abrams, Nicolas Staedler, Oriana\n  Ciani, Rod Taylor and Sylwia Bujkiewicz", "title": "A Bayesian hierarchical meta-analytic method for modelling surrogate\n  relationships that vary across treatment classes using aggregate data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate endpoints play an important role in drug development when they can\nbe used to measure treatment effect early compared to the final clinical\noutcome and to predict clinical benefit or harm. Such endpoints are assessed\nfor their predictive value of clinical benefit by investigating the surrogate\nrelationship between treatment effects on the surrogate and final outcomes\nusing meta-analytic methods. When surrogate relationships vary across treatment\nclasses, such validation may fail due to limited data within each treatment\nclass. In this paper, two alternative Bayesian meta-analytic methods are\nintroduced which allow for borrowing of information from other treatment\nclasses when exploring the surrogacy in a particular class. The first approach\nextends a standard model for the evaluation of surrogate endpoints to a\nhierarchical meta-analysis model assuming full exchangeability of surrogate\nrelationships across all the treatment classes, thus facilitating borrowing of\ninformation across the classes. The second method is able to relax this\nassumption by allowing for partial exchangeability of surrogate relationships\nacross treatment classes to avoid excessive borrowing of information from\ndistinctly different classes. We carried out a simulation study to assess the\nproposed methods in nine data scenarios and compared them with subgroup\nanalysis using the standard model within each treatment class. We also applied\nthe methods to an illustrative example in colorectal cancer which led to\nobtaining the parameters describing the surrogate relationships with higher\nprecision.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 10:58:25 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 11:22:20 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Papanikos", "Tasos", ""], ["Thompson", "John", ""], ["Abrams", "Keith", ""], ["Staedler", "Nicolas", ""], ["Ciani", "Oriana", ""], ["Taylor", "Rod", ""], ["Bujkiewicz", "Sylwia", ""]]}, {"id": "1905.07218", "submitter": "Tom\\'a\\v{s} Rub\\'in", "authors": "Tom\\'a\\v{s} Rub\\'in and Victor M. Panaretos", "title": "Functional Lagged Regression with Sparse Noisy Observations", "comments": null, "journal-ref": null, "doi": "10.1111/jtsa.12551", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A functional (lagged) time series regression model involves the regression of\nscalar response time series on a time series of regressors that consists of a\nsequence of random functions. In practice, the underlying regressor curve time\nseries are not always directly accessible, but are latent processes observed\n(sampled) only at discrete measurement locations. In this paper, we consider\nthe so-called sparse observation scenario where only a relatively small number\nof measurement locations have been observed, possibly different for each curve.\nThe measurements can be further contaminated by additive measurement error. A\nspectral approach to the estimation of the model dynamics is considered. The\nspectral density of the regressor time series and the cross-spectral density\nbetween the regressors and response time series are estimated by kernel\nsmoothing methods from the sparse observations. The impulse response regression\ncoefficients of the lagged regression model are then estimated by means of\nridge regression (Tikhonov regularisation) or PCA regression (spectral\ntruncation). The latent functional time series are then recovered by means of\nprediction, conditioning on all the observed observed data. The performance and\nimplementation of our methods are illustrated by means of a simulation study\nand the analysis of meteorological data.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 11:59:23 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 18:58:57 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Rub\u00edn", "Tom\u00e1\u0161", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1905.07456", "submitter": "Xiao Wu", "authors": "Xiao Wu, Yi Xu, Bradley P. Carlin", "title": "Optimizing Interim Analysis Timing for Bayesian Adaptive Commensurate\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In developing products for rare diseases, statistical challenges arise due to\nthe limited number of patients available for participation in drug trials and\nother clinical research. Bayesian adaptive clinical trial designs offer the\npossibility of increased statistical efficiency, reduced development cost and\nethical hazard prevention via their incorporation of evidence from external\nsources (historical data, expert opinions, and real-world evidence), and\nflexibility in the specification of interim looks. In this paper, we propose a\nnovel Bayesian adaptive commensurate design that borrows adaptively from\nhistorical information and also uses a particular payoff function to optimize\nthe timing of the study's interim analysis. The trial payoff is a function of\nhow many samples can be saved via early stopping and the probability of making\ncorrect early decisions for either futility or efficacy. We calibrate our\nBayesian algorithm to have acceptable long-run frequentist properties (Type I\nerror and power) via simulation at the design stage. We illustrate our approach\nusing a pediatric trial design setting testing the effect of a new drug for a\nrare genetic disease. The optimIA R package available at\nhttps://github.com/wxwx1993/Bayesian_IA_Timing provides an easy-to-use\nimplementation of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:05:29 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 04:23:06 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Wu", "Xiao", ""], ["Xu", "Yi", ""], ["Carlin", "Bradley P.", ""]]}, {"id": "1905.07499", "submitter": "Brian Trippe", "authors": "Brian L. Trippe, Jonathan H. Huggins, Raj Agrawal and Tamara Broderick", "title": "LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data\n  Approximations", "comments": "Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the ease of modern data collection, applied statisticians often have\naccess to a large set of covariates that they wish to relate to some observed\noutcome. Generalized linear models (GLMs) offer a particularly interpretable\nframework for such an analysis. In these high-dimensional problems, the number\nof covariates is often large relative to the number of observations, so we face\nnon-trivial inferential uncertainty; a Bayesian approach allows coherent\nquantification of this uncertainty. Unfortunately, existing methods for\nBayesian inference in GLMs require running times roughly cubic in parameter\ndimension, and so are limited to settings with at most tens of thousand\nparameters. We propose to reduce time and memory costs with a low-rank\napproximation of the data in an approach we call LR-GLM. When used with the\nLaplace approximation or Markov chain Monte Carlo, LR-GLM provides a full\nBayesian posterior approximation and admits running times reduced by a full\nfactor of the parameter dimension. We rigorously establish the quality of our\napproximation and show how the choice of rank allows a tunable\ncomputational-statistical trade-off. Experiments support our theory and\ndemonstrate the efficacy of LR-GLM on real large-scale datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 22:59:56 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Trippe", "Brian L.", ""], ["Huggins", "Jonathan H.", ""], ["Agrawal", "Raj", ""], ["Broderick", "Tamara", ""]]}, {"id": "1905.07530", "submitter": "Dan Yang", "authors": "Rong Chen, Dan Yang and Cun-hui Zhang", "title": "Factor Models for High-Dimensional Tensor Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large tensor (multi-dimensional array) data are now routinely collected in a\nwide range of applications, due to modern data collection capabilities. Often\nsuch observations are taken over time, forming tensor time series. In this\npaper we present a factor model approach for analyzing high-dimensional dynamic\ntensor time series and multi-category dynamic transport networks. Two\nestimation procedures along with their theoretical properties and simulation\nresults are presented. Two applications are used to illustrate the model and\nits interpretations.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 03:40:48 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 02:00:31 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Chen", "Rong", ""], ["Yang", "Dan", ""], ["Zhang", "Cun-hui", ""]]}, {"id": "1905.07596", "submitter": "Nicole Pashley", "authors": "Nicole E. Pashley and Marie-Abele C. Bind", "title": "Causal Inference for Multiple Non-Randomized Treatments using Fractional\n  Factorial Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a framework for addressing causal questions in an observational\nsetting with multiple treatments. This setting involves attempting to\napproximate an experiment from observational data. With multiple treatments,\nthis experiment would be a factorial design. However, certain treatment\ncombinations may be so rare that there are no measured outcomes in the observed\ndata corresponding to them. We propose to conceptualize a hypothetical\nfractional factorial experiment instead of a full factorial experiment and lay\nout a framework for analysis in this setting. We connect our design-based\nmethods to standard regression methods. We finish by illustrating our approach\nusing biomedical data from the 2003-2004 cycle of the National Health and\nNutrition Examination Survey to estimate the effects of four common pesticides\non body mass index.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 14:34:32 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 14:46:38 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 18:06:40 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 20:30:20 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Pashley", "Nicole E.", ""], ["Bind", "Marie-Abele C.", ""]]}, {"id": "1905.07631", "submitter": "Chandan Singh", "authors": "Summer Devlin, Chandan Singh, W. James Murdoch, Bin Yu", "title": "Disentangled Attribution Curves for Interpreting Random Forests and\n  Boosted Trees", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tree ensembles, such as random forests and AdaBoost, are ubiquitous machine\nlearning models known for achieving strong predictive performance across a wide\nvariety of domains. However, this strong performance comes at the cost of\ninterpretability (i.e. users are unable to understand the relationships a\ntrained random forest has learned and why it is making its predictions). In\nparticular, it is challenging to understand how the contribution of a\nparticular feature, or group of features, varies as their value changes. To\naddress this, we introduce Disentangled Attribution Curves (DAC), a method to\nprovide interpretations of tree ensemble methods in the form of (multivariate)\nfeature importance curves. For a given variable, or group of variables, DAC\nplots the importance of a variable(s) as their value changes. We validate DAC\non real data by showing that the curves can be used to increase the accuracy of\nlogistic regression while maintaining interpretability, by including DAC as an\nadditional feature. In simulation studies, DAC is shown to out-perform\ncompeting methods in the recovery of conditional expectations. Finally, through\na case-study on the bike-sharing dataset, we demonstrate the use of DAC to\nuncover novel insights into a dataset.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 19:46:41 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Devlin", "Summer", ""], ["Singh", "Chandan", ""], ["Murdoch", "W. James", ""], ["Yu", "Bin", ""]]}, {"id": "1905.07659", "submitter": "Avleen S. Bijral", "authors": "Avleen S. Bijral", "title": "On Selecting Stable Predictors in Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the feature selection methodology to dependent data and propose a\nnovel time series predictor selection scheme that accommodates statistical\ndependence in a more typical i.i.d sub-sampling based framework. Furthermore,\nthe machinery of mixing stationary processes allows us to quantify the\nimprovements of our approach over any base predictor selection method (such as\nlasso) even in a finite sample setting. Using the lasso as a base procedure we\ndemonstrate the applicability of our methods to simulated and several real time\nseries datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 23:45:32 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bijral", "Avleen S.", ""]]}, {"id": "1905.07670", "submitter": "Rafael Stern", "authors": "Luis G. Esteves, Rafael Izbicki and Rafael B. Stern", "title": "Teaching decision theory proof strategies using a crowdsourcing problem", "comments": "21 pages, 2 figures. This is an Accepted Manuscript of an article\n  published by Taylor & Francis Group in The American Statistician, available\n  online: https://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1264316", "journal-ref": "The American Statistician, 71(4), 336-343 (2017)", "doi": "10.1080/00031305.2016.1264316", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching how to derive minimax decision rules can be challenging because of\nthe lack of examples that are simple enough to be used in the classroom.\nMotivated by this challenge, we provide a new example that illustrates the use\nof standard techniques in the derivation of optimal decision rules under the\nBayes and minimax approaches. We discuss how to predict the value of an unknown\nquantity, $\\theta \\! \\in \\! \\{0,1\\}$, given the opinions of $n$ experts. An\nimportant example of such crowdsourcing problem occurs in modern cosmology,\nwhere $\\theta$ indicates whether a given galaxy is merging or not, and $Y_1,\n\\ldots, Y_n$ are the opinions from $n$ astronomers regarding $\\theta$. We use\nthe obtained prediction rules to discuss advantages and disadvantages of the\nBayes and minimax approaches to decision theory. The material presented here is\nintended to be taught to first-year graduate students.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 01:40:52 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Esteves", "Luis G.", ""], ["Izbicki", "Rafael", ""], ["Stern", "Rafael B.", ""]]}, {"id": "1905.07764", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh, Sebastien J-P.A. Haneuse, James M. Robins, Sarah E.\n  Robertson, Ashley L. Buchanan, Elisabeth A. Stuart, Miguel A. Hern\\'an", "title": "Study designs for extending causal inferences from a randomized trial to\n  a target population", "comments": "first submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine study designs for extending (generalizing or transporting) causal\ninferences from a randomized trial to a target population. Specifically, we\nconsider nested trial designs, where randomized individuals are nested within a\nsample from the target population, and non-nested trial designs, including\ncomposite dataset designs, where a randomized trial is combined with a\nseparately obtained sample of non-randomized individuals from the target\npopulation. We show that the causal quantities that can be identified in each\nstudy design depend on what is known about the probability of sampling\nnon-randomized individuals. For each study design, we examine identification of\npotential outcome means via the g-formula and inverse probability weighting.\nLast, we explore the implications of the sampling properties underlying the\ndesigns for the identification and estimation of the probability of trial\nparticipation.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 16:15:15 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Haneuse", "Sebastien J-P. A.", ""], ["Robins", "James M.", ""], ["Robertson", "Sarah E.", ""], ["Buchanan", "Ashley L.", ""], ["Stuart", "Elisabeth A.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1905.07771", "submitter": "Martina Han\\v{c}ov\\'a", "authors": "Martina Han\\v{c}ov\\'a, Gabriela Voz\\'arikov\\'a, Andrej Gajdo\\v{s},\n  Jozef Han\\v{c}", "title": "Estimating variances in time series linear regression models using\n  empirical BLUPs and convex optimization", "comments": "29 pages, 1 figure, 5 tables", "journal-ref": "Statistical Papers 2020", "doi": "10.1007/s00362-020-01165-5", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage estimation method of variance components in time\nseries models known as FDSLRMs, whose observations can be described by a linear\nmixed model (LMM). We based estimating variances, fundamental quantities in a\ntime series forecasting approach called kriging, on the empirical (plug-in)\nbest linear unbiased predictions of unobservable random components in FDSLRM.\n  The method, providing invariant non-negative quadratic estimators, can be\nused for any absolutely continuous probability distribution of time series\ndata. As a result of applying the convex optimization and the LMM methodology,\nwe resolved two problems $-$ theoretical existence and equivalence between\nleast squares estimators, non-negative (M)DOOLSE, and maximum likelihood\nestimators, (RE)MLE, as possible starting points of our method and a practical\nlack of computational implementation for FDSLRM. As for computing (RE)MLE in\nthe case of $ n $ observed time series values, we also discovered a new\nalgorithm of order $\\mathcal{O}(n)$, which at the default precision is $10^7$\ntimes more accurate and $n^2$ times faster than the best current Python(or\nR)-based computational packages, namely CVXPY, CVXR, nlme, sommer and mixed.\n  We illustrate our results on three real data sets $-$ electricity\nconsumption, tourism and cyber security $-$ which are easily available,\nreproducible, sharable and modifiable in the form of interactive Jupyter\nnotebooks.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 16:46:55 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Han\u010dov\u00e1", "Martina", ""], ["Voz\u00e1rikov\u00e1", "Gabriela", ""], ["Gajdo\u0161", "Andrej", ""], ["Han\u010d", "Jozef", ""]]}, {"id": "1905.07812", "submitter": "Samuele Centorrino", "authors": "Samuele Centorrino, Fr\\'ed\\'erique F\\`eve, Jean-Pierre Florens", "title": "Nonparametric Instrumental Regressions with (Potentially Discrete)\n  Instruments Independent of the Error Term", "comments": "Authors would like to thank Enno Mammen for helpful preliminary\n  conversations on this topic. Samuele Centorrino gratefully acknowledges\n  support from the Stony Brook University Initiative for the Arts, Humanities,\n  and Lettered Social Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a nonparametric instrumental regression model with continuous\nendogenous regressor where instruments are fully independent of the error term.\nThis assumption allows us to extend the reach of this model to cases where the\ninstrumental variable is discrete, and therefore to substantially enlarge its\npotential empirical applications. Under our assumptions, the regression\nfunction becomes solution to a nonlinear integral equation. We contribute to\nexisting literature by providing an exhaustive analysis of identification and a\nsimple iterative estimation procedure. Details on the implementation and on the\nasymptotic properties of this estimation algorithm are given. We conclude the\npaper with a simulation experiment for a binary instrument and an empirical\napplication to the estimation of the Engel curve for food, where we show that\nour estimator delivers results that are consistent with existing evidence under\nseveral discretizations of the instrumental variable.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 21:10:22 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Centorrino", "Samuele", ""], ["F\u00e8ve", "Fr\u00e9d\u00e9rique", ""], ["Florens", "Jean-Pierre", ""]]}, {"id": "1905.07912", "submitter": "Veronique Maume-Deschamps", "authors": "Abdul-Fattah Abu-Awwad (ICJ, PSPM), V\\'eronique Maume-Deschamps (ICJ,\n  PSPM), Pierre Ribereau (PSPM, ICJ)", "title": "Semiparametric estimation for space-time max-stable processes: F\n  -madogram-based estimation approach", "comments": "arXiv admin note: text overlap with arXiv:1507.07750 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes have been expanded to quantify extremal dependence in\nspatio-temporal data. Due to the interaction between space and time,\nspatio-temporal data are often complex to analyze. So, characterizing these\ndependencies is one of the crucial challenges in this field of statistics. This\npaper suggests a semiparametric inference methodology based on the\nspatio-temporal F-madogram for estimating the parameters of a space-time\nmax-stable process using gridded data. The performance of the method is\ninvestigated through various simulation studies. Finally, we apply our\ninferential procedure to quantify the extremal behavior of radar rainfall data\nin a region in the State of Florida.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 07:04:38 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Abu-Awwad", "Abdul-Fattah", "", "ICJ, PSPM"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ,\n  PSPM"], ["Ribereau", "Pierre", "", "PSPM, ICJ"]]}, {"id": "1905.07976", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Richard G. Everitt", "title": "Stratified sampling and bootstrapping for approximate Bayesian\n  computation", "comments": "35 pages, 10 figures. Major revision: uses stratification with\n  rejection and importance sampling ABC; compares several bootstrap procedures;\n  new supernova case study", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approximate Bayesian computation (ABC) is computationally intensive for\ncomplex model simulators. To exploit expensive simulations, data-resampling via\nbootstrapping can be employed to obtain many artificial datasets at little\ncost. However, when using this approach within ABC, the posterior variance is\ninflated, thus resulting in biased posterior inference. Here we use stratified\nMonte Carlo to considerably reduce the bias induced by data resampling. We also\nshow empirically that it is possible to obtain reliable inference using a\nlarger than usual ABC threshold. Finally, we show that with stratified Monte\nCarlo we obtain a less variable ABC likelihood. Ultimately we show how our\napproach improves the computational efficiency of the ABC samplers. We\nconstruct several ABC samplers employing our methodology, such as rejection and\nimportance ABC samplers, and ABC-MCMC samplers. We consider simulation studies\nfor static (Gaussian, g-and-k distribution, Ising model, astronomical model)\nand dynamic models (Lotka-Volterra). We compare against state-of-art sequential\nMonte Carlo ABC samplers, synthetic likelihoods, and likelihood-free Bayesian\noptimization. For a computationally expensive Lotka-Volterra case study, we\nfound that our strategy leads to a more than 10-fold computational saving,\ncompared to a sampler that does not use our novel approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 10:28:24 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 13:44:55 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 08:28:11 GMT"}, {"version": "v4", "created": "Fri, 2 Jul 2021 16:15:19 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Picchini", "Umberto", ""], ["Everitt", "Richard G.", ""]]}, {"id": "1905.08011", "submitter": "Lixing Zhu", "authors": "Falong Tan and Lixing Zhu", "title": "Integrated conditional moment test and beyond: when the number of\n  covariates is divergent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic integrated conditional moment test is a promising method for\ntesting regression model misspecification. However, it severely suffers from\nthe curse of dimensionality. To extend it to handle the testing problem for\nparametric multi-index models with diverging number of covariates, we\ninvestigate three issues in inference in this paper. First, we study the\nconsistency and asymptotically linear representation of the least squares\nestimator of the parameter matrix at faster rates of divergence than those in\nthe literature for nonlinear models. Second, we propose, via sufficient\ndimension reduction techniques, an adaptive-to-model version of the integrated\nconditional moment test. We study the asymptotic properties of the new test\nunder both the null and alternative hypothesis to examine its ability of\nsignificance level maintenance and its sensitivity to the global and local\nalternatives that are distinct from the null at the fastest possible rate in\nhypothesis testing. Third, we derive the consistency of the bootstrap\napproximation for the new test in the diverging dimension setting. The\nnumerical studies show that the new test can very much enhance the performance\nof the original ICM test in high-dimensional scenarios. We also apply the test\nto a real data set for illustrations.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 11:56:33 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 01:17:00 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Tan", "Falong", ""], ["Zhu", "Lixing", ""]]}, {"id": "1905.08082", "submitter": "John Harlim", "authors": "Shixiao W. Jiang and John Harlim", "title": "Modeling of Missing Dynamical Systems: Deriving Parametric Models using\n  a Nonparametric Framework", "comments": null, "journal-ref": null, "doi": "10.1007/s40687-020-00217-4", "report-no": null, "categories": "stat.ME math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider modeling missing dynamics with a nonparametric\nnon-Markovian model, constructed using the theory of kernel embedding of\nconditional distributions on appropriate Reproducing Kernel Hilbert Spaces\n(RKHS), equipped with orthonormal basis functions. Depending on the choice of\nthe basis functions, the resulting closure model from this nonparametric\nmodeling formulation is in the form of parametric model. This suggests that the\nsuccess of various parametric modeling approaches that were proposed in various\ndomains of applications can be understood through the RKHS representations.\nWhen the missing dynamical terms evolve faster than the relevant observable of\ninterest, the proposed approach is consistent with the effective dynamics\nderived from the classical averaging theory. In the linear Gaussian case\nwithout the time-scale gap, we will show that the proposed non-Markovian model\nwith a very long memory yields an accurate estimation of the nontrivial\nautocovariance function for the relevant variable of the full dynamics.\nSupporting numerical results on instructive nonlinear dynamics show that the\nproposed approach is able to replicate high-dimensional missing dynamical terms\non problems with and without the separation of temporal scales.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 16:40:17 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 14:39:35 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 16:46:09 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jiang", "Shixiao W.", ""], ["Harlim", "John", ""]]}, {"id": "1905.08122", "submitter": "Konul Mustafayeva", "authors": "Konul Mustafayeva and Weining Wang", "title": "Non-Parametric Estimation of Spot Covariance Matrix with High-Frequency\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating spot covariance is an important issue to study, especially with\nthe increasing availability of high-frequency financial data. We study the\nestimation of spot covariance using a kernel method for high-frequency data. In\nparticular, we consider first the kernel weighted version of realized\ncovariance estimator for the price process governed by a continuous\nmultivariate semimartingale. Next, we extend it to the threshold kernel\nestimator of the spot covariances when the underlying price process is a\ndiscontinuous multivariate semimartingale with finite activity jumps. We derive\nthe asymptotic distribution of the estimators for both fixed and shrinking\nbandwidth. The estimator in a setting with jumps has the same rate of\nconvergence as the estimator for diffusion processes without jumps. A\nsimulation study examines the finite sample properties of the estimators. In\naddition, we study an application of the estimator in the context of covariance\nforecasting. We discover that the forecasting model with our estimator\noutperforms a benchmark model in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 14:01:22 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Mustafayeva", "Konul", ""], ["Wang", "Weining", ""]]}, {"id": "1905.08308", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca, Matus Maciak, Francois Wahl", "title": "Detection of similar successive groups in a model with diverging number\n  of variable groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a linear model with grouped explanatory variables is\nconsidered. The idea is to perform an automatic detection of different\nsuccessive groups of the unknown coefficients under the assumption that the\nnumber of groups is of the same order as the sample size. The standard least\nsquares loss function and the quantile loss function are both used together\nwith the fused and adaptive fused penalty to simultaneously estimate and group\nthe unknown parameters. The proper convergence rate is given for the obtained\nestimators and the upper bound for the number of different successive group is\nderived. A simulation study is used to compare the empirical performance of the\nproposed fused and adaptive fused estimators and a real application on the air\nquality data demonstrates the practical applicability of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 19:25:18 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Ciuperca", "Gabriela", ""], ["Maciak", "Matus", ""], ["Wahl", "Francois", ""]]}, {"id": "1905.08330", "submitter": "Eric Oh", "authors": "Eric J. Oh, Bryan E. Shepherd, Thomas Lumley, and Pamela A. Shaw", "title": "Raking and Regression Calibration: Methods to Address Bias from\n  Correlated Covariate and Time-to-Event Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical studies that depend on electronic health records (EHR) data are often\nsubject to measurement error, as the data are not collected to support research\nquestions under study. These data errors, if not accounted for in study\nanalyses, can obscure or cause spurious associations between patient exposures\nand disease risk. Methodology to address covariate measurement error has been\nwell developed; however, time-to-event error has also been shown to cause\nsignificant bias but methods to address it are relatively underdeveloped. More\ngenerally, it is possible to observe errors in both the covariate and the\ntime-to-event outcome that are correlated. We propose regression calibration\n(RC) estimators to simultaneously address correlated error in the covariates\nand the censored event time. Although RC can perform well in many settings with\ncovariate measurement error, it is biased for nonlinear regression models, such\nas the Cox model. Thus, we additionally propose raking estimators which are\nconsistent estimators of the parameter defined by the population estimating\nequation. Raking can improve upon RC in certain settings with failure-time\ndata, require no explicit modeling of the error structure, and can be utilized\nunder outcome-dependent sampling designs. We discuss features of the underlying\nestimation problem that affect the degree of improvement the raking estimator\nhas over the RC approach. Detailed simulation studies are presented to examine\nthe performance of the proposed estimators under varying levels of signal,\nerror, and censoring. The methodology is illustrated on observational EHR data\non HIV outcomes from the Vanderbilt Comprehensive Care Clinic.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 20:19:05 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 15:44:39 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Oh", "Eric J.", ""], ["Shepherd", "Bryan E.", ""], ["Lumley", "Thomas", ""], ["Shaw", "Pamela A.", ""]]}, {"id": "1905.08360", "submitter": "Daniel Chicharro", "authors": "Daniel Chicharro, Stefano Panzeri, and Ilya Shpitser", "title": "Conditionally-additive-noise Models for Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Constraint-based structure learning algorithms infer the causal structure of\nmultivariate systems from observational data by determining an equivalent class\nof causal structures compatible with the conditional independencies in the\ndata. Methods based on additive-noise (AN) models have been proposed to further\ndiscriminate between causal structures that are equivalent in terms of\nconditional independencies. These methods rely on a particular form of the\ngenerative functional equations, with an additive noise structure, which allows\ninferring the directionality of causation by testing the independence between\nthe residuals of a nonlinear regression and the predictors\n(nrr-independencies). Full causal structure identifiability has been proven for\nsystems that contain only additive-noise equations and have no hidden\nvariables. We extend the AN framework in several ways. We introduce alternative\nregression-free tests of independence based on conditional variances\n(cv-independencies). We consider conditionally-additive-noise (CAN) models, in\nwhich the equations may have the AN form only after conditioning. We exploit\nasymmetries in nrr-independencies or cv-independencies resulting from the CAN\nform to derive a criterion that infers the causal relation between a pair of\nvariables in a multivariate system without any assumption about the form of the\nequations or the presence of hidden variables.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 22:06:49 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chicharro", "Daniel", ""], ["Panzeri", "Stefano", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1905.08374", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Gaussian Process Learning via Fisher Scoring of Vecchia's Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a single pass algorithm for computing the gradient and Fisher\ninformation of Vecchia's Gaussian process loglikelihood approximation, which\nprovides a computationally efficient means for applying the Fisher scoring\nalgorithm for maximizing the loglikelihood. The advantages of the optimization\ntechniques are demonstrated in numerical examples and in an application to Argo\nocean temperature data. The new methods are more accurate and much faster than\nan optimization method that uses only function evaluations, especially when the\ncovariance function has many parameters. This allows practitioners to fit\nnonstationary models to large spatial and spatial-temporal datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 23:03:03 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1905.08393", "submitter": "Georgios Papageorgiou", "authors": "Georgios Papageorgiou and Benjamin C. Marshall", "title": "Bayesian semiparametric analysis of multivariate continuous responses,\n  with variable selection", "comments": "Journal of Computational and Graphical Statistics (2020)", "journal-ref": null, "doi": "10.1080/10618600.2020.1739534", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an approach to Bayesian semiparametric inference for\nGaussian multivariate response regression. We are motivated by various small\nand medium dimensional problems from the physical and social sciences. The\nstatistical challenges revolve around dealing with the unknown mean and\nvariance functions and in particular, the correlation matrix. To tackle these\nproblems, we have developed priors over the smooth functions and a Markov chain\nMonte Carlo algorithm for inference and model selection. Specifically,\nDirichlet process mixtures of Gaussian distributions are used as the basis for\na cluster-inducing prior over the elements of the correlation matrix. The\nsmooth, multidimensional means and variances are represented using radial basis\nfunction expansions. The complexity of the model, in terms of variable\nselection and smoothness, is then controlled by spike-slab priors. A simulation\nstudy is presented, demonstrating performance as the response dimension\nincreases. Finally, the model is fit to a number of real world datasets. An R\npackage, scripts for replicating synthetic and real data examples, and a\ndetailed description of the MCMC sampler are available in the supplementary\nmaterials online.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 00:56:34 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 17:25:09 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Papageorgiou", "Georgios", ""], ["Marshall", "Benjamin C.", ""]]}, {"id": "1905.08414", "submitter": "Vadim Sokolov", "authors": "Vadim Sokolov and Michael Polson", "title": "Strategic Bayesian Asset Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategic asset allocation requires an investor to select stocks from a given\nbasket of assets. The perspective of our investor is to maximize risk-adjusted\nalpha returns relative to a benchmark index. Historical returns are used to\nprovide inputs into an optimization algorithm. Our approach uses Bayesian\nregularization to not only provide stock selection but also optimal sequential\nportfolio weights. By incorporating investor preferences with a number of\ndifferent regularization penalties we extend the approaches of Black (1992) and\nPuelz (2015). We tailor standard sparse MCMC algorithms to calculate portfolio\nweights and perform selection. We illustrate our methodology on stock selection\nfrom the SP100 stock index and from the top fifty holdings of two hedge funds\nRenaissance Technologies and Viking Global. Finally, we conclude with\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 03:02:48 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 00:12:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sokolov", "Vadim", ""], ["Polson", "Michael", ""]]}, {"id": "1905.08424", "submitter": "Khandoker Akib Mohammad", "authors": "Khandoker Akib Mohammad, Yuichi Hirose, Budhi Surya and Yuan Yao", "title": "Efficient Estimation For The Cox Proportional Hazards Cure Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While analysing time-to-event data, it is possible that a certain fraction of\nsubjects will never experience the event of interest and they are said to be\ncured. When this feature of survival models is taken into account, the models\nare commonly referred to as cure models. In the presence of covariates, the\nconditional survival function of the population can be modelled by using cure\nmodel which depends on the probability of being uncured (incidence) and the\nconditional survival function of the uncured subjects (latency), and a\ncombination of logistic regression and Cox proportional hazards (PH) regression\nis used to model the incidence and latency respectively. In this paper, we have\nshown the asymptotic normality of the profile likelihood estimator via\nasymptotic expansion of the profile likelihood and obtain the explicit form of\nthe variance estimator with an implicit function in the profile likelihood. We\nhave also shown the efficient score function based on projection theory and the\nprofile likelihood score function are equal. Our contribution in this paper is\nthat we have expressed the efficient information matrix as the variance of the\nprofile likelihood score function. A simulation study suggests that the\nestimated standard errors from bootstrap samples (SMCURE package) and the\nprofile likelihood score function (our approach) are providing similar and\ncomparable results. The numerical result of our proposed method is also shown\nby using the melanoma data from SMCURE R-package (Cai et al., 2012) and we\ncompare the results with the output obtained from SMCURE package.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 03:26:40 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 23:02:35 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 03:47:04 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2020 04:35:16 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Mohammad", "Khandoker Akib", ""], ["Hirose", "Yuichi", ""], ["Surya", "Budhi", ""], ["Yao", "Yuan", ""]]}, {"id": "1905.08446", "submitter": "Runmin Wang", "authors": "Runmin Wang, Stanislav Volgushev, Xiaofeng Shao", "title": "Inference for Change Points in High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers change point testing and estimation for high\ndimensional data. In the case of testing for a mean shift, we propose a new\ntest which is based on U-statistics and utilizes the self-normalization\nprinciple. Our test targets dense alternatives in the high dimensional setting\nand involves no tuning parameters. The weak convergence of a sequential\nU-statistic based process is shown as an important theoretical contribution.\nExtensions to testing for multiple unknown change points in the mean, and\ntesting for changes in the covariance matrix are also presented with rigorous\nasymptotic theory and encouraging simulation results. Additionally, we\nillustrate how our approach can be used in combination with wild binary\nsegmentation to estimate the number and location of multiple unknown change\npoints.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 05:30:11 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Wang", "Runmin", ""], ["Volgushev", "Stanislav", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1905.08535", "submitter": "Emmanuel Guerre", "authors": "Marcelo Fernandes, Emmanuel Guerre, Eduardo Horta", "title": "Smoothing quantile regressions", "comments": "Expression of optimal AMSE in Theorem 4 has been corrected. Accepted\n  at Journal of Business Economics and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to smooth the entire objective function, rather than only the\ncheck function, in a linear quantile regression context. Not only does the\nresulting smoothed quantile regression estimator yield a lower mean squared\nerror and a more accurate Bahadur-Kiefer representation than the standard\nestimator, but it is also asymptotically differentiable. We exploit the latter\nto propose a quantile density estimator that does not suffer from the curse of\ndimensionality. This means estimating the conditional density function without\nworrying about the dimension of the covariate vector. It also allows for\ntwo-stage efficient quantile regression estimation. Our asymptotic theory holds\nuniformly with respect to the bandwidth and quantile level. Finally, we propose\na rule of thumb for choosing the smoothing bandwidth that should approximate\nwell the optimal bandwidth. Simulations confirm that our smoothed quantile\nregression estimator indeed performs very well in finite samples.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 10:36:08 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 12:06:41 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 09:54:24 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Fernandes", "Marcelo", ""], ["Guerre", "Emmanuel", ""], ["Horta", "Eduardo", ""]]}, {"id": "1905.08552", "submitter": "Jian He", "authors": "Jian He, Asma Khedher, Peter Spreij", "title": "A Kalman particle filter for online parameter estimation with\n  applications to affine models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of estimating the posterior distribution\nof the static parameters of a continuous time state space model with discrete\ntime observations by an algorithm that combines the Kalman filter and a\nparticle filter. The proposed algorithm is semi-recursive and has a two layer\nstructure, in which the outer layer provides the estimation of the posterior\ndistribution of the unknown parameters and the inner layer provides the\nestimation of the posterior distribution of the state variables. This algorithm\nhas a similar structure as the so-called recursive nested particle filter, but\nunlike the latter filter, in which both layers use a particle filter, this\nproposed algorithm introduces a dynamic kernel to sample the parameter\nparticles in the outer layer to obtain a higher convergence speed. Moreover,\nthis algorithm also implements the Kalman filter in the inner layer to reduce\nthe computational time. This algorithm can also be used to estimate the\nparameters that suddenly change value. We prove that, for a state space model\nwith a certain structure, the estimated posterior distribution of the unknown\nparameters and the state variables converge to the actual distribution in $L_p$\nwith rate of order $\\mathcal{O}(N^{-\\frac{1}{2}}+\\delta^{\\frac{1}{2}})$, where\n$N$ is the number of particles for the parameters in the outer layer and\n$\\delta$ is the maximum time step between two consecutive observations. We\npresent numerical results of the implementation of this algorithm, in\nparticularly we implement this algorithm for affine interest models, possibly\nwith stochastic volatility, although the algorithm can be applied to a much\nbroader class of models.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 11:18:02 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["He", "Jian", ""], ["Khedher", "Asma", ""], ["Spreij", "Peter", ""]]}, {"id": "1905.08659", "submitter": "Kevin Wilson Dr", "authors": "Kevin James Wilson and Malcolm Farrow (School of Mathematics,\n  Statistics & Physics, Newcastle University, UK)", "title": "Assurance for sample size determination in reliability demonstration\n  testing", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manufacturers are required to demonstrate products meet reliability targets.\nA typical way to achieve this is with reliability demonstration tests (RDTs),\nin which a number of products are put on test and the test is passed if a\ntarget reliability is achieved. There are various methods for determining the\nsample size for RDTs, typically based on the power of a hypothesis test\nfollowing the RDT or risk criteria. Bayesian risk criteria approaches can\nconflate the choice of sample size and the analysis to be undertaken once the\ntest has been conducted and rely on the specification of somewhat artificial\nacceptable and rejectable reliability levels. In this paper we offer an\nalternative approach to sample size determination based on the idea of\nassurance. This approach chooses the sample size to answer provide a certain\nprobability that the RDT will result in a successful outcome. It separates the\ndesign and analysis of the RDT, allowing different priors for each. We develop\nthe assurance approach for sample size calculations in RDTs for binomial and\nWeibull likelihoods and propose appropriate prior distributions for the design\nand analysis of the test. In each case, we illustrate the approach with an\nexample based on real data.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 14:08:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Wilson", "Kevin James", "", "School of Mathematics,\n  Statistics & Physics, Newcastle University, UK"], ["Farrow", "Malcolm", "", "School of Mathematics,\n  Statistics & Physics, Newcastle University, UK"]]}, {"id": "1905.08693", "submitter": "Jonathan Bartlett", "authors": "Jonathan W. Bartlett", "title": "Robustness of ANCOVA in randomised trials with unequal randomisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomised trials with continuous outcomes are often analysed using ANCOVA,\nwith adjustment for prognostic baseline covariates. In an article published\nrecently, Wang \\etal proved that in this setting the model based standard error\nestimator for the treamtent effect is consistent under outcome model\nmisspecification, provided the probability of randomisation to each treatment\nis 1/2. In this article, we extend their results allowing for unequal\nrandomisation. These demonstrate that the model based standard error is in\ngeneral inconsistent when the randomisation probability differs from 1/2. In\ncontrast, the sandwich standard error can provide asymptotically valid\ninferences under misspecification when randomisation probabilities are not\nequal, and is therefore recommended when randomisation is unequal.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:24:21 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Bartlett", "Jonathan W.", ""]]}, {"id": "1905.08726", "submitter": "Jessica Silva Lomba", "authors": "Jessica Silva Lomba and Maria Isabel Fraga Alves", "title": "L-moments for automatic threshold selection in extreme value analysis", "comments": null, "journal-ref": "L-moments for automatic threshold selection in extreme value\n  analysis. Stoch Environ Res Risk Assess 34, 465-491 (2020)", "doi": "10.1007/s00477-020-01789-x", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In extreme value analysis, sensitivity of inference to the definition of\nextreme event is a paramount issue. Under the peaks-over-threshold (POT)\napproach, this translates directly into the need of fitting a Generalized\nPareto distribution to observations above a suitable level that balances bias\nversus variance of estimates. Selection methodologies established in the\nliterature face recurrent challenges such as an inherent subjectivity or high\ncomputational intensity. We suggest a truly automated method for threshold\ndetection, aiming at time efficiency and elimination of subjective judgment.\nBased on the well-established theory of L-moments, this versatile data-driven\ntechnique can handle batch processing of large collections of extremes data,\nwhile also presenting good performance on small samples.\n  The technique's performance is evaluated in a large simulation study and\nillustrated with significant wave height data sets from the literature. We find\nthat it compares favorably to other state-of-the-art methods regarding the\nchoice of threshold, associated parameter estimation and the ultimate goal of\ncomputationally efficient return level estimation.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 16:12:56 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lomba", "Jessica Silva", ""], ["Alves", "Maria Isabel Fraga", ""]]}, {"id": "1905.08737", "submitter": "Edwin Fong", "authors": "Edwin Fong, Chris Holmes", "title": "On the marginal likelihood and cross-validation", "comments": "To appear in Biometrika; renamed score to 'cumulative\n  cross-validation score' for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian statistics, the marginal likelihood, also known as the evidence,\nis used to evaluate model fit as it quantifies the joint probability of the\ndata under the prior. In contrast, non-Bayesian models are typically compared\nusing cross-validation on held-out data, either through $k$-fold partitioning\nor leave-$p$-out subsampling. We show that the marginal likelihood is formally\nequivalent to exhaustive leave-$p$-out cross-validation averaged over all\nvalues of $p$ and all held-out test sets when using the log posterior\npredictive probability as the scoring rule. Moreover, the log posterior\npredictive is the only coherent scoring rule under data exchangeability. This\noffers new insight into the marginal likelihood and cross-validation and\nhighlights the potential sensitivity of the marginal likelihood to the choice\nof the prior. We suggest an alternative approach using cumulative\ncross-validation following a preparatory training phase. Our work has\nconnections to prequential analysis and intrinsic Bayes factors but is\nmotivated through a different course.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 16:29:34 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 21:53:27 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Fong", "Edwin", ""], ["Holmes", "Chris", ""]]}, {"id": "1905.09021", "submitter": "Dominik Liebl", "authors": "Dominik Po{\\ss}, Dominik Liebl, Alois Kneip, Hedwig Eisenbarth, Tor D.\n  Wager and Lisa Feldman Barrett", "title": "Super-Consistent Estimation of Points of Impact in Nonparametric\n  Regression with Functional Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting scalar outcomes using functional predictors is a classic problem\nin functional data analysis. In many applications, however, only specific\nlocations or time-points of the functional predictors have an impact on the\noutcome. Such ``points of impact'' are typically unknown and have to be\nestimated in addition to estimating the usual model components. We show that\nour points of impact estimator enjoys a super-consistent convergence rate and\ndoes not require knowledge or pre-estimates of the unknown model components.\nThis remarkable result facilitates the subsequent estimation of the remaining\nmodel components as shown in the theoretical part, where we consider the case\nof nonparametric models and the practically relevant case of generalized linear\nmodels. The finite sample properties of our estimators are assessed by means of\na simulation study. Our methodology is motivated by data from a psychological\nexperiment in which the participants were asked to continuously rate their\nemotional state while watching an affective video eliciting a varying intensity\nof emotional reactions.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 08:45:08 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 13:25:06 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 07:39:23 GMT"}, {"version": "v4", "created": "Fri, 31 May 2019 21:58:30 GMT"}, {"version": "v5", "created": "Tue, 9 Jul 2019 16:55:00 GMT"}, {"version": "v6", "created": "Tue, 18 Feb 2020 16:41:13 GMT"}, {"version": "v7", "created": "Mon, 13 Jul 2020 12:36:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Po\u00df", "Dominik", ""], ["Liebl", "Dominik", ""], ["Kneip", "Alois", ""], ["Eisenbarth", "Hedwig", ""], ["Wager", "Tor D.", ""], ["Barrett", "Lisa Feldman", ""]]}, {"id": "1905.09371", "submitter": "Kori Khan", "authors": "Kori Khan, Catherine A. Calder", "title": "Restricted Spatial Regression Methods: Implications for Inference", "comments": "Minor notation and typo edits. Primary change is to statement and\n  proof of Theorem 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of spatial confounding between the spatial random effect and the\nfixed effects in regression analyses has been identified as a concern in the\nstatistical literature. Multiple authors have offered perspectives and\npotential solutions. In this paper, for the areal spatial data setting, we show\nthat many of the methods designed to alleviate spatial confounding can be\nviewed as special cases of a general class of models. We refer to this class as\nRestricted Spatial Regression (RSR) models, extending terminology currently in\nuse. We offer a mathematically based exploration of the impact that RSR methods\nhave on inference for regression coefficients for the linear model. We then\nexplore whether these results hold in the generalized linear model setting for\ncount data using simulations. We show that the use of these methods have\ncounterintuitive consequences which defy the general expectations in the\nliterature. In particular, our results and the accompanying simulations suggest\nthat RSR methods will typically perform worse than non-spatial methods. These\nresults have important implications for dimension reduction strategies in\nspatial regression modeling. Specifically, we demonstrate that the problems\nwith RSR models cannot be fixed with a selection of \"better\" spatial basis\nvectors or dimension reduction techniques.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 21:23:45 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 15:43:28 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 15:35:56 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Khan", "Kori", ""], ["Calder", "Catherine A.", ""]]}, {"id": "1905.09515", "submitter": "P. Richard Hahn", "authors": "P. Richard Hahn, Vincent Dorie, and Jared S. Murray", "title": "Atlantic Causal Inference Conference (ACIC) Data Analysis Challenge 2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief note documents the data generating processes used in the 2017 Data\nAnalysis Challenge associated with the Atlantic Causal Inference Conference\n(ACIC). The focus of the challenge was estimation and inference for conditional\naverage treatment effects (CATEs) in the presence of targeted selection, which\nleads to strong confounding. The associated data files and further plots can be\nfound on the first author's web page.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 07:49:48 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Hahn", "P. Richard", ""], ["Dorie", "Vincent", ""], ["Murray", "Jared S.", ""]]}, {"id": "1905.09545", "submitter": "Takashi Takahashi", "authors": "Takashi Takahashi, Yoshiyuki Kabashima", "title": "Replicated Vector Approximate Message Passing For Resampling Problem", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resampling techniques are widely used in statistical inference and ensemble\nlearning, in which estimators' statistical properties are essential. However,\nexisting methods are computationally demanding, because repetitions of\nestimation/learning via numerical optimization/integral for each resampled data\nare required. In this study, we introduce a computationally efficient method to\nresolve such problem: replicated vector approximate message passing. This is\nbased on a combination of the replica method of statistical physics and an\naccurate approximate inference algorithm, namely the vector approximate message\npassing of information theory. The method provides tractable densities without\nrepeating estimation/learning, and the densities approximately offer an\narbitrary degree of the estimators' moment in practical time. In the\nexperiment, we apply the proposed method to the stability selection method,\nwhich is commonly used in variable selection problems. The numerical results\nshow its fast convergence and high approximation accuracy for problems\ninvolving both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 09:11:06 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Takahashi", "Takashi", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1905.09693", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Andrew Gelman and Matthijs V\\'ak\\'ar", "title": "Slamming the sham: A Bayesian model for adaptive adjustment with noisy\n  control data", "comments": "20 pages; to appear in Statistics in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not always clear how to adjust for control data in causal inference,\nbalancing the goals of reducing bias and variance. We show how, in a setting\nwith repeated experiments, Bayesian hierarchical modeling yields an adaptive\nprocedure that uses the data to determine how much adjustment to perform. The\nresult is a novel analysis with increased statistical efficiency compared to\nthe default analysis based on difference estimates. We demonstrate this\nprocedure on two real examples, as well as on a series of simulated datasets.\nWe show that the increased efficiency can have real-world consequences in terms\nof the conclusions that can be drawn from the experiments. We also discuss the\nrelevance of this work to causal inference and statistical design and analysis\nmore generally.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 21:14:31 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 15:36:01 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gelman", "Andrew", ""], ["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "1905.09722", "submitter": "Zhantao Lin", "authors": "Zhantao Lin, Nancy Flournoy, William F. Rosenberger", "title": "Random Norming Aids Analysis of Non-linear Regression Models with\n  Sequential Informative Dose Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-stage adaptive optimal design is an attractive option for increasing\nthe efficiency of clinical trials. In these designs, based on interim data, the\nlocally optimal dose is chosen for further exploration, which induces\ndependencies between data from the two stages. When the maximum likelihood\nestimator (MLE) is used under nonlinear regression models with independent\nnormal errors in a pilot study where the first stage sample size is fixed, and\nthe second stage sample size is large, the Fisher information fails to\nnormalize the estimator adequately asymptotically, because of dependencies. In\nthis situation, we present three alternative random information measures and\nshow that they provide better normalization of the MLE asymptotically. The\nperformance of random information measures is investigated in simulation\nstudies, and the results suggest that the observed information performs best\nwhen the sample size is small.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 15:30:16 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Lin", "Zhantao", ""], ["Flournoy", "Nancy", ""], ["Rosenberger", "William F.", ""]]}, {"id": "1905.09751", "submitter": "Xinkun Nie", "authors": "Xinkun Nie, Emma Brunskill, Stefan Wager", "title": "Learning When-to-Treat Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applied decision-making problems have a dynamic component: The\npolicymaker needs not only to choose whom to treat, but also when to start\nwhich treatment. For example, a medical doctor may choose between postponing\ntreatment (watchful waiting) and prescribing one of several available\ntreatments during the many visits from a patient. We develop an \"advantage\ndoubly robust\" estimator for learning such dynamic treatment rules using\nobservational data under the assumption of sequential ignorability. We prove\nwelfare regret bounds that generalize results for doubly robust learning in the\nsingle-step setting, and show promising empirical performance in several\ndifferent contexts. Our approach is practical for policy optimization, and does\nnot need any structural (e.g., Markovian) assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 16:15:46 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 23:30:25 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 06:32:00 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Nie", "Xinkun", ""], ["Brunskill", "Emma", ""], ["Wager", "Stefan", ""]]}, {"id": "1905.09800", "submitter": "Ben Moews", "authors": "Ben Moews and Joe Zuntz", "title": "Gaussbock: Fast parallel-iterative cosmological parameter estimation\n  with Bayesian nonparametrics", "comments": "19 pages, 10 figures, accepted for publication in ApJ", "journal-ref": null, "doi": "10.3847/1538-4357/ab93cb", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and apply Gaussbock, a new embarrassingly parallel iterative\nalgorithm for cosmological parameter estimation designed for an era of cheap\nparallel computing resources. Gaussbock uses Bayesian nonparametrics and\ntruncated importance sampling to accurately draw samples from posterior\ndistributions with an orders-of-magnitude speed-up in wall time over\nalternative methods. Contemporary problems in this area often suffer from both\nincreased computational costs due to high-dimensional parameter spaces and\nconsequent excessive time requirements, as well as the need for fine tuning of\nproposal distributions or sampling parameters. Gaussbock is designed\nspecifically with these issues in mind. We explore and validate the performance\nand convergence of the algorithm on a fast approximation to the Dark Energy\nSurvey Year 1 (DES Y1) posterior, finding reasonable scaling behavior with the\nnumber of parameters. We then test on the full DES Y1 posterior using\nlarge-scale supercomputing facilities, and recover reasonable agreement with\nprevious chains, although the algorithm can underestimate the tails of\npoorly-constrained parameters. Additionally, we discuss and demonstrate how\nGaussbock recovers complex posterior shapes very well at lower dimensions, but\nfaces challenges to perform well on such distributions in higher dimensions. In\naddition, we provide the community with a user-friendly software tool for\naccelerated cosmological parameter estimation based on the methodology\ndescribed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:50:04 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 03:29:46 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Moews", "Ben", ""], ["Zuntz", "Joe", ""]]}, {"id": "1905.09813", "submitter": "Ian Langmore", "authors": "Ian Langmore, Michael Dikovsky, Scott Geraedts, Peter Norgaard, Rob\n  Von Behren", "title": "A Condition Number for Hamiltonian Monte Carlo", "comments": "Significant changes: (i) Added connection to inverse Wishart\n  ensemble, (ii) added estimation of kappa, (iii) checked and corrected proofs,\n  (iv) re-wrote everything for clarity, (v) added authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hamiltonian Monte Carlo is a popular sampling technique for smooth target\ndensities. The scale lengths of the target have long been known to influence\nintegration error and sampling efficiency. However, quantitative measures\nintrinsic to the target have been lacking. In this paper, we restrict attention\nto the multivariate Gaussian and the leapfrog integrator, and obtain a\ncondition number corresponding to sampling efficiency. This number, based on\nthe spectral and Schatten norms, quantifies the number of leapfrog steps needed\nto efficiently sample. We demonstrate its utility by using this condition\nnumber to analyze HMC preconditioning techniques. We also find the condition\nnumber of large inverse Wishart matrices, from which we derive burn-in\nheuristics.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:59:31 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 16:00:24 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 00:16:31 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Langmore", "Ian", ""], ["Dikovsky", "Michael", ""], ["Geraedts", "Scott", ""], ["Norgaard", "Peter", ""], ["Von Behren", "Rob", ""]]}, {"id": "1905.09881", "submitter": "Ardalan Mirshani", "authors": "Ardalan Mirshani, Matthew Reimherr", "title": "Adaptive Function-on-Scalar Regression with a Smoothing Elastic Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new methodology, called AFSSEN, to simultaneously\nselect significant predictors and produce smooth estimates in a\nhigh-dimensional function-on-scalar linear model with a sub-Gaussian errors.\nOutcomes are assumed to lie in a general real separable Hilbert space, H, while\nparameters lie in a subspace known as a Cameron Martin space, K, which are\nclosely related to Reproducing Kernel Hilbert Spaces, so that parameter\nestimates inherit particular properties, such as smoothness or periodicity,\nwithout enforcing such properties on the data. We propose a regularization\nmethod in the style of an adaptive Elastic Net penalty that involves mixing two\ntypes of functional norms, providing a fine tune control of both the smoothing\nand variable selection in the estimated model. Asymptotic theory is provided in\nthe form of a functional oracle property, and the paper concludes with a\nsimulation study demonstrating the advantage of using AFSSEN over existing\nmethods in terms of prediction error and variable selection.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 19:23:58 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Mirshani", "Ardalan", ""], ["Reimherr", "Matthew", ""]]}, {"id": "1905.09948", "submitter": "HaiYing Wang", "authors": "HaiYing Wang", "title": "Divide-and-Conquer Information-Based Optimal Subdata Selection Algorithm", "comments": "21 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.1007/s42519-019-0048-5", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information-based optimal subdata selection (IBOSS) is a computationally\nefficient method to select informative data points from large data sets through\nprocessing full data by columns. However, when the volume of a data set is too\nlarge to be processed in the available memory of a machine, it is infeasible to\nimplement the IBOSS procedure. This paper develops a divide-and-conquer IBOSS\napproach to solving this problem, in which the full data set is divided into\nsmaller partitions to be loaded into the memory and then subsets of data are\nselected from each partitions using the IBOSS algorithm. We derive both finite\nsample properties and asymptotic properties of the resulting estimator.\nAsymptotic results show that if the full data set is partitioned randomly and\nthe number of partitions is not very large, then the resultant estimator has\nthe same estimation efficiency as the original IBOSS estimator. We also carry\nout numerical experiments to evaluate the empirical performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 21:56:38 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "HaiYing", ""]]}, {"id": "1905.09971", "submitter": "Niloy Biswas", "authors": "Niloy Biswas, Pierre E. Jacob, Paul Vanetti", "title": "Estimating Convergence of Markov chains with L-Lag Couplings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods generate samples that are\nasymptotically distributed from a target distribution of interest as the number\nof iterations goes to infinity. Various theoretical results provide upper\nbounds on the distance between the target and marginal distribution after a\nfixed number of iterations. These upper bounds are on a case by case basis and\ntypically involve intractable quantities, which limits their use for\npractitioners. We introduce L-lag couplings to generate computable,\nnon-asymptotic upper bound estimates for the total variation or the Wasserstein\ndistance of general Markov chains. We apply L-lag couplings to the tasks of (i)\ndetermining MCMC burn-in, (ii) comparing different MCMC algorithms with the\nsame target, and (iii) comparing exact and approximate MCMC. Lastly, we (iv)\nassess the bias of sequential Monte Carlo and self-normalized importance\nsamplers.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 23:45:38 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 19:40:48 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 19:18:27 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Biswas", "Niloy", ""], ["Jacob", "Pierre E.", ""], ["Vanetti", "Paul", ""]]}, {"id": "1905.09993", "submitter": "Dingjue Ji", "authors": "Dingjue Ji, Junwei Lu, Yiliang Zhang, Hongyu Zhao, Siyuan Gao", "title": "Inference of Dynamic Graph Changes for Functional Connectome", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and\n  Statistics, 26-28 August 2020, Online, PMLR 108:3230-3240", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic functional connectivity is an effective measure for the brain's\nresponses to continuous stimuli. We propose an inferential method to detect the\ndynamic changes of brain networks based on time-varying graphical models.\nWhereas most existing methods focus on testing the existence of change points,\nthe dynamics in the brain network offer more signals in many neuroscience\nstudies. We propose a novel method to conduct hypothesis testing on changes in\ndynamic brain networks. We introduce a bootstrap statistic to approximate the\nsupreme of the high-dimensional empirical processes over dynamically changing\nedges. Our simulations show that this framework can capture the change points\nwith changed connectivity. Finally, we apply our method to a brain imaging\ndataset under a natural audio-video stimulus and illustrate that we are able to\ndetect temporal changes in brain networks. The functions of the identified\nregions are consistent with specific emotional annotations, which are closely\nassociated with changes inferred by our method.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 01:44:10 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 22:07:08 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ji", "Dingjue", ""], ["Lu", "Junwei", ""], ["Zhang", "Yiliang", ""], ["Zhao", "Hongyu", ""], ["Gao", "Siyuan", ""]]}, {"id": "1905.10019", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla, Yi Yu, Daren Wang, and Alessandro Rinaldo", "title": "Optimal nonparametric change point detection and localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study change point detection and localization for univariate data in fully\nnonparametric settings in which, at each time point, we acquire an i.i.d.\nsample from an unknown distribution. We quantify the magnitude of the\ndistributional changes at the change points using the Kolmogorov--Smirnov\ndistance. We allow all the relevant parameters -- the minimal spacing between\ntwo consecutive change points, the minimal magnitude of the changes in the\nKolmogorov--Smirnov distance, and the number of sample points collected at each\ntime point -- to change with the length of time series. We generalize the\nrenowned binary segmentation (e.g. Scott and Knott, 1974) algorithm and its\nvariant, the wild binary segmentation of Fryzlewicz (2014), both originally\ndesigned for univariate mean change point detection problems, to our\nnonparametric settings and exhibit rates of consistency for both of them. In\nparticular, we prove that the procedure based on wild binary segmentation is\nnearly minimax rate-optimal. We further demonstrate a phase transition in the\nspace of model parameters that separates parameter combinations for which\nconsistent localization is possible from the ones for which this task is\nstatistical unfeasible. Finally, we provide extensive numerical experiments to\nsupport our theory. R code is available at https://github.com/hernanmp/NWBS.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 03:34:23 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Yu", "Yi", ""], ["Wang", "Daren", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1905.10267", "submitter": "Arthur Charpentier", "authors": "Arthur Charpentier and Emmanuel Flachaire", "title": "Extended Scale-Free Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Broido & Clauset (2019) mentioned that (strict) Scale-Free networks\nwere rare, in real life. This might be related to the statement of Stumpf, Wiuf\n& May (2005), that sub-networks of scale-free networks are not scale-free. In\nthe later, those sub-networks are asymptotically scale-free, but one should not\nforget about second-order deviation (possibly also third order actually). In\nthis article, we introduce a concept of extended scale-free network, inspired\nby the extended Pareto distribution, that actually is maybe more realistic to\ndescribe real network than the strict scale free property. This property is\nconsistent with Stumpf, Wiuf & May (2005): sub-network of scale-free larger\nnetworks are not strictly scale-free, but extended scale-free.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 14:56:24 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 18:52:21 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Charpentier", "Arthur", ""], ["Flachaire", "Emmanuel", ""]]}, {"id": "1905.10299", "submitter": "Weixin Cai", "authors": "Weixin Cai and Mark van der Laan", "title": "Nonparametric Bootstrap Inference for the Targeted Highly Adaptive LASSO\n  Estimator", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.09502", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Highly-Adaptive-LASSO Targeted Minimum Loss Estimator (HAL-TMLE) is an\nefficient plug-in estimator of a pathwise differentiable parameter in a\nstatistical model that at minimal (and possibly only) assumes that the\nsectional variation norm of the true nuisance functional parameters (i.e., the\nrelevant part of data distribution) are finite. It relies on an initial\nestimator (HAL-MLE) of the nuisance functional parameters by minimizing the\nempirical risk over the parameter space under the constraint that the sectional\nvariation norm of the candidate functions are bounded by a constant, where this\nconstant can be selected with cross-validation. In this article, we establish\nthat the nonparametric bootstrap for the HAL-TMLE, fixing the value of the\nsectional variation norm at a value larger or equal than the cross-validation\nselector, provides a consistent method for estimating the normal limit\ndistribution of the HAL-TMLE.\n  In order to optimize the finite sample coverage of the nonparametric\nbootstrap confidence intervals, we propose a selection method for this\nsectional variation norm that is based on running the nonparametric bootstrap\nfor all values of the sectional variation norm larger than the one selected by\ncross-validation, and subsequently determining a value at which the width of\nthe resulting confidence intervals reaches a plateau.\n  We demonstrate our method for 1) nonparametric estimation of the average\ntreatment effect based on observing on each unit a covariate vector, binary\ntreatment, and outcome, and for 2) nonparametric estimation of the integral of\nthe square of the multivariate density of the data distribution. In addition,\nwe also present simulation results for these two examples demonstrating the\nexcellent finite sample coverage of bootstrap-based confidence intervals.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:55:38 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 23:21:22 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Cai", "Weixin", ""], ["van der Laan", "Mark", ""]]}, {"id": "1905.10325", "submitter": "Shahin Tavakoli", "authors": "Shahin Tavakoli and Gilles Nisol and Marc Hallin", "title": "Factor Models for High-Dimensional Functional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we set up the theoretical foundations for a high-dimensional\nfunctional factor model approach in the analysis of large cross-sections\n(panels) of functional time series (FTS). We first establish a representation\nresult stating that, under mild assumptions on the covariance operator of the\ncross-section, we can represent each FTS as the sum of a common component\ndriven by scalar factors loaded via functional loadings, and a mildly\ncross-correlated idiosyncratic component. Our model and theory are developed in\na general Hilbert space setting that allows for mixed panels of functional and\nscalar time series. We then turn to the identification of the number of\nfactors, and the estimation of the factors, their loadings, and the common\ncomponents. We provide a family of information criteria for identifying the\nnumber of factors, and prove their consistency. We provide average error bounds\nfor the estimators of the factors, loadings, and common component; our results\nencompass the scalar case, for which they reproduce and extend, under weaker\nconditions, well-established similar results. Under slightly stronger\nassumptions, we also provide uniform bounds for the estimators of factors,\nloadings, and common component, thus extending existing scalar results. Our\nconsistency results in the asymptotic regime where the number $N$ of series and\nthe number $T$ of time observations diverge thus extend to the functional\ncontext the \"blessing of dimensionality\" that explains the success of factor\nmodels in the analysis of high-dimensional (scalar) time series. We provide\nnumerical illustrations that corroborate the convergence rates predicted by the\ntheory, and provide finer understanding of the interplay between $N$ and $T$\nfor estimation purposes. We conclude with an application to forecasting\nmortality curves, where we demonstrate that our approach outperforms existing\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:33:14 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 18:56:20 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 17:30:16 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 16:34:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tavakoli", "Shahin", ""], ["Nisol", "Gilles", ""], ["Hallin", "Marc", ""]]}, {"id": "1905.10330", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran", "title": "Dirac Delta Regression: Conditional Density Estimation with Clinical\n  Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized medicine seeks to identify the causal effect of treatment for a\nparticular patient as opposed to a clinical population at large. Most\ninvestigators estimate such personalized treatment effects by regressing the\noutcome of a randomized clinical trial (RCT) on patient covariates. The\nrealized value of the outcome may however lie far from the conditional\nexpectation. We therefore introduce a method called Dirac Delta Regression\n(DDR) that estimates the entire conditional density from RCT data in order to\nvisualize the probabilities across all possible treatment outcomes. DDR\ntransforms the outcome into a set of asymptotically Dirac delta distributions\nand then estimates the density using non-linear regression. The algorithm can\nidentify significant patient-specific treatment effects even when no population\nlevel effect exists. Moreover, DDR outperforms state-of-the-art algorithms in\nconditional density estimation on average regardless of the need for causal\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:42:56 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1905.10354", "submitter": "Holger Dette", "authors": "Holger Dette, Nina D\\\"ornemann", "title": "Likelihood ratio tests for many groups in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the asymptotic distribution of likelihood ratio\ntests in models with several groups, when the number of groups converges with\nthe dimension and sample size to infinity. We derive central limit theorems for\nthe logarithm of various test statistics and compare our results with the\napproximations obtained from a central limit theorem using a two step\nprocedure: first consider the number of groups fixed and assume that the sample\nsize and dimension converge to infinity, secondly investigating the resulting\ndistribution if the number of groups converges to infinity.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:40:16 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 19:45:28 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Dette", "Holger", ""], ["D\u00f6rnemann", "Nina", ""]]}, {"id": "1905.10413", "submitter": "Dustin Pluta", "authors": "Lingge Li, Dustin Pluta, Babak Shahbaba, Norbert Fortin, Hernando\n  Ombao, Pierre Baldi", "title": "Modeling Dynamic Functional Connectivity with Latent Factor Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic functional connectivity, as measured by the time-varying covariance\nof neurological signals, is believed to play an important role in many aspects\nof cognition. While many methods have been proposed, reliably establishing the\npresence and characteristics of brain connectivity is challenging due to the\nhigh dimensionality and noisiness of neuroimaging data. We present a latent\nfactor Gaussian process model which addresses these challenges by learning a\nparsimonious representation of connectivity dynamics. The proposed model\nnaturally allows for inference and visualization of time-varying connectivity.\nAs an illustration of the scientific utility of the model, application to a\ndata set of rat local field potential activity recorded during a complex\nnon-spatial memory task provides evidence of stimuli differentiation.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 19:10:44 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 04:12:11 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Li", "Lingge", ""], ["Pluta", "Dustin", ""], ["Shahbaba", "Babak", ""], ["Fortin", "Norbert", ""], ["Ombao", "Hernando", ""], ["Baldi", "Pierre", ""]]}, {"id": "1905.10432", "submitter": "Patrick Breheny", "authors": "Biyue Dai and Patrick Breheny", "title": "Cross validation approaches for penalized Cox regression", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross validation is commonly used for selecting tuning parameters in\npenalized regression, but its use in penalized Cox regression models has\nreceived relatively little attention in the literature. Due to its partial\nlikelihood construction, carrying out cross validation for Cox models is not\nstraightforward, and there are several potential approaches for implementation.\nHere, we propose two new cross-validation methods for Cox regression and\ncompare them to approaches that have been proposed elsewhere. Our proposed\napproach of cross-validating the linear predictors seems to offer an attractive\nbalance of performance and numerical stability. We illustrate these advantages\nusing simulated data as well as using them to analyze data from a\nhigh-dimensional study of survival in lung cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 20:11:23 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Dai", "Biyue", ""], ["Breheny", "Patrick", ""]]}, {"id": "1905.10573", "submitter": "Yoshikazu Terada", "authors": "Yoshikazu Terada and Hidetoshi Shimodaira", "title": "Selective inference after feature selection via multiscale bootstrap", "comments": "The title has changed (The previous title is \"Selective inference\n  after variable selection via multiscale bootstrap\"). 23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to show the confidence intervals or $p$-values of selected\nfeatures, or predictor variables in regression, but they often involve\nselection bias. The selective inference approach solves this bias by\nconditioning on the selection event. Most existing studies of selective\ninference consider a specific algorithm, such as Lasso, for feature selection,\nand thus they have difficulties in handling more complicated algorithms.\nMoreover, existing studies often consider unnecessarily restrictive events,\nleading to over-conditioning and lower statistical power. Our novel and\nwidely-applicable resampling method addresses these issues to compute an\napproximately unbiased selective $p$-value for the selected features. We prove\nthat the $p$-value computed by our resampling method is more accurate and more\npowerful than existing methods, while the computational cost is the same order\nas the classical bootstrap method. Numerical experiments demonstrate that our\nalgorithm works well even for more complicated feature selection methods such\nas non-convex regularization.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 11:30:27 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 06:45:35 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 02:10:29 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 06:37:14 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Terada", "Yoshikazu", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1905.10684", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh and James M. Robins and Sebastien J-P.A. Haneuse and\n  Iman Saeed and Sarah E. Robertson and Elisabeth A. Stuart and Miguel A.\n  Hern\\'an", "title": "Sensitivity analysis using bias functions for studies extending\n  inferences from a randomized trial to a target population", "comments": "first submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending (generalizing or transporting) causal inferences from a randomized\ntrial to a target population requires ``generalizability'' or\n``transportability'' assumptions, which state that randomized and\nnon-randomized individuals are exchangeable conditional on baseline covariates.\nThese assumptions are made on the basis of background knowledge, which is often\nuncertain or controversial, and need to be subjected to sensitivity analysis.\nWe present simple methods for sensitivity analyses that do not require detailed\nbackground knowledge about specific unknown or unmeasured determinants of the\noutcome or modifiers of the treatment effect. Instead, our methods directly\nparameterize violations of the assumptions using bias functions. We show how\nthe methods can be applied to non-nested trial designs, where the trial data\nare combined with a separately obtained sample of non-randomized individuals,\nas well as to nested trial designs, where a clinical trial is embedded within a\ncohort sampled from the target population. We illustrate the methods using data\nfrom a clinical trial comparing treatments for chronic hepatitis C infection.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 21:58:52 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Robins", "James M.", ""], ["Haneuse", "Sebastien J-P. A.", ""], ["Saeed", "Iman", ""], ["Robertson", "Sarah E.", ""], ["Stuart", "Elisabeth A.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1905.10705", "submitter": "Guanyang Wang", "authors": "Guanyang Wang, Yumeng Zhang, Yong Deng, Xuxin Huang, {\\L}ukasz\n  Kidzi\\'nski", "title": "Modeling treatment events in disease progression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ability to quantify and predict progression of a disease is fundamental for\nselecting an appropriate treatment. Many clinical metrics cannot be acquired\nfrequently either because of their cost (e.g. MRI, gait analysis) or because\nthey are inconvenient or harmful to a patient (e.g. biopsy, x-ray). In such\nscenarios, in order to estimate individual trajectories of disease progression,\nit is advantageous to leverage similarities between patients, i.e. the\ncovariance of trajectories, and find a latent representation of progression.\nMost of existing methods for estimating trajectories do not account for events\nin-between observations, what dramatically decreases their adequacy for\nclinical practice. In this study, we develop a machine learning framework named\nCoordinatewise-Soft-Impute (CSI) for analyzing disease progression from sparse\nobservations in the presence of confounding events. CSI is guaranteed to\nconverge to the global minimum of the corresponding optimization problem.\nExperimental results also demonstrates the effectiveness of CSI using both\nsimulated and real dataset.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 01:16:36 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Wang", "Guanyang", ""], ["Zhang", "Yumeng", ""], ["Deng", "Yong", ""], ["Huang", "Xuxin", ""], ["Kidzi\u0144ski", "\u0141ukasz", ""]]}, {"id": "1905.10808", "submitter": "Matteo Sordello", "authors": "Matteo Sordello and Dylan S. Small", "title": "A Test for Differential Ascertainment in Case-Control Studies with\n  Application to Child Maltreatment", "comments": "25 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to test for the presence of differential ascertainment in\ncase-control studies, when data are collected by multiple sources. We show\nthat, when differential ascertainment is present, the use of only the observed\ncases leads to severe bias in the computation of the odds ratio. We can\nalleviate the effect of such bias using the estimates that our method of\ntesting for differential ascertainment naturally provides. We apply it to a\ndataset obtained from the National Violent Death Reporting System, with the\ngoal of checking for the presence of differential ascertainment by race in the\ncount of deaths caused by child maltreatment.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 14:41:40 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 15:13:12 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 21:53:17 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Sordello", "Matteo", ""], ["Small", "Dylan S.", ""]]}, {"id": "1905.10888", "submitter": "Huijie Feng", "authors": "Huijie Feng, Yang Ning, Jiwei Zhao", "title": "Nonregular and Minimax Estimation of Individualized Thresholds in High\n  Dimension with Binary Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large number of covariates $Z$, we consider the estimation of a\nhigh-dimensional parameter $\\theta$ in an individualized linear threshold\n$\\theta^T Z$ for a continuous variable $X$, which minimizes the disagreement\nbetween $\\text{sign}(X-\\theta^TZ)$ and a binary response $Y$. While the problem\ncan be formulated into the M-estimation framework, minimizing the corresponding\nempirical risk function is computationally intractable due to discontinuity of\nthe sign function. Moreover, estimating $\\theta$ even in the fixed-dimensional\nsetting is known as a nonregular problem leading to nonstandard asymptotic\ntheory. To tackle the computational and theoretical challenges in the\nestimation of the high-dimensional parameter $\\theta$, we propose an empirical\nrisk minimization approach based on a regularized smoothed loss function. The\nstatistical and computational trade-off of the algorithm is investigated.\nStatistically, we show that the finite sample error bound for estimating\n$\\theta$ in $\\ell_2$ norm is $(s\\log d/n)^{\\beta/(2\\beta+1)}$, where $d$ is the\ndimension of $\\theta$, $s$ is the sparsity level, $n$ is the sample size and\n$\\beta$ is the smoothness of the conditional density of $X$ given the response\n$Y$ and the covariates $Z$. The convergence rate is nonstandard and slower than\nthat in the classical Lasso problems. Furthermore, we prove that the resulting\nestimator is minimax rate optimal up to a logarithmic factor. The Lepski's\nmethod is developed to achieve the adaption to the unknown sparsity $s$ and\nsmoothness $\\beta$. Computationally, an efficient path-following algorithm is\nproposed to compute the solution path. We show that this algorithm achieves\ngeometric rate of convergence for computing the whole path. Finally, we\nevaluate the finite sample performance of the proposed estimator in simulation\nstudies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 21:44:07 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Feng", "Huijie", ""], ["Ning", "Yang", ""], ["Zhao", "Jiwei", ""]]}, {"id": "1905.11232", "submitter": "Deborshee Sen", "authors": "Deborshee Sen, Matthias Sachs, Jianfeng Lu, David Dunson", "title": "Efficient posterior sampling for high-dimensional imbalanced logistic\n  regression", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data are routinely collected in many areas. We are\nparticularly interested in Bayesian classification models in which one or more\nvariables are imbalanced. Current Markov chain Monte Carlo algorithms for\nposterior computation are inefficient as $n$ and/or $p$ increase due to\nworsening time per step and mixing rates. One strategy is to use a\ngradient-based sampler to improve mixing while using data sub-samples to reduce\nper-step computational complexity. However, usual sub-sampling breaks down when\napplied to imbalanced data. Instead, we generalize piece-wise deterministic\nMarkov chain Monte Carlo algorithms to include importance-weighted and\nmini-batch sub-sampling. These approaches maintain the correct stationary\ndistribution with arbitrarily small sub-samples, and substantially outperform\ncurrent competitors. We provide theoretical support and illustrate gains in\nsimulated and real data applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 13:58:23 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 16:54:26 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Sen", "Deborshee", ""], ["Sachs", "Matthias", ""], ["Lu", "Jianfeng", ""], ["Dunson", "David", ""]]}, {"id": "1905.11300", "submitter": "Jaffer Zaidi", "authors": "Jaffer M. Zaidi, Eric J. Tchetgen Tchetgen, and Tyler J. VanderWeele", "title": "Quantifying and Detecting Individual Level `Always Survivor' Causal\n  Effects Under `Truncation by Death' and Censoring Through Time", "comments": "Please email the first author if you want the online supplements. R\n  code is also available on request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The analysis of causal effects when the outcome of interest is possibly\ntruncated by death has a long history in statistics and causal inference. The\nsurvivor average causal effect is commonly identified with more assumptions\nthan those guaranteed by the design of a randomized clinical trial or using\nsensitivity analysis. This paper demonstrates that individual level causal\neffects in the `always survivor' principal stratum can be identified with no\nstronger identification assumptions than randomization. We illustrate the\npractical utility of our methods using data from a clinical trial on patients\nwith prostate cancer. Our methodology is the first and, as of yet, only\nproposed procedure that enables detecting causal effects in the presence of\ntruncation by death using only the assumptions that are guaranteed by design of\nthe clinical trial. This methodology is applicable to all types of outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 15:36:37 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 22:26:45 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 21:38:33 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zaidi", "Jaffer M.", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1905.11386", "submitter": "Yixin Wang", "authors": "Yixin Wang, Jos\\'e R. Zubizarreta", "title": "Large Sample Properties of Matching for Balance", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching methods are widely used for causal inference in observational\nstudies. Among them, nearest neighbor matching is arguably the most popular.\nHowever, nearest neighbor matching does not generally yield an average\ntreatment effect estimator that is $\\sqrt{n}$-consistent (Abadie and Imbens,\n2006). Are matching methods not $\\sqrt{n}$-consistent in general? In this\npaper, we study a recent class of matching methods that use integer programming\nto directly target aggregate covariate balance as opposed to finding close\nneighbor matches. We show that under standard conditions these methods can\nyield simple estimators that are $\\sqrt{n}$-consistent and asymptotically\noptimal provided that the integer program admits a solution.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 21:31:29 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Wang", "Yixin", ""], ["Zubizarreta", "Jos\u00e9 R.", ""]]}, {"id": "1905.11434", "submitter": "Jaffer Zaidi", "authors": "Jaffer M. Zaidi, Tyler J. VanderWeele", "title": "On the identification of individual principal stratum direct, natural\n  direct and pleiotropic effects without cross world independence assumptions", "comments": "Email the first author for the online supplement. Scandinavian\n  Journal of Statistics, 2020", "journal-ref": null, "doi": "10.1002/sjos.12464", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The analysis of natural direct and principal stratum direct effects has a\ncontroversial history in statistics and causal inference as these effects are\ncommonly identified with either untestable cross world independence or\ngraphical assumptions. This paper demonstrates that the presence of individual\nlevel natural direct and principal stratum direct effects can be identified\nwithout cross world independence assumptions. We also define a new type of\ncausal effect, called pleiotropy, that is of interest in genomics, and provide\nempirical conditions to detect such an effect as well. Our results are\napplicable for all types of distributions concerning the mediator and outcome.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 18:13:10 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 20:53:35 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 13:06:45 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zaidi", "Jaffer M.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1905.11436", "submitter": "Maria Jahja", "authors": "Maria Jahja, David C. Farrow, Roni Rosenfeld, Ryan J. Tibshirani", "title": "Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences\n  and Insights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kalman filter (KF) is one of the most widely used tools for data\nassimilation and sequential estimation. In this work, we show that the state\nestimates from the KF in a standard linear dynamical system setting are\nequivalent to those given by the KF in a transformed system, with infinite\nprocess noise (i.e., a \"flat prior\") and an augmented measurement space. This\nreformulation---which we refer to as augmented measurement sensor fusion\n(SF)---is conceptually interesting, because the transformed system here is\nseemingly static (as there is effectively no process model), but we can still\ncapture the state dynamics inherent to the KF by folding the process model into\nthe measurement space. Further, this reformulation of the KF turns out to be\nuseful in settings in which past states are observed eventually (at some lag).\nHere, when the measurement noise covariance is estimated by the empirical\ncovariance, we show that the state predictions from SF are equivalent to those\nfrom a regression of past states on past measurements, subject to particular\nlinear constraints (reflecting the relationships encoded in the measurement\nmap). This allows us to port standard ideas (say, regularization methods) in\nregression over to dynamical systems. For example, we can posit multiple\ncandidate process models, fold all of them into the measurement model,\ntransform to the regression perspective, and apply $\\ell_1$ penalization to\nperform process model selection. We give various empirical demonstrations, and\nfocus on an application to nowcasting the weekly incidence of influenza in the\nUS.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 18:19:40 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 13:34:52 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jahja", "Maria", ""], ["Farrow", "David C.", ""], ["Rosenfeld", "Roni", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1905.11465", "submitter": "Jinjin Tian", "authors": "Jinjin Tian, Aaditya Ramdas", "title": "ADDIS: an adaptive discarding algorithm for online FDR control with\n  conservative nulls", "comments": "Accepted to Neurips 2019. Corrected some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major internet companies routinely perform tens of thousands of A/B tests\neach year. Such large-scale sequential experimentation has resulted in a recent\nspurt of new algorithms that can provably control the false discovery rate\n(FDR) in a fully online fashion. However, current state-of-the-art adaptive\nalgorithms can suffer from a significant loss in power if null p-values are\nconservative (stochastically larger than the uniform distribution), a situation\nthat occurs frequently in practice. In this work, we introduce a new adaptive\ndiscarding method called ADDIS that provably controls the FDR and achieves the\nbest of both worlds: it enjoys appreciable power increase over all existing\nmethods if nulls are conservative (the practical case), and rarely loses power\nif nulls are exactly uniformly distributed (the ideal case). We provide several\npractical insights on robust choices of tuning parameters, and extend the idea\nto asynchronous and offline settings as well.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 19:27:01 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 03:59:58 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 05:12:30 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Tian", "Jinjin", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1905.11496", "submitter": "Daniel Gilbert", "authors": "Daniel E. Gilbert and Martin T. Wells", "title": "Tuning Free Rank-Sparse Bayesian Matrix and Tensor Completion with\n  Global-Local Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix and tensor completion are frameworks for a wide range of problems,\nincluding collaborative filtering, missing data, and image reconstruction.\nMissing entries are estimated by leveraging an assumption that the matrix or\ntensor is low-rank. Most existing Bayesian techniques encourage rank-sparsity\nby modelling factorized matrices and tensors with Normal-Gamma priors. However,\nthe Horseshoe prior and other \"global-local\" formulations provide\ntuning-parameter-free solutions which may better achieve simultaneous\nrank-sparsity and missing-value recovery. We find these global-local priors\noutperform commonly used alternatives in simulations and in a collaborative\nfiltering task predicting board game ratings.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 20:41:33 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Gilbert", "Daniel E.", ""], ["Wells", "Martin T.", ""]]}, {"id": "1905.11497", "submitter": "Nathaniel Corder", "authors": "Nathan Corder and Shu Yang", "title": "Estimating Average Treatment Effects Utilizing Fractional Imputation\n  when Confounders are Subject to Missingness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of missingness in observational data is ubiquitous. When the\nconfounders are missing at random, multiple imputation is commonly used;\nhowever, the method requires congeniality conditions for valid inferences,\nwhich may not be satisfied when estimating average causal treatment effects.\nAlternatively, fractional imputation, proposed by Kim 2011, has been\nimplemented to handling missing values in regression context. In this article,\nwe develop fractional imputation methods for estimating the average treatment\neffects with confounders missing at random. We show that the fractional\nimputation estimator of the average treatment effect is asymptotically normal,\nwhich permits a consistent variance estimate. Via simulation study, we compare\nfractional imputation's accuracy and precision with that of multiple\nimputation.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 20:41:34 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 20:05:40 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Corder", "Nathan", ""], ["Yang", "Shu", ""]]}, {"id": "1905.11502", "submitter": "Lourens  Waldorp", "authors": "Lourens Waldorp and Maarten Marsman", "title": "Intervention in undirected Ising graphs and the partition function", "comments": "Preprint for original paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Undirected graphical models have many applications in such areas as machine\nlearning, image processing, and, recently, psychology. Psychopathology in\nparticular has received a lot of attention, where symptoms of disorders are\nassumed to influence each other. One of the most relevant questions practically\nis on which symptom (node) to intervene to have the most impact. Interventions\nin undirected graphical models is equal to conditioning, and so we have\navailable the machinery with the Ising model to determine the best strategy to\nintervene. In order to perform such calculations the partition function is\nrequired, which is computationally difficult. Here we use a Curie-Weiss\napproach to approximate the partition function in applications of\ninterventions. We show that when the connection weights in the graph are equal\nwithin each clique then we obtain exactly the correct partition function. And\nif the weights vary according to a sub-Gaussian distribution, then the\napproximation is exponentially close to the correct one. We confirm these\nresults with simulations.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 20:54:36 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Waldorp", "Lourens", ""], ["Marsman", "Maarten", ""]]}, {"id": "1905.11505", "submitter": "Niccol\\`o Dalmasso", "authors": "Niccol\\`o Dalmasso, Ann B. Lee, Rafael Izbicki, Taylor Pospisil, Ilmun\n  Kim, Chieh-An Lin", "title": "Validation of Approximate Likelihood and Emulator Models for\n  Computationally Intensive Simulations", "comments": "22 pages, 9 Figures, 2 Tables", "journal-ref": "Proceedings of the Twenty Third International Conference on\n  Artificial Intelligence and Statistics, PMLR 108, 3349-3361, 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex phenomena in engineering and the sciences are often modeled with\ncomputationally intensive feed-forward simulations for which a tractable\nanalytic likelihood does not exist. In these cases, it is sometimes necessary\nto estimate an approximate likelihood or fit a fast emulator model for\nefficient statistical inference; such surrogate models include Gaussian\nsynthetic likelihoods and more recently neural density estimators such as\nautoregressive models and normalizing flows. To date, however, there is no\nconsistent way of quantifying the quality of such a fit. Here we propose a\nstatistical framework that can distinguish any arbitrary misspecified model\nfrom the target likelihood, and that in addition can identify with statistical\nconfidence the regions of parameter as well as feature space where the fit is\ninadequate. Our validation method applies to settings where simulations are\nextremely costly and generated in batches or \"ensembles\" at fixed locations in\nparameter space. At the heart of our approach is a two-sample test that\nquantifies the quality of the fit at fixed parameter values, and a global test\nthat assesses goodness-of-fit across simulation parameters. While our general\nframework can incorporate any test statistic or distance metric, we\nspecifically argue for a new two-sample test that can leverage any regression\nmethod to attain high power and provide diagnostics in complex data settings.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 21:06:51 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 18:29:06 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Dalmasso", "Niccol\u00f2", ""], ["Lee", "Ann B.", ""], ["Izbicki", "Rafael", ""], ["Pospisil", "Taylor", ""], ["Kim", "Ilmun", ""], ["Lin", "Chieh-An", ""]]}, {"id": "1905.11622", "submitter": "Chen Lu", "authors": "Chen Lu, Xinkun Nie, Stefan Wager", "title": "Robust Nonparametric Difference-in-Differences Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of treatment effect estimation in\ndifference-in-differences designs where parallel trends hold only after\nconditioning on covariates. Existing methods for this problem rely on strong\nadditional assumptions, e.g., that any covariates may only have linear effects\non the outcome of interest, or that there is no covariate shift between\ndifferent cross sections taken in the same state. Here, we develop a suite of\nnew methods for nonparametric difference-in-differences estimation that require\nessentially no assumptions beyond conditional parallel trends and a relevant\nform of overlap. Our proposals show promising empirical performance across a\nvariety of simulation setups, and are more robust than the standard methods\nbased on either linear regression or propensity weighting.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 05:56:30 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Lu", "Chen", ""], ["Nie", "Xinkun", ""], ["Wager", "Stefan", ""]]}, {"id": "1905.11676", "submitter": "Xiaolei Xun", "authors": "Xiaolei Xun, Jiguo Cao", "title": "Sparse Estimation of Historical Functional Linear Models with a Nested\n  Group Bridge Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional historical functional linear model relates the current value\nof the functional response at time t to all past values of the functional\ncovariate up to time t. Motivated by situations where it is more reasonable to\nassume that only recent, instead of all, past values of the functional\ncovariate have an impact on the functional response, we investigate in this\nwork the historical functional linear model with an unknown forward time lag\ninto the history. Besides the common goal of estimating the bivariate\nregression coefficient function, we also aim to identify the historical time\nlag from the data, which is important in many applications. Tailored for this\npurpose, we propose an estimation procedure adopting the finite element method\nto conform naturally to the trapezoidal domain of the bivariate coefficient\nfunction. A nested group bridge penalty is developed to provide simultaneous\nestimation of the bivariate coefficient function and the historical lag. The\nmethod is demonstrated in a real data example investigating the effect of\nmuscle activation recorded via the noninvasive electromyography (EMG) method on\nlip acceleration during speech production. The finite sample performance of our\nproposed method is examined via simulation studies in comparison with the\nconventional method.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 08:30:59 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Xun", "Xiaolei", ""], ["Cao", "Jiguo", ""]]}, {"id": "1905.11875", "submitter": "Harlan Campbell", "authors": "Harlan Campbell and Dani\\\"el Lakens", "title": "Can we disregard the whole model? Omnibus non-inferiority testing for\n  $R^{2}$ in multivariable linear regression and $\\hat{\\eta}^{2}$ in ANOVA", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining a lack of association between an outcome variable and a number of\ndifferent explanatory variables is frequently necessary in order to disregard a\nproposed model (i.e., to confirm the lack of an association between an outcome\nand predictors). Despite this, the literature rarely offers information about,\nor technical recommendations concerning, the appropriate statistical\nmethodology to be used to accomplish this task. This paper introduces\nnon-inferiority tests for ANOVA and linear regression analyses, that correspond\nto the standard widely used $F$-test for $\\hat{\\eta}^2$ and $R^{2}$,\nrespectively. A simulation study is conducted to examine the type I error rates\nand statistical power of the tests, and a comparison is made with an\nalternative Bayesian testing approach. The results indicate that the proposed\nnon-inferiority test is a potentially useful tool for 'testing the null.'\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 15:16:36 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 16:15:05 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Campbell", "Harlan", ""], ["Lakens", "Dani\u00ebl", ""]]}, {"id": "1905.11916", "submitter": "Ben Bales", "authors": "Ben Bales, Arya Pourzanjani, Aki Vehtari, Linda Petzold", "title": "Selecting the Metric in Hamiltonian Monte Carlo", "comments": "Data/code available at https://github.com/bbbales2/cmdstan-warmup", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a selection criterion for the Euclidean metric adapted during\nwarmup in a Hamiltonian Monte Carlo sampler that makes it possible for a\nsampler to automatically pick the metric based on the model and the\navailability of warmup draws. Additionally, we present a new adaptation\ninspired by the selection criterion that requires significantly fewer warmup\ndraws to be effective. The effectiveness of the selection criterion and\nadaptation are demonstrated on a number of applied problems. An implementation\nfor the Stan probabilistic programming language is provided.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:25:32 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 16:12:17 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 14:16:44 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Bales", "Ben", ""], ["Pourzanjani", "Arya", ""], ["Vehtari", "Aki", ""], ["Petzold", "Linda", ""]]}, {"id": "1905.11937", "submitter": "Maxime Vono", "authors": "Maxime Vono and Daniel Paulin and Arnaud Doucet", "title": "Efficient MCMC Sampling with Dimension-Free Convergence Rate using\n  ADMM-type Splitting", "comments": "68 pages. Revision of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing exact Bayesian inference for complex models is computationally\nintractable. Markov chain Monte Carlo (MCMC) algorithms can provide reliable\napproximations of the posterior distribution but are expensive for large\ndatasets and high-dimensional models. A standard approach to mitigate this\ncomplexity consists in using subsampling techniques or distributing the data\nacross a cluster. However, these approaches are typically unreliable in\nhigh-dimensional scenarios. We focus here on a recent alternative class of MCMC\nschemes exploiting a splitting strategy akin to the one used by the celebrated\nADMM optimization algorithm. These methods appear to provide empirically\nstate-of-the-art performance but their theoretical behavior in high dimension\nis currently unknown. In this paper, we propose a detailed theoretical study of\none of these algorithms known as the split Gibbs sampler. Under regularity\nconditions, we establish explicit convergence rates for this scheme using Ricci\ncurvature and coupling ideas. We support our theory with numerical\nillustrations.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 08:42:33 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 16:25:23 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 12:50:25 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 15:21:28 GMT"}, {"version": "v5", "created": "Mon, 19 Oct 2020 16:02:51 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Vono", "Maxime", ""], ["Paulin", "Daniel", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1905.12141", "submitter": "Jingyu He", "authors": "Jingyu He, Nicholas G. Polson, Jianeng Xu", "title": "Bayesian Inference for Gamma Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the theory of normal variance-mean mixtures to derive a data\naugmentation scheme for models that include gamma functions. Our methodology\napplies to many situations in statistics and machine learning, including\nMultinomial-Dirichlet distributions, Negative binomial regression,\nPoisson-Gamma hierarchical models, Extreme value models, to name but a few. All\nof those models include a gamma function which does not admit a natural\nconjugate prior distribution providing a significant challenge to inference and\nprediction. To provide a data augmentation strategy, we construct and develop\nthe theory of the class of Exponential Reciprocal Gamma distributions. This\nallows scalable EM and MCMC algorithms to be developed. We illustrate our\nmethodology on a number of examples, including gamma shape inference, negative\nbinomial regression and Dirichlet allocation. Finally, we conclude with\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:15:56 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 14:44:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["He", "Jingyu", ""], ["Polson", "Nicholas G.", ""], ["Xu", "Jianeng", ""]]}, {"id": "1905.12146", "submitter": "Xiang Ji", "authors": "Xiang Ji, Zhenyu Zhang, Andrew Holbrook, Akihiko Nishimura, Guy Baele,\n  Andrew Rambaut, Philippe Lemey and Marc A. Suchard", "title": "Gradients do grow on trees: a linear-time ${\\cal O}\\hspace{-0.2em}\\left(\n  N \\right)$-dimensional gradient for statistical phylogenetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculation of the log-likelihood stands as the computational bottleneck for\nmany statistical phylogenetic algorithms. Even worse is its gradient\nevaluation, often used to target regions of high probability. Order ${\\cal\nO}\\hspace{-0.2em}\\left( N \\right)$-dimensional gradient calculations based on\nthe standard pruning algorithm require ${\\cal O}\\hspace{-0.2em}\\left( N^2\n\\right)$ operations where N is the number of sampled molecular sequences. With\nthe advent of high-throughput sequencing, recent phylogenetic studies have\nanalyzed hundreds to thousands of sequences, with an apparent trend towards\neven larger data sets as a result of advancing technology. Such large-scale\nanalyses challenge phylogenetic reconstruction by requiring inference on larger\nsets of process parameters to model the increasing data heterogeneity. To make\nthis tractable, we present a linear-time algorithm for ${\\cal\nO}\\hspace{-0.2em}\\left( N \\right)$-dimensional gradient evaluation and apply it\nto general continuous-time Markov processes of sequence substitution on a\nphylogenetic tree without a need to assume either stationarity or\nreversibility. We apply this approach to learn the branch-specific evolutionary\nrates of three pathogenic viruses: West Nile virus, Dengue virus and Lassa\nvirus. Our proposed algorithm significantly improves inference efficiency with\na 126- to 234-fold increase in maximum-likelihood optimization and a 16- to\n33-fold computational performance increase in a Bayesian framework.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:33:42 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ji", "Xiang", ""], ["Zhang", "Zhenyu", ""], ["Holbrook", "Andrew", ""], ["Nishimura", "Akihiko", ""], ["Baele", "Guy", ""], ["Rambaut", "Andrew", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1905.12150", "submitter": "Sreelekha Guggilam", "authors": "Sreelekha Guggilam and S. M. Arshad Zaidi and Varun Chandola and Abani\n  Patra", "title": "Bayesian Anomaly Detection Using Extreme Value Theory", "comments": "7 pages, 7 figures, The paper has been withdrawn due to major\n  modification in the automation model", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven anomaly detection methods typically build a model for the normal\nbehavior of the target system, and score each data instance with respect to\nthis model. A threshold is invariably needed to identify data instances with\nhigh (or low) scores as anomalies. This presents a practical limitation on the\napplicability of such methods, since most methods are sensitive to the choice\nof the threshold, and it is challenging to set optimal thresholds. We present a\nprobabilistic framework to explicitly model the normal and anomalous behaviors\nand probabilistically reason about the data. An extreme value theory based\nformulation is proposed to model the anomalous behavior as the extremes of the\nnormal behavior. As a specific instantiation, a joint non-parametric clustering\nand anomaly detection algorithm is proposed that models the normal behavior as\na Dirichlet Process Mixture Model.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:51:03 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 21:21:43 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Guggilam", "Sreelekha", ""], ["Zaidi", "S. M. Arshad", ""], ["Chandola", "Varun", ""], ["Patra", "Abani", ""]]}, {"id": "1905.12269", "submitter": "Shaoxiong Hu", "authors": "Shaoxiong Hu, Hugo Maruri-Aguliar, Zixiang Ma", "title": "Topological Techniques in Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LASSO is an attractive regularisation method for linear regression that\ncombines variable selection with an efficient computation procedure. This paper\nis concerned with enhancing the performance of LASSO for square-free\nhierarchical polynomial models when combining validation error with a measure\nof model complexity. The measure of the complexity is the sum of Betti numbers\nof the model which is seen as a simplicial complex, and we describe the model\nin terms of components and cycles, borrowing from recent developments in\ncomputational topology. We study and propose an algorithm which combines\nstatistical and topological criteria. This compound criterion would allow us to\ndeal with model selection problems in polynomial regression models containing\nhigher-order interactions. Simulation results demonstrate that the compound\ncriteria produce sparser models with lower prediction errors than the\nestimators of several other statistical methods for higher order interaction\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 08:26:25 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Hu", "Shaoxiong", ""], ["Maruri-Aguliar", "Hugo", ""], ["Ma", "Zixiang", ""]]}, {"id": "1905.12275", "submitter": "Kaoru Irie", "authors": "Kaoru Irie", "title": "Bayesian Dynamic Fused LASSO", "comments": "42 pages, 2 table, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new class of Markov processes is proposed to realize the flexible\nshrinkage effects for the dynamic models. The transition density of the new\nprocess consists of two penalty functions, similarly to Bayesian fused LASSO in\nits functional form, that shrink the current state variable to its previous\nvalue and zero. The normalizing constant of the density, which is not ignorable\nin the posterior computation, is shown to be essentially the log-geometric\nmixture of double-exponential densities. This process comprises the state\nequation of the dynamic regression models, which is shown to be conditionally\nGaussian and linear in state variables and utilize the forward filtering and\nbackward sampling in posterior computation by Gibbs sampler. The problem of\novershrinkage that is inherent in lasso is moderated by considering the\nhierarchical extension, which can even realize the shrinkage of horseshoe\npriors marginally. The new prior is compared with the standard\ndouble-exponential prior in the estimation of and prediction by the dynamic\nlinear models for illustration. It is also applied to the time-varying vector\nautoregressive models for the US macroeconomic data, where we examine the\n(dis)similarity of the additional shrinkage effect to dynamic variable\nselection or, specifically, the latent threshold models.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 08:48:39 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 10:23:17 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Irie", "Kaoru", ""]]}, {"id": "1905.12293", "submitter": "Rom\\'an Salmer\\'on", "authors": "Rom\\'an Salmer\\'on G\\'omez, Catalina Garc\\'ia Garc\\'ia y Jos\\'e\n  Garc\\'ia P\\'erez", "title": "Centered and non-centered variance inflation factor", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the diagnostic of near multicollinearity in a multiple\nlinear regression from auxiliary centered regressions (with intercept) and\nnon-centered (without intercept). From these auxiliary regression, the centered\nand non-centered Variance Inflation Factors are calculated, respectively. It is\nalso presented an expression that relate both of them.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 09:49:00 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["G\u00f3mez", "Rom\u00e1n Salmer\u00f3n", ""], ["P\u00e9rez", "Catalina Garc\u00eda Garc\u00eda y Jos\u00e9 Garc\u00eda", ""]]}, {"id": "1905.12440", "submitter": "Minsuk Shin", "authors": "Minsuk Shin, Young Lee, and Jun S. Liu", "title": "Generative Parameter Sampler For Scalable Uncertainty Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification has been a core of the statistical machine\nlearning, but its computational bottleneck has been a serious challenge for\nboth Bayesians and frequentists. We propose a model-based framework in\nquantifying uncertainty, called predictive-matching Generative Parameter\nSampler (GPS). This procedure considers an Uncertainty Quantification (UQ)\ndistribution on the targeted parameter, which matches the corresponding\npredictive distribution to the observed data. This framework adopts a\nhierarchical modeling perspective such that each observation is modeled by an\nindividual parameter. This individual parameterization permits the resulting\ninference to be computationally scalable and robust to outliers. Our approach\nis illustrated for linear models, Poisson processes, and deep neural networks\nfor classification. The results show that the GPS is successful in providing\nuncertainty quantification as well as additional flexibility beyond what is\nallowed by classical statistical procedures under the postulated statistical\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 06:52:51 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 20:15:22 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Shin", "Minsuk", ""], ["Lee", "Young", ""], ["Liu", "Jun S.", ""]]}, {"id": "1905.12684", "submitter": "Geoffrey Peterson", "authors": "Geoffrey Colin Lee Peterson and Joseph Guinness and Adam Terando and\n  Brian J. Reich", "title": "Mean-dependent nonstationary spatial models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonstationarity is a major challenge in analyzing spatial data. For example,\ndaily precipitation measurements may have increased variability and decreased\nspatial smoothness in areas with high mean rainfall. Common nonstationary\ncovariance models introduce parameters specific to each location, giving a\nhighly-parameterized model which is difficult to fit. We develop a\nnonstationary spatial model that uses the mean to determine the covariance in a\nregion, resulting in a far simpler, albeit more specialized, model. We explore\ninferential and predictive properties of the model under various simulated data\nsituations. We show that this model in certain circumstances improves\npredictions compared to a standard stationary spatial model. We further propose\na computationally efficient approximation that has comparable predictive\naccuracy. We also develop a test for nonstationary data and show it reliably\nidentifies nonstationarity. We apply these methods to daily precipitation in\nPuerto Rico.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 19:16:44 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Peterson", "Geoffrey Colin Lee", ""], ["Guinness", "Joseph", ""], ["Terando", "Adam", ""], ["Reich", "Brian J.", ""]]}, {"id": "1905.12696", "submitter": "Xin Bing", "authors": "Xin Bing and Florentina Bunea and Marten Wegkamp", "title": "Inference in latent factor regression with clusterable features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models, in which the observed features $X \\in \\R^p$ and the\nresponse $Y \\in \\R$ depend, jointly, on a lower dimensional, unobserved, latent\nvector $Z \\in \\R^K$, with $K< p$, are popular in a large array of applications,\nand mainly used for predicting a response from correlated features. In\ncontrast, methodology and theory for inference on the regression coefficient\n$\\beta$ relating $Y$ to $Z$ are scarce, since typically the un-observable\nfactor $Z$ is hard to interpret. Furthermore, the determination of the\nasymptotic variance of an estimator of $\\beta$ is a long-standing problem, with\nsolutions known only in a few particular cases. To address some of these\noutstanding questions, we develop inferential tools for $\\beta$ in a class of\nfactor regression models in which the observed features are signed mixtures of\nthe latent factors. The model specifications are practically desirable, in a\nlarge array of applications, render interpretability to the components of $Z$,\nand are sufficient for parameter identifiability. Without assuming that the\nnumber of latent factors $K$ or the structure of the mixture is known in\nadvance, we construct computationally efficient estimators of $\\beta$, along\nwith estimators of other important model parameters. We benchmark the rate of\nconvergence of $\\beta$ by first establishing its $\\ell_2$-norm minimax lower\nbound, and show that our proposed estimator is minimax-rate adaptive. Our main\ncontribution is the provision of a unified analysis of the component-wise\nGaussian asymptotic distribution of $\\wh \\beta$ and, especially, the derivation\nof a closed form expression of its asymptotic variance, together with\nconsistent variance estimators. The resulting inferential tools can be used\nwhen both $K$ and $p$ are independent of the sample size $n$, and when both, or\neither, $p$ and $K$ vary with $n$, while allowing for $p > n$.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 19:37:05 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 03:13:16 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 22:44:32 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Wegkamp", "Marten", ""]]}, {"id": "1905.12707", "submitter": "Giorgio Gnecco", "authors": "Falco J. Bargagli-Stoffi and Kristof De-Witte and Giorgio Gnecco", "title": "Heterogeneous causal effects with imperfect compliance: a novel Bayesian\n  machine learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an innovative Bayesian machine learning algorithm to\ndraw interpretable inference on heterogeneous causal effects in the presence of\nimperfect compliance (e.g., under an irregular assignment mechanism). We show,\nthrough Monte Carlo simulations, that the proposed Bayesian Causal Forest with\nInstrumental Variable (BCF-IV) methodology outperforms other machine learning\ntechniques tailored for causal inference in discovering and estimating the\nheterogeneous causal effects. BCF-IV sheds a light on the heterogeneity of\ncausal effects in instrumental variable scenarios and, in turn, provides the\npolicy-makers with a relevant tool for targeted policies. Its empirical\napplication evaluates the effects of additional funding on students'\nperformances. The results indicate that BCF-IV could be used to enhance the\neffectiveness of school funding on students' performance. Code is available at\nhttps://github.com/fbargaglistoffi/BCF-IV.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 20:25:34 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 09:04:36 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 22:42:39 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bargagli-Stoffi", "Falco J.", ""], ["De-Witte", "Kristof", ""], ["Gnecco", "Giorgio", ""]]}, {"id": "1905.12768", "submitter": "Jeremy Roth", "authors": "Jeremy Roth and Noah Simon", "title": "Using Propensity Scores to Develop and Evaluate Treatment Rules with\n  Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we outline a principled approach to estimate an individualized\ntreatment rule that is appropriate for data from observational studies where,\nin addition to treatment assignment not being independent of individual\ncharacteristics, some characteristics may affect treatment assignment in the\ncurrent study but not be available in future clinical settings where the\nestimated rule would be applied. The estimation framework is quite flexible and\naccommodates any prediction method that uses observation weights, where the\nobservation weights themselves are a ratio of two flexibly estimated propensity\nscores. We also discuss how to obtain a trustworthy estimate of the rule's\npopulation benefit based on simple propensity-score-based estimators of average\ntreatment effect. We implement our approach in the R package DevTreatRules and\nshare the code needed to reproduce our results on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 23:02:14 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 20:20:13 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Roth", "Jeremy", ""], ["Simon", "Noah", ""]]}, {"id": "1905.12793", "submitter": "Yixin Wang", "authors": "Yixin Wang, David M. Blei", "title": "Multiple Causes: A Causal Graphical View", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unobserved confounding is a major hurdle for causal inference from\nobservational data. Confounders---the variables that affect both the causes and\nthe outcome---induce spurious non-causal correlations between the two. Wang &\nBlei (2018) lower this hurdle with \"the blessings of multiple causes,\" where\nthe correlation structure of multiple causes provides indirect evidence for\nunobserved confounding. They leverage these blessings with an algorithm, called\nthe deconfounder, that uses probabilistic factor models to correct for the\nconfounders. In this paper, we take a causal graphical view of the\ndeconfounder. In a graph that encodes shared confounding, we show how the\nmultiplicity of causes can help identify intervention distributions. We then\njustify the deconfounder, showing that it makes valid inferences of the\nintervention. Finally, we expand the class of graphs, and its theory, to those\nthat include other confounders and selection variables. Our results expand the\ntheory in Wang & Blei (2018), justify the deconfounder for causal graphs, and\nextend the settings where it can be used.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 00:03:30 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Wang", "Yixin", ""], ["Blei", "David M.", ""]]}, {"id": "1905.12852", "submitter": "Ishfaq Shah Syed", "authors": "Anwar Hassan, Ishfaq Shah Ahmad, Peer Bilal Ahmad", "title": "A New Mixed Generalized Negative Binomial Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a generalized version of Negative binomial-beta exponential\ndistribution with five parameters have been introduced. Some interesting\nsubmodels have been derived from it. A comprehensive mathematical treatment of\nproposed distribution is being provided. Various expressions like that of\nmoment generating function, moments are derived. The model parameters are\nestimated by the maximum likelihood method. Finally, the application of\nproposed distribution is carried out on one sample of automobile insurance\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 04:45:29 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Hassan", "Anwar", ""], ["Ahmad", "Ishfaq Shah", ""], ["Ahmad", "Peer Bilal", ""]]}, {"id": "1905.13251", "submitter": "Tianyi Yao", "authors": "Tianyi Yao and Genevera I. Allen", "title": "Clustered Gaussian Graphical Model via Symmetric Convex Clustering", "comments": "To appear in IEEE DSW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge of functional groupings of neurons can shed light on structures of\nneural circuits and is valuable in many types of neuroimaging studies. However,\naccurately determining which neurons carry out similar neurological tasks via\ncontrolled experiments is both labor-intensive and prohibitively expensive on a\nlarge scale. Thus, it is of great interest to cluster neurons that have similar\nconnectivity profiles into functionally coherent groups in a data-driven\nmanner. In this work, we propose the clustered Gaussian graphical model (GGM)\nand a novel symmetric convex clustering penalty in an unified convex\noptimization framework for inferring functional clusters among neurons from\nneural activity data. A parallelizable multi-block Alternating Direction Method\nof Multipliers (ADMM) algorithm is used to solve the corresponding convex\noptimization problem. In addition, we establish convergence guarantees for the\nproposed ADMM algorithm. Experimental results on both synthetic data and\nreal-world neuroscientific data demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 18:15:01 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Yao", "Tianyi", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1905.13414", "submitter": "George Shan", "authors": "George Shan, Mark J. van der Laan", "title": "Targeted Estimation of L2 Distance Between Densities and its Application\n  to Geo-spatial Data", "comments": "17 pages, 3 figures, 2 appendices included", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the integrated squared difference, also known as the L2 distance\n(L2D), between two probability densities. Such a distance metric allows for\ncomparison of differences between pairs of distributions or changes in a\ndistribution over time. We propose a targeted maximum likelihood estimator for\nthis parameter based on samples of independent and identically distributed\nobservations from both underlying distributions. We compare our method to\nkernel density estimation and demonstrate superior performance for our method\nwith regards to confidence interval coverage rate and mean squared error.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 05:03:53 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Shan", "George", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1905.13494", "submitter": "Judith Ter Schure", "authors": "Judith ter Schure and Peter D. Gr\\\"unwald", "title": "Accumulation Bias in Meta-Analysis: The Need to Consider Time in Error\n  Control", "comments": "Soon to be published at F1000 Research", "journal-ref": null, "doi": "10.12688/f1000research.19375.1", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies accumulate over time and meta-analyses are mainly retrospective.\nThese two characteristics introduce dependencies between the analysis time, at\nwhich a series of studies is up for meta-analysis, and results within the\nseries. Dependencies introduce bias --- Accumulation Bias --- and invalidate\nthe sampling distribution assumed for p-value tests, thus inflating type-I\nerrors. But dependencies are also inevitable, since for science to accumulate\nefficiently, new research needs to be informed by past results. Here, we\ninvestigate various ways in which time influences error control in\nmeta-analysis testing. We introduce an Accumulation Bias Framework that allows\nus to model a wide variety of practically occurring dependencies, including\nstudy series accumulation, meta-analysis timing, and approaches to multiple\ntesting in living systematic reviews. The strength of this framework is that it\nshows how all dependencies affect p-value-based tests in a similar manner. This\nleads to two main conclusions. First, Accumulation Bias is inevitable, and even\nif it can be approximated and accounted for, no valid p-value tests can be\nconstructed. Second, tests based on likelihood ratios withstand Accumulation\nBias: they provide bounds on error probabilities that remain valid despite the\nbias. We leave the reader with a choice between two proposals to consider time\nin error control: either treat individual (primary) studies and meta-analyses\nas two separate worlds --- each with their own timing --- or integrate\nindividual studies in the meta-analysis world. Taking up likelihood ratios in\neither approach allows for valid tests that relate well to the accumulating\nnature of scientific knowledge. Likelihood ratios can be interpreted as betting\nprofits, earned in previous studies and invested in new ones, while the\nmeta-analyst is allowed to cash out at any time and advise against future\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 10:12:01 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["ter Schure", "Judith", ""], ["Gr\u00fcnwald", "Peter D.", ""]]}, {"id": "1905.13599", "submitter": "Christian P. Robert", "authors": "Gr\\'egoire Clart\\'e, Christian P. Robert, Robin Ryder, and Julien\n  Stoehr (Universit\\'e Paris-Dauphine, CEREMADE, CNRS)", "title": "Component-wise approximate Bayesian computation via Gibbs-like steps", "comments": "28 pages, 13 figures, third revision (accepted for publication in\n  Biometrika on 17 September, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation methods are useful for generative models\nwith intractable likelihoods. These methods are however sensitive to the\ndimension of the parameter space, requiring exponentially increasing resources\nas this dimension grows. To tackle this difficulty, we explore a Gibbs version\nof the ABC approach that runs component-wise approximate Bayesian computation\nsteps aimed at the corresponding conditional posterior distributions, and based\non summary statistics of reduced dimensions. While lacking the standard\njustifications for the Gibbs sampler, the resulting Markov chain is shown to\nconverge in distribution under some partial independence conditions. The\nassociated stationary distribution can further be shown to be close to the true\nposterior distribution and some hierarchical versions of the proposed mechanism\nenjoy a closed form limiting distribution. Experiments also demonstrate the\ngain in efficiency brought by the Gibbs version over the standard solution.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:11:43 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 17:15:44 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 16:36:20 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 11:24:34 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 15:50:04 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Clart\u00e9", "Gr\u00e9goire", "", "Universit\u00e9 Paris-Dauphine, CEREMADE, CNRS"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, CEREMADE, CNRS"], ["Ryder", "Robin", "", "Universit\u00e9 Paris-Dauphine, CEREMADE, CNRS"], ["Stoehr", "Julien", "", "Universit\u00e9 Paris-Dauphine, CEREMADE, CNRS"]]}, {"id": "1905.13657", "submitter": "William Stephenson", "authors": "William T. Stephenson and Tamara Broderick", "title": "Approximate Cross-Validation in High Dimensions with Guarantees", "comments": "Accepted to AISTATS 2020. 33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leave-one-out cross-validation (LOOCV) can be particularly accurate among\ncross-validation (CV) variants for machine learning assessment tasks -- e.g.,\nassessing methods' error or variability. But it is expensive to re-fit a model\n$N$ times for a dataset of size $N$. Previous work has shown that\napproximations to LOOCV can be both fast and accurate -- when the unknown\nparameter is of small, fixed dimension. But these approximations incur a\nrunning time roughly cubic in dimension -- and we show that, besides\ncomputational issues, their accuracy dramatically deteriorates in high\ndimensions. Authors have suggested many potential and seemingly intuitive\nsolutions, but these methods have not yet been systematically evaluated or\ncompared. We find that all but one perform so poorly as to be unusable for\napproximating LOOCV. Crucially, though, we are able to show, both empirically\nand theoretically, that one approximation can perform well in high dimensions\n-- in cases where the high-dimensional parameter exhibits sparsity. Under\ninterpretable assumptions, our theory demonstrates that the problem can be\nreduced to working within an empirically recovered (small) support. This\nprocedure is straightforward to implement, and we prove that its running time\nand error depend on the (small) support size even when the full parameter\ndimension is large.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 14:57:01 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 18:25:09 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 19:55:53 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2020 23:00:08 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Stephenson", "William T.", ""], ["Broderick", "Tamara", ""]]}]