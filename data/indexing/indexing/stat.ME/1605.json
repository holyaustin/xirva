[{"id": "1605.00015", "submitter": "Mohamed Chaouch", "authors": "Mohamed Chaouch, Naamane Laib and Elias Ould-Said", "title": "Nonparametric M-estimation for right censored regression model with\n  stationary ergodic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper deals with a nonparametric M-estimation for right censored\nregression model with stationary ergodic data. Defined as an implicit function,\na kernel type estimator of a family of robust regression is considered when the\ncovariate take its values in R^d (d >= 1) and the data are sampled from\nstationary ergodic process. The strong consistency (with rate) and the\nasymptotic distribution of the estimator are established under mild\nassumptions. Moreover, a usable confidence interval is provided which does not\ndepend on any unknown quantity. Our results hold without any mixing condition\nand do not require the existence of marginal densities. A comparison study\nbased on simulated data is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 20:06:47 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Chaouch", "Mohamed", ""], ["Laib", "Naamane", ""], ["Ould-Said", "Elias", ""]]}, {"id": "1605.00104", "submitter": "Michael Chipeta Mr", "authors": "Michael G. Chipeta, Dianne J. Terlouw, Kamija S. Phiri, Peter J.\n  Diggle", "title": "Inhibitory geostatistical designs for spatial prediction taking account\n  of uncertain covariance structure", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of choosing spatial sampling designs for investigating unobserved\nspatial phenomenon S arises in many contexts, for example in identifying\nhouseholds to select for a prevalence survey to study disease burden and\nheterogeneity in a study region D. We studied randomised inhibitory spatial\nsampling designs to address the problem of spatial prediction whilst taking\naccount of the need to estimate covariance structure. Two specific classes of\ndesign are inhibitory designs and inhibitory designs plus close pairs. In an\ninhibitory design, any pair of sample locations must be separated by at least\nan inhibition distance {$\\delta$}. In an inhibitory plus close pairs design, n\n- k sample locations in an inhibitory design with inhibition distance\n{$\\delta$} are augmented by k locations each positioned close to one of the\nrandomly selected n - k locations in the inhibitory design, uniformly\ndistributed within a disc of radius {$\\zeta$}. We present simulation results\nfor the Matern class of covariance structures. When the nugget variance is\nnon-negligible, inhibitory plus close pairs designs demonstrate improved\npredictive efficiency over designs without close pairs. We illustrate how these\nfindings can be applied to the design of a rolling Malaria Indicator Survey\nthat forms part of an ongoing large-scale, five-year malaria transmission\nreduction project in Malawi.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 13:07:15 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Chipeta", "Michael G.", ""], ["Terlouw", "Dianne J.", ""], ["Phiri", "Kamija S.", ""], ["Diggle", "Peter J.", ""]]}, {"id": "1605.00155", "submitter": "Chad Hazlett", "authors": "Chad Hazlett", "title": "Kernel Balancing: A flexible non-parametric weighting procedure for\n  estimating causal effects", "comments": "Work originally included in PhD Thesis, May 2014, MIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of unobserved confounders, matching and weighting methods are\nwidely used to estimate causal quantities including the Average Treatment\nEffect on the Treated (ATT). Unfortunately, these methods do not necessarily\nachieve their goal of making the multivariate distribution of covariates for\nthe control group identical to that of the treated, leaving some (potentially\nmultivariate) functions of the covariates with different means between the two\ngroups. When these \"imbalanced\" functions influence the non-treatment potential\noutcome, the conditioning on observed covariates fails, and ATT estimates may\nbe biased. Kernel balancing, introduced here, targets a weaker requirement for\nunbiased ATT estimation, specifically, that the expected non-treatment\npotential outcome for the treatment and control groups are equal. The\nconditional expectation of the non-treatment potential outcome is assumed to\nfall in the space of functions associated with a choice of kernel, implying a\nset of basis functions in which this regression surface is linear. Weights are\nthen chosen on the control units such that the treated and control group have\nequal means on these basis functions. As a result, the expectation of the\nnon-treatment potential outcome must also be equal for the treated and control\ngroups after weighting, allowing unbiased ATT estimation by subsequent\ndifference in means or an outcome model using these weights. Moreover, the\nweights produced are (1) precisely those that equalize a particular\nkernel-based approximation of the multivariate distribution of covariates for\nthe treated and control, and (2) equivalent to a form of stabilized inverse\npropensity score weighting, though it does not require assuming any model of\nthe treatment assignment mechanism. An R package, KBAL, is provided to\nimplement this approach.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 19:49:20 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Hazlett", "Chad", ""]]}, {"id": "1605.00185", "submitter": "Xinping Cui", "authors": "Zhen Xiao, Nicolas Brunel, Zhenbiao Yang, Xinping Cui", "title": "Constrained Nonlinear and Mixed Effects Differential Equation Models for\n  Dynamic Cell Polarity Signaling", "comments": "34 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key of tip growth in eukaryotes is the polarized distribution on plasma\nmembrane of a particle named ROP1. This distribution is the result of a\npositive feedback loop, whose mechanism can be described by a Differential\nEquation parametrized by two meaningful parameters kpf and knf . We introduce a\nmechanistic Integro-Differential Equation (IDE) derived from a spatiotemporal\nmodel of cell polarity and we show how this model can be fitted to real data,\ni.e., ROP1 intensities measured on pollen tubes. At first, we provide an\nexistence and uniqueness result for the solution of our IDE model under certain\nconditions. Interestingly, this analysis gives a tractable expression for the\nlikelihood, and our approach can be seen as the estimation of a constrained\nnonlinear model. Moreover, we introduce a population variability by a\nconstrained nonlinear mixed model. We then propose a constrained Least Squares\nmethod to fit the model for the single pollen tube case, and two methods,\nconstrained Methods of Moments and constrained Restricted Maximum Likelihood\n(REML) to fit the model for the multiple pollen tubes case. The performances of\nall three methods are studied through simulations and are used on an in-house\nmultiple pollen tubes dataset generated at UC Riverside.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 23:08:40 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Xiao", "Zhen", ""], ["Brunel", "Nicolas", ""], ["Yang", "Zhenbiao", ""], ["Cui", "Xinping", ""]]}, {"id": "1605.00346", "submitter": "Jie Ding", "authors": "Jie Ding, Yu Xiang, Lu Shen, Vahid Tarokh", "title": "Multiple Change Point Analysis: Fast Implementation And Strong\n  Consistency", "comments": "A preliminary version of this work was presented in ICML 2016 Anomaly\n  Detection Workshop", "journal-ref": null, "doi": "10.1109/TSP.2017.2711558", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in identifying structural changes in stochastic\nprocesses is to carry out analysis for time series with dependency structure in\na computationally tractable way. Another challenge is that the number of true\nchange points is usually unknown, requiring a suitable model selection\ncriterion to arrive at informative conclusions. To address the first challenge,\nwe model the data generating process as a segment-wise autoregression, which is\ncomposed of several segments (time epochs), each of which modeled by an\nautoregressive model. We propose a multi-window method that is both effective\nand efficient for discovering the structural changes. The proposed approach was\nmotivated by transforming a segment-wise autoregression into a multivariate\ntime series that is asymptotically segment-wise independent and identically\ndistributed. To address the second challenge, we derive theoretical guarantees\nfor (almost surely) selecting the true number of change points of segment-wise\nindependent multivariate time series. Specifically, under mild assumptions, we\nshow that a Bayesian Information Criterion (BIC)-like criterion gives a\nstrongly consistent selection of the optimal number of change points, while an\nAkaike Information Criterion (AIC)-like criterion cannot. Finally, we\ndemonstrate the theory and strength of the proposed algorithms by experiments\non both synthetic and real-world data, including the Eastern US temperature\ndata and the El Nino data from 1854 to 2015. The experiment leads to some\ninteresting discoveries about temporal variability of the summer-time\ntemperature over the Eastern US, and about the most dominant factor of ocean\ninfluence on climate.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 04:17:49 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 15:17:10 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Ding", "Jie", ""], ["Xiang", "Yu", ""], ["Shen", "Lu", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1605.00353", "submitter": "Anru R. Zhang", "authors": "T. Tony Cai and Anru Zhang", "title": "Rate-Optimal Perturbation Bounds for Singular Subspaces with\n  Applications to High-Dimensional Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Perturbation bounds for singular spaces, in particular Wedin's $\\sin \\Theta$\ntheorem, are a fundamental tool in many fields including high-dimensional\nstatistics, machine learning, and applied mathematics. In this paper, we\nestablish separate perturbation bounds, measured in both spectral and Frobenius\n$\\sin \\Theta$ distances, for the left and right singular subspaces. Lower\nbounds, which show that the individual perturbation bounds are rate-optimal,\nare also given.\n  The new perturbation bounds are applicable to a wide range of problems. In\nthis paper, we consider in detail applications to low-rank matrix denoising and\nsingular space estimation, high-dimensional clustering, and canonical\ncorrelation analysis (CCA). In particular, separate matching upper and lower\nbounds are obtained for estimating the left and right singular spaces. To the\nbest of our knowledge, this is the first result that gives different optimal\nrates for the left and right singular spaces under the same perturbation. In\naddition to these problems, applications to other high-dimensional problems\nsuch as community detection in bipartite networks, multidimensional scaling,\nand cross-covariance matrix estimation are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 04:47:43 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 13:42:47 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 18:11:06 GMT"}, {"version": "v4", "created": "Mon, 5 Jun 2017 05:42:08 GMT"}, {"version": "v5", "created": "Fri, 5 Jun 2020 05:15:03 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1605.00361", "submitter": "Adrien Hardy", "authors": "R\\'emi Bardenet, Adrien Hardy", "title": "Monte Carlo with Determinantal Point Processes", "comments": "58 pages, 6 figures. To appear in Annals of Applied Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that repulsive random variables can yield Monte Carlo methods with\nfaster convergence rates than the typical $N^{-1/2}$, where $N$ is the number\nof integrand evaluations. More precisely, we propose stochastic numerical\nquadratures involving determinantal point processes associated with\nmultivariate orthogonal polynomials, and we obtain root mean square errors that\ndecrease as $N^{-(1+1/d)/2}$, where $d$ is the dimension of the ambient space.\nFirst, we prove a central limit theorem (CLT) for the linear statistics of a\nclass of determinantal point processes, when the reference measure is a product\nmeasure supported on a hypercube, which satisfies the Nevai-class regularity\ncondition, a result which may be of independent interest. Next, we introduce a\nMonte Carlo method based on these determinantal point processes, and prove a\nCLT with explicit limiting variance for the quadrature error, when the\nreference measure satisfies a stronger regularity condition. As a corollary, by\ntaking a specific reference measure and using a construction similar to\nimportance sampling, we obtain a general Monte Carlo method, which applies to\nany measure with continuously derivable density. Loosely speaking, our method\ncan be interpreted as a stochastic counterpart to Gaussian quadrature, which,\nat the price of some convergence rate, is easily generalizable to any dimension\nand has a more explicit error term.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 06:24:06 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 11:20:01 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Hardy", "Adrien", ""]]}, {"id": "1605.00391", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Arthur Gretton, Bernhard Sch\\\"olkopf, Moritz\n  Grosse-Wentrup", "title": "Recovery of non-linear cause-effect relationships from linearly mixed\n  neuroimaging data", "comments": "arXiv admin note: text overlap with arXiv:1512.01255", "journal-ref": "Pattern Recognition in Neuroimaging (PRNI), International Workshop\n  on, 1-4, 2016", "doi": "10.1109/PRNI.2016.7552331", "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference concerns the identification of cause-effect relationships\nbetween variables. However, often only linear combinations of variables\nconstitute meaningful causal variables. For example, recovering the signal of a\ncortical source from electroencephalography requires a well-tuned combination\nof signals recorded at multiple electrodes. We recently introduced the MERLiN\n(Mixture Effect Recovery in Linear Networks) algorithm that is able to recover,\nfrom an observed linear mixture, a causal variable that is a linear effect of\nanother given variable. Here we relax the assumption of this cause-effect\nrelationship being linear and present an extended algorithm that can pick up\nnon-linear cause-effect relationships. Thus, the main contribution is an\nalgorithm (and ready to use code) that has broader applicability and allows for\na richer model class. Furthermore, a comparative analysis indicates that the\nassumption of linear cause-effect relationships is not restrictive in analysing\nelectroencephalographic data.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 08:45:59 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 20:01:00 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Gretton", "Arthur", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1605.00499", "submitter": "Timothy Christensen", "authors": "Xiaohong Chen, Timothy Christensen and Elie Tamer", "title": "Monte Carlo Confidence Sets for Identified Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complicated/nonlinear parametric models, it is generally hard to know\nwhether the model parameters are point identified. We provide computationally\nattractive procedures to construct confidence sets (CSs) for identified sets of\nfull parameters and of subvectors in models defined through a likelihood or a\nvector of moment equalities or inequalities. These CSs are based on level sets\nof optimal sample criterion functions (such as likelihood or optimally-weighted\nor continuously-updated GMM criterions). The level sets are constructed using\ncutoffs that are computed via Monte Carlo (MC) simulations directly from the\nquasi-posterior distributions of the criterions. We establish new Bernstein-von\nMises (or Bayesian Wilks) type theorems for the quasi-posterior distributions\nof the quasi-likelihood ratio (QLR) and profile QLR in partially-identified\nregular models and some non-regular models. These results imply that our MC CSs\nhave exact asymptotic frequentist coverage for identified sets of full\nparameters and of subvectors in partially-identified regular models, and have\nvalid but potentially conservative coverage in models with reduced-form\nparameters on the boundary. Our MC CSs for identified sets of subvectors are\nshown to have exact asymptotic coverage in models with singularities. We also\nprovide results on uniform validity of our CSs over classes of DGPs that\ninclude point and partially identified models. We demonstrate good\nfinite-sample coverage properties of our procedures in two simulation\nexperiments. Finally, our procedures are applied to two non-trivial empirical\nexamples: an airline entry game and a model of trade flows.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 14:18:33 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 23:39:16 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 00:14:19 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chen", "Xiaohong", ""], ["Christensen", "Timothy", ""], ["Tamer", "Elie", ""]]}, {"id": "1605.00507", "submitter": "Ping Li", "authors": "Ping Li and Syama Sundar Rangapuram and Martin Slawski", "title": "Methods for Sparse and Low-Rank Recovery under Simplex Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The de-facto standard approach of promoting sparsity by means of\n$\\ell_1$-regularization becomes ineffective in the presence of simplex\nconstraints, i.e.,~the target is known to have non-negative entries summing up\nto a given constant. The situation is analogous for the use of nuclear norm\nregularization for low-rank recovery of Hermitian positive semidefinite\nmatrices with given trace. In the present paper, we discuss several strategies\nto deal with this situation, from simple to more complex. As a starting point,\nwe consider empirical risk minimization (ERM). It follows from existing theory\nthat ERM enjoys better theoretical properties w.r.t.~prediction and\n$\\ell_2$-estimation error than $\\ell_1$-regularization. In light of this, we\nargue that ERM combined with a subsequent sparsification step like thresholding\nis superior to the heuristic of using $\\ell_1$-regularization after dropping\nthe sum constraint and subsequent normalization.\n  At the next level, we show that any sparsity-promoting regularizer under\nsimplex constraints cannot be convex. A novel sparsity-promoting regularization\nscheme based on the inverse or negative of the squared $\\ell_2$-norm is\nproposed, which avoids shortcomings of various alternative methods from the\nliterature. Our approach naturally extends to Hermitian positive semidefinite\nmatrices with given trace. Numerical studies concerning compressed sensing,\nsparse mixture density estimation, portfolio optimization and quantum state\ntomography are used to illustrate the key points of the paper.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 14:37:59 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Li", "Ping", ""], ["Rangapuram", "Syama Sundar", ""], ["Slawski", "Martin", ""]]}, {"id": "1605.00533", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "Real time change-point detection in a nonlinear quantile model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most studies in real time change-point detection either focus on the linear\nmodel or use the CUSUM method under classical assumptions on model errors. This\npaper considers the sequential change-point detection in a nonlinear quantile\nmodel. A test statistic based on the CUSUM of the quantile process subgradient\nis proposed and studied. Under null hypothesis that the model does not change,\nthe asymptotic distribution of the test statistic is determined. Under\nalternative hypothesis that at some unknown observation there is a change in\nmodel, the proposed test statistic converges in probability to $\\infty$. These\nresults allow to build the critical regions on open-end and on closed-end\nprocedures. Simulation results, using Monte Carlo technique, investigate the\nperformance of the test statistic, specially for heavy-tailed error\ndistributions. We also compare it with the classical CUSUM test statistic.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 15:40:25 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "1605.00660", "submitter": "Reimar Heinrich Leike", "authors": "Reimar H. Leike, Torsten A. En{\\ss}lin", "title": "Operator Calculus for Information Field Theory", "comments": "In this version there is a numerical example which demonstrates that\n  the derived algorithm works. The paper was restructured such that it is\n  better readable", "journal-ref": "PhysRevE. 94.053306 (2016)", "doi": "10.1103/PhysRevE.94.053306", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal inference problems with non-Gaussian posteriors can be hard to tackle.\nThrough using the concept of Gibbs free energy these posteriors are rephrased\nas Gaussian posteriors for the price of computing various expectation values\nwith respect to a Gaussian distribution. We present a new way of translating\nthese expectation values to a language of operators which is similar to that in\nquantum mechanics. This simplifies many calculations, for instance such\ninvolving log-normal priors. The operator calculus is illustrated by deriving a\nnovel self-calibrating algorithm which is tested with mock data.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 20:00:41 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 08:01:51 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Leike", "Reimar H.", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1605.00758", "submitter": "Elizabeth Hou", "authors": "Jes\\'us Arroyo, Elizabeth Hou", "title": "Efficient Distributed Estimation of Inverse Covariance Matrices", "comments": null, "journal-ref": null, "doi": "10.1109/SSP.2016.7551705", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed systems, communication is a major concern due to issues such\nas its vulnerability or efficiency. In this paper, we are interested in\nestimating sparse inverse covariance matrices when samples are distributed into\ndifferent machines. We address communication efficiency by proposing a method\nwhere, in a single round of communication, each machine transfers a small\nsubset of the entries of the inverse covariance matrix. We show that, with this\nefficient distributed method, the error rates can be comparable with estimation\nin a non-distributed setting, and correct model selection is still possible.\nPractical performance is shown through simulations.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 05:51:21 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Arroyo", "Jes\u00fas", ""], ["Hou", "Elizabeth", ""]]}, {"id": "1605.00779", "submitter": "Sipan Aslan", "authors": "Sipan Aslan, Ceylan Yozgatligil, Cem Iyigun", "title": "Temporal Clustering of Time Series via Threshold Autoregressive Models:\n  Application to Commodity Prices", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aimed to find temporal clusters for several commodity prices using\nthe threshold non-linear autoregressive model. It is expected that the process\nof determining the commodity groups that are time-dependent will advance the\ncurrent knowledge about the dynamics of co-moving and coherent prices, and can\nserve as a basis for multivariate time series analyses. The clustering of\ncommodity prices was examined using the proposed clustering approach based on\ntime series models to incorporate the time varying properties of price series\ninto the clustering scheme. Accordingly, the primary aim in this study was\ngrouping time series according to the similarity between their Data Generating\nMechanisms (DGMs) rather than comparing pattern similarities in the time series\ntraces. The approximation to the DGM of each series was accomplished using\nthreshold autoregressive models, which are recognized for their ability to\nrepresent nonlinear features in time series, such as abrupt changes,\ntime-irreversibility and regime-shifting behavior. Through the use of the\nproposed approach, one can determine and monitor the set of co-moving time\nseries variables across the time dimension. Furthermore, generating a time\nvarying commodity price index and sub-indexes can become possible.\nConsequently, we conducted a simulation study to assess the effectiveness of\nthe proposed clustering approach and the results are presented for both the\nsimulated and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 08:13:58 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Aslan", "Sipan", ""], ["Yozgatligil", "Ceylan", ""], ["Iyigun", "Cem", ""]]}, {"id": "1605.01019", "submitter": "Alberto Llera", "authors": "A. Llera and C. F. Beckmann", "title": "Estimating an Inverse Gamma distribution", "comments": "Donders Institute Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce five different algorithms based on method of\nmoments, maximum likelihood and full Bayesian estimation for learning the\nparameters of the Inverse Gamma distribution. We also provide an expression for\nthe KL divergence for Inverse Gamma distributions which allows us to quantify\nthe estimation accuracy of each of the algorithms. All the presented algorithms\nare novel. The most relevant novelties include the first conjugate prior for\nthe Inverse Gamma shape parameter which allows analytical Bayesian inference,\nand two very fast algorithms, a maximum likelihood and a Bayesian one, both\nbased on likelihood approximation. In order to compute expectations under the\nproposed distributions we use the Laplace approximation. The introduction of\nthese novel Bayesian estimators opens the possibility of including Inverse\nGamma distributions into more complex Bayesian structures, e.g. variational\nBayesian mixture models. The algorithms introduced in this paper are\ncomputationally compared using synthetic data and interesting relationships\nbetween the maximum likelihood and the Bayesian approaches are derived.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:46:12 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 23:06:39 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Llera", "A.", ""], ["Beckmann", "C. F.", ""]]}, {"id": "1605.01095", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel", "title": "Regression with missing Ys: An improved strategy for analyzing multiply\n  imputed data", "comments": null, "journal-ref": "Sociological Methodology (2007) volume 37, pp. 83-117", "doi": "10.1111/j.1467-9531.2007.00180.x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fitting a generalized linear model -- such as a linear regression, a\nlogistic regression, or a hierarchical linear model -- analysts often wonder\nhow to handle missing values of the dependent variable Y. If missing values\nhave been filled in using multiple imputation, the usual advice is to use the\nimputed Y values in analysis. We show, however, that using imputed Ys can add\nneedless noise to the estimates. Better estimates can usually be obtained using\na modified strategy that we call multiple imputation, then deletion (MID).\nUnder MID, all cases are used for imputation, but following imputation cases\nwith imputed Y values are excluded from the analysis. When there is something\nwrong with the imputed Y values, MID protects the estimates from the\nproblematic imputations. And when the imputed Y values are acceptable, MID\nusually offers somewhat more efficient estimates than an ordinary MI strategy.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 21:20:50 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["von Hippel", "Paul T.", ""]]}, {"id": "1605.01146", "submitter": "Minkyoung Kang", "authors": "Minkyoung Kang, Brani Vidakovic", "title": "A note on Bayesian wavelet-based estimation of scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of phenomena in various fields such as geology, atmospheric\nsciences, economics, to list a few, can be modeled as a fractional Brownian\nmotion indexed by Hurst exponent $H$. This exponent is related to the degree of\nregularity and self-similarity present in the signal, and it often captures\nimportant characteristics useful in various applications. Given its importance,\na number of methods have been developed for the estimation of the Hurst\nexponent. Typically, the proposed methods do not utilize prior information\nabout scaling of a signal. Some signals are known to possess a theoretical\nvalue of the Hurst exponent, which motivates us to propose a Bayesian approach\nthat incorporates this information via a suitable elicited prior distribution\non $H$.\n  This significantly improves the accuracy of the estimation, as we demonstrate\nby simulations. Moreover, the proposed method is robust to small\nmisspecifications of the prior location. The proposed method is applied to a\nturbulence time series for which Hurst exponent is theoretically known by\nKolmogorov's K41 theory.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 05:01:36 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Kang", "Minkyoung", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1605.01214", "submitter": "Wenyang Zhang", "authors": "Efang Kong, Jialiang Li and Wenyang Zhang (The corresponding author,\n  Department of Mathematics, University of York)", "title": "Factor Models for Asset Returns Based on Transformed Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fama-French three factor models are commonly used in the description of\nasset returns in finance. Statistically speaking, the Fama-French three factor\nmodels imply that the return of an asset can be accounted for directly by the\nFama-French three factors, i.e. market, size and value factor, through a linear\nfunction. A natural question is: would some kind of transformed Fama-French\nthree factors work better than the three factors? If so, what kind of\ntransformation should be imposed on each factor in order to make the\ntransformed three factors better account for asset returns? In this paper, we\nare going to address these questions through nonparametric modelling. We\npropose a data driven approach to construct the transformation for each factor\nconcerned. A generalised maximum likelihood ratio based hypothesis test is also\nproposed to test whether transformations on the Fama-French three factors are\nneeded for a given data set. Asymptotic properties are established to justify\nthe proposed methods. Intensive simulation studies are conducted to show how\nthe proposed methods work when sample size is finite. Finally, we apply the\nproposed methods to a real data set, which leads to some interesting findings.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 10:45:58 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Kong", "Efang", "", "The corresponding author,\n  Department of Mathematics, University of York"], ["Li", "Jialiang", "", "The corresponding author,\n  Department of Mathematics, University of York"], ["Zhang", "Wenyang", "", "The corresponding author,\n  Department of Mathematics, University of York"]]}, {"id": "1605.01333", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro, Beatriz Pateiro-L\\'opez and Alberto\n  Rodr\\'iguez-Casal", "title": "Minimax Estimation of the Volume of a Set with Smooth Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the volume of a compact domain in a\nEuclidean space based on a uniform sample from the domain. We assume the domain\nhas a boundary with positive reach. We propose a data splitting approach to\ncorrect the bias of the plug-in estimator based on the sample alpha-convex\nhull. We show that this simple estimator achieves a minimax lower bound that we\nderive. Some numerical experiments corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 16:21:08 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Pateiro-L\u00f3pez", "Beatriz", ""], ["Rodr\u00edguez-Casal", "Alberto", ""]]}, {"id": "1605.01384", "submitter": "Konstantinos Zygalakis", "authors": "Michael B. Giles and Mateusz B. Majka and Lukasz Szpruch and Sebastian\n  Vollmer and Konstantinos Zygalakis", "title": "Multilevel Monte Carlo methods for the approximation of invariant\n  measures of stochastic differential equations", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework that allows the use of the multi-level Monte Carlo\n(MLMC) methodology (Giles2015) to calculate expectations with respect to the\ninvariant measure of an ergodic SDE. In that context, we study the\n(over-damped) Langevin equations with a strongly concave potential. We show\nthat, when appropriate contracting couplings for the numerical integrators are\navailable, one can obtain a uniform in time estimate of the MLMC variance in\ncontrast to the majority of the results in the MLMC literature. As a\nconsequence, a root mean square error of $\\mathcal{O}(\\varepsilon)$ is achieved\nwith $\\mathcal{O}(\\varepsilon^{-2})$ complexity on par with Markov Chain Monte\nCarlo (MCMC) methods, which however can be computationally intensive when\napplied to large data sets. Finally, we present a multi-level version of the\nrecently introduced Stochastic Gradient Langevin Dynamics (SGLD) method\n(Welling and Teh, 2011) built for large datasets applications. We show that\nthis is the first stochastic gradient MCMC method with complexity\n$\\mathcal{O}(\\varepsilon^{-2}|\\log {\\varepsilon}|^{3})$, in contrast to the\ncomplexity $\\mathcal{O}(\\varepsilon^{-3})$ of currently available methods.\nNumerical experiments confirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 19:12:13 GMT"}, {"version": "v2", "created": "Sun, 31 Jul 2016 08:57:39 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 11:07:26 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 14:07:59 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Giles", "Michael B.", ""], ["Majka", "Mateusz B.", ""], ["Szpruch", "Lukasz", ""], ["Vollmer", "Sebastian", ""], ["Zygalakis", "Konstantinos", ""]]}, {"id": "1605.01436", "submitter": "Behtash Babadi", "authors": "Abbas Kazemipour, Sina Miran, Piya Pal, Behtash Babadi, and Min Wu", "title": "Sampling Requirements for Stable Autoregressive Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2656848", "report-no": null, "categories": "cs.IT cs.DM math.IT math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters of a linear univariate\nautoregressive model with sub-Gaussian innovations from a limited sequence of\nconsecutive observations. Assuming that the parameters are compressible, we\nanalyze the performance of the $\\ell_1$-regularized least squares as well as a\ngreedy estimator of the parameters and characterize the sampling trade-offs\nrequired for stable recovery in the non-asymptotic regime. In particular, we\nshow that for a fixed sparsity level, stable recovery of AR parameters is\npossible when the number of samples scale sub-linearly with the AR order. Our\nresults improve over existing sampling complexity requirements in AR estimation\nusing the LASSO, when the sparsity level scales faster than the square root of\nthe model order. We further derive sufficient conditions on the sparsity level\nthat guarantee the minimax optimality of the $\\ell_1$-regularized least squares\nestimate. Applying these techniques to simulated data as well as real-world\ndatasets from crude oil prices and traffic speed data confirm our predicted\ntheoretical performance gains in terms of estimation accuracy and model\nselection.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 21:07:04 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 19:22:02 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Kazemipour", "Abbas", ""], ["Miran", "Sina", ""], ["Pal", "Piya", ""], ["Babadi", "Behtash", ""], ["Wu", "Min", ""]]}, {"id": "1605.01438", "submitter": "Sylvain Sardy", "authors": "Sylvain Sardy and Hatef Monajemi", "title": "Threshold Selection for Total Variation Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Total variation (TV) denoising is a nonparametric smoothing method that has\ngood properties for preserving sharp edges and contours in objects with spatial\nstructures like natural images. The estimate is sparse in the sense that TV\nreconstruction leads to a piecewise constant function with a small number of\njumps. A threshold parameter controls the number of jumps and the quality of\nthe estimation. In practice, this threshold is often selected by minimizing a\ngoodness-of-fit criterion like cross-validation, which can be costly as it\nrequires solving the high-dimensional and non-differentiable TV optimization\nproblem many times. We propose instead a two step adaptive procedure via a\nconnection to large deviation of stochastic processes. We also give conditions\nunder which TV denoising achieves exact segmentation. We then apply our\nprocedure to denoise a collection of 1D and 2D test signals verifying the\neffectiveness of our approach in practice.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 21:14:01 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Sardy", "Sylvain", ""], ["Monajemi", "Hatef", ""]]}, {"id": "1605.01440", "submitter": "Debraj Das", "authors": "Debraj Das and Soumendra Nath Lahiri", "title": "Second Order Correctness of Perturbation Bootstrap M-Estimator of\n  Multiple Linear Regression Parameter", "comments": "key words: M-Estimation, S.O.C., Perturbation Bootstrap, Edgeworth\n  Expansion, Studentization, Residual Bootstrap, Generalized Bootstrap, Wild\n  Bootstrap", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the multiple linear regression model $y_{i} = \\boldsymbol{x}'_{i}\n\\boldsymbol{\\beta} + \\epsilon_{i}$, where $\\epsilon_i$'s are independent and\nidentically distributed random variables, $\\mathbf{x}_i$'s are known design\nvectors and $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of parameters. An\neffective way of approximating the distribution of the M-estimator\n$\\boldsymbol{\\bar{\\beta}}_n$, after proper centering and scaling, is the\nPerturbation Bootstrap Method. In this current work, second order results of\nthis non-naive bootstrap method have been investigated. Second order\ncorrectness is important for reducing the approximation error uniformly to\n$o(n^{-1/2})$ to get better inferences. We show that the classical studentized\nversion of the bootstrapped estimator fails to be second order correct. We\nintroduce an innovative modification in the studentized version of the\nbootstrapped statistic and show that the modified bootstrapped pivot is second\norder correct (S.O.C.) for approximating the distribution of the studentized\nM-estimator. Additionally, we show that the Perturbation Bootstrap continues to\nbe S.O.C. when the errors $\\epsilon_i$'s are independent, but may not be\nidentically distributed. These findings establish perturbation Bootstrap\napproximation as a significant improvement over asymptotic normality in the\nregression M-estimation.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 21:22:50 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 23:49:14 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Das", "Debraj", ""], ["Lahiri", "Soumendra Nath", ""]]}, {"id": "1605.01485", "submitter": "Shanshan Ding", "authors": "Shanshan Ding and R. Dennis Cook", "title": "Matrix-Variate Regressions and Envelope Models", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern technology often generates data with complex structures in which both\nresponse and explanatory variables are matrix-valued. Existing methods in the\nliterature are able to tackle matrix-valued predictors but are rather limited\nfor matrix-valued responses. In this article, we study matrix-variate\nregressions for such data, where the response Y on each experimental unit is a\nrandom matrix and the predictor X can be either a scalar, a vector, or a\nmatrix, treated as non-stochastic in terms of the conditional distribution Y|X.\nWe propose models for matrix-variate regressions and then develop envelope\nextensions of these models. Under the envelope framework, redundant variation\ncan be eliminated in estimation and the number of parameters can be notably\nreduced when the matrix-variate dimension is large, possibly resulting in\nsignificant gains in efficiency. The proposed methods are applicable to high\ndimensional settings.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 04:08:30 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 05:46:10 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Ding", "Shanshan", ""], ["Cook", "R. Dennis", ""]]}, {"id": "1605.01559", "submitter": "Alain Durmus", "authors": "Alain Durmus and Eric Moulines", "title": "High-dimensional Bayesian inference via the Unadjusted Langevin\n  Algorithm", "comments": "Supplementary material available at\n  https://hal.inria.fr/hal-01176084/. arXiv admin note: substantial text\n  overlap with arXiv:1507.05021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper the problem of sampling a high-dimensional\nprobability distribution $\\pi$ having a density with respect to the Lebesgue\nmeasure on $\\mathbb{R}^d$, known up to a normalization constant $x \\mapsto\n\\pi(x)= \\mathrm{e}^{-U(x)}/\\int_{\\mathbb{R}^d} \\mathrm{e}^{-U(y)} \\mathrm{d}\ny$. Such problem naturally occurs for example in Bayesian inference and machine\nlearning. Under the assumption that $U$ is continuously differentiable, $\\nabla\nU$ is globally Lipschitz and $U$ is strongly convex, we obtain non-asymptotic\nbounds for the convergence to stationarity in Wasserstein distance of order $2$\nand total variation distance of the sampling method based on the Euler\ndiscretization of the Langevin stochastic differential equation, for both\nconstant and decreasing step sizes. The dependence on the dimension of the\nstate space of these bounds is explicit. The convergence of an appropriately\nweighted empirical measure is also investigated and bounds for the mean square\nerror and exponential deviation inequality are reported for functions which are\nmeasurable and bounded. An illustration to Bayesian inference for binary\nregression is presented to support our claims.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 11:42:35 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 02:19:57 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 08:55:10 GMT"}, {"version": "v4", "created": "Sun, 15 Jul 2018 09:47:23 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Durmus", "Alain", ""], ["Moulines", "Eric", ""]]}, {"id": "1605.01592", "submitter": "Alberto Ferrari", "authors": "Alberto Ferrari and Mario Comelli", "title": "A comparison of methods for the analysis of binomial proportion data in\n  behavioral research", "comments": "26 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In behavioral and psychiatric research, data consisting of a per-subject\nproportion of \"successes\" and \"failures\" over a finite number of trials often\narise. This kind of clustered binary data are usually non-normally distributed,\nwhich can cause issues with parameter estimation and predictions if the usual\ngeneral linear model is applied and sample size is small. Here we studied the\nperformances of some of the available analytic methods applicable to the\nanalysis of proportion data; namely linear regression, Poisson regression,\nbeta-binomial regression and Generalized Linear Mixed Models (GLMMs). We report\nthe conclusions from a simulation study evaluating power and Type I error rates\nof these models in scenarios akin to those met by behavioral researchers and\ndiffering in sample size, cluster size and fixed effects parameters; plus, we\ndescribe results from the application of these methods on data from two real\nbehavioral experiments. Our results show that, while GLMMs and beta-binomial\nregression are powerful instruments for the analysis of clustered binary\noutcomes, linear approximation can still provide reliable hypothesis testing in\nthis context. Poisson regression, on the other hand, can suffer heavily from\nmodel misspecification when used to model proportion data. We conclude\nproviding some guidelines for the choice of appropriate analytical instruments,\nsample and cluster size depending on the conditions of the experiment.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 13:50:49 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 15:47:12 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Ferrari", "Alberto", ""], ["Comelli", "Mario", ""]]}, {"id": "1605.01684", "submitter": "Jonathan Lilly", "authors": "J. M. Lilly, A. M. Sykulski, J. J Early, and S. C. Olhede", "title": "Fractional Brownian motion, the Matern process, and stochastic modeling\n  of turbulent dispersion", "comments": null, "journal-ref": "Nonlinear Processes in Geophysics, 24: 481-514 (2017)", "doi": "10.5194/npg-24-481-2017", "report-no": null, "categories": "stat.ME physics.ao-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic process exhibiting power-law slopes in the frequency domain are\nfrequently well modeled by fractional Brownian motion (fBm). In particular, the\nspectral slope at high frequencies is associated with the degree of small-scale\nroughness or fractal dimension. However, a broad class of real-world signals\nhave a high-frequency slope, like fBm, but a plateau in the vicinity of zero\nfrequency. This low-frequency plateau, it is shown, implies that the temporal\nintegral of the process exhibits diffusive behavior, dispersing from its\ninitial location at a constant rate. Such processes are not well modeled by\nfBm, which has a singularity at zero frequency corresponding to an unbounded\nrate of dispersion. A more appropriate stochastic model is a much lesser-known\nrandom process called the Matern process, which is shown herein to be a damped\nversion of fractional Brownian motion. This article first provides a thorough\nintroduction to fractional Brownian motion, then examines the details of the\nMatern process and its relationship to fBm. An algorithm for the simulation of\nthe Matern process in O(N log N) operations is given. Unlike fBm, the Matern\nprocess is found to provide an excellent match to modeling velocities from\nparticle trajectories in an application to two-dimensional fluid turbulence.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 18:35:36 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 23:08:32 GMT"}, {"version": "v3", "created": "Sat, 2 Sep 2017 04:34:45 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Lilly", "J. M.", ""], ["Sykulski", "A. M.", ""], ["Early", "J. J", ""], ["Olhede", "S. C.", ""]]}, {"id": "1605.02082", "submitter": "Amy Willis", "authors": "Amy Willis, John Bunge, and Thea Whitman", "title": "Improved detection of changes in species richness in high-diversity\n  microbial communities", "comments": "arXiv admin note: text overlap with arXiv:1506.05710", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput sequencing (HTS) continues to expand our understanding of\nmicrobial communities, despite insufficient sequencing depths to detect all\nrare taxa. These low abundance taxa are not accounted for in existing methods\nfor detecting changes in species richness. We address this with a new\nhierarchical model that permits rigorous testing for both heterogeneity and\nbiodiversity changes, and simultaneously improves Type I & II error rates\ncompared to existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 20:17:08 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Willis", "Amy", ""], ["Bunge", "John", ""], ["Whitman", "Thea", ""]]}, {"id": "1605.02138", "submitter": "Juhee Cho", "authors": "Juhee Cho, Donggyu Kim, and Karl Rohe", "title": "Intelligent Initialization and Adaptive Thresholding for Iterative\n  Matrix Completion; Some Statistical and Algorithmic Theory for\n  Adaptive-Impute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, various matrix completion algorithms have been\ndeveloped. Thresholded singular value decomposition (SVD) is a popular\ntechnique in implementing many of them. A sizable number of studies have shown\nits theoretical and empirical excellence, but choosing the right threshold\nlevel still remains as a key empirical difficulty. This paper proposes a novel\nmatrix completion algorithm which iterates thresholded SVD with\ntheoretically-justified and data-dependent values of thresholding parameters.\nThe estimate of the proposed algorithm enjoys the minimax error rate and shows\noutstanding empirical performances. The thresholding scheme that we use can be\nviewed as a solution to a non-convex optimization problem, understanding of\nwhose theoretical convergence guarantee is known to be limited. We investigate\nthis problem by introducing a simpler algorithm, generalized-\\SI, analyzing its\nconvergence behavior, and connecting it to the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 03:15:01 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Cho", "Juhee", ""], ["Kim", "Donggyu", ""], ["Rohe", "Karl", ""]]}, {"id": "1605.02144", "submitter": "Chen Lu", "authors": "Chen Lu", "title": "New Approaches to Identify Gene-by-Gene Interactions in Genome Wide\n  Association Studies", "comments": "This is a Ph.D Thesis work completed in 2013. Boston University,\n  ProQuest Dissertations Publishing, 2013. 3624812", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic variants identified to date by genome-wide association studies only\nexplain a small fraction of total heritability. Gene-by-gene interaction is one\nimportant potential source of unexplained heritability. In the first part of\nthis dissertation, a novel approach to detect such interactions is proposed.\nThis approach utilizes penalized regression and sparse estimation principles,\nand incorporates outside biological knowledge through a network-based penalty.\nThe method is tested on simulated data under various scenarios. Simulations\nshow that with reasonable outside biological knowledge, the new method performs\nnoticeably better than current stage-wise strategies, especially when the\nmarginal strength of main effects is weak.\n  The proposed method is designed for single-cohort analyses. However, it is\ngenerally acknowledged that only multi-cohort analyses have sufficient power to\nuncover genes and gene-by-gene interactions with moderate effects on traits,\nsuch as likely underlie complex diseases. Multi-cohort, meta-analysis\napproaches for penalized regressions are developed and investigated in the\nsecond part of this dissertation. Specifically, I propose two different ways of\nutilizing data-splitting principles in multi-cohort settings and develop three\nprocedures to conduct meta-analysis. Using the method developed in the first\npart of this dissertation as an example of penalized regressions, three\nproposed meta-analysis procedures are compared to mega-analysis using a\nsimulation study. The results suggest that the best approach is to split the\nparticipating cohorts into two groups.\n  In the last part of this dissertation, the novel method developed in the\nfirst part is applied to the Framingham Heart Study measures on total plasma\nImmunoglobulin E (IgE) concentrations, C-reactive protein levels, and Fasting\nGlucose.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 05:51:02 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Lu", "Chen", ""]]}, {"id": "1605.02209", "submitter": "Aris Spanos", "authors": "Aris Spanos", "title": "Revisiting Simpson's Paradox: a statistical misspecification perspective", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary objective of this paper is to revisit Simpson's paradox using a\nstatistical misspecification perspective. It is argued that the reversal of\nstatistical associations is sometimes spurious, stemming from invalid\nprobabilistic assumptions imposed on the data. The concept of statistical\nmisspecification is used to formalize the vague term `spurious results' as\n`statistically untrustworthy' inference results. This perspective sheds new\nlight on the paradox by distingusing between statistically trustworthy vs.\nuntrustworthy association reversals. It turns out that in both cases there is\nnothing counterintuitive to explain or account for. This perspective is also\nused to revisit the causal `resolution' of the paradox in an attempt to\ndelineate the modeling and inference issues raised by the statistical\nmisspecification perspective. The main arguments are illustrated using both\nactual and hypothetical data from the literature, including Yule's\n\"nonsense-correlations\" and the Berkeley admissions study.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 16:26:59 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 14:53:54 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Spanos", "Aris", ""]]}, {"id": "1605.02234", "submitter": "Farouk Nathoo", "authors": "Keelin Greenlaw, Elena Szefer, Jinko Graham, Mary Lesperance and\n  Farouk S. Nathoo", "title": "A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Recent advances in technology for brain imaging and\nhigh-throughput genotyping have motivated studies examining the influence of\ngenetic variation on brain structure. Wang et al. (Bioinformatics, 2012) have\ndeveloped an approach for the analysis of imaging genomic studies using\npenalized multi-task regression with regularization based on a novel group\n$l_{2,1}$-norm penalty which encourages structured sparsity at both the gene\nlevel and SNP level. While incorporating a number of useful features, the\nproposed method only furnishes a point estimate of the regression coefficients;\ntechniques for conducting statistical inference are not provided. A new\nBayesian method is proposed here to overcome this limitation.\n  Results: We develop a Bayesian hierarchical modeling formulation where the\nposterior mode corresponds to the estimator proposed by Wang et al.\n(Bioinformatics, 2012), and an approach that allows for full posterior\ninference including the construction of interval estimates for the regression\nparameters. We show that the proposed hierarchical model can be expressed as a\nthree-level Gaussian scale mixture and this representation facilitates the use\nof a Gibbs sampling algorithm for posterior simulation. Simulation studies\ndemonstrate that the interval estimates obtained using our approach achieve\nadequate coverage probabilities that outperform those obtained from the\nnonparametric bootstrap. Our proposed methodology is applied to the analysis of\nneuroimaging and genetic data collected as part of the Alzheimer's Disease\nNeuroimaging Initiative (ADNI), and this analysis of the ADNI cohort\ndemonstrates clearly the value added of incorporating interval estimation\nbeyond only point estimation when relating SNPs to brain imaging\nendophenotypes.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 19:16:53 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 18:46:27 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Greenlaw", "Keelin", ""], ["Szefer", "Elena", ""], ["Graham", "Jinko", ""], ["Lesperance", "Mary", ""], ["Nathoo", "Farouk S.", ""]]}, {"id": "1605.02256", "submitter": "Javier Contreras-Reyes", "authors": "Javier E. Contreras-Reyes, Freddy Omar L\\'opez Quintero", "title": "Comment on \"A variational Bayesian approach for inverse problems with\n  skew-t error distributions\" (Guha et al., Journal of Computational Physics\n  301 (2015) 377-393)", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brief comment on A variational Bayesian approach for inverse problems with\nskew-t error distributions (Guha et al., Journal of Computational Physics 301\n(2015) 377-393) is given in this letter.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 01:13:21 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 00:18:34 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Contreras-Reyes", "Javier E.", ""], ["Quintero", "Freddy Omar L\u00f3pez", ""]]}, {"id": "1605.02332", "submitter": "Yongli Sang", "authors": "Yongli Sang, Xin Dang and Hailin Sang", "title": "Symmetric Gini Covariance and Correlation", "comments": "20 pages.Accepted by Canadian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Standard Gini covariance and Gini correlation play important roles in\nmeasuring the dependence of random variables with heavy tails. However, the\nasymmetry brings a substantial difficulty in interpretation. In this paper, we\npropose a symmetric Gini-type covariance and a symmetric Gini correlation\n($\\rho_g$) based on the joint rank function. The proposed correlation $\\rho_g$\nis more robust than the Pearson correlation but less robust than the Kendall's\n$\\tau$ correlation. We establish the relationship between $\\rho_g$ and the\nlinear correlation $\\rho$ for a class of random vectors in the family of\nelliptical distributions, which allows us to estimate $\\rho$ based on\nestimation of $\\rho_g$. The asymptotic normality of the resulting estimators of\n$\\rho$ are studied through two approaches: one from influence function and the\nother from U-statistics and the delta method. We compare asymptotic\nefficiencies of linear correlation estimators based on the symmetric Gini,\nregular Gini, Pearson and Kendall's $\\tau$ under various distributions. In\naddition to reasonably balancing between robustness and efficiency, the\nproposed measure $\\rho_g$ demonstrates superior finite sample performance,\nwhich makes it attractive in applications.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 16:42:08 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Sang", "Yongli", ""], ["Dang", "Xin", ""], ["Sang", "Hailin", ""]]}, {"id": "1605.02351", "submitter": "Boris Hejblum", "authors": "Denis Agniel and Boris P Hejblum", "title": "Variance component score test for time-course gene set analysis of\n  longitudinal RNA-seq data", "comments": "23 pages, 6 figures, typo corrections & acceptance acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As gene expression measurement technology is shifting from microarrays to\nsequencing, the statistical tools available for their analysis must be adapted\nsince RNA-seq data are measured as counts. Recently, it has been proposed to\ntackle the count nature of these data by modeling log-count reads per million\nas continuous variables, using nonparametric regression to account for their\ninherent heteroscedasticity. Adopting such a framework, we propose tcgsaseq, a\nprincipled, model-free and efficient top-down method for detecting longitudinal\nchanges in RNA-seq gene sets. Considering gene sets defined a priori, tcgsaseq\nidentifies those whose expression vary over time, based on an original variance\ncomponent score test accounting for both covariates and heteroscedasticity\nwithout assuming any specific parametric distribution for the transformed\ncounts. We demonstrate that despite the presence of a nonparametric component,\nour test statistic has a simple form and limiting distribution, and both may be\ncomputed quickly. A permutation version of the test is additionally proposed\nfor very small sample sizes. Applied to both simulated data and two real\ndatasets, the proposed method is shown to exhibit very good statistical\nproperties, with an increase in stability and power when compared to state of\nthe art methods ROAST, edgeR and DESeq2, which can fail to control the type I\nerror under certain realistic settings. We have made the method available for\nthe community in the R package tcgsaseq.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 19:21:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 20:01:35 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 04:20:31 GMT"}, {"version": "v4", "created": "Thu, 5 Jan 2017 11:48:37 GMT"}, {"version": "v5", "created": "Fri, 6 Jan 2017 14:47:20 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Agniel", "Denis", ""], ["Hejblum", "Boris P", ""]]}, {"id": "1605.02418", "submitter": "Pritam Ranjan", "authors": "Sujay Mukhoti, Pritam Ranjan", "title": "Mean-correction and Higher Order Moments for a Stochastic Volatility\n  Model with Correlated Errors", "comments": "15 pages; 5 figures, submitted to IJSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an efficient stock market, the log-returns and their time-dependent\nvariances are often jointly modelled by stochastic volatility models (SVMs).\nMany SVMs assume that errors in log-return and latent volatility process are\nuncorrelated, which is unrealistic. It turns out that if a non-zero correlation\nis included in the SVM (e.g., Shephard (2005)), then the expected log-return at\ntime t conditional on the past returns is non-zero, which is not a desirable\nfeature of an efficient stock market. In this paper, we propose a\nmean-correction for such an SVM for discrete-time returns with non-zero\ncorrelation. We also find closed form analytical expressions for higher moments\nof log-return and its lead-lag correlations with the volatility process. We\ncompare the performance of the proposed and classical SVMs on S&P 500 index\nreturns obtained from NYSE.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 05:01:35 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Mukhoti", "Sujay", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1605.02561", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (LNE, L2S, GdR MASCOT-NUM), Julien Bect (L2S, GdR\n  MASCOT-NUM), S\\'everine Demeyer (LNE), Nicolas Fischer (LNE), Emmanuel\n  Vazquez (L2S, GdR MASCOT-NUM)", "title": "Gaussian process modeling for stochastic multi-fidelity simulators, with\n  application to fire safety", "comments": null, "journal-ref": "48{\\`e}mes Journ{\\'e}es de Statistique de la SFdS (JdS 2016), May\n  2016, Montpellier, France", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess the possibility of evacuating a building in case of a fire, a\nstandard method consists in simulating the propagation of fire, using finite\ndifference methods and takes into account the random behavior of the fire, so\nthat the result of a simulation is non-deterministic. The mesh fineness tunes\nthe quality of the numerical model, and its computational cost. Depending on\nthe mesh fineness, one simulation can last anywhere from a few minutes to\nseveral weeks. In this article, we focus on predicting the behavior of the fire\nsimulator at fine meshes, using cheaper results, at coarser meshes. In the\nliterature of the design and analysis of computer experiments, such a problem\nis referred to as multi-fidelity prediction. Our contribution is to extend to\nthe case of stochastic simulators the Bayesian multi-fidelity model proposed by\nPicheny and Ginsbourger (2013) and Tuo et al. (2014).\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 12:41:27 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Stroh", "R\u00e9mi", "", "LNE, L2S, GdR MASCOT-NUM"], ["Bect", "Julien", "", "L2S, GdR\n  MASCOT-NUM"], ["Demeyer", "S\u00e9verine", "", "LNE"], ["Fischer", "Nicolas", "", "LNE"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "1605.02718", "submitter": "Kerry Fendick", "authors": "Kerry Fendick", "title": "Gaussian Processes for Local Polynomial Forecasting of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-stationary time series with non-linear trends are frequently encountered\nin applications. We consider here the feasibility of accurately forecasting the\nsignals of multiple such time series considering jointly when the number of\nhistoric samples is inadequate for accurately forecasting the signal of each\nconsidered in isolation. We develop a new forecasting methodology based on\nGaussian process regression that is successful in doing so in examples for\nwhich the method of generalized least-squares is not. The new method employs a\nform of deep machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:55:24 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 20:40:56 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Fendick", "Kerry", ""]]}, {"id": "1605.02880", "submitter": "Christophe Ley", "authors": "Holger Dette, Christophe Ley and Francisco Javier Rubio", "title": "Natural (non-)informative priors for skew-symmetric distributions", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an innovative method for constructing proper priors\nfor the skewness (shape) parameter in the skew-symmetric family of\ndistributions. The proposed method is based on assigning a prior distribution\non the perturbation effect of the shape parameter, which is quantified in terms\nof the Total Variation distance. We discuss strategies to translate prior\nbeliefs about the asymmetry of the data into an informative prior distribution\nof this class. We show via a Monte Carlo simulation study that our\nnoninformative priors induce posterior distributions with good frequentist\nproperties, similar to those of the Jeffreys prior. Our informative priors\nyield better results than their competitors from the literature. We also\npropose a scale- and location-invariant prior structure for models with unknown\nlocation and scale parameters and provide sufficient conditions for the\npropriety of the corresponding posterior distribution. Illustrative examples\nare presented using simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 07:47:53 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 09:53:49 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 11:17:09 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Dette", "Holger", ""], ["Ley", "Christophe", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "1605.03000", "submitter": "Beau Dabbs", "authors": "Beau Dabbs, Brian Junker", "title": "Comparison of Cross-Validation Methods for Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel cross-validation method that we call latinCV and we\ncompare this method to other model selection methods using data generated from\na stochastic block model. Comparing latinCV to other cross-validation methods,\nwe show that latinCV performs similarly to a method described in\n\\cite{hoff2008modeling} and that latinCV has a significantly larger true model\nrecovery accuracy than the NCV method of \\cite{chen2014network}. We also show\nthat the reason for this discrepancy is related to the larger variance of the\nNCV estimate. Comparing latinCV to alternative model selection methods, we show\nthat latinCV performs better than information criteria AIC and BIC, as well as\nthe community detection method infomap and a routine that attempts to maximize\nmodularity. The simulation study in this paper includes a range of network\nsizes and generative parameters for the stochastic block model that allow us to\nexamine the relationship between model selection accuracy and the size and\ncomplexity of the model. Overall, latinCV performs more accurate model\nselection, and avoids overfitting better than any of the other model selection\nmethods considered.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 13:29:13 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Dabbs", "Beau", ""], ["Junker", "Brian", ""]]}, {"id": "1605.03154", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Hira L. Koul, Akshita Chawla and Soumendra N. Lahiri", "title": "Two Stage Non-penalized Corrected Least Squares for High Dimensional\n  Linear Models with Measurement error or Missing Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an alternative to penalized estimators for estimation and\nvari- able selection in high dimensional linear regression models with\nmeasurement error or missing covariates. We propose estimation via bias\ncorrected least squares after model selection. We show that by separating model\nselection and estimation, it is possible to achieve an improved rate of\nconvergence of the L2 estimation error compared to the rate sqrt{s log p/n}\nachieved by simultaneous estimation and variable selection methods such as L1\npenalized corrected least squares. If the correct model is selected with high\nprobability then the L2 rate of convergence for the proposed method is indeed\nthe oracle rate of sqrt{s/n}. Here s, p are the number of non zero parameters\nand the model dimension, respectively, and n is the sample size. Under very\ngeneral model selection criteria, the proposed method is computationally\nsimpler and statistically at least as efficient as the L1 penalized corrected\nleast squares method, performs model selection without the availability of the\nbias correction matrix, and is able to provide estimates with only a small\nsub-block of the bias correction covariance matrix of order s x s in comparison\nto the p x p correction matrix required for computation of the L1 penalized\nversion. Furthermore we show that the model selection requirements are met by a\ncorrelation screening type method and the L1 penalized corrected least squares\nmethod. Also, the proposed methodology when applied to the estimation of\nprecision matrices with missing observations, is seen to perform at least as\nwell as existing L1 penalty based methods. All results are supported\nempirically by a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 19:06:37 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Kaul", "Abhishek", ""], ["Koul", "Hira L.", ""], ["Chawla", "Akshita", ""], ["Lahiri", "Soumendra N.", ""]]}, {"id": "1605.03294", "submitter": "Timothy Daley", "authors": "Timothy Daley and Andrew D Smith", "title": "Better lower bounds for missing species: improved non-parametric\n  moment-based estimation for large experiments", "comments": "27 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation of the number of species or unobserved classes from a random\nsample of the underlying population is a ubiquitous problem in statistics. In\nclassical settings, the size of the sample is usually small. New technologies\nsuch as high-throughput DNA sequencing have allowed for the sampling of\nextremely large and heterogeneous populations at scales not previously\nattainable or even considered. New algorithms are required that take advantage\nof the size of the data to account for heterogeneity, but are also sufficiently\nfast and scale well with large data. We present a non-parametric moment-based\nestimator that is both computationally efficient and is sufficiently flexible\nto account for heterogeneity in the abundances of underlying population. This\nestimator is based on an extension of a popular moment-based lower bound (Chao,\n1984), originally developed by Harris (1959) but unattainable due to the lack\nof economical algorithms to solve the system of nonlinear equation required for\nestimation. We apply results from the classical moment problem to show that\nsolutions can be obtained efficiently, allowing for estimators that are\nsimultaneously conservative and use more information. This is critical for\nmodern genomic applications, where there may be many large experiments that\nrequire the application of species estimation. We present applications of our\nestimator to estimating T-Cell receptor repertoire and dropout in single cell\nRNA-seq experiments.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 06:07:27 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 22:57:14 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 19:40:17 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Daley", "Timothy", ""], ["Smith", "Andrew D", ""]]}, {"id": "1605.03306", "submitter": "Jinchi Lv", "authors": "Zemin Zheng and Yingying Fan and Jinchi Lv", "title": "High dimensional thresholded regression and shrinkage effect", "comments": "23 pages, 3 figures, 5 tables", "journal-ref": "Journal of the Royal Statistical Society Series B 76, 627-649", "doi": "10.1111/rssb.12037", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional sparse modeling via regularization provides a powerful tool\nfor analyzing large-scale data sets and obtaining meaningful, interpretable\nmodels. The use of nonconvex penalty functions shows advantage in selecting\nimportant features in high dimensions, but the global optimality of such\nmethods still demands more understanding. In this paper, we consider sparse\nregression with hard-thresholding penalty, which we show to give rise to\nthresholded regression. This approach is motivated by its close connection with\nthe $L_0$-regularization, which can be unrealistic to implement in practice but\nof appealing sampling properties, and its computational advantage. Under some\nmild regularity conditions allowing possibly exponentially growing\ndimensionality, we establish the oracle inequalities of the resulting\nregularized estimator, as the global minimizer, under various prediction and\nvariable selection losses, as well as the oracle risk inequalities of the\nhard-thresholded estimator followed by a further $L_2$-regularization. The risk\nproperties exhibit interesting shrinkage effects under both estimation and\nprediction losses. We identify the optimal choice of the ridge parameter, which\nis shown to have simultaneous advantages to both the $L_2$-loss and prediction\nloss. These new results and phenomena are evidenced by simulation and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:12:07 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Zheng", "Zemin", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03310", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Jinchi Lv", "title": "Asymptotic equivalence of regularization methods in thresholded\n  parameter space", "comments": "39 pages, 3 figures", "journal-ref": "Journal of the American Statistical Association 108, 1044-1061", "doi": "10.1080/01621459.2013.803972", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data analysis has motivated a spectrum of regularization\nmethods for variable selection and sparse modeling, with two popular classes of\nconvex ones and concave ones. A long debate has been on whether one class\ndominates the other, an important question both in theory and to practitioners.\nIn this paper, we characterize the asymptotic equivalence of regularization\nmethods, with general penalty functions, in a thresholded parameter space under\nthe generalized linear model setting, where the dimensionality can grow up to\nexponentially with the sample size. To assess their performance, we establish\nthe oracle inequalities, as in Bickel, Ritov and Tsybakov (2009), of the global\nminimizer for these methods under various prediction and variable selection\nlosses. These results reveal an interesting phase transition phenomenon. For\npolynomially growing dimensionality, the $L_1$-regularization method of Lasso\nand concave methods are asymptotically equivalent, having the same convergence\nrates in the oracle inequalities. For exponentially growing dimensionality,\nconcave methods are asymptotically equivalent but have faster convergence rates\nthan the Lasso. We also establish a stronger property of the oracle risk\ninequalities of the regularization methods, as well as the sampling properties\nof computable solutions. Our new theoretical results are illustrated and\njustified by simulation and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:39:16 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03311", "submitter": "Jinchi Lv", "authors": "Yinfei Kong and Zemin Zheng and Jinchi Lv", "title": "The constrained Dantzig selector with enhanced consistency", "comments": "to appear in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dantzig selector has received popularity for many applications such as\ncompressed sensing and sparse modeling, thanks to its computational efficiency\nas a linear programming problem and its nice sampling properties. Existing\nresults show that it can recover sparse signals mimicking the accuracy of the\nideal procedure, up to a logarithmic factor of the dimensionality. Such a\nfactor has been shown to hold for many regularization methods. An important\nquestion is whether this factor can be reduced to a logarithmic factor of the\nsample size in ultra-high dimensions under mild regularity conditions. To\nprovide an affirmative answer, in this paper we suggest the constrained Dantzig\nselector, which has more flexible constraints and parameter space. We prove\nthat the suggested method can achieve convergence rates within a logarithmic\nfactor of the sample size of the oracle rates and improved sparsity, under a\nfairly weak assumption on the signal strength. Such improvement is significant\nin ultra-high dimensions. This method can be implemented efficiently through\nsequential linear programming. Numerical studies confirm that the sample size\nneeded for a certain level of accuracy in these problems can be much reduced.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:41:50 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Kong", "Yinfei", ""], ["Zheng", "Zemin", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03313", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Jinchi Lv", "title": "Innovated scalable efficient estimation in ultra-large Gaussian\n  graphical models", "comments": "to appear, The Annals of Statistics (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale precision matrix estimation is of fundamental importance yet\nchallenging in many contemporary applications for recovering Gaussian graphical\nmodels. In this paper, we suggest a new approach of innovated scalable\nefficient estimation (ISEE) for estimating large precision matrix. Motivated by\nthe innovated transformation, we convert the original problem into that of\nlarge covariance matrix estimation. The suggested method combines the strengths\nof recent advances in high-dimensional sparse modeling and large covariance\nmatrix estimation. Compared to existing approaches, our method is scalable and\ncan deal with much larger precision matrices with simple tuning. Under mild\nregularity conditions, we establish that this procedure can recover the\nunderlying graphical structure with significant probability and provide\nefficient estimation of link strengths. Both computational and theoretical\nadvantages of the procedure are evidenced through simulation and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:50:21 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03315", "submitter": "Jinchi Lv", "authors": "Yinfei Kong and Daoji Li and Yingying Fan and Jinchi Lv", "title": "Interaction pursuit in high-dimensional multi-response regression via\n  distance correlation", "comments": "to appear in The Annals of Statistics (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature interactions can contribute to a large proportion of variation in\nmany prediction models. In the era of big data, the coexistence of high\ndimensionality in both responses and covariates poses unprecedented challenges\nin identifying important interactions. In this paper, we suggest a two-stage\ninteraction identification method, called the interaction pursuit via distance\ncorrelation (IPDC), in the setting of high-dimensional multi-response\ninteraction models that exploits feature screening applied to transformed\nvariables with distance correlation followed by feature selection. Such a\nprocedure is computationally efficient, generally applicable beyond the\nheredity assumption, and effective even when the number of responses diverges\nwith the sample size. Under mild regularity conditions, we show that this\nmethod enjoys nice theoretical properties including the sure screening\nproperty, support union recovery, and oracle inequalities in prediction and\nestimation for both interactions and main effects. The advantages of our method\nare supported by several simulation studies and real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:03:14 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Kong", "Yinfei", ""], ["Li", "Daoji", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03321", "submitter": "Yingying Fan", "authors": "Yingying Fan and Cheng Yong Tang", "title": "Tuning parameter selection in high dimensional penalized likelihood", "comments": "38 pages", "journal-ref": "Journal of the Royal Statistical Society Series B 75, 531-552\n  (2013)", "doi": "10.1111/rssb.12001", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining how to appropriately select the tuning parameter is essential in\npenalized likelihood methods for high-dimensional data analysis. We examine\nthis problem in the setting of penalized likelihood methods for generalized\nlinear models, where the dimensionality of covariates p is allowed to increase\nexponentially with the sample size n. We propose to select the tuning parameter\nby optimizing the generalized information criterion (GIC) with an appropriate\nmodel complexity penalty. To ensure that we consistently identify the true\nmodel, a range for the model complexity penalty is identified in GIC. We find\nthat this model complexity penalty should diverge at the rate of some power of\n$\\log p$ depending on the tail probability behavior of the response variables.\nThis reveals that using the AIC or BIC to select the tuning parameter may not\nbe adequate for consistently identifying the true model. Based on our\ntheoretical study, we propose a uniform choice of the model complexity penalty\nand show that the proposed approach consistently identifies the true model\namong candidate models with asymptotic probability one. We justify the\nperformance of the proposed procedure by numerical simulations and a gene\nexpression data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:17:58 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Tang", "Cheng Yong", ""]]}, {"id": "1605.03335", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Jinchi Lv", "title": "Asymptotic properties for combined $L_1$ and concave regularization", "comments": "16 pages", "journal-ref": "Biometrika 101, 57-70", "doi": "10.1093/biomet/ast047", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two important goals of high-dimensional modeling are prediction and variable\nselection. In this article, we consider regularization with combined $L_1$ and\nconcave penalties, and study the sampling properties of the global optimum of\nthe suggested method in ultra-high dimensional settings. The $L_1$-penalty\nprovides the minimum regularization needed for removing noise variables in\norder to achieve oracle prediction risk, while concave penalty imposes\nadditional regularization to control model sparsity. In the linear model\nsetting, we prove that the global optimum of our method enjoys the same oracle\ninequalities as the lasso estimator and admits an explicit bound on the false\nsign rate, which can be asymptotically vanishing. Moreover, we establish oracle\nrisk inequalities for the method and the sampling properties of computable\nsolutions. Numerical studies suggest that our method yields more stable\nestimates than using a concave penalty alone.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:51:05 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03378", "submitter": "Mahsa Ghanbari", "authors": "Mahsa Ghanbari, Julia Lasserre and Martin Vingron", "title": "The Distance Precision Matrix: computing networks from nonlinear\n  relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental method of reconstructing networks, e.g. in the context of gene\nregulation, relies on the precision matrix (the inverse of the\nvariance-covariance matrix) as an indicator which variables are associated with\neach other. The precision matrix assumes Gaussian data and its entries are zero\nfor those pairs of variable which are conditionally independent. Here, we\npropose the Distance Precision Matrix which is based on a measure of possibly\nnon-linear association, the distance covarince. We provide evidence that the\nDistance Precision Matrix can successfully compute networks from non-linear\ndata and does so in a very consistent manner across many data situations.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 11:21:36 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 15:07:57 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Ghanbari", "Mahsa", ""], ["Lasserre", "Julia", ""], ["Vingron", "Martin", ""]]}, {"id": "1605.03471", "submitter": "Reza Solgi", "authors": "Luke Bornn, Neil Shephard, Reza Solgi", "title": "Nonparametric hierarchical Bayesian quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we develop a method for performing nonparametric Bayesian inference on\nquantiles. Relying on geometric measure theory and employing a Hausdorff base\nmeasure, we are able to specify meaningful priors for the quantile while\ntreating the distribution of the data otherwise nonparametrically. We further\nextend the method to a hierarchical model for quantiles of subpopulations,\nlinking subgroups together solely through their quantiles. Our approach is\ncomputationally straightforward, allowing for censored and noisy data. We\ndemonstrate the proposed methodology on simulated data and an applied problem\nfrom sports statistics, where it is observed to stabilize and improve inference\nand prediction.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 15:16:52 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Bornn", "Luke", ""], ["Shephard", "Neil", ""], ["Solgi", "Reza", ""]]}, {"id": "1605.03486", "submitter": "Alexander Tybl", "authors": "Alexander J. Tybl", "title": "An Overview of Spatial Econometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers an expository overview of the field of spatial\neconometrics. It first justifies the necessity of special statistical\nprocedures for the analysis of spatial data and then proceeds to describe the\nfundamentals of these procedures. In particular, this paper covers three\ncrucial techniques for building models with spatial data. First, we discuss how\nto create a spatial weights matrix based on the distances between each data\npoint in a dataset. Next, we describe the conventional methods to formally\ndetect spatial autocorrelation, both global and local. Finally, we outline the\nchief components of a spatial autoregressive model, noting the circumstances\nunder which it would be appropriate to incorporate each component into a model.\nThis paper seeks to offer a concise introduction to spatial econometrics that\nwill be accessible to interested individuals with a background in statistics or\neconometrics.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 15:52:25 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Tybl", "Alexander J.", ""]]}, {"id": "1605.03677", "submitter": "Linbo Wang", "authors": "Linbo Wang, James M. Robins and Thomas S. Richardson", "title": "On falsification of the binary instrumental variable model", "comments": "To appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables are widely used for estimating causal effects in the\npresence of unmeasured confounding. The discrete instrumental variable model\nhas testable implications on the law of the observed data. However, current\nassessments of instrumental validity are typically based solely on\nsubject-matter arguments rather than these testable implications, partly due to\na lack of formal statistical tests with known properties. In this paper, we\ndevelop simple procedures for testing the binary instrumental variable model.\nOur methods are based on existing approaches for comparing two treatments, such\nas the t-test and the Gail--Simon test. We illustrate the importance of testing\nthe instrumental variable model by evaluating the exogeneity of college\nproximity using the National Longitudinal Survey of Young Men.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 04:39:43 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 03:59:41 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wang", "Linbo", ""], ["Robins", "James M.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1605.03697", "submitter": "Suyan Tian", "authors": "Suyan Tian and Howard H. Chang and Chi Wang", "title": "Weighted SAMGSR: combining significance analysis of microarray-gene set\n  reduction algorithm with pathway topology-based weights to select relevant\n  genes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction\n  It has been demonstrated that a pathway-based feature selection method which\nincorporates biological information within pathways into the process of feature\nselection usually outperform a gene-based feature selection algorithm in terms\nof predictive accuracy, stability, and biological interpretation. Significance\nanalysis of microarray-gene set reduction algorithm (SAMGSR), an extension to a\ngene set analysis method with further reduction of the selected pathways to\ntheir respective core subsets, can be regarded as a pathway-based feature\nselection method.\n  Results and Discussion\n  In SAMGSR, whether a gene is selected is mainly determined by its expression\ndifference between the phenotypes, and partially by the number of pathways to\nwhich this gene belongs, but ignoring the topology information among pathways.\nIn this study, we propose a weighted version of the SAMGSR algorithm by\nconstructing weights based on the connectivity among genes and then\nincorporating these weights in the test statistic.\n  Conclusions\n  Using both simulated and real-world data, we evaluate the performance of the\nproposed SAMGSR extension and demonstrate that gene connectivity is indeed\ninformative for feature selection.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 07:11:51 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Tian", "Suyan", ""], ["Chang", "Howard H.", ""], ["Wang", "Chi", ""]]}, {"id": "1605.03707", "submitter": "Xiongtao Dai", "authors": "Xiongtao Dai, Hans-Georg M\\\"uller, Fang Yao", "title": "Optimal Bayes Classifiers for Functional Data and Density Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes classifiers for functional data pose a challenge. This is because\nprobability density functions do not exist for functional data. As a\nconsequence, the classical Bayes classifier using density quotients needs to be\nmodified. We propose to use density ratios of projections on a sequence of\neigenfunctions that are common to the groups to be classified. The density\nratios can then be factored into density ratios of individual functional\nprincipal components whence the classification problem is reduced to a sequence\nof nonparametric one-dimensional density estimates. This is an extension to\nfunctional data of some of the very earliest nonparametric Bayes classifiers\nthat were based on simple density ratios in the one-dimensional case. By means\nof the factorization of the density quotients the curse of dimensionality that\nwould otherwise severely affect Bayes classifiers for functional data can be\navoided. We demonstrate that in the case of Gaussian functional data, the\nproposed functional Bayes classifier reduces to a functional version of the\nclassical quadratic discriminant. A study of the asymptotic behavior of the\nproposed classifiers in the large sample limit shows that under certain\nconditions the misclassification rate converges to zero, a phenomenon that has\nbeen referred to as \"perfect classification\". The proposed classifiers also\nperform favorably in finite sample applications, as we demonstrate in\ncomparisons with other functional classifiers in simulations and various data\napplications, including wine spectral data, functional magnetic resonance\nimaging (fMRI) data for attention deficit hyperactivity disorder (ADHD)\npatients, and yeast gene expression data.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 07:40:07 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Dai", "Xiongtao", ""], ["M\u00fcller", "Hans-Georg", ""], ["Yao", "Fang", ""]]}, {"id": "1605.03824", "submitter": "Esa Ollila", "authors": "Esa Ollila", "title": "Direction of arrival estimation using robust complex Lasso", "comments": "Paper has appeared in the Proceedings of the 10th European Conference\n  on Antennas and Propagation (EuCAP'2016), Davos, Switzerland, April 10-15,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasso (Least Absolute Shrinkage and Selection Operator) has been a\npopular technique for simultaneous linear regression estimation and variable\nselection. In this paper, we propose a new novel approach for robust Lasso that\nfollows the spirit of M-estimation. We define $M$-Lasso estimates of regression\nand scale as solutions to generalized zero subgradient equations. Another\nunique feature of this paper is that we consider complex-valued measurements\nand regression parameters, which requires careful mathematical characterization\nof the problem. An explicit and efficient algorithm for computing the $M$-Lasso\nsolution is proposed that has comparable computational complexity as\nstate-of-the-art algorithm for computing the Lasso solution. Usefulness of the\n$M$-Lasso method is illustrated for direction-of-arrival (DoA) estimation with\nsensor arrays in a single snapshot case.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 14:19:52 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Ollila", "Esa", ""]]}, {"id": "1605.03868", "submitter": "Bhaswar Bhattacharya", "authors": "Kwonsang Lee, Bhaswar B. Bhattacharya, Jing Qin, and Dylan S. Small", "title": "A Nonparametric Likelihood Approach for Inference in Instrumental\n  Variable Models", "comments": "Major changes. Updated BL method. New theorems and data analysis\n  added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods allow for inference about the treatment effect\nby controlling for unmeasured confounding in randomized experiments with\nnoncompliance. However, many studies do not consider the observed compliance\nbehavior in the testing procedure, which can lead to a loss of power. In this\npaper, we propose a novel nonparametric likelihood approach, referred to as the\nbinomial likelihood (BL) method, that incorporates information on compliance\nbehavior while overcoming several limitations of previous techniques and\nutilizing the advantages of likelihood methods. Our proposed method produces\nproper estimates of the counterfactual distribution functions by maximizing the\nbinomial likelihood over the space of distribution functions. Using this we\npropose two versions of a binomial likelihood ratio test for the null\nhypothesis of no treatment effect. We show that both versions are more powerful\nto detect any distributional change than existing methods in finite sample\ncases, and are asymptotically equivalent to the two-sample Anderson-Darling\ntest. We also develop an efficient algorithm for computing our estimates, and\napply the binomial likelihood method to a study of the effect of Medicaid\ncoverage on mental health using the Oregon Health Insurance Experiment.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 15:55:51 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 18:30:18 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 00:19:26 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Lee", "Kwonsang", ""], ["Bhattacharya", "Bhaswar B.", ""], ["Qin", "Jing", ""], ["Small", "Dylan S.", ""]]}, {"id": "1605.03884", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "An Empirical-Bayes Score for Discrete Bayesian Networks", "comments": "12 pages, PGM 2016", "journal-ref": "Journal of Machine Learning Research (52, Proceedings Track, PGM\n  2016), 438-448", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structure learning is often performed in a Bayesian setting,\nby evaluating candidate structures using their posterior probabilities for a\ngiven data set. Score-based algorithms then use those posterior probabilities\nas an objective function and return the maximum a posteriori network as the\nlearned model. For discrete Bayesian networks, the canonical choice for a\nposterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginal\nlikelihood with a uniform (U) graph prior (Heckerman et al., 1995). Its\nfavourable theoretical properties descend from assuming a uniform prior both on\nthe space of the network structures and on the space of the parameters of the\nnetwork. In this paper, we revisit the limitations of these assumptions; and we\nintroduce an alternative set of assumptions and the resulting score: the\nBayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with a\nmarginal uniform (MU) graph prior. We evaluate its performance in an extensive\nsimulation study, showing that MU+BDs is more accurate than U+BDeu both in\nlearning the structure of the network and in predicting new observations, while\nnot being computationally more complex to estimate.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 16:44:05 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 11:24:40 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 16:21:54 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1605.03952", "submitter": "Mattia Rigotti", "authors": "Mattia Rigotti and Stefano Fusi", "title": "Estimating the dimensionality of neural responses with fMRI Repetition\n  Suppression", "comments": "Appears in Proceedings of the 5th NIPS Workshop on Machine Learning\n  and Interpretation in Neuroimaging, Montreal, 2015", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/10", "categories": "q-bio.QM q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method that exploits fMRI Repetition Suppression (RS-fMRI)\nto measure the dimensionality of the set response vectors, i.e. the dimension\nof the space of linear combinations of neural population activity patterns in\nresponse to specific task conditions. RS-fMRI measures the overlap between\nresponse vectors even in brain areas displaying no discernible average\ndifferential BOLD signal. We show how this property can be used to estimate the\nneural response dimensionality in areas lacking macroscopic spatial patterning.\nThe importance of dimensionality derives from how it relates to a neural\ncircuit's functionality. As we show, the dimensionality of the response vectors\nis predicted to be high in areas involved in multi-stream integration, while it\nis low in areas where inputs from independent sources do not interact or merely\noverlap linearly. Our method can be used to identify and functionally\ncharacterize cortical circuits that integrate multiple independent information\npathways.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 19:44:05 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Rigotti", "Mattia", ""], ["Fusi", "Stefano", ""]]}, {"id": "1605.03992", "submitter": "Brian Segal", "authors": "Brian Segal, Thomas Braun, Michael Elliott, Hui Jiang", "title": "Fast Approximation of Small p-values in Permutation Tests by\n  Partitioning the Permutations", "comments": "64 pages, 34 figures, 12 tables including appendices (22 pages, 8\n  figures, 1 table not including appendices)", "journal-ref": "Biometrics. 74 (2018) 196-206", "doi": "10.1111/biom.12731", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in genetics and other life sciences commonly use permutation\ntests to evaluate differences between groups. Permutation tests have desirable\nproperties, including exactness if data are exchangeable, and are applicable\neven when the distribution of the test statistic is analytically intractable.\nHowever, permutation tests can be computationally intensive. We propose both an\nasymptotic approximation and a resampling algorithm for quickly estimating\nsmall permutation p-values (e.g. $<10^{-6}$) for the difference and ratio of\nmeans in two-sample tests. Our methods are based on the distribution of test\nstatistics within and across partitions of the permutations, which we define.\nIn this article, we present our methods and demonstrate their use through\nsimulations and an application to cancer genomic data. Through simulations, we\nfind that our resampling algorithm is more computationally efficient than\nanother leading alternative, particularly for extremely small p-values (e.g.\n$<10^{-30}$). Through application to cancer genomic data, we find that our\nmethods can successfully identify up- and down-regulated genes. While we focus\non the difference and ratio of means, we speculate that our approaches may work\nin other settings.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 21:13:03 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 22:46:32 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Segal", "Brian", ""], ["Braun", "Thomas", ""], ["Elliott", "Michael", ""], ["Jiang", "Hui", ""]]}, {"id": "1605.04049", "submitter": "James Wilson", "authors": "James D. Wilson, Nathaniel T. Stevens, William H. Woodall", "title": "Modeling and detecting change in temporal networks via a dynamic degree\n  corrected stochastic block model", "comments": "27 pages, 7 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications it is of interest to identify anomalous behavior within\na dynamic interacting system. Such anomalous interactions are reflected by\nstructural changes in the network representation of the system. We propose and\ninvestigate the use of a dynamic version of the degree corrected stochastic\nblock model (DCSBM) to model and monitor dynamic networks that undergo a\nsignificant structural change. We apply statistical process monitoring\ntechniques to the estimated parameters of the DCSBM to identify significant\nstructural changes in the network. Application of our surveillance strategy to\nthe dynamic U.S. Senate co-voting network reveals that we are able to detect\nsignificant changes in the network that reflect both times of cohesion and\ntimes of polarization among Republican and Democratic party members. These\nfindings provide valuable insight about the evolution of the bipartisan\npolitical system in the United States. Our analysis demonstrates that the\ndynamic DCSBM monitoring procedure effectively detects local and global\nstructural changes in dynamic networks. The DCSBM approach is an example of a\nmore general framework that combines parametric random graph models and\nstatistical process monitoring techniques for network surveillance.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 05:45:38 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 18:19:48 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Wilson", "James D.", ""], ["Stevens", "Nathaniel T.", ""], ["Woodall", "William H.", ""]]}, {"id": "1605.04055", "submitter": "Florian Heinrichs", "authors": "Maria Konstantinou and Holger Dette", "title": "Bayesian $D$-optimal designs for error-in-variables models", "comments": "Keywords: error-in-variables models, classical errors, Bayesian\n  optimal designs, D-optimality AMS Subject Classification: 62K05", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimality criteria provide a robust design strategy to parameter\nmisspecification. We develop an approximate design theory for Bayesian\n$D$-optimality for non-linear regression models with covariates subject to\nmeasurement errors. Both maximum likelihood and least squares estimation are\nstudied and explicit characterisations of the Bayesian $D$-optimal saturated\ndesigns for the Michaelis-Menten, Emax and exponential regression models are\nprovided. Several data examples are considered for the case of no preference\nfor specific parameter values, where Bayesian $D$-optimal saturated designs are\ncalculated using the uniform prior and compared to several other designs,\nincluding the corresponding locally $D$-optimal designs, which are often used\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 06:16:33 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Konstantinou", "Maria", ""], ["Dette", "Holger", ""]]}, {"id": "1605.04078", "submitter": "Heidi Seibold", "authors": "Heidi Seibold, Achim Zeileis, Torsten Hothorn", "title": "Model-based Recursive Partitioning for Subgroup Analyses", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": "10.1515/ijb-2015-0032", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of patient subgroups with differential treatment effects\nis the first step towards individualised treatments. A current draft guideline\nby the EMA discusses potentials and problems in subgroup analyses and\nformulated challenges to the development of appropriate statistical procedures\nfor the data-driven identification of patient subgroups. We introduce\nmodel-based recursive partitioning as a procedure for the automated detection\nof patient subgroups that are identifiable by predictive factors. The method\nstarts with a model for the overall treatment effect as defined for the primary\nanalysis in the study protocol and uses measures for detecting parameter\ninstabilities in this treatment effect. The procedure produces a segmented\nmodel with differential treatment parameters corresponding to each patient\nsubgroup. The subgroups are linked to predictive factors by means of a decision\ntree. The method is applied to the search for subgroups of patients suffering\nfrom amyotrophic lateral sclerosis that differ with respect to their Riluzole\ntreatment effect, the only currently approved drug for this disease.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 08:08:04 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 06:54:41 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Seibold", "Heidi", ""], ["Zeileis", "Achim", ""], ["Hothorn", "Torsten", ""]]}, {"id": "1605.04212", "submitter": "Julie Josse", "authors": "William Fithian and Julie Josse", "title": "Multiple Correspondence Analysis & the Multilogit Bilinear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Correspondence Analysis (MCA) is a dimension reduction method which\nplays a large role in the analysis of tables with categorical nominal variables\nsuch as survey data. Though it is usually motivated and derived using geometric\nconsiderations, in fact we prove that it amounts to a single proximal Newtown\nstep of a natural bilinear exponential family model for categorical data the\nmultinomial logit bilinear model. We compare and contrast the behavior of MCA\nwith that of the model on simulations and discuss new insights on the\nproperties of both exploratory multivariate methods and their cognate models.\nOne main conclusion is that we could recommend to approximate the multilogit\nmodel parameters using MCA. Indeed, estimating the parameters of the model is\nnot a trivial task whereas MCA has the great advantage of being easily solved\nby singular value decomposition and scalable to large data.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 15:22:47 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Fithian", "William", ""], ["Josse", "Julie", ""]]}, {"id": "1605.04358", "submitter": "Anru Zhang", "authors": "T. Tony Cai and Anru Zhang", "title": "Minimax Rate-optimal Estimation of High-dimensional Covariance Matrices\n  with Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Missing data occur frequently in a wide range of applications. In this paper,\nwe consider estimation of high-dimensional covariance matrices in the presence\nof missing observations under a general missing completely at random model in\nthe sense that the missingness is not dependent on the values of the data.\nBased on incomplete data, estimators for bandable and sparse covariance\nmatrices are proposed and their theoretical and numerical properties are\ninvestigated.\n  Minimax rates of convergence are established under the spectral norm loss and\nthe proposed estimators are shown to be rate-optimal under mild regularity\nconditions. Simulation studies demonstrate that the estimators perform well\nnumerically. The methods are also illustrated through an application to data\nfrom four ovarian cancer studies. The key technical tools developed in this\npaper are of independent interest and potentially useful for a range of related\nproblems in high-dimensional statistical inference with missing data.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 00:48:38 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1605.04391", "submitter": "Virginie Ollier", "authors": "Virginie Ollier, R\\'emy Boyer, Mohammed Nabil El Korso, Pascal\n  Larzabal", "title": "Bayesian Lower Bounds for Dense or Sparse (Outlier) Noise in the RMT\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation is an important and timely research subject. In this paper,\nwe investigate performance lower bounds on the mean-square-error (MSE) of any\nestimator for the Bayesian linear model, corrupted by a noise distributed\naccording to an i.i.d. Student's t-distribution. This class of prior\nparametrized by its degree of freedom is relevant to modelize either dense or\nsparse (accounting for outliers) noise. Using the hierarchical Normal-Gamma\nrepresentation of the Student's t-distribution, the Van Trees' Bayesian\nCram\\'er-Rao bound (BCRB) on the amplitude parameters is derived. Furthermore,\nthe random matrix theory (RMT) framework is assumed, i.e., the number of\nmeasurements and the number of unknown parameters grow jointly to infinity with\nan asymptotic finite ratio. Using some powerful results from the RMT,\nclosed-form expressions of the BCRB are derived and studied. Finally, we\npropose a framework to fairly compare two models corrupted by noises with\ndifferent degrees of freedom for a fixed common target signal-to-noise ratio\n(SNR). In particular, we focus our effort on the comparison of the BCRBs\nassociated with two models corrupted by a sparse noise promoting outliers and a\ndense (Gaussian) noise, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 08:31:57 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 17:20:49 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 13:12:23 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Ollier", "Virginie", ""], ["Boyer", "R\u00e9my", ""], ["Korso", "Mohammed Nabil El", ""], ["Larzabal", "Pascal", ""]]}, {"id": "1605.04446", "submitter": "Bodhisattva Sen", "authors": "Moulinath Banerjee and Cecile Durot and Bodhisattva Sen", "title": "Divide and Conquer in Non-standard Problems and the Super-efficiency\n  Phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how the divide and conquer principle --- partition the available\ndata into subsamples, compute an estimate from each subsample and combine these\nappropriately to form the final estimator --- works in non-standard problems\nwhere rates of convergence are typically slower than $\\sqrt{n}$ and limit\ndistributions are non-Gaussian, with a special emphasis on the least squares\nestimator (and its inverse) of a monotone regression function. We find that the\npooled estimator, obtained by averaging non-standard estimates across the\nmutually exclusive subsamples, outperforms the non-standard estimator based on\nthe entire sample in the sense of pointwise inference. We also show that, under\nappropriate conditions, if the number of subsamples is allowed to increase at\nappropriate rates, the pooled estimator is asymptotically normally distributed\nwith a variance that is empirically estimable from the subsample-level\nestimates. Further, in the context of monotone function estimation we show that\nthis gain in pointwise efficiency comes at a price --- the pooled estimator's\nperformance, in a uniform sense (maximal risk) over a class of models worsens\nas the number of subsamples increases, leading to a version of the\nsuper-efficiency phenomenon. In the process, we develop analytical results for\nthe order of the bias in isotonic regression, which are of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 18:09:10 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 09:01:47 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 07:58:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Banerjee", "Moulinath", ""], ["Durot", "Cecile", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1605.04565", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi and Alessandro Rinaldo", "title": "Hierarchical Models for Independence Structures of Networks", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of network models, called hierarchical network\nmodels, that allow us to represent in an explicit manner the stochastic\ndependence among the dyads (random ties) of the network. In particular, each\nmember of this family can be associated with a graphical model defining\nconditional independence clauses among the dyads of the network, called the\ndependency graph. Every network model with dyadic independence assumption can\nbe generalized to construct members of this new family. Using this new\nframework, we generalize the Erd\\\"os-R\\'enyi and beta-models to create\nhierarchical Erd\\\"os-R\\'enyi and beta-models. We describe various methods for\nparameter estimation as well as simulation studies for models with sparse\ndependency graphs.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 15:29:40 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 01:46:24 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sadeghi", "Kayvan", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1605.04772", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko", "title": "A Note on Efficient Performance Evaluation of the Cumulative Sum Chart\n  and the Sequential Probability Ratio Test", "comments": "15 pages, accepted for publication in Applied Stochastic Models in\n  Business and Industry", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a simple connection between certain in-control characteristics\nof the CUSUM Run Length and their out-of-control counterparts. The connection\nis in the form of paired integral (renewal) equations. The derivation exploits\nWald's likelihood ratio identity and the well-known fact that the CUSUM chart\nis equivalent to repetitive application of Wald's SPRT. The characteristics\nconsidered include the entire Run Length distribution and all of the\ncorresponding moments, starting from the zero-state ARL. A particular practical\nbenefit of our result is that it enables the in- and out-of-control\ncharacteristics of the CUSUM Run Length to be computed concurrently. Moreover,\ndue to the equivalence of the CUSUM chart to a sequence of SPRTs, the ASN and\nOC functions of an SPRT under the null and under the alternative can all be\ncomputed simultaneously as well. This would double up the efficiency of any\nnumerical method one may choose to devise to carry out the actual computations.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 13:56:53 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Polunchenko", "Aleksey S.", ""]]}, {"id": "1605.05278", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski and Donald B. Percival", "title": "Exact Simulation of Noncircular or Improper Complex-Valued Stationary\n  Gaussian Processes using Circulant Embedding", "comments": "Link to published version:\n  http://ieeexplore.ieee.org/document/7738840/", "journal-ref": "2016 IEEE 26th International Workshop on Machine Learning for\n  Signal Processing (MLSP)", "doi": "10.1109/MLSP.2016.7738840", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an algorithm for simulating improper (or noncircular)\ncomplex-valued stationary Gaussian processes. The technique utilizes recently\ndeveloped methods for multivariate Gaussian processes from the circulant\nembedding literature. The method can be performed in $\\mathcal{O}(n\\log_2 n)$\noperations, where $n$ is the length of the desired sequence. The method is\nexact, except when eigenvalues of prescribed circulant matrices are negative.\nWe evaluate the performance of the algorithm empirically, and provide a\npractical example where the method is guaranteed to be exact for all $n$, with\nan improper fractional Gaussian noise process.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 18:22:20 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 17:06:15 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Percival", "Donald B.", ""]]}, {"id": "1605.05309", "submitter": "Linbo Wang", "authors": "Linbo Wang, Xiao-Hua Zhou and Thomas S. Richardson", "title": "Identification and Estimation of Causal Effects with Outcomes Truncated\n  by Death", "comments": "Correct a typo in equation (8)", "journal-ref": "Biometrika 104.3 (2017): 597-612", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in medical studies that the outcome of interest is truncated by\ndeath, meaning that a subject has died before the outcome could be measured. In\nthis case, restricted analysis among survivors may be subject to selection\nbias. Hence, it is of interest to estimate the survivor average causal effect,\ndefined as the average causal effect among the subgroup consisting of subjects\nwho would survive under either exposure. In this paper, we consider the\nidentification and estimation problems of the survivor average causal effect.\nWe propose to use a substitution variable in place of the latent membership in\nthe always-survivor group. The identification conditions required for a\nsubstitution variable are conceptually similar to conditions for a conditional\ninstrumental variable, and may apply to both randomized and observational\nstudies. We show that the survivor average causal effect is identifiable with\nuse of such a substitution variable, and propose novel model parameterizations\nfor estimation of the survivor average causal effect under our identification\nassumptions. Our approaches are illustrated via simulation studies and a data\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 19:56:58 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 15:27:53 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Wang", "Linbo", ""], ["Zhou", "Xiao-Hua", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1605.05397", "submitter": "Geoff Boeing", "authors": "Geoff Boeing and Paul Waddell", "title": "New Insights into Rental Housing Markets across the United States: Web\n  Scraping and Analyzing Craigslist Rental Listings", "comments": "20 pages, 9 figures, Journal of Planning Education and Research.\n  2016. Online first", "journal-ref": null, "doi": "10.1177/0739456X16664789", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current sources of data on rental housing - such as the census or commercial\ndatabases that focus on large apartment complexes - do not reflect recent\nmarket activity or the full scope of the U.S. rental market. To address this\ngap, we collected, cleaned, analyzed, mapped, and visualized 11 million\nCraigslist rental housing listings. The data reveal fine-grained spatial and\ntemporal patterns within and across metropolitan housing markets in the U.S. We\nfind some metropolitan areas have only single-digit percentages of listings\nbelow fair market rent. Nontraditional sources of volunteered geographic\ninformation offer planners real-time, local-scale estimates of rent and housing\ncharacteristics currently lacking in alternative sources, such as census data.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 23:29:58 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 00:02:15 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 03:31:20 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 06:09:52 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Boeing", "Geoff", ""], ["Waddell", "Paul", ""]]}, {"id": "1605.05429", "submitter": "John Snyder", "authors": "Patrick McDermott, John Snyder, Rebecca Willison", "title": "Methods for Bayesian Variable Selection with Binary Response Data using\n  the EM Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional Bayesian variable selection problems are often solved using\ncomputationally expensive Markov Chain Montle Carlo (MCMC) techniques.\nRecently, a Bayesian variable selection technique was developed for continuous\ndata using the EM algorithm called EMVS. We extend the EMVS method to binary\ndata by proposing both a logistic and probit extension. To preserve the\ncomputational speed of EMVS we also implemented the Stochastic Dual Coordinate\nDescent (SDCA) algorithm. Further, we conduct two extensive simulation studies\nto show the computational speed of both methods. These simulation studies\nreveal the power of both methods to quickly identify the correct sparse model.\nWhen these EMVS methods are compared to Stochastic Search Variable Selection\n(SSVS), the EMVS methods surpass SSVS both in terms of computational speed and\ncorrectly identifying significant variables. Finally, we illustrate the\neffectiveness of both methods on two well-known gene expression datasets. Our\nresults mirror the results of previous examinations of these datasets with far\nless computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 04:01:05 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["McDermott", "Patrick", ""], ["Snyder", "John", ""], ["Willison", "Rebecca", ""]]}, {"id": "1605.05454", "submitter": "Takashi Goda", "authors": "Takashi Goda", "title": "Computing the variance of a conditional expectation via non-nested Monte\n  Carlo", "comments": null, "journal-ref": "Operations Research Letters, Volume 45, Issue 1, 63-67, 2017", "doi": "10.1016/j.orl.2016.12.002", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the variance of a conditional expectation has often been of\nimportance in uncertainty quantification. Sun et al. has introduced an unbiased\nnested Monte Carlo estimator, which they call $1\\frac{1}{2}$-level simulation\nsince the optimal inner-level sample size is bounded as the computational\nbudget increases. In this letter we construct unbiased non-nested Monte Carlo\nestimators based on the so-called pick-freeze scheme due to Sobol'. An\nextension of our approach to compute higher order moments of a conditional\nexpectation is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 06:36:23 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 05:00:34 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Goda", "Takashi", ""]]}, {"id": "1605.05537", "submitter": "Jean-Michel Marin", "authors": "Louis Raynal, Jean-Michel Marin, Pierre Pudlo, Mathieu Ribatet,\n  Christian P. Robert and Arnaud Estoup", "title": "ABC random forests for Bayesian parameter inference", "comments": "Main text: 24 pages, 6 figures Supplementary Information: 14 pages, 5\n  figures", "journal-ref": null, "doi": "10.24072/pci.evolbiol.100036", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This preprint has been reviewed and recommended by Peer Community In\nEvolutionary Biology (http://dx.doi.org/10.24072/pci.evolbiol.100036).\nApproximate Bayesian computation (ABC) has grown into a standard methodology\nthat manages Bayesian inference for models associated with intractable\nlikelihood functions. Most ABC implementations require the preliminary\nselection of a vector of informative statistics summarizing raw data.\nFurthermore, in almost all existing implementations, the tolerance level that\nseparates acceptance from rejection of simulated parameter values needs to be\ncalibrated. We propose to conduct likelihood-free Bayesian inferences about\nparameters with no prior selection of the relevant components of the summary\nstatistics and bypassing the derivation of the associated tolerance level. The\napproach relies on the random forest methodology of Breiman (2001) applied in a\n(non parametric) regression setting. We advocate the derivation of a new random\nforest for each component of the parameter vector of interest. When compared\nwith earlier ABC solutions, this method offers significant gains in terms of\nrobustness to the choice of the summary statistics, does not depend on any type\nof tolerance level, and is a good trade-off in term of quality of point\nestimator precision and credible interval estimations for a given computing\ntime. We illustrate the performance of our methodological proposal and compare\nit with earlier ABC methods on a Normal toy example and a population genetics\nexample dealing with human population evolution. All methods designed here have\nbeen incorporated in the R package abcrf (version 1.7) available on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 12:04:38 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 16:28:19 GMT"}, {"version": "v3", "created": "Tue, 4 Jul 2017 14:36:45 GMT"}, {"version": "v4", "created": "Tue, 14 Nov 2017 15:15:23 GMT"}, {"version": "v5", "created": "Fri, 2 Nov 2018 14:17:46 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Raynal", "Louis", ""], ["Marin", "Jean-Michel", ""], ["Pudlo", "Pierre", ""], ["Ribatet", "Mathieu", ""], ["Robert", "Christian P.", ""], ["Estoup", "Arnaud", ""]]}, {"id": "1605.05602", "submitter": "Marco Bottone Dr", "authors": "Mauro Bernardi, Marco Bottone, Lea Petrella", "title": "Bayesian Robust Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Bayesian quantile regression relies on the Asymmetric Laplace\ndistribution (ALD) mainly because of its satisfactory empirical and theoretical\nperformances. However, the ALD displays medium tails and it is not suitable for\ndata characterized by strong deviations from the Gaussian hypothesis. In this\npaper, we propose an extension of the ALD Bayesian quantile regression\nframework to account for fat-tails using the Skew Exponential Power (SEP)\ndistribution. Beside having the $\\tau$-level quantile as parameter, the SEP\ndistribution has an additional key parameter governing the decay of the tails,\nmaking it attractive for robust modeling of conditional quantiles at different\nconfidence levels. Linear and Generalized Additive Models (GAM) with penalized\nspline are considered to show the flexibility of the SEP in the Bayesian\nquantile regression context. Lasso priors are considered in both cases to\naccount for shrinking parameters problem when the parameters space becomes\nwide. To implement the Bayesian inference we propose a new adaptive\nMetropolis--Hastings algorithm in the linear model and an adaptive Metropolis\nwithin Gibbs one in the GAM framework. Empirical evidence of the statistical\nproperties of the proposed SEP Bayesian quantile regression method is provided\nthrough several example based on simulated and real dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 14:42:47 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Bernardi", "Mauro", ""], ["Bottone", "Marco", ""], ["Petrella", "Lea", ""]]}, {"id": "1605.05658", "submitter": "Silvia Platoni", "authors": "Silvia Platoni, Laura Barbieri, Daniele Moro, and Paolo Sckokai", "title": "Heteroscedastic stratified two-way EC models of single equations and SUR\n  systems", "comments": "The paper has been submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A relevant issue in panel data estimation is heteroscedasticity, which often\noccurs when the sample is large and individual units are of varying size.\nFurthermore, many of the available panel data sets are unbalanced in nature,\nbecause of attrition or accretion, and micro-econometric models applied to\npanel data are frequently multi-equation models. This paper considers the\ngeneral least squares estimation of the heteroscedastic stratified two-way\nerror component (EC) models of both single equations and seemingly unrelated\nregressions (SUR) systems (with cross-equations restrictions) on unbalanced\npanel data. The derived heteroscedastic estimators of both single equations and\nSUR systems improve the estimation efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 16:56:58 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 15:01:31 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 15:29:05 GMT"}, {"version": "v4", "created": "Mon, 7 Aug 2017 12:55:11 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Platoni", "Silvia", ""], ["Barbieri", "Laura", ""], ["Moro", "Daniele", ""], ["Sckokai", "Paolo", ""]]}, {"id": "1605.05715", "submitter": "David Soave", "authors": "David Soave and Lei Sun", "title": "A Generalized Levene's Scale Test for Variance Heterogeneity in the\n  Presence of Sample Correlation and Group Uncertainty", "comments": "33 pages (plus 14 pages supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize Levene's test for variance (scale) heterogeneity between $k$\ngroups for more complex data, which includes sample correlation and group\nmembership uncertainty. Following a two-stage regression framework, we show\nthat least absolute deviation regression must be used in the stage 1 analysis\nto ensure a correct asymptotic $\\chi^2_{k-1}/(k-1)$ distribution of the\ngeneralized scale ($gS$) test statistic. We then show that the proposed $gS$\ntest is independent of the generalized location test, under the joint null\nhypothesis of no mean and no variance heterogeneity. Consequently, we\ngeneralize the recently proposed joint location-scale ($gJLS$) test valuable in\nsettings where there is an interaction effect, but one interacting variable is\nnot available. We evaluate the proposed method via an extensive simulation\nstudy, and two genetic association application studies.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 19:37:27 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Soave", "David", ""], ["Sun", "Lei", ""]]}, {"id": "1605.05910", "submitter": "Swati Chandna", "authors": "Swati Chandna, Andrew T. Walden", "title": "A Frequency Domain Test for Propriety of Complex-Valued Vector Time\n  Series", "comments": "13 pages, 3 figures. Methodology (stat.ME), Applications (stat.AP)", "journal-ref": null, "doi": "10.1109/TSP.2016.2639459", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a frequency domain approach to test the hypothesis that a\ncomplex-valued vector time series is proper, i.e., for testing whether the\nvector time series is uncorrelated with its complex conjugate. If the\nhypothesis is rejected, frequency bands causing the rejection will be\nidentified and might usefully be related to known properties of the physical\nprocesses. The test needs the associated spectral matrix which can be estimated\nby multitaper methods using, say, $K$ tapers. Standard asymptotic distributions\nfor the test statistic are of no use since they would require $K \\rightarrow\n\\infty,$ but, as $K$ increases so does resolution bandwidth which causes\nspectral blurring. In many analyses $K$ is necessarily kept small, and hence\nour efforts are directed at practical and accurate methodology for hypothesis\ntesting for small $K.$ Our generalized likelihood ratio statistic combined with\nexact cumulant matching gives very accurate rejection percentages and\noutperforms other methods. We also prove that the statistic on which the test\nis based is comprised of canonical coherencies arising from our complex-valued\nvector time series.Our methodology is demonstrated on ocean current data\ncollected at different depths in the Labrador Sea. Overall this work extends\nresults on propriety testing for complex-valued vectors to the complex-valued\nvector time series setting.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 12:12:33 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Chandna", "Swati", ""], ["Walden", "Andrew T.", ""]]}, {"id": "1605.05985", "submitter": "Masoud  Nasari", "authors": "Masoud M Nasari", "title": "Pivotal Quantities with Arbitrary Small Skewness", "comments": "34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present randomization methods to enhance the accuracy of the\ncentral limit theorem (CLT) based inferences about the population mean $\\mu$.\nWe introduce a broad class of randomized versions of the Student $t$-statistic,\nthe classical pivot for $\\mu$, that continue to possess the pivotal property\nfor $\\mu$ and their skewness can be made arbitrarily small, for each fixed\nsample size $n$. Consequently, these randomized pivots admit CLTs with smaller\nerrors. The randomization framework in this paper also provides an explicit\nrelation between the precision of the CLTs for the randomized pivots and the\nvolume of their associated confidence regions for the mean for both univariate\nand multivariate data. This property allows regulating the trade-off between\nthe accuracy and the volume of the randomized confidence regions discussed in\nthis paper.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 14:57:52 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Nasari", "Masoud M", ""]]}, {"id": "1605.06025", "submitter": "Christian Rohrbeck", "authors": "Christian Rohrbeck and Deborah Costain and Arnoldo Frigessi", "title": "Bayesian Spatial Monotonic Multiple Regression", "comments": "31 pages, 9 Figures", "journal-ref": null, "doi": "10.1093/biomet/asy019", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider monotonic, multiple regression for a set of contiguous regions\n(lattice data). The regression functions permissibly vary between regions and\nexhibit geographical structure. We develop new Bayesian non-parametric\nmethodology which allows for both continuous and discontinuous functional\nshapes and which are estimated using marked point processes and reversible jump\nMarkov Chain Monte Carlo techniques. Geographical dependency is incorporated by\na flexible prior distribution; the parametrisation allows the dependency to\nvary with functional level. The approach is tuned using Bayesian global\noptimization and cross-validation. Estimates enable variable selection,\nthreshold detection and prediction as well as the extrapolation of the\nregression function. Performance and flexibility of our approach is illustrated\nby simulation studies and an application to a Norwegian insurance data set.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 16:06:09 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Rohrbeck", "Christian", ""], ["Costain", "Deborah", ""], ["Frigessi", "Arnoldo", ""]]}, {"id": "1605.06064", "submitter": "Surojit Biswas", "authors": "Surojit Biswas", "title": "The latent logarithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count or non-negative data are often log transformed to improve\nheteroscedasticity and scaling. To avoid undefined values where the data are\nzeros, a small pseudocount (e.g. 1) is added across the dataset prior to\napplying the transformation. This pseudocount considers neither the measured\nobject's a priori abundance nor the confidence with which the measurement was\nmade, making this practice convenient but statistically unfounded. I introduce\nhere the latent logarithm, or lag. lag assumes that each observed measurement\nis a noisy realization of an unmeasured latent abundance. By taking the\nlogarithm of this learned latent abundance, which reflects both sampling\nconfidence/depth and the object's a priori abundance, lag provides a\nprobabilistically coherent, stable, and intuitive alternative to the\nquestionable, but conventional \"log($x$ + pseudocount).\"\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 17:41:58 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Biswas", "Surojit", ""]]}, {"id": "1605.06293", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "The performance of univariate goodness-of-fit tests for normality based\n  on the empirical characteristic function in large samples", "comments": "5 figures, 5 tables", "journal-ref": "Communications in Statistics: Simulation and Computation, 2018,\n  47(4), pp. 1146-1156", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An empirical power comparison is made between two tests based on the\nempirical characteristic function and some of the best performing tests for\nnormality. A simple normality test based on the empirical characteristic\nfunction calculated in a single point is shown to outperform the more\ncomplicated Epps-Pulley test and the frequentist tests included in the study in\nlarge samples.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 11:25:15 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 12:05:41 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["van Zyl", "J. Martin", ""]]}, {"id": "1605.06397", "submitter": "Dong Xi", "authors": "Dong Xi, Ekkehard Glimm, Willi Maurer, Frank Bretz", "title": "On weighted parametric tests", "comments": "10 pages, 1 figures", "journal-ref": null, "doi": "10.1002/bimj.201600233", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general framework for weighted parametric multiple test\nprocedures based on the closure principle. We utilize general weighting\nstrategies that can reflect complex study objectives and include many\nprocedures in the literature as special cases. The proposed weighted parametric\ntests bridge the gap between rejection rules using either adjusted significance\nlevels or adjusted $p$-values. This connection is possible by allowing\nintersection hypotheses to be tested at level smaller than $\\alpha$, which may\nbe needed for certain study considerations. For such cases we introduce a\nsubclass of exact $\\alpha$-level parametric tests which satisfy the consonance\nproperty. When only subsets of test statistics are correlated, a new procedure\nis proposed to fully utilize the parametric assumptions within each subset. We\nillustrate the proposed weighted parametric tests using a clinical trial\nexample.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 15:14:02 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Xi", "Dong", ""], ["Glimm", "Ekkehard", ""], ["Maurer", "Willi", ""], ["Bretz", "Frank", ""]]}, {"id": "1605.06416", "submitter": "Jisu Kim", "authors": "Jisu Kim, Yen-Chi Chen, Sivaraman Balakrishnan, Alessandro Rinaldo,\n  Larry Wasserman", "title": "Statistical Inference for Cluster Trees", "comments": "20 pages, 6 figures, accepted in Neural Information Processing\n  Systems (NIPS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cluster tree provides a highly-interpretable summary of a density function\nby representing the hierarchy of its high-density clusters. It is estimated\nusing the empirical tree, which is the cluster tree constructed from a density\nestimator. This paper addresses the basic question of quantifying our\nuncertainty by assessing the statistical significance of topological features\nof an empirical cluster tree. We first study a variety of metrics that can be\nused to compare different trees, analyze their properties and assess their\nsuitability for inference. We then propose methods to construct and summarize\nconfidence sets for the unknown true cluster tree. We introduce a partial\nordering on cluster trees which we use to prune some of the statistically\ninsignificant features of the empirical tree, yielding interpretable and\nparsimonious cluster trees. Finally, we illustrate the proposed methods on a\nvariety of synthetic examples and furthermore demonstrate their utility in the\nanalysis of a Graft-versus-Host Disease (GvHD) data set.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:04:01 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 03:00:06 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 17:12:00 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kim", "Jisu", ""], ["Chen", "Yen-Chi", ""], ["Balakrishnan", "Sivaraman", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1605.06566", "submitter": "Peng Ding", "authors": "Peng Ding, Avi Feller, Luke Miratrix", "title": "Decomposing Treatment Effect Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and characterizing treatment effect variation in randomized\nexperiments has become essential for going beyond the \"black box\" of the\naverage treatment effect. Nonetheless, traditional statistical approaches often\nignore or assume away such variation. In the context of randomized experiments,\nthis paper proposes a framework for decomposing overall treatment effect\nvariation into a systematic component explained by observed covariates and a\nremaining idiosyncratic component. Our framework is fully randomization-based,\nwith estimates of treatment effect variation that are entirely justified by the\nrandomization itself. Our framework can also account for noncompliance, which\nis an important practical complication. We make several contributions. First,\nwe show that randomization-based estimates of systematic variation are very\nsimilar in form to estimates from fully-interacted linear regression and two\nstage least squares. Second, we use these estimators to develop an omnibus test\nfor systematic treatment effect variation, both with and without noncompliance.\nThird, we propose an $R^2$-like measure of treatment effect variation explained\nby covariates and, when applicable, noncompliance. Finally, we assess these\nmethods via simulation studies and apply them to the Head Start Impact Study, a\nlarge-scale randomized experiment.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 00:48:22 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 17:28:06 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Ding", "Peng", ""], ["Feller", "Avi", ""], ["Miratrix", "Luke", ""]]}, {"id": "1605.06579", "submitter": "Eunice Kim", "authors": "Eunice J. Kim and Zhengyuan Zhu", "title": "One-dimensional Nonstationary Process Variance Function Estimation", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many spatial processes exhibit nonstationary features. We estimate a variance\nfunction from a single process observation where the errors are nonstationary\nand correlated. We propose a difference-based approach for a one-dimensional\nnonstationary process and develop a bandwidth selection method for smoothing,\ntaking into account the correlation in the errors. The estimation results are\ncompared to that of a local-likelihood approach proposed by Anderes and\nStein(2011). A simulation study shows that our method has a smaller integrated\nMSE, easily fixes the boundary bias problem, and requires far less computing\ntime than the likelihood-based method.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 03:23:16 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Kim", "Eunice J.", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1605.06585", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Abhilash Jha and Sanku Dey", "title": "Bayesian Analysis of Modified Weibull distribution under progressively\n  censored competing risk model", "comments": "17 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study bayesian analysis of Modified Weibull distribution\nunder progressively censored competing risk model. This study is made for\nprogressively censored data. We use deterministic scan Gibbs sampling combined\nwith slice sampling to generate from the posterior distribution. Posterior\ndistribution is formed by taking prior distribution as reference prior. A real\nlife data analysis is shown for illustrative purpose.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 04:42:44 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Jha", "Abhilash", ""], ["Dey", "Sanku", ""]]}, {"id": "1605.06718", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Arthur P. Guillaumin, Jonathan M.\n  Lilly, Jeffrey J. Early", "title": "The De-Biased Whittle Likelihood", "comments": "To appear shortly in Biometrika. Full published version includes\n  extensions of theory to non-Gaussian processes, and new simulation examples\n  with an AR(4) and non-Gaussian process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Whittle likelihood is a widely used and computationally efficient\npseudo-likelihood. However, it is known to produce biased parameter estimates\nfor large classes of models. We propose a method for de-biasing Whittle\nestimates for second-order stationary stochastic processes. The de-biased\nWhittle likelihood can be computed in the same $\\mathcal{O}(n\\log n)$\noperations as the standard approach. We demonstrate the superior performance of\nthe method in simulation studies and in application to a large-scale\noceanographic dataset, where in both cases the de-biased approach reduces bias\nby up to two orders of magnitude, achieving estimates that are close to exact\nmaximum likelihood, at a fraction of the computational cost. We prove that the\nmethod yields estimates that are consistent at an optimal convergence rate of\n$n^{-1/2}$, under weaker assumptions than standard theory, where we do not\nrequire that the power spectral density is continuous in frequency. We describe\nhow the method can be easily combined with standard methods of bias reduction,\nsuch as tapering and differencing, to further reduce bias in parameter\nestimates.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 00:47:52 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 23:38:15 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 15:39:43 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Guillaumin", "Arthur P.", ""], ["Lilly", "Jonathan M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "1605.06837", "submitter": "Jiguo Cao", "authors": "Jiguo Cao, Liangliang Wang, Zhongwen Huang, Junyi Gai, and Rongling Wu", "title": "Functional Mapping of Multiple Dynamic Traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological phenomena undergo developmental changes in time and space.\nFunctional mapping, which is aimed at mapping genes that affect developmental\npatterns, is instrumental for studying the genetic architecture of biological\nchanges. Often biological processes are mediated by a network of developmental\nand physiological components and, therefore, are better described by multiple\nphenotypes. In this article, we develop a multivariate model for functional\nmapping that can detect and characterize quantitative trait loci (QTLs) that\nsimultaneously control multiple dynamic traits. Because the true genotypes of\nQTLs are unknown, the measurements for the multiple dynamic traits are modeled\nusing a mixture distribution. The functional means of the multiple dynamic\ntraits are estimated using the nonparametric regression method, which avoids\nany parametric assumption on the functional means. We propose the profile\nlikelihood method to estimate the mixture model. A likelihood ratio test is\nexploited to test for the existence of pleiotropic effects on distinct but\ndevelopmentally correlated traits. A simulation study is implemented to\nillustrate the finite sample performance of our proposed method. We also\ndemonstrate our method by identifying QTLs that simultaneously control three\ndynamic traits of soybeans. The three dynamic traits are the time-course\nbiomass of the leaf, the stem, and the root of the whole soybean. The genetic\nlinkage map is constructed with 950 microsatellite markers. The new model can\naid in our comprehension of the genetic control mechanisms of complex dynamic\ntraits over time.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 19:13:28 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Cao", "Jiguo", ""], ["Wang", "Liangliang", ""], ["Huang", "Zhongwen", ""], ["Gai", "Junyi", ""], ["Wu", "Rongling", ""]]}, {"id": "1605.06995", "submitter": "Mijung Park", "authors": "Mijung Park, Jimmy Foulds, Kamalika Chaudhuri, Max Welling", "title": "DP-EM: Differentially Private Expectation Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iterative nature of the expectation maximization (EM) algorithm presents\na challenge for privacy-preserving estimation, as each iteration increases the\namount of noise needed. We propose a practical private EM algorithm that\novercomes this challenge using two innovations: (1) a novel moment perturbation\nformulation for differentially private EM (DP-EM), and (2) the use of two\nrecently developed composition methods to bound the privacy \"cost\" of multiple\nEM iterations: the moments accountant (MA) and zero-mean concentrated\ndifferential privacy (zCDP). Both MA and zCDP bound the moment generating\nfunction of the privacy loss random variable and achieve a refined tail bound,\nwhich effectively decrease the amount of additive noise. We present empirical\nresults showing the benefits of our approach, as well as similar performance\nbetween these two composition methods in the DP-EM setting for Gaussian mixture\nmodels. Our approach can be readily extended to many iterative learning\nalgorithms, opening up various exciting future directions.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 12:36:55 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 17:17:01 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Park", "Mijung", ""], ["Foulds", "Jimmy", ""], ["Chaudhuri", "Kamalika", ""], ["Welling", "Max", ""]]}, {"id": "1605.07072", "submitter": "Christian Mueller", "authors": "Christian L. M\\\"uller, Richard Bonneau, Zachary Kurtz", "title": "Generalized Stability Approach for Regularized Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting regularization parameters in penalized high-dimensional graphical\nmodels in a principled, data-driven, and computationally efficient manner\ncontinues to be one of the key challenges in high-dimensional statistics. We\npresent substantial computational gains and conceptual generalizations of the\nStability Approach to Regularization Selection (StARS), a state-of-the-art\ngraphical model selection scheme. Using properties of the Poisson-Binomial\ndistribution and convex non-asymptotic distributional modeling we propose lower\nand upper bounds on the StARS graph regularization path which results in\ngreatly reduced computational cost without compromising regularization\nselection. We also generalize the StARS criterion from single edge to induced\nsubgraph (graphlet) stability. We show that simultaneously requiring edge and\ngraphlet stability leads to superior graph recovery performance independent of\ngraph topology. These novel insights render Gaussian graphical model selection\na routine task on standard multi-core computers.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 16:08:46 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["M\u00fcller", "Christian L.", ""], ["Bonneau", "Richard", ""], ["Kurtz", "Zachary", ""]]}, {"id": "1605.07242", "submitter": "Natesh Pillai", "authors": "Joseph J. Lee, Laura Forastiere, Luke Miratrix, Natesh S. Pillai", "title": "More Powerful Multiple Testing in Randomized Experiments with\n  Non-Compliance", "comments": "To appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two common concerns raised in analyses of randomized experiments are (i)\nappropriately handling issues of non-compliance, and (ii) appropriately\nadjusting for multiple tests (e.g., on multiple outcomes or subgroups).\nAlthough simple intention-to-treat (ITT) and Bonferroni methods are valid in\nterms of type I error, they can each lead to a substantial loss of power; when\nemploying both simultaneously, the total loss may be severe. Alternatives exist\nto address each concern. Here we propose an analysis method for experiments\ninvolving both features that merges posterior predictive $p$-values for\ncomplier causal effects with randomization-based multiple comparisons\nadjustments; the results are valid familywise tests that are doubly\nadvantageous: more powerful than both those based on standard ITT statistics\nand those using traditional multiple comparison adjustments. The operating\ncharacteristics and advantages of our method are demonstrated through a series\nof simulated experiments and an analysis of the United States Job Training\nPartnership Act (JTPA) Study, where our methods lead to different conclusions\nregarding the significance of estimated JTPA effects.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 00:22:15 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Lee", "Joseph J.", ""], ["Forastiere", "Laura", ""], ["Miratrix", "Luke", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1605.07244", "submitter": "Zijian Guo", "authors": "Zijian Guo, Wanjie Wang, T. Tony Cai and Hongzhe Li", "title": "Optimal Estimation of Co-heritability in High-dimensional Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-heritability is an important concept that characterizes the genetic\nassociations within pairs of quantitative traits. There has been significant\nrecent interest in estimating the co-heritability based on data from the\ngenome-wide association studies (GWAS). This paper introduces two measures of\nco-heritability in the high-dimensional linear model framework, including the\ninner product of the two regression vectors and a normalized inner product by\ntheir lengths. Functional de-biased estimators (FDEs) are developed to estimate\nthese two co-heritability measures. In addition, estimators of quadratic\nfunctionals of the regression vectors are proposed. Both theoretical and\nnumerical properties of the estimators are investigated. In particular, minimax\nrates of convergence are established and the proposed estimators of the inner\nproduct, the quadratic functionals and the normalized inner product are shown\nto be rate-optimal. Simulation results show that the FDEs significantly\noutperform the naive plug-in estimates. The FDEs are also applied to analyze a\nyeast segregant data set with multiple traits to estimate heritability and\nco-heritability among the traits.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 00:35:55 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Guo", "Zijian", ""], ["Wang", "Wanjie", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1605.07439", "submitter": "Marko Laine", "authors": "Virpi Junttila and Marko Laine", "title": "Bayesian Principal Component Regression model with spatial effects for\n  forest inventory under small field sample size", "comments": null, "journal-ref": null, "doi": "10.1016/j.rse.2017.01.035", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing observations are extensively used for analysis of\nenvironmental variables. These variables often exhibit spatial correlation,\nwhich has to be accounted for in the calibration models used in predictions,\neither by direct modelling of the dependencies or by allowing for spatially\ncorrelated stochastic effects. Another feature in many remote sensing\ninstruments is that the derived predictor variables are highly correlated,\nwhich can lead to unnecessary model over-training and at worst, singularities\nin the estimates. Both of these affect the prediction accuracy, especially when\nthe training set for model calibration is small. To overcome these modelling\nchallenges, we present a general model calibration procedure for remotely\nsensed data and apply it to airborne laser scanning data for forest inventory.\nWe use a linear regression model that accounts for multicollinearity in the\npredictors by principal components and Bayesian regularization. It has a\nspatial random effect component for the spatial correlations that are not\nexplained by a simple linear model. An efficient Markov chain Monte Carlo\nsampling scheme is used to account for the uncertainty in all the model\nparameters. We tested the proposed model against several alternatives and it\noutperformed the other linear calibration models, especially when there were\nspatial effects, multicollinearity and the training set size was small.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 13:18:06 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 06:21:07 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2017 15:05:24 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Junttila", "Virpi", ""], ["Laine", "Marko", ""]]}, {"id": "1605.07514", "submitter": "Gino Bertrand Kpogbezan", "authors": "Gino B. Kpogbezan, Aad W. van der Vaart, Wessel N. van Wieringen,\n  Gwena\\\"el G.R. Leday and Mark A. van de Wiel", "title": "An empirical Bayes approach to network recovery using external knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of a high-dimensional network may benefit substantially from\nthe inclusion of prior knowledge on the network topology. In the case of gene\ninteraction networks such knowledge may come for instance from pathway\nrepositories like KEGG, or be inferred from data of a pilot study. The Bayesian\nframework provides a natural means of including such prior knowledge. Based on\na Bayesian Simultaneous Equation Model, we develop an appealing empirical Bayes\nprocedure which automatically assesses the relevance of the used prior\nknowledge. We use variational Bayes method for posterior densities\napproximation and compare its accuracy with that of Gibbs sampling strategy.\nOur method is computationally fast, and can outperform known competitors. In a\nsimulation study we show that accurate prior data can greatly improve the\nreconstruction of the network, but need not harm the reconstruction if wrong.\nWe demonstrate the benefits of the method in an analysis of gene expression\ndata from GEO. In particular, the edges of the recovered network have superior\nreproducibility (compared to that of competitors) over resampled versions of\nthe data.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 15:53:04 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Kpogbezan", "Gino B.", ""], ["van der Vaart", "Aad W.", ""], ["van Wieringen", "Wessel N.", ""], ["Leday", "Gwena\u00ebl G. R.", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "1605.07521", "submitter": "Giampiero Marra", "authors": "Giampiero Marra, Rosalba Radice", "title": "A Bivariate Copula Additive Model for Location, Scale and Shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rigby & Stasinopoulos (2005) introduced generalized additive models for\nlocation, scale and shape (GAMLSS) where the response distribution is not\nrestricted to belong to the exponential family and its parameters can be\nspecified as functions of additive predictors that allows for several types of\ncovariate effects (e.g., linear, non-linear, random and spatial effects). In\nmany empirical situations, however, modeling simultaneously two or more\nresponses conditional on some covariates can be of considerable relevance. In\nthis article, we extend the scope of GAMLSS by introducing a bivariate copula\nadditive model with continuous margins for location, scale and shape. The\nframework permits the copula dependence and marginal distribution parameters to\nbe estimated simultaneously and, like in GAMLSS, each parameter to be modeled\nusing an additive predictor. Parameter estimation is achieved within a\npenalized likelihood framework using a trust region algorithm with integrated\nautomatic multiple smoothing parameter selection. The proposed approach allows\nfor straightforward inclusion of potentially any parametric continuous marginal\ndistribution and copula function. The models can be easily used via the\ncopulaReg() function in the R package SemiParBIVProbit. The usefulness of the\nproposal is illustrated on two case studies (which use electricity price and\ndemand data, and birth records) and on simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 16:04:18 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Marra", "Giampiero", ""], ["Radice", "Rosalba", ""]]}, {"id": "1605.07646", "submitter": "Shengxin Zhu", "authors": "Shengxin Zhu, Tongxiang Gu, Xingping Liu", "title": "AIMS:Average Information Matrix Splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For linear mixed models with co-variance matrices which are not linearly\ndependent on variance component parameters, we prove that the average of the\nobserved information and the Fisher information can be split into two parts.\nThe essential part enjoys a simple and computational friendly formula, while\nthe other part which involves a lot of computations is a random zero matrix and\nthus is negligible.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 20:16:42 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 03:54:26 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 03:09:40 GMT"}, {"version": "v4", "created": "Fri, 8 May 2020 06:16:39 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zhu", "Shengxin", ""], ["Gu", "Tongxiang", ""], ["Liu", "Xingping", ""]]}, {"id": "1605.07657", "submitter": "Alexander Luedtke", "authors": "Alexander R. Luedtke and Mark J. van der Laan", "title": "Parametric-Rate Inference for One-Sided Differentiable Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose one has a collection of parameters indexed by a (possibly infinite\ndimensional) set. Given data generated from some distribution, the objective is\nto estimate the maximal parameter in this collection evaluated at this\ndistribution. This estimation problem is typically non-regular when the\nmaximizing parameter is non-unique, and as a result standard asymptotic\ntechniques generally fail in this case. We present a technique for developing\nparametric-rate confidence intervals for the quantity of interest in these\nnon-regular settings. We show that our estimator is asymptotically efficient\nwhen the maximizing parameter is unique so that regular estimation is possible.\nWe apply our technique to a recent example from the literature in which one\nwishes to report the maximal absolute correlation between a prespecified\noutcome and one of p predictors. The simplicity of our technique enables an\nanalysis of the previously open case where p grows with sample size.\nSpecifically, we only require that log(p) grows slower than the square root of\nn, where n is the sample size. We show that, unlike earlier approaches, our\nmethod scales to massive data sets: the point estimate and confidence intervals\ncan be constructed in O(np) time.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 20:53:06 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Luedtke", "Alexander R.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1605.07663", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee and Dylan S. Small", "title": "Estimating the Malaria Attributable Fever Fraction Accounting for\n  Parasites Being Killed by Fever and Measurement Error", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is a parasitic disease that is a major health problem in many\ntropical regions. The most characteristic symptom of malaria is fever. The\nfraction of fevers that are attributable to malaria, the malaria attributable\nfever fraction (MAFF), is an important public health measure for assessing the\neffect of malaria control programs and other purposes. Estimating the MAFF is\nnot straightforward because there is no gold standard diagnosis of a malaria\nattributable fever; an individual can have malaria parasites in her blood and a\nfever, but the individual may have developed partial immunity that allows her\nto tolerate the parasites and the fever is being caused by another infection.\nWe define the MAFF using the potential outcome framework for causal inference\nand show what assumptions underlie current estimation methods. Current\nestimation methods rely on an assumption that the parasite density is correctly\nmeasured. However, this assumption does not generally hold because (i) fever\nkills some parasites and (ii) the measurement of parasite density has\nmeasurement error. In the presence of these problems, we show current\nestimation methods do not perform well. We propose a novel maximum likelihood\nestimation method based on exponential family g-modeling. Under the assumption\nthat the measurement error mechanism and the magnitude of the fever killing\neffect are known, we show that our proposed method provides approximately\nunbiased estimates of the MAFF in simulation studies. A sensitivity analysis\ncan be used to assess the impact of different magnitudes of fever killing and\ndifferent measurement error mechanisms. We apply our proposed method to\nestimate the MAFF in Kilombero, Tanzania.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 21:12:53 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Lee", "Kwonsang", ""], ["Small", "Dylan S.", ""]]}, {"id": "1605.07689", "submitter": "Jason Lee", "authors": "Michael I. Jordan, Jason D. Lee, Yun Yang", "title": "Communication-Efficient Distributed Statistical Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Communication-efficient Surrogate Likelihood (CSL) framework for\nsolving distributed statistical inference problems. CSL provides a\ncommunication-efficient surrogate to the global likelihood that can be used for\nlow-dimensional estimation, high-dimensional regularized estimation and\nBayesian inference. For low-dimensional estimation, CSL provably improves upon\nnaive averaging schemes and facilitates the construction of confidence\nintervals. For high-dimensional regularized estimation, CSL leads to a\nminimax-optimal estimator with controlled communication cost. For Bayesian\ninference, CSL can be used to form a communication-efficient quasi-posterior\ndistribution that converges to the true posterior. This quasi-posterior\nprocedure significantly improves the computational efficiency of MCMC\nalgorithms even in a non-distributed setting. We present both theoretical\nanalysis and experiments to explore the properties of the CSL approximation.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 00:12:06 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 05:31:41 GMT"}, {"version": "v3", "created": "Sun, 6 Nov 2016 00:37:39 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Jordan", "Michael I.", ""], ["Lee", "Jason D.", ""], ["Yang", "Yun", ""]]}, {"id": "1605.07787", "submitter": "Peter Carbonetto", "authors": "Zhengrong Xing, Peter Carbonetto and Matthew Stephens", "title": "Flexible signal denoising via flexible empirical Bayes shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal denoising---also known as non-parametric regression---is often\nperformed through shrinkage estimation in a transformed (e.g., wavelet) domain;\nshrinkage in the transformed domain corresponds to smoothing in the original\ndomain. A key question in such applications is how much to shrink, or,\nequivalently, how much to smooth. Empirical Bayes shrinkage methods provide an\nattractive solution to this problem; they use the data to estimate a\ndistribution of underlying \"effects\", hence automatically select an appropriate\namount of shrinkage. However, most existing implementations of Empirical Bayes\nshrinkage are less flexible than they could be--both in their assumptions on\nthe underlying distribution of effects, and in their ability to handle\nheterskedasticity---which limits their signal denoising applications. Here we\naddress this by taking a particularly flexible, stable and computationally\nconvenient Empirical Bayes shrinkage method, and we apply it to several signal\ndenoising problems. These applications include smoothing of Poisson data and\nheteroskedastic Gaussian data. We show through empirical comparisons that the\nresults are competitive with other methods, including both simple thresholding\nrules and purpose-built Empirical Bayes procedures. Our methods are implemented\nin the R package smashr, \"SMoothing by Adaptive SHrinkage in R,\" available at\nhttps://www.github.com/stephenslab/smashr\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 09:12:44 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 19:58:52 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 14:16:22 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Xing", "Zhengrong", ""], ["Carbonetto", "Peter", ""], ["Stephens", "Matthew", ""]]}, {"id": "1605.07811", "submitter": "Jon Cockayne", "authors": "Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami", "title": "Probabilistic Numerical Methods for Partial Differential Equations and\n  Bayesian Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a probabilistic numerical method for solution of partial\ndifferential equations (PDEs) and studies application of that method to\nPDE-constrained inverse problems. This approach enables the solution of\nchallenging inverse problems whilst accounting, in a statistically principled\nway, for the impact of discretisation error due to numerical solution of the\nPDE. In particular, the approach confers robustness to failure of the numerical\nPDE solver, with statistical inferences driven to be more conservative in the\npresence of substantial discretisation error. Going further, the problem of\nchoosing a PDE solver is cast as a problem in the Bayesian design of\nexperiments, where the aim is to minimise the impact of solver error on\nstatistical inferences; here the challenge of non-linear PDEs is also\nconsidered. The method is applied to parameter inference problems in which\ndiscretisation error in non-negligible and must be accounted for in order to\nreach conclusions that are statistically valid.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 10:22:19 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 08:55:10 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 15:22:34 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Cockayne", "Jon", ""], ["Oates", "Chris", ""], ["Sullivan", "Tim", ""], ["Girolami", "Mark", ""]]}, {"id": "1605.07913", "submitter": "Marianna Pensky", "authors": "Pawan Gupta and Marianna Pensky", "title": "Solution of linear ill-posed problems using random dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider application of overcomplete dictionaries to\nsolution of general ill-posed linear inverse problems. In the context of\nregression problems, there has been enormous amount of effort to recover an\nunknown function using such dictionaries. One of the most popular methods,\nlasso and its versions, is based on minimizing empirical likelihood and\nunfortunately, requires stringent assumptions on the dictionary, the, so\ncalled, compatibility conditions. Though compatibility conditions are hard to\nsatisfy, it is well known that this can be accomplished by using random\ndictionaries. In the present paper, we show how one can apply random\ndictionaries to solution of ill-posed linear inverse problems. We put a\ntheoretical foundation under the suggested methodology and study its\nperformance via simulations.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:51:50 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 19:42:16 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 01:35:05 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Gupta", "Pawan", ""], ["Pensky", "Marianna", ""]]}, {"id": "1605.07919", "submitter": "Joseph Guinness", "authors": "Joseph Guinness, Dorit Hammerling", "title": "Compression and Conditional Emulation of Climate Model Output", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2017.1395339", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical climate model simulations run at high spatial and temporal\nresolutions generate massive quantities of data. As our computing capabilities\ncontinue to increase, storing all of the data is not sustainable, and thus it\nis important to develop methods for representing the full datasets by smaller\ncompressed versions. We propose a statistical compression and decompression\nalgorithm based on storing a set of summary statistics as well as a statistical\nmodel describing the conditional distribution of the full dataset given the\nsummary statistics. The statistical model can be used to generate realizations\nrepresenting the full dataset, along with characterizations of the\nuncertainties in the generated data. Thus, the methods are capable of both\ncompression and conditional emulation of the climate models. Considerable\nattention is paid to accurately modeling the original dataset--one year of\ndaily mean temperature data--particularly with regard to the inherent spatial\nnonstationarity in global fields, and to determining the statistics to be\nstored, so that the variation in the original data can be closely captured,\nwhile allowing for fast decompression and conditional emulation on modest\ncomputers.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 15:00:10 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 14:43:08 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Guinness", "Joseph", ""], ["Hammerling", "Dorit", ""]]}, {"id": "1605.07947", "submitter": "Valeria Biignozzi", "authors": "Mauro Bernardi, Valeria Bignozzi and Lea Petrella", "title": "On the Lp-quantiles for the Student t distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L_p-quantiles represent an important class of generalised quantiles and are\ndefined as the minimisers of an expected asymmetric power function, see Chen\n(1996). For p=1 and p=2 they correspond respectively to the quantiles and the\nexpectiles. In his paper Koenker (1993) showed that the tau quantile and the\ntau expectile coincide for every tau in (0,1) for a class of rescaled Student t\ndistributions with two degrees of freedom. Here, we extend this result proving\nthat for the Student t distribution with p degrees of freedom, the tau quantile\nand the tau L_p-quantile coincide for every tau in (0,1) and the same holds for\nany affine transformation. Furthermore, we investigate the properties of\nL_p-quantiles and provide recursive equations for the truncated moments of the\nStudent t distribution.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 15:59:30 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Bernardi", "Mauro", ""], ["Bignozzi", "Valeria", ""], ["Petrella", "Lea", ""]]}, {"id": "1605.07981", "submitter": "Xueying Tang", "authors": "Xueying Tang, Xiaofan Xu, Malay Ghosh, Prasenjit Ghosh", "title": "Bayesian Variable Selection and Estimation Based on Global-Local\n  Shrinkage Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Bayesian variable selection problem of linear\nregression model with global-local shrinkage priors on the regression\ncoefficients. We propose a variable selection procedure that select a variable\nif the ratio of the posterior mean to the ordinary least square estimate of the\ncorresponding coefficient is greater than $1/2$. Under the assumption of\northogonal designs, we show that if the local parameters have polynomial-tailed\npriors, our proposed method enjoys the oracle property in the sense that it can\nachieve variable selection consistency and optimal estimation rate at the same\ntime. However, if, instead, an exponential-tailed prior is used for the local\nparameters, the proposed method does not have the oracle property.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 17:49:12 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Tang", "Xueying", ""], ["Xu", "Xiaofan", ""], ["Ghosh", "Malay", ""], ["Ghosh", "Prasenjit", ""]]}, {"id": "1605.08078", "submitter": "Simon Tindemans", "authors": "James R. Schofield, Simon H. Tindemans, Goran Strbac", "title": "A baseline-free method to identify responsive customers on dynamic\n  time-of-use tariffs", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic time-of-use tariffs incentivise changes in electricity consumption.\nThis paper presents a non-parametric method to retrospectively analyse\nconsumption data and quantify the significance of a customer's observed\nresponse to a dynamic price signal without constructing a baseline demand\nmodel. If data from a control group is available, this can be used to infer\ncustomer responsiveness - individually and collectively - on an absolute scale.\nThe results are illustrated using data from the Low Carbon London project,\nwhich included the UK's first dynamic time-of-use pricing trial.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 21:15:30 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Schofield", "James R.", ""], ["Tindemans", "Simon H.", ""], ["Strbac", "Goran", ""]]}, {"id": "1605.08328", "submitter": "Chris Oates", "authors": "Chris J. Oates, Jessica Kasza and Sach Mukherjee", "title": "Discussion of \"Causal inference using invariant prediction:\n  identification and confidence intervals\" by Peters, B\\\"uhlmann and\n  Meinshausen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contribution to the discussion of the paper \"Causal inference using invariant\nprediction: identification and confidence intervals\" by Peters, B\\\"uhlmann and\nMeinshausen, to appear in the Journal of the Royal Statistical Society, Series\nB.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 23:53:57 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Oates", "Chris J.", ""], ["Kasza", "Jessica", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1605.08441", "submitter": "Qiong Li", "authors": "Qiong Li, Xin Gao and Helene Massam", "title": "Approximate Bayesian estimation in large coloured graphical Gaussian\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed estimation methods have recently been used to compute the maximum\nlikelihood estimate of the precision matrix for large graphical Gaussian\nmodels. Our aim, in this paper, is to give a Bayesian estimate of the precision\nmatrix for large graphical Gaussian models with, additionally, symmetry\nconstraints imposed by an underlying graph which is coloured. We take the\nsample posterior mean of the precision matrix as our estimate. We study its\nasymptotic behaviour under the regular asymptotic regime when the number of\nvariables p is fixed and under the double asymptotic regime when both p and n\ngrow to infinity. We show in particular, that when the number of parameters of\nthe local models is uniformly bounded, the standard convergence rate we obtain\nfor the asymptotic consistency, in the Frobenius norm, of our estimate of the\nprecision matrix compares well with the rates in the current literature for the\nmaximum likelihood estimate.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 20:01:38 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Li", "Qiong", ""], ["Gao", "Xin", ""], ["Massam", "Helene", ""]]}, {"id": "1605.08466", "submitter": "Gourab Mukherjee", "authors": "Lawrence D. Brown, Gourab Mukherjee, Asaf Weinstein", "title": "Empirical Bayes Estimates for a 2-Way Cross-Classified Additive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an empirical Bayes procedure for estimating the cell means in an\nunbalanced, two-way additive model with fixed effects. We employ a hierarchical\nmodel, which reflects exchangeability of the effects within treatment and\nwithin block but not necessarily between them, as suggested before by Lindley\nand Smith (1972). The hyperparameters of this hierarchical model, instead of\nconsidered fixed, are to be substituted with data-dependent values in such a\nway that the point risk of the empirical Bayes estimator is small. Our method\nchooses the hyperparameters by minimizing an unbiased risk estimate and is\nshown to be asymptotically optimal for the estimation problem defined above.\nThe usual empirical Best Linear Unbiased Predictor (BLUP) is shown to be\nsubstantially different from the proposed method in the unbalanced case and\ntherefore performs sub-optimally. Our estimator is implemented through a\ncomputationally tractable algorithm that is scalable to work under large\ndesigns. The case of missing cell observations is treated as well. We\ndemonstrate the advantages of our method over the BLUP estimator through\nsimulations and in a real data example, where we estimate average nitrate\nlevels in water sources based on their locations and the time of the day.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 22:39:25 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Brown", "Lawrence D.", ""], ["Mukherjee", "Gourab", ""], ["Weinstein", "Asaf", ""]]}, {"id": "1605.08473", "submitter": "Roderick Edwards", "authors": "Rena K. Mann, Roderick Edwards, Julie Zhou", "title": "Robust designs for experiments with blocks", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For experiments running in field plots or over time, the observations are\noften correlated due to spatial or serial correlation, which leads to\ncorrelated errors in a linear model analyzing the treatment means. Without\nknowing the exact correlation matrix of the errors, it is not possible to\ncompute the generalized least squares estimator for the treatment means and use\nit to construct optimal designs for the experiments. In this paper we propose\nto use neighbourhoods to model the covariance matrix of the errors, and apply a\nmodified generalized least squares estimator to construct robust designs for\nexperiments with blocks. A minimax design criterion is investigated, and a\nsimulated annealing algorithm is developed to find robust designs. We have\nderived several theoretical results, and representative examples are presented.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 23:20:37 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Mann", "Rena K.", ""], ["Edwards", "Roderick", ""], ["Zhou", "Julie", ""]]}, {"id": "1605.08558", "submitter": "Rapha\\\"el de Fondeville", "authors": "Rapha\\\"el de Fondeville and Anthony C. Davison", "title": "High-dimensional peaks-over-threshold inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes are increasingly widely used for modelling complex\nextreme events, but existing fitting methods are computationally demanding,\nlimiting applications to a few dozen variables. $r$-Pareto processes are\nmathematically simpler and have the potential advantage of incorporating all\nrelevant extreme events, by generalizing the notion of a univariate exceedance.\nIn this paper we investigate score matching for performing high-dimensional\npeaks over threshold inference, focusing on extreme value processes associated\nto log-Gaussian random functions and discuss the behaviour of the proposed\nestimators for regularly-varying distributions with normalized marginals. Their\nperformance is assessed on grids with several hundred locations, simulating\nfrom both the true model and from its domain of attraction. We illustrate the\npotential and flexibility of our methods by modelling extreme rainfall on a\ngrid with $3600$ locations, based on risks for exceedances over local quantiles\nand for large spatially accumulated rainfall, and briefly discuss diagnostics\nof model fit. The differences between the two fitted models highlight the\nimportance of the choice of risk and its impact on the dependence structure.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 09:40:15 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 12:48:49 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["de Fondeville", "Rapha\u00ebl", ""], ["Davison", "Anthony C.", ""]]}, {"id": "1605.08701", "submitter": "Alastair Gregory", "authors": "Alastair Gregory and Colin Cotter", "title": "On the Calibration of Multilevel Monte Carlo Ensemble Forecasts", "comments": "6 pages, 2 figures. Accepted to Quarterly Journal of the Royal\n  Meteorological Society", "journal-ref": null, "doi": "10.1002/qj.3052", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel Monte Carlo can efficiently compute statistical estimates of\ndiscretized random variables, for a given error tolerance. Traditionally, only\na certain statistic is computed from a particular implementation of multilevel\nMonte Carlo. This paper considers the multilevel case when one wants to verify\nand evaluate a single ensemble that forms an empirical approximation to many\ndifferent statistics, namely an ensemble forecast. We propose a simple\nalgorithm that, in the univariate case, allows one to derive a statistically\nconsistent single ensemble forecast from the hierarchy of ensembles that are\nformed during an implementation of multilevel Monte Carlo. This ensemble\nforecast then allows the entire multilevel hierarchy of ensembles to be\nevaluated using standard ensemble forecast verification techniques. We\ndemonstrate the case of evaluating the calibration of the forecast in this\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 16:03:18 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 15:17:50 GMT"}, {"version": "v3", "created": "Wed, 29 Mar 2017 09:36:45 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Gregory", "Alastair", ""], ["Cotter", "Colin", ""]]}, {"id": "1605.08732", "submitter": "Yair Heller", "authors": "Yair Heller, Ruth Heller", "title": "Computing the Bergsma Dassios sign-covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bergsma and Dassios (2014) introduced an independence measure which is zero\nif and only if two random variables are independent. This measure can be\nnaively calculated in $O(n^4)$. Weihs et al. (2015) showed that it can be\ncalculated in $O(n^2 \\log n)$. In this note we will show that using the methods\ndescribed in Heller et al. (2016), the measure can easily be calculated in only\n$O(n^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 18:07:10 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Heller", "Yair", ""], ["Heller", "Ruth", ""]]}, {"id": "1605.08799", "submitter": "John Palowitch", "authors": "John Palowitch, Andrey Shabalin, Yihui Zhou, Andrew B. Nobel, Fred A.\n  Wright", "title": "Estimation of Interpretable eQTL Effect Sizes Using a Log of Linear\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of expression Quantitative Trait Loci (eQTL) is an important\nproblem in genomics and biomedicine. While detection (testing) of eQTL\nassociations has been widely studied, less work has been devoted to the\nestimation of eQTL effect size. To reduce false positives, detection methods\nfrequently rely on linear modeling of rank-based normalized or log-transformed\ngene expression data. Unfortunately, these approaches do not correspond to the\nsimplest model of eQTL action, and thus yield estimates of eQTL association\nthat can be uninterpretable and inaccurate. In this paper we propose a new,\nlog-of-linear model for eQTL action, termed ACME, that captures allelic\ncontributions to cis-acting eQTLs in an additive fashion, yielding effect size\nestimates that correspond to a biologically coherent model of cis-eQTLs. We\ndescribe a non-linear least-squares algorithm to fit the model by maximum\nlikelihood, and obtain corresponding $p$-values. We perform careful\ninvestigation of the model using a combination of simulated data and data from\nthe Genotype Tissue Expression (GTEx) project. Our results reveal little\nevidence for dominance effects, a parsimonious result that accords with a\nsimple biological model for allele-specific expression and supports use of the\nACME model. We show that Type-I error is well-controlled under our approach in\na realistic setting, so that rank-based normalizations are unnecessary.\nFurthermore, we show that such normalizations can be detrimental to power and\nestimation accuracy under the proposed model. We then provide summaries of ACME\neffect sizes for whole-genome cis-eQTLs in the GTEx data.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 20:48:02 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 04:11:56 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Palowitch", "John", ""], ["Shabalin", "Andrey", ""], ["Zhou", "Yihui", ""], ["Nobel", "Andrew B.", ""], ["Wright", "Fred A.", ""]]}, {"id": "1605.08824", "submitter": "Asaf Weinstein", "authors": "Snigdha Panigrahi, Jonathan Taylor, Asaf Weinstein", "title": "Integrative Methods for Post-Selection Inference Under Convex\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference after model selection has been an active research topic in the past\nfew years, with numerous works offering different approaches to addressing the\nperils of the reuse of data. In particular, major progress has been made\nrecently on large and useful classes of problems by harnessing general theory\nof hypothesis testing in exponential families, but these methods have their\nlimitations. Perhaps most immediate is the gap between theory and practice:\nimplementing the exact theoretical prescription in realistic situations---for\nexample, when new data arrives and inference needs to be adjusted\naccordingly---may be a prohibitive task. In this paper we propose a Bayesian\nframework for carrying out inference after model selection in the linear model.\nOur framework is very flexible in the sense that it naturally accommodates\ndifferent models for the data, instead of requiring a case-by-case treatment.\nAt the core of our methods is a new approximation to the exact likelihood\nconditional on selection, the latter being generally intractable. We prove\nthat, under appropriate conditions, our approximation is asymptotically\nconsistent with the exact truncated likelihood. The advantages of our methods\nin practical data analysis are demonstrated in simulations and in application\nto HIV drug-resistance data.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 00:59:00 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 03:22:13 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 20:25:36 GMT"}, {"version": "v4", "created": "Sun, 29 Sep 2019 06:26:25 GMT"}, {"version": "v5", "created": "Wed, 2 Oct 2019 06:13:16 GMT"}, {"version": "v6", "created": "Tue, 26 May 2020 06:06:11 GMT"}, {"version": "v7", "created": "Sat, 30 May 2020 17:50:40 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Taylor", "Jonathan", ""], ["Weinstein", "Asaf", ""]]}, {"id": "1605.08898", "submitter": "Huang Huang", "authors": "Huang Huang and Ying Sun", "title": "Hierarchical low rank approximation of likelihoods for large spatial\n  datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets in the fields of climate and environment are often very large and\nirregularly spaced. To model such datasets, the widely used Gaussian process\nmodels in spatial statis- tics face tremendous challenges due to the\nprohibitive computational burden. Various ap- proximation methods have been\nintroduced to reduce the computational cost. However, most of them rely on\nunrealistic assumptions for the underlying process and retaining statistical\nefficiency remains an issue. We develop a new approximation scheme for maximum\nlikeli- hood estimation. We show how the composite likelihood method can be\nadapted to provide different types of hierarchical low rank approximations that\nare both computationally and statistically efficient. The improvement of the\nproposed method is explored theoretically; the performance is investigated by\nnumerical and simulation studies; and the practicality is illustrated through\napplying our methods to 2 million measurements of soil moisture in the area of\nthe Mississippi River basin, which facilitates a better understanding of the\nclimate variability.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 14:44:08 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Huang", "Huang", ""], ["Sun", "Ying", ""]]}, {"id": "1605.08933", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Yinfei Kong and Daoji Li and Jinchi Lv", "title": "Interaction Pursuit with Feature Screening and Selection", "comments": "34 pages for the main text including 7 figures, 53 pages for the\n  Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how features interact with each other is of paramount\nimportance in many scientific discoveries and contemporary applications. Yet\ninteraction identification becomes challenging even for a moderate number of\ncovariates. In this paper, we suggest an efficient and flexible procedure,\ncalled the interaction pursuit (IP), for interaction identification in\nultra-high dimensions. The suggested method first reduces the number of\ninteractions and main effects to a moderate scale by a new feature screening\napproach, and then selects important interactions and main effects in the\nreduced feature space using regularization methods. Compared to existing\napproaches, our method screens interactions separately from main effects and\nthus can be more effective in interaction screening. Under a fairly general\nframework, we establish that for both interactions and main effects, the method\nenjoys the sure screening property in screening and oracle inequalities in\nselection. Our method and theoretical results are supported by several\nsimulation and real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 20:59:46 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Fan", "Yingying", ""], ["Kong", "Yinfei", ""], ["Li", "Daoji", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.08961", "submitter": "Anastasios Kyrillidis", "authors": "Megasthenis Asteris, Anastasios Kyrillidis, Oluwasanmi Koyejo, Russell\n  Poldrack", "title": "A simple and provable algorithm for sparse diagonal CCA", "comments": "To appear at ICML 2016, 14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT math.IT math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets of variables, derived from a common set of samples, sparse\nCanonical Correlation Analysis (CCA) seeks linear combinations of a small\nnumber of variables in each set, such that the induced canonical variables are\nmaximally correlated. Sparse CCA is NP-hard.\n  We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e.,\nsparse CCA under the additional assumption that variables within each set are\nstandardized and uncorrelated. Our algorithm operates on a low rank\napproximation of the input data and its computational complexity scales\nlinearly with the number of input variables. It is simple to implement, and\nparallelizable. In contrast to most existing approaches, our algorithm\nadministers precise control on the sparsity of the extracted canonical vectors,\nand comes with theoretical data-dependent global approximation guarantees, that\nhinge on the spectrum of the input data. Finally, it can be straightforwardly\nadapted to other constrained variants of CCA enforcing structure beyond\nsparsity.\n  We empirically evaluate the proposed scheme and apply it on a real\nneuroimaging dataset to investigate associations between brain activity and\nbehavior measurements.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 03:56:23 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Koyejo", "Oluwasanmi", ""], ["Poldrack", "Russell", ""]]}, {"id": "1605.08963", "submitter": "David Puelz", "authors": "David Puelz, P. Richard Hahn, Carlos Carvalho", "title": "Variable Selection in Seemingly Unrelated Regressions with Random\n  Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers linear model selection when the response is\nvector-valued and the predictors are randomly observed. We propose a new\napproach that decouples statistical inference from the selection step in a\n\"post-inference model summarization\" strategy. We study the impact of predictor\nuncertainty on the model selection procedure. The method is demonstrated\nthrough an application to asset pricing.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 04:12:13 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 16:06:09 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 21:20:22 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Puelz", "David", ""], ["Hahn", "P. Richard", ""], ["Carvalho", "Carlos", ""]]}, {"id": "1605.09000", "submitter": "Yangguang Zang", "authors": "Yangguang Zang, Yinjun Zhao, Qingzhao Zhang, Hao Chai, Sanguo Zhang\n  and Shuangge Ma", "title": "Identifying Gene-Environment Interactions with A Least Relative Error\n  Approach", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex diseases, the interactions between genetic and environmental risk\nfactors can have important implications beyond the main effects. Many of the\nexisting interaction analyses conduct marginal analysis and cannot accommodate\nthe joint effects of multiple main effects and interactions. In this study, we\nconduct joint analysis which can simultaneously accommodate a large number of\neffects. Significantly different from the existing studies, we adopt loss\nfunctions based on relative errors, which offer a useful alternative to the\n\"classic\" methods such as the least squares and least absolute deviation.\nFurther to accommodate censoring in the response variable, we adopt a weighted\napproach. Penalization is used for identification and regularized estimation.\nComputationally, we develop an effective algorithm which combines the\nmajorize-minimization and coordinate descent. Simulation shows that the\nproposed approach has satisfactory performance. We also analyze lung cancer\nprognosis data with gene expression measurements.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 13:29:28 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Zang", "Yangguang", ""], ["Zhao", "Yinjun", ""], ["Zhang", "Qingzhao", ""], ["Chai", "Hao", ""], ["Zhang", "Sanguo", ""], ["Ma", "Shuangge", ""]]}, {"id": "1605.09026", "submitter": "Juan-Pablo Ortega", "authors": "Lyudmila Grigoryeva and Juan-Pablo Ortega", "title": "Singular ridge regression with homoscedastic residuals: generalization\n  error with estimated parameters", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper characterizes the conditional distribution properties of the\nfinite sample ridge regression estimator and uses that result to evaluate total\nregression and generalization errors that incorporate the inaccuracies\ncommitted at the time of parameter estimation. The paper provides explicit\nformulas for those errors. Unlike other classical references in this setup, our\nresults take place in a fully singular setup that does not assume the existence\nof a solution for the non-regularized regression problem. In exchange, we\ninvoke a conditional homoscedasticity hypothesis on the regularized regression\nresiduals that is crucial in our developments.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 16:45:23 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1605.09101", "submitter": "Robert Kohn", "authors": "David Gunawan and Mohamad A. Khaled and Robert Kohn", "title": "Mixed Marginal Copula Modeling", "comments": "46 pages, 8 tables and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article extends the literature on copulas with discrete or continuous\nmarginals to the case where some of the marginals are a mixture of discrete and\ncontinuous components. We do so by carefully defining the likelihood as the\ndensity of the observations with respect to a mixed measure. The treatment is\nquite general, although we focus focus on mixtures of Gaussian and Archimedean\ncopulas. The inference is Bayesian with the estimation carried out by Markov\nchain Monte Carlo. We illustrate the methodology and algorithms by applying\nthem to estimate a multivariate income dynamics model.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 04:38:32 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 03:19:10 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 10:41:54 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Gunawan", "David", ""], ["Khaled", "Mohamad A.", ""], ["Kohn", "Robert", ""]]}, {"id": "1605.09107", "submitter": "Arthur Guillaumin", "authors": "Arthur P. Guillaumin and Adam M. Sykulski and Sofia C. Olhede and\n  Jeffrey J. Early and Jonathan M. Lilly", "title": "Analysis of nonstationary modulated time series with applications to\n  oceanographic flow measurements", "comments": "31 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.ao-ph physics.flu-dyn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of univariate nonstationary time series models, using\nthe framework of modulated time series, which is appropriate for the analysis\nof rapidly-evolving time series as well as time series observations with\nmissing data. We extend our techniques to a class of bivariate time series that\nare isotropic. Exact inference is often not computationally viable for time\nseries analysis, and so we propose an estimation method based on the\nWhittle-likelihood, a commonly adopted pseudo-likelihood. Our inference\nprocedure is shown to be consistent under standard assumptions, as well as\nhaving considerably lower computational cost than exact likelihood in general.\nWe show the utility of this framework for the analysis of drifting instruments,\nan analysis that is key to characterising global ocean circulation and\ntherefore also for decadal to century-scale climate understanding.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 05:12:02 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 11:40:31 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Guillaumin", "Arthur P.", ""], ["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Early", "Jeffrey J.", ""], ["Lilly", "Jonathan M.", ""]]}, {"id": "1605.09288", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp, Mijke Rhemtulla, Denny Borsboom", "title": "Generalized Network Psychometrics: Combining Network and Latent Variable\n  Models", "comments": "Published in Psychometrika", "journal-ref": null, "doi": "10.1007/s11336-017-9557-x", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the network model as a formal psychometric model,\nconceptualizing the covariance between psychometric indicators as resulting\nfrom pairwise interactions between observable variables in a network structure.\nThis contrasts with standard psychometric models, in which the covariance\nbetween test items arises from the influence of one or more common latent\nvariables. Here, we present two generalizations of the network model that\nencompass latent variable structures, establishing network modeling as parts of\nthe more general framework of Structural Equation Modeling (SEM). In the first\ngeneralization, we model the covariance structure of latent variables as a\nnetwork. We term this framework Latent Network Modeling (LNM) and show that,\nwith LNM, a unique structure of conditional independence relationships between\nlatent variables can be obtained in an explorative manner. In the second\ngeneralization, the residual variance-covariance structure of indicators is\nmodeled as a network. We term this generalization Residual Network Modeling\n(RNM) and show that, within this framework, identifiable models can be obtained\nin which local independence is structurally violated. These generalizations\nallow for a general modeling framework that can be used to fit, and compare,\nSEM models, network models, and the RNM and LNM generalizations. This\nmethodology has been implemented in the free-to-use software package lvnet,\nwhich contains confirmatory model testing as well as two exploratory search\nalgorithms: stepwise search algorithms for low-dimensional datasets and\npenalized maximum likelihood estimation for larger datasets. We show in\nsimulation studies that these search algorithms performs adequately in\nidentifying the structure of the relevant residual or latent networks. We\nfurther demonstrate the utility of these generalizations in an empirical\nexample on a personality inventory dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 15:50:43 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 20:46:17 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 23:25:00 GMT"}, {"version": "v4", "created": "Mon, 11 Sep 2017 20:47:58 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Epskamp", "Sacha", ""], ["Rhemtulla", "Mijke", ""], ["Borsboom", "Denny", ""]]}, {"id": "1605.09454", "submitter": "Guillaume Basse", "authors": "Guillaume W. Basse, Natesh S. Pillai and Aaron Smith", "title": "Parallel Markov Chain Monte Carlo via Spectral Clustering", "comments": "Appeared in Proceedings of the 19th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As it has become common to use many computer cores in routine applications,\nfinding good ways to parallelize popular algorithms has become increasingly\nimportant. In this paper, we present a parallelization scheme for Markov chain\nMonte Carlo (MCMC) methods based on spectral clustering of the underlying state\nspace, generalizing earlier work on parallelization of MCMC methods by state\nspace partitioning. We show empirically that this approach speeds up MCMC\nsampling for multimodal distributions and that it can be usefully applied in\ngreater generality than several related algorithms. Our algorithm converges\nunder reasonable conditions to an `optimal' MCMC algorithm. We also show that\nour approach can be asymptotically far more efficient than naive\nparallelization, even in situations such as completely flat target\ndistributions where no unique optimal algorithm exists. Finally, we combine\ntheoretical and empirical bounds to provide practical guidance on the choice of\ntuning parameters.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 00:40:09 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Basse", "Guillaume W.", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1605.09503", "submitter": "Pritam Ranjan", "authors": "Pritam Ranjan, Mark Thomas, Holger Teismann, Sujay Mukhoti", "title": "Inverse problem for time-series valued computer model via scalarization", "comments": "23 pages (submitted to OJS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an expensive to evaluate computer simulator, even the estimate of the\noverall surface can be a challenging problem. In this paper, we focus on the\nestimation of the inverse solution, i.e., to find the set(s) of input\ncombinations of the simulator that generates (or gives good approximation of) a\npre-determined simulator output. Ranjan et al. (2008) proposed an expected\nimprovement criterion under a sequential design framework for the inverse\nproblem with a scalar valued simulator. In this paper, we focus on the inverse\nproblem for a time-series valued simulator. We have used a few simulated and\ntwo real examples for performance comparison.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 06:00:24 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 06:08:23 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Ranjan", "Pritam", ""], ["Thomas", "Mark", ""], ["Teismann", "Holger", ""], ["Mukhoti", "Sujay", ""]]}, {"id": "1605.09700", "submitter": "Ali Akbar Jafari", "authors": "M. R. Kazemi and A. A. Jafari", "title": "Modified Signed Log-Likelihood Ratio Test for Comparing the Correlation\n  Coefficients of Two Independent Bivariate Normal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use the method of modified signed log-likelihood ratio test\nfor the problem of testing the equality of correlation coefficients in two\nindependent bivariate normal distributions. We compare this method with two\nother %competing approaches, Fisher's Z-transform and generalized test\nvariable, using a Monte Carlo simulation. It indicates that the proposed method\nis better than the other approaches, in terms of the actual sizes and powers\nespecially when the sample sizes are unequal. We illustrate performance of the\nproposed approach, using a real data set.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 16:22:54 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Kazemi", "M. R.", ""], ["Jafari", "A. A.", ""]]}, {"id": "1605.09773", "submitter": "Yuya Sasaki", "authors": "Harold D. Chiang and Yuya Sasaki", "title": "Causal Inference by Quantile Regression Kink Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantile regression kink design (QRKD) is proposed by empirical\nresearchers as a potential method to assess heterogeneous treatment effects\nunder suitable research designs, but its causal interpretation remains unknown.\nWe propose a causal interpretation of the QRKD estimand. Under flexible\nheterogeneity and endogeneity, the QRKD estimand measures a weighted average of\nheterogeneous marginal effects at respective conditional quantiles of outcome\ngiven a designed kink point. In addition, we develop weak convergence results\nfor the QRKD estimator as a local quantile process for the purpose of\nconducting statistical inference on heterogeneous treatment effects using the\nQRKD. Applying our methods to the Continuous Wage and Benefit History Project\n(CWBH) data, we find significantly heterogeneous positive causal effects of\nunemployment insurance benefits on unemployment durations in Louisiana between\n1981 and 1983. These effects are larger for individuals with longer\nunemployment durations.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 19:16:05 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 14:42:38 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 20:05:36 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Chiang", "Harold D.", ""], ["Sasaki", "Yuya", ""]]}]