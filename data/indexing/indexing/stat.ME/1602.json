[{"id": "1602.00047", "submitter": "Nicholas Johnson", "authors": "Nicholas A. Johnson, Frank O. Kuehnel, Ali Nasiri Amini", "title": "A Scalable Blocked Gibbs Sampling Algorithm For Gaussian And Poisson\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods are a popular technique in Bayesian\nstatistical modeling. They have long been used to obtain samples from posterior\ndistributions, but recent research has focused on the scalability of these\ntechniques for large problems. We do not develop new sampling methods but\ninstead describe a blocked Gibbs sampler which is sufficiently scalable to\naccomodate many interesting problems. The sampler we describe applies to a\nrestricted subset of the Generalized Linear Mixed-effects Models (GLMM's); this\nsubset includes Poisson and Gaussian regression models. The blocked Gibbs\nsampling steps jointly update a prior variance parameter along with all of the\nrandom effects underneath it. We also discuss extensions such as flexible prior\ndistributions.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 01:25:37 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Johnson", "Nicholas A.", ""], ["Kuehnel", "Frank O.", ""], ["Amini", "Ali Nasiri", ""]]}, {"id": "1602.00109", "submitter": "Yu-Hsiang Cheng", "authors": "Yu-Hsiang Cheng and Tzee-Ming Huang", "title": "A nonparametric copula density estimator incorporating information on\n  bivariate marginals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a copula density estimator that can include information on\nbivariate marginals when the information is available. We use B-splines for\ncopula density approximation and include information on bivariate marginals via\na penalty term. Our estimator satisfies the constraints for a copula density.\nUnder mild conditions, the proposed estimator is consistent.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 12:02:19 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Cheng", "Yu-Hsiang", ""], ["Huang", "Tzee-Ming", ""]]}, {"id": "1602.00158", "submitter": "Rebecca Wooten", "authors": "Rebecca D. Wooten", "title": "Introduction to Implicit Regression", "comments": "28 pages plus cover page with abstract and 2 pages of reference, 5\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians usually restrict regression to model relationships that are\nexplicitly defined dependent and independent random variables; this paper\noutlines the newly developed method of non-response analysis and rotational\nanalysis for evaluating co-dependent variables without an obvious subject\nresponse. The concepts outlined challenge the notion of fixed effects; unity is\nincluded as a random measure (variable) ignoring the assumption of independence\nand the degree of separation is outlined which is a measure of model quality.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 20:10:02 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Wooten", "Rebecca D.", ""]]}, {"id": "1602.00199", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen", "title": "Gaussian approximation for the sup-norm of high-dimensional\n  matrix-variate U-statistics and its applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the Gaussian approximation of high-dimensional and\nnon-degenerate U-statistics of order two under the supremum norm. We propose a\ntwo-step Gaussian approximation procedure that does not impose structural\nassumptions on the data distribution. Specifically, subject to mild moment\nconditions on the kernel, we establish the explicit rate of convergence that\ndecays polynomially in sample size for a high-dimensional scaling limit, where\nthe dimension can be much larger than the sample size. We also supplement a\npractical Gaussian wild bootstrap method to approximate the quantiles of the\nmaxima of centered U-statistics and prove its asymptotic validity. The wild\nbootstrap is demonstrated on statistical applications for high-dimensional\nnon-Gaussian data including: (i) principled and data-dependent tuning parameter\nselection for regularized estimation of the covariance matrix and its related\nfunctionals; (ii) simultaneous inference for the covariance and rank\ncorrelation matrices. In particular, for the thresholded covariance matrix\nestimator with the bootstrap selected tuning parameter, we show that the\nGaussian-like convergence rates can be achieved for heavy-tailed data, which\nare less conservative than those obtained by the Bonferroni technique that\nignores the dependency in the underlying data distribution. In addition, we\nalso show that even for subgaussian distributions, error bounds of the\nbootstrapped thresholded covariance matrix estimator can be much tighter than\nthose of the minimax estimator with a universal threshold.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 04:29:54 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 03:44:21 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2016 21:04:40 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Chen", "Xiaohui", ""]]}, {"id": "1602.00202", "submitter": "Georgi Dinolov", "authors": "Georgi Dinolov, Abel Rodriguez, and Hongyun Wang", "title": "Bayesian stochastic volatility models for high-frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate a discrete-time Bayesian stochastic volatility model for\nhigh-frequency stock-market data that directly accounts for microstructure\nnoise, and outline a Markov chain Monte Carlo algorithm for parameter\nestimation. The methods described in this paper are designed to be coherent\nacross all sampling timescales, with the goal of estimating the latent\nlog-volatility signal from data collected at arbitrarily short sampling\nperiods. In keeping with this goal, we carefully develop a method for eliciting\npriors. The empirical results derived from both simulated and real data show\nthat directly accounting for microstructure in a state-space formulation allows\nfor well-calibrated estimates of the log-volatility process driving prices.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 06:07:54 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Dinolov", "Georgi", ""], ["Rodriguez", "Abel", ""], ["Wang", "Hongyun", ""]]}, {"id": "1602.00207", "submitter": "Jonathan Friedman", "authors": "Jonathan Malcolm Friedman", "title": "Binomial and Multinomial Proportions: Accurate Estimation and Reliable\n  Assessment of Accuracy", "comments": "61 pages, 24 figures; Small changes occurred (Figs 13-18, A1 & A2,\n  Tables 1, S1) after fixing a slight bug in the the source code. For\n  comparison, version (N-1) prior to fixing the bug is at:\n  http://www.researchgate.net/profile/Jonathan_Friedman", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misestimates of $\\sigma_{P_o}$, the \\emph{uncertainty} in $P_o$ from a\n2-state Bayes equation used for binary classification, apparently arose from\n$\\hat{\\sigma}_{p_i}$, the uncertainty in underlying pdfs estimated from\nexperimental $b$-bin histograms. To address this, several Bayesian estimator\npairs $(\\hat{p}_i, \\hat{\\sigma}_{p_i})$ were compared for agreement between\nnominal confidence level ($\\xi$) and calculated coverage values ($C$). Large\n$\\xi$-to-$C$ inconsistency for large $b$ and $ p_i \\gg \\frac{1}{b}$ arises for\nall multinomial estimators since priors downweight low likelihood, high $p_i$\nvalues. To improve $\\xi$-to-$C$ matching, $(\\xi-C)^2$ was minimized against\n$\\alpha_0$ in a more general prior pdf\n($\\mathcal{B}[\\alpha_0,(b-1)\\alpha_0;x]$) to obtain\n$(\\hat{p_i})_{\\xi\\leftrightarrow C}$. This improved matching for $b=2$, but for\n$b>2$, $\\xi$-to-$C$ matching by $(\\hat{p_i})_{\\xi\\leftrightarrow C}$ required\nan effective value \"$b=2$\" and renormalization, and this reduced\n$\\hat{p}_i$-to-$p_i$ matching. Better $\\hat{p}_i$-to-$p_i$ matching came from\nthe original multinomial estimators, a new discrete-domain estimator\n$\\hat{p}(n_i,N)$, or an earlier \\emph{joint} estimator, $(\\hat{p_i})_{\\bowtie}$\nthat co-adjusted all estimates $p_i$ for James-Stein shrinkage to a mean\nvector. Best simultaneous $\\xi$-to-$C$ and $\\hat{p}_i$-to-$p_i$ matching came\nby \\emph{de-noising} initial estimates of underlying pdfs. For $b=100$,\n$N<12800$, de-noised $\\hat{p}$ needed $\\approx 10\\times$ fewer observations to\nachieve $\\hat{p}_i$-to-$p_i$ matching equivalent to that found for\n$\\hat{p}(n_i,N)$, $(\\hat{p_i})_{\\bowtie}$ or the original multinomial\n$\\hat{p}_i$. De-noising each different type of initial estimate yielded\nsimilarly high accuracy in Monte-Carlo tests.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 06:39:07 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Friedman", "Jonathan Malcolm", ""]]}, {"id": "1602.00245", "submitter": "Shravan Vasishth", "authors": "Bruno Nicenboim and Shravan Vasishth", "title": "Statistical methods for linguistic research: Foundational Ideas - Part\n  II", "comments": "30 pages, 5 figures, 4 tables. Submitted to Language and Linguistics\n  Compass. Comments and suggestions for improvement most welcome", "journal-ref": "Language and Linguistics Compass 2016", "doi": "10.1111/lnc3.12207", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an introductory review of Bayesian data analytical methods, with a\nfocus on applications for linguistics, psychology, psycholinguistics, and\ncognitive science. The empirically oriented researcher will benefit from making\nBayesian methods part of their statistical toolkit due to the many advantages\nof this framework, among them easier interpretation of results relative to\nresearch hypotheses, and flexible model specification. We present an informal\nintroduction to the foundational ideas behind Bayesian data analysis, using, as\nan example, a linear mixed models analysis of data from a typical\npsycholinguistics experiment. We discuss hypothesis testing using the Bayes\nfactor, and model selection using cross-validation. We close with some examples\nillustrating the flexibility of model specification in the Bayesian framework.\nSuggestions for further reading are also provided.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 13:36:20 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Nicenboim", "Bruno", ""], ["Vasishth", "Shravan", ""]]}, {"id": "1602.00346", "submitter": "Art Owen", "authors": "Katelyn Gao and Art B. Owen", "title": "Efficient moment calculations for variance components in large\n  unbalanced crossed random effects models", "comments": null, "journal-ref": null, "doi": "10.1214/17-EJS1236", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large crossed data sets, described by generalized linear mixed models, have\nbecome increasingly common and provide challenges for statistical analysis. At\nvery large sizes it becomes desirable to have the computational costs of\nestimation, inference and prediction (both space and time) grow at most\nlinearly with sample size. Both traditional maximum likelihood estimation and\nnumerous Markov chain Monte Carlo Bayesian algorithms take superlinear time in\norder to obtain good parameter estimates. We propose moment based algorithms\nthat, with at most linear cost, estimate variance components, measure the\nuncertainties of those estimates, and generate shrinkage based predictions for\nmissing observations. When run on simulated normally distributed data, our\nalgorithm performs competitively with maximum likelihood methods.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 23:46:36 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Gao", "Katelyn", ""], ["Owen", "Art B.", ""]]}, {"id": "1602.00355", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Ann B. Lee, Rafael Izbicki", "title": "A Spectral Series Approach to High-Dimensional Nonparametric Regression", "comments": null, "journal-ref": "Electron. J. Statist. Volume 10, Number 1 (2016), 423-463", "doi": "10.1214/16-EJS1112", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key question in modern statistics is how to make fast and reliable\ninferences for complex, high-dimensional data. While there has been much\ninterest in sparse techniques, current methods do not generalize well to data\nwith nonlinear structure. In this work, we present an orthogonal series\nestimator for predictors that are complex aggregate objects, such as natural\nimages, galaxy spectra, trajectories, and movies. Our series approach ties\ntogether ideas from kernel machine learning, and Fourier methods. We expand the\nunknown regression on the data in terms of the eigenfunctions of a kernel-based\noperator, and we take advantage of orthogonality of the basis with respect to\nthe underlying data distribution, P, to speed up computations and tuning of\nparameters. If the kernel is appropriately chosen, then the eigenfunctions\nadapt to the intrinsic geometry and dimension of the data. We provide\ntheoretical guarantees for a radial kernel with varying bandwidth, and we\nrelate smoothness of the regression function with respect to P to sparsity in\nthe eigenbasis. Finally, using simulated and real-world data, we systematically\ncompare the performance of the spectral series approach with classical kernel\nsmoothing, k-nearest neighbors regression, kernel ridge regression, and\nstate-of-the-art manifold and local regression methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:20:29 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lee", "Ann B.", ""], ["Izbicki", "Rafael", ""]]}, {"id": "1602.00564", "submitter": "Frank Miller", "authors": "Per Broberg and Frank Miller", "title": "Conditional Estimation in Two-stage Adaptive Designs", "comments": null, "journal-ref": "Broberg P, Miller F (2017). Conditional estimation in two-stage\n  adaptive designs. Biometrics, 73, 895-904", "doi": "10.1111/biom.12642", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider conditional estimation in two-stage sample size adjustable\ndesigns and the following bias. More specifically, we consider a design which\npermits raising the sample size when interim results look rather promising,\nand, which keeps the originally planned sample size when results look very\npromising. The estimation procedures reported comprise the unconditional\nmaximum likelihood, the conditionally unbiased Rao-Blackwell estimator, the\nconditional median unbiased estimator, and the conditional maximum likelihood\nwith and without bias correction. We compare these estimators based on\nanalytical results and by a simulation study. We show in a real clinical trial\nsetting how they can be applied.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 15:32:30 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 13:35:09 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Broberg", "Per", ""], ["Miller", "Frank", ""]]}, {"id": "1602.00661", "submitter": "Simon De Ridder", "authors": "Simon De Ridder, Benjamin Vandermarliere, Jan Ryckebusch", "title": "Detection and localization of change points in temporal networks with\n  the aid of stochastic block models", "comments": "This is an author-created, un-copyedited version of an article\n  accepted for publication/published in Journal of Statistical Mechanics:\n  Theory and Experiment. IOP Publishing Ltd is not responsible for any errors\n  or omissions in this version of the manuscript or any version derived from\n  it. The Version of Record is available online at\n  http://dx.doi.org/10.1088/1742-5468/2016/11/113302", "journal-ref": "J. Stat. Mech.(2016) 113302", "doi": "10.1088/1742-5468/2016/11/113302", "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework based on generalized hierarchical random graphs (GHRGs) for the\ndetection of change points in the structure of temporal networks has recently\nbeen developed by Peel and Clauset [1]. We build on this methodology and extend\nit to also include the versatile stochastic block models (SBMs) as a parametric\nfamily for reconstructing the empirical networks. We use five different\ntechniques for change point detection on prototypical temporal networks,\nincluding empirical and synthetic ones. We find that none of the considered\nmethods can consistently outperform the others when it comes to detecting and\nlocating the expected change points in empirical temporal networks. With\nrespect to the precision and the recall of the results of the change points, we\nfind that the method based on a degree-corrected SBM has better recall\nproperties than other dedicated methods, especially for sparse networks and\nsmaller sliding time window widths.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 20:17:03 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 11:15:10 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["De Ridder", "Simon", ""], ["Vandermarliere", "Benjamin", ""], ["Ryckebusch", "Jan", ""]]}, {"id": "1602.00719", "submitter": "Yiqiao Zhong", "authors": "Jianqing Fan, Weichen Wang, Yiqiao Zhong", "title": "Robust Covariance Estimation for Approximate Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study robust covariance estimation under the approximate\nfactor model with observed factors. We propose a novel framework to first\nestimate the initial joint covariance matrix of the observed data and the\nfactors, and then use it to recover the covariance matrix of the observed data.\nWe prove that once the initial matrix estimator is good enough to maintain the\nelement-wise optimal rate, the whole procedure will generate an estimated\ncovariance with desired properties. For data with only bounded fourth moments,\nwe propose to use Huber loss minimization to give the initial joint covariance\nestimation. This approach is applicable to a much wider range of distributions,\nincluding sub-Gaussian and elliptical distributions. We also present an\nasymptotic result for Huber's M-estimator with a diverging parameter. The\nconclusions are demonstrated by extensive simulations and real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 21:38:39 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Fan", "Jianqing", ""], ["Wang", "Weichen", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "1602.00769", "submitter": "Francisco Mois\\'es C\\^andido de Medeiros", "authors": "Francisco M.C. Medeiros, Silvia L.P.Ferrari", "title": "Small-sample testing inference in symmetric and log-symmetric linear\n  regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the issue of testing hypothesis in symmetric and\nlog-symmetric linear regression models in small and moderate-sized samples. We\nfocus on four tests, namely the Wald, likelihood ratio, score, and gradient\ntests. These tests rely on asymptotic results and are unreliable when the\nsample size is not large enough to guarantee a good agreement between the exact\ndistribution of the test statistic and the corresponding chi-squared asymptotic\ndistribution. Bartlett and Bartlett-type corrections typically attenuate the\nsize distortion of the tests. These corrections are available in the literature\nfor the likelihood ratio and score tests in symmetric linear regression models.\nHere, we derive a Bartlett-type correction for the gradient test. We show that\nthe corrections are also valid for the log-symmetric linear regression models.\nWe numerically compare the various tests, and bootstrapped tests, through\nsimulations. Our results suggest that the corrected and bootstrapped tests\nexhibit type I probability error closer to the chosen nominal level with\nvirtually no power loss. The analytically corrected tests, including the\nBartlett-corrected gradient test derived in this paper, perform as well as the\nbootstrapped tests with the advantage of not requiring\ncomputationally-intensive calculations. We present two real data applications\nto illustrate the usefulness of the modified tests.\n  Keywords: Symmetric regression models; Bartlett correction; Bartlett-type\ncorrection; Bootstrap; Log-symmetric regression models; gradient statistic;\nscore statistic; likelihood ratio statistic; Wald statistic.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 02:40:02 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Medeiros", "Francisco M. C.", ""], ["Ferrari", "Silvia L. P.", ""]]}, {"id": "1602.00856", "submitter": "Mauro Bernardi", "authors": "Mauro Bernardi and Roberto Casarin and Bertrand Maillet and Lea\n  Petrella", "title": "Dynamic Model Averaging for Bayesian Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general dynamic model averaging (DMA) approach based on\nMarkov-Chain Monte Carlo for the sequential combination and estimation of\nquantile regression models with time-varying parameters. The efficiency and the\neffectiveness of the proposed DMA approach and the MCMC algorithm are shown\nthrough simulation studies and applications to macro-economics and finance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 09:49:42 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Bernardi", "Mauro", ""], ["Casarin", "Roberto", ""], ["Maillet", "Bertrand", ""], ["Petrella", "Lea", ""]]}, {"id": "1602.00933", "submitter": "Sean Simpson", "authors": "Sean L. Simpson and Paul J. Laurienti", "title": "Disentangling Brain Graphs: A Note on the Conflation of Network and\n  Connectivity Analyses", "comments": "In Press, Brain Connectivity 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the human brain remains the Holy Grail in biomedical science,\nand arguably in all of the sciences. Our brains represent the most complex\nsystems in the world (and some contend the universe) comprising nearly one\nhundred billion neurons with septillions of possible connections between them.\nThe structure of these connections engenders an efficient hierarchical system\ncapable of consciousness, as well as complex thoughts, feelings, and behaviors.\nBrain connectivity and network analyses have exploded over the last decade due\nto their potential in helping us understand both normal and abnormal brain\nfunction. Functional connectivity (FC) analysis examines functional\nassociations between time series pairs in specified brain voxels or regions.\nBrain network analysis serves as a distinct subfield of connectivity analysis\nin which associations are quantified for all time series pairs to create an\ninterconnected representation of the brain (a brain network), which allows\nstudying its systemic properties. While connectivity analyses underlie network\nanalyses, the subtle distinction between the two research areas has generally\nbeen overlooked in the literature, with them often being referred to\nsynonymously. However, developing more useful analytic methods and allowing for\nmore precise biological interpretations requires distinguishing these two\ncomplementary domains.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 14:11:40 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Simpson", "Sean L.", ""], ["Laurienti", "Paul J.", ""]]}, {"id": "1602.00947", "submitter": "Sayan Ghosh", "authors": "S. Ghosh and P. Vellaisamy", "title": "Closed form estimates for missing counts in multidimensional incomplete\n  tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A useful technique for analyzing incomplete tables is to model the missing\ndata mechanisms of the variables using log-linear models. In this paper, we use\nlog-linear parametrization and propose estimation methods for arbitrary\nthree-way and $n$-dimensional incomplete tables. All possible cases in which\ndata on one or more of the variables may be missing are considered. We provide\nsimple closed form estimates of expected cell counts and parameters for the\nvarious missing data models. We also obtain explicit boundary estimates under\nnonignorable nonresponse models. Finally, a real-life dataset is analyzed to\nillustrate our results for modelling and estimation in multidimensional\nincomplete tables.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 14:45:58 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 14:40:31 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 16:25:29 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ghosh", "S.", ""], ["Vellaisamy", "P.", ""]]}, {"id": "1602.00954", "submitter": "Sayan Ghosh", "authors": "S. Ghosh and P. Vellaisamy", "title": "Evaluation of missing data mechanisms in two and three dimensional\n  incomplete tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of incomplete contingency tables is a practical and an\ninteresting problem. In this paper, we provide characterizations for the\nvarious missing mechanisms of a variable in terms of response and non-response\nodds for two and three dimensional incomplete tables. Log-linear\nparametrization and some distinctive properties of the missing data models for\nthe above tables are discussed. All possible cases in which data on one, two or\nall variables may be missing are considered. We study the missingness of each\nvariable in a model, which is more insightful for analyzing cross-classified\ndata than the missingness of the outcome vector. For sensitivity analysis of\nthe incomplete tables, we propose easily verifiable procedures to evaluate the\nmissing at random (MAR), missing completely at random (MCAR) and not missing at\nrandom (NMAR) assumptions of the missing data models. These methods depend only\non joint and marginal odds computed from fully and partially observed counts in\nthe tables, respectively. Finally, some real-life datasets are analyzed to\nillustrate our results, which are confirmed based on simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 14:52:57 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 15:45:07 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 15:12:02 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 19:03:28 GMT"}, {"version": "v5", "created": "Fri, 23 Nov 2018 21:24:24 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ghosh", "S.", ""], ["Vellaisamy", "P.", ""]]}, {"id": "1602.01051", "submitter": "Zijian Guo", "authors": "Zijian Guo, Dylan Small", "title": "Control Function Instrumental Variable Estimation of Nonlinear Causal\n  Effect Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instrumental variable method consistently estimates the effect of a\ntreatment when there is unmeasured confounding and a valid instrumental\nvariable. A valid instrumental variable is a variable that is independent of\nunmeasured confounders and affects the treatment but does not have a direct\neffect on the outcome beyond its effect on the treatment. Two commonly used\nestimators for using an instrumental variable to estimate a treatment effect\nare the two stage least squares estimator and the control function estimator.\nFor linear causal effect models, these two estimators are equivalent, but for\nnonlinear causal effect models, the estimators are different. We provide a\nsystematic comparison of these two estimators for nonlinear causal effect\nmodels and develop an approach to combing the two estimators that generally\nperforms better than either one alone. We show that the control function\nestimator is a two stage least squares estimator with an augmented set of\ninstrumental variables. If these augmented instrumental variables are valid,\nthen the control function estimator can be much more efficient than usual two\nstage least squares without the augmented instrumental variables while if the\naugmented instrumental variables are not valid, then the control function\nestimator may be inconsistent while the usual two stage least squares remains\nconsistent. We apply the Hausman test to test whether the augmented\ninstrumental variables are valid and construct a pretest estimator based on\nthis test. The pretest estimator is shown to work well in a simulation study.\nAn application to the effect of exposure to violence on time preference is\nconsidered.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 19:24:18 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Guo", "Zijian", ""], ["Small", "Dylan", ""]]}, {"id": "1602.01063", "submitter": "Claire Bowen", "authors": "Claire McKay Bowen, Fang Liu", "title": "Comparative Study of Differentially Private Data Synthesis Methods", "comments": "The main paper is the first 48 pages (8 pages of reference). The rest\n  of the pages (49 - 67) contain the Supplemental Material", "journal-ref": "Statistical Science 35 (2), 280-307, 2020", "doi": "10.1214/19-STS742", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When sharing data among researchers or releasing data for public use, there\nis a risk of exposing sensitive information of individuals in the data set.\nData synthesis (DS) is a statistical disclosure limitation technique for\nreleasing synthetic data sets with pseudo individual records. Traditional DS\ntechniques often rely on strong assumptions of a data intruder's behaviors and\nbackground knowledge to assess disclosure risk. Differential privacy (DP)\nformulates a theoretical approach for a strong and robust privacy guarantee in\ndata release without having to model intruders' behaviors. Efforts have been\nmade aiming to incorporate the DP concept in the DS process. In this paper, we\nexamine current DIfferentially Private Data Synthesis (DIPS) techniques for\nreleasing individual-level surrogate data for the original data, compare the\ntechniques conceptually, and evaluate the statistical utility and inferential\nproperties of the synthetic data via each DIPS technique through extensive\nsimulation studies. Our work sheds light on the practical feasibility and\nutility of the various DIPS approaches, and suggests future research directions\nfor DIPS.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 19:56:07 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:07:36 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 09:29:37 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 18:59:39 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Bowen", "Claire McKay", ""], ["Liu", "Fang", ""]]}, {"id": "1602.01160", "submitter": "Yan (Dora) Zhang", "authors": "Yan Zhang, Howard D. Bondell", "title": "Variable selection via penalized credible regions with Dirichlet-Laplace\n  global-local shrinkage priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of Bayesian variable selection via penalized credible regions\nseparates model fitting and variable selection. The idea is to search for the\nsparsest solution within the joint posterior credible regions. Although the\napproach was successful, it depended on the use of conjugate normal priors.\nMore recently, improvements in the use of global-local shrinkage priors have\nbeen made for high-dimensional Bayesian variable selection. In this paper, we\nincorporate global-local priors into the credible region selection framework.\nThe Dirichlet-Laplace (DL) prior is adapted to linear regression. Posterior\nconsistency for the normal and DL priors are shown, along with variable\nselection consistency. We further introduce a new method to tune\nhyperparameters in prior distributions for linear regression. We propose to\nchoose the hyperparameters to minimize a discrepancy between the induced\ndistribution on R-square and a prespecified target distribution. Prior\nelicitation on R-square is more natural, particularly when there are a large\nnumber of predictor variables in which elicitation on that scale is not\nfeasible. For a normal prior, these hyperparameters are available in closed\nform to minimize the Kullback-Leibler divergence between the distributions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 00:37:52 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 20:58:31 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Zhang", "Yan", ""], ["Bondell", "Howard D.", ""]]}, {"id": "1602.01192", "submitter": "Tianxi Li", "authors": "Tianxi Li, Elizaveta Levina, Ji Zhu", "title": "Prediction models for network-linked data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction algorithms typically assume the training data are independent\nsamples, but in many modern applications samples come from individuals\nconnected by a network. For example, in adolescent health studies of\nrisk-taking behaviors, information on the subjects' social network is often\navailable and plays an important role through network cohesion, the empirically\nobserved phenomenon of friends behaving similarly. Taking cohesion into account\nin prediction models should allow us to improve their performance. Here we\npropose a network-based penalty on individual node effects to encourage\nsimilarity between predictions for linked nodes, and show that incorporating it\ninto prediction leads to improvement over traditional models both theoretically\nand empirically when network cohesion is present. The penalty can be used with\nmany loss-based prediction methods, such as regression, generalized linear\nmodels, and Cox's proportional hazard model. Applications to predicting levels\nof recreational activity and marijuana usage among teenagers from the AddHealth\nstudy based on both demographic covariates and friendship networks are\ndiscussed in detail and show that our approach to taking friendships into\naccount can significantly improve predictions of behavior while providing\ninterpretable estimates of covariate effects.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 05:28:13 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 05:32:47 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 07:16:12 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 13:09:44 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Li", "Tianxi", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1602.01196", "submitter": "Peng Ding", "authors": "Peng Ding, Jiannan Lu", "title": "Principal stratification analysis using principal scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practitioners are interested in not only the average causal effect of the\ntreatment on the outcome but also the underlying causal mechanism in the\npresence of an intermediate variable between the treatment and outcome.\nHowever, in many cases we cannot randomize the intermediate variable, resulting\nin sample selection problems even in randomized experiments. Therefore, we view\nrandomized experiments with intermediate variables as semi-observational\nstudies. In parallel with the analysis of observational studies, we provide a\ntheoretical foundation for conducting objective causal inference with an\nintermediate variable under the principal stratification framework, with\nprincipal strata defined as the joint potential values of the intermediate\nvariable. Our strategy constructs weighted samples based on principal scores,\ndefined as the conditional probabilities of the latent principal strata given\ncovariates, without access to any outcome data. This principal stratification\nanalysis yields robust causal inference without relying on any model\nassumptions on the outcome distributions. We also propose approaches to\nconducting sensitivity analysis for violations of the ignorability and\nmonotonicity assumptions, the very crucial but untestable identification\nassumptions in our theory. When the assumptions required by the classical\ninstrumental variable analysis cannot be justified by background knowledge or\ncannot be made because of scientific questions of interest, our strategy serves\nas a useful alternative tool to deal with intermediate variables. We illustrate\nour methodologies by using two real data examples, and find scientifically\nmeaningful conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 05:56:30 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Ding", "Peng", ""], ["Lu", "Jiannan", ""]]}, {"id": "1602.01206", "submitter": "Julie Josse", "authors": "Julie Josse, Sylvain Sardy, Stefan Wager", "title": "denoiseR: A Package for Low Rank Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce denoiseR, an R package that provides a unified implementation of\nseveral state-of-the-art proposals for regularized low rank matrix estimation,\nalong with automatic selection of the regularization parameters. We also extend\nthese methods to allow for missing values. The regularization schemes discussed\nin this paper are built around singular-value shrinkage and bootstrap-based\nstability arguments. We illustrate how to use out package by applying it to\nseveral real and simulated datasets, and highlight strengths and weaknesses of\nthe different implemented methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 06:53:51 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 06:27:50 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Josse", "Julie", ""], ["Sardy", "Sylvain", ""], ["Wager", "Stefan", ""]]}, {"id": "1602.01213", "submitter": "Thanakorn Nitithumbundit", "authors": "Thanakorn Nitithumbundit and Jennifer S.K. Chan", "title": "Maximum leave-one-out likelihood estimation for location parameter of\n  unbounded densities", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation of a location parameter fails when the density\nhave unbounded mode. An alternative approach is considered by leaving out a\ndata point to avoid the unbounded density in the full likelihood. This\nmodification give rise to the leave-one-out likelihood. We propose an ECM\nalgorithm which maximises the leave-one-out likelihood. It was shown that the\nestimator which maximises the leave-one-out likelihood is consistent and\nsuper-efficient. However, other asymptotic properties such as the optimal rate\nof convergence and asymptotic distribution is still under question. We use\nsimulations to investigate these asymptotic properties of the location\nestimator using our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 07:16:58 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Nitithumbundit", "Thanakorn", ""], ["Chan", "Jennifer S. K.", ""]]}, {"id": "1602.01445", "submitter": "Tevfik Aktekin", "authors": "Tevfik Aktekin and Nicholas G. Polson and Refik Soyer", "title": "Sequential Bayesian Analysis of Multivariate Count Data", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new class of dynamic multivariate Poisson count models that\nallow for fast online updating and we refer to these models as multivariate\nPoisson-scaled beta (MPSB). The MPSB model allows for serial dependence in the\ncounts as well as dependence across multiple series with a random common\nenvironment. Other notable features include analytic forms for state\npropagation and predictive likelihood densities. Sequential updating occurs\nthrough the updating of the sufficient statistics for static model parameters,\nleading to a fully adapted particle learning algorithm and a new class of\npredictive likelihoods and marginal distributions which we refer to as the\n(dynamic) multivariate confluent hyper-geometric negative binomial distribution\n(MCHG-NB) and the the dynamic multivariate negative binomial (DMNB)\ndistribution. To illustrate our methodology, we use various simulation studies\nand count data on weekly non-durable goods consumer demand.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 20:18:41 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 19:37:03 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Aktekin", "Tevfik", ""], ["Polson", "Nicholas G.", ""], ["Soyer", "Refik", ""]]}, {"id": "1602.01522", "submitter": "Daniel McDonald", "authors": "Darren Homrighausen and Daniel J. McDonald", "title": "A study on tuning parameter selection for the high-dimensional lasso", "comments": "64 pages, 11 figures", "journal-ref": "Journal of Statistical Computation and Simulation (2018), vol. 88,\n  pp. 2865-2892", "doi": "10.1080/00949655.2018.1491575", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional predictive models, those with more measurements than\nobservations, require regularization to be well defined, perform well\nempirically, and possess theoretical guarantees. The amount of regularization,\noften determined by tuning parameters, is integral to achieving good\nperformance. One can choose the tuning parameter in a variety of ways, such as\nthrough resampling methods or generalized information criteria. However, the\ntheory supporting many regularized procedures relies on an estimate for the\nvariance parameter, which is complicated in high dimensions. We develop a suite\nof information criteria for choosing the tuning parameter in lasso regression\nby leveraging the literature on high-dimensional variance estimation. We derive\nintuition showing that existing information-theoretic approaches work poorly in\nthis setting. We compare our risk estimators to existing methods with an\nextensive simulation and derive some theoretical justification. We find that\nour new estimators perform well across a wide range of simulation conditions\nand evaluation criteria.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 01:06:09 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 18:34:39 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Homrighausen", "Darren", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "1602.01650", "submitter": "Louis Aslett", "authors": "Gero Walter, Louis J. M. Aslett, Frank P. A. Coolen", "title": "Bayesian Nonparametric System Reliability using Sets of Priors", "comments": null, "journal-ref": "International Journal of Approximate Reasoning (2017), 80,\n  pp.67-88", "doi": "10.1016/j.ijar.2016.08.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An imprecise Bayesian nonparametric approach to system reliability with\nmultiple types of components is developed. This allows modelling partial or\nimperfect prior knowledge on component failure distributions in a flexible way\nthrough bounds on the functioning probability. Given component level test data\nthese bounds are propagated to bounds on the posterior predictive distribution\nfor the functioning probability of a new system containing components\nexchangeable with those used in testing. The method further enables\nidentification of prior-data conflict at the system level based on component\nlevel test data. New results on first-order stochastic dominance for the\nBeta-Binomial distribution make the technique computationally tractable. Our\nmethodological contributions can be immediately used in applications by\nreliability practitioners as we provide easy to use software tools.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 12:02:45 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Walter", "Gero", ""], ["Aslett", "Louis J. M.", ""], ["Coolen", "Frank P. A.", ""]]}, {"id": "1602.01787", "submitter": "Lu Mao", "authors": "Lu Mao and D. Y. Lin", "title": "Efficient Estimation of Semiparametric Transformation Models for the\n  Cumulative Incidence of Competing Risks", "comments": "This paper has been withdrawn by the author due to some errors in the\n  proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulative incidence is the probability of failure from the cause of\ninterest over a certain time period in the presence of other risks. A\nsemiparametric regression model proposed by Fine and Gray (1999) has become the\nmethod of choice for formulating the effects of covariates on the cumulative\nincidence. Its estimation, however, requires modeling of the censoring\ndistribution and is not statistically efficient. In this paper, we present a\nbroad class of semiparametric transformation models which extends the Fine and\nGray model, and we allow for unknown causes of failure. We derive the\nnonparametric maximum likelihood estimators (NPMLEs) and develop simple and\nfast numerical algorithms using the profile likelihood. We establish the\nconsistency, asymptotic normality, and semiparametric efficiency of the NPMLEs.\nIn addition, we construct graphical and numerical procedures to evaluate and\nselect models. Finally, we demonstrate the advantages of the proposed methods\nover the existing ones through extensive simulation studies and an application\nto a major study on bone marrow transplantation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 18:48:19 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 02:00:17 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Mao", "Lu", ""], ["Lin", "D. Y.", ""]]}, {"id": "1602.01788", "submitter": "Lu Mao", "authors": "Lu Mao, D. Y. Lin, and Donglin Zeng", "title": "Semiparametric Regression Analysis of Interval-Censored Competing Risks\n  Data", "comments": "This paper has been withdrawn by the author due to some errors in the\n  proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval-censored competing risks data arise when each study subject may\nexperience an event or failure from one of several causes and the failure time\nis not observed exactly but rather known to lie in an interval between two\nsuccessive examinations. We formulate the effects of possibly time-varying\ncovariates on the cumulative incidence or sub-distribution function (i.e., the\nmarginal probability of failure from a particular cause) of competing risks\nthrough a broad class of semiparametric regression models that captures both\nproportional and non-proportional hazards structures for the sub-distribution.\nWe allow each subject to have an arbitrary number of examinations and\naccommodate missing information on the cause of failure. We consider\nnonparametric maximum likelihood estimation and devise a fast and stable\nEM-type algorithm for its computation. We then establish the consistency,\nasymptotic normality, and semiparametric efficiency of the resulting estimators\nby appealing to modern empirical process theory. In addition, we show through\nextensive simulation studies that the proposed methods perform well in\nrealistic situations. Finally, we provide an application to a study on HIV-1\ninfection with different viral subtypes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 18:56:35 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 01:58:45 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Mao", "Lu", ""], ["Lin", "D. Y.", ""], ["Zeng", "Donglin", ""]]}, {"id": "1602.01915", "submitter": "Zo\\'e van Havre", "authors": "Zo\\'e van Havre, Nicole White, Judith Rousseau, and Kerrie Mengersen", "title": "Clustering action potential spikes: Insights on the use of overfitted\n  finite mixture models and Dirichlet process mixture models", "comments": "Submitted to Australian & New Zealand Journal of Statistics on\n  31-Aug-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modelling of action potentials from extracellular recordings, or spike\nsorting, is a rich area of neuroscience research in which latent variable\nmodels are often used. Two such models, Overfitted Finite Mixture models (OFMs)\nand Dirichlet Process Mixture models (DPMs) are considered to provide insights\nfor unsupervised clustering of complex, multivariate medical data when the\nnumber of clusters is unknown. OFM and DPM are structured in a similar\nhierarchical fashion but they are based on different philosophies with\ndifferent underlying assumptions. This study investigates how these differences\nimpact on a real study of spike sorting, for the estimation of multivariate\nGaussian location-scale mixture models in the presence of common difficulties\narising from complex medical data. The results provide insights allowing the\nfuture analyst to choose an approach suited to the situation and goal of the\nresearch problem at hand.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 03:36:43 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["van Havre", "Zo\u00e9", ""], ["White", "Nicole", ""], ["Rousseau", "Judith", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1602.02055", "submitter": "Sebastian Weber", "authors": "Sebastian Weber, Andrew Gelman, Daniel Lee, Michael Betancourt, Aki\n  Vehtari, Amy Racine", "title": "Bayesian aggregation of average data: An application in drug development", "comments": "Code is available at https://github.com/wds15/baad and as\n  supplementary material at the publisher site", "journal-ref": "Ann. Appl. Stat., Volume 12, Number 3 (2018), 1583-1604", "doi": "10.1214/17-AOAS1122", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the different phases of a drug development program, randomized\ntrials are used to establish the tolerability, safety, and efficacy of a\ncandidate drug. At each stage one aims to optimize the design of future studies\nby extrapolation from the available evidence at the time. This includes\ncollected trial data and relevant external data. However, relevant external\ndata are typically available as averages only, for example from trials on\nalternative treatments reported in the literature. Here we report on such an\nexample from a drug development for wet age-related macular degeneration. This\ndisease is the leading cause of severe vision loss in the elderly. While\ncurrent treatment options are efficacious, they are also a substantial burden\nfor the patient. Hence, new treatments are under development which need to be\ncompared against existing treatments. The general statistical problem this\nleads to is meta-analysis, which addresses the question of how we can combine\ndatasets collected under different conditions. Bayesian methods have long been\nused to achieve partial pooling. Here we consider the challenge when the model\nof interest is complex (hierarchical and nonlinear) and one dataset is given as\nraw data while the second dataset is given as averages only. In such a\nsituation, common meta-analytic methods can only be applied when the model is\nsufficiently simple for analytic approaches. When the model is too complex, for\nexample nonlinear, an analytic approach is not possible. We provide a Bayesian\nsolution by using simulation to approximately reconstruct the likelihood of the\nexternal summary and allowing the parameters in the model to vary under the\ndifferent conditions. We first evaluate our approach using fake-data\nsimulations and then report results for the drug development program that\nmotivated this research.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 15:12:33 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 12:28:50 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Weber", "Sebastian", ""], ["Gelman", "Andrew", ""], ["Lee", "Daniel", ""], ["Betancourt", "Michael", ""], ["Vehtari", "Aki", ""], ["Racine", "Amy", ""]]}, {"id": "1602.02071", "submitter": "Dennis Dobler", "authors": "Tobias Bluhmki and Dennis Dobler and Jan Beyersmann and Markus Pauly", "title": "The Wild Bootstrap for Multivariate Nelson-Aalen Estimators", "comments": "Tobias Bluhmki and Dennis Dobler are authors with equal contribution,\n  3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We rigorously extend the widely used wild bootstrap resampling technique to\nthe multivariate Nelson-Aalen estimator under Aalen's multiplicative intensity\nmodel. Aalen's model covers general Markovian multistate models including\ncompeting risks subject to independent left-truncation and right-censoring.\nThis leads to various statistical applications such as asymptotically valid\nconfidence bands or tests for equivalence and proportional hazards. This is\nexemplified in a data analysis examining the impact of ventilation on the\nduration of intensive care unit stay. The finite sample properties of the new\nprocedures are investigated in a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 15:51:45 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 16:36:33 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Bluhmki", "Tobias", ""], ["Dobler", "Dennis", ""], ["Beyersmann", "Jan", ""], ["Pauly", "Markus", ""]]}, {"id": "1602.02114", "submitter": "Xenia Miscouridou", "authors": "Adrien Todeschini, Xenia Miscouridou and Fran\\c{c}ois Caron", "title": "Exchangeable Random Measures for Sparse and Modular Graphs with\n  Overlapping Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel statistical model for sparse networks with overlapping\ncommunity structure. The model is based on representing the graph as an\nexchangeable point process, and naturally generalizes existing probabilistic\nmodels with overlapping block-structure to the sparse regime. Our construction\nbuilds on vectors of completely random measures, and has interpretable\nparameters, each node being assigned a vector representing its level of\naffiliation to some latent communities. We develop methods for simulating this\nclass of random graphs, as well as to perform posterior inference. We show that\nthe proposed approach can recover interpretable structure from two real-world\nnetworks and can handle graphs with thousands of nodes and tens of thousands of\nedges.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 18:22:14 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 10:37:49 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Todeschini", "Adrien", ""], ["Miscouridou", "Xenia", ""], ["Caron", "Fran\u00e7ois", ""]]}, {"id": "1602.02176", "submitter": "David Puelz", "authors": "P. Richard Hahn, Carlos M. Carvalho, Jingyu He and David Puelz", "title": "Regularization and confounding in linear regression for treatment effect\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the use of regularization priors in the context of\ntreatment effect estimation using observational data where the number of\ncontrol variables is large relative to the number of observations. First, the\nphenomenon of regularization-induced confounding is introduced, which refers to\nthe tendency of regularization priors to adversely bias treatment effect\nestimates by over-shrinking control variable regression coefficients. Then, a\nsimultaneous regression model is presented which permits regularization priors\nto be specified in a way that avoids this unintentional re-confounding. The new\nmodel is illustrated on synthetic and empirical data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 22:09:17 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 18:57:48 GMT"}, {"version": "v3", "created": "Wed, 28 Dec 2016 01:38:27 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Hahn", "P. Richard", ""], ["Carvalho", "Carlos M.", ""], ["He", "Jingyu", ""], ["Puelz", "David", ""]]}, {"id": "1602.02184", "submitter": "Lu Mao", "authors": "Donglin Zeng, Lu Mao, and D. Y. Lin", "title": "Maximum Likelihood Estimation for Semiparametric Transformation Models\n  with Interval-Censored Data", "comments": "This paper has been withdrawn by the author due to some errors in the\n  proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval censoring arises frequently in clinical, epidemiological, financial,\nand sociological studies, where the event or failure of interest is known only\nto occur within an interval induced by periodic monitoring. We formulate the\neffects of potentially time-dependent covariates on the interval-censored\nfailure time through a broad class of semiparametric transformation models that\nencompasses proportional hazards and proportional odds models. We consider\nnonparametric maximum likelihood estimation for this class of models with an\narbitrary number of monitoring times for each subject. We devise an EM-type\nalgorithm that converges stably, even in the presence of time-dependent\ncovariates, and show that the estimators for the regression parameters are\nconsistent, asymptotically normal, and asymptotically efficient with an easily\nestimated covariance matrix. Finally, we demonstrate the performance of our\nprocedures through extensive simulation studies and application to an HIV/AIDS\nstudy conducted in Thailand.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 22:58:41 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 03:15:32 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Zeng", "Donglin", ""], ["Mao", "Lu", ""], ["Lin", "D. Y.", ""]]}, {"id": "1602.02185", "submitter": "Michael Ho", "authors": "Michael Ho, Jack Xin", "title": "Sparse Kalman Filtering Approaches to Covariance Estimation from High\n  Frequency Data in the Presence of Jumps", "comments": null, "journal-ref": "J. Math. Program. (2019)", "doi": "10.1007/s10107-019-01371-6", "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance matrix of asset returns from high frequency data\nis complicated by asynchronous returns, market mi- crostructure noise and\njumps. One technique for addressing both asynchronous returns and market\nmicrostructure is the Kalman-EM (KEM) algorithm. However the KEM approach\nassumes log-normal prices and does not address jumps in the return process\nwhich can corrupt estimation of the covariance matrix.\n  In this paper we extend the KEM algorithm to price models that include jumps.\nWe propose two sparse Kalman filtering approaches to this problem. In the first\napproach we develop a Kalman Expectation Conditional Maximization (KECM)\nalgorithm to determine the un- known covariance as well as detecting the jumps.\nFor this algorithm we consider Laplace and the spike and slab jump models, both\nof which promote sparse estimates of the jumps. In the second method we take a\nBayesian approach and use Gibbs sampling to sample from the posterior\ndistribution of the covariance matrix under the spike and slab jump model.\nNumerical results using simulated data show that each of these approaches\nprovide for improved covariance estima- tion relative to the KEM method in a\nvariety of settings where jumps occur.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 23:00:32 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 23:42:08 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ho", "Michael", ""], ["Xin", "Jack", ""]]}, {"id": "1602.02198", "submitter": "Garrett Waycaster", "authors": "Garrett Waycaster, Christian Bes, Volodymyr Bilotkach, Christian Gogu,\n  Raphael Haftka, Nam-Ho Kim", "title": "Robustness Metric for Quantifying Causal Model Confidence and Parameter\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods of estimating causal models do not provide estimates of\nconfidence in the resulting model. In this work, a metric is proposed for\nvalidating the output of a causal model fit; the robustness of the model\nstructure with resampled data. The metric is developed for time series causal\nmodels, but is also applicable to non-time series data. The proposed metric may\nbe utilized regardless of the method selected for fitting the causal model. We\nfind that with synthetically generated data, this metric is able to\nsuccessfully identify the true data generating model in most cases.\nAdditionally, the metric provides both a qualitative measure of model\nconfidence represented by the robustness level as well as accurate estimates of\nuncertainty in model coefficients which are important in interpreting model\nresults. The use of this metric is demonstrated on both numerically simulated\ndata and a case study from existing causal model literature.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 01:30:28 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Waycaster", "Garrett", ""], ["Bes", "Christian", ""], ["Bilotkach", "Volodymyr", ""], ["Gogu", "Christian", ""], ["Haftka", "Raphael", ""], ["Kim", "Nam-Ho", ""]]}, {"id": "1602.02200", "submitter": "Georg M. Goerg", "authors": "Georg M. Goerg", "title": "Rebuttal of the 'Letter to the Editor' of Annals of Applied Statistics\n  on Lambert W x F Distributions and the IGMM Algorithm", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I discuss comments and claims made in Stehlik and Hermann (2015) about skewed\nLambert W x F random variables and the IGMM algorithm. I clarify\nmisunderstandings about the definition and use of Lambert W x F distributions\nand show that most of their empirical results cannot be reproduced. I also\nintroduce a variant of location-scale Lambert W x F distributions that are\nwell-defined for random variables X ~ F with non-finite mean and variance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 01:53:12 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "1602.02279", "submitter": "Svetoslav Kostov", "authors": "Svetoslav Kostov, Nick Whiteley", "title": "An algorithm for approximating the second moment of the normalizing\n  constant estimate from a particle filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for approximating the non-asymptotic second moment\nof the marginal likelihood estimate, or normalizing constant, provided by a\nparticle filter. The computational cost of the new method is $O(M)$ per time\nstep, independently of the number of particles $N$ in the particle filter,\nwhere $M$ is a parameter controlling the quality of the approximation. This is\nin contrast to $O(MN)$ for a simple averaging technique using $M$ i.i.d.\nreplicates of a particle filter with $N$ particles. We establish that the\napproximation delivered by the new algorithm is unbiased, strongly consistent\nand, under standard regularity conditions, increasing $M$ linearly with time is\nsufficient to prevent growth of the relative variance of the approximation,\nwhereas for the simple averaging technique it can be necessary to increase $M$\nexponentially with time in order to achieve the same effect. Numerical examples\nillustrate performance in the context of a stochastic Lotka\\textendash Volterra\nsystem and a simple AR(1) model.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:01:24 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 12:11:56 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Kostov", "Svetoslav", ""], ["Whiteley", "Nick", ""]]}, {"id": "1602.02345", "submitter": "Wenge Guo", "authors": "Anjana Grandhi, Wenge Guo, Joseph P. Romano", "title": "Control of Directional Errors in Fixed Sequence Multiple Testing", "comments": "40 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of simultaneously testing many\ntwo-sided hypotheses when rejections of null hypotheses are accompanied by\nclaims of the direction of the alternative. The fundamental goal is to\nconstruct methods that control the mixed directional familywise error rate,\nwhich is the probability of making any type 1 or type 3 (directional) error. In\nparticular, attention is focused on cases where the hypotheses are ordered as\n$H_1 , \\ldots, H_n$, so that $H_{i+1}$ is tested only if $H_1 , \\ldots, H_i$\nhave all been previously rejected. In this situation, one can control the usual\nfamilywise error rate under arbitrary dependence by the basic procedure which\ntests each hypothesis at level $\\alpha$, and no other multiplicity adjustment\nis needed. However, we show that this is far too liberal if one also accounts\nfor directional errors. But, by imposing certain dependence assumptions on the\ntest statistics, one can retain the basic procedure. Through a simulation study\nand a clinical trial example, we numerically illustrate good performance of the\nproposed procedures compared to the existing mdFWER controlling procedures. The\nproposed procedures are also implemented in the R-package FixSeqMTP.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 07:35:18 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 18:14:39 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Grandhi", "Anjana", ""], ["Guo", "Wenge", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1602.02398", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi and Marco Lippi and Matteo Luciani", "title": "Large-Dimensional Dynamic Factor Models: Estimation of Impulse-Response\n  Functions with $I(1)$ Cointegrated Factors", "comments": null, "journal-ref": null, "doi": "10.1016/j.jeconom.2020.05.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a large-dimensional Dynamic Factor Model where: (i)~the vector of\nfactors $\\mathbf F_t$ is $I(1)$ and driven by a number of shocks that is\nsmaller than the dimension of $\\mathbf F_t$; and, (ii)~the idiosyncratic\ncomponents are either $I(1)$ or $I(0)$. Under~(i), the factors $\\mathbf F_t$\nare cointegrated and can be modeled as a Vector Error Correction Model (VECM).\nUnder (i) and (ii), we provide consistent estimators, as both the\ncross-sectional size $n$ and the time dimension $T$ go to infinity, for the\nfactors, the loadings, the shocks, the coefficients of the VECM and therefore\nthe Impulse-Response Functions (IRF) of the observed variables to the\nshocks.~Furthermore: possible deterministic linear trends are fully accounted\nfor, and the case of an unrestricted VAR in the levels $\\mathbf F_t$, instead\nof a VECM, is also studied. The finite-sample properties the proposed\nestimators are explored by means of a MonteCarlo exercise. Finally, we revisit\ntwo distinct and widely studied empirical applications. By correctly modeling\nthe long-run dynamics of the factors, our results partly overturn those\nobtained by recent literature. Specifically, we find that: (i) oil price shocks\nhave just a temporary effect on US real activity; and, (ii) in response to a\npositive news shock, the economy first experiences a significant boom, and then\na milder recession.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 18:04:42 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 16:53:24 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 11:29:48 GMT"}, {"version": "v4", "created": "Wed, 9 Oct 2019 08:05:41 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2020 15:15:25 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Lippi", "Marco", ""], ["Luciani", "Matteo", ""]]}, {"id": "1602.02466", "submitter": "Zo\\'e van Havre PhD", "authors": "Zo\\'e van Havre, Judith Rousseau, Nicole White, and Kerrie Mengersen", "title": "Overfitting hidden Markov models with an unknown number of states", "comments": "Submitted to Bayesian Analysis on 04-August-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new theory and methodology for the Bayesian estimation of\noverfitted hidden Markov models, with finite state space. The goal is then to\nachieve posterior emptying of extra states. A prior configuration is\nconstructed which favours configurations where the hidden Markov chain remains\nergodic although it empties out some of the states. Asymptotic posterior\nconvergence rates are proven theoretically, and demonstrated with a large\nsample simulation. The problem of overfitted HMMs is then considered in the\ncontext of smaller sample sizes, and due to computational and mixing issues two\nalternative prior structures are studied, one commonly used in practice, and a\nmixture of the two priors. The Prior Parallel Tempering approach of van Havre\n(2015) is also extended to HMMs to allow MCMC estimation of the complex\nposterior space. A replicate simulation study and an in-depth exploration is\nperformed to compare the three priors with hyperparameters chosen according to\nthe asymptotic constraints alongside less informative alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 05:23:26 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["van Havre", "Zo\u00e9", ""], ["Rousseau", "Judith", ""], ["White", "Nicole", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1602.02539", "submitter": "Simon Wood", "authors": "Simon N Wood", "title": "Just Another Gibbs Additive Modeller: Interfacing JAGS and mgcv", "comments": "Submitted to the Journal of Statistical Software", "journal-ref": null, "doi": "10.18637/jss.v075.i07", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BUGS language offers a very flexible way of specifying complex\nstatistical models for the purposes of Gibbs sampling, while its JAGS variant\noffers very convenient R integration via the rjags package. However, including\nsmoothers in JAGS models can involve some quite tedious coding, especially for\nmultivariate or adaptive smoothers. Further, if an additive smooth structure is\nrequired then some care is needed, in order to centre smooths appropriately,\nand to find appropriate starting values. R package mgcv implements a wide range\nof smoothers, all in a manner appropriate for inclusion in JAGS code, and\nautomates centring and other smooth setup tasks. The purpose of this note is to\ndescribe an interface between mgcv and JAGS, based around an R function,\n`jagam', which takes a generalized additive model (GAM) as specified in mgcv\nand automatically generates the JAGS model code and data required for inference\nabout the model via Gibbs sampling. Although the auto-generated JAGS code can\nbe run as is, the expectation is that the user would wish to modify it in order\nto add complex stochastic model components readily specified in JAGS. A simple\ninterface is also provided for visualisation and further inference about the\nestimated smooth components using standard mgcv functionality. The methods\ndescribed here will be un-necessarily inefficient if all that is required is\nfully Bayesian inference about a standard GAM, rather than the full flexibility\nof JAGS. In that case the BayesX package would be more efficient.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 12:12:05 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wood", "Simon N", ""]]}, {"id": "1602.02542", "submitter": "Leopoldo Catania", "authors": "Leopoldo Catania and Anna Gloria Bill\\'e", "title": "Dynamic Spatial Autoregressive Models with Autoregressive and\n  Heteroskedastic Disturbances", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of models specifically tailored for spatio-temporal\ndata analysis. To this end, we generalize the spatial autoregressive model with\nautoregressive and heteroskedastic disturbances, i.e. SARAR(1,1), by exploiting\nthe recent advancements in Score Driven (SD) models typically used in time\nseries econometrics. In particular, we allow for time-varying spatial\nautoregressive coefficients as well as time-varying regressor coefficients and\ncross-sectional standard deviations. We report an extensive Monte Carlo\nsimulation study in order to investigate the finite sample properties of the\nMaximum Likelihood estimator for the new class of models as well as its\nflexibility in explaining several dynamic spatial dependence processes. The new\nproposed class of models are found to be economically preferred by rational\ninvestors through an application in portfolio optimization.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 12:25:56 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 08:49:36 GMT"}, {"version": "v3", "created": "Sun, 13 Nov 2016 16:36:43 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Catania", "Leopoldo", ""], ["Bill\u00e9", "Anna Gloria", ""]]}, {"id": "1602.02575", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David Dunson, Chenlei Leng", "title": "DECOrrelated feature space partitioning for distributed sparse\n  regression", "comments": "Correct legend errors in Figure 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting statistical models is computationally challenging when the sample\nsize or the dimension of the dataset is huge. An attractive approach for\ndown-scaling the problem size is to first partition the dataset into subsets\nand then fit using distributed algorithms. The dataset can be partitioned\neither horizontally (in the sample space) or vertically (in the feature space).\nWhile the majority of the literature focuses on sample space partitioning,\nfeature space partitioning is more effective when $p\\gg n$. Existing methods\nfor partitioning features, however, are either vulnerable to high correlations\nor inefficient in reducing the model dimension. In this paper, we solve these\nproblems through a new embarrassingly parallel framework named DECO for\ndistributed variable selection and parameter estimation. In DECO, variables are\nfirst partitioned and allocated to $m$ distributed workers. The decorrelated\nsubset data within each worker are then fitted via any algorithm designed for\nhigh-dimensional problems. We show that by incorporating the decorrelation\nstep, DECO can achieve consistent variable selection and parameter estimation\non each subset with (almost) no assumptions. In addition, the convergence rate\nis nearly minimax optimal for both sparse and weakly sparse models and does NOT\ndepend on the partition number $m$. Extensive numerical experiments are\nprovided to illustrate the performance of the new framework.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 14:17:38 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 13:18:57 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David", ""], ["Leng", "Chenlei", ""]]}, {"id": "1602.02854", "submitter": "Wenge Guo", "authors": "Wenge Guo, Joseph P. Romano", "title": "On Stepwise Control of Directional Errors under Independence and Some\n  Dependence", "comments": "31 pages", "journal-ref": "Journal of Statistical Planning and Inference 2015, Vol. 163,\n  21-33", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of error control of stepwise multiple testing\nprocedures is considered. For two-sided hypotheses, control of both type 1 and\ntype 3 (or directional) errors is required, and thus mixed directional\nfamilywise error rate control and mixed directional false discovery rate\ncontrol are each considered by incorporating both types of errors in the error\nrate. Mixed directional familywise error rate control of stepwise methods in\nmultiple testing has proven to be a challenging problem, as demonstrated in\nShaffer (1980). By an appropriate formulation of the problem, some new stepwise\nprocedures are developed that control type 1 and directional errors under\nindependence and various dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 04:03:10 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Guo", "Wenge", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1602.02889", "submitter": "Kengo Kamatani", "authors": "Kengo Kamatani", "title": "Ergodicity of Markov chain Monte Carlo with reversible proposal", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe ergodic properties of some Metropolis-Hastings (MH) algorithms\nfor heavy-tailed target distributions. The analysis usually falls into\nsub-geometric ergodicity framework but we prove that the mixed preconditioned\nCrank-Nicolson (MpCN) algorithm has geometric ergodicity even for heavy-tailed\ntarget distributions. This useful property comes from the fact that the MpCN\nalgorithm becomes a random-walk Metropolis algorithm under suitable\ntransformation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 08:20:23 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Kamatani", "Kengo", ""]]}, {"id": "1602.02908", "submitter": "Alexia Kakourou", "authors": "Alexia Kakourou, Werner Vach, Simone Nicolardi, Yuri van der Burgt and\n  Bart Mertens", "title": "Statistical development and assessment of summary measures to account\n  for isotopic clustering of Fourier transform mass spectrometry data in\n  clinical diagnostic studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass spectrometry based clinical proteomics has emerged as a powerful tool\nfor highthroughput protein profiling and biomarker discovery. Recent\nimprovements in mass spectrometry technology have boosted the potential of\nproteomic studies in biomedical research. However, the complexity of the\nproteomic expression introduces new statistical challenges in summarizing and\nanalyzing the acquired data. Statistical methods for optimally processing\nproteomic data are currently a growing field of research. In this paper we\npresent simple, yet appropriate methods to preprocess, summarize and analyze\nhigh-throughput MALDI-FTICR mass spectrometry data, collected in a case-control\nfashion, while dealing with the statistical challenges that accompany such\ndata. The known statistical properties of the isotopic distribution of the\npeptide molecules are used to preprocess the spectra and translate the\nproteomic expression into a condensed data set. Information on either the\nintensity level or the shape of the identified isotopic clusters is used to\nderive summary measures on which diagnostic rules for disease status allocation\nwill be based. Results indicate that both the shape of the identified isotopic\nclusters and the overall intensity level carry information on the class outcome\nand can be used to predict the presence or absence of the disease.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 09:07:54 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Kakourou", "Alexia", ""], ["Vach", "Werner", ""], ["Nicolardi", "Simone", ""], ["van der Burgt", "Yuri", ""], ["Mertens", "Bart", ""]]}, {"id": "1602.03283", "submitter": "Samrat Mukhopadhyay", "authors": "Samrat Mukhopadhyay, Bijit Kumar Das, Mrityunjoy Chakraborty", "title": "Performance Analysis of $l_0$ Norm Constrained Recursive Least Squares\n  Algorithm", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT nlin.AO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance analysis of $l_0$ norm constrained Recursive least Squares (RLS)\nalgorithm is attempted in this paper. Though the performance pretty attractive\ncompared to its various alternatives, no thorough study of theoretical analysis\nhas been performed. Like the popular $l_0$ Least Mean Squares (LMS) algorithm,\nin $l_0$ RLS, a $l_0$ norm penalty is added to provide zero tap attractions on\nthe instantaneous filter taps. A thorough theoretical performance analysis has\nbeen conducted in this paper with white Gaussian input data under assumptions\nsuitable for many practical scenarios. An expression for steady state MSD is\nderived and analyzed for variations of different sets of predefined variables.\nAlso a Taylor series expansion based approximate linear evolution of the\ninstantaneous MSD has been performed. Finally numerical simulations are carried\nout to corroborate the theoretical analysis and are shown to match well for a\nwide range of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 07:03:14 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Mukhopadhyay", "Samrat", ""], ["Das", "Bijit Kumar", ""], ["Chakraborty", "Mrityunjoy", ""]]}, {"id": "1602.03574", "submitter": "Rina Barber", "authors": "Rina Foygel Barber and Emmanuel J. Candes", "title": "A knockoff filter for high-dimensional selective inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a framework for testing for associations in a possibly\nhigh-dimensional linear model where the number of features/variables may far\nexceed the number of observational units. In this framework, the observations\nare split into two groups, where the first group is used to screen for a set of\npotentially relevant variables, whereas the second is used for inference over\nthis reduced set of variables; we also develop strategies for leveraging\ninformation from the first part of the data at the inference step for greater\npower. In our work, the inferential step is carried out by applying the\nrecently introduced knockoff filter, which creates a knockoff copy-a fake\nvariable serving as a control-for each screened variable. We prove that this\nprocedure controls the directional false discovery rate (FDR) in the reduced\nmodel controlling for all screened variables; this says that our\nhigh-dimensional knockoff procedure 'discovers' important variables as well as\nthe directions (signs) of their effects, in such a way that the expected\nproportion of wrongly chosen signs is below the user-specified level (thereby\ncontrolling a notion of Type S error averaged over the selected set). This\nresult is non-asymptotic, and holds for any distribution of the original\nfeatures and any values of the unknown regression coefficients, so that\ninference is not calibrated under hypothesized values of the effect sizes. We\ndemonstrate the performance of our general and flexible approach through\nnumerical studies, showing more power than existing alternatives. Finally, we\napply our method to a genome-wide association study to find locations on the\ngenome that are possibly associated with a continuous phenotype.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 23:36:37 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 19:26:34 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 13:34:24 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Candes", "Emmanuel J.", ""]]}, {"id": "1602.03589", "submitter": "Ran Dai", "authors": "Ran Dai, Rina Foygel Barber", "title": "The knockoff filter for FDR control in group-sparse and multitask\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the group knockoff filter, a method for false discovery rate\ncontrol in a linear regression setting where the features are grouped, and we\nwould like to select a set of relevant groups which have a nonzero effect on\nthe response. By considering the set of true and false discoveries at the group\nlevel, this method gains power relative to sparse regression methods. We also\napply our method to the multitask regression problem where multiple response\nvariables share similar sparsity patterns across the set of possible features.\nEmpirically, the group knockoff filter successfully controls false discoveries\nat the group level in both settings, with substantially more discoveries made\nby leveraging the group structure.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 01:13:32 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Dai", "Ran", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1602.03649", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi and Gerald S. Buller and Steve McLaughlin and Paul\n  Honeine", "title": "Bayesian Filtering of Smooth Signals: Application to Altimetry", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Bayesian strategy for the estimation of smooth\nsignals corrupted by Gaussian noise. The method assumes a smooth evolution of a\nsuccession of continuous signals that can have a numerical or an analytical\nexpression with respect to some parameters. The Bayesian model proposed takes\ninto account the Gaussian properties of the noise and the smooth evolution of\nthe successive signals. In addition, a gamma Markov random field prior is\nassigned to the signal energies and to the noise variances to account for their\nknown properties. The resulting posterior distribution is maximized using a\nfast coordinate descent algorithm whose parameters are updated by analytical\nexpressions. The proposed algorithm is tested on satellite altimetric data\ndemonstrating good denoising results on both synthetic and real signals. The\nproposed algorithm is also shown to improve the quality of the altimetric\nparameters when combined with a parameter estimation strategy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 09:30:04 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Buller", "Gerald S.", ""], ["McLaughlin", "Steve", ""], ["Honeine", "Paul", ""]]}, {"id": "1602.03697", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen, Geoffrey J. McLachlan", "title": "Linear Mixed Models with Marginally Symmetric Nonparametric Random\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed models (LMMs) are used as an important tool in the data analysis\nof repeated measures and longitudinal studies. The most common form of LMMs\nutilize a normal distribution to model the random effects. Such assumptions can\noften lead to misspecification errors when the random effects are not normal.\nOne approach to remedy the misspecification errors is to utilize a point-mass\ndistribution to model the random effects; this is known as the nonparametric\nmaximum likelihood-fitted (NPML) model. The NPML model is flexible but requires\na large number of parameters to characterize the random-effects distribution.\nIt is often natural to assume that the random-effects distribution be at least\nmarginally symmetric. The marginally symmetric NPML (MSNPML) random-effects\nmodel is introduced, which assumes a marginally symmetric point-mass\ndistribution for the random effects. Under the symmetry assumption, the MSNPML\nmodel utilizes half the number of parameters to characterize the same number of\npoint masses as the NPML model; thus the model confers an advantage in economy\nand parsimony. An EM-type algorithm is presented for the maximum likelihood\n(ML) estimation of LMMs with MSNPML random effects; the algorithm is shown to\nmonotonically increase the log-likelihood and is proven to be convergent to a\nstationary point of the log-likelihood function in the case of convergence.\nFurthermore, it is shown that the ML estimator is consistent and asymptotically\nnormal under certain conditions, and the estimation of quantities such as the\nrandom-effects covariance matrix and individual a posteriori expectations is\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 12:20:20 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 23:41:58 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1602.03760", "submitter": "Inga Johnson", "authors": "Christopher Cericola, Inga Johnson, Joshua Kiers, Mitchell Krock,\n  Jordan Purdy, and Johanna Torrence", "title": "Extending Hypothesis Testing with Persistence Homology to Three or More\n  Groups", "comments": null, "journal-ref": "Involve 11 (2018) 27-51", "doi": "10.2140/involve.2018.11.27", "report-no": null, "categories": "stat.ME math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the work of Robinson and Turner to use hypothesis testing with\npersistence homology to test for measurable differences in shape between point\nclouds from three or more groups. Using samples of point clouds from three\ndistinct groups, we conduct a large-scale simulation study to validate our\nproposed extension. We consider various combinations of groups, samples sizes\nand measurement errors in the simulation study, providing for each combination\nthe percentage of $p$-values below an alpha-level of 0.05. Additionally, we\napply our method to a Cardiotocography data set and find statistically\nsignificant evidence of measurable differences in shape between normal, suspect\nand pathologic health status groups.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 21:07:51 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Cericola", "Christopher", ""], ["Johnson", "Inga", ""], ["Kiers", "Joshua", ""], ["Krock", "Mitchell", ""], ["Purdy", "Jordan", ""], ["Torrence", "Johanna", ""]]}, {"id": "1602.03765", "submitter": "Sara Algeri", "authors": "Sara Algeri, David A. van Dyk, Jan Conrad, Brandon Anderson", "title": "On methods for correcting for the look-elsewhere effect in searches for\n  new physics", "comments": null, "journal-ref": "Journal of Instrumentation 11 P12010, 2016", "doi": "10.1088/1748-0221/11/12/P12010", "report-no": null, "categories": "physics.data-an astro-ph.HE hep-ex hep-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for new significant peaks over a energy spectrum often involves a\nstatistical multiple hypothesis testing problem. Separate tests of hypothesis\nare conducted at different locations producing an ensemble of local p-values,\nthe smallest of which is reported as evidence for the new resonance.\nUnfortunately, controlling the false detection rate (type I error rate) of such\nprocedures may lead to excessively stringent acceptance criteria. In the recent\nphysics literature, two promising statistical tools have been proposed to\novercome these limitations. In 2005, a method to \"find needles in haystacks\"\nwas introduced by Pilla et al. [1], and a second method was later proposed by\nGross and Vitells [2] in the context of the \"look elsewhere effect\" and trial\nfactors. We show that, for relatively small sample sizes, the former leads to\nan artificial inflation of statistical power that stems from an increase in the\nfalse detection rate, whereas the two methods exhibit similar performance for\nlarge sample sizes. We apply the methods to realistic simulations of the Fermi\nLarge Area Telescope data, in particular the search for dark matter\nannihilation lines. Further, we discuss the counter-intutive scenario where the\nlook-elsewhere corrections are more conservative than much more computationally\nefficient corrections for multiple hypothesis testing. Finally, we provide\ngeneral guidelines for navigating the tradeoffs between statistical and\ncomputational efficiency when selecting a statistical procedure for signal\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 15:15:38 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 15:20:30 GMT"}, {"version": "v3", "created": "Wed, 18 May 2016 15:30:33 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2016 13:13:20 GMT"}, {"version": "v5", "created": "Thu, 15 Dec 2016 17:55:30 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Algeri", "Sara", ""], ["van Dyk", "David A.", ""], ["Conrad", "Jan", ""], ["Anderson", "Brandon", ""]]}, {"id": "1602.03861", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Unified Statistical Theory of Spectral Graph Analysis", "comments": "Major changes have been done in terms of contents and structure of\n  the paper. New set of motivations for GraField, Expanding Section 4,\n  Connections with Diffusion map and Google's PageRank method etc", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to show that there exists a simple, yet universal\nstatistical logic of spectral graph analysis by recasting it into a\nnonparametric function estimation problem. The prescribed viewpoint appears to\nbe good enough to accommodate most of the existing spectral graph techniques as\na consequence of just one single formalism and algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 20:05:38 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 01:24:51 GMT"}, {"version": "v3", "created": "Sun, 20 Mar 2016 13:13:13 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 21:35:31 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1602.03915", "submitter": "Tirthankar Dasgupta", "authors": "Anqi Zhao, Peng Ding, Tirthankar Dasgupta", "title": "Randomization-Based Causal Inference from Unbalanced 2^2 Split-Plot\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two 2-level factors of interest, a 2^2 split-plot design} (a) takes\neach of the $2^2=4$ possible factorial combinations as a treatment, (b)\nidentifies one factor as `whole-plot,' (c) divides the experimental units into\nblocks, and (d) assigns the treatments in such a way that all units within the\nsame block receive the same level of the whole-plot factor.\n  Assuming the potential outcomes framework, we propose in this paper a\nrandomization-based estimation procedure for causal inference from 2^2 designs\nthat are not necessarily balanced. Sampling variances of the point estimates\nare derived in closed form as linear combinations of the between- and\nwithin-block covariances of the potential outcomes. Results are compared to\nthose under complete randomization as measures of design efficiency. Interval\nestimates are constructed based on conservative estimates of the sampling\nvariances, and the frequency coverage properties evaluated via simulation.\nAsymptotic connections of the proposed approach to the model-based\nsuper-population inference are also established. Superiority over existing\nmodel-based alternatives is reported under a variety of settings for both\nbinary and continuous outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 22:23:53 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Zhao", "Anqi", ""], ["Ding", "Peng", ""], ["Dasgupta", "Tirthankar", ""]]}, {"id": "1602.03972", "submitter": "Jiannan Lu", "authors": "Jiannan Lu", "title": "On Randomization-based and Regression-based Inferences for 2^K Factorial\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the randomization-based causal inference framework in Dasgupta et\nal. (2015) for general 2^K factorial designs, and demonstrate the equivalence\nbetween regression-based and randomization-based inferences. Consequently, we\njustify the use of regression-based methods in 2^K factorial designs from a\nfinite-population perspective.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 07:44:20 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Lu", "Jiannan", ""]]}, {"id": "1602.03990", "submitter": "Li Ma", "authors": "Li Ma and Jacopo Soriano", "title": "Efficient functional ANOVA through wavelet-domain Markov groves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a wavelet-domain functional analysis of variance (fANOVA) method\nbased on a Bayesian hierarchical model. The factor effects are modeled through\na spike-and-slab mixture at each location-scale combination along with a\nnormal-inverse-Gamma (NIG) conjugate setup for the coefficients and errors. A\ngraphical model called the Markov grove (MG) is designed to jointly model the\nspike-and-slab statuses at all location-scale combinations, which incorporates\nthe clustering of each factor effect in the wavelet-domain thereby allowing\nborrowing of strength across location and scale. The posterior of this NIG-MG\nmodel is analytically available through a pyramid algorithm of the same\ncomputational complexity as Mallat's pyramid algorithm for discrete wavelet\ntransform, i.e., linear in both the number of observations and the number of\nlocations. Posterior probabilities of factor contributions can also be computed\nthrough pyramid recursion, and exact samples from the posterior can be drawn\nwithout MCMC. We investigate the performance of our method through extensive\nsimulation and show that it outperforms existing wavelet-domain fANOVA methods\nin a variety of common settings. We apply the method to analyzing the orthosis\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 09:31:34 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 03:56:08 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Ma", "Li", ""], ["Soriano", "Jacopo", ""]]}, {"id": "1602.04003", "submitter": "Alberto Sorrentino", "authors": "Valentina Vivaldi and Alberto Sorrentino", "title": "Bayesian smoothing of dipoles in Magneto-/Electro-encephalography", "comments": "16 pages, 5 figures", "journal-ref": "Inverse Problems 32 (2016) 045007", "doi": "10.1088/0266-5611/32/4/045007", "report-no": null, "categories": "math.NA stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel method for dynamic estimation of multi-dipole states from\nMagneto/Electro-encephalography (M/EEG) time series. The new approach builds on\nthe recent development of particle filters for M/EEG; these algorithms\napproximate, with samples and weights, the posterior distribution of the neural\nsources at time t given the data up to time t. However, for off-line inference\npurposes it is preferable to work with the smoothing distribution, i.e. the\ndistribution for the neural sources at time t conditioned on the whole time\nseries. In this study, we use a Monte Carlo algorithm to approximate the\nsmoothing distribution for a time-varying set of current dipoles. We show,\nusing numerical simulations, that the estimates provided by the smoothing\ndistribution are more accurate than those provided by the filtering\ndistribution, particularly at the appearance of the source. We validate the\nproposed algorithm using an experimental dataset recorded from an epileptic\npatient. Improved localization of the source onset can be particularly relevant\nin source modeling of epileptic patients, where the source onset brings\ninformation on the epileptogenic zone.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 10:30:27 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Vivaldi", "Valentina", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "1602.04107", "submitter": "Jonathan Hill", "authors": "Jonathan B. Hill, Kaiji Motegi", "title": "A Max-Correlation White Noise Test for Weakly Dependent Time Series", "comments": null, "journal-ref": "Econom. Theory 36 (2020) 907-960", "doi": "10.1017/S0266466619000367", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a bootstrapped p-value white noise test based on the\nmaximum correlation, for a time series that may be weakly dependent under the\nnull hypothesis. The time series may be prefiltered residuals. The test\nstatistic is a normalized weighted maximum sample correlation, where the\nmaximum lag increases at a rate slower than the sample size. We only require\nuncorrelatedness under the null hypothesis, along with a moment contraction\ndependence property that includes mixing and non-mixing sequences. We show\nShao's (2011) dependent wild bootstrap is valid for a much larger class of\nprocesses than originally considered. It is also valid for residuals from a\ngeneral class of parametric models as long as the bootstrap is applied to a\nfirst order expansion of the sample correlation. We prove the bootstrap\nvalidity without exploiting extreme value theory (standard in the literature)\nor recent Gaussian approximation theory. Finally, we extend Escanciano and\nLobato's (2009) automatic maximum lag selection to our setting with an\nunbounded choice set, and find it works strikingly well in controlled\nexperiments. Our proposed test achieves accurate size under various white noise\nnull hypotheses and high power under various alternative hypotheses including\ndistant serial dependence.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 16:32:26 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 21:25:08 GMT"}, {"version": "v3", "created": "Wed, 5 Jul 2017 23:41:52 GMT"}, {"version": "v4", "created": "Mon, 5 Nov 2018 21:02:12 GMT"}, {"version": "v5", "created": "Mon, 5 Aug 2019 21:42:31 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hill", "Jonathan B.", ""], ["Motegi", "Kaiji", ""]]}, {"id": "1602.04117", "submitter": "Florian Kelma", "authors": "Thomas Hotz, Florian Kelma", "title": "Non-asymptotic Confidence Sets for Extrinsic Means on Spheres and\n  Projective Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence sets from i.i.d. data are constructed for the extrinsic mean of a\nprobabilty measure P on spheres, real projective spaces, and complex projective\nspaces, as well as Grassmann manifolds, with the latter three embedded by the\nVeronese-Whitney embedding. When the data are sufficiently concentrated, these\nare projections of a ball around the corresponding Euclidean sample mean.\nFurthermore, these confidence sets are rate-optimal. The usefulness of this\napproach is illustrated for projective shape data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 16:56:08 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Hotz", "Thomas", ""], ["Kelma", "Florian", ""]]}, {"id": "1602.04139", "submitter": "Christopher Paciorek", "authors": "Soyoung Jeon, Christopher J. Paciorek, Michael F. Wehner", "title": "Quantile-based bias correction and uncertainty quantification of extreme\n  event attribution statements", "comments": "28 pages, 4 figures, 3 tables", "journal-ref": "Weather and Climate Extremes (2016) 12:24-32", "doi": "10.1016/j.wace.2016.02.001", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extreme event attribution characterizes how anthropogenic climate change may\nhave influenced the probability and magnitude of selected individual extreme\nweather and climate events. Attribution statements often involve quantification\nof the fraction of attributable risk (FAR) or the risk ratio (RR) and\nassociated confidence intervals. Many such analyses use climate model output to\ncharacterize extreme event behavior with and without anthropogenic influence.\nHowever, such climate models may have biases in their representation of extreme\nevents. To account for discrepancies in the probabilities of extreme events\nbetween observational datasets and model datasets, we demonstrate an\nappropriate rescaling of the model output based on the quantiles of the\ndatasets to estimate an adjusted risk ratio. Our methodology accounts for\nvarious components of uncertainty in estimation of the risk ratio. In\nparticular, we present an approach to construct a one-sided confidence interval\non the lower bound of the risk ratio when the estimated risk ratio is infinity.\nWe demonstrate the methodology using the summer 2011 central US heatwave and\noutput from the Community Earth System Model. In this example, we find that the\nlower bound of the risk ratio is relatively insensitive to the magnitude and\nprobability of the actual event.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 17:50:24 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Jeon", "Soyoung", ""], ["Paciorek", "Christopher J.", ""], ["Wehner", "Michael F.", ""]]}, {"id": "1602.04528", "submitter": "Harrison Quick", "authors": "Harrison Quick, Lance A. Waller, and Michele Casper", "title": "Hierarchical multivariate space-time methods for modeling counts with an\n  application to stroke mortality data", "comments": null, "journal-ref": "Annals of Applied Statistics, 11 (2017) 2170-2182", "doi": "10.1214/17-AOAS1068", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geographic patterns in stroke mortality have been studied as far back as the\n1960s, when a region of the southeastern United States became known as the\n\"stroke belt\" due to its unusually high rates of stroke mortality. While stroke\nmortality rates are known to increase exponentially with age, an investigation\nof spatiotemporal trends by age group at the county-level is daunting due to\nthe preponderance of small population sizes and/or few stroke events by age\ngroup. Here, we harness the power of a complex, nonseparable multivariate\nspace-time model which borrows strength across space, time, and age group to\nobtain reliable estimates of yearly county-level mortality rates from US\ncounties between 1973 and 2013 for those aged 65+. Furthermore, we propose an\nalternative metric for measuring changes in event rates over time which\naccounts for the full trajectory of a county's event rates, as opposed to\nsimply comparing the rates at the beginning and end of the study period. In our\nanalysis of the stroke data, we identify differing spatiotemporal trends in\nmortality rates across age groups, shed light on the gains achieved in the Deep\nSouth, and provide evidence that a separable model is inappropriate for these\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 23:51:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Quick", "Harrison", ""], ["Waller", "Lance A.", ""], ["Casper", "Michele", ""]]}, {"id": "1602.04565", "submitter": "Phillip M. Alday", "authors": "Jona Sassenhagen, Phillip M. Alday", "title": "A common misapplication of statistical inference: nuisance control with\n  null-hypothesis significance tests", "comments": null, "journal-ref": "Brain and Language 2016", "doi": "10.1016/j.bandl.2016.08.001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental research on behavior and cognition frequently rests on stimulus\nor subject selection where not all characteristics can be fully controlled,\neven when attempting strict matching. For example, when contrasting patients to\ncontrols, variables such as intelligence or socioeconomic status are often\ncorrelated with patient status. Similarly, when presenting word stimuli,\nvariables such as word frequency are often correlated with primary variables of\ninterest. One procedure very commonly employed to control for such nuisance\neffects is conducting inferential tests on confounding stimulus or subject\ncharacteristics. For example, if word length is not significantly different for\ntwo stimulus sets, they are considered as matched for word length. Such a test\nhas high error rates and is conceptually misguided. It reflects a common\nmisunderstanding of statistical tests: interpreting significance not to refer\nto inference about a particular population parameter, but about 1. the sample\nin question, 2. the practical relevance of a sample difference (so that a\nnonsignificant test is taken to indicate evidence for the absence of relevant\ndifferences). We show inferential testing for assessing nuisance effects to be\ninappropriate both pragmatically and philosophically, present a survey showing\nits high prevalence, and briefly discuss an alternative in the form of\nregression including nuisance variables.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 06:21:32 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 11:20:37 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 11:44:51 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Sassenhagen", "Jona", ""], ["Alday", "Phillip M.", ""]]}, {"id": "1602.04805", "submitter": "Jovana Mitrovic", "authors": "Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh", "title": "DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 20:55:57 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Mitrovic", "Jovana", ""], ["Sejdinovic", "Dino", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1602.04862", "submitter": "Gery Geenens", "authors": "Gery Geenens and Craig Wang", "title": "Local-likelihood transformation kernel density estimation for positive\n  random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel estimator is known not to be adequate for estimating the density\nof a positive random variable X. The main reason is the well-known boundary\nbias problems that it suffers from, but also its poor behaviour in the long\nright tail that such a density typically exhibits. A natural approach to this\nproblem is to first estimate the density of the logarithm of X, and obtaining\nan estimate of the density of X using standard results on functions of random\nvariables (`back-transformation'). Although intuitive, the basic application of\nthis idea yields very poor results, as was documented earlier in the\nliterature. In this paper, the main reason for this underachievement is\nidentified, and an easy fix is suggested. It is demonstrated that combining the\ntransformation with local likelihood density estimation methods produces very\ngood estimators of R+-supported densities, not only close to the boundary, but\nalso in the right tail. The asymptotic properties of the proposed `local\nlikelihood transformation kernel density estimators' are derived for a generic\ntransformation, not only for the logarithm, which allows one to consider other\ntransformations as well. One of them, called the `probex' transformation, is\ngiven more focus. Finally, the excellent behaviour of those estimators in\npractice is evidenced through a comprehensive simulation study and the analysis\nof several real data sets. A nice consequence of articulating the method around\nlocal-likelihood estimation is that the resulting density estimates are\ntypically smooth and visually pleasant, without oversmoothing important\nfeatures of the underlying density.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 23:11:46 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Geenens", "Gery", ""], ["Wang", "Craig", ""]]}, {"id": "1602.04910", "submitter": "Shuichi Kawano", "authors": "Kaito Shimamura, Masao Ueki, Shuichi Kawano and Sadanori Konishi", "title": "Bayesian generalized fused lasso modeling via NEG distribution", "comments": "26 pages", "journal-ref": "Communications in Statistics - Theory and Methods 48 (2019)\n  4132-4153", "doi": "10.1080/03610926.2018.1489056", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fused lasso penalizes a loss function by the $L_1$ norm for both the\nregression coefficients and their successive differences to encourage sparsity\nof both. In this paper, we propose a Bayesian generalized fused lasso modeling\nbased on a normal-exponential-gamma (NEG) prior distribution. The NEG prior is\nassumed into the difference of successive regression coefficients. The proposed\nmethod enables us to construct a more versatile sparse model than the ordinary\nfused lasso by using a flexible regularization term. We also propose a sparse\nfused algorithm to produce exact sparse solutions. Simulation studies and real\ndata analyses show that the proposed method has superior performance to the\nordinary fused lasso.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 05:20:14 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Shimamura", "Kaito", ""], ["Ueki", "Masao", ""], ["Kawano", "Shuichi", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1602.05023", "submitter": "Alessio Spantini", "authors": "Youssef Marzouk, Tarek Moselhy, Matthew Parno, and Alessio Spantini", "title": "An introduction to sampling via measure transport", "comments": "To appear in Handbook of Uncertainty Quantification; R. Ghanem, D.\n  Higdon, and H. Owhadi, editors; Springer, 2016", "journal-ref": null, "doi": "10.1007/978-3-319-11259-6_23-1", "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the fundamentals of a measure transport approach to sampling. The\nidea is to construct a deterministic coupling---i.e., a transport map---between\na complex \"target\" probability measure of interest and a simpler reference\nmeasure. Given a transport map, one can generate arbitrarily many independent\nand unweighted samples from the target simply by pushing forward reference\nsamples through the map. We consider two different and complementary scenarios:\nfirst, when only evaluations of the unnormalized target density are available,\nand second, when the target distribution is known only through a finite\ncollection of samples. We show that in both settings the desired transports can\nbe characterized as the solutions of variational problems. We then address\npractical issues associated with the optimization--based construction of\ntransports: choosing finite-dimensional parameterizations of the map, enforcing\nmonotonicity, quantifying the error of approximate transports, and refining\napproximate transports by enriching the corresponding approximation spaces.\nApproximate transports can also be used to \"Gaussianize\" complex distributions\nand thus precondition conventional asymptotically exact sampling schemes. We\nplace the measure transport approach in broader context, describing connections\nwith other optimization--based samplers, with inference and density estimation\nschemes using optimal transport, and with alternative transformation--based\napproaches to simulation. We also sketch current work aimed at the construction\nof transport maps in high dimensions, exploiting essential features of the\ntarget distribution (e.g., conditional independence, low-rank structure). The\napproaches and algorithms presented here have direct applications to Bayesian\ncomputation and to broader problems of stochastic simulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 14:10:57 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Marzouk", "Youssef", ""], ["Moselhy", "Tarek", ""], ["Parno", "Matthew", ""], ["Spantini", "Alessio", ""]]}, {"id": "1602.05063", "submitter": "Robin Ince", "authors": "Robin A. A. Ince", "title": "Measuring multivariate redundant information with pointwise common\n  change in surprisal", "comments": "v3: revisions based on review process at Entropy (expand game-theory\n  and max-ent motivation), v2: add game-theoretic operational definition for\n  maximum entropy constraints; remove thresholding and normalisation of values\n  on lattice", "journal-ref": "Entropy 2017, 19(7), 318", "doi": "10.3390/e19070318", "report-no": null, "categories": "cs.IT math.IT math.ST q-bio.NC q-bio.QM stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The problem of how to properly quantify redundant information is an open\nquestion that has been the subject of much recent research. Redundant\ninformation refers to information about a target variable S that is common to\ntwo or more predictor variables Xi. It can be thought of as quantifying\noverlapping information content or similarities in the representation of S\nbetween the Xi. We present a new measure of redundancy which measures the\ncommon change in surprisal shared between variables at the local or pointwise\nlevel. We provide a game-theoretic operational definition of unique\ninformation, and use this to derive constraints which are used to obtain a\nmaximum entropy distribution. Redundancy is then calculated from this maximum\nentropy distribution by counting only those local co-information terms which\nadmit an unambiguous interpretation as redundant information. We show how this\nredundancy measure can be used within the framework of the Partial Information\nDecomposition (PID) to give an intuitive decomposition of the multivariate\nmutual information into redundant, unique and synergistic contributions. We\ncompare our new measure to existing approaches over a range of example systems,\nincluding continuous Gaussian variables. Matlab code for the measure is\nprovided, including all considered examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 15:57:36 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 10:32:28 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 16:54:30 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ince", "Robin A. A.", ""]]}, {"id": "1602.05125", "submitter": "Anne van Delft Dr.", "authors": "Anne van Delft, Michael Eichler", "title": "Locally Stationary Functional Time Series", "comments": null, "journal-ref": "Electronic Journal of Statistics Volume 12, Number 1 (2018),\n  107-170", "doi": "10.1214/17-EJS1384", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on time series of functional data has focused on processes of\nwhich the probabilistic law is either constant over time or constant up to its\nsecond-order structure. Especially for long stretches of data it is desirable\nto be able to weaken this assumption. This paper introduces a framework that\nwill enable meaningful statistical inference of functional data of which the\ndynamics change over time. We put forward the concept of local stationarity in\nthe functional setting and establish a class of processes that have a\nfunctional time-varying spectral representation. Subsequently, we derive\nconditions that allow for fundamental results from nonstationary multivariate\ntime series to carry over to the function space. In particular, time-varying\nfunctional ARMA processes are investigated and shown to be functional locally\nstationary according to the proposed definition. As a side-result, we establish\na Cram\\'er representation for an important class of weakly stationary\nfunctional processes. Important in our context is the notion of a time-varying\nspectral density operator of which the properties are studied and uniqueness is\nderived. Finally, we provide a consistent nonparametric estimator of this\noperator and show it is asymptotically Gaussian using a weaker tightness\ncriterion than what is usually deemed necessary.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 18:29:17 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 21:28:57 GMT"}, {"version": "v3", "created": "Sun, 10 Dec 2017 16:03:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["van Delft", "Anne", ""], ["Eichler", "Michael", ""]]}, {"id": "1602.05155", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos", "title": "A Dirichlet Process Functional Approach to Heteroscedastic-Consistent\n  Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of Dirichlet process (MDP) defines a flexible prior distribution\non the space of probability measures. This study shows that ordinary\nleast-squares (OLS) estimator, as a functional of the MDP posterior\ndistribution, has posterior mean given by weighted least-squares (WLS), and has\nposterior covariance matrix given by the (weighted) heteroscedastic-consistent\nsandwich estimator. This is according to a pairs bootstrap distribution\napproximation of the posterior, using a P\\'olya urn scheme. Also, when the MDP\nprior baseline distribution is specified as a product of independent\nprobability measures, this WLS solution provides a new type of generalized\nridge regression estimator which can handle multicollinear or singular design\nmatrices even when the number of covariates exceeds the sample size, and which\nshrinks the coefficient estimates of irrelevant covariates towards zero, thus\nuseful for nonlinear regressions. Also, this MDP/OLS functional methodology can\nbe extended to methods for analyzing the sensitivity of the\nheteroscedasticity-consistent causal effect size over a range of hidden biases\ndue to missing covariates omitted from the regression, and more generally\nextended to a Vibration of Effects analysis. The methodology is illustrated\nthrough the analysis of simulated and real data sets. Overall, this study\nestablishes new connections between Dirichlet process functional inference, the\nbootstrap, consistent sandwich covariance estimation, ridge shrinkage\nregression, WLS, and sensitivity analysis, to provide regression methodology\nuseful for inferences of the mean dependent response.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 19:57:30 GMT"}, {"version": "v2", "created": "Sat, 11 Jun 2016 15:42:57 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Karabatsos", "George", ""]]}, {"id": "1602.05236", "submitter": "Linjun Zhang", "authors": "T.Tony Cai, Linjun Zhang", "title": "A Sparse PCA Approach to Clustering", "comments": "This paper is part of a discussion of the paper \"Important feature\n  PCA for high dimensional clustering\" by Jiashun Jin and Wanjie Wang to appear\n  in The Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a clustering method for Gaussian mixture model based on the sparse\nprincipal component analysis (SPCA) method and compare it with the IF-PCA\nmethod. We also discuss the dependent case where the covariance matrix $\\Sigma$\nis not necessarily diagonal.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 22:49:41 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Linjun", ""]]}, {"id": "1602.05238", "submitter": "Jiming Jiang", "authors": "Jiming Jiang, P. Lahiri, Thuan Nguyen", "title": "A Unified Monte-Carlo Jackknife for Small Area Estimation after Model\n  Selection", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of measure of uncertainty in small area estimation\n(SAE) when a procedure of model selection is involved prior to the estimation.\nA unified Monte-Carlo jackknife method, called McJack, is proposed for\nestimating the logarithm of the mean squared prediction error. We prove the\nsecond-order unbiasedness of McJack, and demonstrate the performance of McJack\nin assessing uncertainty in SAE after model selection through empirical\ninvestigations that include simulation studies and real-data analyses.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 23:01:28 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Jiang", "Jiming", ""], ["Lahiri", "P.", ""], ["Nguyen", "Thuan", ""]]}, {"id": "1602.05454", "submitter": "Nil Kamal Hazra", "authors": "Nil Kamal Hazra, Pradip Kundu and Asok K. Nanda", "title": "Some Reliability Properties of Transformed-Transformer Family of\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformed-Transformer family of distributions are the resulting family\nof distributions as transformed from a random variable $T$ through another\ntransformer random variable $X$ using a weight function $\\omega$ of the\ncumulative distribution function of $X$. In this paper, we study different\nstochastic ageing properties, as well as different stochastic orderings of this\nfamily of distributions. We discuss the results with several well known\ndistributions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 15:48:52 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Hazra", "Nil Kamal", ""], ["Kundu", "Pradip", ""], ["Nanda", "Asok K.", ""]]}, {"id": "1602.05455", "submitter": "Ziwei Zhu", "authors": "Jianqing Fan, Han Liu, Weichen Wang and Ziwei Zhu", "title": "Heterogeneity Adjustment with Applications to Graphical Model Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity is an unwanted variation when analyzing aggregated datasets\nfrom multiple sources. Though different methods have been proposed for\nheterogeneity adjustment, no systematic theory exists to justify these methods.\nIn this work, we propose a generic framework named ALPHA (short for Adaptive\nLow-rank Principal Heterogeneity Adjustment) to model, estimate, and adjust\nheterogeneity from the original data. Once the heterogeneity is adjusted, we\nare able to remove the biases of batch effects and to enhance the inferential\npower by aggregating the homogeneous residuals from multiple sources. Under a\npervasive assumption that the latent heterogeneity factors simultaneously\naffect a large fraction of observed variables, we provide a rigorous theory to\njustify the proposed framework. Our framework also allows the incorporation of\ninformative covariates and appeals to the \"Bless of Dimensionality\". As an\nillustrative application of this generic framework, we consider a problem of\nestimating high-dimensional precision matrix for graphical model inference\nbased on multiple datasets. We also provide thorough numerical studies on both\nsynthetic datasets and a brain imaging dataset to demonstrate the efficacy of\nthe developed theory and methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 15:50:52 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Fan", "Jianqing", ""], ["Liu", "Han", ""], ["Wang", "Weichen", ""], ["Zhu", "Ziwei", ""]]}, {"id": "1602.05523", "submitter": "Virginie Stanislas", "authors": "Virginie Stanislas (LaMME), Cyril Dalmasso (LaMME), Christophe\n  Ambroise (LaMME)", "title": "Eigen-Epistasis for detecting Gene-Gene interactions", "comments": null, "journal-ref": "BMC Bioinformatics, BioMed Central, 2017, 18, pp.54", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of research has been devoted to the detection and\ninvestigation of epistatic interactions in genome-wide association studies\n(GWASs). Most of the literature focuses on low-order interactions between\nsingle-nucleotide polymorphisms (SNPs) with significant main effects.In this\npaper we propose an original approach for detecting epistasis at the gene\nlevel, without systematically filtering on significant genes. We first compute\ninteraction variables for each gene pair by finding its Eigen-Epistasis\ncomponent, defined as the linear combination of Gene SNPs having the highest\ncorrelation with the phenotype. The selection of significant effects is done\nusing a penalized regression method based on Group Lasso controlling the False\nDiscovery Rate.The method is tested against two recent alternative proposals\nfrom the literature using synthetic data, and shows good performances in\ndifferent settings. We demonstrate the power of our approach by detecting new\ngene-gene interactions on three genome-wide association studies.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 18:34:55 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 10:45:48 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 08:16:44 GMT"}, {"version": "v4", "created": "Mon, 20 Jun 2016 08:44:27 GMT"}, {"version": "v5", "created": "Fri, 18 Nov 2016 14:41:33 GMT"}, {"version": "v6", "created": "Thu, 16 Feb 2017 14:00:20 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Stanislas", "Virginie", "", "LaMME"], ["Dalmasso", "Cyril", "", "LaMME"], ["Ambroise", "Christophe", "", "LaMME"]]}, {"id": "1602.05540", "submitter": "Bosung Kang", "authors": "Bosung Kang, Vishal Monga, Muralidhar Rangaswamy, Yuri I. Abramovich", "title": "Robust Covariance Estimation under Imperfect Constraints using an\n  Expected Likelihood Approach", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.05069", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of structured covariance matrix estimation for radar\nspace-time adaptive processing (STAP). A priori knowledge of the interference\nenvironment has been exploited in many previous works to enable accurate\nestimators even when training is not generous. Specifically, recent work has\nshown that employing practical constraints such as the rank of clutter subspace\nand the condition number of disturbance covariance leads to powerful estimators\nthat have closed form solutions. While rank and the condition number are very\neffective constraints, often practical non-idealities makes it difficult for\nthem to be known precisely using physical models. Therefore, we propose a\nrobust covariance estimation method for radar STAP via an expected likelihood\n(EL) approach. We analyze covariance estimation algorithms under three cases of\nimperfect constraints: 1) a rank constraint, 2) both rank and noise power\nconstraints, and 3) condition number constraint. In each case, we formulate\nprecise constraint determination as an optimization problem using the EL\ncriterion. For each of the three cases, we derive new analytical results which\nallow for computationally efficient, practical ways of setting these\nconstraints. In particular, we prove formally that both the rank and condition\nnumber as determined by the EL criterion are unique. Through experimental\nresults from a simulation model and the KASSPER data set, we show the estimator\nwith optimal constraints obtained by the EL approach outperforms state of the\nart alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 21:38:48 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Kang", "Bosung", ""], ["Monga", "Vishal", ""], ["Rangaswamy", "Muralidhar", ""], ["Abramovich", "Yuri I.", ""]]}, {"id": "1602.05665", "submitter": "Le Bao", "authors": "Le Bao, Ben Sheng, Xiaoyue Niu, Yuan Tang, Tim Brown, Peter D. Ghys,\n  Jeff W. Eaton", "title": "Incorporating Hierarchical Structure Into Dynamic Systems: An\n  Application Of Estimating HIV Epidemics At Sub-National And Sub-Population\n  Level", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic models have been successfully used in producing estimates of HIV\nepidemics at national level, due to their epidemiological nature and their\nability to simultaneously estimate prevalence, incidence, and mortality rates.\nRecently, HIV interventions and policies have required more information at\nsub-national and sub-population levels to support local planning, decision\nmaking and resource allocation. Unfortunately, many areas and high-risk groups\nlack sufficient data for deriving stable and reliable results, and this is a\ncritical technical barrier to more stratified estimates. One solution is to\nborrow information from other areas and groups within the same country.\nHowever, directly assuming hierarchical structures within the HIV dynamic\nmodels is complicated and computationally time consuming. In this paper, we\npropose a simple and innovative way to incorporate the hierarchical information\ninto the dynamic systems by using auxiliary data. The proposed method\nefficiently uses information from multiple areas and risk groups within each\ncountry without increasing the computational burden. As a result, the new model\nimproves predictive ability in general with especially significant improvement\nin areas and risk groups with sparse data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 02:37:50 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Bao", "Le", ""], ["Sheng", "Ben", ""], ["Niu", "Xiaoyue", ""], ["Tang", "Yuan", ""], ["Brown", "Tim", ""], ["Ghys", "Peter D.", ""], ["Eaton", "Jeff W.", ""]]}, {"id": "1602.05757", "submitter": "Anna Gloria Bill\\'e Ph.D.", "authors": "Anna Gloria Bill\\'e, Roberto Benedetti, Paolo Postiglione", "title": "A two-step approach to account for unobserved spatial heterogeneity", "comments": null, "journal-ref": "Spatial Economic Analysis 2017", "doi": "10.1080/17421772.2017.1286373", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical analysis in economics often faces the difficulty that the data is\ncorrelated and heterogeneous in some unknown form. Spatial parametric\napproaches have been widely used to account for dependence structures, but the\nproblem of directly deal with spatially varying parameters has been largely\nunexplored. The problem can be serious in all those cases in which we have no\nprior information justified by the economic theory. In this paper we propose an\nalgorithm-based procedure which is able to endogenously identify structural\nbreaks in space. The proposed algorithm is illustrated by using two well known\nhouse price data sets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 11:18:14 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 12:04:34 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Bill\u00e9", "Anna Gloria", ""], ["Benedetti", "Roberto", ""], ["Postiglione", "Paolo", ""]]}, {"id": "1602.05795", "submitter": "Matthias Killiches", "authors": "Matthias Killiches, Daniel Kraus and Claudia Czado", "title": "Examination and visualisation of the simplifying assumption for vine\n  copulas in three dimensions", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas are a highly flexible class of dependence models, which are\nbased on the decomposition of the density into bivariate building blocks. For\napplications one usually makes the simplifying assumption that copulas of\nconditional distributions are independent of the variables on which they are\nconditioned. However this assumption has been criticised for being too\nrestrictive. We examine both simplified and non-simplified vine copulas in\nthree dimensions and investigate conceptual differences. We show and compare\ncontour surfaces of three-dimensional vine copula models, which prove to be\nmuch more informative than the contour lines of the bivariate marginals. Our\ninvestigation shows that non-simplified vine copulas can exhibit arbitrarily\nirregular shapes, whereas simplified vine copulas appear to be smooth\nextrapolations of their bivariate margins to three dimensions. In addition to a\nvariety of constructed examples, we also investigate a three-dimensional subset\nof the well-known uranium data set and visually detect that a non-simplified\nvine copula is necessary to capture its complex dependence structure.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 13:38:23 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 12:56:34 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Killiches", "Matthias", ""], ["Kraus", "Daniel", ""], ["Czado", "Claudia", ""]]}, {"id": "1602.05885", "submitter": "Jiwoong Kim", "authors": "Jiwoong Kim", "title": "Goodness-of-Fit Test: Khmaladze Transformation vs Empirical Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares two asymptotic distribution free methods for\ngoodness-of-fit test of one sample of location-scale family: Khmaladze\ntransformation and empirical likelihood methods. The comparison is made from\nthe perspective of empirical level and power obtained from simulations. When\ntesting for normal and logistic null distributions, we try various alternative\ndistributions and find that Khmaladze transformation method has better power in\nmost cases. R-package which was used for the simulation is available online.\nSee section 5 for the detail.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 17:39:22 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 05:05:07 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Kim", "Jiwoong", ""]]}, {"id": "1602.05894", "submitter": "Layla Parast", "authors": "Layla Parast, Tianxi Cai, Lu Tian", "title": "Evaluating Surrogate Marker Information using Censored Data", "comments": "This article has been submitted to Statistics in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the long follow-up periods that are often required for treatment or\nintervention studies, the potential to use surrogate markers to decrease the\nrequired follow-up time is a very attractive goal. However, previous studies\nhave shown that using inadequate markers or making inappropriate assumptions\nabout the relationship between the primary outcome and surrogate marker can\nlead to inaccurate conclusions regarding the treatment effect. Currently\navailable methods for identifying and validating surrogate markers tend to rely\non restrictive model assumptions and/or focus on uncensored outcomes. The\nability to use such methods in practice when the primary outcome of interest is\na time-to-event outcome is difficult due to censoring and missing surrogate\ninformation among those who experience the primary outcome before surrogate\nmarker measurement. In this paper, we propose a novel definition of the\nproportion of treatment effect explained by surrogate information collected up\nto a specified time in the setting of a time-to-event primary outcome. Our\nproposed approach accommodates a setting where individuals may experience the\nprimary outcome before the surrogate marker is measured. We propose a robust\nnonparametric procedure to estimate the defined quantity using censored data\nand use a perturbation-resampling procedure for variance estimation. Simulation\nstudies demonstrate that the proposed procedures perform well in finite\nsamples. We illustrate the proposed procedures by investigating two potential\nsurrogate markers for diabetes using data from the Diabetes Prevention Program.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 17:47:11 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 21:01:29 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Parast", "Layla", ""], ["Cai", "Tianxi", ""], ["Tian", "Lu", ""]]}, {"id": "1602.05906", "submitter": "Camillo Cammarota", "authors": "Camillo Cammarota", "title": "Estimating the turning point location in shifted exponential model of\n  time series", "comments": null, "journal-ref": null, "doi": null, "report-no": "Roma01.Math.MP", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distribution of the turning point location of time series\nmodeled as the sum of deterministic trend plus random noise. If the variables\nare modeled by shifted exponentials, whose location parameters define the\ntrend, we provide a formula for computing the distribution of the turning point\nlocation and consequently to estimate a confidence interval for the location.\nWe test this formula in simulated data series having a trend with asymmetric\nminimum, investigating the coverage rate as a function of a bandwidth\nparameter. The method is applied to estimate the confidence interval of the\nminimum location of the time series of RT intervals extracted from the\nelectrocardiogram recorded during the exercise test. We discuss the connection\nwith stochastic ordering.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 15:47:40 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Cammarota", "Camillo", ""]]}, {"id": "1602.06202", "submitter": "Andrew Brown", "authors": "D. Andrew Brown and Sez Atamturktur", "title": "Nonparametric Functional Calibration of Computer Models", "comments": "Statistica Sinica, 2018", "journal-ref": null, "doi": "10.5705/ss.202015.0344", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard methods in computer model calibration treat the calibration\nparameters as constant throughout the domain of control inputs. In many\napplications, systematic variation may cause the best values for the\ncalibration parameters to change between different settings. When not accounted\nfor in the code, this variation can make the computer model inadequate. In this\narticle, we propose a framework for modeling the calibration parameters as\nfunctions of the control inputs to account for a computer model's incomplete\nsystem representation in this regard while simultaneously allowing for possible\nconstraints imposed by prior expert opinion. We demonstrate how inappropriate\nmodeling assumptions can mislead a researcher into thinking a calibrated model\nis in need of an empirical discrepancy term when it is only needed to allow for\na functional dependence of the calibration parameters on the inputs. We apply\nour approach to plastic deformation of a visco-plastic self-consistent material\nin which the critical resolved shear stress is known to vary with temperature.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 16:09:53 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 21:55:07 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Brown", "D. Andrew", ""], ["Atamturktur", "Sez", ""]]}, {"id": "1602.06366", "submitter": "Linbo Wang", "authors": "Linbo Wang, Yuexia Zhang, Thomas S. Richardson, Xiao-Hua Zhou", "title": "Robust Estimation of Propensity Score Weights via Subclassification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighting estimators based on propensity scores are widely used for causal\nestimation in a variety of contexts, such as observational studies, marginal\nstructural models and interference. They enjoy appealing theoretical properties\nsuch as consistency and possible efficiency under correct model specification.\nHowever, this theoretical appeal may be diminished in practice by sensitivity\nto misspecification of the propensity score model. To improve on this, we\nborrow an idea from an alternative approach to causal effect estimation in\nobservational studies, namely subclassification estimators. It is well known\nthat compared to weighting estimators, subclassification methods are usually\nmore robust to model misspecification. In this paper, we first discuss an\nintrinsic connection between the seemingly unrelated weighting and\nsubclassification estimators, and then use this connection to construct robust\npropensity score weights via subclassification. We illustrate this idea by\nproposing so-called full-classification weights and accompanying estimators for\ncausal effect estimation in observational studies. Our novel estimators are\nboth consistent and robust to model misspecification, thereby combining the\nstrengths of traditional weighting and subclassification estimators for causal\neffect estimation from observational studies. Numerical studies show that the\nproposed estimators perform favorably compared to existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 05:19:04 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 02:57:57 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wang", "Linbo", ""], ["Zhang", "Yuexia", ""], ["Richardson", "Thomas S.", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "1602.06595", "submitter": "Avi Feller", "authors": "Avi Feller, Evan Greif, Nhat Ho, Luke Miratrix, Natesh Pillai", "title": "Weak separation in mixture models and implications for principal\n  stratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal stratification is a widely used framework for addressing\npost-randomization complications. After using principal stratification to\ndefine causal effects of interest, researchers are increasingly turning to\nfinite mixture models to estimate these quantities. Unfortunately, standard\nestimators of mixture parameters, like the MLE, are known to exhibit\npathological behavior. We study this behavior in a simple but fundamental\nexample, a two-component Gaussian mixture model in which only the component\nmeans and variances are unknown, and focus on the setting in which the\ncomponents are weakly separated. In this case, we show that the asymptotic\nconvergence rate of the MLE is quite poor, such as $O(n^{-1/6})$ or even\n$O(n^{-1/8})$. We then demonstrate via theoretical arguments as well as\nextensive simulations that, in finite samples, the MLE behaves like a threshold\nestimator, in the sense that the MLE can give strong evidence that the means\nare equal when the truth is otherwise. We also explore the behavior of the MLE\nwhen the MLE is non-zero, showing that it is difficult to estimate both the\nsign and magnitude of the means in this case. We provide diagnostics for all of\nthese pathologies and apply these ideas to re-analyzing two randomized\nevaluations of job training programs, JOBS II and Job Corps. Our results\nsuggest that the corresponding maximum likelihood estimates should be\ninterpreted with caution in these cases.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 22:50:16 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 04:10:51 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Feller", "Avi", ""], ["Greif", "Evan", ""], ["Ho", "Nhat", ""], ["Miratrix", "Luke", ""], ["Pillai", "Natesh", ""]]}, {"id": "1602.06609", "submitter": "Weixin Yao", "authors": "Weixin Yao and Sijia Xiang", "title": "Nonparametric and Varying Coefficient Modal Regression", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new nonparametric data analysis tool, which we\ncall nonparametric modal regression, to investigate the relationship among\ninterested variables based on estimating the mode of the conditional density of\na response variable Y given predictors X. The nonparametric modal regression is\ndistinguished from the conventional nonparametric regression in that, instead\nof the conditional average or median, it uses the \"most likely\" conditional\nvalues to measures the center. Better prediction performance and robustness are\ntwo important characteristics of nonparametric modal regression compared to\ntraditional nonparametric mean regression and nonparametric median regression.\nWe propose to use local polynomial regression to estimate the nonparametric\nmodal regression. The asymptotic properties of the resulting estimator are\ninvestigated. To broaden the applicability of the nonparametric modal\nregression to high dimensional data or functional/longitudinal data, we further\ndevelop a nonparametric varying coefficient modal regression. A Monte Carlo\nsimulation study and an analysis of health care expenditure data demonstrate\nsome superior performance of the proposed nonparametric modal regression model\nto the traditional nonparametric mean regression and nonparametric median\nregression in terms of the prediction performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 00:07:42 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Yao", "Weixin", ""], ["Xiang", "Sijia", ""]]}, {"id": "1602.06610", "submitter": "Weixin Yao", "authors": "Sijia Xiang and Weixin Yao", "title": "Mixture of Regression Models with Single-Index", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a class of semiparametric mixture regression\nmodels with single-index. We argue that many recently proposed\nsemiparametric/nonparametric mixture regression models can be considered\nspecial cases of the proposed model. However, unlike existing semiparametric\nmixture regression models, the new pro- posed model can easily incorporate\nmultivariate predictors into the nonparametric components. Backfitting\nestimates and the corresponding algorithms have been proposed for to achieve\nthe optimal convergence rate for both the parameters and the nonparametric\nfunctions. We show that nonparametric functions can be esti- mated with the\nsame asymptotic accuracy as if the parameters were known and the index\nparameters can be estimated with the traditional parametric root n convergence\nrate. Simulation studies and an application of NBA data have been conducted to\ndemonstrate the finite sample performance of the proposed models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 00:15:17 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Xiang", "Sijia", ""], ["Yao", "Weixin", ""]]}, {"id": "1602.06693", "submitter": "Maurizio Filippone", "authors": "Kurt Cutajar, Michael A. Osborne, John P. Cunningham, Maurizio\n  Filippone", "title": "Preconditioning Kernel Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational and storage complexity of kernel machines presents the\nprimary barrier to their scaling to large, modern, datasets. A common way to\ntackle the scalability issue is to use the conjugate gradient algorithm, which\nrelieves the constraints on both storage (the kernel matrix need not be stored)\nand computation (both stochastic gradients and parallelization can be used).\nEven so, conjugate gradient is not without its own issues: the conditioning of\nkernel matrices is often such that conjugate gradients will have poor\nconvergence in practice. Preconditioning is a common approach to alleviating\nthis issue. Here we propose preconditioned conjugate gradients for kernel\nmachines, and develop a broad range of preconditioners particularly useful for\nkernel matrices. We describe a scalable approach to both solving kernel\nmachines and learning their hyperparameters. We show this approach is exact in\nthe limit of iterations and outperforms state-of-the-art approximations for a\ngiven computational budget.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:21:48 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 08:05:52 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Cutajar", "Kurt", ""], ["Osborne", "Michael A.", ""], ["Cunningham", "John P.", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1602.06696", "submitter": "Simon Wood", "authors": "Natalya Pya and Simon N Wood", "title": "A note on basis dimension selection in generalized additive modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two new approaches for checking the dimension of the basis functions when\nusing penalized regression smoothers are presented. The first approach is a\ntest for adequacy of the basis dimension based on an estimate of the residual\nvariance calculated by differencing residuals that are neighbours according to\nthe smooth covariates. The second approach is based on estimated degrees of\nfreedom for a smooth of the model residuals with respect to the model\ncovariates. In comparison with basis dimension selection algorithms based on\nsmoothness selection criterion (GCV, AIC, REML) the above procedures are\ncomputationally efficient enough for routine use as part of model checking.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:30:12 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Pya", "Natalya", ""], ["Wood", "Simon N", ""]]}, {"id": "1602.07056", "submitter": "Lars Holden", "authors": "Lars Holden", "title": "The two subset recurrent property of Markov chains", "comments": "19 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": "Norsk Regnesentral ADMIN/01/2016", "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new type of recurrence where we divide the Markov\nchains into intervals that start when the chain enters into a subset A, then\nsample another subset B far away from A and end when the chain again return to\nA. The length of these intervals have the same distribution and if A and B are\nfar apart, almost independent of each other. A and B may be any subsets of the\nstate space that are far apart of each other and such that the movement between\nthe subsets is repeated several times in a long Markov chain. The expected\nlength of the intervals is used in a function that describes the mixing\nproperties of the chain and improves our understanding of Markov chains.\n  The paper proves a theorem that gives a bound on the variance of the estimate\nfor {\\pi}(A), the probability for A under the limiting density of the Markov\nchain. This may be used to find the length of the Markov chain that is needed\nto explore the state space sufficiently. It is shown that the length of the\nperiods between each time A is entered by the Markov chain, has a heavy tailed\ndistribution. This increases the upper bound for the variance of the estimate\n{\\pi}(A).\n  The paper gives a general guideline on how to find the optimal scaling of\nparameters in the Metropolis-Hastings simulation algorithm that implicit\ndetermine the acceptance rate. We find examples where it is optimal to have a\nmuch smaller acceptance rate than what is generally recommended in the\nliterature and also examples where the optimal acceptance rate vanishes in the\nlimit.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 07:06:32 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Holden", "Lars", ""]]}, {"id": "1602.07337", "submitter": "Hao Wu", "authors": "Hao Wu, Xinwei Deng and Naren Ramakrishnan", "title": "Sparse Estimation of Multivariate Poisson Log-Normal Models from Count\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling data with multivariate count responses is a challenging problem due\nto the discrete nature of the responses. Existing methods for univariate count\nresponses cannot be easily extended to the multivariate case since the\ndependency among multiple responses needs to be properly accommodated. In this\npaper, we propose a multivariate Poisson log-normal regression model for\nmultivariate data with count responses. By simultaneously estimating the\nregression coefficients and inverse covariance matrix over the latent variables\nwith an efficient Monte Carlo EM algorithm, the proposed regression model takes\nadvantages of association among multiple count responses to improve the model\nprediction performance. Simulation studies and applications to real world data\nare conducted to systematically evaluate the performance of the proposed method\nin comparison with conventional methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 01:27:08 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 17:15:05 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 18:33:46 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Wu", "Hao", ""], ["Deng", "Xinwei", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1602.07358", "submitter": "Rob Tibshirani", "authors": "Jonathan Taylor and Robert Tibshirani", "title": "Post-selection inference for L1-penalized likelihood models", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for post-selection inference for L1 (lasso)-penalized\nlikelihood models, including generalized regression models. Our approach\ngeneralizes the post-selection framework presented in Lee et al (2014). The\nmethod provides p-values and confidence intervals that are asymptotically\nvalid, conditional on the inherent selection done by the lasso. We present\napplications of this work to (regularized) logistic regression, Cox's\nproportional hazards model and the graphical lasso.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 00:06:43 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 05:58:33 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 22:45:51 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1602.07412", "submitter": "Matt Wand Professor", "authors": "M.P. Wand", "title": "Fast Approximate Inference for Arbitrarily Large Semiparametric\n  Regression Models via Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the notion of message passing can be used to streamline the\nalgebra and computer coding for fast approximate inference in large Bayesian\nsemiparametric regression models. In particular, this approach is amenable to\nhandling arbitrarily large models of particular types once a set of primitive\noperations is established. The approach is founded upon a message passing\nformulation of mean field variational Bayes that utilizes factor graph\nrepresentations of statistical models. The underlying principles apply to\ngeneral Bayesian hierarchical models although we focus on semiparametric\nregression. The notion of factor graph fragments is introduced and is shown to\nfacilitate compartmentalization of the required algebra and coding. The\nresultant algorithms have ready-to-implement closed form expressions and allow\na broad class of arbitrarily large semiparametric regression models to be\nhandled. Ongoing software projects such as Infer.NET and Stan support\nvariational-type inference for particular model classes. This article is not\nconcerned with software packages per se and focuses on the underlying tenets of\nscalable variational inference algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 06:34:29 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 04:46:59 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Wand", "M. P.", ""]]}, {"id": "1602.07428", "submitter": "Jun Zhu", "authors": "Jun Zhu and Jiaming Song and Bei Chen", "title": "Max-Margin Nonparametric Latent Feature Models for Link Prediction", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is a fundamental task in statistical network analysis. Recent\nadvances have been made on learning flexible nonparametric Bayesian latent\nfeature models for link prediction. In this paper, we present a max-margin\nlearning method for such nonparametric latent feature relational models. Our\napproach attempts to unite the ideas of max-margin learning and Bayesian\nnonparametrics to discover discriminative latent features for link prediction.\nIt inherits the advances of nonparametric Bayesian methods to infer the unknown\nlatent social dimension, while for discriminative link prediction, it adopts\nthe max-margin learning principle by minimizing a hinge-loss using the linear\nexpectation operator, without dealing with a highly nonlinear link likelihood\nfunction. For posterior inference, we develop an efficient stochastic\nvariational inference algorithm under a truncated mean-field assumption. Our\nmethods can scale up to large-scale real networks with millions of entities and\ntens of millions of positive links. We also provide a full Bayesian\nformulation, which can avoid tuning regularization hyper-parameters.\nExperimental results on a diverse range of real datasets demonstrate the\nbenefits inherited from max-margin learning and Bayesian nonparametric\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 08:08:05 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Zhu", "Jun", ""], ["Song", "Jiaming", ""], ["Chen", "Bei", ""]]}, {"id": "1602.07559", "submitter": "Michael Donohue", "authors": "Michael C. Donohue and Anthony C. Gamst and Robert A. Rissman and Ian\n  Abramson", "title": "Regression of ranked responses when raw responses are censored", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss semiparametric regression when only the ranks of responses are\nobserved. The model is $Y_i = F (\\mathbf{x}_i'{\\boldsymbol\\beta}_0 +\n\\varepsilon_i)$, where $Y_i$ is the unobserved response, $F$ is a monotone\nincreasing function, $\\mathbf{x}_i$ is a known $p-$vector of covariates,\n${\\boldsymbol\\beta}_0$ is an unknown $p$-vector of interest, and\n$\\varepsilon_i$ is an error term independent of $\\mathbf{x}_i$. We observe\n$\\{(\\mathbf{x}_i,R_n(Y_i)) : i = 1,\\ldots ,n\\}$, where $R_n$ is the ordinal\nrank function. We explore a novel estimator under Gaussian assumptions. We\ndiscuss the literature, apply the method to an Alzheimer's disease biomarker,\nconduct simulation studies, and prove consistency and asymptotic normality.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 15:28:33 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Donohue", "Michael C.", ""], ["Gamst", "Anthony C.", ""], ["Rissman", "Robert A.", ""], ["Abramson", "Ian", ""]]}, {"id": "1602.07640", "submitter": "Feng Liang", "authors": "Xichen Huang, Jin Wang, Feng Liang", "title": "A Variational Algorithm for Bayesian Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an intense development on the estimation of a sparse\nregression coefficient vector in statistics, machine learning and related\nfields. In this paper, we focus on the Bayesian approach to this problem, where\nsparsity is incorporated by the so-called spike-and-slab prior on the\ncoefficients. Instead of replying on MCMC for posterior inference, we propose a\nfast and scalable algorithm based on variational approximation to the posterior\ndistribution. The updating scheme employed by our algorithm is different from\nthe one proposed by Carbonetto and Stephens (2012). Those changes seem crucial\nfor us to show that our algorithm can achieve asymptotic consistency even when\nthe feature dimension diverges exponentially fast with the sample size.\nEmpirical results have demonstrated the effectiveness and efficiency of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 19:16:36 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Huang", "Xichen", ""], ["Wang", "Jin", ""], ["Liang", "Feng", ""]]}, {"id": "1602.07800", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Xiangyu Wang, Changyou Chen, Ricardo Henao, Kai Fan and\n  Lawrence Carin", "title": "Towards Unifying Hamiltonian Monte Carlo and Slice Sampling", "comments": "updated version", "journal-ref": "Advances in Neural Information Processing Systems, pages\n  1741--1749, year 2016", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling,\ndemonstrating their connection via the Hamiltonian-Jacobi equation from\nHamiltonian mechanics. This insight enables extension of HMC and slice sampling\nto a broader family of samplers, called Monomial Gamma Samplers (MGS). We\nprovide a theoretical analysis of the mixing performance of such samplers,\nproving that in the limit of a single parameter, the MGS draws decorrelated\nsamples from the desired target distribution. We further show that as this\nparameter tends toward this limit, performance gains are achieved at a cost of\nincreasing numerical difficulty and some practical convergence issues. Our\ntheoretical results are validated with synthetic data and real-world\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 05:14:45 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 01:27:26 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 02:18:51 GMT"}, {"version": "v4", "created": "Mon, 17 Oct 2016 03:13:47 GMT"}, {"version": "v5", "created": "Wed, 10 Jan 2018 19:26:02 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Zhang", "Yizhe", ""], ["Wang", "Xiangyu", ""], ["Chen", "Changyou", ""], ["Henao", "Ricardo", ""], ["Fan", "Kai", ""], ["Carin", "Lawrence", ""]]}, {"id": "1602.07933", "submitter": "Michael Schomaker", "authors": "Michael Schomaker, Christian Heumann", "title": "Bootstrap Inference when Using Multiple Imputation", "comments": null, "journal-ref": null, "doi": "10.1002/sim.7654", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern estimators require bootstrapping to calculate confidence\nintervals because either no analytic standard error is available or the\ndistribution of the parameter of interest is non-symmetric. It remains however\nunclear how to obtain valid bootstrap inference when dealing with multiple\nimputation to address missing data. We present four methods which are\nintuitively appealing, easy to implement, and combine bootstrap estimation with\nmultiple imputation. We show that three of the four approaches yield valid\ninference, but that the performance of the methods varies with respect to the\nnumber of imputed data sets and the extent of missingness. Simulation studies\nreveal the behavior of our approaches in finite samples. A topical analysis\nfrom HIV treatment research, which determines the optimal timing of\nantiretroviral treatment initiation in young children, demonstrates the\npractical implications of the four methods in a sophisticated and realistic\nsetting. This analysis suffers from missing data and uses the $g$-formula for\ninference, a method for which no standard errors are available.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 13:52:28 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 07:25:36 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 10:23:32 GMT"}, {"version": "v4", "created": "Thu, 21 Sep 2017 08:00:29 GMT"}, {"version": "v5", "created": "Tue, 13 Feb 2018 12:29:05 GMT"}, {"version": "v6", "created": "Tue, 5 Jun 2018 07:08:22 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Schomaker", "Michael", ""], ["Heumann", "Christian", ""]]}, {"id": "1602.07960", "submitter": "Ming Li", "authors": "Lijue Liu, Ming Li, Sha Wen", "title": "Measuring and Discovering Correlations in Large Data Sets", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a class of statistics named ART (the alternant recursive\ntopology statistics) is proposed to measure the properties of correlation\nbetween two variables. A wide range of bi-variable correlations both linear and\nnonlinear can be evaluated by ART efficiently and equitably even if nothing is\nknown about the specific types of those relationships. ART compensates the\ndisadvantages of Reshef's model in which no polynomial time precise algorithm\nexists and the \"local random\" phenomenon can not be identified. As a class of\nnonparametric exploration statistics, ART is applied for analyzing a dataset of\n10 American classical indexes, as a result, lots of bi-variable correlations\nare discovered.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 09:58:16 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Liu", "Lijue", ""], ["Li", "Ming", ""], ["Wen", "Sha", ""]]}, {"id": "1602.08006", "submitter": "Nicolas Verzelen", "authors": "Nicolas Verzelen and Elisabeth Gassiat", "title": "Adaptive estimation of High-Dimensional Signal-to-Noise Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the equivalent problems of estimating the residual variance, the\nproportion of explained variance $\\eta$ and the signal strength in a\nhigh-dimensional linear regression model with Gaussian random design. Our aim\nis to understand the impact of not knowing the sparsity of the regression\nparameter and not knowing the distribution of the design on minimax estimation\nrates of $\\eta$. Depending on the sparsity $k$ of the regression parameter,\noptimal estimators of $\\eta$ either rely on estimating the regression parameter\nor are based on U-type statistics, and have minimax rates depending on $k$. In\nthe important situation where $k$ is unknown, we build an adaptive procedure\nwhose convergence rate simultaneously achieves the minimax risk over all $k$ up\nto a logarithmic loss which we prove to be non avoidable. Finally, the\nknowledge of the design distribution is shown to play a critical role. When the\ndistribution of the design is unknown, consistent estimation of explained\nvariance is indeed possible in much narrower regimes than for known design\ndistribution.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 17:35:27 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 08:00:56 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Verzelen", "Nicolas", ""], ["Gassiat", "Elisabeth", ""]]}, {"id": "1602.08062", "submitter": "Junxian Geng", "authors": "Junxian Geng and Anirban Bhattacharya and Debdeep Pati", "title": "Probabilistic community detection with unknown number of communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in network analysis is clustering the nodes into groups\nwhich share a similar connectivity pattern. Existing algorithms for community\ndetection assume the knowledge of the number of clusters or estimate it a\npriori using various selection criteria and subsequently estimate the community\nstructure. Ignoring the uncertainty in the first stage may lead to erroneous\nclustering, particularly when the community structure is vague. We instead\npropose a coherent probabilistic framework for simultaneous estimation of the\nnumber of communities and the community structure, adapting recently developed\nBayesian nonparametric techniques to network models. An efficient Markov chain\nMonte Carlo (MCMC) algorithm is proposed which obviates the need to perform\nreversible jump MCMC on the number of clusters. The methodology is shown to\noutperform recently developed community detection algorithms in a variety of\nsynthetic data examples and in benchmark real-datasets. Using an appropriate\nmetric on the space of all configurations, we develop non-asymptotic Bayes risk\nbounds even when the number of clusters is unknown. Enroute, we develop\nconcentration properties of non-linear functions of Bernoulli random variables,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 20:01:22 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 17:16:26 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 00:24:39 GMT"}, {"version": "v4", "created": "Thu, 29 Mar 2018 02:40:19 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Geng", "Junxian", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "1602.08154", "submitter": "Gregor Kastner", "authors": "Gregor Kastner, Sylvia Fr\\\"uhwirth-Schnatter, Hedibert Freitas Lopes", "title": "Efficient Bayesian Inference for Multivariate Factor Stochastic\n  Volatility Models", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 26(4), 905-917\n  (2017)", "doi": "10.1080/10618600.2017.1322091", "report-no": null, "categories": "stat.CO econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss efficient Bayesian estimation of dynamic covariance matrices in\nmultivariate time series through a factor stochastic volatility model. In\nparticular, we propose two interweaving strategies (Yu and Meng, Journal of\nComputational and Graphical Statistics, 20(3), 531-570, 2011) to substantially\naccelerate convergence and mixing of standard MCMC approaches. Similar to\nmarginal data augmentation techniques, the proposed acceleration procedures\nexploit non-identifiability issues which frequently arise in factor models. Our\nnew interweaving strategies are easy to implement and come at almost no extra\ncomputational cost; nevertheless, they can boost estimation efficiency by\nseveral orders of magnitude as is shown in extensive simulation studies. To\nconclude, the application of our algorithm to a 26-dimensional exchange rate\ndata set illustrates the superior performance of the new approach for\nreal-world data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 00:01:27 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:07:01 GMT"}, {"version": "v3", "created": "Wed, 19 Jul 2017 13:11:14 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Kastner", "Gregor", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Lopes", "Hedibert Freitas", ""]]}, {"id": "1602.08450", "submitter": "Ricardo Ehlers", "authors": "Paulo Ferreira, Jhon Gonzales, Vera Tomazella, Ricardo Ehlers,\n  Francisco Louzada, Eveliny Silva", "title": "Objective Bayesian Analysis for the Lomax Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to make Bayesian inferences for the parameters of\nthe Lomax distribution using non-informative priors, namely the Jeffreys prior\nand the reference prior. We assess Bayesian estimation through a Monte Carlo\nstudy with 500 simulated data sets. To evaluate the possible impact of prior\nspecification on estimation, two criteria were considered: the bias and square\nroot of the mean square error. The developed procedures are illustrated on a\nreal data set.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 19:41:21 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Ferreira", "Paulo", ""], ["Gonzales", "Jhon", ""], ["Tomazella", "Vera", ""], ["Ehlers", "Ricardo", ""], ["Louzada", "Francisco", ""], ["Silva", "Eveliny", ""]]}, {"id": "1602.08521", "submitter": "Olga Savchuk Y", "authors": "Olga Y. Savchuk and Jeffrey D. Hart", "title": "Theoretical Properties and Practical Performance of Fully Robust\n  One-Sided Cross-Validation", "comments": "9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully robust OSCV is a modification of the OSCV method that produces\nconsistent bandwidth in the cases of smooth and nonsmooth regression functions.\nThe current implementation of the method uses the kernel $H_I$ that is almost\nindistinguishable from the Gaussian kernel on the interval $[-4,4]$, but has\nnegative tails. The theoretical properties and practical performances of the\n$H_I$- and $\\phi$-based OSCV versions are compared. The kernel $H_I$ tends to\nproduce too low bandwidths in the smooth case. The $H_I$-based OSCV curves are\nshown to have wiggles appearing in the neighborhood of zero. The kernel $H_I$\nuncovers sensitivity of the OSCV method to a tiny modification of the kernel\nused for the cross-validation purposes. The recently found robust bimodal\nkernels tend to produce OSCV curves with multiple local minima. The problem of\nfinding a robust unimodal nonnegative kernel remains open.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 22:18:49 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Savchuk", "Olga Y.", ""], ["Hart", "Jeffrey D.", ""]]}, {"id": "1602.08548", "submitter": "Dandan Jiang", "authors": "Dandan Jiang and QiBin Zhang", "title": "Modifications of Wald's score tests on large dimensional covariance\n  matrices structure", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers testing the covariance matrices structure based on\nWald's score test in large dimensional setting. The hypothesis $H_0: \\Sigma\n=\\Sigma_0 $ for a given matrix $\\Sigma_0$, which covers the identity hypothesis\ntest and sphericity hypothesis test as the special cases, is reviewed by the\ngeneralized CLT (Central Limit Theorem) for the linear spectral statistics of\nlarge dimensional sample covariance matrices from Jiang(2015) . The proposed\ntests can be applicable for large dimensional non-Gaussian variables in a wider\nrange. Furthermore, the simulation study is provided to compare the proposed\ntests with other large dimensional covariance matrix tests for evaluation of\ntheir performances. As seen from the simulation results, our proposed tests are\nfeasible for large dimensional data without restriction of population\ndistribution and provide the accurate and steady empirical sizes, which are\nalmost around the nominal size.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 03:44:46 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Jiang", "Dandan", ""], ["Zhang", "QiBin", ""]]}, {"id": "1602.08558", "submitter": "Saptarshi Chakraborty", "authors": "Saptarshi Chakraborty and Kshitij Khare (Department of Statistics,\n  University of Florida)", "title": "Convergence properties of Gibbs samplers for Bayesian probit regression\n  with proper priors", "comments": "40 pages, 6 figures; typos corrected", "journal-ref": "Electron. J. Statist. 11, no. 1, 177--210 (2017)", "doi": "10.1214/16-EJS1219", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian probit regression model (Albert and Chib (1993)) is popular and\nwidely used for binary regression. While the improper flat prior for the\nregression coefficients is an appropriate choice in the absence of any prior\ninformation, a proper normal prior is desirable when prior information is\navailable or in modern high dimensional settings where the number of\ncoefficients ($p$) is greater than the sample size ($n$). For both choices of\npriors, the resulting posterior density is intractable and a Data Dugmentation\n(DA) Markov chain is used to generate approximate samples from the posterior\ndistribution. Establishing geometric ergodicity for this DA Markov chain is\nimportant as it provides theoretical guarantees for constructing standard\nerrors for Markov chain based estimates of posterior quantities. In this paper,\nwe first show that in case of proper normal priors, the DA Markov chain is\ngeometrically ergodic *for all* choices of the design matrix $X$, $n$ and $p$\n(unlike the improper prior case, where $n \\geq p$ and another condition on $X$\nare required for posterior propriety itself). We also derive sufficient\nconditions under which the DA Markov chain is trace-class, i.e., the\neigenvalues of the corresponding operator are summable. In particular, this\nallows us to conclude that the Haar PX-DA sandwich algorithm (obtained by\ninserting an inexpensive extra step in between the two steps of the DA\nalgorithm) is strictly better than the DA algorithm in an appropriate sense.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 05:52:23 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 18:33:08 GMT"}, {"version": "v3", "created": "Sat, 5 Mar 2016 13:49:19 GMT"}, {"version": "v4", "created": "Tue, 3 Jan 2017 04:48:42 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Chakraborty", "Saptarshi", "", "Department of Statistics,\n  University of Florida"], ["Khare", "Kshitij", "", "Department of Statistics,\n  University of Florida"]]}, {"id": "1602.08590", "submitter": "Marcelo Pereyra", "authors": "Marcelo Pereyra", "title": "Maximum-a-posteriori estimation with Bayesian confidence regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solutions to inverse problems that are ill-conditioned or ill-posed may have\nsignificant intrinsic uncertainty. Unfortunately, analysing and quantifying\nthis uncertainty is very challenging, particularly in high-dimensional\nproblems. As a result, while most modern mathematical imaging methods produce\nimpressive point estimation results, they are generally unable to quantify the\nuncertainty in the solutions delivered. This paper presents a new general\nmethodology for approximating Bayesian high-posterior-density credibility\nregions in inverse problems that are convex and potentially very\nhigh-dimensional. The approximations are derived by using recent concentration\nof measure results related to information theory for log-concave random\nvectors. A remarkable property of the approximations is that they can be\ncomputed very efficiently, even in large-scale problems, by using standard\nconvex optimisation techniques. In particular, they are available as a\nby-product in problems solved by maximum-a-posteriori estimation. The\napproximations also have favourable theoretical properties, namely they\nouter-bound the true high-posterior-density credibility regions, and they are\nstable with respect to model dimension. The proposed methodology is illustrated\non two high-dimensional imaging inverse problems related to tomographic\nreconstruction and sparse deconvolution, where the approximations are used to\nperform Bayesian hypothesis tests and explore the uncertainty about the\nsolutions, and where proximal Markov chain Monte Carlo algorithms are used as\nbenchmark to compute exact credible regions and measure the approximation\nerror.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 13:34:38 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 15:03:13 GMT"}, {"version": "v3", "created": "Mon, 11 Jul 2016 12:33:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Pereyra", "Marcelo", ""]]}, {"id": "1602.08787", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen, Geoffrey J McLachlan, Pierre Orban, Pierre Bellec,\n  Andrew L Janke", "title": "Maximum Pseudolikelihood Estimation for Model-Based Clustering of Time\n  Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of autoregressions (MoAR) models provide a model-based approach to\nthe clustering of time series data. The maximum likelihood (ML) estimation of\nMoAR models requires the evaluation of products of large numbers of densities\nof normal random variables. In practical scenarios, these products converge to\nzero as the length of the time series increases, and thus the ML estimation of\nMoAR models becomes infeasible without the use of numerical tricks. We propose\na maximum pseudolikelihood (MPL) estimation approach as an alternative to the\nuse of numerical tricks. The MPL estimator is proved to be consistent and can\nbe computed via an EM (expectation--maximization) algorithm. Simulations are\nused to assess the performance of the MPL estimator against that of the ML\nestimator in cases where the latter was able to be calculated. An application\nto the clustering of time series data arising from a resting-state fMRI\nexperiment is presented as a demonstration of the methodology.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 00:11:22 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 03:23:32 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Nguyen", "Hien D", ""], ["McLachlan", "Geoffrey J", ""], ["Orban", "Pierre", ""], ["Bellec", "Pierre", ""], ["Janke", "Andrew L", ""]]}, {"id": "1602.08793", "submitter": "Meng Li", "authors": "Meng Li, Kehui Wang, Arnab Maity and Ana-Maria Staicu", "title": "Inference in Functional Linear Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study statistical inference in functional quantile\nregression for scalar response and a functional covariate. Specifically, we\nconsider a functional linear quantile regression model where the effect of the\ncovariate on the quantile of the response is modeled through the inner product\nbetween the functional covariate and an unknown smooth regression parameter\nfunction that varies with the level of quantile. The objective is to test that\nthe regression parameter is constant across several quantile levels of\ninterest. The parameter function is estimated by combining ideas from\nfunctional principal component analysis and quantile regression. An adjusted\nWald testing procedure is proposed for this hypothesis of interest, and its\nchi-square asymptotic null distribution is derived. The testing procedure is\ninvestigated numerically in simulations involving sparse and noisy functional\ncovariates and in a capital bike share data application. The proposed approach\nis easy to implement and the {\\tt R} code is published online at\n\\url{https://github.com/xylimeng/fQR-testing}.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 01:09:26 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 02:36:47 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Li", "Meng", ""], ["Wang", "Kehui", ""], ["Maity", "Arnab", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1602.08807", "submitter": "Boris Beranger", "authors": "Boris Beranger, Tarn Duong, Sarah E. Perkins-Kirkpatrick, Scott A.\n  Sisson", "title": "Exploratory data analysis for moderate extreme values using\n  non-parametric kernel methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many settings it is critical to accurately model the extreme tail\nbehaviour of a random process. Non-parametric density estimation methods are\ncommonly implemented as exploratory data analysis techniques for this purpose\nas they possess excellent visualisation properties, and can naturally avoid the\nmodel specification biases implied by using parametric estimators. In\nparticular, kernel-based estimators place minimal assumptions on the data, and\nprovide improved visualisation over scatterplots and histograms. However kernel\ndensity estimators are known to perform poorly when estimating extreme tail\nbehaviour, which is important when interest is in process behaviour above some\nlarge threshold, and they can over-emphasise bumps in the density for heavy\ntailed data. In this article we develop a transformation kernel density\nestimator, and demonstrate that its mean integrated squared error (MISE)\nefficiency is equivalent to that of standard, non-tail focused kernel density\nestimators. Estimator performance is illustrated in numerical studies, and in\nan expanded analysis of the ability of well known global climate models to\nreproduce observed temperature extremes in Sydney, Australia.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 03:14:29 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 05:01:01 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 23:29:57 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Beranger", "Boris", ""], ["Duong", "Tarn", ""], ["Perkins-Kirkpatrick", "Sarah E.", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1602.08927", "submitter": "Martin Spindler", "authors": "Ye Luo and Martin Spindler", "title": "High-Dimensional $L_2$Boosting: Rate of Convergence", "comments": "19 pages, 4 tables; AMS 2000 subject classifications: Primary 62J05,\n  62J07, 41A25; secondary 49M15, 68Q32", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is one of the most significant developments in machine learning.\nThis paper studies the rate of convergence of $L_2$Boosting, which is tailored\nfor regression, in a high-dimensional setting. Moreover, we introduce so-called\n\\textquotedblleft post-Boosting\\textquotedblright. This is a post-selection\nestimator which applies ordinary least squares to the variables selected in the\nfirst stage by $L_2$Boosting. Another variant is \\textquotedblleft Orthogonal\nBoosting\\textquotedblright\\ where after each step an orthogonal projection is\nconducted. We show that both post-$L_2$Boosting and the orthogonal boosting\nachieve the same rate of convergence as LASSO in a sparse, high-dimensional\nsetting. We show that the rate of convergence of the classical $L_2$Boosting\ndepends on the design matrix described by a sparse eigenvalue constant. To show\nthe latter results, we derive new approximation results for the pure greedy\nalgorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also\nintroduce feasible rules for early stopping, which can be easily implemented\nand used in applied work. Our results also allow a direct comparison between\nLASSO and boosting which has been missing from the literature. Finally, we\npresent simulation studies and applications to illustrate the relevance of our\ntheoretical results and to provide insights into the practical aspects of\nboosting. In these simulation studies, post-$L_2$Boosting clearly outperforms\nLASSO.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 12:05:53 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 14:35:38 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Luo", "Ye", ""], ["Spindler", "Martin", ""]]}, {"id": "1602.09082", "submitter": "Sergei Rodionov", "authors": "Sergei Rodionov", "title": "A comparison of two methods for detecting abrupt changes in the variance\n  of climatic time series", "comments": "32 pages, 11 figures", "journal-ref": "Adv. Stat. Clim. Meteorol. Oceanogr., 2, 63-78 (2016)", "doi": "10.5194/ascmo-2-63-2016", "report-no": null, "categories": "stat.AP physics.ao-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two methods for detecting abrupt shifts in the variance, Integrated\nCumulative Sum of Squares (ICSS) and Sequential Regime Shift Detector (SRSD),\nhave been compared on both synthetic and observed time series. In Monte Carlo\nexperiments, SRSD outperformed ICSS in the overwhelming majority of the\nmodelled scenarios with different sequences of variance regimes. The SRSD\nadvantage was particularly apparent in the case of outliers in the series. When\ntested on climatic time series, in most cases both methods detected the same\nchange points in the longer series (252-787 monthly values). The only exception\nwas the Arctic Ocean SST series, when ICSS found one extra change point that\nappeared to be spurious. As for the shorter time series (66-136 yearly values),\nICSS failed to detect any change points even when the variance doubled or\ntripled from one regime to another. For these time series, SRSD is recommended.\nInterestingly, all the climatic time series tested, from the Arctic to the\nTropics, had one thing in common: the last shift detected in each of these\nseries was toward a high-variance regime. This is consistent with other\nfindings of increased climate variability in recent decades.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 18:35:22 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Rodionov", "Sergei", ""]]}, {"id": "1602.09100", "submitter": "Libo Wang", "authors": "Libo Wang, Yuanyuan Tang, Debajyoti Sinha, Debdeep Pati, and Stuart\n  Lipsitz", "title": "Bayesian Variable Selection for Skewed Heteroscedastic Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose new Bayesian methods for selecting and estimating\na sparse coefficient vector for skewed heteroscedastic response. Our novel\nBayesian procedures effectively estimate the median and other quantile\nfunctions, accommodate non-local prior for regression effects without\ncompromising ease of implementation via sampling based tools, and\nasymptotically select the true set of predictors even when the number of\ncovariates increases in the same order of the sample size. We also extend our\nmethod to deal with some observations with very large errors. Via simulation\nstudies and a re-analysis of a medical cost study with large number of\npotential predictors, we illustrate the ease of implementation and other\npractical advantages of our approach compared to existing methods for such\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 19:02:51 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 07:17:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Wang", "Libo", ""], ["Tang", "Yuanyuan", ""], ["Sinha", "Debajyoti", ""], ["Pati", "Debdeep", ""], ["Lipsitz", "Stuart", ""]]}, {"id": "1602.09128", "submitter": "Wei Ning", "authors": "Ramadha D. Piyadi Gamage, Wei Ning and Arjun K. Gupta", "title": "Adjusted Empirical Likelihood for Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical likelihood method has been applied to dependent observations by\nMonti (1997) through the Whittle's estimation method. Similar asymptotic\ndistribution of the empirical likelihood ratio statistic for stationary time\nseries has been derived to construct the confidence regions for the parameters.\nHowever, required numerical problem of computing profile empirical likelihood\nfunction which involves constrained maximization has no solution sometimes,\nwhich leads to the drawbacks of using the original version of the empirical\nlikelihood ratio. In this paper, we propose an adjusted empirical likelihood\nratio statistic to modify the one proposed by Monti so that it guarantees the\nexistence of the solution of the required maximization problem, while\nmaintaining the similar asymptotic properties as Monti obtained. Simulations\nhave been conducted to illustrate the coverage probabilities obtained by the\nadjusted version for different time series models which are better than the\nones obtained by Monti's version, especially for small sample sizes.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 20:34:41 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Gamage", "Ramadha D. Piyadi", ""], ["Ning", "Wei", ""], ["Gupta", "Arjun K.", ""]]}]