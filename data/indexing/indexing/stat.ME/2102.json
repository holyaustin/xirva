[{"id": "2102.00058", "submitter": "Jae-Kwang Kim", "authors": "Hengfang Wang, Jae-Kwang Kim", "title": "Statistical Inference after Kernel Ridge Regression Imputation under\n  item nonresponse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Imputation is a popular technique for handling missing data. We consider a\nnonparametric approach to imputation using the kernel ridge regression\ntechnique and propose consistent variance estimation. The proposed variance\nestimator is based on a linearization approach which employs the entropy method\nto estimate the density ratio. The root-n consistency of the imputation\nestimator is established when a Sobolev space is utilized in the kernel ridge\nregression imputation, which enables us to develop the proposed variance\nestimator. Synthetic data experiments are presented to confirm our theory.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 20:46:33 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Hengfang", ""], ["Kim", "Jae-Kwang", ""]]}, {"id": "2102.00067", "submitter": "Lingjing Jiang", "authors": "Lingjing Jiang, Chris Elrod, Jane J. Kim, Austin D. Swafford, Rob\n  Knight, Wesley K. Thompson", "title": "Multi-Block Sparse Functional Principal Components Analysis for\n  Longitudinal Microbiome Multi-Omics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microbiome researchers often need to model the temporal dynamics of multiple\ncomplex, nonlinear outcome trajectories simultaneously. This motivates our\ndevelopment of multivariate Sparse Functional Principal Components Analysis\n(mSFPCA), extending existing SFPCA methods to simultaneously characterize\nmultiple temporal trajectories and their inter-relationships. As with existing\nSFPCA methods, the mSFPCA algorithm characterizes each trajectory as a smooth\nmean plus a weighted combination of the smooth major modes of variation about\nthe mean, where the weights are given by the component scores for each subject.\nUnlike existing SFPCA methods, the mSFPCA algorithm allows estimation of\nmultiple trajectories simultaneously, such that the component scores, which are\nconstrained to be independent within a particular outcome for identifiability,\nmay be arbitrarily correlated with component scores for other outcomes. A\nCholesky decomposition is used to estimate the component score covariance\nmatrix efficiently and guarantee positive semi-definiteness given these\nconstraints. Mutual information is used to assess the strength of marginal and\nconditional temporal associations across outcome trajectories. Importantly, we\nimplement mSFPCA as a Bayesian algorithm using R and stan, enabling easy use of\npackages such as PSIS-LOO for model selection and graphical posterior\npredictive checks to assess the validity of mSFPCA models. Although we focus on\napplication of mSFPCA to microbiome data in this paper, the mSFPCA model is of\ngeneral utility and can be used in a wide range of real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 21:05:25 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 21:57:57 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jiang", "Lingjing", ""], ["Elrod", "Chris", ""], ["Kim", "Jane J.", ""], ["Swafford", "Austin D.", ""], ["Knight", "Rob", ""], ["Thompson", "Wesley K.", ""]]}, {"id": "2102.00102", "submitter": "Ivana Malenica", "authors": "Ivana Malenica, Aurelien Bibaut and Mark J. van der Laan", "title": "Adaptive Sequential Design for a Single Time-Series", "comments": "arXiv admin note: text overlap with arXiv:1809.00734", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The current work is motivated by the need for robust statistical methods for\nprecision medicine; as such, we address the need for statistical methods that\nprovide actionable inference for a single unit at any point in time. We aim to\nlearn an optimal, unknown choice of the controlled components of the design in\norder to optimize the expected outcome; with that, we adapt the randomization\nmechanism for future time-point experiments based on the data collected on the\nindividual over time. Our results demonstrate that one can learn the optimal\nrule based on a single sample, and thereby adjust the design at any point t\nwith valid inference for the mean target parameter. This work provides several\ncontributions to the field of statistical precision medicine. First, we define\na general class of averages of conditional causal parameters defined by the\ncurrent context for the single unit time-series data. We define a nonparametric\nmodel for the probability distribution of the time-series under few\nassumptions, and aim to fully utilize the sequential randomization in the\nestimation procedure via the double robust structure of the efficient influence\ncurve of the proposed target parameter. We present multiple\nexploration-exploitation strategies for assigning treatment, and methods for\nestimating the optimal rule. Lastly, we present the study of the data-adaptive\ninference on the mean under the optimal treatment rule, where the target\nparameter adapts over time in response to the observed context of the\nindividual. Our target parameter is pathwise differentiable with an efficient\ninfluence function that is doubly robust - which makes it easier to estimate\nthan previously proposed variations. We characterize the limit distribution of\nour estimator under a Donsker condition expressed in terms of a notion of\nbracketing entropy adapted to martingale settings.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 22:51:45 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 15:16:45 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Malenica", "Ivana", ""], ["Bibaut", "Aurelien", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2102.00136", "submitter": "Yoshiyuki Ninomiya", "authors": "Daeju Kim, Shuichi Kawano, Yoshiyuki Ninomiya", "title": "Smoothly varying ridge regularization", "comments": "21 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A basis expansion with regularization methods is much appealing to the\nflexible or robust nonlinear regression models for data with complex\nstructures. When the underlying function has inhomogeneous smoothness, it is\nwell known that conventional reguralization methods do not perform well. In\nthis case, an adaptive procedure such as a free-knot spline or a local\nlikelihood method is often introduced as an effective method. However, both\nmethods need intensive computational loads. In this study, we consider a new\nefficient basis expansion by proposing a smoothly varying regularization method\nwhich is constructed by some special penalties. We call them adaptive-type\npenalties. In our modeling, adaptive-type penalties play key rolls and it has\nbeen successful in giving good estimation for inhomogeneous smoothness\nfunctions. A crucial issue in the modeling process is the choice of a suitable\nmodel among candidates. To select the suitable model, we derive an approximated\ngeneralized information criterion (GIC). The proposed method is investigated\nthrough Monte Carlo simulations and real data analysis. Numerical results\nsuggest that our method performs well in various situations.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 02:58:13 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kim", "Daeju", ""], ["Kawano", "Shuichi", ""], ["Ninomiya", "Yoshiyuki", ""]]}, {"id": "2102.00208", "submitter": "Christian M. Dahl", "authors": "Christian M. Dahl, Emil N. S{\\o}rensen", "title": "Time Series (re)sampling using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel bootstrap procedure for dependent data based on Generative\nAdversarial networks (GANs). We show that the dynamics of common stationary\ntime series processes can be learned by GANs and demonstrate that GANs trained\non a single sample path can be used to generate additional samples from the\nprocess. We find that temporal convolutional neural networks provide a suitable\ndesign for the generator and discriminator, and that convincing samples can be\ngenerated on the basis of a vector of iid normal noise. We demonstrate the\nfinite sample properties of GAN sampling and the suggested bootstrap using\nsimulations where we compare the performance to circular block bootstrapping in\nthe case of resampling an AR(1) time series processes. We find that resampling\nusing the GAN can outperform circular block bootstrapping in terms of empirical\ncoverage.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 10:58:15 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dahl", "Christian M.", ""], ["S\u00f8rensen", "Emil N.", ""]]}, {"id": "2102.00249", "submitter": "Evandro Konzen", "authors": "Evandro Konzen, Yafeng Cheng, Jian Qing Shi", "title": "Gaussian Process for Functional Data Analysis: The GPFDA Package for R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and describe the GPFDA package for R. The package provides\nflexible functionalities for dealing with Gaussian process regression (GPR)\nmodels for functional data. Multivariate functional data, functional data with\nmultidimensional inputs, and nonseparable and/or nonstationary covariance\nstructures can be modeled. In addition, the package fits functional regression\nmodels where the mean function depends on scalar and/or functional covariates\nand the covariance structure is modeled by a GPR model. In this paper, we\npresent the versatility of GPFDA with respect to mean function and covariance\nfunction specifications and illustrate the implementation of estimation and\nprediction of some models through reproducible numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:45:07 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Konzen", "Evandro", ""], ["Cheng", "Yafeng", ""], ["Shi", "Jian Qing", ""]]}, {"id": "2102.00305", "submitter": "Bo Ning", "authors": "Bo Ning", "title": "Spike and slab Bayesian sparse principal component analysis", "comments": "27 pages, 5 tables, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sparse principal component analysis (PCA) is a popular tool for dimensional\nreduction of high-dimensional data. Despite its massive popularity, there is\nstill a lack of theoretically justifiable Bayesian sparse PCA that is\ncomputationally scalable. A major challenge is choosing a suitable prior for\nthe loadings matrix, as principal components are mutually orthogonal. We\npropose a spike and slab prior that meets this orthogonality constraint and\nshow that the posterior enjoys both theoretical and computational advantages.\nTwo computational algorithms, the PX-CAVI and the PX-EM algorithms, are\ndeveloped. Both algorithms use parameter expansion to deal with the\northogonality constraint and to accelerate their convergence speeds. We found\nthat the PX-CAVI algorithm has superior empirical performance than the PX-EM\nalgorithm and two other penalty methods for sparse PCA. The PX-CAVI algorithm\nis then applied to study a lung cancer gene expression dataset. $\\mathsf{R}$\npackage $\\mathsf{VBsparsePCA}$ with an implementation of the algorithm is\navailable on The Comprehensive R Archive Network.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 20:28:30 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ning", "Bo", ""]]}, {"id": "2102.00330", "submitter": "Nathan James", "authors": "Nathan T. James, Frank E. Harrell Jr., Bryan E. Shepherd", "title": "Bayesian Cumulative Probability Models for Continuous and Mixed Outcomes", "comments": "30 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ordinal cumulative probability models (CPMs) -- also known as cumulative link\nmodels -- such as the proportional odds regression model are typically used for\ndiscrete ordered outcomes, but can accommodate both continuous and mixed\ndiscrete/continuous outcomes since these are also ordered. Recent papers by Liu\net al. and Tian et al. describe ordinal CPMs in this setting using\nnon-parametric maximum likelihood estimation. We formulate a Bayesian CPM for\ncontinuous or mixed outcome data. Bayesian CPMs inherit many of the benefits of\nfrequentist CPMs and have advantages with regard to interpretation,\nflexibility, and exact inference (within simulation error) for parameters and\nfunctions of parameters. We explore characteristics of the Bayesian CPM through\nsimulations and a case study using HIV biomarker data. In addition, we provide\nthe package 'bayesCPM' which implements Bayesian CPM models using the R\ninterface to the Stan probabilistic programing language. The Bayesian CPM for\ncontinuous outcomes can be implemented with only minor modifications to the\nprior specification and -- despite several limitations -- has generally good\nstatistical performance with moderate or large sample sizes.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 22:25:46 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["James", "Nathan T.", ""], ["Harrell", "Frank E.", "Jr."], ["Shepherd", "Bryan E.", ""]]}, {"id": "2102.00384", "submitter": "Chanwoo Lee", "authors": "Chanwoo Lee, Miaoyan Wang", "title": "Beyond the Signs: Nonparametric Tensor Completion via Sign Series", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of tensor estimation from noisy observations with\npossibly missing entries. A nonparametric approach to tensor completion is\ndeveloped based on a new model which we coin as sign representable tensors. The\nmodel represents the signal tensor of interest using a series of structured\nsign tensors. Unlike earlier methods, the sign series representation\neffectively addresses both low- and high-rank signals, while encompassing many\nexisting tensor models -- including CP models, Tucker models, single index\nmodels, several hypergraphon models -- as special cases. We show that the sign\ntensor series is theoretically characterized, and computationally estimable,\nvia classification tasks with carefully-specified weights. Excess risk bounds,\nestimation error rates, and sample complexities are established. We demonstrate\nthe outperformance of our approach over previous methods on two datasets, one\non human brain connectivity networks and the other on topic data mining.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 05:27:01 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lee", "Chanwoo", ""], ["Wang", "Miaoyan", ""]]}, {"id": "2102.00391", "submitter": "Jiangeng Huang", "authors": "Jiangeng Huang and Robert B. Gramacy", "title": "Multi-output calibration of a honeycomb seal via on-site surrogates", "comments": "39 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider large-scale industrial computer model calibration, combining\nmulti-output simulation with limited physical observation, involved in the\ndevelopment of a honeycomb seal. Toward that end, we adopt a localized sampling\nand emulation strategy called \"on-site surrogates (OSSs)\", designed to cope\nwith the amalgamated challenges of high-dimensional inputs, large-scale\nsimulation campaigns, and nonstationary response surfaces. In previous\napplications, OSSs were one-at-a-time affairs for multiple outputs. We\ndemonstrate that this leads to dissonance in calibration efforts for a common\nparameter set across outputs for the honeycomb. Instead, a conceptually\nstraightforward, but implementationally intricate, principal-components\nrepresentation, adapted from ordinary Gaussian process surrogate modeling to\nthe OSS setting, can resolve this tension. With a two-pronged -\noptimization-based and fully Bayesian - approach, we show how pooled\ninformation across outputs can reduce uncertainty and enhance (statistical and\ncomputational) efficiency in calibrated parameters for the honeycomb relative\nto the previous, \"data-poor\" univariate analog.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 06:25:11 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Huang", "Jiangeng", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "2102.00409", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson, Kijoeng Nam, Dai Feng", "title": "Nonparametric Analysis of Delayed Treatment Effects using\n  Single-Crossing Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials involving novel immuno-oncology (IO) therapies frequently\nexhibit survival profiles which violate the proportional hazards assumption due\nto a delay in treatment effect, and in such settings, the survival curves in\nthe two treatment arms may have a crossing before the two curves eventually\nseparate. To flexibly model such scenarios, we describe a nonparametric\napproach for estimating the treatment arm-specific survival functions which\nconstrains these two survival functions to cross at most once without making\nany additional assumptions about how the survival curves are related. A main\nadvantage of our approach is that it provides an estimate of a crossing time if\nsuch a crossing exists, and moreover, our method generates interpretable\nmeasures of treatment benefit including crossing-conditional survival\nprobabilities and crossing-conditional estimates of restricted residual mean\nlife. We demonstrate the use and effectiveness of our approach with a large\nsimulation study and an analysis of reconstructed outcomes from a recent\ncombination-therapy trial.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 08:25:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Nam", "Kijoeng", ""], ["Feng", "Dai", ""]]}, {"id": "2102.00415", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz", "title": "Ordinal Trees and Random Forests: Score-Free Recursive Partitioning and\n  Improved Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Existing ordinal trees and random forests typically use scores that are\nassigned to the ordered categories, which implies that a higher scale level is\nused. Versions of ordinal trees are proposed that take the scale level\nseriously and avoid the assignment of artificial scores. The basic construction\nprinciple is based on an investigation of the binary models that are implicitly\nused in parametric ordinal regression. These building blocks can be fitted by\ntrees and combined in a similar way as in parametric models. The obtained trees\nuse the ordinal scale level only. Since binary trees and random forests are\nconstituent elements of the trees one can exploit the wide range of binary\ntrees that have already been developed. A further topic is the potentially poor\nperformance of random forests, which seems to have ignored in the literature.\nEnsembles that include parametric models are proposed to obtain prediction\nmethods that tend to perform well in a wide range of settings. The performance\nof the methods is evaluated empirically by using several data sets.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 08:58:23 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Tutz", "Gerhard", ""]]}, {"id": "2102.00630", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Johannes Ruf, Martin Larsson, Wouter Koolen", "title": "Testing exchangeability: fork-convexity, supermartingales, and\n  e-processes", "comments": "34 pages, 7 figures, accepted at the International Journal of\n  Approximate Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we observe an infinite series of coin flips $X_1,X_2,\\ldots$, and\nwish to sequentially test the null that these binary random variables are\nexchangeable. Nonnegative supermartingales (NSMs) are a workhorse of sequential\ninference, but we prove that they are powerless for this problem. First,\nutilizing a geometric concept called fork-convexity (a sequential analog of\nconvexity), we show that any process that is an NSM under a set of\ndistributions, is also necessarily an NSM under their \"fork-convex hull\".\nSecond, we demonstrate that the fork-convex hull of the exchangeable null\nconsists of all possible laws over binary sequences; this implies that any NSM\nunder exchangeability is necessarily nonincreasing, hence always yields a\npowerless test for any alternative. Since testing arbitrary deviations from\nexchangeability is information theoretically impossible, we focus on Markovian\nalternatives. We combine ideas from universal inference and the method of\nmixtures to derive a \"safe e-process\", which is a nonnegative process with\nexpectation at most one under the null at any stopping time, and is upper\nbounded by a martingale, but is not itself an NSM. This in turn yields a level\n$\\alpha$ sequential test that is consistent; regret bounds from universal\ncoding also demonstrate rate-optimal power. We present ways to extend these\nresults to any finite alphabet and to Markovian alternatives of any order using\na \"double mixture\" approach. We provide an array of simulations, and give\ngeneral approaches based on betting for unstructured or ill-specified\nalternatives. Finally, inspired by Shafer, Vovk, and Ville, we provide\ngame-theoretic interpretations of our e-processes and pathwise results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 04:31:44 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 23:31:09 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 14:51:43 GMT"}, {"version": "v4", "created": "Fri, 23 Jul 2021 17:02:11 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Ruf", "Johannes", ""], ["Larsson", "Martin", ""], ["Koolen", "Wouter", ""]]}, {"id": "2102.00751", "submitter": "Jason Lin", "authors": "Jason Z. Lin and Jelena Bradic", "title": "Learning to Combat Noisy Labels via Classification Margins", "comments": "21 pages, 8 sets of figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A deep neural network trained on noisy labels is known to quickly lose its\npower to discriminate clean instances from noisy ones. After the early learning\nphase has ended, the network memorizes the noisy instances, which leads to a\ndegradation in generalization performance. To resolve this issue, we propose\nMARVEL (MARgins Via Early Learning), where we track the goodness of \"fit\" for\nevery instance by maintaining an epoch-history of its classification margins.\nBased on consecutive negative margins, we discard suspected noisy instances by\nzeroing out their weights. In addition, MARVEL+ upweights arduous instances\nenabling the network to learn a more nuanced representation of the\nclassification boundary. Experimental results on benchmark datasets with\nsynthetic label noise show that MARVEL outperforms other baselines consistently\nacross different noise levels, with a significantly larger margin under\nasymmetric noise.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 10:35:25 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lin", "Jason Z.", ""], ["Bradic", "Jelena", ""]]}, {"id": "2102.00796", "submitter": "Huaqing Jin", "authors": "Huaqing Jin and Guosheng Yin", "title": "Unit Information Prior for Adaptive Information Borrowing from Multiple\n  Historical Datasets", "comments": "4 figures; 2 tables in manuscript. 2 figures and one table in\n  supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical trials, there often exist multiple historical studies for the\nsame or related treatment investigated in the current trial. Incorporating\nhistorical data in the analysis of the current study is of great importance, as\nit can help to gain more information, improve efficiency, and provide a more\ncomprehensive evaluation of treatment. Enlightened by the unit information\nprior (UIP) concept in the reference Bayesian test, we propose a new\ninformative prior called UIP from an information perspective that can\nadaptively borrow information from multiple historical datasets. We consider\nboth binary and continuous data and also extend the new UIP methods to linear\nregression settings. Extensive simulation studies demonstrate that our method\nis comparable to other commonly used informative priors, while the\ninterpretation of UIP is intuitive and its implementation is relatively easy.\nOne distinctive feature of UIP is that its construction only requires summary\nstatistics commonly reported in the literature rather than the patient-level\ndata. By applying our UIP methods to phase III clinical trials for\ninvestigating the efficacy of memantine in Alzheimer's disease, we illustrate\nits ability of adaptively borrowing information from multiple historical\ndatasets in the real application.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 12:17:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Jin", "Huaqing", ""], ["Yin", "Guosheng", ""]]}, {"id": "2102.00877", "submitter": "Toni Karvonen", "authors": "Toni Karvonen, Jon Cockayne, Filip Tronarp, Simo S\\\"arkk\\\"a", "title": "A Probabilistic Taylor Expansion with Applications in Filtering and\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of Gaussian processes for which the posterior mean, for a\nparticular choice of data, replicates a truncated Taylor expansion of any\norder. The data consists of derivative evaluations at the expansion point and\nthe prior covariance kernel belongs to the class of Taylor kernels, which can\nbe written in a certain power series form. This permits statistical modelling\nof the uncertainty in a variety of algorithms that exploit first and second\norder Taylor expansions. To demonstrate the utility of this Gaussian process\nmodel we introduce new probabilistic versions of the classical extended Kalman\nfilter for non-linear state estimation and the Euler method for solving\nordinary differential equations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 14:36:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Karvonen", "Toni", ""], ["Cockayne", "Jon", ""], ["Tronarp", "Filip", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2102.00884", "submitter": "Zak Varty", "authors": "Zak Varty, Jonathan A. Tawn, Peter M. Atkinson and Stijn Bierman", "title": "Inference for extreme earthquake magnitudes accounting for a\n  time-varying measurement process", "comments": "Submitted to the Annals of Applied Statistics. Main text: 20 pages,\n  11 figures. Supplement: 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investment in measuring a process more completely or accurately is only\nuseful if these improvements can be utilised during modelling and inference. We\nconsider how improvements to data quality over time can be incorporated when\nselecting a modelling threshold and in the subsequent inference of an extreme\nvalue analysis. Motivated by earthquake catalogues, we consider variable data\nquality in the form of rounded and incompletely observed data. We develop an\napproach to select a time-varying modelling threshold that makes best use of\nthe available data, accounting for uncertainty in the magnitude model and for\nthe rounding of observations. We show the benefits of the proposed approach on\nsimulated data and apply the method to a catalogue of earthquakes induced by\ngas extraction in the Netherlands. This more than doubles the usable catalogue\nsize and greatly increases the precision of high magnitude quantile estimates.\nThis has important consequences for the design and cost of earthquake defences.\nFor the first time, we find compelling data-driven evidence against the\napplicability of the Gutenberg-Richer law to these earthquakes. Furthermore,\nour approach to automated threshold selection appears to have much potential\nfor generic applications of extreme value methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 14:49:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Varty", "Zak", ""], ["Tawn", "Jonathan A.", ""], ["Atkinson", "Peter M.", ""], ["Bierman", "Stijn", ""]]}, {"id": "2102.00911", "submitter": "Rou Zhong", "authors": "Rou Zhong, Shishi Liu, Jingxiao Zhang, Haocheng Li", "title": "Robust Functional Principal Component Analysis for Non-Gaussian\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional principal component analysis is essential in functional data\nanalysis, but the inferences will become unconvincing when some non-Gaussian\ncharacteristics occur, such as heavy tail and skewness. The focus of this paper\nis to develop a robust functional principal component analysis methodology in\ndealing with non-Gaussian longitudinal data, for which sparsity and\nirregularity along with non-negligible measurement errors must be considered.\nWe introduce a Kendall's $\\tau$ function whose particular properties make it a\nnice proxy for the covariance function in the eigenequation when handling\nnon-Gaussian cases. Moreover, the estimation procedure is presented and the\nasymptotic theory is also established. We further demonstrate the superiority\nand robustness of our method through simulation studies and apply the method to\nthe longitudinal CD4 cell count data in an AIDS study.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:30:35 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhong", "Rou", ""], ["Liu", "Shishi", ""], ["Zhang", "Jingxiao", ""], ["Li", "Haocheng", ""]]}, {"id": "2102.01053", "submitter": "Davide Bernardini", "authors": "Davide Bernardini, Sandra Paterlini, Emanuele Taufer", "title": "New estimation approaches for graphical models with elastic net penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the context of undirected Gaussian graphical models, we introduce three\nestimators based on elastic net penalty for the underlying dependence graph.\nOur goal is to estimate the sparse precision matrix, from which to retrieve\nboth the underlying conditional dependence graph and the partial correlation\ngraph. The first estimator is derived from the direct penalization of the\nprecision matrix in the likelihood function, while the second from using\nconditional penalized regressions to estimate the precision matrix. Finally,\nthe third estimator relies on a 2-stages procedure that estimates the edge set\nfirst and then the precision matrix elements. Through simulations we\ninvestigate the performances of the proposed methods on a large set of\nwell-known network structures. Empirical results on simulated data show that\nthe 2-stages procedure outperforms all other estimators both w.r.t. estimating\nthe sparsity pattern in the graph and the edges' weights. Finally, using\nreal-world data on US economic sectors, we estimate dependencies and show the\nimpact of Covid-19 pandemic on the network strength.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:43:30 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bernardini", "Davide", ""], ["Paterlini", "Sandra", ""], ["Taufer", "Emanuele", ""]]}, {"id": "2102.01144", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Double bootstrapping for visualising the distribution of descriptive\n  statistics of functional data", "comments": "22 pages, 9 figures, 1 table, to appear at the Journal of Statistical\n  Computation and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a double bootstrap procedure for reducing coverage error in the\nconfidence intervals of descriptive statistics for independent and identically\ndistributed functional data. Through a series of Monte Carlo simulations, we\ncompare the finite sample performance of single and double bootstrap procedures\nfor estimating the distribution of descriptive statistics for independent and\nidentically distributed functional data. At the cost of longer computational\ntime, the double bootstrap with the same bootstrap method reduces confidence\nlevel error and provides improved coverage accuracy than the single bootstrap.\nIllustrated by a Canadian weather station data set, the double bootstrap\nprocedure presents a tool for visualising the distribution of the descriptive\nstatistics for the functional data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:22:43 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "2102.01155", "submitter": "Kayla Kilpatrick", "authors": "Kayla W. Kilpatrick and Michael G. Hudgens", "title": "G-Formula for Observational Studies with Partial Interference, with\n  Application to Bed Net Use on Malaria", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessing population-level effects of vaccines and other infectious disease\nprevention measures is important to the field of public health. In infectious\ndisease studies, one person's treatment may affect another individual's\noutcome, i.e., there may be interference between units. For example, use of bed\nnets to prevent malaria by one individual may have an indirect or spillover\neffect to other individuals living in close proximity. In some settings,\nindividuals may form groups or clusters where interference only occurs within\ngroups, i.e., there is partial interference. Inverse probability weighted\nestimators have previously been developed for observational studies with\npartial interference. Unfortunately, these estimators are not well suited for\nstudies with large clusters. Therefore, in this paper, the parametric g-formula\nis extended to allow for partial interference. G-formula estimators are\nproposed of overall effects, spillover effects when treated, and spillover\neffects when untreated. The proposed estimators can accommodate large clusters\nand do not suffer from the g-null paradox that may occur in the absence of\ninterference. The large sample properties of the proposed estimators are\nderived, and simulation studies are presented demonstrating the finite-sample\nperformance of the proposed estimators. The Demographic and Health Survey from\nthe Democratic Republic of the Congo is then analyzed using the proposed\ng-formula estimators to assess the overall and spillover effects of bed net use\non malaria.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:49:27 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Kilpatrick", "Kayla W.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "2102.01286", "submitter": "Rou Zhong", "authors": "Rou Zhong, Shishi Liu, Haocheng Li, Jingxiao Zhang", "title": "Robust Functional Principal Component Analysis for Non-Gaussian\n  Continuous Data to Analyze Physical Activity Data Generated by Accelerometers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by energy expenditure observations measured by wearable device, we\npropose two functional principal component analysis (FPCA) methods, Spearman\nFPCA and Kendall FPCA, for non-Gaussian continuous data. The energy expenditure\nrecords measured during physical activity can be modeled as functional data.\nThey often involve non-Gaussian features, where the classical FPCA could be\ninvalid. To handle this issue, we develop two robust FPCA estimators using the\nframework of rank statistics. Via an extension of the Spearman's rank\ncorrelation estimator and the Kendall's $\\tau$ correlation estimator, a robust\nalgorithm for FPCA is developed to fit the model. The two estimators are\napplied to analyze the physical activity data collected by a wearable\naccelerometer monitor. The effectiveness of the proposed methods is also\ndemonstrated through a comprehensive simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 03:52:22 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zhong", "Rou", ""], ["Liu", "Shishi", ""], ["Li", "Haocheng", ""], ["Zhang", "Jingxiao", ""]]}, {"id": "2102.01681", "submitter": "Walter Dempsey", "authors": "Jieru Shi, Zhenke Wu, Walter Dempsey", "title": "Assessing Time-Varying Causal Effect Moderation in the Presence of\n  Cluster-Level Treatment Effect Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Micro-randomized trial (MRT) is a sequential randomized experimental design\nto empirically evaluate the effectiveness of mobile health (mHealth)\nintervention components that may be delivered at hundreds or thousands of\ndecision points. The MRT context has motivated a new class of causal estimands,\ntermed \"causal excursion effects\", for which inference can be made by a\nweighted, centered least squares approach (Boruvka et al., 2017). Existing\nmethods assume between-subject independence and non-interference. Deviations\nfrom these assumptions often occur which, if unaccounted for, may result in\nbias and overconfident variance estimates. In this paper, causal excursion\neffects are considered under potential cluster-level correlation and\ninterference and when the treatment effect of interest depends on cluster-level\nmoderators. The utility of our proposed methods is shown by analyzing data from\na multi-institution cohort of first year medical residents in the United\nStates. The approach paves the way for construction of mHealth interventions\nthat account for observed social network information.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:51:18 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shi", "Jieru", ""], ["Wu", "Zhenke", ""], ["Dempsey", "Walter", ""]]}, {"id": "2102.01753", "submitter": "Alexander Giessing", "authors": "Alexander Giessing and Jingshen Wang", "title": "Inference on Heterogeneous Quantile Treatment Effects via Rank-Score\n  Balancing", "comments": "94 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding treatment effect heterogeneity in observational studies is of\ngreat practical importance to many scientific fields because the same treatment\nmay affect different individuals differently. Quantile regression provides a\nnatural framework for modeling such heterogeneity. In this paper, we propose a\nnew method for inference on heterogeneous quantile treatment effects that\nincorporates high-dimensional covariates. Our estimator combines a debiased\n$\\ell_1$-penalized regression adjustment with a quantile-specific covariate\nbalancing scheme. We present a comprehensive study of the theoretical\nproperties of this estimator, including weak convergence of the heterogeneous\nquantile treatment effect process to the sum of two independent, centered\nGaussian processes. We illustrate the finite-sample performance of our approach\nthrough Monte Carlo experiments and an empirical example, dealing with the\ndifferential effect of mothers' education on infant birth weights.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 21:06:41 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 00:08:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Giessing", "Alexander", ""], ["Wang", "Jingshen", ""]]}, {"id": "2102.01769", "submitter": "Adriano Zambom", "authors": "Adriano Zanin Zambom, Qing Wang, Ronaldo Dias", "title": "A Basis Approach to Surface Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel method for clustering surfaces. The proposal\ninvolves first using basis functions in a tensor product to smooth the data and\nthus reduce the dimension to a finite number of coefficients, and then using\nthese estimated coefficients to cluster the surfaces via the k-means algorithm.\nAn extension of the algorithm to clustering tensors is also discussed. We show\nthat the proposed algorithm exhibits the property of strong consistency, with\nor without measurement errors, in correctly clustering the data as the sample\nsize increases. Simulation studies suggest that the proposed method outperforms\nthe benchmark k-means algorithm which uses the original vectorized data. In\naddition, an EGG real data example is considered to illustrate the practical\napplication of the proposal.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 21:41:28 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Zambom", "Adriano Zanin", ""], ["Wang", "Qing", ""], ["Dias", "Ronaldo", ""]]}, {"id": "2102.01784", "submitter": "Scott Bruce", "authors": "Pramita Bagchi and Scott A. Bruce", "title": "Adaptive Frequency Band Analysis for Functional Time Series", "comments": "33 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The frequency-domain properties of nonstationary functional time series often\ncontain valuable information. These properties are characterized through its\ntime-varying power spectrum. Practitioners seeking low-dimensional summary\nmeasures of the power spectrum often partition frequencies into bands and\ncreate collapsed measures of power within bands. However, standard frequency\nbands have largely been developed through manual inspection of time series data\nand may not adequately summarize power spectra. In this article, we propose a\nframework for adaptive frequency band estimation of nonstationary functional\ntime series that optimally summarizes the time-varying dynamics of the series.\nWe develop a scan statistic and search algorithm to detect changes in the\nfrequency domain. We establish theoretical properties of this framework and\ndevelop a computationally-efficient implementation. The validity of our method\nis also justified through numerous simulation studies and an application to\nanalyzing electroencephalogram data in participants alternating between eyes\nopen and eyes closed conditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 22:33:37 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 21:19:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bagchi", "Pramita", ""], ["Bruce", "Scott A.", ""]]}, {"id": "2102.01790", "submitter": "John O'Leary", "authors": "John O'Leary", "title": "Couplings of the Random-Walk Metropolis algorithm", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Couplings play a central role in contemporary Markov chain Monte Carlo\nmethods and in the analysis of their convergence to stationarity. In most\ncases, a coupling must induce relatively fast meeting between chains to ensure\ngood performance. In this paper we fix attention on the random walk Metropolis\nalgorithm and examine a range of coupling design choices. We introduce proposal\nand acceptance step couplings based on geometric, optimal transport, and\nmaximality considerations. We consider the theoretical properties of these\nchoices and examine their implication for the meeting time of the chains. We\nconclude by extracting a few general principles and hypotheses on the design of\neffective couplings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 22:46:26 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["O'Leary", "John", ""]]}, {"id": "2102.01821", "submitter": "Ian Frankenburg", "authors": "Ian Frankenburg and Sudipto Banerjee", "title": "A Compartment Model of Human Mobility and Early Covid-19 Dynamics in NYC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we build a mechanistic system to understand the relation\nbetween a reduction in human mobility and Covid-19 spread dynamics within New\nYork City. To this end, we propose a multivariate compartmental model that\njointly models smartphone mobility data and case counts during the first 90\ndays of the epidemic. Parameter calibration is achieved through the formulation\nof a general Bayesian hierarchical model to provide uncertainty quantification\nof resulting estimates. The open-source probabilistic programming language Stan\nis used for the requisite computation. Through sensitivity analysis and\nout-of-sample forecasting, we find our simple and interpretable model provides\nevidence that reductions in human mobility altered case dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 01:00:00 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 19:59:27 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Frankenburg", "Ian", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2102.01840", "submitter": "Yuanlu Bai", "authors": "Yuanlu Bai, Zhiyuan Huang, Henry Lam", "title": "Model Calibration via Distributionally Robust Optimization: On the NASA\n  Langley Uncertainty Quantification Challenge", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.15689", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a methodology to tackle the NASA Langley Uncertainty Quantification\nChallenge, a model calibration problem under both aleatory and epistemic\nuncertainties. Our methodology is based on an integration of robust\noptimization, more specifically a recent line of research known as\ndistributionally robust optimization, and importance sampling in Monte Carlo\nsimulation. The main computation machinery in this integrated methodology\namounts to solving sampled linear programs. We present theoretical statistical\nguarantees of our approach via connections to nonparametric hypothesis testing,\nand numerical performances including parameter calibration and downstream\ndecision and risk evaluation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 02:08:17 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Bai", "Yuanlu", ""], ["Huang", "Zhiyuan", ""], ["Lam", "Henry", ""]]}, {"id": "2102.01844", "submitter": "Yanrong Yang", "authors": "Lingyu He, Fei Huang, Jianjie Shi, Yanrong Yang", "title": "Mortality Forecasting using Factor Models: Time-varying or\n  Time-invariant Factor Loadings?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many existing mortality models follow the framework of classical factor\nmodels, such as the Lee-Carter model and its variants. Latent common factors in\nfactor models are defined as time-related mortality indices (such as $\\kappa_t$\nin the Lee-Carter model). Factor loadings, which capture the linear\nrelationship between age variables and latent common factors (such as $\\beta_x$\nin the Lee-Carter model), are assumed to be time-invariant in the classical\nframework. This assumption is usually too restrictive in reality as mortality\ndatasets typically span a long period of time. Driving forces such as medical\nimprovement of certain diseases, environmental changes and technological\nprogress may significantly influence the relationship of different variables.\nIn this paper, we first develop a factor model with time-varying factor\nloadings (time-varying factor model) as an extension of the classical factor\nmodel for mortality modelling. Two forecasting methods to extrapolate the\nfactor loadings, the local regression method and the naive method, are proposed\nfor the time-varying factor model. From the empirical data analysis, we find\nthat the new model can capture the empirical feature of time-varying factor\nloadings and improve mortality forecasting over different horizons and\ncountries. Further, we propose a novel approach based on change point analysis\nto estimate the optimal `boundary' between short-term and long-term\nforecasting, which is favoured by the local linear regression and naive method,\nrespectively. Additionally, simulation studies are provided to show the\nperformance of the time-varying factor model under various scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 02:28:16 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["He", "Lingyu", ""], ["Huang", "Fei", ""], ["Shi", "Jianjie", ""], ["Yang", "Yanrong", ""]]}, {"id": "2102.01935", "submitter": "Wen Wei Loh", "authors": "Wen Wei Loh, Stijn Vansteelandt", "title": "Sensitivity Analysis for Unmeasured Confounding via Effect Extrapolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring the causal effect of a non-randomly assigned exposure on an outcome\nrequires adjusting for common causes of the exposure and outcome to avoid\nbiased conclusions. Notwithstanding the efforts investigators routinely make to\nmeasure and adjust for such common causes (or confounders), some confounders\ntypically remain unmeasured, raising the prospect of biased inference in\nobservational studies. Therefore, it is crucial that investigators can\npractically assess their substantive conclusions' relative (in)sensitivity to\npotential unmeasured confounding. In this article, we propose a sensitivity\nanalysis strategy that is informed by the stability of the exposure effect over\ndifferent, well-chosen subsets of the measured confounders. The proposal\nentails first approximating the process for recording confounders to learn\nabout how the effect is potentially affected by varying amounts of unmeasured\nconfounding, then extrapolating to the effect had hypothetical unmeasured\nconfounders been additionally adjusted for. A large set of measured confounders\ncan thus be exploited to provide insight into the likely presence of unmeasured\nconfounding bias, albeit under an assumption about how data on the confounders\nare recorded. The proposal's ability to reveal the true effect and ensure valid\ninference after extrapolation is empirically compared with existing methods\nusing simulation studies. We demonstrate the procedure using two different\npublicly available datasets commonly used for causal inference.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:35:46 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Loh", "Wen Wei", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2102.01943", "submitter": "Efstathios Paparoditis", "authors": "Marco Meyer and Efstathios Paparoditis", "title": "A Frequency Domain Bootstrap for General Multivariate Stationary\n  Processes", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many relevant statistics of multivariate time series, no valid frequency\ndomain bootstrap procedures exist. This is mainly due to the fact that the\ndistribution of such statistics depends on the fourth-order moment structure of\nthe underlying process in nearly every scenario, except for some special cases\nlike Gaussian time series. In contrast to the univariate case, even additional\nstructural assumptions such as linearity of the multivariate process or a\nstandardization of the statistic of interest do not solve the problem. This\npaper focuses on integrated periodogram statistics as well as functions thereof\nand presents a new frequency domain bootstrap procedure for multivariate time\nseries, the multivariate frequency domain hybrid bootstrap (MFHB), to fill this\ngap. Asymptotic validity of the MFHB procedure is established for general\nclasses of periodogram-based statistics and for stationary multivariate\nprocesses satisfying rather weak dependence conditions. A simulation study is\ncarried out which compares the finite sample performance of the MFHB with that\nof the moving block bootstrap.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:43:35 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Meyer", "Marco", ""], ["Paparoditis", "Efstathios", ""]]}, {"id": "2102.01946", "submitter": "Jan Gertheiss", "authors": "Jan Gertheiss, Fabian Scheipl, Tina Lauer, Harald Ehrhardt", "title": "Statistical Inference for Ordinal Predictors in Generalized Linear and\n  Additive Models with Application to Bronchopulmonary Dysplasia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Discrete but ordered covariates are quite common in applied statistics, and\nsome regularized fitting procedures have been proposed for proper handling of\nordinal predictors in statistical modeling. In this study, we show how\nquadratic penalties on adjacent dummy coefficients of ordinal predictors\nproposed in the literature can be incorporated in the framework of generalized\nadditive models, making tools for statistical inference developed there\navailable for ordinal predictors as well. Motivated by an application from\nneonatal medicine, we discuss whether results obtained when constructing\nconfidence intervals and testing significance of smooth terms in generalized\nadditive models are useful with ordinal predictors/penalties as well.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:54:10 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Gertheiss", "Jan", ""], ["Scheipl", "Fabian", ""], ["Lauer", "Tina", ""], ["Ehrhardt", "Harald", ""]]}, {"id": "2102.01950", "submitter": "Matthieu Simeoni", "authors": "Matthieu Simeoni and Paul Hurley", "title": "SiML: Sieved Maximum Likelihood for Array Signal Processing", "comments": "5 pages, 2 figures. Published in ICASSP 2021-2021 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for\n  6-11 June 2021 in Toronto, Ontario, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Maximum Likelihood (SML) is a popular direction of arrival (DOA)\nestimation technique in array signal processing. It is a parametric method that\njointly estimates signal and instrument noise by maximum likelihood, achieving\nexcellent statistical performance. Some drawbacks are the computational\noverhead as well as the limitation to a point-source data model with fewer\nsources than sensors. In this work, we propose a Sieved Maximum Likelihood\n(SiML) method. It uses a general functional data model, allowing an\nunrestricted number of arbitrarily-shaped sources to be recovered. To this end,\nwe leverage functional analysis tools and express the data in terms of an\ninfinite-dimensional sampling operator acting on a Gaussian random function. We\nshow that SiML is computationally more efficient than traditional SML,\nresilient to noise, and results in much better accuracy than spectral-based\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:01:06 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Simeoni", "Matthieu", ""], ["Hurley", "Paul", ""]]}, {"id": "2102.01982", "submitter": "Michael Fop", "authors": "Michael Fop, Pierre-Alexandre Mattei, Charles Bouveyron, Thomas\n  Brendan Murphy", "title": "Unobserved classes and extra variables in high-dimensional discriminant\n  analysis", "comments": "29 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In supervised classification problems, the test set may contain data points\nbelonging to classes not observed in the learning phase. Moreover, the same\nunits in the test data may be measured on a set of additional variables\nrecorded at a subsequent stage with respect to when the learning sample was\ncollected. In this situation, the classifier built in the learning phase needs\nto adapt to handle potential unknown classes and the extra dimensions. We\nintroduce a model-based discriminant approach, Dimension-Adaptive Mixture\nDiscriminant Analysis (D-AMDA), which can detect unobserved classes and adapt\nto the increasing dimensionality. Model estimation is carried out via a full\ninductive approach based on an EM algorithm. The method is then embedded in a\nmore general framework for adaptive variable selection and classification\nsuitable for data of large dimensions. A simulation study and an artificial\nexperiment related to classification of adulterated honey samples are used to\nvalidate the ability of the proposed framework to deal with complex situations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 10:01:52 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fop", "Michael", ""], ["Mattei", "Pierre-Alexandre", ""], ["Bouveyron", "Charles", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "2102.02091", "submitter": "Subhankar Dutta", "authors": "Subhankar Dutta, Suchandan Kayal", "title": "Estimation of parameters of the logistic exponential distribution under\n  progressive type-I hybrid censored sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of estimation of the model parameters of the\nlogistic exponential distribution based on progressive type-I hybrid censored\nsample. The maximum likelihood estimates are obtained and computed numerically\nusing Newton-Raphson method. Further, the Bayes estimates are derived under\nsquared error, LINEX and generalized entropy loss functions. Two types\n(independent and bivariate) of prior distributions are considered for the\npurpose of Bayesian estimation. It is seen that the Bayes estimates are not of\nexplicit forms.Thus, Lindley's approximation technique is employed to get\napproximate Bayes estimates. Interval estimates of the parameters based on\nnormal approximate of the maximum likelihood estimates and normal approximation\nof the log-transformed maximum likelihood estimates are constructed. The\nhighest posterior density credible intervals are obtained by using the\nimportance sampling method. Furthermore, numerical computations are reported to\nreview some of the results obtained in the paper. A real life dataset is\nconsidered for the purpose of illustrations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 14:46:45 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Dutta", "Subhankar", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2102.02123", "submitter": "Hongsheng Dai PhD", "authors": "Hongsheng Dai, Murray Pollock, Gareth Roberts", "title": "Bayesian Fusion: Scalable unification of distributed statistical\n  analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been considerable interest in addressing the problem of\nunifying distributed statistical analyses into a single coherent inference.\nThis problem naturally arises in a number of situations, including in big-data\nsettings, when working under privacy constraints, and in Bayesian model choice.\nThe majority of existing approaches have relied upon convenient approximations\nof the distributed analyses. Although typically being computationally\nefficient, and readily scaling with respect to the number of analyses being\nunified, approximate approaches can have significant shortcomings -- the\nquality of the inference can degrade rapidly with the number of analyses being\nunified, and can be substantially biased even when unifying a small number of\nanalyses that do not concur. In contrast, the recent Fusion approach of Dai et\nal. (2019) is a rejection sampling scheme which is readily parallelisable and\nis exact (avoiding any form of approximation other than Monte Carlo error),\nalbeit limited in applicability to unifying a small number of low-dimensional\nanalyses. In this paper we introduce a practical Bayesian Fusion approach. We\nextend the theory underpinning the Fusion methodology and, by embedding it\nwithin a sequential Monte Carlo algorithm, we are able to recover the correct\ntarget distribution. By means of extensive guidance on the implementation of\nthe approach, we demonstrate theoretically and empirically that Bayesian Fusion\nis robust to increasing numbers of analyses, and coherently unifying analyses\nwhich do not concur. This is achieved while being computationally competitive\nwith approximate schemes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 16:03:09 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Dai", "Hongsheng", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth", ""]]}, {"id": "2102.02159", "submitter": "Daniel G. Rasines", "authors": "Daniel G. Rasines and G. Alastair Young", "title": "Splitting strategies for post-selection inference", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of providing valid inference for a selected parameter\nin a sparse regression setting. It is well known that classical regression\ntools can be unreliable in this context due to the bias generated in the\nselection step. Many approaches have been proposed in recent years to ensure\ninferential validity. Here, we consider a simple alternative to data splitting\nbased on randomising the response vector, which allows for higher selection and\ninferential power than the former and is applicable with an arbitrary selection\nrule. We provide a theoretical and empirical comparison of both methods and\nextend the randomisation approach to non-normal settings. Our investigations\nshow that the gain in power can be substantial.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 17:27:52 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Rasines", "Daniel G.", ""], ["Young", "G. Alastair", ""]]}, {"id": "2102.02285", "submitter": "Siyun Yang", "authors": "Siyun Yang, Fan Li, Laine E. Thomas, Fan Li", "title": "Covariate adjustment in subgroup analyses of randomized clinical trials:\n  A propensity score approach", "comments": "30 pages, 5 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Subgroup analyses are frequently conducted in randomized clinical\ntrials to assess evidence of heterogeneous treatment effect across patient\nsubpopulations. Although randomization balances covariates within subgroups in\nexpectation, chance imbalance may be amplified in small subgroups and harm the\nprecision. Covariate adjustment in overall analysis of RCT is often conducted,\nvia either ANCOVA or propensity score weighting, but for subgroup analysis has\nbeen rarely discussed. In this article, we develop propensity score weighting\nmethodology for covariate adjustment to improve the precision and power of\nsubgroup analyses in RCTs. Methods: We extend the propensity score weighting\nmethodology to subgroup analyses by fitting a logistic propensity model with\npre-specified covariate-subgroup interactions. We show that, by construction,\noverlap weighting exactly balances the covariates with interaction terms in\neach subgroup. Extensive simulations were performed to compare the operating\ncharacteristics of unadjusted, different propensity score weighting and the\nANCOVA estimator. We apply these methods to the HF-ACTION trial to evaluate the\neffect of exercise training on 6-minute walk test in pre-specified subgroups.\nResults: Standard errors of the adjusted estimators are smaller than those of\nthe unadjusted estimator. The propensity score weighting estimator is as\nefficient as ANCOVA, and is often more efficient when subgroup sample size is\nsmall (e.g.<125), and/or when outcome model is misspecified. The weighting\nestimators with full-interaction propensity model consistently outperform the\nstandard main-effect propensity model. Conclusion: Propensity score weighting\nis a transparent and objective method to adjust chance imbalance of important\ncovariates in subgroup analyses of RCTs. It is crucial to include the full\ncovariate-subgroup interactions in the propensity score model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 20:45:02 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:47:42 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 14:45:00 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 16:03:26 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Yang", "Siyun", ""], ["Li", "Fan", ""], ["Thomas", "Laine E.", ""], ["Li", "Fan", ""]]}, {"id": "2102.02297", "submitter": "Benjamin Bolker", "authors": "Steve Cygu, Jonathan Dushoff, and Benjamin M. Bolker", "title": "pcoxtime: Penalized Cox Proportional Hazard Model for Time-dependent\n  Covariates", "comments": "Submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The penalized Cox proportional hazard model is a popular analytical approach\nfor survival data with a large number of covariates. Such problems are\nespecially challenging when covariates vary over follow-up time (i.e., the\ncovariates are time-dependent). The standard R packages for fully penalized Cox\nmodels cannot currently incorporate time-dependent covariates. To address this\ngap, we implement a variant of gradient descent algorithm (proximal gradient\ndescent) for fitting penalized Cox models. We apply our implementation to real\nand simulated data sets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 21:09:12 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 16:46:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cygu", "Steve", ""], ["Dushoff", "Jonathan", ""], ["Bolker", "Benjamin M.", ""]]}, {"id": "2102.02381", "submitter": "Farzaneh Boroumand Miss", "authors": "Farzaneh Boroumand, Mohammad T. Shakeri, Nino Kordzakhia, Mahdi\n  Salehi, Hassan Doosti", "title": "Tilted Nonparametric Regression Function Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides the theory about the convergence rate of the tilted\nversion of linear smoother. We study tilted linear smoother, a nonparametric\nregression function estimator, which is obtained by minimizing the distance to\nan infinite order flat-top trapezoidal kernel estimator. We prove that the\nproposed estimator achieves a high level of accuracy. Moreover, it preserves\nthe attractive properties of the infinite order flat-top kernel estimator. We\nalso present an extensive numerical study for analysing the performance of two\nmembers of the tilted linear smoother class named tilted Nadaraya-Watson and\ntilted local linear in the finite sample. The simulation study shows that\ntilted Nadaraya-Watson and tilted local linear perform better than their\nclassical analogs in some conditions in terms of Mean Integrated Squared Error\n(MISE). Finally, the performance of these estimators as well as the\nconventional estimators were illustrated by curve fitting to COVID-19 data for\n12 countries and a dose-response data set.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 02:45:00 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Boroumand", "Farzaneh", ""], ["Shakeri", "Mohammad T.", ""], ["Kordzakhia", "Nino", ""], ["Salehi", "Mahdi", ""], ["Doosti", "Hassan", ""]]}, {"id": "2102.02572", "submitter": "Juan A. Cuesta-Albertos", "authors": "E. del Barrio, J.A. Cuesta-Albertos and C. Matran", "title": "The complex behaviour of Galton rank order statistic", "comments": "35 pages. No figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Galton's rank order statistic is one of the oldest statistical tools for\ntwo-sample comparisons. It is also a very natural index to measure departures\nfrom stochastic dominance. Yet, its asymptotic behaviour has been investigated\nonly partially, under restrictive assumptions. This work provides a\ncomprehensive {study} of this behaviour, based on the analysis of the so-called\ncontact set (a modification of the set in which the quantile functions\ncoincide). We show that a.s. convergence to the population counterpart holds if\nand only if {the} contact set has zero Lebesgue measure. When this set is\nfinite we show that the asymptotic behaviour is determined by the local\nbehaviour of a suitable reparameterization of the quantile functions in a\nneighbourhood of the contact points. Regular crossings result in standard rates\nand Gaussian limiting distributions, but higher order contacts (in the sense\nintroduced in this work) or contacts at the extremes of the supports may result\nin different rates and non-Gaussian limits.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 12:24:04 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["del Barrio", "E.", ""], ["Cuesta-Albertos", "J. A.", ""], ["Matran", "C.", ""]]}, {"id": "2102.02580", "submitter": "Yuan Gao Dr.", "authors": "Yuan Gao, Han Lin Shang, Yanrong Yang", "title": "Factor-augmented Smoothing Model for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose modeling raw functional data as a mixture of a smooth function and\na highdimensional factor component. The conventional approach to retrieving the\nsmooth function from the raw data is through various smoothing techniques.\nHowever, the smoothing model is not adequate to recover the smooth curve or\ncapture the data variation in some situations. These include cases where there\nis a large amount of measurement error, the smoothing basis functions are\nincorrectly identified, or the step jumps in the functional mean levels are\nneglected. To address these challenges, a factor-augmented smoothing model is\nproposed, and an iterative numerical estimation approach is implemented in\npractice. Including the factor model component in the proposed method solves\nthe aforementioned problems since a few common factors often drive the\nvariation that cannot be captured by the smoothing model. Asymptotic theorems\nare also established to demonstrate the effects of including factor structures\non the smoothing results. Specifically, we show that the smoothing coefficients\nprojected on the complement space of the factor loading matrix is\nasymptotically normal. As a byproduct of independent interest, an estimator for\nthe population covariance matrix of the raw data is presented based on the\nproposed model. Extensive simulation studies illustrate that these factor\nadjustments are essential in improving estimation accuracy and avoiding the\ncurse of dimensionality. The superiority of our model is also shown in modeling\nCanadian weather data and Australian temperature data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 12:49:53 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Gao", "Yuan", ""], ["Shang", "Han Lin", ""], ["Yang", "Yanrong", ""]]}, {"id": "2102.02697", "submitter": "Roland Jucknewitz", "authors": "Roland Jucknewitz, Oliver Weidinger, Anja Schramm", "title": "Covid-19 risk factors: Statistical learning from German healthcare\n  claims data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse prior risk factors for severe, critical or fatal courses of\nCovid-19 based on a retrospective cohort using claims data of the AOK Bayern.\nAs our main methodological contribution, we avoid prior grouping and\npre-selection of candidate risk factors. Instead, fine-grained hierarchical\ninformation from medical classification systems for diagnoses, pharmaceuticals\nand procedures are used, resulting in more than 33,000 covariates. Our approach\nhas better predictive ability than well-specified morbidity groups but does not\nneed prior subject-matter knowledge. The methodology and estimated coefficients\nare made available to decision makers to prioritize protective measures towards\nvulnerable subpopulations and to researchers who like to adjust for a large set\nof confounders in studies of individual risk factors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:48:21 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 07:46:49 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jucknewitz", "Roland", ""], ["Weidinger", "Oliver", ""], ["Schramm", "Anja", ""]]}, {"id": "2102.02731", "submitter": "Marco Benedetti", "authors": "Marco H. Benedetti, Veronica J. Berrocal, Naveen N. Narisetty", "title": "Identifying regions of inhomogeneities in spatial processes via an M-RA\n  and mixture priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Soils have been heralded as a hidden resource that can be leveraged to\nmitigate and address some of the major global environmental challenges.\nSpecifically, the organic carbon stored in soils, called Soil Organic Carbon\n(SOC), can, through proper soil management, help offset fuel emissions,\nincrease food productivity, and improve water quality. As collecting data on\nSOC is costly and time consuming, not much data on SOC is available, although\nunderstanding the spatial variability in SOC is of fundamental importance for\neffective soil management.\n  In this manuscript, we propose a modeling framework that can be used to gain\na better understanding of the dependence structure of a spatial process by\nidentifying regions within a spatial domain where the process displays the same\nspatial correlation range. To achieve this goal, we propose a generalization of\nthe Multi-Resolution Approximation (M-RA) modeling framework of Katzfuss (2017)\noriginally introduced as a strategy to reduce the computational burden\nencountered when analyzing massive spatial datasets.\n  To allow for the possibility that the correlation of a spatial process might\nbe characterized by a different range in different subregions of a spatial\ndomain, we provide the M-RA basis functions weights with a two-component\nmixture prior with one of the mixture components a shrinking prior. We call our\napproach the mixture M-RA. Application of the mixture M-RA model to both\nstationary and non-stationary data shows that the mixture M-RA model can handle\nboth types of data, can correctly establish the type of spatial dependence\nstructure in the data (e.g. stationary vs not), and can identify regions of\nlocal stationarity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:40:51 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Benedetti", "Marco H.", ""], ["Berrocal", "Veronica J.", ""], ["Narisetty", "Naveen N.", ""]]}, {"id": "2102.02752", "submitter": "Lisa Hampson V", "authors": "Lisa V Hampson, Bj\\\"orn Bornkamp, Bj\\\"orn Holzhauer, Joseph Kahn,\n  Markus R Lange, Wen-Lin Luo, Giovanni Della Cioppa, Kelvin Stott, Steffen\n  Ballerstedt", "title": "Improving the assessment of the probability of success in late stage\n  drug development", "comments": "22 pages, 9 figures, 3 tables, 45 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There are several steps to confirming the safety and efficacy of a new\nmedicine. A sequence of trials, each with its own objectives, is usually\nrequired. Quantitative risk metrics can be useful for informing decisions about\nwhether a medicine should transition from one stage of development to the next.\nTo obtain an estimate of the probability of regulatory approval, pharmaceutical\ncompanies may start with industry-wide success rates and then apply to these\nsubjective adjustments to reflect program-specific information. However, this\napproach lacks transparency and fails to make full use of data from previous\nclinical trials. We describe a quantitative Bayesian approach for calculating\nthe probability of success (PoS) at the end of phase II which incorporates\ninternal clinical data from one or more phase IIb studies, industry-wide\nsuccess rates, and expert opinion or external data if needed. Using an example,\nwe illustrate how PoS can be calculated accounting for differences between the\nphase IIb data and future phase III trials, and discuss how the methods can be\nextended to accommodate accelerated drug development pathways.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:29:47 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Hampson", "Lisa V", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Holzhauer", "Bj\u00f6rn", ""], ["Kahn", "Joseph", ""], ["Lange", "Markus R", ""], ["Luo", "Wen-Lin", ""], ["Della Cioppa", "Giovanni", ""], ["Stott", "Kelvin", ""], ["Ballerstedt", "Steffen", ""]]}, {"id": "2102.02794", "submitter": "Scott Bruce", "authors": "Zeda Li, Scott A. Bruce, and Tian Cai", "title": "Classification of Categorical Time Series Using the Spectral Envelope\n  and Optimal Scalings", "comments": "29 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces a novel approach to the classification of categorical\ntime series under the supervised learning paradigm. To construct meaningful\nfeatures for categorical time series classification, we consider two relevant\nquantities: the spectral envelope and its corresponding set of optimal\nscalings. These quantities characterize oscillatory patterns in a categorical\ntime series as the largest possible power at each frequency, or spectral\nenvelope, obtained by assigning numerical values, or scalings, to categories\nthat optimally emphasize oscillations at each frequency. Our procedure combines\nthese two quantities to produce an interpretable and parsimonious feature-based\nclassifier that can be used to accurately determine group membership for\ncategorical time series. Classification consistency of the proposed method is\ninvestigated, and simulation studies are used to demonstrate accuracy in\nclassifying categorical time series with various underlying group structures.\nFinally, we use the proposed method to explore key differences in oscillatory\npatterns of sleep stage time series for patients with different sleep disorders\nand accurately classify patients accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:29:27 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Zeda", ""], ["Bruce", "Scott A.", ""], ["Cai", "Tian", ""]]}, {"id": "2102.02852", "submitter": "Bj\\\"orn Holzhauer", "authors": "Bj\\\"orn Holzhauer (1), Lisa V. Hampson (1), John Paul Gosling (2),\n  Bj\\\"orn Bornkamp (1), Joseph Kahn (3), Markus R. Lange (1), Wen-Lin Luo (3),\n  Caterina Brindicci (1), David Lawrence (1), Steffen Ballerstedt (1), Anthony\n  O'Hagan (4) ((1) Novartis Pharma AG, Basel, Switzerland, (2) JBA Risk\n  Management Ltd, Skipton, United Kingdom, (3) Novartis Pharmaceuticals\n  Corporation, East Hanover, USA, (4) The University of Sheffield, School of\n  Mathematics and Statistics, Sheffield, United Kingdom)", "title": "Eliciting judgements about dependent quantities of interest: The SHELF\n  extension and copula methods illustrated using an asthma case study", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pharmaceutical companies regularly need to make decisions about drug\ndevelopment programs based on the limited knowledge from early stage clinical\ntrials. In this situation, eliciting the judgements of experts is an attractive\napproach for synthesising evidence on the unknown quantities of interest. When\ncalculating the probability of success for a drug development program, multiple\nquantities of interest - such as the effect of a drug on different endpoints -\nshould not be treated as unrelated.\n  We discuss two approaches for establishing a multivariate distribution for\nseveral related quantities within the SHeffield ELicitation Framework (SHELF).\nThe first approach elicits experts' judgements about a quantity of interest\nconditional on knowledge about another one. For the second approach, we first\nelicit marginal distributions for each quantity of interest. Then, for each\npair of quantities, we elicit the concordance probability that both lie on the\nsame side of their respective elicited medians. This allows us to specify a\ncopula to obtain the joint distribution of the quantities of interest.\n  We show how these approaches were used in an elicitation workshop that was\nperformed to assess the probability of success of the registrational program of\nan asthma drug. The judgements of the experts, which were obtained prior to\ncompletion of the pivotal studies, were well aligned with the final trial\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 19:37:14 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:10:53 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Holzhauer", "Bj\u00f6rn", ""], ["Hampson", "Lisa V.", ""], ["Gosling", "John Paul", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Kahn", "Joseph", ""], ["Lange", "Markus R.", ""], ["Luo", "Wen-Lin", ""], ["Brindicci", "Caterina", ""], ["Lawrence", "David", ""], ["Ballerstedt", "Steffen", ""], ["O'Hagan", "Anthony", ""]]}, {"id": "2102.02871", "submitter": "Lubna Amro", "authors": "Lubna Amro, Frank Konietschke, Markus Pauly", "title": "Incompletely observed nonparametric factorial designs with repeated\n  measurements: A wild bootstrap approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many life science experiments or medical studies, subjects are repeatedly\nobserved and measurements are collected in factorial designs with multivariate\ndata. The analysis of such multivariate data is typically based on multivariate\nanalysis of variance (MANOVA) or mixed models, requiring complete data, and\ncertain assumption on the underlying parametric distribution such as continuity\nor a specific covariance structure, e.g., compound symmetry. However, these\nmethods are usually not applicable when discrete data or even ordered\ncategorical data are present. In such cases, nonparametric rank-based methods\nthat do not require stringent distributional assumptions are the preferred\nchoice. However, in the multivariate case, most rank-based approaches have only\nbeen developed for complete observations. It is the aim of this work is to\ndevelop asymptotic correct procedures that are capable of handling missing\nvalues, allowing for singular covariance matrices and are applicable for\nordinal or ordered categorical data. This is achieved by applying a wild\nbootstrap procedure in combination with quadratic form-type test statistics.\nBeyond proving their asymptotic correctness, extensive simulation studies\nvalidate their applicability for small samples. Finally, two real data examples\nare analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 20:17:20 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Amro", "Lubna", ""], ["Konietschke", "Frank", ""], ["Pauly", "Markus", ""]]}, {"id": "2102.02911", "submitter": "Leiwen Gao", "authors": "Leiwen Gao, Abhirup Datta, Sudipto Banerjee", "title": "Hierarchical Multivariate Directed Acyclic Graph Auto-Regressive\n  (MDAGAR) models for spatial diseases mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disease mapping is an important statistical tool used by epidemiologists to\nassess geographic variation in disease rates and identify lurking environmental\nrisk factors from spatial patterns. Such maps rely upon spatial models for\nregionally aggregated data, where neighboring regions tend to exhibit similar\noutcomes than those farther apart. We contribute to the literature on\nmultivariate disease mapping, which deals with measurements on multiple (two or\nmore) diseases in each region. We aim to disentangle associations among the\nmultiple diseases from spatial autocorrelation in each disease. We develop\nMultivariate Directed Acyclic Graphical Autoregression (MDAGAR) models to\naccommodate spatial and inter-disease dependence. The hierarchical construction\nimparts flexibility and richness, interpretability of spatial autocorrelation\nand inter-disease relationships, and computational ease, but depends upon the\norder in which the cancers are modeled. To obviate this, we demonstrate how\nBayesian model selection and averaging across orders are easily achieved using\nbridge sampling. We compare our method with a competitor using simulation\nstudies and present an application to multiple cancer mapping using data from\nthe Surveillance, Epidemiology, and End Results (SEER) Program.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 22:05:59 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Gao", "Leiwen", ""], ["Datta", "Abhirup", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2102.02999", "submitter": "Won Chang", "authors": "J. Park, W. Chang, B. Choi", "title": "An Interaction Neyman-Scott Point Process Model for Coronavirus\n  Disease-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid transmission, the coronavirus disease 2019 (COVID-19) has led to\nover 2 million deaths worldwide, posing significant societal challenges.\nUnderstanding the spatial patterns of patient visits and detecting the local\nspreading events are crucial to controlling disease outbreaks. We analyze\nhighly detailed COVID-19 contact tracing data collected from Seoul, which\nprovides a unique opportunity to understand the mechanism of patient visit\noccurrence. Analyzing contact tracing data is challenging because patient\nvisits show strong clustering patterns while clusters of events may have\ncomplex interaction behavior. To account for such behaviors, we develop a novel\ninteraction Neyman-Scott process that regards the observed patient visit events\nas offsprings generated from a parent spreading event. Inference for such\nmodels is complicated since the likelihood involves intractable normalizing\nfunctions. To address this issue, we embed an auxiliary variable algorithm into\nour Markov chain Monte Carlo. We fit our model to several simulated and real\ndata examples under different outbreak scenarios and show that our method can\ndescribe spatial patterns of patient visits well. We also provide visualization\ntools that can inform public health interventions for infectious diseases such\nas social distancing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 05:11:47 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Park", "J.", ""], ["Chang", "W.", ""], ["Choi", "B.", ""]]}, {"id": "2102.03106", "submitter": "Valeria Policastro", "authors": "Valeria Policastro, Dario Righelli, Annamaria Carissimo, Luisa Cutillo\n  and Italia De Feis", "title": "ROBustness In Network (robin): an R package for Comparison and\n  Validation of communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In network analysis, many community detection algorithms have been developed,\nhowever, their implementation leaves unaddressed the question of the\nstatistical validation of the results. Here we present robin(ROBustness In\nNetwork), an R package to assess the robustness of the community structure of a\nnetwork found by one or more methods to give indications about their\nreliability. The procedure initially detects if the community structure found\nby a set of algorithms is statistically significant and then compares two\nselected detection algorithms on the same graph to choose the one that better\nfits the network of interest. We demonstrate the use of our package on the\nAmerican College Football benchmark dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 11:12:18 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Policastro", "Valeria", ""], ["Righelli", "Dario", ""], ["Carissimo", "Annamaria", ""], ["Cutillo", "Luisa", ""], ["De Feis", "Italia", ""]]}, {"id": "2102.03249", "submitter": "Philip White", "authors": "Philip A. White and Henry Frye and Michael F. Christensen and Alan E.\n  Gelfand and John A. Silander Jr", "title": "Spatial Functional Data Modeling of Plant Reflectances", "comments": "20 pages main manuscript, 20 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant reflectance spectra - the profile of light reflected by leaves across\ndifferent wavelengths - supply the spectral signature for a species at a\nspatial location to enable estimation of functional and taxonomic diversity for\nplants. We consider leaf spectra as \"responses\" to be explained spatially.\nThese spectra/reflectances are functions over a wavelength band that respond to\nthe environment.\n  Our motivating data are gathered for several families from the Cape Floristic\nRegion (CFR) in South Africa and lead us to develop rich novel spatial models\nthat can explain spectra for genera within families. Wavelength responses for\nan individual leaf are viewed as a function of wavelength, leading to\nfunctional data modeling. Local environmental features become covariates. We\nintroduce wavelength - covariate interaction since the response to\nenvironmental regressors may vary with wavelength, so may variance. Formal\nspatial modeling enables prediction of reflectances for genera at unobserved\nlocations with known environmental features. We incorporate spatial dependence,\nwavelength dependence, and space-wavelength interaction (in the spirit of\nspace-time interaction). We implement out-of-sample validation to select a best\nmodel, discovering that the model features listed above are all informative for\nthe functional data analysis. We then supply interpretation of the results\nunder the selected model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:53:24 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 15:56:50 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 04:18:17 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["White", "Philip A.", ""], ["Frye", "Henry", ""], ["Christensen", "Michael F.", ""], ["Gelfand", "Alan E.", ""], ["Silander", "John A.", "Jr"]]}, {"id": "2102.03257", "submitter": "\\\"Ozge Sahin", "authors": "\\\"Ozge Sahin, Claudia Czado", "title": "Vine copula mixture models and clustering for non-Gaussian data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of finite mixture models suffer from not allowing asymmetric\ntail dependencies within components and not capturing non-elliptical clusters\nin clustering applications. Since vine copulas are very flexible in capturing\nthese types of dependencies, we propose a novel vine copula mixture model for\ncontinuous data. We discuss the model selection and parameter estimation\nproblems and further formulate a new model-based clustering algorithm. The use\nof vine copulas in clustering allows for a range of shapes and dependency\nstructures for the clusters. Our simulation experiments illustrate a\nsignificant gain in clustering accuracy when notably asymmetric tail\ndependencies or/and non-Gaussian margins within the components exist. The\nanalysis of real data sets accompanies the proposed method. We show that the\nmodel-based clustering algorithm with vine copula mixture models outperforms\nthe other model-based clustering techniques, especially for the non-Gaussian\nmultivariate data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:04:26 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Sahin", "\u00d6zge", ""], ["Czado", "Claudia", ""]]}, {"id": "2102.03306", "submitter": "Lars Lau Raket", "authors": "Lars Lau Raket", "title": "Differential equations, splines and Gaussian processes", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CA stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore the connections between Green's functions for certain differential\nequations, covariance functions for Gaussian processes, and the smoothing\nsplines problem. Conventionally, the smoothing spline problem is considered in\na setting of reproducing kernel Hilbert spaces, but here we present a more\ndirect approach. With this approach, some choices that are implicit in the\nreproducing kernel Hilbert space setting stand out, one example being choice of\nboundary conditions and more elaborate shape restrictions.\n  The paper first explores the Laplace operator and the Poisson equation and\nstudies the corresponding Green's functions under various boundary conditions\nand constraints. Explicit functional forms are derived in a range of examples.\nThese examples include several novel forms of the Green's function that, to the\nauthor's knowledge, have not previously been presented. Next we present a\nsmoothing spline problem where we penalize the integrated squared derivative of\nthe function to be estimated. We then show how the solution can be explicitly\ncomputed using the Green's function for the Laplace operator. In the last part\nof the paper, we explore the connection between Gaussian processes and\ndifferential equations, and show how the Laplace operator is related to\nBrownian processes and how processes that arise due to boundary conditions and\nshape constraints can be viewed as conditional Gaussian processes. The\npresented connection between Green's functions for the Laplace operator and\ncovariance functions for Brownian processes allows us to introduce several new\nnovel Brownian processes with specific behaviors. Finally, we consider the\nconnection between Gaussian process priors and smoothing splines.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 00:01:17 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Raket", "Lars Lau", ""]]}, {"id": "2102.03316", "submitter": "Winston Chou", "authors": "Winston Chou", "title": "Randomized Controlled Trials with Minimal Data Retention", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amidst rising appreciation for privacy and data usage rights, researchers\nhave increasingly recognized the principle of data minimization, which holds\nthat the accessibility, collection, and retention of subjects' data should be\nkept to the minimum necessary to answer focused research questions. Applying\nthis principle to randomized controlled trials (RCTs), this paper presents\nalgorithms for drawing precise inferences from RCTs under stringent data\nretention and anonymization policies. In particular, we show how to use\nrecursive algorithms to construct running estimates of treatment effects in\nRCTs, thereby allowing individualized records to be deleted or anonymized\nshortly after collection. Devoting special attention to the case of non-i.i.d.\ndata, we further demonstrate how to draw robust inferences from RCTs by\ncombining recursive algorithms with bootstrap and federated strategies.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 17:42:14 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Chou", "Winston", ""]]}, {"id": "2102.03499", "submitter": "Yongming Qu", "authors": "Junxiang Luo, Stephen J. Ruberg, Yongming Qu", "title": "Estimating the treatment effect for adherers using multiple imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Randomized controlled trials are considered the gold standard to evaluate the\ntreatment effect (estimand) for efficacy and safety. According to the recent\nInternational Council on Harmonisation (ICH)-E9 addendum (R1), intercurrent\nevents (ICEs) need to be considered when defining an estimand, and principal\nstratum is one of the five strategies used to handle ICEs. Qu et al. (2020,\nStatistics in Biopharmaceutical Research 12:1-18) proposed estimators for the\nadherer average causal effect (AdACE) for estimating the treatment difference\nfor those who adhere to one or both treatments based on the causal-inference\nframework, and demonstrated the consistency of those estimators. No variance\nestimation formula is provided, however, due to the complexity of the\nestimators. In addition, it is difficult to evaluate the performance of the\nbootstrap confidence interval (CI) due to computational intensity in the\ncomplex estimation procedure. The current research implements estimators for\nAdACE using multiple imputation (MI) and constructs CI through bootstrapping. A\nsimulation study shows that the MI-based estimators provide consistent\nestimators with nominal coverage probability of CIs for the treatment\ndifference for the adherent populations of interest. Application to a real\ndataset is illustrated by comparing two basal insulins for patients with type 1\ndiabetes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 03:59:30 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Luo", "Junxiang", ""], ["Ruberg", "Stephen J.", ""], ["Qu", "Yongming", ""]]}, {"id": "2102.03639", "submitter": "Ranjan Maitra", "authors": "Wei-Chen Chen and Ranjan Maitra", "title": "A Practical Model-based Segmentation Approach for Accurate Activation\n  Detection in Single-Subject functional Magnetic Resonance Imaging Studies", "comments": "20 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Imaging (fMRI) maps cerebral activation in\nresponse to stimuli but this activation is often difficult to detect,\nespecially in low-signal contexts and single-subject studies. Accurate\nactivation detection can be guided by the fact that very few voxels are, in\nreality, truly activated and that activated voxels are spatially localized, but\nit is challenging to incorporate both these facts. We provide a computationally\nfeasible and methodologically sound model-based approach, implemented in the R\npackage MixfMRI, that bounds the a priori expected proportion of activated\nvoxels while also incorporating spatial context. Results on simulation\nexperiments for different levels of activation detection difficulty are\nuniformly encouraging. The value of the methodology in low-signal and\nsingle-subject fMRI studies is illustrated on a sports imagination experiment.\nConcurrently, we also extend the potential use of fMRI as a clinical tool to,\nfor example, detect awareness and improve treatment in individual patients in\npersistent vegetative state, such as traumatic brain injury survivors.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 18:46:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chen", "Wei-Chen", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2102.03645", "submitter": "Christian Hennig", "authors": "Christian Hennig", "title": "An empirical comparison and characterisation of nine popular clustering\n  methods", "comments": "44 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Nine popular clustering methods are applied to 42 real data sets. The aim is\nto give a detailed characterisation of the methods by means of several cluster\nvalidation indexes that measure various individual aspects of the resulting\nclusters such as small within-cluster distances, separation of clusters,\ncloseness to a Gaussian distribution etc. as introduced in Hennig (2019). 30 of\nthe data sets come with a \"true\" clustering. On these data sets the similarity\nof the clusterings from the nine methods to the \"true\" clusterings is explored.\nFurthermore, a mixed effects regression relates the observable individual\naspects of the clusters to the similarity with the \"true\" clusterings, which in\nreal clustering problems is unobservable. The study gives new insight not only\ninto the ability of the methods to discover \"true\" clusterings, but also into\nproperties of clusterings that can be expected from the methods, which is\ncrucial for the choice of a method in a real situation without a given \"true\"\nclustering.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 19:08:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hennig", "Christian", ""]]}, {"id": "2102.03652", "submitter": "Yaakov Malinovsky", "authors": "Yaakov Malinovsky and Paul S. Albert", "title": "Nested Group Testing Procedures for Screening", "comments": "arXiv admin note: text overlap with arXiv:1608.06330", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article reviews a class of adaptive group testing procedures that\noperate under a probabilistic model assumption as follows. Consider a set of\n$N$ items, where item $i$ has the probability $p$ ($p_i$ in the generalized\ngroup testing) to be defective, and the probability $1-p$ to be non-defective\nindependent from the other items. A group test applied to any subset of size\n$n$ is a binary test with two possible outcomes, positive or negative. The\noutcome is negative if all $n$ items are non-defective, whereas the outcome is\npositive if at least one item among the $n$ items is defective. The goal is\ncomplete identification of all $N$ items with the minimum expected number of\ntests.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 19:50:12 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 23:10:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Malinovsky", "Yaakov", ""], ["Albert", "Paul S.", ""]]}, {"id": "2102.03892", "submitter": "Ye Tian", "authors": "Ye Tian, Yang Feng", "title": "RaSE: A Variable Screening Framework via Random Subspace Ensembles", "comments": "58 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable screening methods have been shown to be effective in dimension\nreduction under the ultra-high dimensional setting. Most existing screening\nmethods are designed to rank the predictors according to their individual\ncontributions to the response. As a result, variables that are marginally\nindependent but jointly dependent with the response could be missed. In this\nwork, we propose a new framework for variable screening, Random Subspace\nEnsemble (RaSE), which works by evaluating the quality of random subspaces that\nmay cover multiple predictors. This new screening framework can be naturally\ncombined with any subspace evaluation criterion, which leads to an array of\nscreening methods. The framework is capable to identify signals with no\nmarginal effect or with high-order interaction effects. It is shown to enjoy\nthe sure screening property and rank consistency. We also develop an iterative\nversion of RaSE screening with theoretical support. Extensive simulation\nstudies and real-data analysis show the effectiveness of the new screening\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 19:24:52 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 21:14:13 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 16:04:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tian", "Ye", ""], ["Feng", "Yang", ""]]}, {"id": "2102.04112", "submitter": "Karl L. Hallgren", "authors": "Karl L. Hallgren, Nicholas A. Heard and Melissa J. M. Turcotte", "title": "Changepoint detection on a graph of time series", "comments": "31 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analysing multiple time series that may be subject to changepoints, it\nis sometimes possible to specify a priori, by means of a graph G, which pairs\nof time series are likely to be impacted by simultaneous changepoints. This\narticle proposes a novel Bayesian changepoint model for multiple time series\nthat borrows strength across clusters of connected time series in G to detect\nweak signals for synchronous changepoints. The graphical changepoint model is\nfurther extended to allow dependence between nearby but not necessarily\nsynchronous changepoints across neighbour time series in G. A novel reversible\njump MCMC algorithm making use of auxiliary variables is proposed to sample\nfrom the graphical changepoint model. The merit of the proposed model is\ndemonstrated via a changepoint analysis of real network authentication data\nfrom Los Alamos National Laboratory (LANL), with some success at detecting weak\nsignals for network intrusions across users that are linked by network\nconnectivity, whilst limiting the number of false alerts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:31:48 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hallgren", "Karl L.", ""], ["Heard", "Nicholas A.", ""], ["Turcotte", "Melissa J. M.", ""]]}, {"id": "2102.04143", "submitter": "Ar\\'is Fanjul-Hevia", "authors": "Ar\\'is Fanjul-Hevia, Juan Carlos Pardo-Fern\\'andez, Ingrid Van\n  Keilegom and Wenceslao Gonz\\'alez-Manteiga", "title": "A test for comparing conditional ROC curves with multidimensional\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The comparison of Receiver Operating Characteristic (ROC) curves is\nfrequently used in the literature to compare the discriminatory capability of\ndifferent classification procedures based on diagnostic variables. The\nperformance of these variables can be sometimes influenced by the presence of\nother covariates, and thus they should be taken into account when making the\ncomparison. A new non-parametric test is proposed here for testing the equality\nof two or more dependent ROC curves conditioned to the value of a\nmultidimensional covariate. Projections are used for transforming the problem\ninto a one-dimensional approach easier to handle. Simulations are carried out\nto study the practical performance of the new methodology. A real data set of\npatients with Pleural Effusion is analysed to illustrate this procedure.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 11:41:28 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Fanjul-Hevia", "Ar\u00eds", ""], ["Pardo-Fern\u00e1ndez", "Juan Carlos", ""], ["Van Keilegom", "Ingrid", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "2102.04273", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i, Niccol\\`o Cao, Enrico Rubaltelli, Luigi Lombardi", "title": "A psychometric modeling approach to fuzzy rating data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling fuzziness and imprecision in human rating data is a crucial problem\nin many research areas, including applied statistics, behavioral, social, and\nhealth sciences. Because of the interplay between cognitive, affective, and\ncontextual factors, the process of answering survey questions is a complex\ntask, which can barely be captured by standard (crisp) rating responses. Fuzzy\nrating scales have progressively been adopted to overcome some of the\nlimitations of standard rating scales, including their inability to disentangle\ndecision uncertainty from individual responses. The aim of this article is to\nprovide a novel fuzzy scaling procedure which uses Item Response Theory trees\n(IRTrees) as a psychometric model for the stage-wise latent response process.\nIn so doing, fuzziness of rating data is modeled using the overall rater's\npattern of responses instead of being computed using a single-item based\napproach. This offers a consistent system for interpreting fuzziness in terms\nof individual-based decision uncertainty. A simulation study and two empirical\napplications are adopted to assess the characteristics of the proposed model\nand provide converging results about its effectiveness in modeling fuzziness\nand imprecision in rating data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 15:13:28 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Cao", "Niccol\u00f2", ""], ["Rubaltelli", "Enrico", ""], ["Lombardi", "Luigi", ""]]}, {"id": "2102.04423", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty", "title": "Prepivoted permutation tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general approach to constructing permutation tests that are both\nexact for the null hypothesis of equality of distributions and asymptotically\ncorrect for testing equality of parameters of distributions while allowing the\ndistributions themselves to differ. These robust permutation tests transform a\ngiven test statistic by a consistent estimator of its limiting distribution\nfunction before enumerating its permutation distribution. This transformation,\nknown as prepivoting, aligns the unconditional limiting distribution for the\ntest statistic with the probability limit of its permutation distribution.\nThrough prepivoting, the tests permute one minus an asymptotically valid\n$p$-value for testing the null of equality of parameters. We describe two\napproaches for prepivoting within permutation tests, one directly using\nasymptotic normality and the other using the bootstrap. We further illustrate\nthat permutation tests using bootstrap prepivoting can provide improvements to\nthe order of the error in rejection probability relative to competing\ntransformations when testing equality of parameters, while maintaining\nexactness under equality of distributions. Simulation studies highlight the\nversatility of the proposal, illustrating the restoration of asymptotic\nvalidity to a wide range of permutation tests conducted when only the\nparameters of distributions are equal.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:35:08 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 19:29:28 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Fogarty", "Colin B.", ""]]}, {"id": "2102.04481", "submitter": "Marzieh Shahmandi", "authors": "Marzieh Shahmandi, Paul Wilson, and Mike Thelwall", "title": "A Bayesian Hurdle Quantile Regression Model for Citation Analysis with\n  Mass Points at Lower Values", "comments": "Accepted in Quantitative Science Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression presents a complete picture of the effects on the\nlocation, scale, and shape of the dependent variable at all points, not just\nthe mean. We focus on two challenges for citation count analysis by quantile\nregression: discontinuity and substantial mass points at lower counts. A\nBayesian hurdle quantile regression model for count data with a substantial\nmass point at zero was proposed by King and Song (2019). It uses quantile\nregression for modeling the nonzero data and logistic regression for modeling\nthe probability of zeros versus nonzeros. We show that substantial mass points\nfor low citation counts will nearly certainly also affect parameter estimation\nin the quantile regression part of the model, similar to a mass point at zero.\nWe update the King and Song model by shifting the hurdle point past the main\nmass points. This model delivers more accurate quantile regression for\nmoderately to highly cited articles, especially at quantiles corresponding to\nvalues just beyond the mass points, and enables estimates of the extent to\nwhich factors influence the chances that an article will be low cited. To\nillustrate the potential of this method, it is applied to simulated citation\ncounts and data from Scopus.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 19:03:35 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 09:44:06 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Shahmandi", "Marzieh", ""], ["Wilson", "Paul", ""], ["Thelwall", "Mike", ""]]}, {"id": "2102.04543", "submitter": "Jacob Dorn", "authors": "Jacob Dorn and Kevin Guo", "title": "Sharp Sensitivity Analysis for Inverse Propensity Weighting via Quantile\n  Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inverse propensity weighting (IPW) is a popular method for estimating\ntreatment effects from observational data. However, its correctness relies on\nthe untestable (and frequently implausible) assumption that all confounders\nhave been measured. This paper introduces a robust sensitivity analysis for IPW\nthat estimates the range of treatment effects compatible with a given amount of\nunobserved confounding. The estimated range converges to the narrowest possible\ninterval (under the given assumptions) that must contain the true treatment\neffect. Our proposal is a refinement of the influential sensitivity analysis by\nZhao, Small, and Bhattacharya (2019), which we show gives bounds that are too\nwide even asymptotically. This analysis is based on new partial identification\nresults for Tan (2006)'s marginal sensitivity model.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 21:47:23 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 18:27:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Dorn", "Jacob", ""], ["Guo", "Kevin", ""]]}, {"id": "2102.04687", "submitter": "Sudeep Bapat", "authors": "Sudeep R. Bapat and Rohit Bhardwaj", "title": "On an Inflated Unit-Lindley Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling fractional data in various real life scenarios is a challenging\ntask. This paper consider situations where fractional data is observed on the\ninterval [0,1]. The unit-Lindley distribution has been discussed in the\nliterature where its support lies between 0 and 1. In this paper, we focus on\nan inflated variant of the unit-Lindley distribution, where the inflation\noccurs at both 0 and 1. Various properties of the inflated unit-Lindley\ndistribution are discussed and examined, including point estimation based on\nthe maximum likelihood method and interval estimation. Finally, extensive Monte\nCarlo simulation and real-data analyses are carried out to compare the fit of\nour proposed distribution along with some of the existing ones such as the\ninflated beta and the inflated Kumaraswamy distributions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 07:30:07 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bapat", "Sudeep R.", ""], ["Bhardwaj", "Rohit", ""]]}, {"id": "2102.04740", "submitter": "Christopher Carignan", "authors": "Christopher Carignan and Ander Egurtzegi", "title": "Principal components variable importance reconstruction (PC-VIR):\n  Exploring predictive importance in multicollinear acoustic speech data", "comments": "10 pages, 3 figures, GitHub repository", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method of exploring the relative predictive importance\nof individual variables in multicollinear data sets at three levels of\nsignificance: strong importance, moderate importance, and no importance.\nImplementation of Bonferroni adjustment to control for Type I error in the\nmethod is described, and results with and without the correction are compared.\nAn example of the method in binary logistic modeling is demonstrated by using a\nset of 20 acoustic features to discriminate vocalic nasality in the speech of\nsix speakers of the Mixean variety of Low Navarrese Basque. Validation of the\nmethod is presented by comparing the direction of significant effects to those\nobserved in separate logistic mixed effects models, as well as goodness of fit\nand prediction accuracy compared to partial least squares logistic regression.\nThe results show that the proposed method yields: (1) similar, but more\nconservative estimates in comparison to separate logistic regression models,\n(2) models that fit data as well as partial least squares methods, and (3)\npredictions for new data that are as accurate as partial least squares methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 10:06:43 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Carignan", "Christopher", ""], ["Egurtzegi", "Ander", ""]]}, {"id": "2102.04791", "submitter": "Linda Nab", "authors": "Linda Nab, Maarten van Smeden, Ruth H. Keogh, Rolf H.H. Groenwold", "title": "mecor: An R package for measurement error correction in linear\n  regression models with a continuous outcome", "comments": "34 pages (including appendix), software package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measurement error in a covariate or the outcome of regression models is\ncommon, but is often ignored, even though measurement error can lead to\nsubstantial bias in the estimated covariate-outcome association. While several\ntexts on measurement error correction methods are available, these methods\nremain seldomly applied. To improve the use of measurement error correction\nmethodology, we developed mecor, an R package that implements measurement error\ncorrection methods for regression models with continuous outcomes. Measurement\nerror correction requires information about the measurement error model and its\nparameters. This information can be obtained from four types of studies, used\nto estimate the parameters of the measurement error model: an internal\nvalidation study, a replicates study, a calibration study and an external\nvalidation study. In the package mecor, regression calibration methods and a\nmaximum likelihood method are implemented to correct for measurement error in a\ncontinuous covariate in regression analyses. Additionally, methods of moments\nmethods are implemented to correct for measurement error in the continuous\noutcome in regression analyses. Variance estimation of the corrected estimators\nis provided in closed form and using the bootstrap.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 12:22:12 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Nab", "Linda", ""], ["van Smeden", "Maarten", ""], ["Keogh", "Ruth H.", ""], ["Groenwold", "Rolf H. H.", ""]]}, {"id": "2102.04873", "submitter": "Marija Tepegjozova", "authors": "Marija Tepegjozova, Jing Zhou, Gerda Claeskens and Claudia Czado", "title": "Nonparametric C- and D-vine based quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a field with steadily growing importance in\nstatistical modeling. It is a complementary method to linear regression, since\ncomputing a range of conditional quantile functions provides a more accurate\nmodelling of the stochastic relationship among variables, especially in the\ntails. We introduce a novel non-restrictive and highly flexible nonparametric\nquantile regression approach based on C- and D-vine copulas. Vine copulas allow\nfor separate modeling of marginal distributions and the dependence structure in\nthe data, and can be expressed through a graph theoretical model given by a\nsequence of trees. This way we obtain a quantile regression model, that\novercomes typical issues of quantile regression such as quantile crossings or\ncollinearity, the need for transformations and interactions of variables. Our\napproach incorporates a two-step ahead ordering of variables, by maximizing the\nconditional log-likelihood of the tree sequence, while taking into account the\nnext two tree levels. Further, we show that the nonparametric conditional\nquantile estimator is consistent. The performance of the proposed methods is\nevaluated in both low- and high-dimensional settings using simulated and real\nworld data. The results support the superior prediction ability of the proposed\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 15:17:50 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Tepegjozova", "Marija", ""], ["Zhou", "Jing", ""], ["Claeskens", "Gerda", ""], ["Czado", "Claudia", ""]]}, {"id": "2102.05100", "submitter": "Franz-Georg Wieland", "authors": "Franz-Georg Wieland, Adrian L. Hauber, Marcus Rosenblatt, Christian\n  T\\\"onsing, Jens Timmer", "title": "On structural and practical identifiability", "comments": null, "journal-ref": null, "doi": "10.1016/j.coisb.2021.03.005", "report-no": null, "categories": "stat.ME physics.data-an q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We discuss issues of structural and practical identifiability of partially\nobserved differential equations which are often applied in systems biology. The\ndevelopment of mathematical methods to investigate structural\nnon-identifiability has a long tradition. Computationally efficient methods to\ndetect and cure it have been developed recently. Practical non-identifiability\non the other hand has not been investigated at the same conceptually clear\nlevel. We argue that practical identifiability is more challenging than\nstructural identifiability when it comes to modelling experimental data. We\ndiscuss that the classical approach based on the Fisher information matrix has\nsevere shortcomings. As an alternative, we propose using the profile\nlikelihood, which is a powerful approach to detect and resolve practical\nnon-identifiability.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 19:58:29 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 10:29:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wieland", "Franz-Georg", ""], ["Hauber", "Adrian L.", ""], ["Rosenblatt", "Marcus", ""], ["T\u00f6nsing", "Christian", ""], ["Timmer", "Jens", ""]]}, {"id": "2102.05103", "submitter": "Thomas Maullin-Sapey", "authors": "Thomas Maullin-Sapey, Thomas E. Nichols", "title": "Fisher Scoring for crossed factor Linear Mixed Models", "comments": "For supplementary material see\n  https://www.overleaf.com/read/bvscgqrvqnjh . For code and notebooks, see\n  https://github.com/TomMaullin/LMMPaper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of longitudinal, heterogeneous or unbalanced clustered data is\nof primary importance to a wide range of applications. The Linear Mixed Model\n(LMM) is a popular and flexible extension of the linear model specifically\ndesigned for such purposes. Historically, a large proportion of material\npublished on the LMM concerns the application of popular numerical optimization\nalgorithms, such as Newton-Raphson, Fisher Scoring and Expectation Maximization\nto single-factor LMMs (i.e. LMMs that only contain one \"factor\" by which\nobservations are grouped). However, in recent years, the focus of the LMM\nliterature has moved towards the development of estimation and inference\nmethods for more complex, multi-factored designs. In this paper, we present and\nderive new expressions for the extension of an algorithm classically used for\nsingle-factor LMM parameter estimation, Fisher Scoring, to multiple,\ncrossed-factor designs. Through simulation and real data examples, we compare\nfive variants of the Fisher Scoring algorithm with one another, as well as\nagainst a baseline established by the R package lmer, and find evidence of\ncorrectness and strong computational efficiency for four of the five proposed\napproaches. Additionally, we provide a new method for LMM Satterthwaite degrees\nof freedom estimation based on analytical results, which does not require\niterative gradient estimation. Via simulation, we find that this approach\nproduces estimates with both lower bias and lower variance than the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 20:00:11 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 20:35:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Maullin-Sapey", "Thomas", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "2102.05135", "submitter": "Taman Narayan", "authors": "Taman Narayan, Serena Wang, Kevin Canini, Maya Gupta", "title": "Regularization Strategies for Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate different methods for regularizing quantile regression when\npredicting either a subset of quantiles or the full inverse CDF. We show that\nminimizing an expected pinball loss over a continuous distribution of quantiles\nis a good regularizer even when only predicting a specific quantile. For\npredicting multiple quantiles, we propose achieving the classic goal of\nnon-crossing quantiles by using deep lattice networks that treat the quantile\nas a monotonic input feature, and we discuss why monotonicity on other features\nis an apt regularizer for quantile regression. We show that lattice models\nenable regularizing the predicted distribution to a location-scale family.\nLastly, we propose applying rate constraints to improve the calibration of the\nquantile predictions on specific subsets of interest and improve fairness\nmetrics. We demonstrate our contributions on simulations, benchmark datasets,\nand real quantile regression problems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 21:10:35 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Narayan", "Taman", ""], ["Wang", "Serena", ""], ["Canini", "Kevin", ""], ["Gupta", "Maya", ""]]}, {"id": "2102.05223", "submitter": "Jiaqi Gu", "authors": "Jiaqi Gu and Guosheng Yin", "title": "Bayesian Knockoff Filter Using Gibbs Sampler", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In many fields, researchers are interested in discovering features with\nsubstantial effect on the response from a large number of features and\ncontrolling the proportion of false discoveries. By incorporating the knockoff\nprocedure in the Bayesian framework, we develop the Bayesian knockoff filter\n(BKF) for selecting features that have important effect on the response. In\ncontrast to the fixed knockoff variables in the frequentist procedures, we\nallow the knockoff variables to be continuously updated in the Markov chain\nMonte Carlo. Based on the posterior samples and elaborated greedy selection\nprocedures, our method can distinguish the truly important features as well as\ncontrolling the Bayesian false discovery rate at a desirable level. Numerical\nexperiments on both synthetic and real data demonstrate the advantages of our\nmethod over existing knockoff methods and Bayesian variable selection\napproaches, i.e., the BKF possesses higher power and yields a lower false\ndiscovery rate.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 02:21:41 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Gu", "Jiaqi", ""], ["Yin", "Guosheng", ""]]}, {"id": "2102.05329", "submitter": "Pernille Hansen Ms", "authors": "Pernille EH. Hansen, Rasmus Waagepetersen, Anne Marie Svane, Jon\n  Sporring, Hans JT. Stephensen, Stine Hasselholt and Stefan Sommer", "title": "Currents and K-functions for Fiber Point Processes", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of images of sets of fibers such as myelin sheaths or skeletal\nmuscles must account for both the spatial distribution of fibers and\ndifferences in fiber shape. This necessitates a combination of point process\nand shape analysis methodology. In this paper, we develop a K-function for\nshape-valued point processes by embedding shapes as currents, thus equipping\nthe point process domain with metric structure inherited from a reproducing\nkernel Hilbert space. We extend Ripley's K-function which measures deviations\nfrom spatial homogeneity of point processes to fiber data. The paper provides a\ntheoretical account of the statistical foundation of the K-function and its\nextension to fiber data, and we test the developed K-function on simulated as\nwell as real data sets. This includes a fiber data set consisting of myelin\nsheaths, visualizing the spatial and fiber shape behavior of myelin\nconfigurations at different debts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 09:02:19 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Hansen", "Pernille EH.", ""], ["Waagepetersen", "Rasmus", ""], ["Svane", "Anne Marie", ""], ["Sporring", "Jon", ""], ["Stephensen", "Hans JT.", ""], ["Hasselholt", "Stine", ""], ["Sommer", "Stefan", ""]]}, {"id": "2102.05386", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Shyamal Ghosh and Prajamitra Bhuyan and Maxim Finkelstein", "title": "On a Bivariate Copula for Modeling Negative Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new bivariate copula is proposed for modeling negative dependence between\ntwo random variables. We show that it complies with most of the popular notions\nof negative dependence reported in the literature and study some of its basic\nproperties. Specifically, the Spearman's rho and the Kendall's tau for the\nproposed copula have a simple one-parameter form with negative values in the\nfull range. Some important ordering properties comparing the strength of\nnegative dependence with respect to the parameter involved are considered.\nSimple examples of the corresponding bivariate distributions with popular\nmarginals are presented. Application of the proposed copula is illustrated\nusing a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 11:35:38 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Ghosh", "Shyamal", ""], ["Bhuyan", "Prajamitra", ""], ["Finkelstein", "Maxim", ""]]}, {"id": "2102.05569", "submitter": "Sander Greenland", "authors": "Sander Greenland", "title": "There are natural scores: Full comment on Shafer, \"Testing by betting: A\n  strategy for statistical and scientific communication\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Shafer (2021) offers a betting perspective on statistical testing which may\nbe useful for foundational debates, given that disputes over such testing\ncontinue to be intense. To be helpful for researchers, however, this\nperspective will need more elaboration using real examples in which (a) the\nbetting score has a justification and interpretation in terms of study goals\nthat distinguishes it from the uncountable mathematical possibilities, and (b)\nthe assumptions in the sampling model are uncertain. On justification, Shafer\nsays 'No one has made a convincing case for any particular choice' of a score\nderived from a P-value and then states that 'the choice is fundamentally\narbitrary'. Yet some (but not most) scores can be motivated by study goals\n(e.g., information measurement; decision making). The one I have seen\nrepeatedly in information statistics and data mining is the surprisal, logworth\nor S-value s = -log(p), where the log base determines the scale. The present\ncomment explains the rationale for this choice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 17:02:50 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Greenland", "Sander", ""]]}, {"id": "2102.05895", "submitter": "Veronique Maume-Deschamps", "authors": "Kevin Elie-Dit-Cosaque (PSPM, ICJ), V\\'eronique Maume-Deschamps (ICJ,\n  PSPM)", "title": "On quantile oriented sensitivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study quantile oriented sensitivity indices (QOSA indices) and\nquantile oriented Shapley effects (QOSE). Some theoretical properties of QOSA\nindices will be given and several calculations of QOSA indices and QOSE will\nallow to better understand the behaviour and the interest of these indices.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 08:59:01 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Elie-Dit-Cosaque", "Kevin", "", "PSPM, ICJ"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ,\n  PSPM"]]}, {"id": "2102.06002", "submitter": "Harris Quach", "authors": "Harris Quach, Bing Li", "title": "Generalized Forward Sufficient Dimension Reduction for Categorical and\n  Ordinal Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a forward sufficient dimension reduction method for categorical or\nordinal responses by extending the outer product of gradients and minimum\naverage variance estimator to multinomial generalized linear model. Previous\nwork in this direction extend forward regression to binary responses, and are\napplied in a pairwise manner to multinomial data, which is less efficient than\nour approach. Like other forward regression-based sufficient dimension\nreduction methods, our approach avoids the relatively stringent distributional\nrequirements necessary for inverse regression alternatives. We show consistency\nof our proposed estimator and derive its convergence rate. We develop an\nalgorithm for our methods based on repeated applications of available\nalgorithms for forward regression. We also propose a clustering-based tuning\nprocedure to estimate the tuning parameters. The effectiveness of our estimator\nand related algorithms is demonstrated via simulations and applications.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:47:31 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 23:16:20 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Quach", "Harris", ""], ["Li", "Bing", ""]]}, {"id": "2102.06048", "submitter": "Trang Nguyen", "authors": "Trang Quynh Nguyen, Elizabeth L. Ogburn, Elizabeth B. Sarker, Noah\n  Greifer, Ian Schmid, Ina M. Koning, Elizabeth A. Stuart", "title": "Causal mediation analysis: From simple to more robust strategies for\n  estimation of marginal natural (in)direct effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to provide practitioners of causal mediation analysis with a\nbetter understanding of estimation options. We take as inputs two familiar\nstrategies (weighting and model-based prediction) and a simple way of combining\nthem (weighted models), and show how we can generate a range of estimators with\ndifferent modeling requirements and robustness properties. The primary goal is\nto help build intuitive appreciation for robust estimation that is conducive to\nsound practice. A second goal is to provide a \"menu\" of estimators that\npractitioners can choose from for the estimation of marginal natural (in)direct\neffects. The estimators generated from this exercise include some that coincide\nor are similar to existing estimators and others that have not previously\nappeared in the literature. We note several different ways to estimate the\nweights for cross-world weighting based on three expressions of the weighting\nfunction, including one that is novel; and show how to check the resulting\ncovariate and mediator balance. We use a random continuous weights bootstrap to\nobtain confidence intervals, and also derive general asymptotic (sandwich)\nvariance formulas for the estimators. The estimators are illustrated using data\nfrom an adolescent alcohol use prevention study.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 14:34:47 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 18:53:03 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nguyen", "Trang Quynh", ""], ["Ogburn", "Elizabeth L.", ""], ["Sarker", "Elizabeth B.", ""], ["Greifer", "Noah", ""], ["Schmid", "Ian", ""], ["Koning", "Ina M.", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2102.06130", "submitter": "Zhiyang Zhou", "authors": "Zhiyang Zhou and Peijun Sang", "title": "Continuum centroid classifier for functional data", "comments": "38 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at the binary classification of functional data, we propose the\ncontinuum centroid classifier (CCC) built upon projections of functional data\nonto one specific direction. This direction is obtained via bridging the\nregression and classification. Controlling the extent of supervision, our\ntechnique is neither unsupervised nor fully supervised. Thanks to the intrinsic\ninfinite dimension of functional data, one of two subtypes of CCC enjoys the\n(asymptotic) zero misclassification rate. Our proposal includes an effective\nalgorithm that yields a consistent empirical counterpart of CCC. Simulation\nstudies demonstrate the performance of CCC in different scenarios. Finally, we\napply CCC to two real examples.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 17:08:13 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Zhou", "Zhiyang", ""], ["Sang", "Peijun", ""]]}, {"id": "2102.06197", "submitter": "Johannes Ernst-Emanuel Buck", "authors": "Ngoc Mai Tran and Johannes Buck and Claudia Kl\\\"uppelberg", "title": "Causal Discovery of a River Network from its Extremes", "comments": "26 pages, 15 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference for extremes aims to discover cause and effect relations\nbetween large observed values of random variables. Over the last years, a\nnumber of methods have been proposed for solving the Hidden River Problem, with\nthe Danube data set as benchmark. In this paper, we provide \\QTree, a new and\nsimple algorithm to solve the Hidden River Problem that outperforms existing\nmethods. \\QTree\\ returns a directed graph and achieves almost perfect recovery\non the Danube as well as on new data from the Lower Colorado River. It can\nhandle missing data, has an automated parameter tuning procedure, and runs in\ntime $O(n |V|^2)$, where $n$ is the number of observations and $|V|$ the number\nof nodes in the graph. \\QTree\\ relies on qualitative aspects of the max-linear\nBayesian network model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:57:21 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Tran", "Ngoc Mai", ""], ["Buck", "Johannes", ""], ["Kl\u00fcppelberg", "Claudia", ""]]}, {"id": "2102.06202", "submitter": "Anastasios Angelopoulos", "authors": "Anastasios N. Angelopoulos and Stephen Bates and Tijana Zrnic and\n  Michael I. Jordan", "title": "Private Prediction Sets", "comments": "Code available at\n  https://github.com/aangelopoulos/private_prediction_sets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world settings involving consequential decision-making, the\ndeployment of machine learning systems generally requires both reliable\nuncertainty quantification and protection of individuals' privacy. We present a\nframework that treats these two desiderata jointly. Our framework is based on\nconformal prediction, a methodology that augments predictive models to return\nprediction sets that provide uncertainty quantification -- they provably cover\nthe true response with a user-specified probability, such as 90%. One might\nhope that when used with privately-trained models, conformal prediction would\nyield privacy guarantees for the resulting prediction sets; unfortunately this\nis not the case. To remedy this key problem, we develop a method that takes any\npre-trained predictive model and outputs differentially private prediction\nsets. Our method follows the general approach of split conformal prediction; we\nuse holdout data to calibrate the size of the prediction sets but preserve\nprivacy by using a privatized quantile subroutine. This subroutine compensates\nfor the noise introduced to preserve privacy in order to guarantee correct\ncoverage. We evaluate the method with experiments on the CIFAR-10, ImageNet,\nand CoronaHack datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:59:11 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Angelopoulos", "Anastasios N.", ""], ["Bates", "Stephen", ""], ["Zrnic", "Tijana", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2102.06416", "submitter": "Martin Jullum PhD", "authors": "Kjersti Aas, Thomas Nagler, Martin Jullum, Anders L{\\o}land", "title": "Explaining predictive models using Shapley values and non-parametric\n  vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original development of Shapley values for prediction explanation relied\non the assumption that the features being described were independent. If the\nfeatures in reality are dependent this may lead to incorrect explanations.\nHence, there have recently been attempts of appropriately modelling/estimating\nthe dependence between the features. Although the proposed methods clearly\noutperform the traditional approach assuming independence, they have their\nweaknesses. In this paper we propose two new approaches for modelling the\ndependence between the features.\n  Both approaches are based on vine copulas, which are flexible tools for\nmodelling multivariate non-Gaussian distributions able to characterise a wide\nrange of complex dependencies.\n  The performance of the proposed methods is evaluated on simulated data sets\nand a real data set. The experiments demonstrate that the vine copula\napproaches give more accurate approximations to the true Shapley values than\nits competitors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 09:43:28 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Aas", "Kjersti", ""], ["Nagler", "Thomas", ""], ["Jullum", "Martin", ""], ["L\u00f8land", "Anders", ""]]}, {"id": "2102.06437", "submitter": "Federico Castelletti", "authors": "Federico Castelletti and Stefano Peluso", "title": "Equivalence class selection of categorical graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the structure of dependence relations between variables is a\npervasive issue in the statistical literature. A directed acyclic graph (DAG)\ncan represent a set of conditional independences, but different DAGs may encode\nthe same set of relations and are indistinguishable using observational data.\nEquivalent DAGs can be collected into classes, each represented by a partially\ndirected graph known as essential graph (EG). Structure learning directly\nconducted on the EG space, rather than on the allied space of DAGs, leads to\ntheoretical and computational benefits. Still, the majority of efforts in the\nliterature has been dedicated to Gaussian data, with less attention to methods\ndesigned for multivariate categorical data. We then propose a Bayesian\nmethodology for structure learning of categorical EGs. Combining a constructive\nparameter prior elicitation with a graph-driven likelihood decomposition, we\nderive a closed-form expression for the marginal likelihood of a categorical EG\nmodel. Asymptotic properties are studied, and an MCMC sampler scheme developed\nfor approximate posterior inference. We evaluate our methodology on both\nsimulated scenarios and real data, with appreciable performance in comparison\nwith state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 10:50:44 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Castelletti", "Federico", ""], ["Peluso", "Stefano", ""]]}, {"id": "2102.06522", "submitter": "Samuel Wiqvist", "authors": "Samuel Wiqvist, Jes Frellsen, Umberto Picchini", "title": "Sequential Neural Posterior and Likelihood Approximation", "comments": "28 pages, 8 tables, 14 figures. The supplementary material is\n  attached to the main paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the sequential neural posterior and likelihood approximation\n(SNPLA) algorithm. SNPLA is a normalizing flows-based algorithm for inference\nin implicit models, and therefore is a simulation-based inference method that\nonly requires simulations from a generative model. SNPLA avoids Markov chain\nMonte Carlo sampling and correction-steps of the parameter proposal function\nthat are introduced in similar methods, but that can be numerically unstable or\nrestrictive. By utilizing the reverse KL divergence, SNPLA manages to learn\nboth the likelihood and the posterior in a sequential manner. Over four\nexperiments, we show that SNPLA performs competitively when utilizing the same\nnumber of model simulations as used in other methods, even though the inference\nproblem for SNPLA is more complex due to the joint learning of posterior and\nlikelihood function. Due to utilizing normalizing flows SNPLA generates\nposterior draws much faster (4 orders of magnitude) than MCMC-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 13:46:47 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 09:21:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wiqvist", "Samuel", ""], ["Frellsen", "Jes", ""], ["Picchini", "Umberto", ""]]}, {"id": "2102.06573", "submitter": "Alberto Caron", "authors": "Alberto Caron, Gianluca Baio and Ioanna Manolopoulou", "title": "Sparse Bayesian Causal Forests for Heterogeneous Treatment Effects\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a sparsity-inducing version of Bayesian Causal Forests, a\nrecently proposed nonparametric causal regression model that employs Bayesian\nAdditive Regression Trees and is specifically designed to estimate\nheterogeneous treatment effects using observational data. The sparsity-inducing\ncomponent we introduce is motivated by empirical studies where the number of\npre-treatment covariates available is non-negligible, leading to different\ndegrees of sparsity underlying the surfaces of interest in the estimation of\nindividual treatment effects. The extended version presented in this work,\nwhich we name Sparse Bayesian Causal Forest, is equipped with an additional\npair of priors allowing the model to adjust the weight of each covariate\nthrough the corresponding number of splits in the tree ensemble. These priors\nimprove the model's adaptability to sparse settings and allow to perform fully\nBayesian variable selection in a framework for treatment effects estimation,\nand thus to uncover the moderating factors driving heterogeneity. In addition,\nthe method allows prior knowledge about the relevant confounding pre-treatment\ncovariates and the relative magnitude of their impact on the outcome to be\nincorporated in the model. We illustrate the performance of our method in\nsimulated studies, in comparison to Bayesian Causal Forest and other\nstate-of-the-art models, to demonstrate how it scales up with an increasing\nnumber of covariates and how it handles strongly confounded scenarios. Finally,\nwe also provide an example of application using real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 15:24:50 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 18:13:08 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 20:59:06 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Caron", "Alberto", ""], ["Baio", "Gianluca", ""], ["Manolopoulou", "Ioanna", ""]]}, {"id": "2102.06586", "submitter": "Harold Chiang", "authors": "Harold D. Chiang, Kengo Kato, Yuya Sasaki and Takuya Ura", "title": "Linear programming approach to nonparametric inference under shape\n  restrictions: with an application to regression kink designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method of constructing confidence bands for nonparametric\nregression functions under shape constraints. This method can be implemented\nvia a linear programming, and it is thus computationally appealing. We\nillustrate a usage of our proposed method with an application to the regression\nkink design (RKD). Econometric analyses based on the RKD often suffer from wide\nconfidence intervals due to slow convergence rates of nonparametric derivative\nestimators. We demonstrate that economic models and structures motivate shape\nrestrictions, which in turn contribute to shrinking the confidence interval for\nan analysis of the causal effects of unemployment insurance benefits on\nunemployment durations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 15:52:05 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Chiang", "Harold D.", ""], ["Kato", "Kengo", ""], ["Sasaki", "Yuya", ""], ["Ura", "Takuya", ""]]}, {"id": "2102.06731", "submitter": "Andrew Jones", "authors": "Andrew Jones, F. William Townes, Didong Li, Barbara E. Engelhardt", "title": "Contrastive latent variable modeling with application to case-control\n  sequencing experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput RNA-sequencing (RNA-seq) technologies are powerful tools for\nunderstanding cellular state. Often it is of interest to quantify and summarize\nchanges in cell state that occur between experimental or biological conditions.\nDifferential expression is typically assessed using univariate tests to measure\ngene-wise shifts in expression. However, these methods largely ignore changes\nin transcriptional correlation. Furthermore, there is a need to identify the\nlow-dimensional structure of the gene expression shift to identify collections\nof genes that change between conditions. Here, we propose contrastive latent\nvariable models designed for count data to create a richer portrait of\ndifferential expression in sequencing data. These models disentangle the\nsources of transcriptional variation in different conditions, in the context of\nan explicit model of variation at baseline. Moreover, we develop a model-based\nhypothesis testing framework that can test for global and gene subset-specific\nchanges in expression. We test our model through extensive simulations and\nanalyses with count-based gene expression data from perturbation and\nobservational sequencing experiments. We find that our methods can effectively\nsummarize and quantify complex transcriptional changes in case-control\nexperimental sequencing data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 19:26:03 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jones", "Andrew", ""], ["Townes", "F. William", ""], ["Li", "Didong", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "2102.06746", "submitter": "Jacopo Diquigiovanni", "authors": "Jacopo Diquigiovanni, Matteo Fontana, Simone Vantini", "title": "The Importance of Being a Band: Finite-Sample Exact Distribution-Free\n  Prediction Sets for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional Data Analysis represents a field of growing interest in\nstatistics. Despite several studies have been proposed leading to fundamental\nresults, the problem of obtaining valid and efficient prediction sets has not\nbeen thoroughly covered. Indeed, the great majority of methods currently in the\nliterature rely on strong distributional assumptions (e.g, Gaussianity),\ndimension reduction techniques and/or asymptotic arguments. In this work, we\npropose a new nonparametric approach in the field of Conformal Prediction based\non a new family of nonconformity measures inducing conformal predictors able to\ncreate closed-form finite-sample valid or exact prediction sets under very\nminimal distributional assumptions. In addition, our proposal ensures that the\nprediction sets obtained are bands, an essential feature in the functional\nsetting that allows the visualization and interpretation of such sets. The\nprocedure is also fast, scalable, does not rely on functional dimension\nreduction techniques and allows the user to select different nonconformity\nmeasures depending on the problem at hand always obtaining valid bands. Within\nthis family of measures, we propose also a specific measure leading to\nprediction bands asymptotically no less efficient than those with constant\nwidth.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:03:38 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 10:05:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Diquigiovanni", "Jacopo", ""], ["Fontana", "Matteo", ""], ["Vantini", "Simone", ""]]}, {"id": "2102.06770", "submitter": "Peter Schochet", "authors": "Peter Z. Schochet", "title": "Statistical Power for Estimating Treatment Effects Using\n  Difference-in-Differences and Comparative Interrupted Time Series Designs\n  with Variation in Treatment Timing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article develops new closed-form variance expressions for power analyses\nfor commonly used panel model estimators. The main contribution is to\nincorporate variation in treatment timing into the analysis, but the variance\nformulas also account for other key design features that arise in practice:\nautocorrelated errors, unequal measurement intervals, and clustering due to the\nunit of treatment assignment. We consider power formulas for both\ncross-sectional and longitudinal models and allow for covariates to improve\nprecision. An illustrative power analysis provides guidance on appropriate\nsample sizes for various model specifications. An available Shiny R dashboard\nperforms the sample size calculations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:56:12 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Schochet", "Peter Z.", ""]]}, {"id": "2102.06801", "submitter": "Blair Bilodeau", "authors": "Blair Bilodeau, Alex Stringer, Yanbo Tang", "title": "Stochastic Convergence Rates and Applications of Adaptive Quadrature in\n  Bayesian Inference", "comments": "61 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first stochastic convergence rates for a family of adaptive\nquadrature rules used to normalize the posterior distribution in Bayesian\nmodels. Our results apply to the uniform relative error in the approximate\nposterior density, the coverage probabilities of approximate credible sets, and\napproximate moments and quantiles, therefore guaranteeing fast asymptotic\nconvergence of approximate summary statistics used in practice. The family of\nquadrature rules includes adaptive Gauss-Hermite quadrature, and we apply this\nrule in two challenging low-dimensional examples. Further, we demonstrate how\nadaptive quadrature can be used as a crucial component of a modern approximate\nBayesian inference procedure for high-dimensional additive models. The method\nis implemented and made publicly available in the aghq package for the R\nlanguage, available on CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 22:37:27 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 01:19:03 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Bilodeau", "Blair", ""], ["Stringer", "Alex", ""], ["Tang", "Yanbo", ""]]}, {"id": "2102.06851", "submitter": "Juan Domingo Gonzalez", "authors": "Juan D. Gonzalez, Ricardo Maronna, Victor J. Yohai and Ruben H.Zamar", "title": "Robust Model-Based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of robust and Fisher-consistent estimators for mixture\nmodels. These estimators can be used to construct robust model-based clustering\nprocedures. We study in detail the case of multivariate normal mixtures and\npropose a procedure that uses S estimators of multivariate location and\nscatter. We develop an algorithm to compute the estimators and to build the\nclusters which is quite similar to the EM algorithm. An extensive Monte Carlo\nsimulation study shows that our proposal compares favorably with other robust\nand non robust model-based clustering procedures. We apply ours and alternative\nprocedures to a real data set and again find that the best results are obtained\nusing our proposal.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 02:51:42 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 21:17:49 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 16:30:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gonzalez", "Juan D.", ""], ["Maronna", "Ricardo", ""], ["Yohai", "Victor J.", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "2102.07060", "submitter": "Anand Deo", "authors": "Anand Deo, Karthyek Murthy", "title": "Achieving Efficiency in Black Box Simulation of Distribution Tails with\n  Self-structuring Importance Samplers", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the increasing adoption of models which facilitate greater\nautomation in risk management and decision-making, this paper presents a novel\nImportance Sampling (IS) scheme for measuring distribution tails of objectives\nmodelled with enabling tools such as feature-based decision rules, mixed\ninteger linear programs, deep neural networks, etc. Conventional efficient IS\napproaches suffer from feasibility and scalability concerns due to the need to\nintricately tailor the sampler to the underlying probability distribution and\nthe objective. This challenge is overcome in the proposed black-box scheme by\nautomating the selection of an effective IS distribution with a transformation\nthat implicitly learns and replicates the concentration properties observed in\nless rare samples. This novel approach is guided by a large deviations\nprinciple that brings out the phenomenon of self-similarity of optimal IS\ndistributions. The proposed sampler is the first to attain asymptotically\noptimal variance reduction across a spectrum of multivariate distributions\ndespite being oblivious to the underlying structure. The large deviations\nprinciple additionally results in new distribution tail asymptotics capable of\nyielding operational insights. The applicability is illustrated by considering\nproduct distribution networks and portfolio credit risk models informed by\nneural networks as examples.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 03:37:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 03:41:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Deo", "Anand", ""], ["Murthy", "Karthyek", ""]]}, {"id": "2102.07094", "submitter": "Raphael Huser", "authors": "Pavel Krupskii, Rapha\\\"el Huser", "title": "Modeling Spatial Dependence with Cauchy Convolution Processes", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the class of dependence models for spatial data obtained from Cauchy\nconvolution processes based on different types of kernel functions. We show\nthat the resulting spatial processes have appealing tail dependence properties,\nsuch as tail dependence at short distances and independence at long distances\nwith suitable kernel functions. We derive the extreme-value limits of these\nprocesses, study their smoothness properties, and detail some interesting\nspecial cases. To get higher flexibility at sub-asymptotic levels and\nseparately control the bulk and the tail dependence properties, we further\npropose spatial models constructed by mixing a Cauchy convolution process with\na Gaussian process. We demonstrate that this framework indeed provides a rich\nclass of models for the joint modeling of the bulk and the tail behaviors. Our\nproposed inference approach relies on matching model-based and empirical\nsummary statistics, and an extensive simulation study shows that it yields\naccurate estimates. We demonstrate our new methodology by application to a\ntemperature dataset measured at 97 monitoring stations in the state of\nOklahoma, US. Our results indicate that our proposed model provides a very good\nfit to the data, and that it captures both the bulk and the tail dependence\nstructures accurately.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 07:42:12 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 22:15:28 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 08:38:19 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Krupskii", "Pavel", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "2102.07162", "submitter": "Alexander Ly", "authors": "Alexander Ly and Eric-Jan Wagenmakers", "title": "Bayes Factors for Peri-Null Hypotheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A perennial objection against Bayes factor point-null hypothesis tests is\nthat the point-null hypothesis is known to be false from the outset. Following\nMorey and Rouder (2011) we examine the consequences of approximating the sharp\npoint-null hypothesis by a hazy `peri-null' hypothesis instantiated as a narrow\nprior distribution centered on the point of interest. The peri-null Bayes\nfactor then equals the point-null Bayes factor multiplied by a correction term\nwhich is itself a Bayes factor. For moderate sample sizes, the correction term\nis relatively inconsequential; however, for large sample sizes the correction\nterm becomes influential and causes the peri-null Bayes factor to be\ninconsistent and approach a limit that depends on the ratio of prior ordinates\nevaluated at the maximum likelihood estimate. We characterize the asymptotic\nbehavior of the peri-null Bayes factor and discuss how to construct peri-null\nBayes factor hypothesis tests that are also consistent.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 14:44:42 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Ly", "Alexander", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "2102.07211", "submitter": "Yiliang Zhang", "authors": "Yiliang Zhang, Zhiqi Bu", "title": "Efficient Designs of SLOPE Penalty Sequences in Finite Dimension", "comments": "Accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In linear regression, SLOPE is a new convex analysis method that generalizes\nthe Lasso via the sorted L1 penalty: larger fitted coefficients are penalized\nmore heavily. This magnitude-dependent regularization requires an input of\npenalty sequence $\\lambda$, instead of a scalar penalty as in the Lasso case,\nthus making the design extremely expensive in computation. In this paper, we\npropose two efficient algorithms to design the possibly high-dimensional SLOPE\npenalty, in order to minimize the mean squared error. For Gaussian data\nmatrices, we propose a first order Projected Gradient Descent (PGD) under the\nApproximate Message Passing regime. For general data matrices, we present a\nzero-th order Coordinate Descent (CD) to design a sub-class of SLOPE, referred\nto as the k-level SLOPE. Our CD allows a useful trade-off between the accuracy\nand the computation speed. We demonstrate the performance of SLOPE with our\ndesigns via extensive experiments on synthetic data and real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 18:06:56 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 02:59:27 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Zhang", "Yiliang", ""], ["Bu", "Zhiqi", ""]]}, {"id": "2102.07356", "submitter": "Pedro Ramos", "authors": "Pedro L. Ramos, Eduardo Ramos, Francisco A. Rodrigues, Francisco\n  Louzada", "title": "A modified closed-form maximum likelihood estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum likelihood estimator plays a fundamental role in statistics.\nHowever, for many models, the estimators do not have closed-form expressions.\nThis limitation can be significant in situations where estimates and\npredictions need to be computed in real-time, such as in applications based on\nembedded technology, in which numerical methods can not be implemented. This\npaper provides a modification in the maximum likelihood estimator that allows\nus to obtain the estimators in closed-form expressions under some conditions.\nUnder mild conditions, the estimator is invariant under one-to-one\ntransformations, consistent, and has an asymptotic normal distribution. The\nproposed modified version of the maximum likelihood estimator is illustrated on\nthe Gamma, Nakagami, and Beta distributions and compared with the standard\nmaximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 06:03:48 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 20:54:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ramos", "Pedro L.", ""], ["Ramos", "Eduardo", ""], ["Rodrigues", "Francisco A.", ""], ["Louzada", "Francisco", ""]]}, {"id": "2102.07378", "submitter": "Sayantan Banerjee", "authors": "Sayantan Banerjee", "title": "Horseshoe shrinkage methods for Bayesian fusion estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of estimation and structure learning of high\ndimensional signals via a normal sequence model, where the underlying parameter\nvector is piecewise constant, or has a block structure. We develop a Bayesian\nfusion estimation method by using the Horseshoe prior to induce a strong\nshrinkage effect on successive differences in the mean parameters,\nsimultaneously imposing sufficient prior concentration for non-zero values of\nthe same. The proposed method thus facilitates consistent estimation and\nstructure recovery of the signal pieces. We provide theoretical justifications\nof our approach by deriving posterior convergence rates and establishing\nselection consistency under suitable assumptions. We also extend our proposed\nmethod to signal de-noising over arbitrary graphs and develop efficient\ncomputational methods along with providing theoretical guarantees. We\ndemonstrate the superior performance of the Horseshoe based Bayesian fusion\nestimation method through extensive simulations and two real-life examples on\nsignal de-noising in biological and geophysical applications. We also\ndemonstrate the estimation performance of our method on a real-world large\nnetwork for the graph signal de-noising problem.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 07:38:57 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 05:37:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Banerjee", "Sayantan", ""]]}, {"id": "2102.07612", "submitter": "Ath\\'ena\\\"is Gautier", "authors": "Ath\\'ena\\\"is Gautier, David Ginsbourger, Guillaume Pirot", "title": "Goal-oriented adaptive sampling under random field modelling of response\n  probability distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the study of natural and artificial complex systems, responses that are\nnot completely determined by the considered decision variables are commonly\nmodelled probabilistically, resulting in response distributions varying across\ndecision space. We consider cases where the spatial variation of these response\ndistributions does not only concern their mean and/or variance but also other\nfeatures including for instance shape or uni-modality versus multi-modality.\nOur contributions build upon a non-parametric Bayesian approach to modelling\nthe thereby induced fields of probability distributions, and in particular to a\nspatial extension of the logistic Gaussian model.\n  The considered models deliver probabilistic predictions of response\ndistributions at candidate points, allowing for instance to perform\n(approximate) posterior simulations of probability density functions, to\njointly predict multiple moments and other functionals of target distributions,\nas well as to quantify the impact of collecting new samples on the state of\nknowledge of the distribution field of interest. In particular, we introduce\nadaptive sampling strategies leveraging the potential of the considered random\ndistribution field models to guide system evaluations in a goal-oriented way,\nwith a view towards parsimoniously addressing calibration and related problems\nfrom non-linear (stochastic) inversion and global optimisation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 15:55:23 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:42:47 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Gautier", "Ath\u00e9na\u00efs", ""], ["Ginsbourger", "David", ""], ["Pirot", "Guillaume", ""]]}, {"id": "2102.07673", "submitter": "Pedro D\\'iez", "authors": "Marc Rocas, Alberto Garc\\'ia-Gonz\\'alez, Sergio Zlotnik, Xabier\n  Larr\\'ayoz and Pedro D\\'iez", "title": "Nonintrusive Uncertainty Quantification for automotive crash problems\n  with VPS/Pamcrash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty Quantification (UQ) is a key discipline for computational\nmodeling of complex systems, enhancing reliability of engineering simulations.\nIn crashworthiness, having an accurate assessment of the behavior of the model\nuncertainty allows reducing the number of prototypes and associated costs.\nCarrying out UQ in this framework is especially challenging because it requires\nhighly expensive simulations. In this context, surrogate models (metamodels)\nallow drastically reducing the computational cost of Monte Carlo process.\nDifferent techniques to describe the metamodel are considered, Ordinary\nKriging, Polynomial Response Surfaces and a novel strategy (based on Proper\nGeneralized Decomposition) denoted by Separated Response Surface (SRS). A large\nnumber of uncertain input parameters may jeopardize the efficiency of the\nmetamodels. Thus, previous to define a metamodel, kernel Principal Component\nAnalysis (kPCA) is found to be effective to simplify the model outcome\ndescription. A benchmark crash test is used to show the efficiency of combining\nmetamodels with kPCA.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 16:59:39 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Rocas", "Marc", ""], ["Garc\u00eda-Gonz\u00e1lez", "Alberto", ""], ["Zlotnik", "Sergio", ""], ["Larr\u00e1yoz", "Xabier", ""], ["D\u00edez", "Pedro", ""]]}, {"id": "2102.07695", "submitter": "Aritra Guha", "authors": "Sunrit Chakraborty, Aritra Guha, Rayleigh Lei, XuanLong Nguyen", "title": "Scalable nonparametric Bayesian learning for heterogeneous and dynamic\n  velocity fields", "comments": "5 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of heterogeneous patterns in complex spatio-temporal data finds\nusage across various domains in applied science and engineering, including\ntraining autonomous vehicles to navigate in complex traffic scenarios.\nMotivated by applications arising in the transportation domain, in this paper\nwe develop a model for learning heterogeneous and dynamic patterns of velocity\nfield data. We draw from basic nonparameric Bayesian modeling elements such as\nhierarchical Dirichlet process and infinite hidden Markov model, while the\nsmoothness of each homogeneous velocity field element is captured with a\nGaussian process prior. Of particular focus is a scalable approximate inference\nmethod for the proposed model; this is achieved by employing sequential MAP\nestimates from the infinite HMM model and an efficient sequential GP posterior\ncomputation technique, which is shown to work effectively on simulated data\nsets. Finally, we demonstrate the effectiveness of our techniques to the NGSIM\ndataset of complex multi-vehicle interactions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 17:45:46 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Chakraborty", "Sunrit", ""], ["Guha", "Aritra", ""], ["Lei", "Rayleigh", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "2102.07752", "submitter": "Jalmar Manuel  Farfan Carrasco", "authors": "Lizandra Castilho Fabio, Cristian Villegas, Jalmar M. F. Carrasco,\n  M\\'ario de Castro", "title": "Diagnostic tools for a multivariate negative binomial model for fitting\n  correlated data with overdispersion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We focus on the development of diagnostic tools and an R package called MNB\nfor a multivariate negative binomial (MNB) regression model for detecting\natypical and influential subjects. The MNB model is deduced from a Poisson\nmixed model in which the random intercept follows the generalized log-gamma\n(GLG) distribution. The MNB model for correlated count data leads to an MNB\nregression model that inherits the features of a hierarchical model to\naccommodate the intraclass correlation and the occurrence of overdispersion\nsimultaneously. The asymptotic consistency of the dispersion parameter\nestimator depends on the asymmetry of the GLG distribution. Inferential\nprocedures for the MNB regression model are simple, although it can provide\ninconsistent estimates of the asymptotic variance when the correlation\nstructure is misspecified. We propose the randomized quantile residual for\nchecking the adequacy of the multivariate model, and derive global and local\ninfluence measures from the multivariate model to assess influential subjects.\nFinally, two applications are presented in the data analysis section. The code\nfor installing the MNB package and the code used in the two examples is\nexhibited in the Appendix.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:48:11 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Fabio", "Lizandra Castilho", ""], ["Villegas", "Cristian", ""], ["Carrasco", "Jalmar M. F.", ""], ["de Castro", "M\u00e1rio", ""]]}, {"id": "2102.07826", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama, Masaya Abe, Kei Nakagawa, Kenichiro McAlinn", "title": "Controlling False Discovery Rates under Cross-Sectional Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider controlling the false discovery rate for testing many time series\nwith an unknown cross-sectional correlation structure. Given a large number of\nhypotheses, false and missing discoveries can plague an analysis. While many\nprocedures have been proposed to control false discovery, most of them either\nassume independent hypotheses or lack statistical power. A problem of\nparticular interest is in financial asset pricing, where the goal is to\ndetermine which ``factors\" lead to excess returns out of a large number of\npotential factors. Our contribution is two-fold. First, we show the consistency\nof Fama and French's prominent method under multiple testing. Second, we\npropose a novel method for false discovery control using double bootstrapping.\nWe achieve superior statistical power to existing methods and prove that the\nfalse discovery rate is controlled. Simulations and a real data application\nillustrate the efficacy of our method over existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:07:17 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 04:53:42 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Komiyama", "Junpei", ""], ["Abe", "Masaya", ""], ["Nakagawa", "Kei", ""], ["McAlinn", "Kenichiro", ""]]}, {"id": "2102.07921", "submitter": "Raj Agrawal", "authors": "Raj Agrawal, Chandler Squires, Neha Prasad, Caroline Uhler", "title": "The DeCAMFounder: Non-Linear Causal Discovery in the Presence of Hidden\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real-world decision-making tasks require learning casual relationships\nbetween a set of variables. Typical causal discovery methods, however, require\nthat all variables are observed, which might not be realistic in practice.\nUnfortunately, in the presence of latent confounding, recovering casual\nrelationships from observational data without making additional assumptions is\nan ill-posed problem. Fortunately, in practice, additional structure among the\nconfounders can be expected, one such example being pervasive confounding,\nwhich has been exploited for consistent causal estimation in the special case\nof linear causal models. In this paper, we provide a proof and method to\nestimate causal relationships in the non-linear, pervasive confounding setting.\nThe heart of our procedure relies on the ability to estimate the pervasive\nconfounding variation through a simple spectral decomposition of the observed\ndata matrix. We derive a DAG score function based on this insight, and\nempirically compare our method to existing procedures. We show improved\nperformance on both simulated and real datasets by explicitly accounting for\nboth confounders and non-linear effects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 02:17:36 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Agrawal", "Raj", ""], ["Squires", "Chandler", ""], ["Prasad", "Neha", ""], ["Uhler", "Caroline", ""]]}, {"id": "2102.07967", "submitter": "Dhruv Medarametla", "authors": "Dhruv Medarametla, Emmanuel J. Cand\\`es", "title": "Distribution-Free Conditional Median Inference", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing confidence intervals for the median\nof a response $Y \\in \\mathbb{R}$ conditional on features $X = x \\in\n\\mathbb{R}^d$ in a situation where we are not willing to make any assumption\nwhatsoever on the underlying distribution of the data $(X,Y)$. We propose a\nmethod based upon ideas from conformal prediction and establish a theoretical\nguarantee of coverage while also going over particular distributions where its\nperformance is sharp. Further, we provide a lower bound on the length of any\npossible conditional median confidence interval. This lower bound is\nindependent of sample size and holds for all distributions with no point\nmasses.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 05:38:43 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Medarametla", "Dhruv", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2102.07992", "submitter": "Amiya Ranjan Bhowmick Dr.", "authors": "Md Aktar Ul Karim, Supriya Ramdas Bhagat and Amiya Ranjan Bhowmick", "title": "A New Method to Determine the Presence of Continuous Variation in\n  Parameters of Biological Growth Curve Models", "comments": "52 pages, 56 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative assessment of the growth of biological organisms has produced\nmany mathematical equations. Many efforts have been given on statistical\nidentification of the correct growth model from experimental data. Every growth\nequation is unique in terms of mathematical structures; however, one model may\nserve as a close approximation of the other by appropriate choice of the\nparameter(s). It is still a challenging problem to select the best estimating\nmodel from a set of model equations whose shapes are similar in nature. Our aim\nin this manuscript is to develop methodology that will reduce the efforts in\nmodel selection. This is achieved by utilizing an existing model selection\ncriterion in an innovative way that reduces the number of model fitting\nexercises substantially. In this manuscript, we have shown that one model can\nbe obtained from the other by choosing a suitable continuous transformation of\nthe parameters. This idea builds an interconnection between many equations\nwhich are scattered in the literature. We also get several new growth\nequations; out of them large number of equations can be obtained from a few key\nmodels. Given a set of training data points and the key models, we utilize the\nidea of interval specific rate parameter (ISRP) proposed by Bhowmick et al\n(2014) to obtain a suitable mathematical model for the data. The ISRP profile\nof the parameters of simpler models indicates the nature of variation in\nparameters with time, thus, enable the experimenter to extrapolate the\ninference to more complex models. Our proposed methodology significantly\nreduces the efforts involved in model fitting exercises. The proposed idea is\nverified by using simulated and real data sets. In addition, theoretical\njustifications have been provided by investigating the statistical properties\nof the estimators.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 07:39:35 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Karim", "Md Aktar Ul", ""], ["Bhagat", "Supriya Ramdas", ""], ["Bhowmick", "Amiya Ranjan", ""]]}, {"id": "2102.08114", "submitter": "Assaf Rabinowicz", "authors": "Assaf Rabinowicz and Saharon Rosset", "title": "Trees-Based Models for Correlated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new approach for trees-based regression, such as simple\nregression tree, random forest and gradient boosting, in settings involving\ncorrelated data. We show the problems that arise when implementing standard\ntrees-based regression models, which ignore the correlation structure. Our new\napproach explicitly takes the correlation structure into account in the\nsplitting criterion, stopping rules and fitted values in the leaves, which\ninduces some major modifications of standard methodology. The superiority of\nour new approach over trees-based models that do not account for the\ncorrelation is supported by simulation experiments and real data analyses.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 12:30:48 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Rabinowicz", "Assaf", ""], ["Rosset", "Saharon", ""]]}, {"id": "2102.08232", "submitter": "Mark de Rooij", "authors": "Mark de Rooij and Patrick J. F. Groenen", "title": "The MELODIC family for simultaneous binary logistic regression in a\n  reduced space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logistic regression is a commonly used method for binary classification.\nResearchers often have more than a single binary response variable and\nsimultaneous analysis is beneficial because it provides insight into the\ndependencies among response variables as well as between the predictor\nvariables and the responses. Moreover, in such a simultaneous analysis the\nequations can lend each other strength, which might increase predictive\naccuracy. In this paper, we propose the MELODIC family for simultaneous binary\nlogistic regression modeling. In this family, the regression models are defined\nin a Euclidean space of reduced dimension, based on a distance rule. The model\nmay be interpreted in terms of logistic regression coefficients or in terms of\na biplot. We discuss a fast iterative majorization (or MM) algorithm for\nparameter estimation. Two applications are shown in detail: one relating\npersonality characteristics to drug consumption profiles and one relating\npersonality characteristics to depressive and anxiety disorders. We present a\nthorough comparison of our MELODIC family with alternative approaches for\nmultivariate binary data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 15:47:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["de Rooij", "Mark", ""], ["Groenen", "Patrick J. F.", ""]]}, {"id": "2102.08255", "submitter": "Josph Feldman", "authors": "Joseph Feldman and Daniel Kowal", "title": "A Bayesian Framework for Generation of Fully Synthetic Mixed Datasets", "comments": "18 pages, 6 figures, submitted to The Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the micro data used for epidemiological studies contain sensitive\nmeasurements on real individuals. As a result, such micro data cannot be\npublished out of privacy concerns, rendering any published statistical analyses\non them nearly impossible to reproduce. To promote the dissemination of key\ndatasets for analysis without jeopardizing the privacy of individuals, we\nintroduce a cohesive Bayesian framework for the generation of fully synthetic,\nhigh dimensional micro datasets of mixed categorical, binary, count, and\ncontinuous variables. This process centers around a joint Bayesian model that\nis simultaneously compatible with all of these data types, enabling the\ncreation of mixed synthetic datasets through posterior predictive sampling.\nFurthermore, a focal point of epidemiological data analysis is the study of\nconditional relationships between various exposures and key outcome variables\nthrough regression analysis. We design a modified data synthesis strategy to\ntarget and preserve these conditional relationships, including both\nnonlinearities and interactions. The proposed techniques are deployed to create\na synthetic version of a confidential dataset containing dozens of health,\ncognitive, and social measurements on nearly 20,000 North Carolina children.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 16:27:29 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 13:43:36 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Feldman", "Joseph", ""], ["Kowal", "Daniel", ""]]}, {"id": "2102.08533", "submitter": "Aahlad Manas Puli", "authors": "Aahlad Puli, Adler J. Perotte, Rajesh Ranganath", "title": "Causal Estimation with Functional Confounders", "comments": "17 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference relies on two fundamental assumptions: ignorability and\npositivity. We study causal inference when the true confounder value can be\nexpressed as a function of the observed data; we call this setting estimation\nwith functional confounders (EFC). In this setting, ignorability is satisfied,\nhowever positivity is violated, and causal inference is impossible in general.\nWe consider two scenarios where causal effects are estimable. First, we discuss\ninterventions on a part of the treatment called functional interventions and a\nsufficient condition for effect estimation of these interventions called\nfunctional positivity. Second, we develop conditions for nonparametric effect\nestimation based on the gradient fields of the functional confounder and the\ntrue outcome function. To estimate effects under these conditions, we develop\nLevel-set Orthogonal Descent Estimation (LODE). Further, we prove error bounds\non LODE's effect estimates, evaluate our methods on simulated and real data,\nand empirically demonstrate the value of EFC.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 02:16:21 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Puli", "Aahlad", ""], ["Perotte", "Adler J.", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2102.08591", "submitter": "Anthony Christidis", "authors": "Anthony-Alexander Christidis, Stefan Van Aelst, Ruben Zamar", "title": "Data-Driven Diverse Logistic Regression Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel framework for statistical learning is introduced which combines ideas\nfrom regularization and ensembling. This framework is applied to learn an\nensemble of logistic regression models for high-dimensional binary\nclassification. In the new framework the models in the ensemble are learned\nsimultaneously by optimizing a multi-convex objective function. To enforce\ndiversity between the models the objective function penalizes overlap between\nthe models in the ensemble. Measures of diversity in classifier ensembles are\nused to show how our method learns the ensemble by exploiting the\naccuracy-diversity trade-off for ensemble models. In contrast to other\nensembling approaches, the resulting ensemble model is fully interpretable as a\nlogistic regression model, asymptotically consistent, and at the same time\nyields excellent prediction accuracy as demonstrated in an extensive simulation\nstudy and gene expression data applications. The models found by the proposed\nensemble methodology can also reveal alternative mechanisms that can explain\nthe relationship between the predictors and the response variable. An\nopen-source compiled software library implementing the proposed method is\nbriefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 05:57:26 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 05:58:10 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 20:00:37 GMT"}, {"version": "v4", "created": "Wed, 28 Jul 2021 04:12:02 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Christidis", "Anthony-Alexander", ""], ["Van Aelst", "Stefan", ""], ["Zamar", "Ruben", ""]]}, {"id": "2102.08782", "submitter": "Efstathia Bura", "authors": "Lukas Fertl and Efstathia Bura", "title": "Conditional Variance Estimator for Sufficient Dimension Reduction", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Variance Estimation (CVE) is a novel sufficient dimension\nreduction (SDR) method for additive error regressions with continuous\npredictors and link function. It operates under the assumption that the\npredictors can be replaced by a lower dimensional projection without loss of\ninformation. In contrast to the majority of moment based sufficient dimension\nreduction methods, Conditional Variance Estimation is fully data driven, does\nnot require the restrictive linearity and constant variance conditions, and is\nnot based on inverse regression. CVE is shown to be consistent and its\nobjective function to be uniformly convergent. CVE outperforms the mean average\nvariance estimation, (MAVE), its main competitor, in several simulation\nsettings, remains on par under others, while it always outperforms the usual\ninverse regression based linear SDR methods, such as Sliced Inverse Regression.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 14:17:54 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Fertl", "Lukas", ""], ["Bura", "Efstathia", ""]]}, {"id": "2102.08931", "submitter": "Roberto Viviani", "authors": "Roberto Viviani", "title": "Overcoming bias in representational similarity analysis", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Representational similarity analysis (RSA) is a multivariate technique to\ninvestigate cortical representations of objects or constructs. While avoiding\nill-posed matrix inversions that plague multivariate approaches in the presence\nof many outcome variables, it suffers from the confound arising from the\nnon-orthogonality of the design matrix. Here, a partial correlation approach\nwill be explored to adjust for this source of bias by partialling out this\nconfound. A formal analysis will show the dependence of this confound on the\ntemporal correlation model of the sequential observations, motivating a\ndata-driven approach that avoids the problem of misspecification of this model.\nHowever, where the autocorrelation locally diverges from the volume average,\nbias may be difficult to control for exactly (local bias), given the\ndifficulties of estimating the precise form of the confound at each voxel.\nApplication to real data shows the effectiveness of the partial correlation\napproach, suggesting the impact of local bias to be minor. However, where the\ncontrol for bias locally fails, possible spurious associations with the\nsimilarity matrix of the stimuli may emerge. This limitation may be intrinsic\nto RSA applied to non-orthogonal designs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 18:39:39 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Viviani", "Roberto", ""]]}, {"id": "2102.08975", "submitter": "Masahiro Kato", "authors": "Masahiro Kato", "title": "Adaptive Doubly Robust Estimator from Non-stationary Logging Policy\n  under a Convergence of Average Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive experiments, including efficient average treatment effect estimation\nand multi-armed bandit algorithms, have garnered attention in various\napplications, such as social experiments, clinical trials, and online\nadvertisement optimization. This paper considers estimating the mean outcome of\nan action from samples obtained in adaptive experiments. In causal inference,\nthe mean outcome of an action has a crucial role, and the estimation is an\nessential task, where the average treatment effect estimation and off-policy\nvalue estimation are its variants. In adaptive experiments, the probability of\nchoosing an action (logging policy) is allowed to be sequentially updated based\non past observations. Due to this logging policy depending on the past\nobservations, the samples are often not independent and identically distributed\n(i.i.d.), making developing an asymptotically normal estimator difficult. A\ntypical approach for this problem is to assume that the logging policy\nconverges in a time-invariant function. However, this assumption is restrictive\nin various applications, such as when the logging policy fluctuates or becomes\nzero at some periods. To mitigate this limitation, we propose another\nassumption that the average logging policy converges to a time-invariant\nfunction and show the doubly robust (DR) estimator's asymptotic normality.\nUnder the assumption, the logging policy itself can fluctuate or be zero for\nsome actions. We also show the empirical properties by simulations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:05:53 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 13:41:07 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kato", "Masahiro", ""]]}, {"id": "2102.08994", "submitter": "Martin Spindler", "authors": "Barbara Felderer, Jannis Kueck, Martin Spindler", "title": "Big Data meets Causal Survey Research: Understanding Nonresponse in the\n  Recruitment of a Mixed-mode Online Panel", "comments": "33 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survey scientists increasingly face the problem of high-dimensionality in\ntheir research as digitization makes it much easier to construct\nhigh-dimensional (or \"big\") data sets through tools such as online surveys and\nmobile applications. Machine learning methods are able to handle such data, and\nthey have been successfully applied to solve \\emph{predictive} problems.\nHowever, in many situations, survey statisticians want to learn about\n\\emph{causal} relationships to draw conclusions and be able to transfer the\nfindings of one survey to another. Standard machine learning methods provide\nbiased estimates of such relationships. We introduce into survey statistics the\ndouble machine learning approach, which gives approximately unbiased estimators\nof causal parameters, and show how it can be used to analyze survey nonresponse\nin a high-dimensional panel setting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:37:53 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Felderer", "Barbara", ""], ["Kueck", "Jannis", ""], ["Spindler", "Martin", ""]]}, {"id": "2102.09008", "submitter": "Suchit Mehrotra", "authors": "Suchit Mehrotra, Halley Brantley, Peter Onglao, Patricia Bata, Roland\n  Romero, Jacob Westman, Lauren Bangerter, Arnab Maity", "title": "Divide-and-Conquer MCMC for Multivariate Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of large scale medical claims data has the potential to improve\nquality of care by generating insights which can be used to create tailored\nmedical programs. In particular, the multivariate probit model can be used to\ninvestigate the correlation between multiple binary responses of interest in\nsuch data, e.g. the presence of multiple chronic conditions. Bayesian modeling\nis well suited to such analyses because of the automatic uncertainty\nquantification provided by the posterior distribution. A complicating factor is\nthat large medical claims datasets often do not fit in memory, which renders\nthe estimation of the posterior using traditional Markov Chain Monte Carlo\n(MCMC) methods computationally infeasible. To address this challenge, we extend\nexisting divide-and-conquer MCMC algorithms to the multivariate probit model,\ndemonstrating, via simulation, that they should be preferred over mean-field\nvariational inference when the estimation of the latent correlation structure\nbetween binary responses is of primary interest. We apply this algorithm to a\nlarge database of de-identified Medicare Advantage claims from a single large\nUS health insurance provider, where we find medically meaningful groupings of\ncommon chronic conditions and asses the impact of the urban-rural health gap by\nidentifying underutilized provider specialties in rural areas.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 20:02:17 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 20:36:37 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 17:19:34 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Mehrotra", "Suchit", ""], ["Brantley", "Halley", ""], ["Onglao", "Peter", ""], ["Bata", "Patricia", ""], ["Romero", "Roland", ""], ["Westman", "Jacob", ""], ["Bangerter", "Lauren", ""], ["Maity", "Arnab", ""]]}, {"id": "2102.09052", "submitter": "Eli Ben-Michael", "authors": "Eli Ben-Michael, Avi Feller, Erin Hartman", "title": "Multilevel calibration weighting for survey data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A pressing challenge in modern survey research is to find calibration weights\nwhen covariates are high dimensional and especially when interactions between\nvariables are important. Traditional approaches like raking typically fail to\nbalance higher-order interactions; and post-stratification, which exactly\nbalances all interactions, is only feasible for a small number of variables. In\nthis paper, we propose multilevel calibration weighting, which enforces tight\nbalance constraints for marginal balance and looser constraints for\nhigher-order interactions. This incorporates some of the benefits of\npost-stratification while retaining the guarantees of raking. We then correct\nfor the bias due to the relaxed constraints via a flexible outcome model; we\ncall this approach Double Regression with Post-stratification (DRP). We\ncharacterize the asymptotic properties of these estimators and show that the\nproposed calibration approach has a dual representation as a multilevel model\nfor survey response. We assess the performance of this method via an extensive\nsimulation study and show how it can reduce bias in a case-study of a\nlarge-scale survey of voter intention in the 2016 U.S. presidential election.\nThe approach is available in the multical R package.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 22:18:07 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ben-Michael", "Eli", ""], ["Feller", "Avi", ""], ["Hartman", "Erin", ""]]}, {"id": "2102.09053", "submitter": "X. Jessie Jeng", "authors": "X. Jessie Jeng", "title": "Estimating The Proportion of Signal Variables Under Arbitrary Covariance\n  Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the proportion of signals hidden in a large amount of noise\nvariables is of interest in many scientific inquires. In this paper, we\nconsider realistic but theoretically challenging settings with arbitrary\ncovariance dependence between variables. We define mean absolute correlation\n(MAC) to measure the overall dependence level and investigate a family of\nestimators for their performances in the full range of MAC. We explicit the\njoint effect of MAC dependence and signal sparsity on the performances of the\nfamily of estimators and discover that no single estimator in the family is\nmost powerful under different MAC dependence levels. Informed by the\ntheoretical insight, we propose a new estimator to better adapt to arbitrary\ncovariance dependence. The proposed method compares favorably to several\nexisting methods in extensive finite-sample settings with strong to weak\ncovariance dependence and real dependence structures from genetic association\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 22:20:13 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 16:45:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Jeng", "X. Jessie", ""]]}, {"id": "2102.09071", "submitter": "Daniel Mork", "authors": "Daniel Mork and Ander Wilson", "title": "Estimating Perinatal Critical Windows of Susceptibility to Environmental\n  Mixtures via Structured Bayesian Regression Tree Pairs", "comments": "22 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maternal exposure to environmental chemicals during pregnancy can alter birth\nand children's health outcomes. Research seeks to identify critical windows,\ntime periods when the exposures can change future health outcomes, and estimate\nthe exposure-response relationship. Existing statistical approaches focus on\nestimation of the association between maternal exposure to a single\nenvironmental chemical observed at high-temporal resolution, such as weekly\nthroughout pregnancy, and children's health outcomes. Extending to multiple\nchemicals observed at high temporal resolution poses a dimensionality problem\nand statistical methods are lacking. We propose a tree-based model for mixtures\nof exposures that are observed at high temporal resolution. The proposed\napproach uses an additive ensemble of structured tree-pairs that define\nstructured main effects and interactions between time-resolved predictors and\nvariable selection to select out of the model predictors not correlated with\nthe outcome. We apply our method in a simulation and the analysis of the\nrelationship between five exposures measured weekly throughout pregnancy and\nresulting birth weight in a Denver, Colorado birth cohort. We identified\ncritical windows during which fine particulate matter, sulfur dioxide, and\ntemperature are negatively associated with birth weight and an interaction\nbetween fine particulate matter and temperature. Software is made available in\nthe R package dlmtree.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 23:28:37 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 15:28:53 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 18:49:04 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Mork", "Daniel", ""], ["Wilson", "Ander", ""]]}, {"id": "2102.09080", "submitter": "Cheng Yong Tang", "authors": "Sanat K. Sarkar and Cheng Yong Tang", "title": "Adjusting the Benjamini-Hochberg method for controlling the false\n  discovery rate in knockoff assisted variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the knockoff-based multiple testing setup considered in\nBarber & Candes (2015) for variable selection applied to a linear regression\nmodel with $n\\ge 2d$, where $n$ is the sample size and $d$ is the number of\nexplanatory variables. The BH method based on ordinary least squares estimates\nof the regressions coefficients is adjusted to this setup, making it a valid\n$p$-value based FDR controlling method that does not rely on any specific\ncorrelation structure of the explanatory variables. Simulations and real data\napplications demonstrate that our proposed method in its original form and its\ndata-adaptive version incorporating estimated proportion of truly unimportant\nexplanatory variables are powerful competitors of the FDR controlling methods\nin Barber & Candes (2015).\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 00:13:24 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Sarkar", "Sanat K.", ""], ["Tang", "Cheng Yong", ""]]}, {"id": "2102.09232", "submitter": "Wei-Ting Lai", "authors": "Wei-Ting Lai, Ray-Bing Chen, Ying Chen, Thorsten Koch", "title": "The Variational Bayesian Inference for Network Autoregression Models", "comments": "40 pages, 38 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a variational Bayesian (VB) approach for estimating large-scale\ndynamic network models in the network autoregression framework. The VB approach\nallows for the automatic identification of the dynamic structure of such a\nmodel and obtains a direct approximation of the posterior density. Compared to\nMarkov Chain Monte Carlo (MCMC) based sampling approaches, the VB approach\nachieves enhanced computational efficiency without sacrificing estimation\naccuracy. In the simulation study conducted here, the proposed VB approach\ndetects various types of proper active structures for dynamic network models.\nCompared to the alternative approach, the proposed method achieves similar or\nbetter accuracy, and its computational time is halved. In a real data analysis\nscenario of day-ahead natural gas flow prediction in the German gas\ntransmission network with 51 nodes between October 2013 and September 2015, the\nVB approach delivers promising forecasting accuracy along with clearly detected\nstructures in terms of dynamic dependence.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 09:26:39 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Lai", "Wei-Ting", ""], ["Chen", "Ray-Bing", ""], ["Chen", "Ying", ""], ["Koch", "Thorsten", ""]]}, {"id": "2102.09248", "submitter": "Boyao Zhang", "authors": "Boyao Zhang, Tobias Hepp, Sonja Greven, Elisabeth Bergherr", "title": "Adaptive Step-Length Selection in Gradient Boosting for Generalized\n  Additive Models for Location, Scale and Shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning of model-based boosting algorithms relies mainly on the number of\niterations, while the step-length is fixed at a predefined value. For complex\nmodels with several predictors such as Generalized Additive Models for\nLocation, Scale and Shape (GAMLSS), imbalanced updates of predictors, where\nsome distribution parameters are updated more frequently than others, can be a\nproblem that prevents some submodels to be appropriately fitted within a\nlimited number of boosting iterations. We propose an approach using adaptive\nstep-length (ASL) determination within a non-cyclical boosting algorithm for\nGAMLSS to prevent such imbalance. Moreover, for the important special case of\nthe Gaussian distribution, we discuss properties of the ASL and derive a\nsemi-analytical form of the ASL that avoids manual selection of the search\ninterval and numerical optimization to find the optimal step-length, and\nconsequently improves computational efficiency. We show competitive behavior of\nthe proposed approaches compared to penalized maximum likelihood and boosting\nwith a fixed step-length for GAMLSS models in two simulations and two\napplications, in particular for cases of large variance and/or more variables\nthan observations. In addition, the idea of the ASL is also applicable to other\nmodels with more than one predictor like zero-inflated count model, and brings\nup insights into the choice of the reasonable defaults for the step-length in\nsimpler special case of (Gaussian) additive models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 10:06:43 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Zhang", "Boyao", ""], ["Hepp", "Tobias", ""], ["Greven", "Sonja", ""], ["Bergherr", "Elisabeth", ""]]}, {"id": "2102.09412", "submitter": "Alexander Franks", "authors": "Jiajing Zheng, Alexander D'Amour, and Alexander Franks", "title": "Copula-based Sensitivity Analysis for Multi-Treatment Causal Inference\n  with Unobserved Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has focused on the potential and pitfalls of causal\nidentification in observational studies with multiple simultaneous treatments.\nOn the one hand, a latent variable model fit to the observed treatments can\nidentify essential aspects of the distribution of unobserved confounders. On\nthe other hand, it has been shown that even when the latent confounder\ndistribution is known exactly, causal effects are still not point identifiable.\nThus, the practical benefits of latent variable modeling in multi-treatment\nsettings remain unclear. We clarify these issues with a sensitivity analysis\nmethod that can be used to characterize the range of causal effects that are\ncompatible with the observed data. Our method is based on a copula\nfactorization of the joint distribution of outcomes, treatments, and\nconfounders, and can be layered on top of arbitrary observed data models. We\npropose a practical implementation of this approach making use of the Gaussian\ncopula, and establish conditions under which causal effects can be bounded. We\nalso describe approaches for reasoning about effects, including calibrating\nsensitivity parameters, quantifying robustness of effect estimates, and\nselecting models which are most consistent with prior hypotheses.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:01:18 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Zheng", "Jiajing", ""], ["D'Amour", "Alexander", ""], ["Franks", "Alexander", ""]]}, {"id": "2102.09448", "submitter": "Xinwei Deng", "authors": "Xiaoning Kang, Lulu Kang, Wei Chen and Xinwei Deng", "title": "A Generative Approach to Joint Modeling of Quantitative and Qualitative\n  Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many scientific areas, data with quantitative and qualitative (QQ)\nresponses are commonly encountered with a large number of predictors. By\nexploring the association between QQ responses, existing approaches often\nconsider a joint model of QQ responses given the predictor variables. However,\nthe dependency among predictive variables also provides useful information for\nmodeling QQ responses. In this work, we propose a generative approach to model\nthe joint distribution of the QQ responses and predictors. The proposed\ngenerative model provides efficient parameter estimation under a penalized\nlikelihood framework. It achieves accurate classification for qualitative\nresponse and accurate prediction for quantitative response with efficient\ncomputation. Because of the generative approach framework, the asymptotic\noptimality of classification and prediction of the proposed method can be\nestablished under some regularity conditions. The performance of the proposed\nmethod is examined through simulations and real case studies in material\nscience and genetics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:10:45 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Kang", "Xiaoning", ""], ["Kang", "Lulu", ""], ["Chen", "Wei", ""], ["Deng", "Xinwei", ""]]}, {"id": "2102.09612", "submitter": "Ka Kin Lam", "authors": "Ka Kin Lam, Bo Wang", "title": "Multipopulation mortality modelling and forecasting: The multivariate\n  functional principal component with time weightings approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human mortality patterns and trajectories in closely related populations are\nlikely linked together and share similarities. It is always desirable to model\nthem simultaneously while taking their heterogeneity into account. This paper\nintroduces two new models for joint mortality modelling and forecasting\nmultiple subpopulations in adaptations of the multivariate functional principal\ncomponent analysis techniques. The first model extends the independent\nfunctional data model to a multi-population modelling setting. In the second\none, we propose a novel multivariate functional principal component method for\ncoherent modelling. Its design primarily fulfils the idea that when several\nsubpopulation groups have similar socio-economic conditions or common\nbiological characteristics, such close connections are expected to evolve in a\nnon-diverging fashion. We demonstrate the proposed methods by using\nsex-specific mortality data. Their forecast performances are further compared\nwith several existing models, including the independent functional data model\nand the Product-Ratio model, through comparisons with mortality data of ten\ndeveloped countries. Our experiment results show that the first proposed model\nmaintains a comparable forecast ability with the existing methods. In contrast,\nthe second proposed model outperforms the first model as well as the current\nmodels in terms of forecast accuracy, in addition to several desirable\nproperties.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 21:01:58 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Lam", "Ka Kin", ""], ["Wang", "Bo", ""]]}, {"id": "2102.09650", "submitter": "Anna Kutschireiter", "authors": "Anna Kutschireiter, Luke Rast, Jan Drugowitsch", "title": "Angular Path Integration by Projection Filtering with Increment\n  Observations", "comments": "14 pages, 5 figures, submitted to IEEE Transactions on Automatic\n  Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Angular path integration is the ability of a system to estimate its own\nheading direction from potentially noisy angular velocity (or increment)\nobservations. Despite its importance for robot and animal navigation, current\nalgorithms for angular path integration lack probabilistic descriptions that\ntake into account the reliability of such observations, which is essential for\nappropriately weighing one's current heading direction estimate against\nincoming information. In a probabilistic setting, angular path integration can\nbe formulated as a continuous-time nonlinear filtering problem (circular\nfiltering) with increment observations. The circular symmetry of heading\ndirection makes this inference task inherently nonlinear, thereby precluding\nthe use of popular inference algorithms such as Kalman filters and rendering\nthe problem analytically inaccessible. Here, we derive an approximate solution\nto circular continuous-time filtering, which integrates increment observations\nwhile maintaining a fixed representation through both state propagation and\nobservational updates. Specifically, we extend the established\nprojection-filtering method to account for increment observations and apply\nthis framework to the circular filtering problem. We further propose a\ngenerative model for continuous-time angular-valued direct observations of the\nhidden state, which we integrate seamlessly into the projection filter.\nApplying the resulting scheme to a model of probabilistic angular path\nintegration, we derive an algorithm for circular filtering, which we term the\ncircular Kalman filter. Importantly, this algorithm is analytically accessible,\ninterpretable, and outperforms an alternative filter based on a Gaussian\napproximation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 22:35:45 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Kutschireiter", "Anna", ""], ["Rast", "Luke", ""], ["Drugowitsch", "Jan", ""]]}, {"id": "2102.09676", "submitter": "Ka Kin Lam", "authors": "Ka Kin Lam, Bo Wang", "title": "Robust non-parametric mortality and fertility modelling and forecasting:\n  Gaussian process regression approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A rapid decline in mortality and fertility has become major issues in many\ndeveloped countries over the past few decades. A precise model for forecasting\ndemographic movements is important for decision making in social welfare\npolicies and resource budgeting among the government and many industry sectors.\nThis article introduces a novel non-parametric approach using Gaussian process\nregression with a natural cubic spline mean function and a spectral mixture\ncovariance function for mortality and fertility modelling and forecasting.\nUnlike most of the existing approaches in demographic modelling literature,\nwhich rely on time parameters to decide the movements of the whole mortality or\nfertility curve shifting from one year to another over time, we consider the\nmortality and fertility curves from their components of all age-specific\nmortality and fertility rates and assume each of them following a Gaussian\nprocess over time to fit the whole curves in a discrete but intensive style.\nThe proposed Gaussian process regression approach shows significant\nimprovements in terms of preciseness and robustness compared to other\nmainstream demographic modelling approaches in the short-, mid- and long-term\nforecasting using the mortality and fertility data of several developed\ncountries in our numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:49:25 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Lam", "Ka Kin", ""], ["Wang", "Bo", ""]]}, {"id": "2102.09705", "submitter": "Brian Trippe", "authors": "Brian L. Trippe, Sameer K. Deshpande, Tamara Broderick", "title": "Confidently Comparing Estimators with the c-value", "comments": "23 pages plus supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern statistics provides an ever-expanding toolkit for estimating unknown\nparameters. Consequently, applied statisticians frequently face a difficult\ndecision: retain a parameter estimate from a familiar method or replace it with\nan estimate from a newer or complex one. While it is traditional to compare\nestimators using risk, such comparisons are rarely conclusive in realistic\nsettings. In response, we propose the \"c-value\" as a measure of confidence that\na new estimate achieves smaller loss than an old estimate on a given dataset.\nWe show that it is unlikely that a computed c-value is large and that the new\nestimate has larger loss than the old. Therefore, just as a small p-value\nprovides evidence to reject a null hypothesis, a large c-value provides\nevidence to use a new estimate in place of the old. For a wide class of\nproblems and estimators, we show how to compute a c-value by first constructing\na data-dependent high-probability lower bound on the difference in loss. The\nc-value is frequentist in nature, but we show that it can provide a validation\nof Bayesian estimates in real data applications involving hierarchical models\nand Gaussian processes.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 01:47:06 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Trippe", "Brian L.", ""], ["Deshpande", "Sameer K.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2102.09715", "submitter": "Philippe Boileau", "authors": "Philippe Boileau (1), Nima S. Hejazi (1 and 2), Mark J. van der Laan\n  (3, 4 and 2), Sandrine Dudoit (4, 3 and 2) ((1) Graduate Group in\n  Biostatistics, University of California, Berkeley, (2) Center for\n  Computational Biology, University of California, Berkeley, (3) Division of\n  Biostatistics, University of California, Berkeley, (4) Department of\n  Statistics, University of California, Berkeley)", "title": "Cross-Validated Loss-Based Covariance Matrix Estimator Selection in High\n  Dimensions", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The covariance matrix plays a fundamental role in many modern exploratory and\ninferential statistical procedures, including dimensionality reduction,\nhypothesis testing, and regression. In low-dimensional regimes, where the\nnumber of observations far exceeds the number of variables, the optimality of\nthe sample covariance matrix as an estimator of this parameter is\nwell-established. High-dimensional regimes do not admit such a convenience,\nhowever. As such, a variety of estimators have been derived to overcome the\nshortcomings of the sample covariance matrix in these settings. Yet, the\nquestion of selecting an optimal estimator from among the plethora available\nremains largely unaddressed. Using the framework of cross-validated loss-based\nestimation, we develop the theoretical underpinnings of just such an estimator\nselection procedure. In particular, we propose a general class of loss\nfunctions for covariance matrix estimation and establish finite-sample risk\nbounds and conditions for the asymptotic optimality of the cross-validated\nestimator selector with respect to these loss functions. We evaluate our\nproposed approach via a comprehensive set of simulation experiments and\ndemonstrate its practical benefits by application in the exploratory analysis\nof two single-cell transcriptome sequencing datasets. A free and open-source\nsoftware implementation of the proposed methodology, the cvCovEst R package, is\nbriefly introduced.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 02:42:23 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Boileau", "Philippe", "", "1 and 2"], ["Hejazi", "Nima S.", "", "1 and 2"], ["van der Laan", "Mark J.", "", "3, 4 and 2"], ["Dudoit", "Sandrine", "", "4, 3 and 2"]]}, {"id": "2102.09912", "submitter": "Jan O. Bauer", "authors": "Jan O. Bauer", "title": "Correlation Based Principal Loading Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Principal loading analysis is a dimension reduction method that discards\nvariables which have only a small distorting effect on the covariance matrix.\nWe complement principal loading analysis and propose to rather use a mix of\nboth, the correlation and covariance matrix instead. Further, we suggest to use\nrescaled eigenvectors and provide updated algorithms for all proposed changes.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 13:08:30 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Bauer", "Jan O.", ""]]}, {"id": "2102.09948", "submitter": "Soichiro Yamauchi", "authors": "Naoki Egami, Soichiro Yamauchi", "title": "Using Multiple Pre-treatment Periods to Improve\n  Difference-in-Differences and Staggered Adoption Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While difference-in-differences (DID) was originally developed with one pre-\nand one post-treatment periods, data from additional pre-treatment periods is\noften available. How can researchers improve the DID design with such multiple\npre-treatment periods under what conditions? We first use potential outcomes to\nclarify three benefits of multiple pre-treatment periods: (1) assessing the\nparallel trends assumption, (2) improving estimation accuracy, and (3) allowing\nfor a more flexible parallel trends assumption. We then propose a new\nestimator, double DID, which combines all the benefits through the generalized\nmethod of moments and contains the two-way fixed effects regression as a\nspecial case. In a wide range of applications where several pre-treatment\nperiods are available, the double DID improves upon the standard DID both in\nterms of identification and estimation accuracy. We also generalize the double\nDID to the staggered adoption design where different units can receive the\ntreatment in different time periods. We illustrate the proposed method with two\nempirical applications, covering both the basic DID and staggered adoption\ndesigns. We offer an open-source R package that implements the proposed\nmethodologies.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:28:40 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Egami", "Naoki", ""], ["Yamauchi", "Soichiro", ""]]}, {"id": "2102.09964", "submitter": "Adrien Corenflos", "authors": "Adrien Corenflos, Zheng Zhao, Simo S\\\"arkk\\\"a", "title": "Temporal Gaussian Process Regression in Logarithmic Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this article is to present a novel parallelization method for\ntemporal Gaussian process (GP) regression problems. The method allows for\nsolving GP regression problems in logarithmic O(log N) time, where N is the\nnumber of time steps. Our approach uses the state-space representation of GPs\nwhich in its original form allows for linear O(N) time GP regression by\nleveraging the Kalman filtering and smoothing methods. By using a recently\nproposed parallelization method for Bayesian filters and smoothers, we are able\nto reduce the linear computational complexity of the temporal GP regression\nproblems into logarithmic span complexity. This ensures logarithmic time\ncomplexity when run on parallel hardware such as a graphics processing unit\n(GPU). We experimentally demonstrate the computational benefits on simulated\nand real datasets via our open-source implementation leveraging the GPflow\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 14:57:17 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 07:30:40 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 19:36:16 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 07:23:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Corenflos", "Adrien", ""], ["Zhao", "Zheng", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2102.10003", "submitter": "Yuxiang Gao", "authors": "Yuxiang Gao, Lauren Kennedy, Daniel Simpson", "title": "Treatment effect estimation with Multilevel Regression and\n  Poststratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilevel regression and poststratification (MRP) is a flexible modeling\ntechnique that has been used in a broad range of small-area estimation\nproblems. Traditionally, MRP studies have been focused on non-causal settings,\nwhere estimating a single population value using a nonrepresentative sample was\nof primary interest. In this manuscript, MRP-style estimators will be evaluated\nin an experimental causal inference setting. We simulate a large-scale\nrandomized control trial with a stratified cluster sampling design, and compare\ntraditional and nonparametric treatment effect estimation methods with MRP\nmethodology. Using MRP-style estimators, treatment effect estimates for areas\nas small as 1.3$\\%$ of the population have lower bias and variance than\nstandard causal inference methods, even in the presence of treatment effect\nheterogeneity. The design of our simulation studies also requires us to build\nupon a MRP variant that allows for non-census covariates to be incorporated\ninto poststratification.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 16:14:10 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Gao", "Yuxiang", ""], ["Kennedy", "Lauren", ""], ["Simpson", "Daniel", ""]]}, {"id": "2102.10020", "submitter": "Herbert Susmann", "authors": "Herbert Susmann, Monica Alexander, Leontine Alkema", "title": "Temporal models for demographic and global health outcomes in multiple\n  populations: Introducing a new framework to review and standardize\n  documentation of model assumptions and facilitate model comparison", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is growing interest in producing estimates of demographic and global\nhealth indicators in populations with limited data. Statistical models are\nneeded to combine data from multiple data sources into estimates and\nprojections with uncertainty. Diverse modeling approaches have been applied to\nthis problem, making comparisons between models difficult. We propose a model\nclass, Temporal Models for Multiple Populations (TMMPs), to facilitate\ndocumentation of model assumptions in a standardized way and comparison across\nmodels. The class distinguishes between latent trends and the observed data,\nwhich may be noisy or exhibit systematic biases. We provide general\nformulations of the process model, which describes the latent trend of the\nindicator of interest. We show how existing models for a variety of indicators\ncan be written as TMMPs and how the TMMP-based description can be used to\ncompare and contrast model assumptions. We end with a discussion of outstanding\nquestions and future directions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 16:42:45 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Susmann", "Herbert", ""], ["Alexander", "Monica", ""], ["Alkema", "Leontine", ""]]}, {"id": "2102.10039", "submitter": "Miao-jung Ou", "authors": "Chuan Bi, Miao-jung Yvonne Ou, Wenshu Qian, Kenneth W. Fishbein,\n  Mustapha Bouhrara, Richard G. Spencer", "title": "Multi-Regularization Reconstruction of One-Dimensional $T_2$\n  Distributions in Magnetic Resonance Relaxometry with a Gaussian Basis", "comments": "18 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the inverse problem of recovering the probability distribution\nfunction of $T_2$ relaxation times from NMR transverse relaxometry experiments.\nThis problem is a variant of the inverse Laplace transform and hence ill-posed.\nWe cast this within the framework of a Gaussian mixture model to obtain a\nleast-square problem with an $L_2$ regularization term. We propose a new method\nfor incorporating regularization into the solution; rather than seeking to\nreplace the native problem with a suitable mathematically close, regularized,\nversion, we instead augment the native formulation with regularization. We term\nthis new approach 'multi-regularization'; it avoids the treacherous process of\nselecting a single best regularization parameter $\\lambda$ and instead permits\nincorporation of several degrees of regularization into the solution. We\nillustrate the method with extensive simulation results as well as application\nto real experimental data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:17:40 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 22:01:16 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Bi", "Chuan", ""], ["Ou", "Miao-jung Yvonne", ""], ["Qian", "Wenshu", ""], ["Fishbein", "Kenneth W.", ""], ["Bouhrara", "Mustapha", ""], ["Spencer", "Richard G.", ""]]}, {"id": "2102.10048", "submitter": "Martin Magris", "authors": "Magris Martin, Iosifidis Alexandros", "title": "Approximate Bayes factors for unit root testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a feasible and practical Bayesian method for unit root\ntesting in financial time series. We propose a convenient approximation of the\nBayes factor in terms of the Bayesian Information Criterion as a\nstraightforward and effective strategy for testing the unit root hypothesis.\nOur approximate approach relies on few assumptions, is of general\napplicability, and preserves a satisfactory error rate. Among its advantages,\nit does not require the prior distribution on model's parameters to be\nspecified. Our simulation study and empirical application on real exchange\nrates show great accordance between the suggested simple approach and both\nBayesian and non-Bayesian alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 17:31:21 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 12:49:42 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Martin", "Magris", ""], ["Alexandros", "Iosifidis", ""]]}, {"id": "2102.10080", "submitter": "Guang Cheng", "authors": "Yang Yu, Shih-Kang Chao, Guang Cheng", "title": "Distributed Bootstrap for Simultaneous Inference Under High\n  Dimensionality", "comments": "arXiv admin note: text overlap with arXiv:2002.08443", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a distributed bootstrap method for simultaneous inference on\nhigh-dimensional massive data that are stored and processed with many machines.\nThe method produces a $\\ell_\\infty$-norm confidence region based on a\ncommunication-efficient de-biased lasso, and we propose an efficient\ncross-validation approach to tune the method at every iteration. We\ntheoretically prove a lower bound on the number of communication rounds\n$\\tau_{\\min}$ that warrants the statistical accuracy and efficiency.\nFurthermore, $\\tau_{\\min}$ only increases logarithmically with the number of\nworkers and intrinsic dimensionality, while nearly invariant to the nominal\ndimensionality. We test our theory by extensive simulation studies, and a\nvariable screening task on a semi-synthetic dataset based on the US Airline\nOn-time Performance dataset. The code to reproduce the numerical results is\navailable at GitHub: https://github.com/skchao74/Distributed-bootstrap.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:28:29 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Yu", "Yang", ""], ["Chao", "Shih-Kang", ""], ["Cheng", "Guang", ""]]}, {"id": "2102.10154", "submitter": "Chudamani Poudyal", "authors": "Chudamani Poudyal", "title": "Truncated, Censored, and Actuarial Payment-type Moments for Robust\n  Fitting of a Single-parameter Pareto Distribution", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics, 388 (2021),\n  113310, 18", "doi": "10.1016/j.cam.2020.113310", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With some regularity conditions maximum likelihood estimators (MLEs) always\nproduce asymptotically optimal (in the sense of consistency, efficiency,\nsufficiency, and unbiasedness) estimators. But in general, the MLEs lead to\nnon-robust statistical inference, for example, pricing models and risk\nmeasures. Actuarial claim severity is continuous, right-skewed, and frequently\nheavy-tailed. The data sets that such models are usually fitted to contain\noutliers that are difficult to identify and separate from genuine data.\nMoreover, due to commonly used actuarial \"loss control strategies\" in financial\nand insurance industries, the random variables we observe and wish to model are\naffected by truncation (due to deductibles), censoring (due to policy limits),\nscaling (due to coinsurance proportions) and other transformations. To\nalleviate the lack of robustness of MLE-based inference in risk modeling, here\nin this paper, we propose and develop a new method of estimation - method of\ntruncated moments (MTuM) and generalize it for different scenarios of loss\ncontrol mechanism. Various asymptotic properties of those estimates are\nestablished by using central limit theory. New connections between different\nestimators are found. A comparative study of newly-designed methods with the\ncorresponding MLEs is performed. Detail investigation has been done for a\nsingle parameter Pareto loss model including a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 20:50:23 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Poudyal", "Chudamani", ""]]}, {"id": "2102.10186", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus, Menggang Yu and Jin Xu", "title": "Studentized Permutation Method for Comparing Restricted Mean Survival\n  Times with Small Sample from Randomized Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent observations, especially in cancer immunotherapy clinical trials with\ntime-to-event outcomes, show that the commonly used proportial hazard\nassumption is often not justifiable, hampering an appropriate analyse of the\ndata by hazard ratios. An attractive alternative advocated is given by the\nrestricted mean survival time (RMST), which does not rely on any model\nassumption and can always be interpreted intuitively. As pointed out recently\nby Horiguchi and Uno (2020), methods for the RMST based on asymptotic theory\nsuffer from inflated type-I error under small sample sizes. To overcome this\nproblem, they suggested a permutation strategy leading to more convincing\nresults in simulations. However, their proposal requires an exchangeable data\nset-up between comparison groups which may be limiting in practice. In\naddition, it is not possible to invert their testing procedure to obtain valid\nconfidence intervals, which can provide more in-depth information. In this\npaper, we address these limitations by proposing a studentized permutation test\nas well as the corresponding permutation-based confidence intervals. In our\nextensive simulation study, we demonstrate the advantage of our new method,\nespecially in situations with relative small sample sizes and unbalanced\ngroups. Finally we illustrate the application of the proposed method by\nre-analysing data from a recent lung cancer clinical trial.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:33:07 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Yu", "Menggang", ""], ["Xu", "Jin", ""]]}, {"id": "2102.10237", "submitter": "Evan Rosenman", "authors": "Evan Rosenman, Art B. Owen", "title": "Designing Experiments Informed by Observational Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of passively observed data has yielded a growing\nmethodological interest in \"data fusion.\" These methods involve merging data\nfrom observational and experimental sources to draw causal conclusions -- and\nthey typically require a precarious tradeoff between the unknown bias in the\nobservational dataset and the often-large variance in the experimental dataset.\nWe propose an alternative approach to leveraging observational data, which\navoids this tradeoff: rather than using observational data for inference, we\nuse it to design a more efficient experiment. We consider the case of a\nstratified experiment with a binary outcome, and suppose pilot estimates for\nthe stratum potential outcome variances can be obtained from the observational\nstudy. We extend results from Zhao et al. (2019) in order to generate\nconfidence sets for these variances, while accounting for the possibility of\nunmeasured confounding. Then, we pose the experimental design problem as one of\nregret minimization, subject to the constraints imposed by our confidence sets.\nWe show that this problem can be converted into a convex minimization and\nsolved using conventional methods. Lastly, we demonstrate the practical utility\nof our methods using data from the Women's Health Initiative.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 02:55:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rosenman", "Evan", ""], ["Owen", "Art B.", ""]]}, {"id": "2102.10263", "submitter": "Hayden Helm", "authors": "Hayden S. Helm, Weiwei Yang, Sujeeth Bharadwaj, Kate Lytvynets, Oriana\n  Riva, Christopher White, Ali Geisa, Carey E. Priebe", "title": "Inducing a hierarchy for multi-class classification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications where categorical labels follow a natural hierarchy,\nclassification methods that exploit the label structure often outperform those\nthat do not. Un-fortunately, the majority of classification datasets do not\ncome pre-equipped with a hierarchical structure and classical flat classifiers\nmust be employed. In this paper, we investigate a class of methods that induce\na hierarchy that can similarly improve classification performance over flat\nclassifiers. The class of methods follows the structure of first clustering the\nconditional distributions and subsequently using a hierarchical classifier with\nthe induced hierarchy. We demonstrate the effectiveness of the class of methods\nboth for discovering a latent hierarchy and for improving accuracy in\nprincipled simulation settings and three real data applications.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 05:40:42 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Helm", "Hayden S.", ""], ["Yang", "Weiwei", ""], ["Bharadwaj", "Sujeeth", ""], ["Lytvynets", "Kate", ""], ["Riva", "Oriana", ""], ["White", "Christopher", ""], ["Geisa", "Ali", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2102.10434", "submitter": "Shiyang Ma", "authors": "Shiyang Ma, Michael P. McDermott", "title": "Adaptive dose-response studies to establish proof-of-concept in\n  learning-phase clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In learning-phase clinical trials in drug development, adaptive designs can\nbe efficient and highly informative when used appropriately. In this article,\nwe extend the multiple comparison procedures with modeling techniques (MCP-Mod)\nprocedure with generalized multiple contrast tests (GMCTs) to two-stage\nadaptive designs for establishing proof-of-concept. The results of an interim\nanalysis of first-stage data are used to adapt the candidate dose-response\nmodels in the second stage. GMCTs are used in both stages to obtain stage-wise\np-values, which are then combined to determine an overall p-value. An\nalternative approach is also considered that combines the t-statistics across\nstages, employing the conditional rejection probability (CRP) principle to\npreserve the Type I error probability. Simulation studies demonstrate that the\nadaptive designs are advantageous compared to the corresponding tests in a\nnon-adaptive design if the selection of the candidate set of dose-response\nmodels is not well informed by evidence from preclinical and early-phase\nstudies.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 20:17:04 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ma", "Shiyang", ""], ["McDermott", "Michael P.", ""]]}, {"id": "2102.10443", "submitter": "Jean-Jacques Forneron", "authors": "Jean-Jacques Forneron, Serena Ng", "title": "Estimation and Inference by Stochastic Optimization: Three Examples", "comments": "5 pages, no appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper illustrates two algorithms designed in Forneron & Ng (2020): the\nresampled Newton-Raphson (rNR) and resampled quasi-Newton (rqN) algorithms\nwhich speed-up estimation and bootstrap inference for structural models. An\nempirical application to BLP shows that computation time decreases from nearly\n5 hours with the standard bootstrap to just over 1 hour with rNR, and only 15\nminutes using rqN. A first Monte-Carlo exercise illustrates the accuracy of the\nmethod for estimation and inference in a probit IV regression. A second\nexercise additionally illustrates statistical efficiency gains relative to\nstandard estimation for simulation-based estimation using a dynamic panel\nregression example.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 20:57:45 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Forneron", "Jean-Jacques", ""], ["Ng", "Serena", ""]]}, {"id": "2102.10473", "submitter": "David Zhao", "authors": "David Zhao, Niccol\\`o Dalmasso, Rafael Izbicki, Ann B. Lee", "title": "Diagnostics for Conditional Density Models and Bayesian Inference\n  Algorithms", "comments": "Appearing in 37th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2021), Spotlight Talk; camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been growing interest in the AI community for precise uncertainty\nquantification. Conditional density models f(y|x), where x represents\npotentially high-dimensional features, are an integral part of uncertainty\nquantification in prediction and Bayesian inference. However, it is challenging\nto assess conditional density estimates and gain insight into modes of failure.\nWhile existing diagnostic tools can determine whether an approximated\nconditional density is compatible overall with a data sample, they lack a\nprincipled framework for identifying, locating, and interpreting the nature of\nstatistically significant discrepancies over the entire feature space. In this\npaper, we present rigorous and easy-to-interpret diagnostics such as (i) the\n\"Local Coverage Test\" (LCT), which distinguishes an arbitrarily misspecified\nmodel from the true conditional density of the sample, and (ii) \"Amortized\nLocal P-P plots\" (ALP) which can quickly provide interpretable graphical\nsummaries of distributional differences at any location x in the feature space.\nOur validation procedures scale to high dimensions and can potentially adapt to\nany type of data at hand. We demonstrate the effectiveness of LCT and ALP\nthrough a simulated experiment and applications to prediction and parameter\ninference for image data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 23:28:10 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 04:15:26 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 15:36:59 GMT"}, {"version": "v4", "created": "Wed, 30 Jun 2021 17:34:24 GMT"}, {"version": "v5", "created": "Sat, 10 Jul 2021 23:27:31 GMT"}, {"version": "v6", "created": "Fri, 23 Jul 2021 16:29:53 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Zhao", "David", ""], ["Dalmasso", "Niccol\u00f2", ""], ["Izbicki", "Rafael", ""], ["Lee", "Ann B.", ""]]}, {"id": "2102.10478", "submitter": "Xiao Wu", "authors": "Xiao Wu, Kate R. Weinberger, Gregory A. Wellenius, Francesca Dominici,\n  Danielle Braun", "title": "Assessing the causal effects of a stochastic intervention in time series\n  data: Are heat alerts effective in preventing deaths and hospitalizations?", "comments": "31 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new causal inference framework for time series data aimed at\nassessing the effectiveness of heat alerts in reducing mortality and\nhospitalization risks. We are interested in addressing the following question:\nhow many deaths and hospitalizations could be averted if we were to increase\nthe frequency of issuing heat alerts in a given location? In the context of\ntime series data, the overlap assumption - each unit must have a positive\nprobability of receiving the treatment - is often violated. This is because, in\na given location, issuing a heat alert is a rare event on an average\ntemperature day as heat alerts are almost always issued on extremely hot days.\nTo overcome this challenge, first we introduce a new class of causal estimands\nunder a stochastic intervention (i.e., increasing the odds of issuing a heat\nalert) for a single time series corresponding to a given location. We develop\nthe theory to show that these causal estimands can be identified and estimated\nunder a weaker version of the overlap assumption. Second, we propose\nnonparametric estimators based on time-varying propensity scores, and derive\npoint-wise confidence bands for these estimators. Third, we extend this\nframework to multiple time series corresponding to multiple locations. Via\nsimulations, we show that the proposed estimator has good performance with\nrespect to bias and root mean squared error. We apply our proposed method to\nestimate the causal effects of increasing the odds of issuing heat alerts in\nreducing deaths and hospitalizations among Medicare enrollees in 2817 U.S.\ncounties. We found weak evidence of a causal link between increasing the odds\nof issuing heat alerts during the warm seasons of 2006-2016 and a reduction in\ndeaths and cause-specific hospitalizations across the 2817 counties.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 23:49:57 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 16:58:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wu", "Xiao", ""], ["Weinberger", "Kate R.", ""], ["Wellenius", "Gregory A.", ""], ["Dominici", "Francesca", ""], ["Braun", "Danielle", ""]]}, {"id": "2102.10537", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee, Francesca Dominici", "title": "Accounting for recall bias in case-control studies: a causal inference\n  approach", "comments": "19 pages, 3 figures, 4 tables. Supplementary materials are available.\n  The R files are available at\n  https://github.com/kwonsang/recall_bias_case_control_study", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A case-control study is designed to help determine if an exposure is\nassociated with an outcome. However, since case-control studies are\nretrospective, they are often subject to recall bias. Recall bias can occur\nwhen study subjects do not remember previous events accurately. In this paper,\nwe first define the estimand of interest: the causal odds ratio (COR) for a\ncase-control study. Second, we develop estimation approaches for the COR and\npresent estimates as a function of recall bias. Third, we define a new quantity\ncalled the \\textit{R-factor}, which denotes the minimal amount of recall bias\nthat leads to altering the initial conclusion. We show that a failure to\naccount for recall bias can significantly bias estimation of the COR. Finally,\nwe apply the proposed framework to a case-control study of the causal effect of\nchildhood physical abuse on adulthood mental health.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 07:33:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lee", "Kwonsang", ""], ["Dominici", "Francesca", ""]]}, {"id": "2102.10583", "submitter": "Jinglai Li", "authors": "Hongqiao Wang, Ziqiao Ao, Tengchao Yu and Jinglai Li", "title": "Inverse Gaussian Process regression for likelihood-free inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider Bayesian inference problems with intractable\nlikelihood functions. We present a method to compute an approximate of the\nposterior with a limited number of model simulations. The method features an\ninverse Gaussian Process regression (IGPR), i.e., one from the output of a\nsimulation model to the input of it. Within the method, we provide an adaptive\nalgorithm with a tempering procedure to construct the approximations of the\nmarginal posterior distributions. With examples we demonstrate that IGPR has a\ncompetitive performance compared to some commonly used algorithms, especially\nin terms of statistical stability and computational efficiency, while the price\nto pay is that it can only compute a weighted Gaussian approximation of the\nmarginal posteriors.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 11:04:10 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wang", "Hongqiao", ""], ["Ao", "Ziqiao", ""], ["Yu", "Tengchao", ""], ["Li", "Jinglai", ""]]}, {"id": "2102.10631", "submitter": "Shengyi He", "authors": "Shengyi He, Guangxin Jiang, Henry Lam, Michael C. Fu", "title": "Adaptive Importance Sampling for Efficient Stochastic Root Finding and\n  Quantile Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In solving simulation-based stochastic root-finding or optimization problems\nthat involve rare events, such as in extreme quantile estimation, running crude\nMonte Carlo can be prohibitively inefficient. To address this issue, importance\nsampling can be employed to drive down the sampling error to a desirable level.\nHowever, selecting a good importance sampler requires knowledge of the solution\nto the problem at hand, which is the goal to begin with and thus forms a\ncircular challenge. We investigate the use of adaptive importance sampling to\nuntie this circularity. Our procedure sequentially updates the importance\nsampler to reach the optimal sampler and the optimal solution simultaneously,\nand can be embedded in both sample average approximation and stochastic\napproximation-type algorithms. Our theoretical analysis establishes strong\nconsistency and asymptotic normality of the resulting estimators. We also\ndemonstrate, via a minimax perspective, the key role of using adaptivity in\ncontrolling asymptotic errors. Finally, we illustrate the effectiveness of our\napproach via numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 16:01:05 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["He", "Shengyi", ""], ["Jiang", "Guangxin", ""], ["Lam", "Henry", ""], ["Fu", "Michael C.", ""]]}, {"id": "2102.10660", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Sayed H. Kadhem and Aristidis K. Nikoloulopoulos", "title": "Bi-factor and second-order copula models for item response data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bi-factor and second-order models based on copulas are proposed for item\nresponse data, where the items can be split into non-overlapping groups such\nthat there is a homogeneous dependence within each group. Our general models\ninclude the Gaussian bi-factor and second-order models as special cases and can\nlead to more probability in the joint upper or lower tail compared with the\nGaussian bi-factor and second-order models. Details on maximum likelihood\nestimation of parameters for the bi-factor and second-order copula models are\ngiven, as well as model selection and goodness-of-fit techniques. Our general\nmethodology is demonstrated with an extensive simulation study and illustrated\nfor the Toronto Alexithymia Scale. Our studies suggest that there can be a\nsubstantial improvement over the Gaussian bi-factor and second-order models\nboth conceptually, as the items can have interpretations of latent\nmaxima/minima or mixtures of means in comparison with latent means, and in fit\nto data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 18:31:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kadhem", "Sayed H.", ""], ["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "2102.10669", "submitter": "Xueheng Shi", "authors": "Colin Gallagher, Rebecca Killick, Robert Lund, Xueheng Shi", "title": "Autocovariance Estimation in the Presence of Changepoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article studies estimation of a stationary autocovariance structure in\nthe presence of an unknown number of mean shifts. Here, a Yule-Walker moment\nestimator for the autoregressive parameters in a dependent time series\ncontaminated by mean shift changepoints is proposed and studied. The estimator\nis based on first order differences of the series and is proven consistent and\nasymptotically normal when the number of changepoints $m$ and the series length\n$N$ satisfies $m/N \\rightarrow 0$ as $N \\rightarrow \\infty$\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 19:52:36 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 22:14:58 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gallagher", "Colin", ""], ["Killick", "Rebecca", ""], ["Lund", "Robert", ""], ["Shi", "Xueheng", ""]]}, {"id": "2102.10670", "submitter": "Jonathan Boss", "authors": "Jonathan Boss, Jyotishka Datta, Xin Wang, Sung Kyun Park, Jian Kang,\n  Bhramar Mukherjee", "title": "Group Inverse-Gamma Gamma Shrinkage for Sparse Regression with\n  Block-Correlated Predictors", "comments": "44 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed continuous shrinkage priors, such as the horseshoe prior, are\nwidely used for sparse estimation problems. However, there is limited work\nextending these priors to predictors with grouping structures. Of particular\ninterest in this article, is regression coefficient estimation where pockets of\nhigh collinearity in the covariate space are contained within known covariate\ngroupings. To assuage variance inflation due to multicollinearity we propose\nthe group inverse-gamma gamma (GIGG) prior, a heavy-tailed prior that can\ntrade-off between local and group shrinkage in a data adaptive fashion. A\nspecial case of the GIGG prior is the group horseshoe prior, whose shrinkage\nprofile is correlated within-group such that the regression coefficients\nmarginally have exact horseshoe regularization. We show posterior consistency\nfor regression coefficients in linear regression models and posterior\nconcentration results for mean parameters in sparse normal means models. The\nfull conditional distributions corresponding to GIGG regression can be derived\nin closed form, leading to straightforward posterior computation. We show that\nGIGG regression results in low mean-squared error across a wide range of\ncorrelation structures and within-group signal densities via simulation. We\napply GIGG regression to data from the National Health and Nutrition\nExamination Survey for associating environmental exposures with liver\nfunctionality.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 19:55:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Boss", "Jonathan", ""], ["Datta", "Jyotishka", ""], ["Wang", "Xin", ""], ["Park", "Sung Kyun", ""], ["Kang", "Jian", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2102.10724", "submitter": "Yu Man Tam", "authors": "Raymond C. W. Leung and Yu-Man Tam", "title": "A Small-Uniform Statistic for the Inference of Functional Linear\n  Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a \"small-uniform\" statistic for the inference of the functional\nPCA estimator in a functional linear regression model. The literature has shown\ntwo extreme behaviors: on the one hand, the FPCA estimator does not converge in\ndistribution in its norm topology; but on the other hand, the FPCA estimator\ndoes have a pointwise asymptotic normal distribution. Our statistic takes a\nmiddle ground between these two extremes: after a suitable rate normalization,\nour small-uniform statistic is constructed as the maximizer of a fractional\nprogramming problem of the FPCA estimator over a finite-dimensional subspace,\nand whose dimensions will grow with sample size. We show the rate for which our\nscalar statistic converges in probability to the supremum of a Gaussian\nprocess. The small-uniform statistic has applications in hypothesis testing.\nSimulations show our statistic has comparable to slightly better power\nproperties for hypothesis testing than the two statistics of Cardot, Ferraty,\nMas and Sarda (2003).\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 00:44:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Leung", "Raymond C. W.", ""], ["Tam", "Yu-Man", ""]]}, {"id": "2102.10778", "submitter": "Boyan Duan", "authors": "Boyan Duan, Larry Wasserman, Aaditya Ramdas", "title": "Interactive identification of individuals with positive treatment effect\n  while controlling false discoveries", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out of the participants in a randomized experiment with anticipated\nheterogeneous treatment effects, is it possible to identify which ones have a\npositive treatment effect, even though each has only taken either treatment or\ncontrol but not both? While subgroup analysis has received attention, claims\nabout individual participants are more challenging. We frame the problem in\nterms of multiple hypothesis testing: we think of each individual as a null\nhypothesis (the potential outcomes are equal, for example) and aim to identify\nindividuals for whom the null is false (the treatment potential outcome\nstochastically dominates the control, for example). We develop a novel\nalgorithm that identifies such a subset, with nonasymptotic control of the\nfalse discovery rate (FDR). Our algorithm allows for interaction -- a human\ndata scientist (or a computer program acting on the human's behalf) may\nadaptively guide the algorithm in a data-dependent manner to gain high\nidentification power. We also propose several extensions: (a) relaxing the null\nto nonpositive effects, (b) moving from unpaired to paired samples, and (c)\nsubgroup identification. We demonstrate via numerical experiments and\ntheoretical analysis that the proposed method has valid FDR control in finite\nsamples and reasonably high identification power.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:21:56 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Duan", "Boyan", ""], ["Wasserman", "Larry", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2102.10783", "submitter": "Rahul Ghosal", "authors": "Rahul Ghosal, Vijay R. Varma, Dmitri Volfson, Inbar Hillel, Jacek\n  Urbanek, Jeffrey M. Hausdorff, Amber Watts and Vadim Zipunnikov", "title": "Distributional data analysis via quantile functions and its application\n  to modelling digital biomarkers of gait in Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of continuous health monitoring via wearable devices, users\nnow generate their unique streams of continuous data such as minute-level\nphysical activity or heart rate. Aggregating these streams into scalar\nsummaries ignores the distributional nature of data and often leads to the loss\nof critical information. We propose to capture the distributional properties of\nwearable data via user-specific quantile functions that are further used in\nfunctional regression and multi-modal distributional modelling. In addition, we\npropose to encode user-specific distributional information with user-specific\nL-moments, robust rank-based analogs of traditional moments. Importantly, this\nL-moment encoding results in mutually consistent functional and distributional\ninterpretation of the results of scalar-on-function regression. We also\ndemonstrate how L-moments can be flexibly employed for analyzing joint and\nindividual sources of variation in multi-modal distributional data. The\nproposed methods are illustrated in a study of association of\naccelerometry-derived digital gait biomarkers with Alzheimer's disease (AD) and\nin people with normal cognitive function. Our analysis shows that the proposed\nquantile-based representation results in a much higher predictive performance\ncompared to simple distributional summaries and attains much stronger\nassociations with clinical cognitive scales.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:30:40 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ghosal", "Rahul", ""], ["Varma", "Vijay R.", ""], ["Volfson", "Dmitri", ""], ["Hillel", "Inbar", ""], ["Urbanek", "Jacek", ""], ["Hausdorff", "Jeffrey M.", ""], ["Watts", "Amber", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "2102.10873", "submitter": "Oskar Allerbo", "authors": "Oskar Allerbo, Rebecka J\\\"ornsten", "title": "Non-linear, Sparse Dimensionality Reduction via Path Lasso Penalized\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data sets are often analyzed and explored via the\nconstruction of a latent low-dimensional space which enables convenient\nvisualization and efficient predictive modeling or clustering. For complex data\nstructures, linear dimensionality reduction techniques like PCA may not be\nsufficiently flexible to enable low-dimensional representation. Non-linear\ndimension reduction techniques, like kernel PCA and autoencoders, suffer from\nloss of interpretability since each latent variable is dependent of all input\ndimensions. To address this limitation, we here present path lasso penalized\nautoencoders. This structured regularization enhances interpretability by\npenalizing each path through the encoder from an input to a latent variable,\nthus restricting how many input variables are represented in each latent\ndimension. Our algorithm uses a group lasso penalty and non-negative matrix\nfactorization to construct a sparse, non-linear latent representation. We\ncompare the path lasso regularized autoencoder to PCA, sparse PCA, autoencoders\nand sparse autoencoders on real and simulated data sets. We show that the\nalgorithm exhibits much lower reconstruction errors than sparse PCA and\nparameter-wise lasso regularized autoencoders for low-dimensional\nrepresentations. Moreover, path lasso representations provide a more accurate\nreconstruction match, i.e. preserved relative distance between objects in the\noriginal and reconstructed spaces.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 10:14:46 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Allerbo", "Oskar", ""], ["J\u00f6rnsten", "Rebecka", ""]]}, {"id": "2102.10894", "submitter": "Christian Soize", "authors": "Christian Soize and Roger Ghanem", "title": "Probabilistic Learning on Manifolds (PLoM) with Partition", "comments": "20 pages, 13 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The probabilistic learning on manifolds (PLoM) introduced in 2016 has solved\ndifficult supervised problems for the ``small data'' limit where the number N\nof points in the training set is small. Many extensions have since been\nproposed, making it possible to deal with increasingly complex cases. However,\nthe performance limit has been observed and explained for applications for\nwhich $N$ is very small (50 for example) and for which the dimension of the\ndiffusion-map basis is close to $N$. For these cases, we propose a novel\nextension based on the introduction of a partition in independent random\nvectors. We take advantage of this novel development to present improvements of\nthe PLoM such as a simplified algorithm for constructing the diffusion-map\nbasis and a new mathematical result for quantifying the concentration of the\nprobability measure in terms of a probability upper bound. The analysis of the\nefficiency of this novel extension is presented through two applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 10:58:15 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Soize", "Christian", ""], ["Ghanem", "Roger", ""]]}, {"id": "2102.10906", "submitter": "Jordan Richards", "authors": "Jordan Richards and Jonathan A. Tawn and Simon Brown", "title": "Modelling Extremes of Spatial Aggregates of Precipitation using\n  Conditional Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inference on the extremal behaviour of spatial aggregates of precipitation is\nimportant for quantifying river flood risk. There are two classes of previous\napproach, with one failing to ensure self-consistency in inference across\ndifferent regions of aggregation and the other requiring highly inflexible\nmarginal and spatial dependence structure assumptions. To overcome these\nissues, we propose a model for high-resolution precipitation data, from which\nwe can simulate realistic fields and explore the behaviour of spatial\naggregates. Recent developments in spatial extremes literature have seen\npromising progress with spatial extensions of the Heffernan and Tawn (2004)\nmodel for conditional multivariate extremes, which can handle a wide range of\ndependence structures. Our contribution is twofold: new parametric forms for\nthe dependence parameters of this model; and a novel framework for deriving\naggregates addressing edge effects and sub-regions without rain. We apply our\nmodelling approach to gridded East-Anglia, UK precipitation data. Return-level\ncurves for spatial aggregates over different regions of various sizes are\nestimated and shown to fit very well to the data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:14:12 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Richards", "Jordan", ""], ["Tawn", "Jonathan A.", ""], ["Brown", "Simon", ""]]}, {"id": "2102.11022", "submitter": "Giulio D'Agostini", "authors": "Giulio D'Agostini and Alfredo Esposito", "title": "What is the probability that a vaccinated person is shielded from\n  Covid-19? A Bayesian MCMC based reanalysis of published data with emphasis on\n  what should be reported as `efficacy'", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.HO q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the information communicated in press releases, and finally\npublished towards the end of 2020 by Pfizer, Moderna and AstraZeneca, we have\nbuilt up a simple Bayesian model, in which the main quantity of interest plays\nthe role of {\\em vaccine efficacy} (`$\\epsilon$'). The resulting Bayesian\nNetwork is processed by a Markov Chain Monte Carlo (MCMC), implemented in JAGS\ninterfaced to R via rjags. As outcome, we get several probability density\nfunctions (pdf's) of $\\epsilon$, each conditioned on the data provided by the\nthree pharma companies. The result is rather stable against large variations of\nthe number of people participating in the trials and it is `somehow' in good\nagreement with the results provided by the companies, in the sense that their\nvalues correspond to the most probable value (`mode') of the pdf's resulting\nfrom MCMC, thus reassuring us about the validity of our simple model. However\nwe maintain that the number to be reported as `vaccine efficacy' should be the\nmean of the distribution, rather than the mode, as it was already very clear to\nLaplace about 250 years ago (its `rule of succession' follows from the simplest\nproblem of the kind). This is particularly important in the case in which the\nnumber of successes equals the numbers of trials, as it happens with the\nefficacy against `severe forms' of infection, claimed by Moderna to be 100%.\nThe implication of the various uncertainties on the predicted number of\nvaccinated infectees is also shown, using both MCMC and approximated formulae.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 22:02:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["D'Agostini", "Giulio", ""], ["Esposito", "Alfredo", ""]]}, {"id": "2102.11048", "submitter": "Xinwei Deng", "authors": "Qiao Liang, Shyam Ranganathan, Kaibo Wang and Xinwei Deng", "title": "JST-RR Model: Joint Modeling of Ratings and Reviews in Sentiment-Topic\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of online reviews has attracted great attention with broad\napplications. Often times, the textual reviews are coupled with the numerical\nratings in the data. In this work, we propose a probabilistic model to\naccommodate both textual reviews and overall ratings with consideration of\ntheir intrinsic connection for a joint sentiment-topic prediction. The key of\nthe proposed method is to develop a unified generative model where the topic\nmodeling is constructed based on review texts and the sentiment prediction is\nobtained by combining review texts and overall ratings. The inference of model\nparameters are obtained by an efficient Gibbs sampling procedure. The proposed\nmethod can enhance the prediction accuracy of review data and achieve an\neffective detection of interpretable topics and sentiments. The merits of the\nproposed method are elaborated by the case study from Amazon datasets and\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:47:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Liang", "Qiao", ""], ["Ranganathan", "Shyam", ""], ["Wang", "Kaibo", ""], ["Deng", "Xinwei", ""]]}, {"id": "2102.11108", "submitter": "Yulin Pan", "authors": "Xianliang Gong, Yulin Pan", "title": "Sequential Bayesian experimental design for estimation of extreme-event\n  probability in stochastic dynamical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a dynamical system with two sources of uncertainties: (1)\nparameterized input with a known probability distribution and (2) stochastic\ninput-to-response (ItR) function with heteroscedastic randomness. Our purpose\nis to efficiently quantify the extreme response probability when the ItR\nfunction is expensive to evaluate. The problem setup arises often in physics\nand engineering problems, with randomness in ItR coming from either intrinsic\nuncertainties (say, as a solution to a stochastic equation) or additional\n(critical) uncertainties that are not incorporated in the input parameter\nspace. To reduce the required sampling numbers, we develop a sequential\nBayesian experimental design method leveraging the variational heteroscedastic\nGaussian process regression (VHGPR) to account for the stochastic ItR, along\nwith a new criterion to select the next-best samples sequentially. The validity\nof our new method is first tested in two synthetic problems with the stochastic\nItR functions defined artificially. Finally, we demonstrate the application of\nour method to an engineering problem of estimating the extreme ship motion\nprobability in ensemble of wave groups, where the uncertainty in ItR naturally\noriginates from the uncertain initial condition of ship motion in each wave\ngroup.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:27:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gong", "Xianliang", ""], ["Pan", "Yulin", ""]]}, {"id": "2102.11116", "submitter": "Giona Casiraghi", "authors": "Giona Casiraghi", "title": "The likelihood-ratio test for multi-edge network models", "comments": "15 pages, 3 figures", "journal-ref": "Journal of Physics: Complexity (2021), Volume 2, Number 3", "doi": "10.1088/2632-072X/ac0493", "report-no": null, "categories": "stat.ME physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity underlying real-world systems implies that standard\nstatistical hypothesis testing methods may not be adequate for these peculiar\napplications. Specifically, we show that the likelihood-ratio test's\nnull-distribution needs to be modified to accommodate the complexity found in\nmulti-edge network data. When working with independent observations, the\np-values of likelihood-ratio tests are approximated using a $\\chi^2$\ndistribution. However, such an approximation should not be used when dealing\nwith multi-edge network data. This type of data is characterized by multiple\ncorrelations and competitions that make the standard approximation unsuitable.\nWe provide a solution to the problem by providing a better approximation of the\nlikelihood-ratio test null-distribution through a Beta distribution. Finally,\nwe empirically show that even for a small multi-edge network, the standard\n$\\chi^2$ approximation provides erroneous results, while the proposed Beta\napproximation yields the correct p-value estimation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:48:35 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Casiraghi", "Giona", ""]]}, {"id": "2102.11150", "submitter": "David Mallinson", "authors": "David C. Mallinson, Felix Elwert", "title": "Estimating Sibling Spillover Effects with Unobserved Confounding Using\n  Gain-Scores", "comments": "Revision (May 5, 2021): Simulation code updated for readability and\n  efficiency", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing area of research in epidemiology is the identification of\nhealth-related sibling spillover effects, or the effect of one individual's\nexposure on their sibling's outcome. The health and health care of family\nmembers may be inextricably confounded by unobserved factors, rendering\nidentification of spillover effects within families particularly challenging.\nWe demonstrate a gain-score regression method for identifying\nexposure-to-outcome spillover effects within sibling pairs in a linear fixed\neffects framework. The method can identify the exposure-to-outcome spillover\neffect if only one sibling's exposure affects the other's outcome; and it\nidentifies the difference between the spillover effects if both siblings'\nexposures affect the others' outcomes. The method fails in the presence of\noutcome-to-exposure spillover and outcome-to-outcome spillover. Analytic\nresults and Monte Carlo simulations demonstrate the method and its limitations.\nTo exercise this method, we estimate the spillover effect of a child's preterm\nbirth on an older sibling's literacy skills, measured by the Phonological\nAwarenesses Literacy Screening-Kindergarten test. We analyze 20,010 sibling\npairs from a population-wide, Wisconsin-based (United States) birth cohort.\nWithout covariate adjustment, we estimate that preterm birth modestly decreases\nan older sibling's test score (-2.11 points; 95% confidence interval: -3.82,\n-0.40 points). In conclusion, gain-scores are a promising strategy for\nidentifying exposure-to-outcome spillovers in sibling pairs while controlling\nfor sibling-invariant unobserved confounding in linear settings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:18:59 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 02:23:38 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 15:34:13 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mallinson", "David C.", ""], ["Elwert", "Felix", ""]]}, {"id": "2102.11157", "submitter": "Lihao Yin", "authors": "Lihao Yin, Huiyan Sang", "title": "Fused Spatial Point Process Intensity Estimation with Varying\n  Coefficients on Complex Constrained Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large spatial data geocoded at accurate locations has\nfueled a growing interest in spatial modeling and analysis of point processes.\nThe proposed research is motivated by the intensity estimation problem for\nlarge spatial point patterns on complex domains in $\\mathbb{R}^2$ (e.g.,\ndomains with irregular boundaries, sharp concavities, and/or interior holes due\nto geographic constraints) and linear networks, where many existing spatial\npoint process models suffer from the problems of \"leakage\" and computation. We\npropose an efficient intensity estimation algorithm to estimate the spatially\nvarying intensity function and to study the varying relationship between\nintensity and explanatory variables on complex domains. The method is built\nupon a graph regularization technique and hence can be flexibly applied to\npoint patterns on complex domains such as regions with irregular boundaries and\nholes, or linear networks. An efficient proximal gradient optimization\nalgorithm is proposed to handle large spatial point patterns. We also derive\nthe asymptotic error bound for the proposed estimator. Numerical studies are\nconducted to illustrate the performance of the method. Finally, we apply the\nmethod to study and visualize the intensity patterns of the accidents on the\nWestern Australia road network, and the spatial variations in the effects of\nincome, lights condition, and population density on the Toronto homicides\noccurrences.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:27:52 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 14:27:54 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Yin", "Lihao", ""], ["Sang", "Huiyan", ""]]}, {"id": "2102.11212", "submitter": "Brinnae Bent", "authors": "Brinnae Bent, Maria Henriquez, Jessilyn Dunn", "title": "cgmquantify: Python and R packages for comprehensive analysis of\n  interstitial glucose and glycemic variability from continuous glucose monitor\n  data", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Continuous glucose monitoring (CGM) systems provide real-time, dynamic\nglucose information by tracking interstitial glucose values throughout the day\n(typically values are recorded every 5 minutes). CGMs are commonly used in\ndiabetes management by clinicians and patients and in research to understand\nhow factors of longitudinal glucose and glucose variability relate to disease\nonset and severity and the efficacy of interventions. CGM data presents unique\nbioinformatic challenges because the data is longitudinal, temporal, and there\nare nearly infinite possible ways to summarize and use this data. There are\nover 20 metrics of glucose variability, no standardization of metrics, and\nlittle validation across studies. Here we present open source python and R\npackages called cgmquantify, which contains over 20 functions with over 25\nclinically validated metrics of glucose and glucose variability and functions\nfor visualizing longitudinal CGM data. This is expected to be useful for\nresearchers and may provide additional insights to patients and clinicians\nabout glucose patterns.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:21:48 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Bent", "Brinnae", ""], ["Henriquez", "Maria", ""], ["Dunn", "Jessilyn", ""]]}, {"id": "2102.11229", "submitter": "Debarghya Mukherjee", "authors": "Debarghya Mukherjee, Moulinath Banerjee and Ya'acov Ritov", "title": "Regression discontinuity design: estimating the treatment effect with\n  standard parametric rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regression discontinuity design models are widely used for the assessment of\ntreatment effects in psychology, econometrics and biomedicine, specifically in\nsituations where treatment is assigned to an individual based on their\ncharacteristics (e.g. scholarship is allocated based on merit) instead of being\nallocated randomly, as is the case, for example, in randomized clinical trials.\nPopular methods that have been largely employed till date for estimation of\nsuch treatment effects suffer from slow rates of convergence (i.e. slower than\n$\\sqrt{n}$). In this paper, we present a new model and method that allows\nestimation of the treatment effect at $\\sqrt{n}$ rate in the presence of fairly\ngeneral forms of confoundedness. Moreover, we show that our estimator is also\nsemi-parametrically efficient in certain situations. We analyze two real\ndatasets via our method and compare our results with those obtained by using\nprevious approaches. We conclude this paper with a discussion on some possible\nextensions of our method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:59:48 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 18:38:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Mukherjee", "Debarghya", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2102.11253", "submitter": "Jinjin Tian", "authors": "Jinjin Tian, Xu Chen, Eugene Katsevich, Jelle Goeman, Aaditya Ramdas", "title": "Large-scale simultaneous inference under dependence", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous, post-hoc inference is desirable in large-scale hypotheses\ntesting as it allows for exploration of data while deciding on criteria for\nproclaiming discoveries. It was recently proved that all admissible post-hoc\ninference methods for the number of true discoveries must be based on closed\ntesting. In this paper we investigate tractable and efficient closed testing\nwith local tests of different properties, such as monotonicty, symmetry and\nseparability, meaning that the test thresholds a monotonic or symmetric\nfunction or a function of sums of test scores for the individual hypotheses.\nThis class includes well-known global null tests by Fisher, Stouffer and\nRuschendorf, as well as newly proposed ones based on harmonic means and Cauchy\ncombinations. Under monotonicity, we propose a new linear time statistic\n(\"coma\") that quantifies the cost of multiplicity adjustments. If the tests are\nalso symmetric and separable, we develop several fast (mostly linear-time)\nalgorithms for post-hoc inference, making closed testing tractable. Paired with\nrecent advances in global null tests based on generalized means, our work\nimmediately instantiates a series of simultaneous inference methods that can\nhandle many complex dependence structures and signal compositions. We provide\nguidance on choosing from these methods via theoretical investigation of the\nconservativeness and sensitivity for different local tests, as well as\nsimulations that find analogous behavior for local tests and full closed\ntesting. One result of independent interest is the following: if\n$P_1,\\dots,P_d$ are $p$-values from a multivariate Gaussian with arbitrary\ncovariance, then their arithmetic average P satisfies $Pr(P \\leq t) \\leq t$ for\n$t \\leq \\frac{1}{2d}$.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:36:37 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Tian", "Jinjin", ""], ["Chen", "Xu", ""], ["Katsevich", "Eugene", ""], ["Goeman", "Jelle", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2102.11279", "submitter": "Albert Navarro", "authors": "Gilma Hern\\'andez-Herrera, David Mori\\~na, Albert Navarro", "title": "Left-censored recurrent event analysis in epidemiological studies: a\n  proposal when the number of previous episodes is unknown", "comments": "1 table, 2 supplementary tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Left censoring can occur with relative frequency when analysing recurrent\nevents in epidemiological studies, especially observational ones. Concretely,\nthe inclusion of individuals that were already at risk before the effective\ninitiation in a cohort study, may cause the unawareness of prior episodes that\nhave already been experienced, and this will easily lead to biased and\ninefficient estimates. The objective of this paper is to propose a statistical\nmethod that performs successfully in these circumstances. Our proposal is based\non the use of models with specific baseline hazard, imputing the number of\nprior episodes when unknown, with a stratified model depending on whether the\nindividual had or had not previously been at risk, and the use of a frailty\nterm. The performance is examined in different scenarios through a\ncomprehensive simulation study.The proposed method achieves notable performance\neven when the percentage of subjects at risk before the beginning of the\nfollow-up is very elevated, with biases that are often under 10\\% and coverages\nof around 95\\%, sometimes somewhat conservative. If the baseline hazard is\nconstant, it seems to be that the ``Gap Time'' approach is better; if it is not\nconstant, the ``Counting Process'' seems to be a better choice. Because of the\nlack of knowledge of the prior episodes that have been experienced by a part\n(or all) of subjects, the use of common baseline methods is not advised. Our\nproposal seems to perform acceptably in the majority of the scenarios proposed,\nbecoming an interesting alternative in this context.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:26:24 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hern\u00e1ndez-Herrera", "Gilma", ""], ["Mori\u00f1a", "David", ""], ["Navarro", "Albert", ""]]}, {"id": "2102.11309", "submitter": "Steven Xu", "authors": "Steven G. Xu, Brian J. Reich", "title": "Bayesian Non-parametric Quantile Process Regression and Estimation of\n  Marginal Quantile Effects", "comments": "50 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Flexible estimation of multiple conditional quantiles is of interest in\nnumerous applications, such as studying the effect of pregnancy-related factors\non very low or high birth weight. We propose a Bayesian non-parametric method\nto simultaneously estimate non-crossing, non-linear quantile curves. We expand\nthe conditional distribution function of the response in I-spline basis\nfunctions where the covariate-dependent coefficients are modeled using neural\nnetworks. By leveraging the approximation power of splines and neural networks,\nour model can approximate any continuous quantile function. Compared to\nexisting models, our model estimates all rather than a finite subset of\nquantiles, scales well to high dimensions, and accounts for estimation\nuncertainty. While the model is arbitrarily flexible, interpretable marginal\nquantile effects are estimated using accumulative local effect plots and\nvariable importance measures. A simulation study shows that our model can\nbetter recover quantiles of the response distribution when the data is sparse,\nand illustrative applications providing new insights on analyses of birth\nweight and tropical cyclone intensity are presented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 19:04:48 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 20:31:21 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Xu", "Steven G.", ""], ["Reich", "Brian J.", ""]]}, {"id": "2102.11334", "submitter": "Charles Manski", "authors": "Charles F Manski, Michael Gmeiner, Anat Tamburc", "title": "Misguided Use of Observed Covariates to Impute Missing Covariates in\n  Conditional Prediction: A Shrinkage Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers regularly perform conditional prediction using imputed values of\nmissing data. However, applications of imputation often lack a firm foundation\nin statistical theory. This paper originated when we were unable to find\nanalysis substantiating claims that imputation of missing data has good\nfrequentist properties when data are missing at random (MAR). We focused on the\nuse of observed covariates to impute missing covariates when estimating\nconditional means of the form E(y|x, w). Here y is an outcome whose\nrealizations are always observed, x is a covariate whose realizations are\nalways observed, and w is a covariate whose realizations are sometimes\nunobserved. We examine the probability limit of simple imputation estimates of\nE(y|x, w) as sample size goes to infinity. We find that these estimates are not\nconsistent when covariate data are MAR. To the contrary, the estimates suffer\nfrom a shrinkage problem. They converge to points intermediate between the\nconditional mean of interest, E(y|x, w), and the mean E(y|x) that conditions\nonly on x. We use a type of genotype imputation to illustrate.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 20:06:08 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Manski", "Charles F", ""], ["Gmeiner", "Michael", ""], ["Tamburc", "Anat", ""]]}, {"id": "2102.11338", "submitter": "Jingshen Wang", "authors": "Xinzhou Guo, Linqing Wei, Chong Wu, Jingshen Wang", "title": "Sharp Inference on Selected Subgroups in Observational Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In modern drug development, the broader availability of high-dimensional\nobservational data provides opportunities for scientist to explore subgroup\nheterogeneity, especially when randomized clinical trials are unavailable due\nto cost and ethical constraints. However, a common practice that naively\nsearches the subgroup with a high treatment level is often misleading due to\nthe \"subgroup selection bias.\" More importantly, the nature of high-dimensional\nobservational data has further exacerbated the challenge of accurately\nestimating the subgroup treatment effects. To resolve these issues, we provide\nnew inferential tools based on resampling to assess the replicability of\npost-hoc identified subgroups from observational studies. Through careful\ntheoretical justification and extensive simulations, we show that our proposed\napproach delivers asymptotically sharp confidence intervals and debiased\nestimates for the selected subgroup treatment effects in the presence of\nhigh-dimensional covariates. We further demonstrate the merit of the proposed\nmethods by analyzing the UK Biobank data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 20:13:11 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Guo", "Xinzhou", ""], ["Wei", "Linqing", ""], ["Wu", "Chong", ""], ["Wang", "Jingshen", ""]]}, {"id": "2102.11341", "submitter": "Ricardo Masini", "authors": "Jianqing Fan, Ricardo Masini and Marcelo C. Medeiros", "title": "Bridging factor and sparse models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Factor and sparse models are two widely used methods to impose a\nlow-dimensional structure in high-dimension. They are seemingly mutually\nexclusive. In this paper, we propose a simple lifting method that combines the\nmerits of these two models in a supervised learning methodology that allows to\nefficiently explore all the information in high-dimensional datasets. The\nmethod is based on a flexible model for panel data, called factor-augmented\nregression model with both observable, latent common factors, as well as\nidiosyncratic components as high-dimensional covariate variables. This model\nnot only includes both factor regression and sparse regression as specific\nmodels but also significantly weakens the cross-sectional dependence and hence\nfacilitates model selection and interpretability. The methodology consists of\nthree steps. At each step, the remaining cross-section dependence can be\ninferred by a novel test for covariance structure in high-dimensions. We\ndeveloped asymptotic theory for the factor-augmented sparse regression model\nand demonstrated the validity of the multiplier bootstrap for testing\nhigh-dimensional covariance structure. This is further extended to testing\nhigh-dimensional partial covariance structures. The theory and methods are\nfurther supported by an extensive simulation study and applications to the\nconstruction of a partial covariance network of the financial returns and a\nprediction exercise for a large panel of macroeconomic time series from FRED-MD\ndatabase.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 20:25:38 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 19:44:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Fan", "Jianqing", ""], ["Masini", "Ricardo", ""], ["Medeiros", "Marcelo C.", ""]]}, {"id": "2102.11425", "submitter": "Francesco Denti", "authors": "Francesco Denti", "title": "intRinsic: an R package for model-based estimation of the intrinsic\n  dimension of a dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The estimation of the intrinsic dimension of a dataset is a fundamental step\nin most dimensionality reduction techniques. This article illustrates\nintRinsic, an R package that implements novel state-of-the-art likelihood-based\nestimators of the intrinsic dimension of a dataset. In detail, the methods\nincluded in this package are the TWO-NN, Gride, and Hidalgo models. To allow\nthese novel estimators to be easily accessible, the package contains a few\nhigh-level, intuitive functions that rely on a broader set of efficient,\nlow-level routines. intRinsic encompasses models that fall into two categories:\nhomogeneous and heterogeneous intrinsic dimension estimators. The first\ncategory contains the TWO-NN and Gride models. The functions dedicated to these\ntwo methods carry out inference under both the frequentist and Bayesian\nframeworks. In the second category we find Hidalgo, a Bayesian mixture model,\nfor which an efficient Gibbs sampler is implemented. After discussing the\ntheoretical background, we demonstrate the performance of the models on\nsimulated datasets. This way, we can assess the results by comparing them with\nthe ground truth. Then, we employ the package to study the intrinsic dimension\nof the Alon dataset, obtained from a famous microarray experiment. We show how\nthe estimation of homogeneous and heterogeneous intrinsic dimensions allows us\nto gain valuable insights about the topological structure of a dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 00:09:22 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Denti", "Francesco", ""]]}, {"id": "2102.11445", "submitter": "Landon Hurley", "authors": "Landon Hurley", "title": "A robust multivariate linear non-parametric maximum likelihood model for\n  ties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical analysis in applied research, across almost every field (e.g.,\nbiomedical, economics, computer science, and psychological) makes use of\nsamples upon which the explicit error distribution of the dependent variable is\nunknown or, at best, difficult to linearly model. Yet, these assumptions are\nextremely common. Unknown distributions are of course biased when incorrectly\nspecified, compromising the generalisability of our interpretations -- the\nlinearly unbiased Euclidean distance is very difficult to correctly identify\nupon finite samples and therefore results in an estimator which is neither\nunbiased nor maximally informative when incorrectly applied. The alternative\ncommon solution to the problem however, the use of non-parametric statistics,\nhas its own fundamental flaws. In particular, these flaws revolve around the\nproblem of order-statistics and the estimation in the presence of ties, which\noften removes the introduction of multiple independent variables and the\nestimation of interactions. We introduce a competitor to the Euclidean norm,\nthe Kemeny norm, which we prove to be a valid Banach space, and construct a\nmultivariate linear expansion of the Kendall-Theil-Sen estimator, which\nperforms without compromising the parameter space extensibility, and establish\nits linear maximum likelihood properties. Empirical demonstrations upon both\nsimulated and empirical data shall be used to demonstrate these properties,\nsuch that the new estimator is nearly equivalent in power for the glm upon\nGaussian data, but grossly superior for a vast array of analytic scenarios,\nincluding finite ordinal sum-score analysis, thereby aiding in the resolution\nof replication in the Applied Sciences.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 01:20:50 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hurley", "Landon", ""]]}, {"id": "2102.11501", "submitter": "Zhen Liu", "authors": "Zhen Liu, Xiaoqian Sun, Yu-Bo Wang", "title": "A Bayesian Spatial Modeling Approach to Mortality Forecasting", "comments": "The corresponding author considers there is a huge drawback at the\n  main methodology to revise in a long time and is not willing to be the duty\n  of corresponding author. Hence, I request the publisher to withdraw this\n  paper. Thanks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper extends Bayesian mortality projection models for multiple\npopulations considering the stochastic structure and the effect of spatial\nautocorrelation among the observations. We explain high levels of\noverdispersion according to adjacent locations based on the conditional\nautoregressive model. In an empirical study, we compare different hierarchical\nprojection models for the analysis of geographical diversity in mortality\nbetween the Japanese counties in multiple years, according to age. By a Markov\nchain Monte Carlo (MCMC) computation, results have demonstrated the flexibility\nand predictive performance of our proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 05:29:53 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 17:11:48 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Liu", "Zhen", ""], ["Sun", "Xiaoqian", ""], ["Wang", "Yu-Bo", ""]]}, {"id": "2102.11543", "submitter": "Quentin Clairon", "authors": "Quentin Clairon and Chlo\\'e Pasin and Irene Balelli and Rodolphe\n  Thi\\'ebaut and M\\'elanie Prague", "title": "Parameter estimation in nonlinear mixed effect models based on ordinary\n  differential equations: an optimal control approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a parameter estimation method for nonlinear mixed effect models\nbased on ordinary differential equations (NLME-ODEs). The method presented here\naims at regularizing the estimation problem in presence of model\nmisspecifications, practical identifiability issues and unknown initial\nconditions. For doing so, we define our estimator as the minimizer of a cost\nfunction which incorporates a possible gap between the assumed model at the\npopulation level and the specific individual dynamic. The cost function\ncomputation leads to formulate and solve optimal control problems at the\nsubject level. This control theory approach allows to bypass the need to know\nor estimate initial conditions for each subject and it regularizes the\nestimation problem in presence of poorly identifiable parameters. Comparing to\nmaximum likelihood, we show on simulation examples that our method improves\nestimation accuracy in possibly partially observed systems with unknown initial\nconditions or poorly identifiable parameters with or without model error. We\nconclude this work with a real application on antibody concentration data after\nvaccination against Ebola virus coming from phase 1 trials. We use the\nestimated model discrepancy at the subject level to analyze the presence of\nmodel misspecification.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 08:15:55 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Clairon", "Quentin", ""], ["Pasin", "Chlo\u00e9", ""], ["Balelli", "Irene", ""], ["Thi\u00e9baut", "Rodolphe", ""], ["Prague", "M\u00e9lanie", ""]]}, {"id": "2102.11575", "submitter": "Juan Kuntz", "authors": "Juan Kuntz, Francesca R. Crucinio, Adam M. Johansen", "title": "Product-form estimators: exploiting independence to scale up Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of Monte Carlo estimators for product-form target\ndistributions that aim to overcome the rapid growth of variance with dimension\noften observed for standard estimators. We identify them with a class of\ngeneralized U-Statistics, and thus establish their unbiasedness, consistency,\nand asymptotic normality. Moreover, we show that they achieve lower variances\nthan their conventional counterparts given the same number of samples drawn\nfrom the target, investigate the gap in variance via several examples, and\nidentify the situations in which the difference is most, and least, pronounced.\nWe further study the estimators' computational cost and delineate the settings\nin which they are most efficient. We illustrate their utility beyond the\nsetting of product-form distributions by detailing two simple extensions (one\nto targets that are mixtures of product-form distributions and another to\ntargets that are absolutely continuous with respect to product-form\ndistributions) and conclude by discussing further possible uses.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:27:30 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:54:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kuntz", "Juan", ""], ["Crucinio", "Francesca R.", ""], ["Johansen", "Adam M.", ""]]}, {"id": "2102.11658", "submitter": "Chihiro Watanabe", "authors": "Chihiro Watanabe, Taiji Suzuki", "title": "A Goodness-of-fit Test on the Number of Biclusters in a Relational Data\n  Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering is a method for detecting homogeneous submatrices in a given\nobserved matrix, and it is an effective tool for relational data analysis.\nAlthough there are many studies that estimate the underlying bicluster\nstructure of a matrix, few have enabled us to determine the appropriate number\nof biclusters in an observed matrix. Recently, a statistical test on the number\nof biclusters has been proposed for a regular-grid bicluster structure, where\nwe assume that the latent bicluster structure can be represented by row-column\nclustering. However, when the latent bicluster structure does not satisfy such\nregular-grid assumption, the previous test requires a larger number of\nbiclusters than necessary (i.e., a finer bicluster structure than necessary)\nfor the null hypothesis to be accepted, which is not desirable in terms of\ninterpreting the accepted bicluster structure. In this study, we propose a new\nstatistical test on the number of biclusters that does not require the\nregular-grid assumption and derive the asymptotic behavior of the proposed test\nstatistic in both null and alternative cases. We illustrate the effectiveness\nof the proposed method by applying it to both synthetic and practical\nrelational data matrices.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 12:25:58 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 06:03:40 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 14:25:32 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Watanabe", "Chihiro", ""], ["Suzuki", "Taiji", ""]]}, {"id": "2102.11759", "submitter": "Anna Vesely", "authors": "Anna Vesely, Livio Finos and Jelle J. Goeman", "title": "Permutation-Based True Discovery Guarantee by Sum Tests", "comments": "Main: 27 pages, 5 figures. Appendices: 15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sum-based global tests are highly popular in multiple hypothesis testing. In\nthis paper we propose a general closed testing procedure for sum tests, which\nprovides lower confidence bounds for the proportion of true discoveries (TDP),\nsimultaneously over all subsets of hypotheses; these simultaneous inferences\ncome for free, i.e., without any adjustment of the alpha-level, whenever a\nglobal test is used. Our method allows for an exploratory approach, as\nsimultaneity ensures control of the TDP even when the subset of interest is\nselected post hoc. It adapts to the unknown joint distribution of the data\nthrough permutation testing. Any sum test may be employed, depending on the\ndesired power properties. We present an iterative shortcut for the closed\ntesting procedure, based on the branch and bound algorithm, which converges to\nthe full closed testing results, often after few iterations; even if it is\nstopped early, it controls the TDP. The feasibility of the method for high\ndimensional data is illustrated on brain imaging data, then we compare the\nproperties of different choices for the sum test through simulations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 15:40:58 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 09:09:55 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 15:09:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Vesely", "Anna", ""], ["Finos", "Livio", ""], ["Goeman", "Jelle J.", ""]]}, {"id": "2102.11761", "submitter": "Sam Witty", "authors": "Sam Witty, David Jensen, Vikash Mansinghka", "title": "A Simulation-Based Test of Identifiability for Bayesian Causal Inference", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a procedure for testing the identifiability of Bayesian\nmodels for causal inference. Although the do-calculus is sound and complete\ngiven a causal graph, many practical assumptions cannot be expressed in terms\nof graph structure alone, such as the assumptions required by instrumental\nvariable designs, regression discontinuity designs, and within-subjects\ndesigns. We present simulation-based identifiability (SBI), a fully automated\nidentification test based on a particle optimization scheme with simulated\nobservations. This approach expresses causal assumptions as priors over\nfunctions in a structural causal model, including flexible priors using\nGaussian processes. We prove that SBI is asymptotically sound and complete, and\nproduces practical finite-sample bounds. We also show empirically that SBI\nagrees with known results in graph-based identification as well as with\nwidely-held intuitions for designs in which graph-based methods are\ninconclusive.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 15:42:06 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Witty", "Sam", ""], ["Jensen", "David", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "2102.11772", "submitter": "Cen Wu", "authors": "Xi Lu, Kun Fan, Jie Ren and Cen Wu", "title": "Identifying Gene-environment interactions with robust marginal Bayesian\n  variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-throughput genetics studies, an important aim is to identify\ngene-environment interactions associated with the clinical outcomes. Recently,\nmultiple marginal penalization methods have been developed and shown to be\neffective in G$\\times$E studies. However, within the Bayesian framework,\nmarginal variable selection has not received much attention. In this study, we\npropose a novel marginal Bayesian variable selection method for G$\\times$E\nstudies. In particular, our marginal Bayesian method is robust to data\ncontamination and outliers in the outcome variables. With the incorporation of\nspike-and-slab priors, we have implemented the Gibbs sampler based on MCMC. The\nproposed method outperforms a number of alternatives in extensive simulation\nstudies. The utility of the marginal robust Bayesian variable selection method\nhas been further demonstrated in the case studies using data from the Nurse\nHealth Study (NHS). Some of the identified main and interaction effects from\nthe real data analysis have important biological implications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 16:09:01 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Lu", "Xi", ""], ["Fan", "Kun", ""], ["Ren", "Jie", ""], ["Wu", "Cen", ""]]}, {"id": "2102.11780", "submitter": "Ines Wilms", "authors": "Alain Hecq and Marie Ternes and Ines Wilms", "title": "Hierarchical Regularizers for Mixed-Frequency Vector Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixed-frequency Vector AutoRegressions (MF-VAR) model the dynamics between\nvariables recorded at different frequencies. However, as the number of series\nand high-frequency observations per low-frequency period grow, MF-VARs suffer\nfrom the \"curse of dimensionality\". We curb this curse through a regularizer\nthat permits various hierarchical sparsity patterns by prioritizing the\ninclusion of coefficients according to the recency of the information they\ncontain. Additionally, we investigate the presence of nowcasting relations by\nsparsely estimating the MF-VAR error covariance matrix. We study predictive\nGranger causality relations in a MF-VAR for the U.S. economy and construct a\ncoincident indicator of GDP growth.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 16:30:15 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hecq", "Alain", ""], ["Ternes", "Marie", ""], ["Wilms", "Ines", ""]]}, {"id": "2102.11824", "submitter": "Lisa Elkin", "authors": "Lisa A. Elkin, Matthew Kay, James J. Higgins, and Jacob O. Wobbrock", "title": "An Aligned Rank Transform Procedure for Multifactor Contrast Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data from multifactor HCI experiments often violates the normality assumption\nof parametric tests (i.e., nonconforming data). The Aligned Rank Transform\n(ART) is a popular nonparametric analysis technique that can find main and\ninteraction effects in nonconforming data, but leads to incorrect results when\nused to conduct contrast tests. We created a new algorithm called ART-C for\nconducting contrasts within the ART paradigm and validated it on 72,000 data\nsets. Our results indicate that ART-C does not inflate Type I error rates,\nunlike contrasts based on ART, and that ART-C has more statistical power than a\nt-test, Mann-Whitney U test, Wilcoxon signed-rank test, and ART. We also\nextended a tool called ARTool with our ART-C algorithm for both Windows and R.\nOur validation had some limitations (e.g., only six distribution types, no\nmixed factorial designs, no random slopes), and data drawn from Cauchy\ndistributions should not be analyzed with ART-C.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:54:14 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Elkin", "Lisa A.", ""], ["Kay", "Matthew", ""], ["Higgins", "James J.", ""], ["Wobbrock", "Jacob O.", ""]]}, {"id": "2102.11904", "submitter": "Irina Degtiar", "authors": "Irina Degtiar and Sherri Rose", "title": "A Review of Generalizability and Transportability", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assessing causal effects, determining the target population to which the\nresults are intended to generalize is a critical decision. Randomized and\nobservational studies each have strengths and limitations for estimating causal\neffects in a target population. Estimates from randomized data may have\ninternal validity but are often not representative of the target population.\nObservational data may better reflect the target population, and hence be more\nlikely to have external validity, but are subject to potential bias due to\nunmeasured confounding. While much of the causal inference literature has\nfocused on addressing internal validity bias, both internal and external\nvalidity are necessary for unbiased estimates in a target population. This\npaper presents a framework for addressing external validity bias, including a\nsynthesis of approaches for generalizability and transportability, the\nassumptions they require, as well as tests for the heterogeneity of treatment\neffects and differences between study and target populations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 19:34:13 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Degtiar", "Irina", ""], ["Rose", "Sherri", ""]]}, {"id": "2102.11926", "submitter": "Alexander Tarr", "authors": "Alexander Tarr and Kosuke Imai", "title": "Estimating Average Treatment Effects with Support Vector Machines", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machine (SVM) is one of the most popular classification\nalgorithms in the machine learning literature. We demonstrate that SVM can be\nused to balance covariates and estimate average causal effects under the\nunconfoundedness assumption. Specifically, we adapt the SVM classifier as a\nkernel-based weighting procedure that minimizes the maximum mean discrepancy\nbetween the treatment and control groups while simultaneously maximizing\neffective sample size. We also show that SVM is a continuous relaxation of the\nquadratic integer program for computing the largest balanced subset,\nestablishing its direct relation to the cardinality matching method. Another\nimportant feature of SVM is that the regularization parameter controls the\ntrade-off between covariate balance and effective sample size. As a result, the\nexisting SVM path algorithm can be used to compute the balance-sample size\nfrontier. We characterize the bias of causal effect estimation arising from\nthis trade-off, connecting the proposed SVM procedure to the existing kernel\nbalancing methods. Finally, we conduct simulation and empirical studies to\nevaluate the performance of the proposed methodology and find that SVM is\ncompetitive with the state-of-the-art covariate balancing methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 20:22:56 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 17:41:14 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tarr", "Alexander", ""], ["Imai", "Kosuke", ""]]}, {"id": "2102.11948", "submitter": "Xiaojing Zhu", "authors": "Xiaojing Zhu, Heather Shappell, Mark A. Kramer, Catherine J. Chu, Eric\n  D. Kolaczyk", "title": "Inferring the Type of Phase Transitions Undergone in Epileptic Seizures\n  Using Random Graph Hidden Markov Models for Percolation in Noisy Dynamic\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical neuroscience, epileptic seizures have been associated with the\nsudden emergence of coupled activity across the brain. The resulting functional\nnetworks - in which edges indicate strong enough coupling between brain regions\n- are consistent with the notion of percolation, which is a phenomenon in\ncomplex networks corresponding to the sudden emergence of a giant connected\ncomponent. Traditionally, work has concentrated on noise-free percolation with\na monotonic process of network growth, but real-world networks are more\ncomplex. We develop a class of random graph hidden Markov models (RG-HMMs) for\ncharacterizing percolation regimes in noisy, dynamically evolving networks in\nthe presence of edge birth and edge death, as well as noise. This class is used\nto understand the type of phase transitions undergone in a seizure, and in\nparticular, distinguishing between different percolation regimes in epileptic\nseizures. We develop a hypothesis testing framework for inferring putative\npercolation mechanisms. As a necessary precursor, we present an EM algorithm\nfor estimating parameters from a sequence of noisy networks only observed at a\nlongitudinal subsampling of time points. Our results suggest that different\ntypes of percolation can occur in human seizures. The type inferred may suggest\ntailored treatment strategies and provide new insights into the fundamental\nscience of epilepsy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 21:31:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zhu", "Xiaojing", ""], ["Shappell", "Heather", ""], ["Kramer", "Mark A.", ""], ["Chu", "Catherine J.", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2102.11971", "submitter": "Guillermo Granados", "authors": "Guillermo Granados-Garcia, Mark Fiecas, Babak Shahbaba, Norbert Fortin\n  and Hernando Ombao", "title": "Brain Waves Analysis Via a Non-parametric Bayesian Mixture of\n  Autoregressive Kernels", "comments": "21 pages, 7 Figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The standard approach to analyzing brain electrical activity is to examine\nthe spectral density function (SDF) and identify predefined frequency bands\nthat have the most substantial relative contributions to the overall variance\nof the signal. However, a limitation of this approach is that the precise\nfrequency and bandwidth of oscillations vary with cognitive demands. Thus they\nshould not be arbitrarily defined a priori in an experiment. In this paper, we\ndevelop a data-driven approach that identifies (i) the number of prominent\npeaks, (ii) the frequency peak locations, and (iii) their corresponding\nbandwidths (or spread of power around the peaks). We propose a Bayesian mixture\nauto-regressive decomposition method (BMARD), which represents the standardized\nSDFas a Dirichlet process mixture based on a kernel derived from second-order\nauto-regressive processes which completely characterize the location (peak)and\nscale (bandwidth) parameters. We present a Metropolis-Hastings within Gibbs\nalgorithm to sample from the posterior distribution of the mixture parameters.\nSimulation studies demonstrate the robustness and performance of the BMARD\nmethod. Finally, we use the proposed BMARD method to analyze local field\npotential (LFP) activity from the hippocampus of laboratory rats across\ndifferent conditions in a non-spatial sequence memory experiment to identify\nthe most interesting frequency bands and examine the link between specific\npatterns of activity and trial-specific cognitive demands.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 22:37:14 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 11:30:31 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 08:40:18 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Granados-Garcia", "Guillermo", ""], ["Fiecas", "Mark", ""], ["Shahbaba", "Babak", ""], ["Fortin", "Norbert", ""], ["Ombao", "Hernando", ""]]}, {"id": "2102.12034", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Sivaraman Balakrishnan, Larry Wasserman", "title": "Semiparametric counterfactual density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal effects are often characterized with averages, which can give an\nincomplete picture of the underlying counterfactual distributions. Here we\nconsider estimating the entire counterfactual density and generic functionals\nthereof. We focus on two kinds of target parameters. The first is a density\napproximation, defined by a projection onto a finite-dimensional model using a\ngeneralized distance metric, which includes f-divergences as well as $L_p$\nnorms. The second is the distance between counterfactual densities, which can\nbe used as a more nuanced effect measure than the mean difference, and as a\ntool for model selection. We study nonparametric efficiency bounds for these\ntargets, giving results for smooth but otherwise generic models and distances.\nImportantly, we show how these bounds connect to means of particular\nnon-trivial functions of counterfactuals, linking the problems of density and\nmean estimation. We go on to propose doubly robust-style estimators for the\ndensity approximations and distances, and study their rates of convergence,\nshowing they can be optimally efficient in large nonparametric models. We also\ngive analogous methods for model selection and aggregation, when many models\nmay be available and of interest. Our results all hold for generic models and\ndistances, but throughout we highlight what happens for particular choices,\nsuch as $L_2$ projections on linear models, and KL projections on exponential\nfamilies. Finally we illustrate by estimating the density of CD4 count among\npatients with HIV, had all been treated with combination therapy versus\nzidovudine alone, as well as a density effect. Our results suggest combination\ntherapy may have increased CD4 count most for high-risk patients. Our methods\nare implemented in the freely available R package npcausal on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 02:32:32 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "2102.12225", "submitter": "Shunichiro Orihara", "authors": "Shunichiro Orihara", "title": "Valid Instrumental Variables Selection Methods using Auxiliary Variable\n  and Constructing Efficient Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, we are usually interested in estimating causal\neffects between treatments and outcomes. When some covariates are not observed,\nan unbiased estimator usually cannot be obtained. In this paper, we focus on\ninstrumental variable (IV) methods. By using IVs, an unbiased estimator for\ncausal effects can be estimated even if there exists some unmeasured\ncovariates. Constructing a linear combination of IVs solves weak IV problems,\nhowever, there are risks estimating biased causal effects by including some\ninvalid IVs. In this paper, we use Negative Control Outcomes as auxiliary\nvariables to select valid IVs. By using NCOs, there are no necessity to specify\nnot only the set of valid IVs but also invalid one in advance: this point is\ndifferent from previous methods. We prove that the estimated causal effects has\nthe same asymptotic variance as the estimator using Generalized Method of\nMoments that has the semiparametric efficiency. Also, we confirm properties of\nour method and previous methods through simulations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:34:25 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Orihara", "Shunichiro", ""]]}, {"id": "2102.12230", "submitter": "Kody Law", "authors": "Jeremy Heng, Ajay Jasra, Kody J. H. Law, Alexander Tarakanov", "title": "On Unbiased Estimation for Discretized Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider computing expectations w.r.t. probability\nmeasures which are subject to discretization error. Examples include partially\nobserved diffusion processes or inverse problems, where one may have to\ndiscretize time and/or space, in order to practically work with the probability\nof interest. Given access only to these discretizations, we consider the\nconstruction of unbiased Monte Carlo estimators of expectations w.r.t. such\ntarget probability distributions. It is shown how to obtain such estimators\nusing a novel adaptation of randomization schemes and Markov simulation\nmethods. Under appropriate assumptions, these estimators possess finite\nvariance and finite expected cost. There are two important consequences of this\napproach: (i) unbiased inference is achieved at the canonical complexity rate,\nand (ii) the resulting estimators can be generated independently, thereby\nallowing strong scaling to arbitrarily many parallel processors. Several\nalgorithms are presented, and applied to some examples of Bayesian inference\nproblems, with both simulated and real observed data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:48:07 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Heng", "Jeremy", ""], ["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Tarakanov", "Alexander", ""]]}, {"id": "2102.12262", "submitter": "Guosheng Yin", "authors": "Hengtao Zhang, Guosheng Yin and Donald B. Rubin", "title": "PCA Rerandomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mahalanobis distance between treatment group and control group covariate\nmeans is often adopted as a balance criterion when implementing a\nrerandomization strategy. However, this criterion may not work well for\nhigh-dimensional cases because it balances all orthogonalized covariates\nequally. Here, we propose leveraging principal component analysis (PCA) to\nidentify proper subspaces in which Mahalanobis distance should be calculated.\nNot only can PCA effectively reduce the dimensionality for high-dimensional\ncases while capturing most of the information in the covariates, but it also\nprovides computational simplicity by focusing on the top orthogonal components.\nWe show that our PCA rerandomization scheme has desirable theoretical\nproperties on balancing covariates and thereby on improving the estimation of\naverage treatment effects. We also show that this conclusion is supported by\nnumerical studies using both simulated and real examples.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:51:22 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zhang", "Hengtao", ""], ["Yin", "Guosheng", ""], ["Rubin", "Donald B.", ""]]}, {"id": "2102.12290", "submitter": "Anass El Yaagoubi Bourakna", "authors": "Anass El Yaagoubi Bourakna, Marco Pinto, Norbert Fortin, Hernando\n  Ombao", "title": "Smooth Online Parameter Estimation for time varying VAR models with\n  application to rat's LFP data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series data appear often as realizations of non-stationary\nprocesses where the covariance matrix or spectral matrix smoothly evolve over\ntime. Most of the current approaches estimate the time-varying spectral\nproperties only retrospectively - that is, after the entire data has been\nobserved. Retrospective estimation is a major limitation in many adaptive\ncontrol applications where it is important to estimate these properties and\ndetect changes in the system as they happen in real-time. One major obstacle in\nonline estimation is the computational cost due to the high-dimensionality of\nthe parameters. Existing methods such as the Kalman filter or local least\nsquares are feasible. However, they are not always suitable because they\nprovide noisy estimates and can become prohibitively costly as the dimension of\nthe time series increases. In our brain signal application, it is critical to\ndevelop a robust method that can estimate, in real-time, the properties of the\nunderlying stochastic process, in particular, the spectral brain connectivity\nmeasures. For these reasons we propose a new smooth online parameter estimation\napproach (SOPE) that has the ability to control for the smoothness of the\nestimates with a reasonable computational complexity. Consequently, the models\nare fit in real-time even for high dimensional time series. We demonstrate that\nour proposed SOPE approach is as good as the Kalman filter in terms of\nmean-squared error for small dimensions. However, unlike the Kalman filter, the\nSOPE has lower computational cost and hence scalable for higher dimensions.\nFinally, we apply the SOPE method to a rat's local field potential data during\na hippocampus-dependent sequence-memory task. As demonstrated in the video, the\nproposed SOPE method is able to capture the dynamics of the connectivity as the\nrat performs the sequence of non-spatial working memory tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 13:55:30 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 16:52:12 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bourakna", "Anass El Yaagoubi", ""], ["Pinto", "Marco", ""], ["Fortin", "Norbert", ""], ["Ombao", "Hernando", ""]]}, {"id": "2102.12526", "submitter": "Will Consagra", "authors": "William Consagra, Arun Venkataraman, and Zhengwu Zhang", "title": "Optimized Diffusion Imaging for Brain Structural Connectome Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High angular resolution diffusion imaging (HARDI), a type of diffusion\nmagnetic resonance imaging (dMRI) that measures diffusion signals on a sphere\nin q-space, is widely used in data acquisition for human brain structural\nconnectome analysis. Accurate estimation of the local diffusion, and thus the\nstructural connectome, typically requires dense sampling in HARDI, resulting in\nlong acquisition times and logistical challenges. We propose a method to select\nan optimal set of q-space directions for recovery of the local diffusion under\na sparsity constraint on the sampling budget. Relevant historical dMRI data is\nleveraged to estimate a prior distribution of the local diffusion in a template\nspace using reduced rank Gaussian process models. For a new subject to be\nscanned, the priors are mapped into the subject-specific coordinate and used to\nguide an optimized q-space sampling which minimizes the expected integrated\nsquared error of a diffusion function estimator from sparse samples. The\noptimized sampling locations are inferred by an efficient greedy algorithm with\ntheoretical bounds approximating the global optimum. Simulation studies and a\nreal data application using the Human Connectome Project data demonstrate that\nour proposed method provides substantial advantages over its competitors.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:42:27 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Consagra", "William", ""], ["Venkataraman", "Arun", ""], ["Zhang", "Zhengwu", ""]]}, {"id": "2102.12561", "submitter": "Indrayudh Ghosal", "authors": "Indrayudh Ghosal, Giles Hooker", "title": "Generalised Boosted Forests", "comments": "Paper: 14 pages, 4 figures, 3 tables; Appendix: 34 pages, 28 figures,\n  1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper extends recent work on boosting random forests to model\nnon-Gaussian responses. Given an exponential family $\\mathbb{E}[Y|X] =\ng^{-1}(f(X))$ our goal is to obtain an estimate for $f$. We start with an\nMLE-type estimate in the link space and then define generalised residuals from\nit. We use these residuals and some corresponding weights to fit a base random\nforest and then repeat the same to obtain a boost random forest. We call the\nsum of these three estimators a \\textit{generalised boosted forest}. We show\nwith simulated and real data that both the random forest steps reduces test-set\nlog-likelihood, which we treat as our primary metric. We also provide a\nvariance estimator, which we can obtain with the same computational cost as the\noriginal estimate itself. Empirical experiments on real-world data and\nsimulations demonstrate that the methods can effectively reduce bias, and that\nconfidence interval coverage is conservative in the bulk of the covariate\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 21:17:31 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 23:15:16 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ghosal", "Indrayudh", ""], ["Hooker", "Giles", ""]]}, {"id": "2102.12675", "submitter": "Mohammad Reza Ehsani", "authors": "Hoshin V Gupta, Mohammed Reza Ehsani, Tirthankar Roy, Maria A\n  Sans-Fuentes, Uwe Ehret, Ali Behrangi", "title": "Computing Accurate Probabilistic Estimates of One-D Entropy from\n  Equiprobable Random Samples", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": "10.3390/e23060740", "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a simple Quantile Spacing (QS) method for accurate probabilistic\nestimation of one-dimensional entropy from equiprobable random samples, and\ncompare it with the popular Bin-Counting (BC) method. In contrast to BC, which\nuses equal-width bins with varying probability mass, the QS method uses\nestimates of the quantiles that divide the support of the data generating\nprobability density function (pdf) into equal-probability-mass intervals.\nWhereas BC requires optimal tuning of a bin-width hyper-parameter whose value\nvaries with sample size and shape of the pdf, QS requires specification of the\nnumber of quantiles to be used. Results indicate, for the class of\ndistributions tested, that the optimal number of quantile-spacings is a fixed\nfraction of the sample size (empirically determined to be ~0.25-0.35), and that\nthis value is relatively insensitive to distributional form or sample size,\nproviding a clear advantage over BC since hyperparameter tuning is not\nrequired. Bootstrapping is used to approximate the sampling variability\ndistribution of the resulting entropy estimate, and is shown to accurately\nreflect the true uncertainty. For the four distributional forms studied\n(Gaussian, Log-Normal, Exponential and Bimodal Gaussian Mixture), expected\nestimation bias is less than 1% and uncertainty is relatively low even for very\nsmall sample sizes. We speculate that estimating quantile locations, rather\nthan bin-probabilities, results in more efficient use of the information in the\ndata to approximate the underlying shape of an unknown data generating pdf.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 04:15:39 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 14:42:38 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gupta", "Hoshin V", ""], ["Ehsani", "Mohammed Reza", ""], ["Roy", "Tirthankar", ""], ["Sans-Fuentes", "Maria A", ""], ["Ehret", "Uwe", ""], ["Behrangi", "Ali", ""]]}, {"id": "2102.12698", "submitter": "Nikola Surjanovic", "authors": "Nikola Surjanovic and Thomas M. Loughin", "title": "Improving the Hosmer-Lemeshow Goodness-of-Fit Test in Large Models with\n  Replicated Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hosmer-Lemeshow (HL) test is a commonly used global goodness-of-fit (GOF)\ntest that assesses the quality of the overall fit of a logistic regression\nmodel. In this paper, we give results from simulations showing that the type 1\nerror rate (and hence power) of the HL test decreases as model complexity\ngrows, provided that the sample size remains fixed and binary replicates are\npresent in the data. We demonstrate that the generalized version of the HL test\nby Surjanovic et al. (2020) can offer some protection against this power loss.\nWe conclude with a brief discussion explaining the behaviour of the HL test,\nalong with some guidance on how to choose between the two tests.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 05:53:40 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Surjanovic", "Nikola", ""], ["Loughin", "Thomas M.", ""]]}, {"id": "2102.12735", "submitter": "Veronique Maume-Deschamps", "authors": "Kevin Elie-Dit-Cosaque, V\\'eronique Maume-Deschamps (ICJ, PSPM)", "title": "Random Forest based Qantile Oriented Sensitivity Analysis indices\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a random forest based estimation procedure for Quantile Oriented\nSensitivity Analysis-QOSA. In order to be efficient, a cross validation step on\nthe leaf size of trees is required. Our full estimation procedure is tested on\nboth simulated data and a real dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 09:02:51 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Elie-Dit-Cosaque", "Kevin", "", "ICJ, PSPM"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ, PSPM"]]}, {"id": "2102.12752", "submitter": "Minseok Shin", "authors": "Minseok Shin, Donggyu Kim, Jianqing Fan", "title": "Adaptive Robust Large Volatility Matrix Estimation Based on\n  High-Frequency Financial Data", "comments": "51 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several novel statistical methods have been developed to estimate large\nintegrated volatility matrices based on high-frequency financial data. To\ninvestigate their asymptotic behaviors, they require a sub-Gaussian or finite\nhigh-order moment assumption for observed log-returns, which cannot account for\nthe heavy tail phenomenon of stock returns. Recently, a robust estimator was\ndeveloped to handle heavy-tailed distributions with some bounded fourth-moment\nassumption. However, we often observe that log-returns have heavier tail\ndistribution than the finite fourth-moment and that the degrees of heaviness of\ntails are heterogeneous over the asset and time period. In this paper, to deal\nwith the heterogeneous heavy-tailed distributions, we develop an adaptive\nrobust integrated volatility estimator that employs pre-averaging and\ntruncation schemes based on jump-diffusion processes. We call this an adaptive\nrobust pre-averaging realized volatility (ARP) estimator. We show that the ARP\nestimator has a sub-Weibull tail concentration with only finite 2$\\alpha$-th\nmoments for any $\\alpha>1$. In addition, we establish matching upper and lower\nbounds to show that the ARP estimation procedure is optimal. To estimate large\nintegrated volatility matrices using the approximate factor model, the ARP\nestimator is further regularized using the principal orthogonal complement\nthresholding (POET) method. The numerical study is conducted to check the\nfinite sample performance of the ARP estimator.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 09:42:53 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Shin", "Minseok", ""], ["Kim", "Donggyu", ""], ["Fan", "Jianqing", ""]]}, {"id": "2102.12893", "submitter": "Somit Gupta", "authors": "Soheil Sadeghi, Somit Gupta, Stefan Gramatovici, Jiannan Lu, Hao Ai,\n  Ruhan Zhang", "title": "Novelty and Primacy: A Long-Term Estimator for Online Experiments", "comments": "Submitted to the Technometrics Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online experiments are the gold standard for evaluating impact on user\nexperience and accelerating innovation in software. However, since experiments\nare typically limited in duration, observed treatment effects are not always\npermanently stable, sometimes revealing increasing or decreasing patterns over\ntime. There are multiple causes for a treatment effect to change over time. In\nthis paper, we focus on a particular cause, user-learning, which is primarily\nassociated with novelty or primacy. Novelty describes the desire to use new\ntechnology that tends to diminish over time. Primacy describes the growing\nengagement with technology as a result of adoption of the innovation.\nUser-learning estimation is critical because it holds experimentation\nresponsible for trustworthiness, empowers organizations to make better\ndecisions by providing a long-term view of expected impact, and prevents user\ndissatisfaction. In this paper, we propose an observational approach, based on\ndifference-in-differences technique to estimate user-learning at scale. We use\nthis approach to test and estimate user-learning in many experiments at\nMicrosoft. We compare our approach with the existing experimental method to\nshow its benefits in terms of ease of use and higher statistical power, and to\ndiscuss its limitation in presence of other forms of treatment interaction with\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 00:49:53 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Sadeghi", "Soheil", ""], ["Gupta", "Somit", ""], ["Gramatovici", "Stefan", ""], ["Lu", "Jiannan", ""], ["Ai", "Hao", ""], ["Zhang", "Ruhan", ""]]}, {"id": "2102.12938", "submitter": "Nilabja Guha", "authors": "Nilabja Guha and Jyotishka Datta", "title": "On Posterior consistency of Bayesian Changepoint models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there have been a lot of recent developments in the context of Bayesian\nmodel selection and variable selection for high dimensional linear models,\nthere is not much work in the presence of change point in literature, unlike\nthe frequentist counterpart. We consider a hierarchical Bayesian linear model\nwhere the active set of covariates that affects the observations through a mean\nmodel can vary between different time segments. Such structure may arise in\nsocial sciences/ economic sciences, such as sudden change of house price based\non external economic factor, crime rate changes based on social and\nbuilt-environment factors, and others. Using an appropriate adaptive prior, we\noutline the development of a hierarchical Bayesian methodology that can select\nthe true change point as well as the true covariates, with high probability. We\nprovide the first detailed theoretical analysis for posterior consistency with\nor without covariates, under suitable conditions. Gibbs sampling techniques\nprovide an efficient computational strategy. We also consider small sample\nsimulation study as well as application to crime forecasting applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 15:34:03 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Guha", "Nilabja", ""], ["Datta", "Jyotishka", ""]]}, {"id": "2102.12976", "submitter": "Eric Chuu", "authors": "Eric Chuu, Debdeep Pati, Anirban Bhattacharya", "title": "A Hybrid Approximation to the Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the marginal likelihood or evidence is one of the core challenges\nin Bayesian analysis. While there are many established methods for estimating\nthis quantity, they predominantly rely on using a large number of posterior\nsamples obtained from a Markov Chain Monte Carlo (MCMC) algorithm. As the\ndimension of the parameter space increases, however, many of these methods\nbecome prohibitively slow and potentially inaccurate. In this paper, we propose\na novel method in which we use the MCMC samples to learn a high probability\npartition of the parameter space and then form a deterministic approximation\nover each of these partition sets. This two-step procedure, which constitutes\nboth a probabilistic and a deterministic component, is termed a Hybrid\napproximation to the marginal likelihood. We demonstrate its versatility in a\nplethora of examples with varying dimension and sample size, and we also\nhighlight the Hybrid approximation's effectiveness in situations where there is\neither a limited number or only approximate MCMC samples available.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 05:09:41 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Chuu", "Eric", ""], ["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "2102.13103", "submitter": "Marie Davidian", "authors": "Anastasios A. Tsiatis and Marie Davidian", "title": "Estimating Vaccine Efficacy Over Time After a Randomized Study is\n  Unblinded", "comments": "32 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic due to the novel coronavirus SARS CoV-2 has inspired\nremarkable breakthroughs in development of vaccines against the virus and the\nlaunch of several phase 3 vaccine trials in Summer 2020 to evaluate vaccine\nefficacy (VE). Trials of vaccine candidates using mRNA delivery systems\ndeveloped by Pfizer-BioNTech and Moderna have shown substantial VEs of 94-95%,\nleading the US Food and Drug Administration to issue Emergency Use\nAuthorizations and subsequent widespread administration of the vaccines. As the\ntrials continue, a key issue is the possibility that VE may wane over time.\nEthical considerations dictate that all trial participants be unblinded and\nthose randomized to placebo be offered vaccine, leading to trial protocol\namendments specifying unblinding strategies. Crossover of placebo subjects to\nvaccine complicates inference on waning of VE. We focus on the particular\nfeatures of the Moderna trial and propose a statistical framework based on a\npotential outcomes formulation within which we develop methods for inference on\nwhether or not VE wanes over time and estimation of VE at any post-vaccination\ntime. The framework clarifies assumptions made regarding individual- and\npopulation-level phenomena and acknowledges the possibility that subjects who\nare more or less likely to become infected may be crossed over to vaccine\ndifferentially over time. The principles of the framework can be adapted\nstraightforwardly to other trials.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:44:33 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Tsiatis", "Anastasios A.", ""], ["Davidian", "Marie", ""]]}, {"id": "2102.13209", "submitter": "Fotios Petropoulos", "authors": "Fotios Petropoulos and Yael Grushka-Cockayne", "title": "Fast and frugal time series forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the years, families of forecasting models, such as the exponential\nsmoothing family and Autoregressive Integrated Moving Average, have expanded to\ncontain multiple possible forms and forecasting profiles. In this paper, we\nquestion the need to consider such large families of models. We argue that\nparsimoniously identifying suitable subsets of models will not decrease the\nforecasting accuracy nor will it reduce the ability to estimate the forecast\nuncertainty. We propose a framework that balances forecasting performance\nversus computational cost, resulting in a set of reduced families of models and\nempirically demonstrate this trade-offs. We translate computational benefits to\nmonetary cost savings and discuss the implications of our results in the\ncontext of large retailers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 22:00:55 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Petropoulos", "Fotios", ""], ["Grushka-Cockayne", "Yael", ""]]}, {"id": "2102.13218", "submitter": "Dan Soriano", "authors": "Dan Soriano, Eli Ben-Michael, Peter J. Bickel, Avi Feller, Samuel D.\n  Pimentel", "title": "Interpretable Sensitivity Analysis for Balancing Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing sensitivity to unmeasured confounding is an important step in\nobservational studies, which typically estimate effects under the assumption\nthat all confounders are measured. In this paper, we develop a sensitivity\nanalysis framework for balancing weights estimators, an increasingly popular\napproach that solves an optimization problem to obtain weights that directly\nminimizes covariate imbalance. In particular, we adapt a sensitivity analysis\nframework using the percentile bootstrap for a broad class of balancing weights\nestimators. We prove that the percentile bootstrap procedure can, with only\nminor modifications, yield valid confidence intervals for causal effects under\nrestrictions on the level of unmeasured confounding. We also propose an\namplification to allow for interpretable sensitivity parameters in the\nbalancing weights framework. We illustrate our method through extensive real\ndata examples.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 23:04:24 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 20:45:39 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Soriano", "Dan", ""], ["Ben-Michael", "Eli", ""], ["Bickel", "Peter J.", ""], ["Feller", "Avi", ""], ["Pimentel", "Samuel D.", ""]]}, {"id": "2102.13232", "submitter": "Pengfei Li", "authors": "Meng Yuan, Pengfei Li, and Changbao Wu", "title": "Semiparametric empirical likelihood inference with estimating equations\n  under density ratio models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The density ratio model (DRM) provides a flexible and useful platform for\ncombining information from multiple sources. In this paper, we consider\nstatistical inference under two-sample DRMs with additional parameters defined\nthrough and/or additional auxiliary information expressed as estimating\nequations. We examine the asymptotic properties of the maximum empirical\nlikelihood estimators (MELEs) of the unknown parameters in the DRMs and/or\ndefined through estimating equations, and establish the chi-square limiting\ndistributions for the empirical likelihood ratio (ELR) statistics. We show that\nthe asymptotic variance of the MELEs of the unknown parameters does not\ndecrease if one estimating equation is dropped. Similar properties are obtained\nfor inferences on the cumulative distribution function and quantiles of each of\nthe populations involved. We also propose an ELR test for the validity and\nusefulness of the auxiliary information. Simulation studies show that correctly\nspecified estimating equations for the auxiliary information result in more\nefficient estimators and shorter confidence intervals. Two real-data examples\nare used for illustrations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 23:47:25 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Yuan", "Meng", ""], ["Li", "Pengfei", ""], ["Wu", "Changbao", ""]]}, {"id": "2102.13245", "submitter": "Tiangang Cui", "authors": "Tiangang Cui and Olivier Zahm", "title": "Data-Free Likelihood-Informed Dimension Reduction of Bayesian Inverse\n  Problems", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/abeafb", "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Identifying a low-dimensional informed parameter subspace offers a viable\npath to alleviating the dimensionality challenge in the sampled-based solution\nto large-scale Bayesian inverse problems. This paper introduces a novel\ngradient-based dimension reduction method in which the informed subspace does\nnot depend on the data. This permits an online-offline computational strategy\nwhere the expensive low-dimensional structure of the problem is detected in an\noffline phase, meaning before observing the data. This strategy is particularly\nrelevant for multiple inversion problems as the same informed subspace can be\nreused. The proposed approach allows controlling the approximation error (in\nexpectation over the data) of the posterior distribution. We also present\nsampling strategies that exploit the informed subspace to draw efficiently\nsamples from the exact posterior distribution. The method is successfully\nillustrated on two numerical examples: a PDE-based inverse problem with a\nGaussian process prior and a tomography problem with Poisson data and a\nBesov-$\\mathcal{B}^2_{11}$ prior.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 00:31:19 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Cui", "Tiangang", ""], ["Zahm", "Olivier", ""]]}, {"id": "2102.13252", "submitter": "Linda Valeri", "authors": "Linda Valeri, C\\'ecile Proust-Lima, Weijia Fan, Jarvis T. Chen and\n  H\\'el\\`ene Jacqmin-Gadda", "title": "A multistate approach for mediation analysis in the presence of\n  semi-competing risks with application in cancer survival disparities", "comments": "31 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel methodology to quantify the effect of stochastic\ninterventions on non-terminal time-to-events that lie on the pathway between an\nexposure and a terminal time-to-event outcome. Investigating these effects is\nparticularly important in health disparities research when we seek to quantify\ninequities in timely delivery of treatment and its impact on patients survival\ntime. Current approaches fail to account for semi-competing risks arising in\nthis setting. Under the potential outcome framework, we define and provide\nidentifiability conditions for causal estimands for stochastic direct and\nindirect effects. Causal contrasts are estimated in continuous time within a\nmultistate modeling framework and analytic formulae for the estimators of the\ncausal contrasts are developed. We show via simulations that ignoring censoring\nin mediator and or outcome time-to-event processes, or ignoring competing risks\nmay give misleading results. This work demonstrates that rigorous definition of\nthe direct and indirect effects and joint estimation of the outcome and\nmediator time-to-event distributions in the presence of semi-competing risks\nare crucial for valid investigation of mechanisms in continuous time. We employ\nthis novel methodology to investigate the role of delaying treatment uptake in\nexplaining racial disparities in cancer survival in a cohort study of colon\ncancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 01:31:22 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Valeri", "Linda", ""], ["Proust-Lima", "C\u00e9cile", ""], ["Fan", "Weijia", ""], ["Chen", "Jarvis T.", ""], ["Jacqmin-Gadda", "H\u00e9l\u00e8ne", ""]]}, {"id": "2102.13278", "submitter": "Eric Lock", "authors": "Elise F. Palzer, Christine Wendt, Russell Bowler, Craig P. Hersh,\n  Sandra E. Safo, and Eric F. Lock", "title": "sJIVE: Supervised Joint and Individual Variation Explained", "comments": "23 pages, 8 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing multi-source data, which are multiple views of data on the same\nsubjects, has become increasingly common in molecular biomedical research.\nRecent methods have sought to uncover underlying structure and relationships\nwithin and/or between the data sources, and other methods have sought to build\na predictive model for an outcome using all sources. However, existing methods\nthat do both are presently limited because they either (1) only consider data\nstructure shared by all datasets while ignoring structures unique to each\nsource, or (2) they extract underlying structures first without consideration\nto the outcome. We propose a method called supervised joint and individual\nvariation explained (sJIVE) that can simultaneously (1) identify shared (joint)\nand source-specific (individual) underlying structure and (2) build a linear\nprediction model for an outcome using these structures. These two components\nare weighted to compromise between explaining variation in the multi-source\ndata and in the outcome. Simulations show sJIVE to outperform existing methods\nwhen large amounts of noise are present in the multi-source data. An\napplication to data from the COPDGene study reveals gene expression and\nproteomic patterns that are predictive of lung function. Functions to perform\nsJIVE are included in the R.JIVE package, available online at\nhttp://github.com/lockEF/r.jive .\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 02:54:45 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Palzer", "Elise F.", ""], ["Wendt", "Christine", ""], ["Bowler", "Russell", ""], ["Hersh", "Craig P.", ""], ["Safo", "Sandra E.", ""], ["Lock", "Eric F.", ""]]}, {"id": "2102.13287", "submitter": "Xiaoping Shi", "authors": "Xiaoping Shi, Meiqian Chen and Yucheng Dong", "title": "Exploring the space-time pattern of log-transformed infectious count of\n  COVID-19: a clustering-segmented autoregressive sigmoid model", "comments": "29 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  At the end of April 20, 2020, there were only a few new COVID-19 cases\nremaining in China, whereas the rest of the world had shown increases in the\nnumber of new cases. It is of extreme importance to develop an efficient\nstatistical model of COVID-19 spread, which could help in the global fight\nagainst the virus. We propose a clustering-segmented autoregressive sigmoid\n(CSAS) model to explore the space-time pattern of the log-transformed\ninfectious count. Four key characteristics are included in this CSAS model,\nincluding unknown clusters, change points, stretched S-curves, and\nautoregressive terms, in order to understand how this outbreak is spreading in\ntime and in space, to understand how the spread is affected by epidemic control\nstrategies, and to apply the model to updated data from an extended period of\ntime. We propose a nonparametric graph-based clustering method for discovering\ndissimilarity of the curve time series in space, which is justified with\ntheoretical support to demonstrate how the model works under mild and easily\nverified conditions. We propose a very strict purity score that penalizes\noverestimation of clusters. Simulations show that our nonparametric graph-based\nclustering method is faster and more accurate than the parametric clustering\nmethod regardless of the size of data sets. We provide a Bayesian information\ncriterion (BIC) to identify multiple change points and calculate a confidence\ninterval for a mean response. By applying the CSAS model to the collected data,\nwe can explain the differences between prevention and control policies in China\nand selected countries.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 03:08:50 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Shi", "Xiaoping", ""], ["Chen", "Meiqian", ""], ["Dong", "Yucheng", ""]]}, {"id": "2102.13299", "submitter": "Abhirup Datta", "authors": "Abhirup Datta", "title": "Sparse Cholesky matrices in spatial statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GP) is a staple in the toolkit of a spatial statistician.\nWell-documented computing roadblocks in the analysis of large geospatial\ndatasets using Gaussian Processes have now been successfully mitigated via\nseveral recent statistical innovations. Nearest Neighbor Gaussian Processes\n(NNGP) has emerged as one of the leading candidates for such massive-scale\ngeospatial analysis owing to their empirical success. This article reviews the\nconnection of NNGP to sparse Cholesky factors of the spatial precision\n(inverse-covariance) matrices. Focus of the review is on these sparse Cholesky\nmatrices which are versatile and have recently found many diverse applications\nbeyond the primary usage of NNGP for fast parameter estimation and prediction\nin the spatial (generalized) linear models. In particular, we discuss\napplications of sparse NNGP Cholesky matrices to address multifaceted\ncomputational issues in spatial bootstrapping, simulation of large-scale\nrealizations of Gaussian random fields, and extensions to non-parametric mean\nfunction estimation of a Gaussian Process using Random Forests. We also review\na sparse-Cholesky-based model for areal (geographically-aggregated) data that\naddresses interpretability issues of existing areal models. Finally, we\nhighlight some yet-to-be-addressed issues of such sparse Cholesky\napproximations that warrants further research.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 04:25:17 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Datta", "Abhirup", ""]]}, {"id": "2102.13384", "submitter": "Kailash Budhathoki", "authors": "Kailash Budhathoki, Dominik Janzing, Patrick Bloebaum, Hoiyi Ng", "title": "Why did the distribution change?", "comments": "Proceedings of the Twenty Fourth International Conference on\n  Artificial Intelligence and Statistics (AISTATS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a formal approach based on graphical causal models to identify\nthe \"root causes\" of the change in the probability distribution of variables.\nAfter factorizing the joint distribution into conditional distributions of each\nvariable, given its parents (the \"causal mechanisms\"), we attribute the change\nto changes of these causal mechanisms. This attribution analysis accounts for\nthe fact that mechanisms often change independently and sometimes only some of\nthem change. Through simulations, we study the performance of our distribution\nchange attribution method. We then present a real-world case study identifying\nthe drivers of the difference in the income distribution between men and women.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 10:22:59 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 07:53:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Budhathoki", "Kailash", ""], ["Janzing", "Dominik", ""], ["Bloebaum", "Patrick", ""], ["Ng", "Hoiyi", ""]]}, {"id": "2102.13435", "submitter": "Efstathia Bura", "authors": "Lukas Fertl and Efstathia Bura", "title": "Ensemble Conditional Variance Estimator for Sufficient Dimension\n  Reduction", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble Conditional Variance Estimation (ECVE) is a novel sufficient\ndimension reduction (SDR) method in regressions with continuous response and\npredictors. ECVE applies to general non-additive error regression models. It\noperates under the assumption that the predictors can be replaced by a lower\ndimensional projection without loss of information. It is a semiparametric\nforward regression model based exhaustive sufficient dimension reduction\nestimation method that is shown to be consistent under mild assumptions. It is\nshown to outperform central subspace mean average variance estimation (csMAVE),\nits main competitor, under several simulation settings and in a benchmark data\nset analysis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 12:47:39 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Fertl", "Lukas", ""], ["Bura", "Efstathia", ""]]}, {"id": "2102.13443", "submitter": "Leonhard Held", "authors": "Leonhard Held, Robert Matthews, Manuela Ott, Samuel Pawel", "title": "Reverse-Bayes methods for evidence assessment and research synthesis", "comments": "revised version of original manuscript \"Reverse-Bayes methods: a\n  review of recent technical advances\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now widely accepted that the standard inferential toolkit used by the\nscientific research community -- null-hypothesis significance testing (NHST) --\nis not fit for purpose. Yet despite the threat posed to the scientific\nenterprise, there is no agreement concerning alternative approaches for\nevidence assessment. This lack of consensus reflects long-standing issues\nconcerning Bayesian methods, the principal alternative to NHST. We report on\nrecent work that builds on an approach to inference put forward over 70 years\nago to address the well-known \"Problem of Priors\" in Bayesian analysis, by\nreversing the conventional prior-likelihood-posterior (\"forward\") use of\nBayes's Theorem. Such Reverse-Bayes analysis allows priors to be deduced from\nthe likelihood by requiring that the posterior achieve a specified level of\ncredibility. We summarise the technical underpinning of this approach, and show\nhow it opens up new approaches to common inferential challenges, such as\nassessing the credibility of scientific findings, setting them in appropriate\ncontext, estimating the probability of successful replications, and extracting\nmore insight from NHST while reducing the risk of misinterpretation. We argue\nthat Reverse-Bayes methods have a key role to play in making Bayesian methods\nmore accessible and attractive for evidence assessment and research synthesis.\nAs a running example we consider a recently published meta-analysis from\nseveral randomized controlled clinical trials investigating the association\nbetween corticosteroids and mortality in hospitalized patients with COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 12:59:52 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 07:32:07 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Held", "Leonhard", ""], ["Matthews", "Robert", ""], ["Ott", "Manuela", ""], ["Pawel", "Samuel", ""]]}, {"id": "2102.13467", "submitter": "Donggyu Kim", "authors": "Donggyu Kim, Yazhen Wang", "title": "Overnight GARCH-It\\^o Volatility Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various parametric volatility models for financial data have been developed\nto incorporate high-frequency realized volatilities and better capture market\ndynamics. However, because high-frequency trading data are not available during\nthe close-to-open period, the volatility models often ignore volatility\ninformation over the close-to-open period and thus may suffer from loss of\nimportant information relevant to market dynamics. In this paper, to account\nfor whole-day market dynamics, we propose an overnight volatility model based\non It\\^o diffusions to accommodate two different instantaneous volatility\nprocesses for the open-to-close and close-to-open periods. We develop a\nweighted least squares method to estimate model parameters for two different\nperiods and investigate its asymptotic properties. We conduct a simulation\nstudy to check the finite sample performance of the proposed model and method.\nFinally, we apply the proposed approaches to real trading data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 21:51:44 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kim", "Donggyu", ""], ["Wang", "Yazhen", ""]]}, {"id": "2102.13518", "submitter": "Thorsten Simon", "authors": "Thomas Muschinski, Georg J. Mayr, Thorsten Simon, Achim Zeileis", "title": "Cholesky-based multivariate Gaussian regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multivariate Gaussian regression is embedded into a general distributional\nregression framework using flexible additive predictors determining all\ndistributional parameters. While this is relatively straightforward for the\nmeans of the multivariate dependent variable, it is more challenging for the\nfull covariance matrix {\\Sigma} due to two main difficulties: (i) ensuring\npositive-definiteness of {\\Sigma} and (ii) regularizing the high model\ncomplexity. Both challenges are addressed by adopting a parameterization of\n{\\Sigma} based on its basic or modified Cholesky decomposition, respectively.\nUnlike the decomposition into variances and a correlation matrix, the Cholesky\ndecomposition guarantees positive-definiteness for any predictor values\nregardless of the distributional dimension. Thus, this enables linking all\ndistributional parameters to flexible predictors without any joint constraints\nthat would substantially complicate other parameterizations. Moreover, this\napproach enables regularization of the flexible additive predictors through\npenalized maximum likelihood or Bayesian estimation as for other distributional\nregression models. Finally, the Cholesky decomposition allows to reduce the\nnumber of parameters when the components of the multivariate dependent variable\nhave a natural order (typically time) and a maximum lag can be assumed for the\ndependencies among the components.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 14:46:35 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Muschinski", "Thomas", ""], ["Mayr", "Georg J.", ""], ["Simon", "Thorsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "2102.13548", "submitter": "Ronaldo Dias", "authors": "Larissa Alves and Ronaldo Dias and Helio S. Migon", "title": "Variational Full Bayes Lasso: Knots Selection in Regression Splines", "comments": "The authors contributed equally to the design and implementation of\n  the research, to the analysis of the results and to the writing of the\n  manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We develop a fully automatic Bayesian Lasso via variational inference. This\nis a scalable procedure for approximating the posterior distribution. Special\nattention is driven to the knot selection in regression spline. In order to\ncarry through our proposal, a full automatic variational Bayesian Lasso, a\nJefferey's prior is proposed for the hyperparameters and a decision theoretical\napproach is introduced to decide if a knot is selected or not. Extensive\nsimulation studies were developed to ensure the effectiveness of the proposed\nalgorithms. The performance of the algorithms were also tested in some real\ndata sets, including data from the world pandemic Covid-19. Again, the\nalgorithms showed a very good performance in capturing the data structure.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 15:41:28 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Alves", "Larissa", ""], ["Dias", "Ronaldo", ""], ["Migon", "Helio S.", ""]]}, {"id": "2102.13550", "submitter": "Madan Kundu", "authors": "Madan G. Kundu, Sandipan Samanta and Shoubhik Mondal", "title": "An introduction to the determination of the probability of a successful\n  trial: Frequentist and Bayesian approaches", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Determination of posterior probability for go-no-go decision and predictive\npower are becoming increasingly common for resource optimization in clinical\ninvestigation. There are vast published literature on these topics; however,\nthe terminologies are not consistently used across the literature. Further,\nthere is a lack of consolidated presentation of various concepts of the\nprobability of success. We attempted to fill this gap. This paper first\nprovides a detailed derivation of these probability of success measures under\nthe frequentist and Bayesian paradigms in a general setting. Subsequently, we\nhave presented the analytical formula for these probability of success measures\nfor continuous, binary, and time-to-event endpoints separately. This paper can\nbe used as a single point reference to determine the following measures: (a)\nthe conditional power (CP) based on interim results, (b) the predictive power\nof success (PPoS) based on interim results with or without prior distribution,\nand (d) the probability of success (PoS) for a prospective trial at the design\nstage. We have discussed both clinical success and trial success. This paper's\ndiscussion is mostly based on the normal approximation for prior distribution\nand the estimate of the parameter of interest. Besides, predictive power using\nthe beta prior for the binomial case is also presented. Some examples are given\nfor illustration. R functions to calculate CP and PPoS are available through\nthe LongCART package. An R shiny app is also available at\nhttps://ppos.herokuapp.com/.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 15:42:20 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kundu", "Madan G.", ""], ["Samanta", "Sandipan", ""], ["Mondal", "Shoubhik", ""]]}, {"id": "2102.13638", "submitter": "Marinho Bertanha", "authors": "Marinho Bertanha, EunYi Chung", "title": "Permutation Tests at Nonparametric Rates", "comments": "One file contains main paper (30 pages) and supplement (47 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical two-sample permutation tests for equality of distributions have\nexact size in finite samples, but they fail to control size for testing\nequality of parameters that summarize each distribution. This paper proposes\npermutation tests for equality of parameters that are estimated at root-n or\nslower rates. Our general framework applies to both parametric and\nnonparametric models, with two samples or one sample split into two subsamples.\nOur tests have correct size asymptotically while preserving exact size in\nfinite samples when distributions are equal. They have no loss in\nlocal-asymptotic power compared to tests that use asymptotic critical values.\nWe propose confidence sets with correct coverage in large samples that also\nhave exact coverage in finite samples if distributions are equal up to a\ntransformation. We apply our theory to four commonly-used hypothesis tests of\nnonparametric functions evaluated at a point. Lastly, simulations show good\nfinite sample properties of our tests.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 18:30:22 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Bertanha", "Marinho", ""], ["Chung", "EunYi", ""]]}, {"id": "2102.13647", "submitter": "Alexander Reisach", "authors": "Alexander G. Reisach, Christof Seiler, Sebastian Weichwald", "title": "Beware of the Simulated DAG! Varsortability in Additive Noise Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive noise models are a class of causal models in which each variable is\ndefined as a function of its causes plus independent noise. In such models, the\nordering of variables by marginal variances may be indicative of the causal\norder. We introduce varsortability as a measure of agreement between the\nordering by marginal variance and the causal order. We show how varsortability\ndominates the performance of continuous structure learning algorithms on\nsynthetic data. On real-world data, varsortability is an implausible and\nuntestable assumption and we find no indication of high varsortability. We aim\nto raise awareness that varsortability easily occurs in simulated additive\nnoise models. We provide a baseline method that explicitly exploits\nvarsortability and advocate reporting varsortability in benchmarking data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 18:52:27 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Reisach", "Alexander G.", ""], ["Seiler", "Christof", ""], ["Weichwald", "Sebastian", ""]]}]