[{"id": "1112.0152", "submitter": "Maurice Berk", "authors": "Maurice Berk and Giovanni Montana", "title": "A Skew-t-Normal Multi-Level Reduced-Rank Functional PCA Model with\n  Applications to Replicated `Omics Time Series Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A powerful study design in the fields of genomics and metabolomics is the\n'replicated time course experiment' where individual time series are observed\nfor a sample of biological units, such as human patients, termed replicates.\nStandard practice for analysing these data sets is to fit each variable (e.g.\ngene transcript) independently with a functional mixed-effects model to account\nfor between-replicate variance. However, such an independence assumption is\nbiologically implausible given that the variables are known to be highly\ncorrelated.\n  In this article we present a skew-t-normal multi-level reduced-rank\nfunctional principal components analysis (FPCA) model for simultaneously\nmodelling the between-variable and between-replicate variance. The reduced-rank\nFPCA model is computationally efficient and, analogously with a standard PCA\nfor vectorial data, provides a low dimensional representation that can be used\nto identify the major patterns of temporal variation. Using an example case\nstudy exploring the genetic response to BCG infection we demonstrate that these\nlow dimensional representations are eminently biologically interpretable. We\nalso show using a simulation study that modelling all variables simultaneously\ngreatly reduces the estimation error compared to the independence assumption.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 12:14:20 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["Berk", "Maurice", ""], ["Montana", "Giovanni", ""]]}, {"id": "1112.0514", "submitter": "Joan del Castillo", "authors": "Joan del Castillo, Jalila Daoudi and Richard Lockhart", "title": "Methods to distinguish between polynomial and exponential tails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article two methods to distinguish between polynomial and exponential\ntails are introduced. The methods are mainly based on the properties of the\nresidual coefficient of variation for the exponential and non-exponential\ndistributions. A graphical method, called CV-plot, shows departures from\nexponentiality in the tails. It is, in fact, the empirical coefficient of\nvariation of the conditional excedance over a threshold. The plot is applied to\nthe daily log-returns of exchange rates of US dollar and Japan yen. New\nstatistics are introduced for testing the exponentiality of tails using\nmultiple thresholds. Some simulation studies present the critical points and\ncompare them with the corresponding asymptotic critical points. Moreover, the\npowers of new statistics have been compared with the powers of some others\nstatistics for different sample size.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 17:25:37 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2011 11:33:35 GMT"}], "update_date": "2011-12-21", "authors_parsed": [["del Castillo", "Joan", ""], ["Daoudi", "Jalila", ""], ["Lockhart", "Richard", ""]]}, {"id": "1112.0712", "submitter": "Lixing Zhu", "authors": "Lu Lin, Lixing Zhu and Yujie Gai", "title": "Estimation and inference for high-dimensional non-sparse models", "comments": "This is a substantial revision of the manuscript Adaptive\n  post-Dantzig estimation and prediction for non-sparse \"large $p$ and small\n  $n$\" models [arXiv:1008.1345]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To successfully work on variable selection, sparse model structure has become\na basic assumption for all existing methods. However, this assumption is\nquestionable as it is hard to hold in most of cases and none of existing\nmethods may provide consistent estimation and accurate model prediction in\nnons-parse scenarios. In this paper, we propose semiparametric re-modeling and\ninference when the linear regression model under study is possibly non-sparse.\nAfter an initial working model is selected by a method such as the Dantzig\nselector adopted in this paper, we re-construct a globally unbiased\nsemiparametric model by use of suitable instrumental variables and\nnonparametric adjustment. The newly defined model is identifiable, and the\nestimator of parameter vector is asymptotically normal. The consistency,\ntogether with the re-built model, promotes model prediction. This method\nnaturally works when the model is indeed sparse and thus is of robustness\nagainst non-sparseness in certain sense. Simulation studies show that the new\napproach has, particularly when $p$ is much larger than $n$, significant\nimprovement of estimation and prediction accuracies over the Gaussian Dantzig\nselector and other classical methods. Even when the model under study is\nsparse, our method is also comparable to the existing methods designed for\nsparse models.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2011 02:23:05 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Lin", "Lu", ""], ["Zhu", "Lixing", ""], ["Gai", "Yujie", ""]]}, {"id": "1112.0716", "submitter": "Surya Tokdar Surya Tokdar", "authors": "Surya T. Tokdar", "title": "Dimension adaptability of Gaussian process models with variable\n  selection and projection", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now known that an extended Gaussian process model equipped with\nrescaling can adapt to different smoothness levels of a function valued\nparameter in many nonparametric Bayesian analyses, offering a posterior\nconvergence rate that is optimal (up to logarithmic factors) for the smoothness\nclass the true function belongs to. This optimal rate also depends on the\ndimension of the function's domain and one could potentially obtain a faster\nrate of convergence by casting the analysis in a lower dimensional subspace\nthat does not amount to any loss of information about the true function. In\ngeneral such a subspace is not known a priori but can be explored by equipping\nthe model with variable selection or linear projection. We demonstrate that for\nnonparametric regression, classification, density estimation and density\nregression, a rescaled Gaussian process model equipped with variable selection\nor linear projection offers a posterior convergence rate that is optimal (up to\nlogarithmic factors) for the lowest dimension in which the analysis could be\ncast without any loss of information about the true function. Theoretical\nexploration of such dimension reduction features appears novel for Bayesian\nnonparametric models with or without Gaussian processes.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2011 04:35:07 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Tokdar", "Surya T.", ""]]}, {"id": "1112.0840", "submitter": "Pavel N. Krivitsky", "authors": "Pavel N. Krivitsky, Eric D. Kolaczyk", "title": "On the Question of Effective Sample Size in Network Modeling: An\n  Asymptotic Inquiry", "comments": "Published at http://dx.doi.org/10.1214/14-STS502 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 184-198", "doi": "10.1214/14-STS502", "report-no": "IMS-STS-STS502", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling and analysis of networks and network data has seen an explosion\nof interest in recent years and represents an exciting direction for potential\ngrowth in statistics. Despite the already substantial amount of work done in\nthis area to date by researchers from various disciplines, however, there\nremain many questions of a decidedly foundational nature - natural analogues of\nstandard questions already posed and addressed in more classical areas of\nstatistics - that have yet to even be posed, much less addressed. Here we raise\nand consider one such question in connection with network modeling.\nSpecifically, we ask, \"Given an observed network, what is the sample size?\"\nUsing simple, illustrative examples from the class of exponential random graph\nmodels, we show that the answer to this question can very much depend on basic\nproperties of the networks expected under the model, as the number of vertices\n$n_V$ in the network grows. In particular, adopting the (asymptotic) scaling of\nthe variance of the maximum likelihood parameter estimates as a notion of\neffective sample size ($n_{\\mathrm{eff}}$), we show that when modeling the\noverall propensity to have ties and the propensity to reciprocate ties, whether\nthe networks are sparse or not under the model (i.e., having a constant or an\nincreasing number of ties per vertex, respectively) is sufficient to yield an\norder of magnitude difference in $n_{\\mathrm{eff}}$, from $O(n_V)$ to\n$O(n^2_V)$. In addition, we report simulation study results that suggest\nsimilar properties for models for triadic (friend-of-a-friend) effects. We then\nexplore some practical implications of this result, using both simulation and\ndata on food-sharing from Lamalera, Indonesia.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 05:53:34 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2012 04:40:58 GMT"}, {"version": "v3", "created": "Sun, 21 Oct 2012 23:27:50 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2015 06:08:02 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Krivitsky", "Pavel N.", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "1112.0918", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "On best subset regression", "comments": "This paper has been withdrawn by the author. A related paper entitled\n  \"Better subset regression\" is arXiv:1212.0634", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss the variable selection method from \\ell0-norm\nconstrained regression, which is equivalent to the problem of finding the best\nsubset of a fixed size. Our study focuses on two aspects, consistency and\ncomputation. We prove that the sparse estimator from such a method can retain\nall of the important variables asymptotically for even exponentially growing\ndimensionality under regularity conditions. This indicates that the best subset\nregression method can efficiently shrink the full model down to a submodel of a\nsize less than the sample size, which can be analyzed by well-developed\nregression techniques for such cases in a follow-up study. We provide an\niterative algorithm, called orthogonalizing subset selection (OSS), to address\ncomputational issues in best subset regression. OSS is an EM algorithm, and\nthus possesses the monotonicity property. For any sparse estimator, OSS can\nimprove its fit of the model by putting it as an initial point. After this\nimprovement, the sparsity of the estimator is kept. Another appealing feature\nof OSS is that, similarly to an effective algorithm for a continuous\noptimization problem, OSS can converge to the global solution to the \\ell0-norm\nconstrained regression problem if the initial point lies in a neighborhood of\nthe global solution. An accelerating algorithm of OSS and its combination with\nforward stepwise selection are also investigated. Simulations and a real\nexample are presented to evaluate the performances of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 13:13:25 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 07:29:32 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1112.0929", "submitter": "Arthur Charpentier", "authors": "Mathieu Boudreault and Arthur Charpentier", "title": "Multivariate integer-valued autoregressive models applied to earthquake\n  counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various situations in the insurance industry, in finance, in epidemiology,\netc., one needs to represent the joint evolution of the number of occurrences\nof an event. In this paper, we present a multivariate integer-valued\nautoregressive (MINAR) model, derive its properties and apply the model to\nearthquake occurrences across various pairs of tectonic plates. The model is an\nextension of Pedelis & Karlis (2011) where cross autocorrelation (spatial\ncontagion in a seismic context) is considered. We fit various bivariate count\nmodels and find that for many contiguous tectonic plates, spatial contagion is\nsignificant in both directions. Furthermore, ignoring cross autocorrelation can\nunderestimate the potential for high numbers of occurrences over the\nshort-term. Our overall findings seem to further confirm Parsons & Velasco\n(2001).\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 14:11:03 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Boudreault", "Mathieu", ""], ["Charpentier", "Arthur", ""]]}, {"id": "1112.1868", "submitter": "Matthias Troffaes", "authors": "Matthias C. M. Troffaes and John Paul Gosling", "title": "Robust detection of exotic infectious diseases in animal herds: A\n  comparative study of three decision methodologies under severe uncertainty", "comments": "14 pages, 5 figures, 4 tables; v2: 15 pages, minor corrections,\n  references added", "journal-ref": "International Journal of Approximate Reasoning 53 (2012) 1271-1281", "doi": "10.1016/j.ijar.2012.06.020", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When animals are transported and pass through customs, some of them may have\ndangerous infectious diseases. Typically, due to the cost of testing, not all\nanimals are tested: a reasonable selection must be made. How to test\neffectively whilst avoiding costly disease outbreaks? First, we extend a model\nproposed in the literature for the detection of invasive species to suit our\npurpose. Secondly, we explore and compare three decision methodologies on the\nproblem at hand, namely, Bayesian statistics, info-gap theory and imprecise\nprobability theory, all of which are designed to handle severe uncertainty. We\nshow that, under rather general conditions, every info-gap solution is maximal\nwith respect to a suitably chosen imprecise probability model, and that\ntherefore, perhaps surprisingly, the set of maximal options can be inferred at\nleast partly---and sometimes entirely---from an info-gap analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 16:12:03 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2012 11:29:10 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Troffaes", "Matthias C. M.", ""], ["Gosling", "John Paul", ""]]}, {"id": "1112.1869", "submitter": "Maurice Berk", "authors": "Maurice Berk and Giovanni Montana", "title": "Functional modelling of microarray time series with covariate curves", "comments": null, "journal-ref": "Statistica 2-3 (2009) 153-177", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have demonstrated a complete framework for the analysis of\nmicroarray time series data. The unique characteristics of microarry data lend\nthemselves well to a functional data analysis approach and we have shown how\nthis naturally extends to the inclusion of covariates such as age and sex. Our\nmodel presented here is a specialisation of the more general functional\nmixed-effects model and, to the best of our knowledge, we are the first to show\nhow to derive the maximum-likelihood estimators, EM-algorithm, confidence\nintervals and smoother matrix with more than one fixed-effects function.\n  We were motivated by a real data set characterising healthy human gene\nexpression levels over time and we have aimed to improve upon the existing\nresults with a more flexible model. By taking a roughness penalty approach,\nthis is achieved while avoiding overfitting, allowing for a departure from the\noriginal linear mixed-effects model when the data permits it. A deeper\nbiological interpretation is required to fully assess our success here, but the\nresults we have highlighted in this paper suggest that we can easily attach\nmeaning to our findings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 16:23:10 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Berk", "Maurice", ""], ["Montana", "Giovanni", ""]]}, {"id": "1112.1977", "submitter": "Tucker S. McElroy", "authors": "Tucker S. McElroy, Scott H. Holan", "title": "Asymptotic theory of cepstral random fields", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1180 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 1, 64-86", "doi": "10.1214/13-AOS1180", "report-no": "IMS-AOS-AOS1180", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random fields play a central role in the analysis of spatially correlated\ndata and, as a result, have a significant impact on a broad array of scientific\napplications. This paper studies the cepstral random field model, providing\nrecursive formulas that connect the spatial cepstral coefficients to an\nequivalent moving-average random field, which facilitates easy computation of\nthe autocovariance matrix. We also provide a comprehensive treatment of the\nasymptotic theory for two-dimensional random field models: we establish\nasymptotic results for Bayesian, maximum likelihood and quasi-maximum\nlikelihood estimation of random field parameters and regression parameters. The\ntheoretical results are presented generally and are of independent interest,\npertaining to a wide class of random field models. The results for the cepstral\nmodel facilitate model-building: because the cepstral coefficients are\nunconstrained in practice, numerical optimization is greatly simplified, and we\nare always guaranteed a positive definite covariance matrix. We show that\ninference for individual coefficients is possible, and one can refine models in\na disciplined manner. Our results are illustrated through simulation and the\nanalysis of straw yield data in an agricultural field experiment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 22:39:02 GMT"}, {"version": "v2", "created": "Fri, 18 May 2012 21:07:08 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2013 19:26:50 GMT"}, {"version": "v4", "created": "Thu, 16 Jan 2014 12:09:11 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["McElroy", "Tucker S.", ""], ["Holan", "Scott H.", ""]]}, {"id": "1112.2330", "submitter": "Georgiy Shevchenko", "authors": "Yuriy Kozachenko and Alexander Melnikov and Yuliya Mishura", "title": "On drift parameter estimation in models with fractional Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic differential equation involving standard and\nfractional Brownian motion with unknown drift parameter to be estimated. We\ninvestigate the standard maximum likelihood estimate of the drift parameter,\ntwo non-standard estimates and three estimates for the sequential estimation.\nModel strong consistency and some other properties are proved. The linear model\nand Ornstein-Uhlenbeck model are studied in detail. As an auxiliary result, an\nasymptotic behavior of the fractional derivative of the fractional Brownian\nmotion is established.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 07:44:03 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Kozachenko", "Yuriy", ""], ["Melnikov", "Alexander", ""], ["Mishura", "Yuliya", ""]]}, {"id": "1112.2432", "submitter": "Zongming Ma", "authors": "Zongming Ma", "title": "Sparse principal component analysis and iterative thresholding", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1097 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 2, 772-801", "doi": "10.1214/13-AOS1097", "report-no": "IMS-AOS-AOS1097", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a classical dimension reduction method\nwhich projects data onto the principal subspace spanned by the leading\neigenvectors of the covariance matrix. However, it behaves poorly when the\nnumber of features p is comparable to, or even much larger than, the sample\nsize n. In this paper, we propose a new iterative thresholding approach for\nestimating principal subspaces in the setting where the leading eigenvectors\nare sparse. Under a spiked covariance model, we find that the new approach\nrecovers the principal subspace and leading eigenvectors consistently, and even\noptimally, in a range of high-dimensional sparse settings. Simulated examples\nalso demonstrate its competitive performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 03:47:59 GMT"}, {"version": "v2", "created": "Fri, 24 May 2013 10:35:26 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Ma", "Zongming", ""]]}, {"id": "1112.2433", "submitter": "Dan Yang", "authors": "Dan Yang, Zongming Ma and Andreas Buja", "title": "A Sparse SVD Method for High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new computational approach to approximating a large, noisy data\ntable by a low-rank matrix with sparse singular vectors. The approximation is\nobtained from thresholded subspace iterations that produce the singular vectors\nsimultaneously, rather than successively as in competing proposals. We\nintroduce novel ways to estimate thresholding parameters which obviate the need\nfor computationally expensive cross-validation. We also introduce a way to\nsparsely initialize the algorithm for computational savings that allow our\nalgorithm to outperform the vanilla SVD on the full data table when the signal\nis sparse. A comparison with two existing sparse SVD methods suggests that our\nalgorithm is computationally always faster and statistically always at least\ncomparable to the better of the two competing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 03:52:57 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Yang", "Dan", ""], ["Ma", "Zongming", ""], ["Buja", "Andreas", ""]]}, {"id": "1112.2615", "submitter": "Korbinian Strimmer", "authors": "Bernd Klaus and Korbinian Strimmer", "title": "Signal Identification for Rare and Weak Features: Higher Criticism or\n  False Discovery Rates?", "comments": "19 pages, 3 figures, 2 tables", "journal-ref": "Biostatistics 2013, Vol. 14, 129-143", "doi": "10.1093/biostatistics/kxs030", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal identification in large-dimensional settings is a challenging problem\nin biostatistics. Recently, the method of higher criticism (HC) was shown to be\nan effective means for determining appropriate decision thresholds. Here, we\nstudy HC from a false discovery rate (FDR) perspective. We show that the HC\nthreshold may be viewed as an approximation to a natural class boundary (CB) in\ntwo-class discriminant analysis which in turn is expressible as FDR threshold.\nWe demonstrate that in a rare-weak setting in the region of the phase space\nwhere signal identification is possible both thresholds are practicably\nindistinguishable, and thus HC thresholding is identical to using a simple\nlocal FDR cutoff. The relationship of the HC and CB thresholds and their\nproperties are investigated both analytically and by simulations, and are\nfurther compared by application to four cancer gene expression data sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 16:55:59 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2012 13:43:12 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Klaus", "Bernd", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1112.2680", "submitter": "Rob Hall", "authors": "Rob Hall, Alessandro Rinaldo, Larry Wasserman", "title": "Random Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a relaxed privacy definition called {\\em random differential\nprivacy} (RDP). Differential privacy requires that adding any new observation\nto a database will have small effect on the output of the data-release\nprocedure. Random differential privacy requires that adding a {\\em randomly\ndrawn new observation} to a database will have small effect on the output. We\nshow an analog of the composition property of differentially private procedures\nwhich applies to our new definition. We show how to release an RDP histogram\nand we show that RDP histograms are much more accurate than histograms obtained\nusing ordinary differential privacy. We finally show an analog of the global\nsensitivity framework for the release of functions under our privacy\ndefinition.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 20:16:03 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Hall", "Rob", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1112.2682", "submitter": "Hailin Sang", "authors": "Hailin Sang and Yan Sun", "title": "Simultaneous sparse model selection and coefficient estimation for\n  heavy-tailed autoregressive processes", "comments": "to appear in Statistics; 26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sparse coefficient estimation and automated model selection\nprocedure for autoregressive (AR) processes with heavy-tailed innovations based\non penalized conditional maximum likelihood. Under mild moment conditions on\nthe innovation processes, the penalized conditional maximum likelihood\nestimator (PCMLE) satisfies a strong consistency, $O_P(N^{-1/2})$ consistency,\nand the oracle properties, where N is the sample size. We have the freedom in\nchoosing penalty functions based on the weak conditions on them. Two penalty\nfunctions, least absolute shrinkage and selection operator (LASSO) and smoothly\nclipped average deviation (SCAD), are compared. The proposed method provides a\ndistribution-based penalized inference to AR models, which is especially useful\nwhen the other estimation methods fail or under perform for AR processes with\nheavy-tailed innovations (see \\cite{Resnick}). A simulation study confirms our\ntheoretical results. At the end, we apply our method to a historical price data\nof the US Industrial Production Index for consumer goods, and obtain very\npromising results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 20:20:24 GMT"}, {"version": "v2", "created": "Wed, 2 May 2012 02:14:48 GMT"}, {"version": "v3", "created": "Sat, 21 Sep 2013 04:06:08 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Sang", "Hailin", ""], ["Sun", "Yan", ""]]}, {"id": "1112.2720", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "A new method for deriving incidence rates from prevalence data and its\n  application to dementia in Germany", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper descibes a new method for deriving incidence rates of a chronic\ndisease from prevalence data. It is based on a new ordinary differential\nequation, which relates the change in the age-specific prevalence to the\nagespecific incidence and mortality rates. The method allows the extraction of\nlongtudinal information from cross-sectional studies. Applicability of the\nmethod is tested in the prevalence of dementia in Germany. The derived\nage-specific incidence is in good agreement with published values.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 21:15:06 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1112.3373", "submitter": "Subhadeep Mukhopadhyay", "authors": "S. Mukhopadhyay, Emanuel Parzen and S. N. Lahiri", "title": "Quantile Based Variable Mining : Detection, FDR based Extraction and\n  Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines a unified framework for high dimensional variable\nselection for classification problems. Traditional approaches to finding\ninteresting variables mostly utilize only partial information through moments\n(like mean difference). On the contrary, in this paper we address the question\nof variable selection in full generality from a distributional point of view.\nIf a variable is not important for classification, then it will have similar\ndistributional aspect under different classes. This simple and straightforward\nobservation motivates us to quantify `How and Why' the distribution of a\nvariable changes over classes through CR-statistic. The second contribution of\nour paper is to develop and investigate the FDR based thresholding technology\nfrom a completely new point of view for adaptive thresholding, which leads to a\nelegant algorithm called CDfdr. This paper attempts to show how all of these\nproblems of detection, extraction and interpretation for interesting variables\ncan be treated in a unified way under one broad general theme - comparison\nanalysis. It is proposed that a key to accomplishing this unification is to\nthink in terms of the quantile function and the comparison density. We\nillustrate and demonstrate the power of our methodology using three real data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 22:07:09 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Mukhopadhyay", "S.", ""], ["Parzen", "Emanuel", ""], ["Lahiri", "S. N.", ""]]}, {"id": "1112.3605", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou and Lauren Hannah and David Dunson and Lawrence Carin", "title": "Beta-Negative Binomial Process and Poisson Factor Analysis", "comments": "Appearing in AISTATS 2012 (submitted on Oct. 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A beta-negative binomial (BNB) process is proposed, leading to a\nbeta-gamma-Poisson process, which may be viewed as a \"multi-scoop\"\ngeneralization of the beta-Bernoulli process. The BNB process is augmented into\na beta-gamma-gamma-Poisson hierarchical structure, and applied as a\nnonparametric Bayesian prior for an infinite Poisson factor analysis model. A\nfinite approximation for the beta process Levy random measure is constructed\nfor convenient implementation. Efficient MCMC computations are performed with\ndata augmentation and marginalization techniques. Encouraging results are shown\non document count matrix factorization.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2011 18:59:45 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2011 19:02:44 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2012 03:46:24 GMT"}, {"version": "v4", "created": "Sat, 4 Feb 2012 16:43:41 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Hannah", "Lauren", ""], ["Dunson", "David", ""], ["Carin", "Lawrence", ""]]}, {"id": "1112.3735", "submitter": "Len Bos", "authors": "T. Bloom, L. Bos and N. Levenberg", "title": "The Asymptotics of Optimal Designs for Polynomial Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the asymptotics for D-optimal (equivalently G-optimal) designs on a\ncompact (possibly complex) design space.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 08:40:14 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Bloom", "T.", ""], ["Bos", "L.", ""], ["Levenberg", "N.", ""]]}, {"id": "1112.3891", "submitter": "Mathieu Ribatet", "authors": "Cl\\'ement Dombry, Fr\\'ed\\'eric \\'Eyi-Minko and Mathieu Ribatet", "title": "Conditional simulations of Brown-Resnick processes", "comments": "This paper has been withdrawn since it was merged with another one as\n  suggested by the referees during the review process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since many environmental processes such as heat waves or precipitation are\nspatial in extent, it is likely that a single extreme event affects several\nlocations and the areal modeling of extremes is therefore essential if the\nspatial dependence of extremes has to be appropriately taken into account.\nAlthough some progress has been made to develop a geostatistic of extremes,\nconditional simulation of max-stable processes is still in its early stage.\nThis paper proposes a framework to get conditional simulations of Brown-Resnick\nprocesses. Although closed forms for the regular conditional distribution of\nBrown-Resnick processes were recently found, sampling from this conditional\ndistribution is a considerable challenge as it leads quickly to a combinatorial\nexplosion. To bypass this computational burden, a Markov chain Monte-Carlo\nalgorithm is presented. We test the method on simulated data and give an\napplication to extreme rainfall around Zurich. Results show that the proposed\nframework provides accurate conditional simulations of Brown-Resnick processes\nand can handle real-sized problems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 17:03:04 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2012 12:54:34 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Dombry", "Cl\u00e9ment", ""], ["\u00c9yi-Minko", "Fr\u00e9d\u00e9ric", ""], ["Ribatet", "Mathieu", ""]]}, {"id": "1112.4048", "submitter": "Luca Martino", "authors": "Luca Martino and Victor Pascual Del Olmo and Jesse Read", "title": "A multi-point Metropolis scheme with generic weight functions", "comments": null, "journal-ref": "Statistics & Probability Letters, Volume 82, Issue 7, Pages\n  1445-1453, 2012", "doi": "10.1016/j.spl.2012.04.008", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-point Metropolis algorithm is an advanced MCMC technique based on\ndrawing several correlated samples at each step and choosing one of them\naccording to some normalized weights. We propose a variation of this technique\nwhere the weight functions are not specified, i.e., the analytic form can be\nchosen arbitrarily. This has the advantage of greater flexibility in the design\nof high-performance MCMC samplers. We prove that our method fulfills the\nbalance condition, and provide a numerical simulation. We also give new insight\ninto the functionality of different MCMC algorithms, and the connections\nbetween them.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2011 11:47:30 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2011 11:18:23 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2012 23:31:17 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Martino", "Luca", ""], ["Del Olmo", "Victor Pascual", ""], ["Read", "Jesse", ""]]}, {"id": "1112.4118", "submitter": "Leo Stein", "authors": "Michael Betancourt and Leo C. Stein", "title": "The Geometry of Hamiltonian Monte Carlo", "comments": "9 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With its systematic exploration of probability distributions, Hamiltonian\nMonte Carlo is a potent Markov Chain Monte Carlo technique; it is an approach,\nhowever, ultimately contingent on the choice of a suitable Hamiltonian\nfunction. By examining both the symplectic geometry underlying Hamiltonian\ndynamics and the requirements of Markov Chain Monte Carlo, we construct the\ngeneral form of admissible Hamiltonians and propose a particular choice with\npotential application in Bayesian inference.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 06:15:19 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Betancourt", "Michael", ""], ["Stein", "Leo C.", ""]]}, {"id": "1112.4138", "submitter": "Vladimir Minin", "authors": "Julia A. Palacios and Vladimir N. Minin", "title": "Gaussian Process-Based Bayesian Nonparametric Inference of Population\n  Trajectories from Gene Genealogies", "comments": "25 pages with 7 figures, revised version; added total variation\n  metric to compare methods; influenza example updated with new GP results,\n  added discussion and implementation of alternative GP kernels (OU and sparse\n  approximation of the integrated Brownian motion)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changes in population size influence genetic diversity of the population and,\nas a result, leave a signature of these changes in individual genomes in the\npopulation. We are interested in the inverse problem of reconstructing past\npopulation dynamics from genomic data. We start with a standard framework based\non the coalescent, a stochastic process that generates genealogies connecting\nrandomly sampled individuals from the population of interest. These genealogies\nserve as a glue between the population demographic history and genomic\nsequences. It turns out that only the times of genealogical lineage\ncoalescences contain information about population size dynamics. Viewing these\ncoalescent times as a point process, estimating population size trajectories is\nequivalent to estimating a conditional intensity of this point process.\nTherefore, our inverse problem is similar to estimating an inhomogeneous\nPoisson process intensity function. We demonstrate how recent advances in\nGaussian process-based nonparametric inference for Poisson processes can be\nextended to Bayesian nonparametric estimation of population size dynamics under\nthe coalescent. We compare our Gaussian process (GP) approach to one of the\nstate of the art Gaussian Markov random field (GMRF) methods for estimating\npopulation trajectories. Using simulated data, we demonstrate that our method\nhas better accuracy and precision. Next, we analyze two genealogies\nreconstructed from real sequences of hepatitis C and human Influenza A viruses.\nIn both cases, we recover more believed aspects of the viral demographic\nhistories than the GMRF approach. We also find that our GP method produces more\nreasonable uncertainty estimates than the GMRF method.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 09:31:31 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2012 22:10:57 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Palacios", "Julia A.", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1112.4204", "submitter": "Michael Smith", "authors": "Michael Stanley Smith", "title": "Bayesian Approaches to Copula Modelling", "comments": null, "journal-ref": null, "doi": "10.1093/acprof:oso/9780199695607.001.0001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula models have become one of the most widely used tools in the applied\nmodelling of multivariate data. Similarly, Bayesian methods are increasingly\nused to obtain efficient likelihood-based inference. However, to date, there\nhas been only limited use of Bayesian approaches in the formulation and\nestimation of copula models. This article aims to address this shortcoming in\ntwo ways. First, to introduce copula models and aspects of copula theory that\nare especially relevant for a Bayesian analysis. Second, to outline Bayesian\napproaches to formulating and estimating copula models, and their advantages\nover alternative methods. Copulas covered include Archimedean, copulas\nconstructed by inversion, and vine copulas; along with their interpretation as\ntransformations. A number of parameterisations of a correlation matrix of a\nGaussian copula are considered, along with hierarchical priors that allow for\nBayesian selection and model averaging for each parameterisation. Markov chain\nMonte Carlo sampling schemes for fitting Gaussian and D-vine copulas, with and\nwithout selection, are given in detail. The relationship between the prior for\nthe parameters of a D-vine, and the prior for a correlation matrix of a\nGaussian copula, is discussed. Last, it is shown how to compute Bayesian\ninference when the data are discrete-valued using data augmentation. This\napproach generalises popular Bayesian methods for the estimation of models for\nmultivariate binary and other ordinal data to more general copula models.\nBayesian data augmentation has substantial advantages over other methods of\nestimation for this class of models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 23:59:28 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Smith", "Michael Stanley", ""]]}, {"id": "1112.4213", "submitter": "Giles Hooker", "authors": "Giles Hooker and Anand Vidyashankar", "title": "Bayesian Model Robustness via Disparities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a methodology for robust Bayesian inference through the\nuse of disparities. Metrics such as Hellinger distance and negative exponential\ndisparity have a long history in robust estimation in frequentist inference. We\ndemonstrate that an equivalent robustification may be made in Bayesian\ninference by substituting an appropriately scaled disparity for the log\nlikelihood to which standard Monte Carlo Markov Chain methods may be applied. A\nparticularly appealing property of minimum-disparity methods is that while they\nyield robustness with a breakdown point of 1/2, the resulting parameter\nestimates are also efficient when the posited probabilistic model is correct.\nWe demonstrate that a similar property holds for disparity-based Bayesian\ninference. We further show that in the Bayesian setting, it is also possible to\nextend these methods to robustify regression models, random effects\ndistributions and other hierarchical models. The methods are demonstrated on\nreal world data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 02:06:05 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 21:42:26 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Hooker", "Giles", ""], ["Vidyashankar", "Anand", ""]]}, {"id": "1112.4321", "submitter": "Mohit Dayal", "authors": "Mohit Dayal", "title": "A New Algorithm for Exploratory Projection Pursuit", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new algorithm for exploratory projection pursuit.\nThe basis of the algorithm is the insight that previous approaches used fairly\nnarrow definitions of interestingness / non interestingness. We argue that\nallowing these definitions to depend on the problem / data at hand is a more\nnatural approach in an exploratory technique. This also allows our technique\nmuch greater applicability than the approaches extant in the literature.\nComplementing this insight, we propose a class of projection indices based on\nthe spatial distribution function that can make use of such information.\n  Finally, with the help of real datasets, we demonstrate how a range of\nmultivariate exploratory tasks can be addressed with our algorithm. The\nexamples further demonstrate that the proposed indices are quite capable of\nfocussing on the interesting structure in the data, even when this structure is\notherwise hard to detect or arises from very subtle patterns.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 12:44:03 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Dayal", "Mohit", ""]]}, {"id": "1112.4627", "submitter": "Ruth Heller", "authors": "Ruth Heller and Hadas Gur", "title": "False discovery rate controlling procedures for discrete tests", "comments": "Withdrawn due to error in the proof of proposition 2.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benjamini and Hochberg (1995) proposed the false discovery rate (FDR) as an\nalternative to the family-wise error rate in multiple testing problems, and\nproposed a procedure to control the FDR. For discrete data this procedure may\nbe highly conservative. We investigate alternative, more powerful, procedures\nthat exploit the discreteness of the tests and have FDR levels closer in\nmagnitude to the desired nominal level. Moreover, we develop a novel step-down\nprocedure that dominates the step-down procedure of Benjamini and Liu (1999)\nfor discrete data. We consider an application to pharmacovigilance spontaneous\nreporting systems, that serve for early detection of adverse reactions of\nmarketed drugs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 09:49:33 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2012 12:11:35 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 14:17:48 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Heller", "Ruth", ""], ["Gur", "Hadas", ""]]}, {"id": "1112.4755", "submitter": "Scott Sisson", "authors": "D. J. Nott, Y. Fan, L. Marshall, S. A. Sisson", "title": "Approximate Bayesian computation and Bayes linear analysis: Towards\n  high-dimensional ABC", "comments": "To appear in Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes linear analysis and approximate Bayesian computation (ABC) are\ntechniques commonly used in the Bayesian analysis of complex models. In this\narticle we connect these ideas by demonstrating that regression-adjustment ABC\nalgorithms produce samples for which first and second order moment summaries\napproximate adjusted expectation and variance for a Bayes linear analysis. This\ngives regression-adjustment methods a useful interpretation and role in\nexploratory analysis in high-dimensional problems. As a result, we propose a\nnew method for combining high-dimensional, regression-adjustment ABC with\nlower-dimensional approaches (such as using MCMC for ABC). This method first\nobtains a rough estimate of the joint posterior via regression-adjustment ABC,\nand then estimates each univariate marginal posterior distribution separately\nin a lower-dimensional analysis. The marginal distributions of the initial\nestimate are then modified to equal the separately estimated marginals, thereby\nproviding an improved estimate of the joint posterior. We illustrate this\nmethod with several examples. Supplementary materials for this article are\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 16:30:09 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 03:02:51 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Nott", "D. J.", ""], ["Fan", "Y.", ""], ["Marshall", "L.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1112.5016", "submitter": "Ariel Kleiner", "authors": "Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael I. Jordan", "title": "A Scalable Bootstrap for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bootstrap provides a simple and powerful means of assessing the quality\nof estimators. However, in settings involving large datasets---which are\nincreasingly prevalent---the computation of bootstrap-based quantities can be\nprohibitively demanding computationally. While variants such as subsampling and\nthe $m$ out of $n$ bootstrap can be used in principle to reduce the cost of\nbootstrap computations, we find that these methods are generally not robust to\nspecification of hyperparameters (such as the number of subsampled data\npoints), and they often require use of more prior information (such as rates of\nconvergence of estimators) than the bootstrap. As an alternative, we introduce\nthe Bag of Little Bootstraps (BLB), a new procedure which incorporates features\nof both the bootstrap and subsampling to yield a robust, computationally\nefficient means of assessing the quality of estimators. BLB is well suited to\nmodern parallel and distributed computing architectures and furthermore retains\nthe generic applicability and statistical efficiency of the bootstrap. We\ndemonstrate BLB's favorable statistical performance via a theoretical analysis\nelucidating the procedure's properties, as well as a simulation study comparing\nBLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In\naddition, we present results from a large-scale distributed implementation of\nBLB demonstrating its computational superiority on massive data, a method for\nadaptively selecting BLB's hyperparameters, an empirical study applying BLB to\nseveral real datasets, and an extension of BLB to time series data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 13:18:57 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2012 03:30:16 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Kleiner", "Ariel", ""], ["Talwalkar", "Ameet", ""], ["Sarkar", "Purnamrita", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1112.5130", "submitter": "Carlo Berzuini", "authors": "Carlo Berzuini, Stijn Vansteelandt, Luisa Foco, Roberta Pastorino and\n  Luisa Bernardinelli", "title": "Direct genetic effects and their estimation from matched case-control\n  data", "comments": "20 pages, including two Appendices, two figures, four tables and 41\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetic association studies, a single marker is often associated with\nmultiple, correlated phenotypes (e.g., obesity and cardiovascular disease, or\nnicotine dependence and lung cancer). A pervasive question is then whether that\nmarker has independent effects on all phenotypes. In this article, we address\nthis question by assessing whether there is a direct genetic effect on one\nphenotype that is not mediated through the other phenotypes. In particular, we\ninvestigate how to identify and estimate such direct genetic effects on the\nbasis of (matched) case-control data. We discuss conditions under which such\neffects are identifiable from the available (matched) case-control data. We\nfind that direct genetic effects are sometimes estimable via standard\nregression methods, and sometimes via a more general G-estimation method, which\nhas previously been proposed for random samples and unmatched case-control\nstudies (Vansteelandt, 2009) and is here extended to matched case-control\nstudies. The results are used to assess whether the FTO gene is associated with\nmyocardial infarction other than via an effect on obesity.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 18:48:40 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Berzuini", "Carlo", ""], ["Vansteelandt", "Stijn", ""], ["Foco", "Luisa", ""], ["Pastorino", "Roberta", ""], ["Bernardinelli", "Luisa", ""]]}, {"id": "1112.5304", "submitter": "Carlo Albert", "authors": "Carlo Albert", "title": "A Mechanistic Dynamic Emulator", "comments": "12 pages, 3 figures", "journal-ref": "J. Nonlinear Analysis B 13, 2747-2754 (2012)", "doi": "10.1016/j.nonrwa.2012.04.003", "report-no": null, "categories": "stat.ME physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applied sciences, we often deal with deterministic simulation models that\nare too slow for simulation-intensive tasks such as calibration or real-time\ncontrol. In this paper, an emulator for a generic dynamic model, given by a\nsystem of ordinary non-linear differential equations, is developed. The\nnon-linear differential equations are linearized and Gaussian white noise is\nadded to account for the non-linearities. The resulting linear stochastic\nsystem is conditioned on a set of solutions of the non-linear equations that\nhave been calculated prior to the emulation. A path-integral approach is used\nto derive the Gaussian distribution of the emulated solution. The solution\nreveals that most of the computational burden can be shifted to the\nconditioning phase of the emulator and the complexity of the actual emulation\nstep only scales like $\\mathcal O(Nn)$ in multiplications of matrices of the\ndimension of the state space. Here, $N$ is the number of time-points at which\nthe solution is to be emulated and $n$ the number of solutions the emulator is\nconditioned on.\n  The applicability of the algorithm is demonstrated with the hydrological\nmodel logSPM.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 13:27:06 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2012 13:46:06 GMT"}], "update_date": "2012-07-06", "authors_parsed": [["Albert", "Carlo", ""]]}, {"id": "1112.5510", "submitter": "Joshua Vogelstein", "authors": "Carey E. Priebe, David J. Marchette, Zhiliang Ma, Sancar Adali", "title": "Manifold Matching: Joint Optimization of Fidelity and Commensurability", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusion and inference from multiple and massive disparate data sources - the\nrequirement for our most challenging data analysis problems and the goal of our\nmost ambitious statistical pattern recognition methodologies - -has many and\nvaried aspects which are currently the target of intense research and\ndevelopment. One aspect of the overall challenge is manifold matching -\nidentifying embeddings of multiple disparate data spaces into the same\nlow-dimensional space where joint inference can be pursued. We investigate this\nmanifold matching task from the perspective of jointly optimizing the fidelity\nof the embeddings and their commensurability with one another, with a specific\nstatistical inference exploitation task in mind. Our results demonstrate when\nand why our joint optimization methodology is superior to either version of\nseparate optimization. The methodology is illustrated with simulations and an\napplication in document matching.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 03:09:40 GMT"}], "update_date": "2011-12-26", "authors_parsed": [["Priebe", "Carey E.", ""], ["Marchette", "David J.", ""], ["Ma", "Zhiliang", ""], ["Adali", "Sancar", ""]]}, {"id": "1112.5582", "submitter": "D. A. S. Fraser", "authors": "D. A. S. Fraser", "title": "Is Bayes Posterior just Quick and Dirty Confidence?", "comments": "Published in at http://dx.doi.org/10.1214/11-STS352 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 299-316", "doi": "10.1214/11-STS352", "report-no": "IMS-STS-STS352", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes [Philos. Trans. R. Soc. Lond. 53 (1763) 370--418; 54 296--325]\nintroduced the observed likelihood function to statistical inference and\nprovided a weight function to calibrate the parameter; he also introduced a\nconfidence distribution on the parameter space but did not provide present\njustifications. Of course the names likelihood and confidence did not appear\nuntil much later: Fisher [Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng.\nSci. 222 (1922) 309--368] for likelihood and Neyman [Philos. Trans. R. Soc.\nLond. Ser. A Math. Phys. Eng. Sci. 237 (1937) 333--380] for confidence. Lindley\n[J. Roy. Statist. Soc. Ser. B 20 (1958) 102--107] showed that the Bayes and the\nconfidence results were different when the model was not location. This paper\nexamines the occurrence of true statements from the Bayes approach and from the\nconfidence approach, and shows that the proportion of true statements in the\nBayes case depends critically on the presence of linearity in the model; and\nwith departure from this linearity the Bayes approach can be a poor\napproximation and be seriously misleading. Bayesian integration of weighted\nlikelihood thus provides a first-order linear approximation to confidence, but\nwithout linearity can give substantially incorrect results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 13:28:07 GMT"}], "update_date": "2011-12-26", "authors_parsed": [["Fraser", "D. A. S.", ""]]}, {"id": "1112.5854", "submitter": "Mohamed Cherfi", "authors": "Mohamed Cherfi", "title": "On Bayesian Estimation via Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Note we introduce a new methodology for Bayesian inference through\nthe use of $\\phi$-divergences and the duality technique. The asymptotic laws of\nthe estimates are established.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2011 13:04:36 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Cherfi", "Mohamed", ""]]}, {"id": "1112.5966", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "Calculation of the mean duration and age of onset of a chronic disease\n  and application to dementia in Germany", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper descibes a new method of calculating the mean duration and mean\nage of onset of a chronic disease from incidence and mortality rates. It is\nbased on an ordinary differential equation resulting from a simple compartment\nmodel. Applicability of the method is demonstrated in data about dementia in\nGermany.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 14:23:55 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1112.5969", "submitter": "Mikko Tuomi", "authors": "Mikko Tuomi and Hugh R. A. Jones", "title": "Probabilities of exoplanet signals from posterior samplings", "comments": "7 pages, 2 Figs. Accepted for publication in the Astronomy and\n  Astrophysics", "journal-ref": null, "doi": "10.1051/0004-6361/201118114", "report-no": null, "categories": "astro-ph.EP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the marginal likelihoods is an essential feature of model\nselection in the Bayesian context. It is especially crucial to have good\nestimates when assessing the number of planets orbiting stars when the models\nexplain the noisy data with different numbers of Keplerian signals. We\nintroduce a simple method for approximating the marginal likelihoods in\npractice when a statistically representative sample from the parameter\nposterior density is available.\n  We use our truncated posterior mixture estimate to receive accurate model\nprobabilities for models with differing number of Keplerian signals in radial\nvelocity data. We test this estimate in simple scenarios to assess its accuracy\nand rate of convergence in practice when the corresponding estimates calculated\nusing deviance information criterion can be applied to receive trustworthy\nresults for reliable comparison. As a test case, we determine the posterior\nprobability of a planet orbiting HD 3651 given Lick and Keck radial velocity\ndata.\n  The posterior mixture estimate appears to be a simple and an accurate way of\ncalculating marginal integrals from posterior samples. We show, that it can be\nused to estimate the marginal integrals reliably in practice, given a suitable\nselection of parameter \\lambda, that controls its accuracy and convergence\nrate. It is also more accurate than the one block Metropolis-Hastings estimate\nand can be used in any application because it is not based on assumptions on\nthe nature of the posterior density nor the amount of data or parameters in the\nstatistical model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 15:14:53 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2011 13:33:17 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2012 20:16:07 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2012 14:09:54 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Tuomi", "Mikko", ""], ["Jones", "Hugh R. A.", ""]]}, {"id": "1112.6308", "submitter": "Fabio Fajardo", "authors": "Valderio A. Reisen and Fabio A. Fajardo", "title": "Robust estimation in time series with long and short memory properties", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent developments of robust estimation in linear time\nseries models, with short and long memory correlation structures, in the\npresence of additive outliers. Based on the manuscripts Fajardo et al. (2009)\nand L\\'evy-Leduc et al. (2011a), the emphasis in this paper is given in the\nfollowing directions; the influence of additive outliers in the estimation of a\ntime series, the asymptotic properties of a robust autocovariance function and\na robust semiparametric estimation method of the fractional parameter d in\nARFIMA(p, d, q) models. Some simulations are used to support the use of the\nrobust method when a time series has additive outliers. The invariance property\nof the estimators for the first difference in ARFIMA model with outliers is\nalso discussed. In general, the robust long-memory estimator leads to be\noutlier resistant and is invariant to first differencing.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 13:40:43 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Reisen", "Valderio A.", ""], ["Fajardo", "Fabio A.", ""]]}]