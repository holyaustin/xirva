[{"id": "1209.0012", "submitter": "Lee Dicker", "authors": "Lee H. Dicker", "title": "Residual variance and the signal-to-noise ratio in high-dimensional\n  linear models", "comments": "50 pages, including supplemental text (included after the\n  bibliography); 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual variance and the signal-to-noise ratio are important quantities in\nmany statistical models and model fitting procedures. They play an important\nrole in regression diagnostics, in determining the performance limits in\nestimation and prediction problems, and in shrinkage parameter selection in\nmany popular regularized regression methods for high-dimensional data analysis.\nWe propose new estimators for the residual variance, the l2-signal strength,\nand the signal-to-noise ratio that are consistent and asymptotically normal in\nhigh-dimensional linear models with Gaussian predictors and errors, where the\nnumber of predictors d is proportional to the number of observations n.\nExisting results on residual variance estimation in high-dimensional linear\nmodels depend on sparsity in the underlying signal. Our results require no\nsparsity assumptions and imply that the residual variance may be consistently\nestimated even when d > n and the underlying signal itself is non-estimable.\nBasic numerical work suggests that some of the distributional assumptions made\nfor our theoretical results may be relaxed.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 20:30:26 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Dicker", "Lee H.", ""]]}, {"id": "1209.0089", "submitter": "Aaron Clauset", "authors": "Aaron Clauset, Ryan Woodard", "title": "Estimating the historical and future probabilities of large terrorist\n  events", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS614 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1838-1865", "doi": "10.1214/12-AOAS614", "report-no": "IMS-AOAS-AOAS614", "categories": "physics.data-an cs.LG physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantities with right-skewed distributions are ubiquitous in complex social\nsystems, including political conflict, economics and social networks, and these\nsystems sometimes produce extremely large events. For instance, the 9/11\nterrorist events produced nearly 3000 fatalities, nearly six times more than\nthe next largest event. But, was this enormous loss of life statistically\nunlikely given modern terrorism's historical record? Accurately estimating the\nprobability of such an event is complicated by the large fluctuations in the\nempirical distribution's upper tail. We present a generic statistical algorithm\nfor making such estimates, which combines semi-parametric models of tail\nbehavior and a nonparametric bootstrap. Applied to a global database of\nterrorist events, we estimate the worldwide historical probability of observing\nat least one 9/11-sized or larger event since 1968 to be 11-35%. These results\nare robust to conditioning on global variations in economic development,\ndomestic versus international events, the type of weapon used and a truncated\nhistory that stops at 1998. We then use this procedure to make a data-driven\nstatistical forecast of at least one similar event over the next decade.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 12:58:35 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 05:52:57 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2014 08:38:09 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Clauset", "Aaron", ""], ["Woodard", "Ryan", ""]]}, {"id": "1209.0185", "submitter": "Adam Persing", "authors": "Adam Persing and Ajay Jasra", "title": "Marginal Likelihood Computation for Hidden Markov Models via Generalized\n  Two-Filter Smoothing", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we introduce an estimate for the marginal likelihood associated\nto hidden Markov models (HMMs) using sequential Monte Carlo (SMC)\napproximations of the generalized two-filter smoothing decomposition (Briers,\n2010). This estimate is shown to be unbiased and a central limit theorem (CLT)\nis established. This latter CLT also allows one to prove a CLT associated to\nestimates of expectations w.r.t. a marginal of the joint smoothing\ndistribution; these form some of the first theoretical results associated to\nthe SMC approximation of the generalized two-filter smoothing decomposition.\nThe new estimate and its application is investigated from a numerical\nperspective.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2012 15:52:09 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Persing", "Adam", ""], ["Jasra", "Ajay", ""]]}, {"id": "1209.0253", "submitter": "Jamie Hall", "authors": "Jamie Hall, Michael K. Pitt, Robert Kohn", "title": "Bayesian inference for nonlinear structural time series models", "comments": "Typo correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses a partially adapted particle filter for estimating the\nlikelihood of a nonlinear structural econometric state space models whose state\ntransition density cannot be expressed in closed form. The filter generates the\ndisturbances in the state transition equation and allows for multiple modes in\nthe conditional disturbance distribution. The particle filter produces an\nunbiased estimate of the likelihood and so can be used to carry out Bayesian\ninference in a particle Markov chain Monte Carlo framework. We show empirically\nthat when the signal to noise ratio is high, the new filter can be much more\nefficient than the standard particle filter, in the sense that it requires far\nfewer particles to give the same accuracy. The new filter is applied to several\nsimulated and real examples and in particular to a dynamic stochastic general\nequilibrium model.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 06:36:47 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 00:30:42 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Hall", "Jamie", ""], ["Pitt", "Michael K.", ""], ["Kohn", "Robert", ""]]}, {"id": "1209.0661", "submitter": "Kristian Lum", "authors": "Kristian Lum", "title": "Bayesian variable selection for spatially dependent generalized linear\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the abundance of methods for variable selection and accommodating\nspatial structure in regression models, there is little precedent for\nincorporating spatial dependence in covariate inclusion probabilities for\nregionally varying regression models. The lone existing approach is limited by\ndifficult computation and the requirement that the spatial dependence be\nrepresented on a lattice, making this method inappropriate for areal models\nwith irregular structures that often arise in ecology, epidemiology, and the\nsocial sciences. Here we present a novel method for spatial variable selection\nin areal generalized linear models that can accommodate arbitrary spatial\nstructures and works with a broad subset of GLM likelihoods. The method uses a\nlatent probit model with a spatial dependence structure where the binary\nresponse is taken as a covariate inclusion indicator for area-specific GLMs.\nThe covariate inclusion indicators arise via thresholding of latent standard\nnormals on which we place a conditionally autoregressive prior. We propose an\nefficient MCMC algorithm for computation that is entirely conjugate in any\nmodel with a conditionally Gaussian representation of the likelihood, thereby\nencompassing logistic, probit, multinomial probit and logit, Gaussian, and\nnegative binomial regressions through the use of existing data augmentation\nmethods. We demonstrate superior parameter recovery and prediction in\nsimulation studies as well as in applications to geographic voting patterns and\npopulation estimation. Though the method is very broadly applicable, we note in\nparticular that prior to this work, spatial population\nestimation/capture-recapture models allowing for varying list dependence\nstructures has not been possible.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 14:53:39 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Lum", "Kristian", ""]]}, {"id": "1209.0833", "submitter": "Emily Fox", "authors": "Emily B. Fox and David B. Dunson", "title": "Multiresolution Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiresolution Gaussian process to capture long-range,\nnon-Markovian dependencies while allowing for abrupt changes. The\nmultiresolution GP hierarchically couples a collection of smooth GPs, each\ndefined over an element of a random nested partition. Long-range dependencies\nare captured by the top-level GP while the partition points define the abrupt\nchanges. Due to the inherent conjugacy of the GPs, one can analytically\nmarginalize the GPs and compute the conditional likelihood of the observations\ngiven the partition tree. This property allows for efficient inference of the\npartition itself, for which we employ graph-theoretic techniques. We apply the\nmultiresolution GP to the analysis of Magnetoencephalography (MEG) recordings\nof brain activity.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 00:34:41 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Fox", "Emily B.", ""], ["Dunson", "David B.", ""]]}, {"id": "1209.0901", "submitter": "Julia C. Sommer", "authors": "Julia C. Sommer and Volker J. Schmid", "title": "Spatial two tissue compartment model for DCE-MRI", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics) 2014 (63), 5, pp. 695-713", "doi": "10.1111/rssc.12057", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quantitative analysis of Dynamic Contrast-Enhanced Magnetic Resonance\nImaging (DCE-MRI) compartment models allow to describe the uptake of contrast\nmedium with biological meaningful kinetic parameters. As simple models often\nfail to adequately describe the observed uptake behavior, more complex\ncompartment models have been proposed. However, the nonlinear regression\nproblem arising from more complex compartment models often suffers from\nparameter redundancy. In this paper, we incorporate spatial smoothness on the\nkinetic parameters of a two tissue compartment model by imposing Gaussian\nMarkov random field priors on them. We analyse to what extent this spatial\nregularisation helps to avoid parameter redundancy and to obtain stable\nparameter estimates. Choosing a full Bayesian approach, we obtain posteriors\nand point estimates running Markov Chain Monte Carlo simulations. The proposed\napproach is evaluated for simulated concentration time curves as well as for in\nvivo data from a breast cancer study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 09:23:29 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Sommer", "Julia C.", ""], ["Schmid", "Volker J.", ""]]}, {"id": "1209.1119", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou and Lawrence Carin", "title": "Augment-and-Conquer Negative Binomial Processes", "comments": "Neural Information Processing Systems, NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By developing data augmentation methods unique to the negative binomial (NB)\ndistribution, we unite seemingly disjoint count and mixture models under the NB\nprocess framework. We develop fundamental properties of the models and derive\nefficient Gibbs sampling inference. We show that the gamma-NB process can be\nreduced to the hierarchical Dirichlet process with normalization, highlighting\nits unique theoretical, structural and computational advantages. A variety of\nNB processes with distinct sharing mechanisms are constructed and applied to\ntopic modeling, with connections to existing algorithms, showing the importance\nof inferring both the NB dispersion and probability parameters.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 21:06:32 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 16:30:49 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1209.1145", "submitter": "Sinead Williamson", "authors": "Sinead Williamson, Zoubin Ghahramani, Steven N. MacEachern, Eric P.\n  Xing", "title": "Restricting exchangeable nonparametric distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributions over exchangeable matrices with infinitely many columns, such\nas the Indian buffet process, are useful in constructing nonparametric latent\nvariable models. However, the distribution implied by such models over the\nnumber of features exhibited by each data point may be poorly- suited for many\nmodeling tasks. In this paper, we propose a class of exchangeable nonparametric\npriors obtained by restricting the domain of existing models. Such models allow\nus to specify the distribution over the number of features per data point, and\ncan achieve better performance on data sets where the number of features is not\nwell-modeled by the original distribution.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 23:33:28 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Williamson", "Sinead", ""], ["Ghahramani", "Zoubin", ""], ["MacEachern", "Steven N.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1209.1341", "submitter": "Xiang Zhou", "authors": "Xiang Zhou and Peter Carbonetto and Matthew Stephens", "title": "Polygenic Modeling with Bayesian Sparse Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both linear mixed models (LMMs) and sparse regression models are widely used\nin genetics applications, including, recently, polygenic modeling in\ngenome-wide association studies. These two approaches make very different\nassumptions, so are expected to perform well in different situations. However,\nin practice, for a given data set one typically does not know which assumptions\nwill be more accurate. Motivated by this, we consider a hybrid of the two,\nwhich we refer to as a \"Bayesian sparse linear mixed model\" (BSLMM) that\nincludes both these models as special cases. We address several key\ncomputational and statistical issues that arise when applying BSLMM, including\nappropriate prior specification for the hyper-parameters, and a novel Markov\nchain Monte Carlo algorithm for posterior inference. We apply BSLMM and compare\nit with other methods for two polygenic modeling applications: estimating the\nproportion of variance in phenotypes explained (PVE) by available genotypes,\nand phenotype (or breeding value) prediction. For PVE estimation, we\ndemonstrate that BSLMM combines the advantages of both standard LMMs and sparse\nregression modeling. For phenotype prediction it considerably outperforms\neither of the other two methods, as well as several other large-scale\nregression methods previously suggested for this problem. Software implementing\nour method is freely available from\nhttp://stephenslab.uchicago.edu/software.html\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 16:48:45 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2012 22:30:27 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Zhou", "Xiang", ""], ["Carbonetto", "Peter", ""], ["Stephens", "Matthew", ""]]}, {"id": "1209.1371", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "On the age-, time- and migration dependent dynamics of diseases", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalizes a previously published differential equation that\ndescribes the relation between the age-specific incidence, remission, and\nmortality of a disease with its prevalence. The underlying model is a simple\ncompartment model with three states (illness-death model). In contrast to the\nformer work, migration- and calendar time-effects are included. As an\napplication of the theoretical findings, a hypothetical example of an\nirreversible disease is treated.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 19:04:35 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1209.1544", "submitter": "Ma{\\l}gorzata Snarska", "authors": "Jerzy P. Rydlewski, Ma{\\l}gorzata Snarska", "title": "On Geometric Ergodicity of Skewed - SVCHARME models", "comments": null, "journal-ref": "Statistics & Probability Letters, Volume 84, January 2014, Pages\n  192-197", "doi": "10.1016/j.spl.2013.10.008", "report-no": null, "categories": "q-fin.ST math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo is repeatedly used to analyze the properties of\nintractable distributions in a convenient way. In this paper we derive\nconditions for geometric ergodicity of a general class of nonparametric\nstochastic volatility models with skewness driven by hidden Markov Chain with\nswitching.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 14:15:51 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Rydlewski", "Jerzy P.", ""], ["Snarska", "Ma\u0142gorzata", ""]]}, {"id": "1209.1588", "submitter": "Victoria Zinde-Walsh", "authors": "Victoria Zinde-Walsh", "title": "Identification and well-posedness in nonparametric models with\n  independence conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a nonparametric analysis for several classes of models,\nwith cases such as classical measurement error, regression with errors in\nvariables, factor models and other models that may be represented in a form\ninvolving convolution equations. The focus here is on conditions for existence\nof solutions, nonparametric identification and well-posedness in the space of\ngeneralized functions (tempered distributions). This space provides advantages\nover working in function spaces by relaxing assumptions and extending the\nresults to include a wider variety of models, for example by not requiring\nexistence of density. Classes of (generalized) functions for which solutions\nexist are defined; identification conditions, partial identification and its\nimplications are discussed. Conditions for well-posedness are given and the\nrelated issues of plug-in estimation and regularization are examined.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 16:59:18 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Zinde-Walsh", "Victoria", ""]]}, {"id": "1209.1625", "submitter": "Hao Chen", "authors": "Hao Chen, Nancy Zhang", "title": "Graph-Based Change-Point Detection", "comments": null, "journal-ref": "The Annals of Statistics, Volume 43, Number 1 (2015), 139-176", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the testing and estimation of change-points -- locations where\nthe distribution abruptly changes -- in a data sequence. A new approach, based\non scan statistics utilizing graphs representing the similarity between\nobservations, is proposed. The graph-based approach is non-parametric, and can\nbe applied to any data set as long as an informative similarity measure on the\nsample space can be defined. Accurate analytic approximations to the\nsignificance of graph-based scan statistics for both the single change-point\nand the changed interval alternatives are provided. Simulations reveal that the\nnew approach has better power than existing approaches when the dimension of\nthe data is moderate to high. The new approach is illustrated on two\napplications: The determination of authorship of a classic novel, and the\ndetection of change in a network over time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 19:57:37 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 17:37:37 GMT"}, {"version": "v3", "created": "Tue, 26 Aug 2014 00:08:26 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Chen", "Hao", ""], ["Zhang", "Nancy", ""]]}, {"id": "1209.1706", "submitter": "Abel Rodriguez", "authors": "Abel Rodriguez", "title": "Default Bayesian Analysis for the Multivariate Ewens Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the Jeffreys prior for the parameter of the Multivariate Ewens\nDistribution and study some of its properties. In particular, we show that this\nprior is proper and has no finite moments. We also investigate the impact of\nthis default prior on the a priori distribution of the number of species and\nthe a priori probability of discovery of a new species, which are usually\nemployed in subjective prior elicitation. The effect of the Jeffreys prior for\nposterior inference is illustrated using examples arising in the context of\ninference for species sampling models and Dirichlet process mixture models.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 11:31:49 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Rodriguez", "Abel", ""]]}, {"id": "1209.1740", "submitter": "Kinjal Basu", "authors": "Kinjal Basu and Debapriya Sengupta", "title": "Spline Smoothing for Estimation of Circular Probability Distributions\n  via Spectral Isomorphism and its Spatial Adaptation", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem when $X_1,X_2,..., X_n$ are distributed on a circle\nfollowing an unknown distribution $F$ on $S^1$. In this article we have\nconsider the absolute general set-up where the density can have local features\nsuch as discontinuities and edges. Furthermore, there can be outlying data\nwhich can follow some discrete distributions. The traditional Kernel Density\nEstimation methods fail to identify such local features in the data. Here we\ndevice a non-parametric density estimate on $S^1$, by the use of a novel\ntechnique which we term as Fourier Spline. We have also tried to identify and\nincorporate local features such as support, discontinuity or edges in the final\ndensity estimate. Several new results are proved in this regard. Simulation\nstudies have also been performed to see how our methodology works. Finally a\nreal life example is also shown.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 18:39:48 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Basu", "Kinjal", ""], ["Sengupta", "Debapriya", ""]]}, {"id": "1209.1826", "submitter": "Kinjal Basu", "authors": "Kinjal Basu and Debapriya Sengupta", "title": "A spatio-spectral hybridization for edge preservation and noisy image\n  restoration via local parametric mixtures and Lagrangian relaxation", "comments": "29 Pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a fully unsupervised statistical method for edge\npreserving image restoration and compression using a spatial decomposition\nscheme. Smoothed maximum likelihood is used for local estimation of edge pixels\nfrom mixture parametric models of local templates. For the complementary smooth\npart the traditional L2-variational problem is solved in the Fourier domain\nwith Thin Plate Spline (TPS) regularization. It is well known that naive\nFourier compression of the whole image fails to restore a piece-wise smooth\nnoisy image satisfactorily due to Gibbs phenomenon. Images are interpreted as\nrelative frequency histograms of samples from bi-variate densities where the\nsample sizes might be unknown. The set of discontinuities is assumed to be\ncompletely unsupervised Lebesgue-null, compact subset of the plane in the\ncontinuous formulation of the problem. Proposed spatial decomposition uses a\nwidely used topological concept, partition of unity. The decision on edge pixel\nneighborhoods are made based on the multiple testing procedure of Holms.\nStatistical summary of the final output is decomposed into two layers of\ninformation extraction, one for the subset of edge pixels and the other for the\nsmooth region. Robustness is also demonstrated by applying the technique on\nnoisy degradation of clean images.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2012 18:23:21 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Basu", "Kinjal", ""], ["Sengupta", "Debapriya", ""]]}, {"id": "1209.1902", "submitter": "Francisco Javier Rubio Mr.", "authors": "J. A. Montoya and F. J. Rubio", "title": "Nonparametric inference for $P(X<Y)$ with paired variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two classes of nonparametric point estimators of $\\theta=P(X<Y)$\nin the case where $(X,Y)$ are paired, possibly dependent, absolutely continuous\nrandom variables. The proposed estimators are based on nonparametric estimators\nof the joint density of $(X,Y)$ and the distribution function of $Z=Y - X$. We\nexplore the use of several density and distribution function estimators and\ncharacterise the convergence of the resulting estimators of $\\theta$. We\nconsider the use of bootstrap methods to obtain confidence intervals. The\nperformance of these estimators is illustrated using simulated and real data.\nThese examples show that not accounting for pairing and dependence may lead to\nerroneous conclusions about the relationship between $X$ and $Y$.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 08:27:18 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2013 22:12:03 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2013 14:19:24 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Montoya", "J. A.", ""], ["Rubio", "F. J.", ""]]}, {"id": "1209.1994", "submitter": "Heng Peng", "authors": "Heng Peng", "title": "Nonconcave Penalized Spline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Regression spline is a useful tool in nonparametric regression. However,\nfinding the optimal knot locations is a known difficult problem. In this\narticle, we introduce the Non-concave Penalized Regression Spline. This\nproposal method not only produces smoothing spline with optimal convergence\nrate, but also can adaptively select optimal knots simultaneously. It is\ninsensitive to the number of origin knots. The method's performance in a\nsimulation has been studied to compare the other methods. The problem of how to\nchoose smoothing parameters, i.e. penalty parameters in the non-concave\nregression spline is addressed.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 13:57:26 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Peng", "Heng", ""]]}, {"id": "1209.2072", "submitter": "Adityanand Guntuboyina", "authors": "Adityanand Guntuboyina and Russell Barbour and Robert Heimer", "title": "On the impossibility of constructing good population mean estimators in\n  a realistic Respondent Driven Sampling model", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for population mean estimation from data collected by\nRespondent Driven Sampling (RDS) are based on the Horvitz-Thompson estimator\ntogether with a set of assumptions on the sampling model under which the\ninclusion probabilities can be determined from the information contained in the\ndata. In this paper, we argue that such set of assumptions are too simplistic\nto be realistic and that under realistic sampling models, the situation is far\nmore complicated. Specifically, we study a realistic RDS sampling model that is\nmotivated by a real world RDS dataset. We show that, for this model, the\ninclusion probabilities, which are necessary for the application of the\nHorvitz-Thompson estimator, can not be determined by the information in the\nsample alone. An implication is that, unless additional information about the\nunderlying population network is obtained, it is hopeless to conceive of a\ngeneral theory of population mean estimation from current RDS data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 17:42:25 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 18:12:42 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Guntuboyina", "Adityanand", ""], ["Barbour", "Russell", ""], ["Heimer", "Robert", ""]]}, {"id": "1209.2437", "submitter": "Lie Wang", "authors": "Han Liu and Lie Wang", "title": "TIGER: A Tuning-Insensitive Approach for Optimally Estimating Gaussian\n  Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new procedure for estimating high dimensional Gaussian graphical\nmodels. Our approach is asymptotically tuning-free and non-asymptotically\ntuning-insensitive: it requires very few efforts to choose the tuning parameter\nin finite sample settings. Computationally, our procedure is significantly\nfaster than existing methods due to its tuning-insensitive property.\nTheoretically, the obtained estimator is simultaneously minimax optimal for\nprecision matrix estimation under different norms. Empirically, we illustrate\nthe advantages of our method using thorough simulated and real examples. The R\npackage bigmatrix implementing the proposed methods is available on the\nComprehensive R Archive Network: http://cran.r-project.org/.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 20:53:45 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Liu", "Han", ""], ["Wang", "Lie", ""]]}, {"id": "1209.2566", "submitter": "Jakob Teichmann", "authors": "Jakob Teichmann, Felix Ballani and Karl Gerald van den Boogaart", "title": "Generalizations of Mat\\'ern's hard-core point processes", "comments": "21 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mat\\'ern's hard-core processes are valuable point process models in spatial\nstatistics. In order to extend their field of application, Mat\\'ern's original\nmodels are generalized here, both as point processes and particle processes.\nThe thinning rule uses a distance-dependent probability function, which\ncontrols deletion of points close together. For this general setting, explicit\nformulas for first- and second-order characteristics can be given. Two examples\nfrom materials science illustrate the application of the models.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 11:24:19 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Teichmann", "Jakob", ""], ["Ballani", "Felix", ""], ["Boogaart", "Karl Gerald van den", ""]]}, {"id": "1209.2669", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Likelihood Estimation with Incomplete Array Variate Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data is an important challenge when dealing with high dimensional\ndata arranged in the form of an array. In this paper, we propose methods for\nestimation of the parameters of array variate normal probability model from\npartially observed multiway data. The methods developed here are useful for\nmissing data imputation, estimation of mean and covariance parameters for\nmultiway data. A multiway semi-parametric mixed effects model that allows\nseparation of multiway covariance effects is also defined and an efficient\nalgorithm for estimation is recommended. We provide simulation results along\nwith real life data from genetics to demonstrate these methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 17:08:36 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2012 16:10:45 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2013 18:59:47 GMT"}, {"version": "v4", "created": "Mon, 2 Jun 2014 19:45:27 GMT"}, {"version": "v5", "created": "Sun, 6 Jul 2014 00:28:39 GMT"}, {"version": "v6", "created": "Sat, 11 Oct 2014 11:18:04 GMT"}, {"version": "v7", "created": "Wed, 24 Dec 2014 18:48:16 GMT"}, {"version": "v8", "created": "Tue, 30 Dec 2014 14:49:14 GMT"}, {"version": "v9", "created": "Mon, 5 Jan 2015 15:54:01 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1209.2829", "submitter": "Ruth Heller", "authors": "Ruth Heller, Daniel Yekutieli", "title": "Replicability analysis for genome-wide association studies", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS697 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 481-498", "doi": "10.1214/13-AOAS697", "report-no": "IMS-AOAS-AOAS697", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paramount importance of replicating associations is well recognized in\nthe genome-wide associaton (GWA) research community, yet methods for assessing\nreplicability of associations are scarce. Published GWA studies often combine\nseparately the results of primary studies and of the follow-up studies.\nInformally, reporting the two separate meta-analyses, that of the primary\nstudies and follow-up studies, gives a sense of the replicability of the\nresults. We suggest a formal empirical Bayes approach for discovering whether\nresults have been replicated across studies, in which we estimate the optimal\nrejection region for discovering replicated results. We demonstrate, using\nrealistic simulations, that the average false discovery proportion of our\nmethod remains small. We apply our method to six type two diabetes (T2D) GWA\nstudies. Out of 803 SNPs discovered to be associated with T2D using a typical\nmeta-analysis, we discovered 219 SNPs with replicated associations with T2D. We\nrecommend complementing a meta-analysis with a replicability analysis for GWA\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 09:32:55 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2013 13:23:14 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 14:07:06 GMT"}, {"version": "v4", "created": "Tue, 29 Apr 2014 13:35:05 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Heller", "Ruth", ""], ["Yekutieli", "Daniel", ""]]}, {"id": "1209.2978", "submitter": "Robin Evans", "authors": "Robin J. Evans", "title": "Graphical methods for inequality constraints in marginalized DAGs", "comments": "A final version will appear in the proceedings of the 22nd Workshop\n  on Machine Learning and Signal Processing, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a graphical approach to deriving inequality constraints for\ndirected acyclic graph (DAG) models, where some variables are unobserved. In\nparticular we show that the observed distribution of a discrete model is always\nrestricted if any two observed variables are neither adjacent in the graph, nor\nshare a latent parent; this generalizes the well known instrumental inequality.\nThe method also provides inequalities on interventional distributions, which\ncan be used to bound causal effects. All these constraints are characterized in\nterms of a new graphical separation criterion, providing an easy and intuitive\nmethod for their derivation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 18:05:26 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Evans", "Robin J.", ""]]}, {"id": "1209.3442", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou and Lawrence Carin", "title": "Negative Binomial Process Count and Mixture Modeling", "comments": "To appear in IEEE Trans. Pattern Analysis and Machine Intelligence:\n  Special Issue on Bayesian Nonparametrics. 14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seemingly disjoint problems of count and mixture modeling are united\nunder the negative binomial (NB) process. A gamma process is employed to model\nthe rate measure of a Poisson process, whose normalization provides a random\nprobability measure for mixture modeling and whose marginalization leads to an\nNB process for count modeling. A draw from the NB process consists of a Poisson\ndistributed finite number of distinct atoms, each of which is associated with a\nlogarithmic distributed number of data samples. We reveal relationships between\nvarious count- and mixture-modeling distributions and construct a\nPoisson-logarithmic bivariate distribution that connects the NB and Chinese\nrestaurant table distributions. Fundamental properties of the models are\ndeveloped, and we derive efficient Bayesian inference. It is shown that with\naugmentation and normalization, the NB process and gamma-NB process can be\nreduced to the Dirichlet process and hierarchical Dirichlet process,\nrespectively. These relationships highlight theoretical, structural and\ncomputational advantages of the NB process. A variety of NB processes,\nincluding the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and\nzero-inflated-NB processes, with distinct sharing mechanisms, are also\nconstructed. These models are applied to topic modeling, with connections made\nto existing algorithms under Poisson factor analysis. Example results show the\nimportance of inferring both the NB dispersion and probability parameters.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 21:55:36 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 19:29:50 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2013 01:01:39 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1209.3550", "submitter": "Matt Wand Professor", "authors": "Jan Luts, Tamara Broderick, Matt P. Wand", "title": "Real-time semiparametric regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop algorithms for performing semiparametric regression analysis in\nreal time, with data processed as it is collected and made immediately\navailable via modern telecommunications technologies. Our definition of\nsemiparametric regression is quite broad and includes, as special cases,\ngeneralized linear mixed models, generalized additive models, geostatistical\nmodels, wavelet nonparametric regression models and their various combinations.\nFast updating of regression fits is achieved by couching semiparametric\nregression into a Bayesian hierarchical model or, equivalently, graphical model\nframework and employing online mean field variational ideas. An internet site\nattached to this article, realtime-semiparametric-regression.net, illustrates\nthe methodology for continually arriving stock market, real estate and airline\ndata. Flexible real-time analyses, based on increasingly ubiquitous streaming\ndata sources stand to benefit.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 04:52:10 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 06:29:07 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Luts", "Jan", ""], ["Broderick", "Tamara", ""], ["Wand", "Matt P.", ""]]}, {"id": "1209.3963", "submitter": "Georg Hahn", "authors": "Axel Gandy, Georg Hahn", "title": "MMCTest - A Safe Algorithm for Implementing Multiple Monte Carlo Tests", "comments": null, "journal-ref": "Scand J Stat (2014), 41(4):1083--1101", "doi": "10.1111/sjos.12085", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider testing multiple hypotheses using tests that can only be evaluated\nby simulation, such as permutation tests or bootstrap tests. This article\nintroduces MMCTest, a sequential algorithm which gives, with arbitrarily high\nprobability, the same classification as a specific multiple testing procedure\napplied to ideal p-values. The method can be used with a class of multiple\ntesting procedures which includes the Benjamini & Hochberg False Discovery Rate\n(FDR) procedure and the Bonferroni correction controlling the Familywise Error\nRate. One of the key features of the algorithm is that it stops sampling for\nall the hypotheses which can already be decided as being rejected or\nnon-rejected. MMCTest can be interrupted at any stage and then returns three\nsets of hypotheses: the rejected, the non-rejected and the undecided\nhypotheses. A simulation study motivated by actual biological data shows that\nMMCTest is usable in practice and that, despite the additional guarantee, it\ncan be computationally more efficient than other methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 13:59:54 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2012 00:28:59 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2012 15:50:18 GMT"}, {"version": "v4", "created": "Wed, 20 Feb 2013 13:28:56 GMT"}, {"version": "v5", "created": "Wed, 24 Apr 2013 09:30:22 GMT"}, {"version": "v6", "created": "Wed, 5 Jun 2013 10:58:01 GMT"}, {"version": "v7", "created": "Wed, 16 Oct 2013 14:34:45 GMT"}, {"version": "v8", "created": "Wed, 12 Feb 2014 00:25:33 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Gandy", "Axel", ""], ["Hahn", "Georg", ""]]}, {"id": "1209.4495", "submitter": "Enno Mammen", "authors": "Enno Mammen, Maria Dolores Martinez Miranda, Jens Perch Nielsen and\n  Stefan Sperlich", "title": "A comparative study of new cross-validated bandwidth selectors for\n  kernel density estimation", "comments": "19 pages, 8 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent contributions to kernel smoothing show that the performance of\ncross-validated bandwidth selectors improve significantly from indirectness.\nIndirect crossvalidation first estimates the classical cross-validated\nbandwidth from a more rough and difficult smoothing problem than the original\none and then rescales this indirect bandwidth to become a bandwidth of the\noriginal problem. The motivation for this approach comes from the observation\nthat classical crossvalidation tends to work better when the smoothing problem\nis difficult. In this paper we find that the performance of indirect\ncrossvalidation improves theoretically and practically when the polynomial\norder of the indirect kernel increases, with the Gaussian kernel as limiting\nkernel when the polynomial order goes to infinity. These theoretical and\npractical results support the often proposed choice of the Gaussian kernel as\nindirect kernel. However, for do-validation our study shows a discrepancy\nbetween asymptotic theory and practical performance. As for indirect\ncrossvalidation, in asymptotic theory the performance of indirect do-validation\nimproves with increasing polynomial order of the used indirect kernel. But this\ntheoretical improvements do not carry over to practice and the original\ndo-validation still seems to be our preferred bandwidth selector. We also\nconsider plug-in estimation and combinations of plug-in bandwidths and\ncrossvalidated bandwidths. These latter bandwidths do not outperform the\noriginal do-validation estimator either.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 11:11:03 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Mammen", "Enno", ""], ["Miranda", "Maria Dolores Martinez", ""], ["Nielsen", "Jens Perch", ""], ["Sperlich", "Stefan", ""]]}, {"id": "1209.4690", "submitter": "Wei-Yin Loh", "authors": "Wei-Yin Loh, Wei Zheng", "title": "Regression trees for longitudinal and multiresponse data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS596 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 495-522", "doi": "10.1214/12-AOAS596", "report-no": "IMS-AOAS-AOAS596", "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous algorithms for constructing regression tree models for longitudinal\nand multiresponse data have mostly followed the CART approach. Consequently,\nthey inherit the same selection biases and computational difficulties as CART.\nWe propose an alternative, based on the GUIDE approach, that treats each\nlongitudinal data series as a curve and uses chi-squared tests of the residual\ncurve patterns to select a variable to split each node of the tree. Besides\nbeing unbiased, the method is applicable to data with fixed and random time\npoints and with missing values in the response or predictor variables.\nSimulation results comparing its mean squared prediction error with that of\nMVPART are given, as well as examples comparing it with standard linear mixed\neffects and generalized estimating equation models. Conditions for asymptotic\nconsistency of regression tree function estimates are also given.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 02:07:31 GMT"}, {"version": "v2", "created": "Mon, 27 May 2013 05:34:09 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Loh", "Wei-Yin", ""], ["Zheng", "Wei", ""]]}, {"id": "1209.4951", "submitter": "Tu Xu", "authors": "Tu Xu and Junhui Wang", "title": "An efficient model-free estimation of multiclass conditional probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multiclass conditional probability estimation methods, such as\nFisher's discriminate analysis and logistic regression, often require\nrestrictive distributional model assumption. In this paper, a model-free\nestimation method is proposed to estimate multiclass conditional probability\nthrough a series of conditional quantile regression functions. Specifically,\nthe conditional class probability is formulated as difference of corresponding\ncumulative distribution functions, where the cumulative distribution functions\ncan be converted from the estimated conditional quantile regression functions.\nThe proposed estimation method is also efficient as its computation cost does\nnot increase exponentially with the number of classes. The theoretical and\nnumerical studies demonstrate that the proposed estimation method is highly\ncompetitive against the existing competitors, especially when the number of\nclasses is relatively large.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 01:50:43 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2013 15:08:40 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2013 17:44:53 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Xu", "Tu", ""], ["Wang", "Junhui", ""]]}, {"id": "1209.5356", "submitter": "Nicole Kraemer", "authors": "Nicole Kraemer and Eike C. Brechmann and Daniel Silvestrini and\n  Claudia Czado", "title": "Total loss estimation using copula-based regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a joint copula-based model for insurance claims and sizes. It uses\nbivariate copulae to accommodate for the dependence between these quantities.\nWe derive the general distribution of the policy loss without the restrictive\nassumption of independence. We illustrate that this distribution tends to be\nskewed and multi-modal, and that an independence assumption can lead to\nsubstantial bias in the estimation of the policy loss. Further, we extend our\nframework to regression models by combining marginal generalized linear models\nwith a copula. We show that this approach leads to a flexible class of models,\nand that the parameters can be estimated efficiently using maximum-likelihood.\nWe propose a test procedure for the selection of the optimal copula family. The\nusefulness of our approach is illustrated in a simulation study and in an\nanalysis of car insurance policies.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 18:28:04 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Kraemer", "Nicole", ""], ["Brechmann", "Eike C.", ""], ["Silvestrini", "Daniel", ""], ["Czado", "Claudia", ""]]}, {"id": "1209.5860", "submitter": "Yangbo He", "authors": "Yangbo He, Jinzhu Jia, Bin Yu", "title": "Reversible MCMC on Markov equivalence classes of sparse directed acyclic\n  graphs", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1125 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 4, 1742-1779", "doi": "10.1214/13-AOS1125", "report-no": "IMS-AOS-AOS1125", "categories": "stat.ML cs.DM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are popular statistical tools which are used to represent\ndependent or causal complex systems. Statistically equivalent causal or\ndirected graphical models are said to belong to a Markov equivalent class. It\nis of great interest to describe and understand the space of such classes.\nHowever, with currently known algorithms, sampling over such classes is only\nfeasible for graphs with fewer than approximately 20 vertices. In this paper,\nwe design reversible irreducible Markov chains on the space of Markov\nequivalent classes by proposing a perfect set of operators that determine the\ntransitions of the Markov chain. The stationary distribution of a proposed\nMarkov chain has a closed form and can be computed easily. Specifically, we\nconstruct a concrete perfect set of operators on sparse Markov equivalence\nclasses by introducing appropriate conditions on each possible operator.\nAlgorithms and their accelerated versions are provided to efficiently generate\nMarkov chains and to explore properties of Markov equivalence classes of sparse\ndirected acyclic graphs (DAGs) with thousands of vertices. We find\nexperimentally that in most Markov equivalence classes of sparse DAGs, (1) most\nedges are directed, (2) most undirected subgraphs are small and (3) the number\nof these undirected subgraphs grows approximately linearly with the number of\nvertices. The article contains supplement arXiv:1303.0632,\nhttp://dx.doi.org/10.1214/13-AOS1125SUPP\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 08:13:54 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2013 08:03:10 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 09:05:44 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["He", "Yangbo", ""], ["Jia", "Jinzhu", ""], ["Yu", "Bin", ""]]}, {"id": "1209.5908", "submitter": "Peter B\\\"uhlmann", "authors": "Peter B\\\"uhlmann, Philipp R\\\"utimann, Sara van de Geer and Cun-Hui\n  Zhang", "title": "Correlated variables in regression: clustering and sparse estimation", "comments": "40 pages, 6 figures", "journal-ref": "Journal of Statistical Planning and Inference 2013, Vol. 143,\n  1835-1858", "doi": "10.1016/j.jspi.2013.05.019", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation in a high-dimensional linear model with strongly\ncorrelated variables. We propose to cluster the variables first and do\nsubsequent sparse estimation such as the Lasso for cluster-representatives or\nthe group Lasso based on the structure from the clusters. Regarding the first\nstep, we present a novel and bottom-up agglomerative clustering algorithm based\non canonical correlations, and we show that it finds an optimal solution and is\nstatistically consistent. We also present some theoretical arguments that\ncanonical correlation based clustering leads to a better-posed compatibility\nconstant for the design matrix which ensures identifiability and an oracle\ninequality for the group Lasso. Furthermore, we discuss circumstances where\ncluster-representatives and using the Lasso as subsequent estimator leads to\nimproved results for prediction and detection of variables. We complement the\ntheoretical analysis with various empirical results.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 11:52:44 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["R\u00fctimann", "Philipp", ""], ["van de Geer", "Sara", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1209.5911", "submitter": "Yuan Liao", "authors": "Jushan Bai, Yuan Liao", "title": "Efficient Estimation of Approximate Factor Models via Regularized\n  Maximum Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of a high dimensional approximate factor model in the\npresence of both cross sectional dependence and heteroskedasticity. The\nclassical method of principal components analysis (PCA) does not efficiently\nestimate the factor loadings or common factors because it essentially treats\nthe idiosyncratic error to be homoskedastic and cross sectionally uncorrelated.\nFor efficient estimation it is essential to estimate a large error covariance\nmatrix. We assume the model to be conditionally sparse, and propose two\napproaches to estimating the common factors and factor loadings; both are based\non maximizing a Gaussian quasi-likelihood and involve regularizing a large\ncovariance sparse matrix. In the first approach the factor loadings and the\nerror covariance are estimated separately while in the second approach they are\nestimated jointly. Extensive asymptotic analysis has been carried out. In\nparticular, we develop the inferential theory for the two-step estimation.\nBecause the proposed approaches take into account the large error covariance\nmatrix, they produce more efficient estimators than the classical PCA methods\nor methods based on a strict factor model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 12:20:49 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 00:53:38 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Bai", "Jushan", ""], ["Liao", "Yuan", ""]]}, {"id": "1209.6048", "submitter": "Yi Sun", "authors": "Yi Sun and Faustino Gomez and Juergen Schmidhuber", "title": "Improving the Asymptotic Performance of Markov Chain Monte-Carlo by\n  Inserting Vortices", "comments": "Published in NIPS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new way of converting a reversible finite Markov chain into a\nnon-reversible one, with a theoretical guarantee that the asymptotic variance\nof the MCMC estimator based on the non-reversible chain is reduced. The method\nis applicable to any reversible chain whose states are not connected through a\ntree, and can be interpreted graphically as inserting vortices into the state\ntransition graph. Our result confirms that non-reversible chains are\nfundamentally better than reversible ones in terms of asymptotic performance,\nand suggests interesting directions for further improving MCMC.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 19:53:45 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Sun", "Yi", ""], ["Gomez", "Faustino", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1209.6241", "submitter": "Krista Gile", "authors": "Mark S. Handcock and Krista J. Gile and Corinne M. Mar", "title": "Estimating Hidden Population Size using Respondent-Driven Sampling Data", "comments": "36 pages, 7 figures, including color figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-Driven Sampling (RDS) is an approach to sampling design and\ninference in hard-to-reach human populations. Typically, a sampling frame is\nnot available, and population members are difficult to identify or recruit from\nbroader sampling frames. Common examples include injecting drug users, men who\nhave sex with men, and female sex workers. Most analysis of RDS data has\nfocused on estimating aggregate characteristics, such as disease prevalence.\nHowever, RDS is often conducted in settings where the population size is\nunknown and of great independent interest. This paper presents an approach to\nestimating the size of a target population based on data collected through RDS.\n  The proposed approach uses a successive sampling approximation to RDS to\nleverage information in the ordered sequence of observed personal network\nsizes. The inference uses the Bayesian framework, allowing for the\nincorporation of prior knowledge. A flexible class of priors for the population\nsize is proposed that aids elicitation. An extensive simulation study provides\ninsight into the performance of the method for estimating population size under\na broad range of conditions. A further study shows the approach also improves\nestimation of aggregate characteristics. A particular choice of the prior\nproduces interval estimates with good frequentist properties. Finally, the\nmethod demonstrates sensible results when used to estimate the numbers of\nsub-populations most at risk for HIV in two cities in El Salvador.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 14:17:41 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Handcock", "Mark S.", ""], ["Gile", "Krista J.", ""], ["Mar", "Corinne M.", ""]]}, {"id": "1209.6254", "submitter": "Krista Gile", "authors": "Krista J. Gile and Lisa G. Johnston and Matthew J. Salganik", "title": "Diagnostics for Respondent-driven Sampling", "comments": "41 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a widely used method for sampling from\nhard-to-reach human populations, especially groups most at-risk for HIV/AIDS.\nData are collected through a peer-referral process in which current sample\nmembers harness existing social networks to recruit additional sample members.\nRDS has proven to be a practical method of data collection in many difficult\nsettings and has been adopted by leading public health organizations around the\nworld. Unfortunately, inference from RDS data requires many strong assumptions\nbecause the sampling design is not fully known and is partially beyond the\ncontrol of the researcher. In this paper, we introduce diagnostic tools for\nmost of the assumptions underlying RDS inference. We also apply these\ndiagnostics in a case study of 12 populations at increased risk for HIV/AIDS.\nWe developed these diagnostics to enable RDS researchers to better understand\ntheir data and to encourage future statistical research on RDS.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 15:00:30 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Gile", "Krista J.", ""], ["Johnston", "Lisa G.", ""], ["Salganik", "Matthew J.", ""]]}, {"id": "1209.6344", "submitter": "Soyoung Jeon", "authors": "Soyoung Jeon, Richard L. Smith", "title": "Dependence Structure of Spatial Extremes Using Threshold Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of spatial extremes requires the joint modeling of a spatial\nprocess at a large number of stations and max-stable processes have been\ndeveloped as a class of stochastic processes suitable for studying spatial\nextremes. Spatial dependence structure in the extreme value analysis can be\nmeasured by max-stable processes. However, there have been few works on the\nthreshold approach of max-stable processes. We propose a threshold version of\nmax-stable process estimation and we apply the pairwise composite likelihood\nmethod by Padoan et al. (2010) to estimate spatial dependence parameters. It is\nof interest to establish limit behavior of the estimates based on the settings\nof increasing domain asymptotics with stochastic sampling design. Two different\ntypes of asymptotic normality are drawn under the second-order regular\nvariation condition for the distribution satisfying the domain of attraction.\nThe theoretical property of dependence parameter estimators in limiting sense\nis implemented by simulation and a choice of optimal threshold is discussed in\nthis paper.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 19:47:17 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Jeon", "Soyoung", ""], ["Smith", "Richard L.", ""]]}, {"id": "1209.6433", "submitter": "Harry van Zanten", "authors": "Harry van Zanten", "title": "Nonparametric Bayesian methods for one-dimensional diffusion models", "comments": null, "journal-ref": "Mathematical Biosciences 243, no. 2, pp. 215-222, 2013", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review recently developed methods for nonparametric Bayesian\ninference for one-dimensional diffusion models. We discuss different possible\nprior distributions, computational issues, and asymptotic results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 07:02:11 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2013 13:19:23 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["van Zanten", "Harry", ""]]}, {"id": "1209.6463", "submitter": "Antonio Punzo", "authors": "Sanjeena Subedi and Antonio Punzo and Salvatore Ingrassia and Paul D.\n  McNicholas", "title": "Clustering and Classification via Cluster-Weighted Factor Analyzers", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s11634-013-0124-8", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based clustering and classification, the cluster-weighted model\nconstitutes a convenient approach when the random vector of interest\nconstitutes a response variable Y and a set p of explanatory variables X.\nHowever, its applicability may be limited when p is high. To overcome this\nproblem, this paper assumes a latent factor structure for X in each mixture\ncomponent. This leads to the cluster-weighted factor analyzers (CWFA) model. By\nimposing constraints on the variance of Y and the covariance matrix of X, a\nnovel family of sixteen CWFA models is introduced for model-based clustering\nand classification. The alternating expectation-conditional maximization\nalgorithm, for maximum likelihood estimation of the parameters of all the\nmodels in the family, is described; to initialize the algorithm, a 5-step\nhierarchical procedure is proposed, which uses the nested structures of the\nmodels within the family and thus guarantees the natural ranking among the\nsixteen likelihoods. Artificial and real data show that these models have very\ngood clustering and classification performance and that the algorithm is able\nto recover the parameters very well.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 09:50:35 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Subedi", "Sanjeena", ""], ["Punzo", "Antonio", ""], ["Ingrassia", "Salvatore", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1209.6487", "submitter": "Guodong Li", "authors": "Guodong Li, Yang Li and Chih-Ling Tsai", "title": "Quantile correlations and quantile autoregressive modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two important measures, quantile correlation (QCOR)\nand quantile partial correlation (QPCOR). We then apply them to quantile\nautoregressive (QAR) models, and introduce two valuable quantities, the\nquantile autocorrelation function (QACF) and the quantile partial\nautocorrelation function (QPACF). This allows us to extend the classical\nBox-Jenkins approach to quantile autoregressive models. Specifically, the QPACF\nof an observed time series can be employed to identify the autoregressive\norder, while the QACF of residuals obtained from the fitted model can be used\nto assess the model adequacy. We not only demonstrate the asymptotic properties\nof QCOR, QPCOR, QACF, and PQACF, but also show the large sample results of the\nQAR estimates and the quantile version of the Ljung-Box test. Simulation\nstudies indicate that the proposed methods perform well in finite samples, and\nan empirical example is presented to illustrate usefulness.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:39:04 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Li", "Guodong", ""], ["Li", "Yang", ""], ["Tsai", "Chih-Ling", ""]]}, {"id": "1209.6604", "submitter": "Thomas Kreuz", "authors": "Thomas Kreuz, Daniel Chicharro, Conor Houghton, Ralph G Andrzejak,\n  Florian Mormann", "title": "Monitoring spike train synchrony", "comments": "16 pages, 10 figures, 35 references; 1 supplementary figure, 1\n  supplementary movie (see author's webpage\n  http://www.fi.isc.cnr.it/users/thomas.kreuz/sourcecode.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.bio-ph physics.med-ph q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the SPIKE-distance has been proposed as a parameter-free and\ntime-scale independent measure of spike train synchrony. This measure is\ntime-resolved since it relies on instantaneous estimates of spike train\ndissimilarity. However, its original definition led to spuriously high\ninstantaneous values for event-like firing patterns. Here we present a\nsubstantial improvement of this measure which eliminates this shortcoming. The\nreliability gained allows us to track changes in instantaneous clustering,\ni.e., time-localized patterns of (dis)similarity among multiple spike trains.\nAdditional new features include selective and triggered temporal averaging as\nwell as the instantaneous comparison of spike train groups. In a second step, a\ncausal SPIKE-distance is defined such that the instantaneous values of\ndissimilarity rely on past information only so that time-resolved spike train\nsynchrony can be estimated in real-time. We demonstrate that these methods are\ncapable of extracting valuable information from field data by monitoring the\nsynchrony between neuronal spike trains during an epileptic seizure. Finally,\nthe applicability of both the regular and the real-time SPIKE-distance to\ncontinuous data is illustrated on model electroencephalographic (EEG)\nrecordings.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 18:54:45 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2012 01:59:20 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Kreuz", "Thomas", ""], ["Chicharro", "Daniel", ""], ["Houghton", "Conor", ""], ["Andrzejak", "Ralph G", ""], ["Mormann", "Florian", ""]]}]