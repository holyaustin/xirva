[{"id": "1511.00028", "submitter": "Gourab Mukherjee", "authors": "Gourab Mukherjee, Lawrence D. Brown and Paat Rusmevichientong", "title": "Efficient Empirical Bayes prediction under check loss using Asymptotic\n  Risk Estimates", "comments": "massively revamped; new insights, proofs and applied examples added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel Empirical Bayes methodology for prediction under check\nloss in high-dimensional Gaussian models. The check loss is a piecewise linear\nloss function having differential weights for measuring the amount of\nunderestimation or overestimation. Prediction under it differs in fundamental\naspects from estimation or prediction under weighted-quadratic losses. Because\nof the nature of this loss, our inferential target is a pre-chosen quantile of\nthe predictive distribution rather than the mean of the predictive\ndistribution. We develop a new method for constructing uniformly efficient\nasymptotic risk estimates which are then minimized to produce effective linear\nshrinkage predictive rules. In calculating the magnitude and direction of\nshrinkage, our proposed predictive rules incorporate the asymmetric nature of\nthe loss function and are shown to be asymptotically optimal. Using numerical\nexperiments we compare the performance of our method with traditional Empirical\nBayes procedures and obtain encouraging results.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 20:57:45 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 00:50:37 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Mukherjee", "Gourab", ""], ["Brown", "Lawrence D.", ""], ["Rusmevichientong", "Paat", ""]]}, {"id": "1511.00108", "submitter": "Satoshi Kuriki", "authors": "Satoshi Kuriki, Kunihiko Takahashi, Hisayuki Hara", "title": "Recursive computation for evaluating the exact $p$-values of temporal\n  and spatial scan statistics", "comments": "23 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $V$ be a finite set of indices, and let $B_i$, $i=1,\\ldots,m$, be subsets\nof $V$ such that $V=\\bigcup_{i=1}^{m}B_i$. Let $X_i$, $i\\in V$, be independent\nrandom variables, and let $X_{B_i}=(X_j)_{j\\in B_i}$. In this paper, we propose\na recursive computation method to calculate the conditional expectation\n$E\\bigl[\\prod_{i=1}^m\\chi_i(X_{B_i}) \\,|\\, N\\bigr]$ with $N=\\sum_{i\\in V}X_i$\ngiven, where $\\chi_i$ is an arbitrary function. Our method is based on the\nrecursive summation/integration technique using the Markov property in\nstatistics. To extract the Markov property, we define an undirected graph whose\ncliques are $B_j$, and obtain its chordal extension, from which we present the\nexpressions of the recursive formula. This methodology works for a class of\ndistributions including the Poisson distribution (that is, the conditional\ndistribution is the multinomial). This problem is motivated from the evaluation\nof the multiplicity-adjusted $p$-value of scan statistics in spatial\nepidemiology. As an illustration of the approach, we present the real data\nanalyses to detect temporal and spatial clustering.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 10:36:41 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Kuriki", "Satoshi", ""], ["Takahashi", "Kunihiko", ""], ["Hara", "Hisayuki", ""]]}, {"id": "1511.00119", "submitter": "Aluisio Pinheiro", "authors": "Airton Kist, Aluisio Pinheiro", "title": "Wavelet Functional Data Analysis for FANOVA Models under Dependent\n  Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the wavelet tests for fixed effects FANOVA models with iid errors,\nproposed in Abramovich et al, 2004 to FANOVA models with dependent errors and\nprovide an iterative Cochrane-Orcutt type procedure to estimate the parameters\nand the functional. The function is estimated through a nonlinear wavelet\nestimator. Nonparametric tests based on the optimal performance of nonlinear\nwavelet estimators are also proposed. The method is illustrated on real data\nsets and in simulated studies. The simulation also addresses the test\nperformance under realistic sample sizes.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 12:33:28 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Kist", "Airton", ""], ["Pinheiro", "Aluisio", ""]]}, {"id": "1511.00128", "submitter": "Naveen Narisetty", "authors": "Naveen N. Narisetty, Vijayan N. Nair", "title": "Extremal Depth for Functional Data and Applications", "comments": "Journal of the American Statistical Association (Theory and Methods),\n  To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new notion called `extremal depth' (ED) for functional data,\ndiscuss its properties, and compare its performance with existing concepts. The\nproposed notion is based on a measure of extreme `outlyingness'. ED has several\ndesirable properties that are not shared by other notions and is especially\nwell suited for obtaining central regions of functional data and function\nspaces. In particular: a) the central region achieves the nominal (desired)\nsimultaneous coverage probability; b) there is a correspondence between\nED-based (simultaneous) central regions and appropriate point-wise central\nregions; and c) the method is resistant to certain classes of functional\noutliers. The paper examines the performance of ED and compares it with other\ndepth notions. Its usefulness is demonstrated through applications to\nconstructing central regions, functional boxplots, outlier detection, and\nsimultaneous confidence bands in regression problems.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 14:17:09 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Narisetty", "Naveen N.", ""], ["Nair", "Vijayan N.", ""]]}, {"id": "1511.00154", "submitter": "Christos Merkatas", "authors": "Christos Merkatas, Konstantinos Kaloudis, Spyridon J. Hatjispyros", "title": "A Bayesian Nonparametric approach to Reconstruction and Prediction of\n  Random Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric mixture model for the reconstruction and\nprediction from observed time series data, of discretized stochastic dynamical\nsystems, based on Markov Chain Monte Carlo methods (MCMC). Our results can be\nused by researchers in physical modeling interested in a fast and accurate\nestimation of low dimensional stochastic models when the size of the observed\ntime series is small and the noise process (perhaps) is non-Gaussian. The\ninference procedure is demonstrated specifically in the case of polynomial maps\nof arbitrary degree and when a Geometric Stick Breaking mixture process prior\nover the space of densities, is applied to the additive errors. Our method is\nparsimonious compared to Bayesian nonparametric techniques based on Dirichlet\nprocess mixtures, flexible and general. Simulations based on synthetic time\nseries are presented.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 17:26:19 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 08:15:36 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Merkatas", "Christos", ""], ["Kaloudis", "Konstantinos", ""], ["Hatjispyros", "Spyridon J.", ""]]}, {"id": "1511.00237", "submitter": "Koby Todros", "authors": "Koby Todros and Alfred O. Hero", "title": "Measure-Transformed Quasi Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the Gaussian quasi maximum likelihood estimator (GQMLE) is\ngeneralized by applying a transform to the probability distribution of the\ndata. The proposed estimator, called measure-transformed GQMLE (MT-GQMLE),\nminimizes the empirical Kullback-Leibler divergence between a transformed\nprobability distribution of the data and a hypothesized Gaussian probability\nmeasure. By judicious choice of the transform we show that, unlike the GQMLE,\nthe proposed estimator can gain sensitivity to higher-order statistical moments\nand resilience to outliers leading to significant mitigation of the model\nmismatch effect on the estimates. Under some mild regularity conditions we show\nthat the MT-GQMLE is consistent, asymptotically normal and unbiased.\nFurthermore, we derive a necessary and sufficient condition for asymptotic\nefficiency. A data driven procedure for optimal selection of the measure\ntransformation parameters is developed that minimizes the trace of an empirical\nestimate of the asymptotic mean-squared-error matrix. The MT-GQMLE is applied\nto linear regression and source localization and numerical comparisons\nillustrate its robustness and resilience to outliers.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 12:15:47 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 12:54:42 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 19:18:15 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Todros", "Koby", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1511.00273", "submitter": "Daniel McCarthy", "authors": "Daniel McCarthy, Kai Zhang, Lawrence Brown, Richard Berk, Andreas\n  Buja, Edward George, and Linda Zhao", "title": "Calibrated Percentile Double Bootstrap For Robust Linear Regression\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference for the parameters of a linear model when the\ncovariates are random and the relationship between response and covariates is\npossibly non-linear. Conventional inference methods such as z-intervals perform\npoorly in these cases. We propose a double bootstrap-based calibrated\npercentile method, perc-cal, as a general-purpose CI method which performs very\nwell relative to alternative methods in challenging situations such as these.\nThe superior performance of perc-cal is demonstrated by a thorough,\nfull-factorial design synthetic data study as well as a real data example\ninvolving the length of criminal sentences. We also provide theoretical\njustification for the perc-cal method under mild conditions. The method is\nimplemented in the R package `perccal', available through CRAN and coded\nprimarily in C++, to make it easier for practitioners to use.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 16:49:02 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 13:18:44 GMT"}, {"version": "v3", "created": "Mon, 16 Jan 2017 17:40:56 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["McCarthy", "Daniel", ""], ["Zhang", "Kai", ""], ["Brown", "Lawrence", ""], ["Berk", "Richard", ""], ["Buja", "Andreas", ""], ["George", "Edward", ""], ["Zhao", "Linda", ""]]}, {"id": "1511.00282", "submitter": "Ning Hao", "authors": "Yue Selena Niu, Ning Hao, and Bin Dong", "title": "A New Reduced-Rank Linear Discriminant Analysis Method and Its\n  Applications", "comments": "This is the accepted version which may be slightly different from the\n  published version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-class classification problems for high dimensional data.\nFollowing the idea of reduced-rank linear discriminant analysis (LDA), we\nintroduce a new dimension reduction tool with a flavor of supervised principal\ncomponent analysis (PCA). The proposed method is computationally efficient and\ncan incorporate the correlation structure among the features. Besides the\ntheoretical insights, we show that our method is a competitive classification\ntool by simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 17:46:50 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 23:19:48 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Niu", "Yue Selena", ""], ["Hao", "Ning", ""], ["Dong", "Bin", ""]]}, {"id": "1511.00284", "submitter": "Gregory Rice", "authors": "Lajos Horv\\'ath and Gregory Rice", "title": "Empirical eigenvalue based testing for structural breaks in linear panel\n  data models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for stability in linear panel data models has become an important\ntopic in both the statistics and econometrics research communities. The\navailable methodologies address testing for changes in the mean/linear trend,\nor testing for breaks in the covariance structure by checking for the constancy\nof common factor loadings. In such cases when an external shock induces a\nchange to the stochastic structure of panel data, it is unclear whether the\nchange would be reflected in the mean, the covariance structure, or both. In\nthis paper, we develop a test for structural stability of linear panel data\nmodels that is based on monitoring for changes in the largest eigenvalue of the\nsample covariance matrix. The asymptotic distribution of the proposed test\nstatistic is established under the null hypothesis that the mean and covariance\nstructure of the panel data's cross sectional units remain stable during the\nobservation period. We show that the test is consistent assuming common breaks\nin the mean or factor loadings. These results are investigated by means of a\nMonte Carlo simulation study, and their usefulness is demonstrated with an\napplication to U.S. treasury yield curve data, in which some interesting\nfeatures of the 2007-2008 subprime crisis are illuminated.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 18:26:47 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Horv\u00e1th", "Lajos", ""], ["Rice", "Gregory", ""]]}, {"id": "1511.00323", "submitter": "Steven Thompson", "authors": "Steven K. Thompson", "title": "Interventions in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interventions are made in networks to change the network or its values in a\ndesired way. The intervention strategies evaluated in the study described here\nuse network sampling designs to find units to which interventions are applied.\nAn intervention applied to a network node or link can change a value associated\nwith that unit. Over time the effect of the intervention can have an effect on\nthe population that goes beyond the sample units to which it is directly\napplied. This paper describes the methods used for this study. These include a\nvariety of link-tracing sampling designs in networks, a number of types of\ninterventions, and a temporal spatial network model in which the intervention\nstrategies are evaluated. An intervention strategy is associated with an agent\nand different intervention strategies interact and adapt to each other over\ntime. Some preliminary results are summarized regarding potential intervention\nstrategies to help alleviate the HIV epidemic.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 22:29:07 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Thompson", "Steven K.", ""]]}, {"id": "1511.00370", "submitter": "Chen Chen", "authors": "Chen Chen, Min Ren, Min Zhang, and Dabao Zhang", "title": "A Two-Stage Penalized Least Squares Method for Constructing Large\n  Systems of Structural Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage penalized least squares method to build large systems\nof structural equations based on the instrumental variables view of the\nclassical two-stage least squares method. We show that, with large numbers of\nendogenous and exogenous variables, the system can be constructed via\nconsistent estimation of a set of conditional expectations at the first stage,\nand consistent selection of regulatory effects at the second stage. While the\nconsistent estimation at the first stage can be obtained via the ridge\nregression, the adaptive lasso is employed at the second stage to achieve the\nconsistent selection. The resultant estimates of regulatory effects enjoy the\noracle properties. This method is computationally fast and allows for parallel\nimplementation. We demonstrate its effectiveness via simulation studies and\nreal data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 03:45:39 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 00:23:21 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 02:20:48 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Chen", "Chen", ""], ["Ren", "Min", ""], ["Zhang", "Min", ""], ["Zhang", "Dabao", ""]]}, {"id": "1511.00420", "submitter": "Holger Drees", "authors": "Holger Drees", "title": "Bootstrapping Empirical Processes of Cluster Functionals with\n  Application to Extremograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the extreme value analysis of time series, not only the tail behavior is\nof interest, but also the serial dependence plays a crucial role. Drees and\nRootz\\'en (2010) established limit theorems for a general class of empirical\nprocesses of so-called cluster functionals which can be used to analyse various\naspects of the extreme value behavior of mixing time series. However, usually\nthe limit distribution is too complex to enable a direct construction of\nconfidence regions. Therefore, we suggest a multiplier block bootstrap analog\nto the empirical processes of cluster functionals. It is shown that under\nvirtually the same conditions as used by Drees and Rootz\\'en (2010),\nconditionally on the data, the bootstrap processes converge to the same limit\ndistribution. These general results are applied to construct confidence regions\nfor the empirical extremogram introduced by Davis and Mikosch (2009). In a\nsimulation study, the confidence intervals constructed by our multiplier block\nbootstrap approach compare favorably to the stationary bootstrap proposed by\nDavis et al.\\ (2012).\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 09:25:58 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Drees", "Holger", ""]]}, {"id": "1511.00507", "submitter": "Guillaume Chauvet", "authors": "H\\'el\\`ene Juillard and Guillaume Chauvet and Anne Ruiz-Gazen", "title": "Estimation under cross-classified sampling with application to a\n  childhood survey", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-classified sampling design consists in drawing samples from a\ntwo-dimension population, independently in each dimension. Such design is\ncommonly used in consumer price index surveys and has been recently applied to\ndraw a sample of babies in the French ELFE survey, by crossing a sample of\nmaternity units and a sample of days. We propose to derive a general theory of\nestimation for this sampling design. We consider the Horvitz-Thompson estimator\nfor a total, and show that the cross-classified design will usually result in a\nloss of efficiency as compared to the widespread two-stage design. We obtain\nthe asymptotic distribution of the Horvitz-Thompson estimator, and several\nunbiased variance estimators. Facing the problem of possibly negative values,\nwe propose simplified non-negative variance estimators and study their bias\nunder a super-population model. The proposed estimators are compared for totals\nand ratios on simulated data. An application on real data from the ELFE survey\nis also presented, and we make some recommendations. Supplementary materials\nare available online.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 14:11:01 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Juillard", "H\u00e9l\u00e8ne", ""], ["Chauvet", "Guillaume", ""], ["Ruiz-Gazen", "Anne", ""]]}, {"id": "1511.00521", "submitter": "Laura Forastiere", "authors": "Laura Forastiere, Fabrizia Mealli, Luke Miratrix", "title": "Posterior Predictive P-values with Fisher Randomization Tests in\n  Noncompliance Settings: Test Statistics vs Discrepancy Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In randomized experiments with noncompliance, tests may focus on compliers\nrather than on the overall sample. Rubin (1998) put forth such a method, and\nargued that testing for the complier average causal effect and averaging\npermutation based p-values over the posterior distribution of the compliance\nstatus could increase power, as compared to general intent-to-treat tests. The\ngeneral scheme is to repeatedly do a two-step process of imputing missing\ncompliance statuses and conducting a permutation test with the completed data.\nIn this paper, we explore this idea further, comparing the use of discrepancy\nmeasures, which depend on unknown but imputed parameters, to classical test\nstatistics and exploring different approaches for imputing the unknown\ncompliance statuses. We also examine consequences of model misspecification in\nthe imputation step, and discuss to what extent this additional modeling\nundercuts the permutation test's model independence. We find that, especially\nfor discrepancy measures, modeling choices can impact both power and validity.\nIn particular, imputing missing compliance statuses assuming the null can\nradically reduce power, but not doing so can jeopardize validity. Fortunately,\ncovariates predictive of compliance status can mitigate these results. Finally,\nwe compare this overall approach to Bayesian model-based tests, that is tests\nthat are directly derived from posterior credible intervals, under both correct\nand incorrect model specification. We find that adding the permutation step in\nan otherwise Bayesian approach improves robustness to model specification\nwithout substantial loss of power.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 14:35:17 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 14:30:45 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2016 15:54:08 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Forastiere", "Laura", ""], ["Mealli", "Fabrizia", ""], ["Miratrix", "Luke", ""]]}, {"id": "1511.00632", "submitter": "Linglong Kong", "authors": "Dengdeng Yu and Linglong Kong and Ivan Mizera", "title": "Partial Functional Linear Quantile Regression for Neuroimaging Data\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a prediction procedure for the functional linear quantile\nregression model by using partial quantile covariance techniques and develop a\nsimple partial quantile regression (SIMPQR) algorithm to efficiently extract\npartial quantile regression (PQR) basis for estimating functional coefficients.\nWe further extend our partial quantile covariance techniques to functional\ncomposite quantile regression (CQR) defining partial composite quantile\ncovariance. There are three major contributions. (1) We define partial quantile\ncovariance between two scalar variables through linear quantile regression. We\ncompute PQR basis by sequentially maximizing the partial quantile covariance\nbetween the response and projections of functional covariates. (2) In order to\nefficiently extract PQR basis, we develop a SIMPQR algorithm analogous to\nsimple partial least squares (SIMPLS). (3) Under the homoscedasticity\nassumption, we extend our techniques to partial composite quantile covariance\nand use it to find the partial composite quantile regression (PCQR) basis. The\nSIMPQR algorithm is then modified to obtain the SIMPCQR algorithm. Two\nsimulation studies show the superiority of our proposed methods. Two real data\nfrom ADHD-200 sample and ADNI are analyzed using our proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 19:04:35 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Yu", "Dengdeng", ""], ["Kong", "Linglong", ""], ["Mizera", "Ivan", ""]]}, {"id": "1511.00634", "submitter": "Hadeel Klakattawi", "authors": "Hadeel S. Klakattawi, Veronica Vinciotti and Keming Yu", "title": "A Simple and Adaptive Dispersion Regression Model for Count Data", "comments": null, "journal-ref": "Entropy 20, no. 2 (2018): 142", "doi": "10.3390/e20020142", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression for count data is widely performed by models such as Poisson,\nnegative binomial (NB) and zero-inflated regression. A challenge often faced by\npractitioners is the selection of the right model to take into account\ndispersion, which typically occurs in count datasets. It is highly desirable to\nhave a unified model that can automatically adapt to the underlying dispersion\nand that can be easily implemented in practice. In this paper, a discrete\nWeibull regression model is shown to be able to adapt in a simple way to\ndifferent types of dispersions relative to Poisson regression: overdispersion,\nunderdispersion and covariate-specific dispersion. Maximum likelihood can be\nused for efficient parameter estimation. The description of the model,\nparameter inference and model diagnostics is accompanied by simulated and real\ndata analyses.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 19:12:05 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 17:02:19 GMT"}, {"version": "v3", "created": "Tue, 12 Jul 2016 20:01:31 GMT"}, {"version": "v4", "created": "Wed, 1 Aug 2018 01:36:09 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Klakattawi", "Hadeel S.", ""], ["Vinciotti", "Veronica", ""], ["Yu", "Keming", ""]]}, {"id": "1511.00718", "submitter": "Yin Xia", "authors": "Yin Xia and Lexin Li", "title": "Hypothesis Testing of Matrix Graph Model with Application to Brain\n  Connectivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain connectivity analysis is now at the foreground of neuroscience\nresearch. A connectivity network is characterized by a graph, where nodes\nrepresent neural elements such as neurons and brain regions, and links\nrepresent statistical dependences that are often encoded in terms of partial\ncorrelations. Such a graph is inferred from matrix-valued neuroimaging data\nsuch as electroencephalography and functional magnetic resonance imaging. There\nhave been a good number of successful proposals for sparse precision matrix\nestimation under normal or matrix normal distribution; however, this family of\nsolutions do not offer a statistical significance quantification for the\nestimated links. In this article, we adopt a matrix normal distribution\nframework and formulate the brain connectivity analysis as a precision matrix\nhypothesis testing problem. Based on the separable spatial-temporal dependence\nstructure, we develop oracle and data-driven procedures to test the global\nhypothesis that all spatial locations are conditionally independent, which are\nshown to be particularly powerful against the sparse alternatives. In addition,\nsimultaneous tests for identifying conditional dependent spatial locations with\nfalse discovery rate control are proposed in both oracle and data-driven\nsettings. Theoretical results show that the data-driven procedures perform\nasymptotically as well as the oracle procedures and enjoy certain optimality\nproperties. The empirical finite-sample performance of the proposed tests is\nstudied via simulations, and the new tests are applied on a real\nelectroencephalography data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 21:33:17 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Xia", "Yin", ""], ["Li", "Lexin", ""]]}, {"id": "1511.00764", "submitter": "James Johndrow", "authors": "James E. Johndrow and Anirban Bhattacharya", "title": "Optimal Gaussian approximations to the posterior for log-linear models\n  with Diaconis-Ylvisaker priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contingency table analysis, sparse data is frequently encountered for even\nmodest numbers of variables, resulting in non-existence of maximum likelihood\nestimates. A common solution is to obtain regularized estimates of the\nparameters of a log-linear model. Bayesian methods provide a coherent approach\nto regularization, but are often computationally intensive. Conjugate priors\nease computational demands, but the conjugate Diaconis-Ylvisaker priors for the\nparameters of log-linear models do not give rise to closed form credible\nregions, complicating posterior inference. Here we derive the optimal Gaussian\napproximation to the posterior for log-linear models with Diaconis-Ylvisaker\npriors, and provide convergence rate and finite-sample bounds for the\nKullback-Leibler divergence between the exact posterior and the optimal\nGaussian approximation. We demonstrate empirically in simulations and a real\ndata application that the approximation is highly accurate, even in relatively\nsmall samples. The proposed approximation provides a computationally scalable\nand principled approach to regularized estimation and approximate Bayesian\ninference for log-linear models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 03:37:40 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Johndrow", "James E.", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1511.00961", "submitter": "Palaniappan Vellaisamy", "authors": "P. Vellaisamy", "title": "The Unbiasedness Approach to Linear Regression Models", "comments": "19 pages including two tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear regression models are widely used statistical techniques in\nnumerous practical applications. The standard regression model requires several\nassumptions about the regres- sors and the error term. The regression\nparameters are estimated using the least-squares method. In this paper, we\nconsider the regression model with arbitrary regressors and with- out the error\nterm. An explicit expression for the regression parameters vector is obtained.\nThe unbiasedness approach is used to estimate the regression parameters and its\nvarious properties are investigated. It is shown that the resulting unbiased\nestimator equals the least-squares estimator for the fixed design model. The\nanalysis of residuals and the regres- sion sum of squares can be carried out in\na natural way. The unbiased estimator of the dispersion matrix of the unbiased\nestimator is also obtained. Applications to AR(p) model and numerical examples\nare also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 16:07:21 GMT"}], "update_date": "2016-10-23", "authors_parsed": [["Vellaisamy", "P.", ""]]}, {"id": "1511.00990", "submitter": "Guillaume Chauvet", "authors": "H\\'el\\`ene Chaput, Guillaume Chauvet, David Haziza, Laurianne\n  Salembier and Julie Solard", "title": "Joint imputation procedures for categorical variables", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal imputation, which consists of imputing each item requiring\nimputation separately, is often used in surveys. This type of imputation\nprocedures leads to asymptotically unbiased estimators of simple parameters\nsuch as population totals (or means), but tends to distort relationships\nbetween variables. As a result, it generally leads to biased estimators of\nbivariate parameters such as coefficients of correlation or odd-ratios.\nHousehold and social surveys typically collect categorical variables, for which\nmissing values are usually handled by nearest-neighbour imputation or random\nhot-deck imputation. In this paper, we propose a simple random imputation\nprocedure, closely related to random hot-deck imputation, which succeeds in\npreserving the relationship between categorical variables. Also, a fully\nefficient version of the latter procedure is proposed. A limited simulation\nstudy compares several estimation procedures in terms of relative bias and\nrelative efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 17:13:08 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Chaput", "H\u00e9l\u00e8ne", ""], ["Chauvet", "Guillaume", ""], ["Haziza", "David", ""], ["Salembier", "Laurianne", ""], ["Solard", "Julie", ""]]}, {"id": "1511.01124", "submitter": "Heng Lian", "authors": "Ming-Yen Cheng, Sanying Feng, Gaorong Li, Heng Lian", "title": "Greedy Forward Regression for Variable Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two popular variable screening methods under the ultra-high dimensional\nsetting with the desirable sure screening property are the sure independence\nscreening (SIS) and the forward regression (FR). Both are classical variable\nscreening methods and recently have attracted greater attention under the new\nlight of high-dimensional data analysis. We consider a new and simple screening\nmethod that incorporates multiple predictors in each step of forward\nregression, with decision on which variables to incorporate based on the same\ncriterion. If only one step is carried out, it actually reduces to the SIS.\nThus it can be regarded as a generalization and unification of the FR and the\nSIS. More importantly, it preserves the sure screening property and has similar\ncomputational complexity as FR in each step, yet it can discover the relevant\ncovariates in fewer steps. Thus, it reduces the computational burden of FR\ndrastically while retaining advantages of the latter over SIS. Furthermore, we\nshow that it can find all the true variables if the number of steps taken is\nthe same as the correct model size, even when using the original FR. An\nextensive simulation study and application to two real data examples\ndemonstrate excellent performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 21:31:19 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Cheng", "Ming-Yen", ""], ["Feng", "Sanying", ""], ["Li", "Gaorong", ""], ["Lian", "Heng", ""]]}, {"id": "1511.01214", "submitter": "Giri Gopalan", "authors": "Giri Gopalan", "title": "Quantification of observed prior and likelihood information in\n  parametric Bayesian modeling", "comments": "Abbreviated and edited conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two data-dependent information metrics are developed to quantify the\ninformation of the prior and likelihood functions within a parametric Bayesian\nmodel, one of which is closely related to the reference priors from Berger,\nBernardo, and Sun, and information measure introduced by Lindley. A combination\nof theoretical, empirical, and computational support provides evidence that\nthese information-theoretic metrics may be useful diagnostic tools when\nperforming a Bayesian analysis.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 06:07:57 GMT"}, {"version": "v10", "created": "Fri, 24 Mar 2017 12:39:47 GMT"}, {"version": "v11", "created": "Mon, 27 Mar 2017 17:46:05 GMT"}, {"version": "v12", "created": "Sun, 18 Jun 2017 20:18:45 GMT"}, {"version": "v13", "created": "Thu, 7 Sep 2017 00:56:38 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 18:08:01 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 20:53:12 GMT"}, {"version": "v4", "created": "Sat, 12 Dec 2015 16:42:04 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2015 20:11:08 GMT"}, {"version": "v6", "created": "Thu, 7 Jan 2016 21:10:04 GMT"}, {"version": "v7", "created": "Mon, 7 Mar 2016 05:37:01 GMT"}, {"version": "v8", "created": "Sat, 25 Jun 2016 05:17:53 GMT"}, {"version": "v9", "created": "Mon, 12 Dec 2016 01:43:46 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Gopalan", "Giri", ""]]}, {"id": "1511.01343", "submitter": "Matthieu Marbac", "authors": "Matthieu Marbac and Mohammed Sedki", "title": "A Family of Blockwise One-Factor Distributions for Modelling\n  High-Dimensional Binary Data", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of one factor distributions for high-dimensional\nbinary data. The model provides an explicit probability for each event, thus\navoiding the numeric approximations often made by existing methods. Model\ninterpretation is easy since each variable is described by two continuous\nparameters (corresponding to its marginal probability and to its strength of\ndependency with the other variables) and by one binary parameter (defining if\nthe dependencies are positive or negative). An extension of this new model is\nproposed by assuming that the variables are split into independent blocks which\nfollow the new one factor distribution. Parameter estimation is performed by\nthe inference margin procedure where the second step is achieved by an\nexpectation-maximization algorithm. Model selection is carried out by a\ndeterministic approach which strongly reduces the number of competing models.\nThis approach uses a hierarchical ascendant classification of the variables\nbased on the empirical version of Cramer's V for selecting a narrow subset of\nmodels. The consistency of such procedure is shown. The new model is evaluated\non numerical experiments and on a real data set. The procedure is implemented\nin the R package MvBinary available on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 14:20:52 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Marbac", "Matthieu", ""], ["Sedki", "Mohammed", ""]]}, {"id": "1511.01400", "submitter": "Joshua Habiger D", "authors": "Joshua Habiger and David Watts and Michael Anderson", "title": "Multiple Testing with Heterogeneous Multinomial Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False discovery rate (FDR) procedures provide misleading inference when\ntesting multiple null hypotheses with heterogeneous multinomial data. For\nexample, in the motivating study the goal is to identify species of bacteria\nnear the roots of wheat plants (rhizobacteria) that are associated with\nproductivity, but standard procedures discover the most abundant species even\nwhen the association is weak or negligible, and fail to discover strong\nassociations when species are not abundant. Consequently, a list of abundant\nspecies is produced by the multiple testing procedure even though the goal was\nto provide a list of producitivity-associated species. This paper provides an\nFDR method based on a mixture of multinomial distributions and shows that it\ntends to discover more non-negligible effects and fewer negligible effects when\nthe data are heterogeneous across tests. The proposed method and competing\nmethods are applied to the motivating data. The new method identifies more\nspecies that are strongly associated with productivity and identifies fewer\nspecies that are weakly associated with productivity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 17:11:53 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Habiger", "Joshua", ""], ["Watts", "David", ""], ["Anderson", "Michael", ""]]}, {"id": "1511.01443", "submitter": "Cheng Huang", "authors": "Cheng Huang and Xiaoming Huo", "title": "A Distributed One-Step Estimator", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed statistical inference has recently attracted enormous attention.\nMany existing work focuses on the averaging estimator. We propose a one-step\napproach to enhance a simple-averaging based distributed estimator. We derive\nthe corresponding asymptotic properties of the newly proposed estimator. We\nfind that the proposed one-step estimator enjoys the same asymptotic properties\nas the centralized estimator. The proposed one-step approach merely requires\none additional round of communication in relative to the averaging estimator;\nso the extra communication burden is insignificant. In finite sample cases,\nnumerical examples show that the proposed estimator outperforms the simple\naveraging estimator with a large margin in terms of the mean squared errors. A\npotential application of the one-step approach is that one can use multiple\nmachines to speed up large scale statistical inference with little compromise\nin the quality of estimators. The proposed method becomes more valuable when\ndata can only be available at distributed machines with limited communication\nbandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 19:18:41 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 20:04:10 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Huang", "Cheng", ""], ["Huo", "Xiaoming", ""]]}, {"id": "1511.01448", "submitter": "Fl\\'avio de Melo", "authors": "Fl\\'avio Eler De Melo, Simon Maskell, Matteo Fasiolo, Fred Daum", "title": "Stochastic Particle Flow for Nonlinear High-Dimensional Filtering\n  Problems", "comments": "79 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A series of novel filters for probabilistic inference that propose an\nalternative way of performing Bayesian updates, called particle flow filters,\nhave been attracting recent interest. These filters provide approximate\nsolutions to nonlinear filtering problems. They do so by defining a continuum\nof densities between the prior probability density and the posterior, i.e. the\nfiltering density. Building on these methods' successes, we propose a novel\nfilter. The new filter aims to address the shortcomings of sequential Monte\nCarlo methods when applied to important nonlinear high-dimensional filtering\nproblems. The novel filter uses equally weighted samples, each of which is\nassociated with a local solution of the Fokker-Planck equation. This hybrid of\nMonte Carlo and local parametric approximation gives rise to a global\napproximation of the filtering density of interest. We show that, when compared\nwith state-of-the-art methods, the Gaussian-mixture implementation of the new\nfiltering technique, which we call Stochastic Particle Flow, has utility in the\ncontext of benchmark nonlinear high-dimensional filtering problems. In\naddition, we extend the original particle flow filters for tackling\nmulti-target multi-sensor tracking problems to enable a comparison with the new\nfilter.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 19:40:16 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 14:27:22 GMT"}, {"version": "v3", "created": "Thu, 23 Mar 2017 17:04:23 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["De Melo", "Fl\u00e1vio Eler", ""], ["Maskell", "Simon", ""], ["Fasiolo", "Matteo", ""], ["Daum", "Fred", ""]]}, {"id": "1511.01453", "submitter": "Clement de Chaisemartin", "authors": "Clement de Chaisemartin, Luc Behaghel", "title": "Estimating the effect of treatments allocated by randomized waiting\n  lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oversubscribed treatments are often allocated using randomized waiting lists.\nApplicants are ranked randomly, and treatment offers are made following that\nranking until all seats are filled. To estimate causal effects, researchers\noften compare applicants getting and not getting an offer. We show that those\ntwo groups are not statistically comparable. Therefore, the estimator arising\nfrom that comparison is inconsistent. We propose a new estimator, and show that\nit is consistent. Finally, we revisit an application, and we show that using\nour estimator can lead to sizably different results from those obtained using\nthe commonly used estimator.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 17:46:49 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 17:39:21 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 18:03:04 GMT"}, {"version": "v4", "created": "Thu, 16 Feb 2017 17:42:20 GMT"}, {"version": "v5", "created": "Sat, 23 Dec 2017 11:26:33 GMT"}, {"version": "v6", "created": "Mon, 22 Oct 2018 18:16:38 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["de Chaisemartin", "Clement", ""], ["Behaghel", "Luc", ""]]}, {"id": "1511.01478", "submitter": "Joshua Loftus", "authors": "Joshua R. Loftus and Jonathan E. Taylor", "title": "Selective inference in regression models with groups of variables", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general mathematical framework for selective inference with\nsupervised model selection procedures characterized by quadratic forms in the\noutcome variable. Forward stepwise with groups of variables is an important\nspecial case as it allows models with categorical variables or factors. Models\ncan be chosen by AIC, BIC, or a fixed number of steps. We provide an exact\nsignificance test for each group of variables in the selected model based on an\nappropriately truncated $\\chi$ or $F$ distribution for the cases of known and\nunknown $\\sigma^2$ respectively. An efficient software implementation is\navailable as a package in the R statistical programming language.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 20:56:25 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Loftus", "Joshua R.", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1511.01609", "submitter": "Benjamin Risk", "authors": "Benjamin B. Risk, David S. Matteson, David Ruppert", "title": "Linear Non-Gaussian Component Analysis via Maximum Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is popular in many applications,\nincluding cognitive neuroscience and signal processing. Due to computational\nconstraints, principal component analysis is used for dimension reduction prior\nto ICA (PCA+ICA), which could remove important information. The problem is that\ninteresting independent components (ICs) could be mixed in several principal\ncomponents that are discarded and then these ICs cannot be recovered. We\nformulate a linear non-Gaussian component model with Gaussian noise components.\nTo estimate this model, we propose likelihood component analysis (LCA), in\nwhich dimension reduction and latent variable estimation are achieved\nsimultaneously. Our method orders components by their marginal likelihood\nrather than ordering components by variance as in PCA. We present a parametric\nLCA using the logistic density and a semi-parametric LCA using tilted Gaussians\nwith cubic B-splines. Our algorithm is scalable to datasets common in\napplications (e.g., hundreds of thousands of observations across hundreds of\nvariables with dozens of latent components). In simulations, latent components\nare recovered that are discarded by PCA+ICA methods. We apply our method to\nmultivariate data and demonstrate that LCA is a useful data visualization and\ndimension reduction tool that reveals features not apparent from PCA or\nPCA+ICA. We also apply our method to an fMRI experiment from the Human\nConnectome Project and identify artifacts missed by PCA+ICA. We present\ntheoretical results on identifiability of the linear non-Gaussian component\nmodel and consistency of LCA.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 05:12:27 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 21:50:34 GMT"}, {"version": "v3", "created": "Mon, 2 Oct 2017 01:25:49 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Risk", "Benjamin B.", ""], ["Matteson", "David S.", ""], ["Ruppert", "David", ""]]}, {"id": "1511.01611", "submitter": "Ping Li", "authors": "Tung-Lung Wu and Ping Li", "title": "Tests for High-Dimensional Covariance Matrices Using Random Matrix\n  Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic likelihood ratio test for testing the equality of two covariance\nmatrices breakdowns due to the singularity of the sample covariance matrices\nwhen the data dimension $p$ is larger than the sample size $n$. In this paper,\nwe present a conceptually simple method using random projection to project the\ndata onto the one-dimensional random subspace so that the conventional methods\ncan be applied. Both one-sample and two-sample tests for high-dimensional\ncovariance matrices are studied. Asymptotic results are established and\nnumerical results are given to compare our method with state-of-the-art methods\nin the literature.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 05:24:02 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Wu", "Tung-Lung", ""], ["Li", "Ping", ""]]}, {"id": "1511.01677", "submitter": "Tsagris Michail", "authors": "Michael Tsagris, Ioannis Elmatzoglou and Christos C. Frangos", "title": "The Assessment of Performance of Correlation Estimates in Discrete\n  Bivariate Distributions Using Bootstrap Methodology", "comments": "22 pages with 5 tables and 2 figures", "journal-ref": "Communications in Statistics - Theory and Methods, 2012, 41(1):\n  138--152", "doi": "10.1080/03610926.2010.521281", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Little attention has been given to the correlation coefficient when data come\nfrom discrete or continuous non-normal populations. In this article, we\nconsider the efficiency of two correlation coefficients which are from the same\nfamily, Pearson's and Spearman's estimators. Two discrete bivariate\ndistributions were examined: the Poisson and the Negative Binomial. The\ncomparison between these two estimators took place using classical and\nbootstrap techniques for the construction of confidence intervals. Thus, these\ntechniques are also subject to comparison. Simulation studies were also used\nfor the relative efficiency and bias of the two estimators. Pearson's estimator\nperformed slightly better than Spearman's.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 09:55:44 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Tsagris", "Michael", ""], ["Elmatzoglou", "Ioannis", ""], ["Frangos", "Christos C.", ""]]}, {"id": "1511.01720", "submitter": "Damien McParland", "authors": "Damien McParland and Isobel Claire Gormley", "title": "Model Based Clustering for Mixed Data: clustMD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model based clustering procedure for data of mixed type, clustMD, is\ndeveloped using a latent variable model. It is proposed that a latent variable,\nfollowing a mixture of Gaussian distributions, generates the observed data of\nmixed type. The observed data may be any combination of continuous, binary,\nordinal or nominal variables. clustMD employs a parsimonious covariance\nstructure for the latent variables, leading to a suite of six clustering models\nthat vary in complexity and provide an elegant and unified approach to\nclustering mixed data. An expectation maximisation (EM) algorithm is used to\nestimate clustMD; in the presence of nominal data a Monte Carlo EM algorithm is\nrequired. The clustMD model is illustrated by clustering simulated mixed type\ndata and prostate cancer patients, on whom mixed data have been recorded.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 12:50:26 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["McParland", "Damien", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "1511.01863", "submitter": "Anders Eklund", "authors": "Anders Eklund, Thomas Nichols, Hans Knutsson", "title": "Can parametric statistical methods be trusted for fMRI based group\n  studies?", "comments": null, "journal-ref": "PNAS (2016), vol. 113 no. 28, 7900 - 7905", "doi": "10.1073/pnas.1602413113", "report-no": null, "categories": "stat.AP math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The most widely used task fMRI analyses use parametric methods that depend on\na variety of assumptions. While individual aspects of these fMRI models have\nbeen evaluated, they have not been evaluated in a comprehensive manner with\nempirical data. In this work, a total of 2 million random task fMRI group\nanalyses have been performed using resting state fMRI data, to compute\nempirical familywise error rates for the software packages SPM, FSL and AFNI,\nas well as a standard non-parametric permutation method. While there is some\nvariation, for a nominal familywise error rate of 5% the parametric statistical\nmethods are shown to be conservative for voxel-wise inference and invalid for\ncluster-wise inference; in particular, cluster size inference with a cluster\ndefining threshold of p = 0.01 generates familywise error rates up to 60%. We\nconduct a number of follow up analyses and investigations that suggest the\ncause of the invalid cluster inferences is spatial auto correlation functions\nthat do not follow the assumed Gaussian shape. By comparison, the\nnon-parametric permutation test, which is based on a small number of\nassumptions, is found to produce valid results for voxel as well as cluster\nwise inference. Using real task data, we compare the results between one\nparametric method and the permutation test, and find stark differences in the\nconclusions drawn between the two using cluster inference. These findings speak\nto the need of validating the statistical methods being used in the\nneuroimaging field.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:34:57 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Eklund", "Anders", ""], ["Nichols", "Thomas", ""], ["Knutsson", "Hans", ""]]}, {"id": "1511.01870", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Christoph Dann, Hannes Nickisch", "title": "Thoughts on Massively Scalable Gaussian Processes", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework and early results for massively scalable Gaussian\nprocesses (MSGP), significantly extending the KISS-GP approach of Wilson and\nNickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs)\non billions of datapoints, without requiring distributed inference, or severe\nassumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GP\nlearning and inference to $O(n)$, and the standard $O(n^2)$ complexity per test\npoint prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices as\nKronecker products of Toeplitz matrices approximated by circulant matrices.\nThis multi-level circulant approximation allows one to unify the orthogonal\ncomputational benefits of fast Kronecker and Toeplitz approaches, and is\nsignificantly faster than either approach in isolation; 2) local kernel\ninterpolation and inducing points to allow for arbitrarily located data inputs,\nand $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block\nstructure (BTTB), which enables fast inference and learning when\nmultidimensional Kronecker structure is not present; and 4) projections of the\ninput space to flexibly model correlated inputs and high dimensional data. The\nability to handle many ($m \\approx n$) inducing points allows for near-exact\naccuracy and large scale kernel learning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:51:31 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Dann", "Christoph", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1511.01881", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Maria Konstantinou, Anatoly Zhigljavsky", "title": "A new approach to optimal designs for correlated observations", "comments": "Keywords and Phrases: linear regression, correlated observations,\n  optimal design, Gaussian white mouse model, Doob representation, quadrature\n  formulas AMS Subject classification: Primary 62K05; Secondary: 62M05", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new and efficient method for the construction of\noptimal designs for regression models with dependent error processes. In\ncontrast to most of the work in this field, which starts with a model for a\nfinite number of observations and considers the asymptotic properties of\nestimators and designs as the sample size converges to infinity, our approach\nis based on a continuous time model. We use results from stochastic anal- ysis\nto identify the best linear unbiased estimator (BLUE) in this model. Based on\nthe BLUE, we construct an efficient linear estimator and corresponding optimal\ndesigns in the model for finite sample size by minimizing the mean squared\nerror between the opti- mal solution in the continuous time model and its\ndiscrete approximation with respect to the weights (of the linear estimator)\nand the optimal design points, in particular in the multi-parameter case. In\ncontrast to previous work on the subject the resulting estimators and\ncorresponding optimal designs are very efficient and easy to implement. This\nmeans that they are practi- cally not distinguishable from the weighted least\nsquares estimator and the corresponding optimal designs, which have to be found\nnumerically by non-convex discrete optimization. The advantages of the new\napproach are illustrated in several numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 20:16:34 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Dette", "Holger", ""], ["Konstantinou", "Maria", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1511.01973", "submitter": "Tirthankar Dasgupta", "authors": "Zach Branson, Tirthankar Dasgupta, Donald B. Rubin", "title": "Improving Covariate Balance in 2^K Factorial Designs via Rerandomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial designs are widely used in agriculture, engineering, and the social\nsciences to study the causal effects of several factors simultaneously on a\nresponse. The objective of such a design is to estimate all factorial effects\nof interest, which typically include main effects and interactions among\nfactors. To estimate factorial effects with high precision when a large number\nof pre-treatment covariates are present, balance among covariates across\ntreatment groups should be ensured. We propose utilizing rerandomization to\nensure covariate balance in factorial designs. Although both factorial designs\nand rerandomization have been discussed before, the combination has not. Here,\ntheoretical properties of rerandomization for factorial designs are\nestablished, and empirical results are explored using an application from the\nNew York Department of Education.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 02:57:13 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Branson", "Zach", ""], ["Dasgupta", "Tirthankar", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1511.02140", "submitter": "Mathias Raschke -", "authors": "Mathias Raschke", "title": "A novel model and estimation method for the individual random component\n  of earthquake ground-motion relations", "comments": null, "journal-ref": null, "doi": "10.1007/s10950-016-9573-9", "report-no": null, "categories": "physics.geo-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I introduce a novel approach to modelling the individual\nrandom component (also called the intra-event uncertainty) of a ground-motion\nrelation (GMR), as well as a novel approach to estimating the corresponding\nparameters. In essence, I contend that the individual random component is\nreproduced adequately by a simple stochastic mechanism of random impulses\nacting in the horizontal plane, with random directions. The random number of\nimpulses was Poisson distributed. The parameters of the model were estimated\naccording to a proposal by Raschke (2013a), with the sample of random\ndifference xi=ln(Y1)-ln(Y2), in which Y1 and Y2 are the horizontal components\nof local ground-motion intensity. Any GMR element was eliminated by\nsubtraction, except the individual random components. In the estimation\nprocedure the distribution of difference xi was approximated by combining a\nlarge Monte Carlo simulated sample and Kernel smoothing. The estimated model\nsatisfactorily fitted the difference xi of the sample of peak ground\naccelerations, and the variance of the individual random components was\nconsiderably smaller than that of conventional GMRs. In addition, the\ndependence of variance on the epicentre distance was considered; however, a\ndependence of variance on the magnitude was not detected. Finally, the\ninfluence of the novel model and the corresponding approximations on PSHA was\nresearched. The applied approximations of distribution of the individual random\ncomponent were satisfactory for the researched example of PSHA.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 16:27:57 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 17:45:38 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Raschke", "Mathias", ""]]}, {"id": "1511.02178", "submitter": "Boujemaa Ait-El-Fquih", "authors": "Boujemaa Ait-El-Fquih, Mohamad El Gharamti and Ibrahim Hoteit", "title": "A Bayesian Consistent Dual Ensemble Kalman Filter for State-Parameter\n  Estimation in Subsurface Hydrology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.geo-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble Kalman filtering (EnKF) is an efficient approach to addressing\nuncertainties in subsurface groundwater models. The EnKF sequentially\nintegrates field data into simulation models to obtain a better\ncharacterization of the model's state and parameters. These are generally\nestimated following joint and dual filtering strategies, in which, at each\nassimilation cycle, a forecast step by the model is followed by an update step\nwith incoming observations. The Joint-EnKF directly updates the augmented\nstate-parameter vector while the Dual-EnKF employs two separate filters, first\nestimating the parameters and then estimating the state based on the updated\nparameters. In this paper, we reverse the order of the forecast-update steps\nfollowing the one-step-ahead (OSA) smoothing formulation of the Bayesian\nfiltering problem, based on which we propose a new dual EnKF scheme, the\nDual-EnKF$_{\\rm OSA}$. Compared to the Dual-EnKF, this introduces a new update\nstep to the state in a fully consistent Bayesian framework, which is shown to\nenhance the performance of the dual filtering approach without any significant\nincrease in the computational cost. Numerical experiments are conducted with a\ntwo-dimensional synthetic groundwater aquifer model to assess the performance\nand robustness of the proposed Dual-EnKF$_{\\rm OSA}$, and to evaluate its\nresults against those of the Joint- and Dual-EnKFs. The proposed scheme is able\nto successfully recover both the hydraulic head and the aquifer conductivity,\nfurther providing reliable estimates of their uncertainties. Compared with the\nstandard Joint- and Dual-EnKFs, the proposed scheme is found more robust to\ndifferent assimilation settings, such as the spatial and temporal distribution\nof the observations, and the level of noise in the data. Based on our\nexperimental setups, it yields up to 25% more accurate state and parameters\nestimates.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 23:21:27 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Ait-El-Fquih", "Boujemaa", ""], ["Gharamti", "Mohamad El", ""], ["Hoteit", "Ibrahim", ""]]}, {"id": "1511.02189", "submitter": "Thais Paiva", "authors": "Thais Paiva and Jerry Reiter", "title": "Stop or Continue Data Collection: A Nonignorable Missing Data Approach\n  for Continuous Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to inform decisions about nonresponse follow-up\nsampling. The basic idea is (i) to create completed samples by imputing\nnonrespondents' data under various assumptions about the nonresponse\nmechanisms, (ii) take hypothetical samples of varying sizes from the completed\nsamples, and (iii) compute and compare measures of accuracy and cost for\ndifferent proposed sample sizes. As part of the methodology, we present a new\napproach for generating imputations for multivariate continuous data with\nnonignorable unit nonresponse. We fit mixtures of multivariate normal\ndistributions to the respondents' data, and adjust the probabilities of the\nmixture components to generate nonrespondents' distributions with desired\nfeatures. We illustrate the approaches using data from the 2007 U. S. Census of\nManufactures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 18:41:10 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 18:27:54 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 12:46:50 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Paiva", "Thais", ""], ["Reiter", "Jerry", ""]]}, {"id": "1511.02199", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Yulai Cong, Bo Chen", "title": "The Poisson Gamma Belief Network", "comments": "Neural Information Processing Systems (NIPS2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To infer a multilayer representation of high-dimensional count vectors, we\npropose the Poisson gamma belief network (PGBN) that factorizes each of its\nlayers into the product of a connection weight matrix and the nonnegative real\nhidden units of the next layer. The PGBN's hidden layers are jointly trained\nwith an upward-downward Gibbs sampler, each iteration of which upward samples\nDirichlet distributed connection weight vectors starting from the first layer\n(bottom data layer), and then downward samples gamma distributed hidden units\nstarting from the top hidden layer. The gamma-negative binomial process\ncombined with a layer-wise training strategy allows the PGBN to infer the width\nof each layer given a fixed budget on the width of the first layer. The PGBN\nwith a single hidden layer reduces to Poisson factor analysis. Example results\non text analysis illustrate interesting relationships between the width of the\nfirst layer and the inferred network structure, and demonstrate that the PGBN,\nwhose hidden units are imposed with correlated gamma priors, can add more\nlayers to increase its performance gains over Poisson factor analysis, given\nthe same limit on the width of the first layer.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 19:16:50 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 15:39:50 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Cong", "Yulai", ""], ["Chen", "Bo", ""]]}, {"id": "1511.02222", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing", "title": "Deep Kernel Learning", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce scalable deep kernels, which combine the structural properties\nof deep learning architectures with the non-parametric flexibility of kernel\nmethods. Specifically, we transform the inputs of a spectral mixture base\nkernel with a deep architecture, using local kernel interpolation, inducing\npoints, and structure exploiting (Kronecker and Toeplitz) algebra for a\nscalable kernel representation. These closed-form kernels can be used as\ndrop-in replacements for standard kernels, with benefits in expressive power\nand scalability. We jointly learn the properties of these kernels through the\nmarginal likelihood of a Gaussian process. Inference and learning cost $O(n)$\nfor $n$ training points, and predictions cost $O(1)$ per test point. On a large\nand diverse collection of applications, including a dataset with 2 million\nexamples, we show improved performance over scalable Gaussian processes with\nflexible kernel learning models, and stand-alone deep architectures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 20:38:08 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Hu", "Zhiting", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1511.02339", "submitter": "Dimitris Kugiumtzis", "authors": "Maria Papapetrou and Dimitris Kugiumtzis", "title": "Markov chain order estimation with parametric significance tests of\n  conditional mutual information", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the different approaches suggested in the literature, accurate\nestimation of the order of a Markov chain from a given symbol sequence is an\nopen issue, especially when the order is moderately large. Here, parametric\nsignificance tests of conditional mutual information (CMI) of increasing order\n$m$, $I_c(m)$, on a symbol sequence are conducted for increasing orders $m$ in\norder to estimate the true order $L$ of the underlying Markov chain. CMI of\norder $m$ is the mutual information of two variables in the Markov chain being\n$m$ time steps apart, conditioning on the intermediate variables of the chain.\nThe null distribution of CMI is approximated with a normal and gamma\ndistribution deriving analytic expressions of their parameters, and a gamma\ndistribution deriving its parameters from the mean and variance of the normal\ndistribution. The accuracy of order estimation is assessed with the three\nparametric tests, and the parametric tests are compared to the randomization\nsignificance test and other known order estimation criteria using Monte Carlo\nsimulations of Markov chains with different order $L$, length of symbol\nsequence $N$ and number of symbols $K$. The parametric test using the gamma\ndistribution (with directly defined parameters) is consistently better than the\nother two parametric tests and matches well the performance of the\nrandomization test. The tests are applied to genes and intergenic regions of\nDNA sequences, and the estimated orders are interpreted in view of the results\nfrom the simulation study. The application shows the usefulness of the\nparametric gamma test for long symbol sequences where the randomization test\nbecomes prohibitively slow to compute.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 11:12:11 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Papapetrou", "Maria", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1511.02363", "submitter": "Leon Brian Lucy", "authors": "L.B. Lucy", "title": "Frequentist tests for Bayesian models", "comments": "8 pages, 4 figures. Section 8 rewritten. Additional references.\n  Accepted by Astronomy & Astrophysics", "journal-ref": "A&A 588, A19 (2016)", "doi": "10.1051/0004-6361/201527709", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analogues of the frequentist chi-square and F tests are proposed for testing\ngoodness-of-fit and consistency for Bayesian models. Simple examples exhibit\nthese tests' detection of inconsistency between consecutive experiments with\nidentical parameters, when the first experiment provides the prior for the\nsecond. In a related analysis, a quantitative measure is derived for judging\nthe degree of tension between two different experiments with partially\noverlapping parameter vectors.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 14:57:15 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 10:21:40 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2016 10:19:56 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Lucy", "L. B.", ""]]}, {"id": "1511.02386", "submitter": "Dustin Tran", "authors": "Rajesh Ranganath, Dustin Tran, David M. Blei", "title": "Hierarchical Variational Models", "comments": "Appears in International Conference on Machine Learning, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black box variational inference allows researchers to easily prototype and\nevaluate an array of models. Recent advances allow such algorithms to scale to\nhigh dimensions. However, a central question remains: How to specify an\nexpressive variational distribution that maintains efficient computation? To\naddress this, we develop hierarchical variational models (HVMs). HVMs augment a\nvariational approximation with a prior on its parameters, which allows it to\ncapture complex structure for both discrete and continuous latent variables.\nThe algorithm we develop is black box, can be used for any HVM, and has the\nsame computational efficiency as the original approximation. We study HVMs on a\nvariety of deep discrete latent variable models. HVMs generalize other\nexpressive variational distributions and maintains higher fidelity to the\nposterior.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 19:01:48 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 21:16:38 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1511.02552", "submitter": "Linglong Kong", "authors": "Linglong Kong and Haoxu Shu and Giseon Heo and Qianchuan Chad He", "title": "Estimation for bivariate quantile varying coefficient model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a bivariate quantile regression method for the bivariate varying\ncoefficient model through a directional approach. The varying coefficients are\napproximated by the B-spline basis and an $L_{2}$ type penalty is imposed to\nachieve desired smoothness. We develop a multistage estimation procedure based\nthe Propagation-Separation~(PS) approach to borrow information from nearby\ndirections. The PS method is capable of handling the computational complexity\nraised by simultaneously considering multiple directions to efficiently\nestimate varying coefficients while guaranteeing certain smoothness along\ndirections. We reformulate the optimization problem and solve it by the\nAlternating Direction Method of Multipliers~(ADMM), which is implemented using\nR while the core is written in C to speed it up. Simulation studies are\nconducted to confirm the finite sample performance of our proposed method. A\nreal data on Diffusion Tensor Imaging~(DTI) properties from a clinical study on\nneurodevelopment is analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 01:54:07 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Kong", "Linglong", ""], ["Shu", "Haoxu", ""], ["Heo", "Giseon", ""], ["He", "Qianchuan Chad", ""]]}, {"id": "1511.02644", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo and Simon N. Wood", "title": "Approximate methods for dynamic ecological models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is due to appear as a chapter of the forthcoming Handbook of\nApproximate Bayesian Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont.\nHere we describe some of the circumstances under which statistical ecologists\nmight benefit from using methods that base statistical inference on a set of\nsummary statistics, rather than on the full data. We focus particularly on one\nsuch approach, Synthetic Likelihood, and we show how this method represents an\nalternative to particle filters, for the purpose of fitting State Space Models\nof ecological interest. As an example application, we consider the\nprey-predator model of Turchin and Ellner (2000), and we use it to analyse the\nobserved population dynamics of Fennoscandian voles.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 11:45:38 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Wood", "Simon N.", ""]]}, {"id": "1511.02688", "submitter": "Matthieu Wilhelm", "authors": "Matthieu Wilhelm and Laura M. Sangalli", "title": "Generalized Spatial Regression with Differential Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim at analyzing geostatistical and areal data observed over irregularly\nshaped spatial domains and having a distribution within the exponential family.\nWe propose a generalized additive model that allows to account for\nspatially-varying covariate information. The model is fitted by maximizing a\npenalized log-likelihood function, with a roughness penalty term that involves\na differential quantity of the spatial field, computed over the domain of\ninterest. Efficient estimation of the spatial field is achieved resorting to\nthe finite element method, which provides a basis for piecewise polynomial\nsurfaces. The proposed model is illustrated by an application to the study of\ncriminality in the city of Portland, Oregon, USA.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 14:14:31 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 08:28:38 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Wilhelm", "Matthieu", ""], ["Sangalli", "Laura M.", ""]]}, {"id": "1511.02744", "submitter": "Hui Li", "authors": "Hui Li", "title": "A New Class of Nonsymmetric Multivariate Dependence Measures", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following our previous work on copula-based nonsymmetric bivariate dependence\nmeasures, we propose a new set of conditions on nonsymmetric multivariate\ndependence measures which characterize both independence and complete\ndependence of one random variable on a group of random variables. The measures\nare nonparametric in that they are copula-based and are invariant under\ncontinuous bijective transformations on the group of random variables. We also\nconstruct explicitly new measures that satisfy the conditions. Besides, we\nextend the star product on bivariate copulas to multivariate copulas and prove\nthe DPI condition and self-equitability for the new measures. A further\nextension to measures of dependence of one group of random variables on another\ngroup of random variables is also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 16:25:18 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 15:49:24 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Li", "Hui", ""]]}, {"id": "1511.02796", "submitter": "Ricardo Silva", "authors": "Ricardo Silva", "title": "Bayesian Inference in Cumulative Distribution Fields", "comments": "14 pages, 4 figures. Presented at the 12th Brazilian Meeting on\n  Bayesian Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach for constructing copula functions is by multiplication. Given\nthat products of cumulative distribution functions (CDFs) are also CDFs, an\nadjustment to this multiplication will result in a copula model, as discussed\nby Liebscher (J Mult Analysis, 2008). Parameterizing models via products of\nCDFs has some advantages, both from the copula perspective (e.g., it is\nwell-defined for any dimensionality) and from general multivariate analysis\n(e.g., it provides models where small dimensional marginal distributions can be\neasily read-off from the parameters). Independently, Huang and Frey (J Mach\nLearn Res, 2011) showed the connection between certain sparse graphical models\nand products of CDFs, as well as message-passing (dynamic programming) schemes\nfor computing the likelihood function of such models. Such schemes allows\nmodels to be estimated with likelihood-based methods. We discuss and\ndemonstrate MCMC approaches for estimating such models in a Bayesian context,\ntheir application in copula modeling, and how message-passing can be strongly\nsimplified. Importantly, our view of message-passing opens up possibilities to\nscaling up such methods, given that even dynamic programming is not a scalable\nsolution for calculating likelihood functions in many models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 18:27:22 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Silva", "Ricardo", ""]]}, {"id": "1511.02995", "submitter": "Bryant Chen", "authors": "Bryant Chen, Judea Pearl, Elias Bareinboim", "title": "Incorporating Knowledge into Structural Equation Models using Auxiliary\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend graph-based identification methods by allowing\nbackground knowledge in the form of non-zero parameter values. Such information\ncould be obtained, for example, from a previously conducted randomized\nexperiment, from substantive understanding of the domain, or even an\nidentification technique. To incorporate such information systematically, we\npropose the addition of auxiliary variables to the model, which are constructed\nso that certain paths will be conveniently cancelled. This cancellation allows\nthe auxiliary variables to help conventional methods of identification (e.g.,\nsingle-door criterion, instrumental variables, half-trek criterion), as well as\nmodel testing (e.g., d-separation, over-identification). Moreover, by\niteratively alternating steps of identification and adding auxiliary variables,\nwe can improve the power of existing identification methods via a bootstrapping\napproach that does not require external knowledge. We operationalize this\nmethod for simple instrumental sets (a generalization of instrumental\nvariables) and show that the resulting method is able to identify at least as\nmany models as the most general identification method for linear systems known\nto date. We further discuss the application of auxiliary variables to the tasks\nof model testing and z-identification.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 05:19:00 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 01:59:59 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 01:19:50 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Chen", "Bryant", ""], ["Pearl", "Judea", ""], ["Bareinboim", "Elias", ""]]}, {"id": "1511.03122", "submitter": "Jing Tian", "authors": "Jing Tian, Xiang-Gen Xia, Gang Yang, Wei Cui and Si-Liang Wu", "title": "A Coherent Integration Method Based on Radon Non-uniform FRFT for Random\n  Pulse Repetition Interval (RPRI) Radar", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the range cell migration (RCM) and spectrum spread during the\nintegration time induced by the motion of a target, this paper proposes a new\ncoherent integration method based on Radon non-uniform FRFT (NUFRFT) for random\npulse repetition interval (RPRI) radar. In this method, RCM is eliminated via\nsearching in the motion parameters space and the spectrum spread is resolved by\nusing NUFRFT. Comparisons with other popular methods, moving target detection\n(MTD), Radon-Fourier transform (RFT), and Radon-Fractional Fourier Transform\n(RFRFT) are performed. The simulation results demonstrate that the proposed\nmethod can detect the moving target even in low SNR scenario and is superior to\nthe other two methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:56:29 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Tian", "Jing", ""], ["Xia", "Xiang-Gen", ""], ["Yang", "Gang", ""], ["Cui", "Wei", ""], ["Wu", "Si-Liang", ""]]}, {"id": "1511.03145", "submitter": "Clara Grazian", "authors": "Clara Grazian and Christian Robert", "title": "Jeffreys priors for mixture estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Jeffreys priors usually are well-defined for the parameters of mixtures\nof distributions, they are not available in closed form. Furthermore, they\noften are improper priors. Hence, they have never been used to draw inference\non the mixture parameters. We study in this paper the implementation and the\nproperties of Jeffreys priors in several mixture settings, show that the\nassociated posterior distributions most often are improper, and then propose a\nnoninformative alternative for the analysis of mixtures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 15:35:30 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 16:43:54 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Grazian", "Clara", ""], ["Robert", "Christian", ""]]}, {"id": "1511.03334", "submitter": "Rajen Shah", "authors": "Rajen D. Shah and Peter B\\\"uhlmann", "title": "Goodness of fit tests for high-dimensional linear models", "comments": "42 pages, 12 figures", "journal-ref": null, "doi": "10.1111/rssb.12234", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a framework for constructing goodness of fit tests in\nboth low and high-dimensional linear models. We advocate applying regression\nmethods to the scaled residuals following either an ordinary least squares or\nLasso fit to the data, and using some proxy for prediction error as the final\ntest statistic. We call this family Residual Prediction (RP) tests. We show\nthat simulation can be used to obtain the critical values for such tests in the\nlow-dimensional setting, and demonstrate using both theoretical results and\nextensive numerical studies that some form of the parametric bootstrap can do\nthe same when the high-dimensional linear model is under consideration. We show\nthat RP tests can be used to test for significance of groups or individual\nvariables as special cases, and here they compare favourably with state of the\nart methods, but we also argue that they can be designed to test for as diverse\nmodel misspecifications as heteroscedasticity and nonlinearity.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 23:22:55 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 11:26:39 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 11:35:14 GMT"}, {"version": "v4", "created": "Sat, 8 Apr 2017 13:20:03 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Shah", "Rajen D.", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1511.03355", "submitter": "Stephen Marsland", "authors": "Stephen Marsland, Carole J Twining", "title": "Principal Autoparallel Analysis: Data Analysis in Weitzenb\\\"{o}ck Space", "comments": "9 pages, conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of data lying on a differentiable, locally\nEuclidean, manifold introduces a variety of challenges because the analogous\nmeasures to standard Euclidean statistics are local, that is only defined\nwithin a neighbourhood of each datapoint. This is because the curvature of the\nspace means that the connection of Riemannian geometry is path dependent. In\nthis paper we transfer the problem to Weitzenb\\\"{o}ck space, which has torsion,\nbut not curvature, meaning that parallel transport is path independent, and\nrather than considering geodesics, it is natural to consider autoparallels,\nwhich are `straight' in the sense that they follow the local basis vectors. We\ndemonstrate how to generate these autoparallels in a data-driven fashion, and\nshow that the resulting representation of the data is a useful space in which\nto perform further analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 01:52:24 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Marsland", "Stephen", ""], ["Twining", "Carole J", ""]]}, {"id": "1511.03376", "submitter": "Yue Wang", "authors": "Yue Wang and Jaewoo Lee and Daniel Kifer", "title": "Revisiting Differentially Private Hypothesis Tests for Categorical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider methods for performing hypothesis tests on data\nprotected by a statistical disclosure control technology known as differential\nprivacy. Previous approaches to differentially private hypothesis testing\neither perturbed the test statistic with random noise having large variance\n(and resulted in a significant loss of power) or added smaller amounts of noise\ndirectly to the data but failed to adjust the test in response to the added\nnoise (resulting in biased, unreliable $p$-values). In this paper, we develop a\nvariety of practical hypothesis tests that address these problems. Using a\ndifferent asymptotic regime that is more suited to hypothesis testing with\nprivacy, we show a modified equivalence between chi-squared tests and\nlikelihood ratio tests. We then develop differentially private likelihood ratio\nand chi-squared tests for a variety of applications on tabular data (i.e.,\nindependence, sample proportions, and goodness-of-fit tests). Experimental\nevaluations on small and large datasets using a wide variety of privacy\nsettings demonstrate the practicality and reliability of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 03:36:38 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 03:19:19 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 04:09:27 GMT"}, {"version": "v4", "created": "Sat, 18 Mar 2017 06:55:30 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Wang", "Yue", ""], ["Lee", "Jaewoo", ""], ["Kifer", "Daniel", ""]]}, {"id": "1511.03402", "submitter": "Zhanfeng Wang", "authors": "Zhanfeng Wang and Jianqing Shi and Youngjo Lee", "title": "Extended T-process Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression (GPR) model has been widely used to fit data when\nthe regression function is unknown and its nice properties have been well\nestablished. In this article, we introduce an extended t-process regression\n(eTPR) model, which gives a robust best linear unbiased predictor (BLUP). Owing\nto its succinct construction, it inherits many attractive properties from the\nGPR model, such as having closed forms of marginal and predictive distributions\nto give an explicit form for robust BLUP procedures, and easy to cope with\nlarge dimensional covariates with an efficient implementation by slightly\nmodifying existing BLUP procedures. Properties of the robust BLUP are studied.\nSimulation studies and real data applications show that the eTPR model gives a\nrobust fit in the presence of outliers in both input and output spaces and has\na good performance in prediction, compared with the GPR and locally weighted\nscatterplot smoothing (LOESS) methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 07:01:17 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 07:02:06 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Shi", "Jianqing", ""], ["Lee", "Youngjo", ""]]}, {"id": "1511.03463", "submitter": "Dimitris Kugiumtzis", "authors": "Elsa Siggiridou and Dimitris Kugiumtzis", "title": "Granger Causality in Multi-variate Time Series using a Time Ordered\n  Restricted Vector Autoregressive Model", "comments": "15 pages, 4 tables, 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2500893", "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality has been used for the investigation of the inter-dependence\nstructure of the underlying systems of multi-variate time series. In\nparticular, the direct causal effects are commonly estimated by the conditional\nGranger causality index (CGCI). In the presence of many observed variables and\nrelatively short time series, CGCI may fail because it is based on vector\nautoregressive models (VAR) involving a large number of coefficients to be\nestimated. In this work, the VAR is restricted by a scheme that modifies the\nrecently developed method of backward-in-time selection (BTS) of the lagged\nvariables and the CGCI is combined with BTS. Further, the proposed approach is\ncompared favorably to other restricted VAR representations, such as the\ntop-down strategy, the bottom-up strategy, and the least absolute shrinkage and\nselection operator (LASSO), in terms of sensitivity and specificity of CGCI.\nThis is shown by using simulations of linear and nonlinear, low and\nhigh-dimensional systems and different time series lengths. For nonlinear\nsystems, CGCI from the restricted VAR representations are compared with\nanalogous nonlinear causality indices. Further, CGCI in conjunction with BTS\nand other restricted VAR representations is applied to multi-channel scalp\nelectroencephalogram (EEG) recordings of epileptic patients containing\nepileptiform discharges. CGCI on the restricted VAR, and BTS in particular,\ncould track the changes in brain connectivity before, during and after\nepileptiform discharges, which was not possible using the full VAR\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 11:35:21 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Siggiridou", "Elsa", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1511.03688", "submitter": "David Degras", "authors": "Herv\\'e Cardot and David Degras", "title": "Online Principal Component Analysis in High Dimension: Which Algorithm\n  to Choose?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current context of data explosion, online techniques that do not\nrequire storing all data in memory are indispensable to routinely perform tasks\nlike principal component analysis (PCA). Recursive algorithms that update the\nPCA with each new observation have been studied in various fields of research\nand found wide applications in industrial monitoring, computer vision,\nastronomy, and latent semantic indexing, among others. This work provides\nguidance for selecting an online PCA algorithm in practice. We present the main\napproaches to online PCA, namely, perturbation techniques, incremental methods,\nand stochastic optimization, and compare their statistical accuracy,\ncomputation time, and memory requirements using artificial and real data.\nExtensions to missing data and to functional data are discussed. All studied\nalgorithms are available in the R package onlinePCA on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 21:25:26 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["Degras", "David", ""]]}, {"id": "1511.03722", "submitter": "Nan Jiang", "authors": "Nan Jiang and Lihong Li", "title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning", "comments": "14 pages; 4 figures; ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy value evaluation in reinforcement learning\n(RL), where one aims to estimate the value of a new policy based on data\ncollected by a different policy. This problem is often a critical step when\napplying RL in real-world problems. Despite its importance, existing general\nmethods either have uncontrolled bias or suffer high variance. In this work, we\nextend the doubly robust estimator for bandits to sequential decision-making\nproblems, which gets the best of both worlds: it is guaranteed to be unbiased\nand can have a much lower variance than the popular importance sampling\nestimators. We demonstrate the estimator's accuracy in several benchmark\nproblems, and illustrate its use as a subroutine in safe policy improvement. We\nalso provide theoretical results on the hardness of the problem, and show that\nour estimator can match the lower bound in certain scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 22:59:51 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 01:23:10 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 15:43:08 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Jiang", "Nan", ""], ["Li", "Lihong", ""]]}, {"id": "1511.03761", "submitter": "Norm Matloff PhD", "authors": "Norm Matloff", "title": "A New Framework for Random Effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A different general philosophy, to be called Full Randomness (FR), for the\nanalysis of random effects models is presented, involving a notion of reducing\nor preferably eliminating fixed effects, at least formally. For example, under\nFR applied to a repeated measures model, even the number of repetitions would\nbe modeled as random. It is argued that in many applications such quantities\nreally are random, and that recognizing this enables the construction of much\nricher, more probing analyses. Methodology for this approach will be developed\nhere, and suggestions will be made for the broader use of the approach. It is\nargued that even in settings in which some factors are fixed by the\nexperimental design, FR still \"gives the right answers.\" In addition,\ncomputational advantages to such methods will be shown.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 02:29:17 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 22:54:25 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Matloff", "Norm", ""]]}, {"id": "1511.03796", "submitter": "Yuancheng Zhu", "authors": "Yuancheng Zhu, Zhe Liu and Siqi Sun", "title": "Learning Nonparametric Forest Graphical Models with Prior Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for incorporating prior information into nonparametric\nestimation of graphical models. To avoid distributional assumptions, we\nrestrict the graph to be a forest and build on the work of forest density\nestimation (FDE). We reformulate the FDE approach from a Bayesian perspective,\nand introduce prior distributions on the graphs. As two concrete examples, we\napply this framework to estimating scale-free graphs and learning multiple\ngraphs with similar structures. The resulting algorithms are equivalent to\nfinding a maximum spanning tree of a weighted graph with a penalty term on the\nconnectivity pattern of the graph. We solve the optimization problem via a\nminorize-maximization procedure with Kruskal's algorithm. Simulations show that\nthe proposed methods outperform competing parametric methods, and are robust to\nthe true data distribution. They also lead to improvement in predictive power\nand interpretability in two real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 06:36:53 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 03:18:09 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Zhu", "Yuancheng", ""], ["Liu", "Zhe", ""], ["Sun", "Siqi", ""]]}, {"id": "1511.03864", "submitter": "Simon Wood", "authors": "Simon N. Wood, Natalya Pya, Benjamin S\\\"afken", "title": "Smoothing parameter and model selection for general smooth models", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2016.1180986", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a general framework for smoothing parameter estimation\nfor models with regular likelihoods constructed in terms of unknown smooth\nfunctions of covariates. Gaussian random effects and parametric terms may also\nbe present. By construction the method is numerically stable and convergent,\nand enables smoothing parameter uncertainty to be quantified. The latter\nenables us to fix a well known problem with AIC for such models. The smooth\nfunctions are represented by reduced rank spline like smoothers, with\nassociated quadratic penalties measuring function smoothness. Model estimation\nis by penalized likelihood maximization, where the smoothing parameters\ncontrolling the extent of penalization are estimated by Laplace approximate\nmarginal likelihood. The methods cover, for example, generalized additive\nmodels for non-exponential family responses (for example beta, ordered\ncategorical, scaled t distribution, negative binomial and Tweedie\ndistributions), generalized additive models for location scale and shape (for\nexample two stage zero inflation models, and Gaussian location-scale models),\nCox proportional hazards models and multivariate additive models. The framework\nreduces the implementation of new model classes to the coding of some standard\nderivatives of the log likelihood.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 11:37:42 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 07:12:42 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Wood", "Simon N.", ""], ["Pya", "Natalya", ""], ["S\u00e4fken", "Benjamin", ""]]}, {"id": "1511.03895", "submitter": "Pau Closas", "authors": "Pau Closas and Antoni Guillamon", "title": "Sequential estimation of intrinsic activity and synaptic input in single\n  neurons by particle filtering with optimal importance density", "comments": "Submitted for publication in the Special Issue on Advanced Signal\n  Processing in Brain Networks of the IEEE Journal on Selected Topics in Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of inferring the signals and parameters\nthat cause neural activity to occur. The ultimate challenge being to unveil\nbrain's connectivity, here we focus on a microscopic vision of the problem,\nwhere single neurons (potentially connected to a network of peers) are at the\ncore of our study. The sole observation available are noisy, sampled voltage\ntraces obtained from intracellular recordings. We design algorithms and\ninference methods using the tools provided by stochastic filtering, that allow\na probabilistic interpretation and treatment of the problem. Using particle\nfiltering we are able to reconstruct traces of voltages and estimate the time\ncourse of auxiliary variables. By extending the algorithm, through PMCMC\nmethodology, we are able to estimate hidden physiological parameters as well,\nlike intrinsic conductances or reversal potentials. Last, but not least, the\nmethod is applied to estimate synaptic conductances arriving at a target cell,\nthus reconstructing the synaptic excitatory/inhibitory input traces. Notably,\nthese estimations have a bound-achieving performance even in spiking regimes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 13:47:06 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Closas", "Pau", ""], ["Guillamon", "Antoni", ""]]}, {"id": "1511.03947", "submitter": "Chris Glynn", "authors": "Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard", "title": "Bayesian Analysis of Dynamic Linear Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic topic modeling, the proportional contribution of a topic to a\ndocument depends on the temporal dynamics of that topic's overall prevalence in\nthe corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by\nexplicitly modeling document level topic proportions with covariates and\ndynamic structure that includes polynomial trends and periodicity. A Markov\nChain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation\nis developed for posterior inference. Conditional independencies in the model\nand sampling are made explicit, and our MCMC algorithm is parallelized where\npossible to allow for inference in large corpora. To address computational\nbottlenecks associated with Polya-Gamma sampling, we appeal to the Central\nLimit Theorem to develop a Gaussian approximation to the Polya-Gamma random\nvariable. This approximation is fast and reliable for parameter values relevant\nin the text mining domain. Our model and inference algorithm are validated with\nmultiple simulation examples, and we consider the application of modeling\ntrends in PubMed abstracts. We demonstrate that sharing information across\ndocuments is critical for accurately estimating document-specific topic\nproportions. We also show that explicitly modeling polynomial and periodic\nbehavior improves our ability to predict topic prevalence at future time\npoints.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 16:26:13 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Glynn", "Chris", ""], ["Tokdar", "Surya T.", ""], ["Banks", "David L.", ""], ["Howard", "Brian", ""]]}, {"id": "1511.03968", "submitter": "Spyridon Hatjispyros", "authors": "Spyridon J. Hatjispyros, Stephen G. Walker", "title": "A High Accuracy Stochastic Estimation of a Nonlinear Deterministic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an approach to estimating a nonlinear deterministic model is\npresented. We introduce a stochastic model with extremely small variances so\nthat the deterministic and stochastic models are essentially indistinguishable\nfrom each other. This point is explained in the paper. The estimation is then\ncarried out using stochastic optimisation based on Markov chain Monte Carlo\n(MCMC) methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:01:37 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Hatjispyros", "Spyridon J.", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1511.03982", "submitter": "Pau Closas", "authors": "Adri\\`a Gusi-Amig\\'o, Pau Closas, and Luc Vandendorpe", "title": "Mean Square Error bounds for parameter estimation under model\n  misspecification", "comments": "Submitted for publication in the IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parameter estimation, assumptions about the model are typically considered\nwhich allow us to build optimal estimation methods under many statistical\nsenses. However, it is usually the case where such models are inaccurately\nknown or not capturing the complexity of the observed phenomenon. A natural\nquestion arises to whether we can find fundamental estimation bounds under\nmodel mismatches. This paper derives a general bound on the mean square error\n(MSE) following the Ziv-Zakai methodology for the widely used additive Gaussian\nmodel. The general result accounts for erroneous functionals, hyperparameters,\nand distributions differing from the Gaussian. The result is then\nparticularized to gain some insight into specific problems and some\nillustrative examples demonstrate the predictive capabilities of the bound.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:39:43 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 10:18:58 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Gusi-Amig\u00f3", "Adri\u00e0", ""], ["Closas", "Pau", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1511.04020", "submitter": "Alexander Aue", "authors": "Alexander Aue, Gregory Rice, Ozan S\\\"onmez", "title": "Detecting and dating structural breaks in functional data without\n  dimension reduction", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methodology is proposed to uncover structural breaks in functional data that\nis \"fully functional\" in the sense that it does not rely on dimension reduction\ntechniques. A thorough asymptotic theory is developed for a fully functional\nbreak detection procedure as well as for a break date estimator, assuming a\nfixed break size and a shrinking break size. The latter result is utilized to\nderive confidence intervals for the unknown break date. The main results\nhighlight that the fully functional procedures perform best under conditions\nwhen analogous fPCA based estimators are at their worst, namely when the\nfeature of interest is orthogonal to the leading principal components of the\ndata. The theoretical findings are confirmed by means of a Monte Carlo\nsimulation study in finite samples. An application to annual temperature curves\nillustrates the practical relevance of the proposed procedures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 19:21:49 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2017 21:41:32 GMT"}, {"version": "v3", "created": "Tue, 29 Aug 2017 21:15:54 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Aue", "Alexander", ""], ["Rice", "Gregory", ""], ["S\u00f6nmez", "Ozan", ""]]}, {"id": "1511.04112", "submitter": "Kasia B Taylor", "authors": "Omiros Papaspiliopoulos, Gareth O. Roberts, Kasia B. Taylor", "title": "Exact sampling of diffusions with a discontinuity in the drift", "comments": "13 pages", "journal-ref": "Adv. in Appl. Probab. 48 (2016) 249-259", "doi": "10.1017/apr.2016.54", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce exact methods for the simulation of sample paths of\none-dimensional diffusions with a discontinuity in the drift function. Our\nprocedures require the simulation of finite-dimensional candidate draws from\nprobability laws related to those of Brownian motion and its local time and are\nbased on the principle of retrospective rejection sampling. A simple\nillustration is provided.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 22:20:54 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Roberts", "Gareth O.", ""], ["Taylor", "Kasia B.", ""]]}, {"id": "1511.04128", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly", "title": "A Widely Linear Complex Autoregressive Process of Order One", "comments": "Link to published version:\n  http://ieeexplore.ieee.org/abstract/document/7539658/", "journal-ref": "IEEE Transactions on Signal Processing, 64(23), 6200-6210, 2016", "doi": "10.1109/TSP.2016.2599503", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple stochastic process for modeling improper or noncircular\ncomplex-valued signals. The process is a natural extension of a complex-valued\nautoregressive process, extended to include a widely linear autoregressive\nterm. This process can then capture elliptical, as opposed to circular,\nstochastic oscillations in a bivariate signal. The process is order one and is\nmore parsimonious than alternative stochastic modeling approaches in the\nliterature. We provide conditions for stationarity, and derive the form of the\ncovariance and relation sequence of this model. We describe how parameter\nestimation can be efficiently performed both in the time and frequency domain.\nWe demonstrate the practical utility of the process in capturing elliptical\noscillations that are naturally present in seismic signals.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 23:44:04 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 11:01:55 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 17:23:31 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""]]}, {"id": "1511.04485", "submitter": "St\\'ephane Guerrier", "authors": "Stephane Guerrier and Maria-Pia Victoria-Feser", "title": "A Prediction Divergence Criterion for Model Selection", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of model selection is inevitable in an increasingly large number\nof applications involving partial theoretical knowledge and vast amounts of\ninformation, like in medicine, biology or economics. The associated techniques\nare intended to determine which variables are \"important\" to \"explain a\nphenomenon under investigation. The terms \"important\" and \"explain\" can have\nvery different meanings according to the context and, in fact, model selection\ncan be applied to any situation where one tries to balance variability with\ncomplexity. In this paper, we introduce a new class of error measures and of\nmodel selection criteria, to which many well know selection criteria belong.\nMoreover, this class enables us to derive a novel criterion, based on a\ndivergence measure between the predictions produced by two nested models,\ncalled the Prediction Divergence Criterion (PDC). Our selection procedure is\ndeveloped for linear regression models, but has the potential to be extended to\nother models. We demonstrate that, under some regularity conditions, it is\nasymptotically loss efficient and can also be consistent. In the linear case,\nthe PDC is a counterpart to Mallow's Cp but with a lower asymptotic probability\nof overfitting. In a case study and by means of simulations, the PDC is shown\nto be particularly well suited in \"sparse\" settings with correlated covariates\nwhich we believe to be common in real applications.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 00:45:23 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Guerrier", "Stephane", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1511.04486", "submitter": "Jonas Mueller", "authors": "Jonas Mueller, Tommi Jaakkola, David Gifford", "title": "Modeling Persistent Trends in Distributions", "comments": "To appear in: Journal of the American Statistical Association", "journal-ref": "Journal of the American Statistical Association,\n  113(523):1296-1310, 2018", "doi": "10.1080/01621459.2017.1341412", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonparametric framework to model a short sequence of probability\ndistributions that vary both due to underlying effects of sequential\nprogression and confounding noise. To distinguish between these two types of\nvariation and estimate the sequential-progression effects, our approach\nleverages an assumption that these effects follow a persistent trend. This work\nis motivated by the recent rise of single-cell RNA-sequencing experiments over\na brief time course, which aim to identify genes relevant to the progression of\na particular biological process across diverse cell populations. While\nclassical statistical tools focus on scalar-response regression or\norder-agnostic differences between distributions, it is desirable in this\nsetting to consider both the full distributions as well as the structure\nimposed by their ordering. We introduce a new regression model for ordinal\ncovariates where responses are univariate distributions and the underlying\nrelationship reflects consistent changes in the distributions over increasing\nlevels of the covariate. This concept is formalized as a \"trend\" in\ndistributions, which we define as an evolution that is linear under the\nWasserstein metric. Implemented via a fast alternating projections algorithm,\nour method exhibits numerous strengths in simulations and analyses of\nsingle-cell gene expression data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 00:52:39 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 19:59:39 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Mueller", "Jonas", ""], ["Jaakkola", "Tommi", ""], ["Gifford", "David", ""]]}, {"id": "1511.04556", "submitter": "Franck Picard", "authors": "Madison Giacofc and Sophie Lambert-Lacroix and Franck Picard", "title": "Minimax wavelet estimation for multisample heteroscedastic\n  non-parametric regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the baseline signal from multisample noisy curves\nis investigated. We consider the functional mixed effects model, and we suppose\nthat the functional fixed effect belongs to the Besov class. This framework\nallows us to model curves that can exhibit strong irregularities, such as peaks\nor jumps for instance. The lower bound for the $L_2$ minimax risk is provided,\nas well as the upper bound of the minimax rate, that is derived by constructing\na wavelet estimator for the functional fixed effect. Our work constitutes the\nfirst theoretical functional results in multisample non parametric regression.\nOur approach is illustrated on realistic simulated datasets as well as on\nexperimental data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 12:48:26 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Giacofc", "Madison", ""], ["Lambert-Lacroix", "Sophie", ""], ["Picard", "Franck", ""]]}, {"id": "1511.04621", "submitter": "Adam Jaeger", "authors": "Adam Jaeger", "title": "Computation of Two and Three Dimensional Confidence Regions with the\n  Likelihood Ratio", "comments": null, "journal-ref": "The American Statistician, 70(4), 395-398, 2016", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic results pertaining to the distribution of the log likelihood\nratio allow for the creation of a confidence region, which is a general\nextension of the confidence interval. Two and three dimensional regions can be\ndisplayed visually in order to describe the plausible region of the parameters\nof interest simultaneously. While most advanced statistical textbooks on\ninference discuss these asymptotic confidence regions, there is no exploration\nof how to numerically compute these regions for graphical purposes. This\narticle demonstrates the application of a simple trigonometric identity to\ncompute two and three dimensional confidence regions, we transform the\nCartesian coordinates to create what we call the radial profile log likelihood.\nThe method is applicable to any distribution with a defined likelihood\nfunction, so it is not limited to specific data distributions or model\nparadigms. We describe the method along with the algorithm, follow with an\nexample of our method and end with an examination of computation time.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 21:28:37 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Jaeger", "Adam", ""]]}, {"id": "1511.04626", "submitter": "Jonathan Hill", "authors": "Jonathan B. Hill", "title": "A Smoothed P-Value Test When There is a Nuisance Parameter under the\n  Alternative", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new test when there is a nuisance parameter under the\nalternative hypothesis. The test exploits the p-value occupation time [PVOT],\nthe measure of the subset of a nuisance parameter on which a p-value test\nrejects the null hypothesis. Key contributions are: (i) An asymptotic critical\nvalue upper bound for our test is the significance level, making inference\neasy. Conversely, test statistic functionals need a bootstrap or simulation\nstep which can still lead to size and power distortions, and bootstrapped or\nsimulated critical values are not asymptotically valid under weak or\nnon-identification. (ii) We only require the test statistic to have a known or\nbootstrappable limit distribution, hence we do not require root(n)-Gaussian\nasymptotics, and weak or non-identification is allowed. Finally, (iii) a test\nbased on the sup-p-value may be conservative and in some cases have nearly\ntrivial power, while the PVOT naturally controls for this by smoothing over the\nnuisance parameter space. We give examples and related controlled experiments\nconcerning PVOT tests of: omitted nonlinearity with and without the possibility\nof weakly identified model parameters; GARCH effects. Across cases, the PVOT\ntest variously matches, dominates or strongly dominates standard tests based on\nthe supremum p-value, or supremum or average test statistic (with wild\nbootstrapped p-value)\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 21:46:45 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 11:18:24 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 17:18:20 GMT"}, {"version": "v4", "created": "Thu, 19 Nov 2020 23:48:55 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hill", "Jonathan B.", ""]]}, {"id": "1511.04656", "submitter": "Xiao Li", "authors": "Xiao Li and Jinzhu Jia and Yuan Yao", "title": "Mixed and missing data: a unified treatment with latent graphical models", "comments": "11 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn latent graphical models when data have mixed variables\nand missing values. This model could be used for further data analysis,\nincluding regression, classification, ranking etc. It also could be used for\nimputing missing values. We specify a latent Gaussian model for the data, where\nthe categorical variables are generated by discretizing an unobserved variable\nand the latent variables are multivariate Gaussian. The observed data consists\nof two parts: observed Gaussian variables and observed categorical variables,\nwhere the latter part is considered as partially missing Gaussian variables. We\nuse the Expectation-Maximization algorithm to fit the model. To prevent\noverfitting we use sparse inverse covariance estimation to obtain sparse\nestimate of the latent covariance matrix, equivalently, the graphical model.\nThe fitted model then could be used for problems including re- gression,\nclassification and ranking. Such an approach is applied to a medical data set\nwhere our method outperforms the state-of-the-art methods. Simulation studies\nand real data results suggest that our proposed model performs better than\nrandom forest in terms of prediction error when the model is correctly\nspecified, and is a better imputation method than hot deck imputation even if\nthe model is not correctly specified.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 04:01:44 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Li", "Xiao", ""], ["Jia", "Jinzhu", ""], ["Yao", "Yuan", ""]]}, {"id": "1511.04745", "submitter": "Claudio Fuentes", "authors": "Andrew J Womack and Claudio Fuentes and Daniel Taylor-Rodriguez", "title": "Model Space Priors for Objective Sparse Bayesian Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the construction of model space priors from an\nalternative point of view to the usual indicators for inclusion of covariates\nin a given model. Assumptions about indicator variables often lead to\nBeta-Binomial priors on the model space, which do not appropriately penalize\nfor model complexity when the parameters are fixed. This can be alleviated by\nchanging the parameters of the prior to depend on the number of covariates,\nthough justification for this is lacking from a first-principles point of view.\nWe propose viewing the model space as a partially ordered set. When the number\nof covariates increases, an isometry argument leads to the Poisson distribution\nas the unique, natural limiting prior over model dimension. This limiting prior\nis derived using two constructions that view an individual model as though it\nis a \"local\" null hypothesis and compares its prior probability to the\nprobability of the alternatives that nest it. We show that this prior induces a\nposterior that concentrates on a finite true model asymptotically.\nAdditionally, we provide a deterministic algorithm that takes advantage of the\nnature of the prior and explores good models in polynomial time.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 18:11:44 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Womack", "Andrew J", ""], ["Fuentes", "Claudio", ""], ["Taylor-Rodriguez", "Daniel", ""]]}, {"id": "1511.04803", "submitter": "Yuan-chin Ivan Chang", "authors": "Yuan-chin Ivan Chang", "title": "Performance and Interpretation of Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a common statistical task in many areas. In order to\nameliorate the performance of the existing methods, there are always some new\nclassification procedures proposed. These procedures, especially those raised\nin the machine learning and data-mining literature, are usually complicated,\nand therefore extra effort is required to understand them and the impacts of\nindividual variables in these procedures. However, in some applications, for\nexample, pharmaceutical and medical related research, future developments\nand/or research plans will rely on the interpretation of the classification\nrule, such as the role of individual variables in a diagnostic rule/model.\nHence, in these kinds of research, despite the optimal performance of the\ncomplicated models, the model with the balanced ease of interpretability and\nsatisfactory performance is preferred. The complication of a classification\nrule might diminish its advantage in performance and become an obstacle to be\nused in those applications. In this paper, we study how to improve the\nclassification performance, in terms of area under the receiver operating\ncharacteristic curve of a conventional logistic model, while retaining its ease\nof interpretation. The proposed method increases the sensitivity at the whole\nrange of specificity and hence is especially useful when the performance in the\nhigh-specificity range of a receiver operating characteristic curve is of\ninterest. Theoretical justification is presented, and numerical results using\nboth simulated data and two real data sets are reported.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 02:10:41 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 13:24:35 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1511.04993", "submitter": "Benjamin Eltzner", "authors": "Benjamin Eltzner, Stephan Huckemann, Kanti V. Mardia", "title": "Torus Principal Component Analysis with an Application to RNA Structures", "comments": "35 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several cutting edge applications needing PCA methods for data on\ntori and we propose a novel torus-PCA method with important properties that can\nbe generally applied. There are two existing general methods: tangent space PCA\nand geodesic PCA. However, unlike tangent space PCA, our torus-PCA honors the\ncyclic topology of the data space whereas, unlike geodesic PCA, our torus-PCA\nproduces a variety of non-winding, non-dense descriptors. This is achieved by\ndeforming tori into spheres and then using a variant of the recently developed\nprinciple nested spheres analysis. This PCA analysis involves a step of small\nsphere fitting and we provide an improved test to avoid overfitting. However,\ndeforming tori into spheres creates singularities. We introduce a data-adaptive\npre-clustering technique to keep the singularities away from the data. For the\nfrequently encountered case that the residual variance around the PCA main\ncomponent is small, we use a post-mode hunting technique for more fine-grained\nclustering. Thus in general, there are three successive interrelated key steps\nof torus-PCA in practice: pre-clustering, deformation, and post-mode hunting.\nWe illustrate our method with two recently studied RNA structure (tori) data\nsets: one is a small RNA data set which is established as the benchmark for PCA\nand we validate our method through this data. Another is a large RNA data set\n(containing the small RNA data set) for which we show that our method provides\ninterpretable principal components as well as giving further insight into its\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 15:42:15 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Eltzner", "Benjamin", ""], ["Huckemann", "Stephan", ""], ["Mardia", "Kanti V.", ""]]}, {"id": "1511.05303", "submitter": "Hans Colonius", "authors": "Hans Colonius", "title": "An invitation to coupling and copulas: with applications to multisensory\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an introduction to the stochastic concepts of\n\\emph{coupling} and \\emph{copula}. Coupling means the construction of a joint\ndistribution of two or more random variables that need not be defined on one\nand the same probability space, whereas a copula is a function that joins a\nmultivariate distribution to its one-dimensional margins. Their role in\nstochastic modeling is illustrated by examples from multisensory perception.\nPointers to more advanced and recent treatments are provided.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 08:01:14 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Colonius", "Hans", ""]]}, {"id": "1511.05309", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff, Daphna Weinshall", "title": "Optimized Linear Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in real-world datasets, especially in high dimensional data, some\nfeature values are missing. Since most data analysis and statistical methods do\nnot handle gracefully missing values, the first step in the analysis requires\nthe imputation of missing values. Indeed, there has been a long standing\ninterest in methods for the imputation of missing values as a pre-processing\nstep. One recent and effective approach, the IRMI stepwise regression\nimputation method, uses a linear regression model for each real-valued feature\non the basis of all other features in the dataset. However, the proposed\niterative formulation lacks convergence guarantee. Here we propose a closely\nrelated method, stated as a single optimization problem and a block\ncoordinate-descent solution which is guaranteed to converge to a local minimum.\nExperiments show results on both synthetic and benchmark datasets, which are\ncomparable to the results of the IRMI method whenever it converges. However,\nwhile in the set of experiments described here IRMI often does not converge,\nthe performance of our methods is shown to be markedly superior in comparison\nwith other methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 08:26:40 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 17:46:18 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 13:28:52 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Resheff", "Yehezkel S.", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1511.05350", "submitter": "Juan A. Cuesta-Albertos", "authors": "P. C. \\'Alvarez-Esteban, E. del Barrio, J.A. Cuesta-Albertos and C.\n  Matr\\'an", "title": "Wide Consensus for Parallelized Inference", "comments": "27 pages, 3 fogures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general theory to address a consensus-based combination of\nestimations in a parallelized or distributed estimation setting. Taking into\naccount the possibility of very discrepant estimations, instead of a full\nconsensus we consider a \"wide consensus\" procedure. The approach is based on\nthe consideration of trimmed barycenters in the Wasserstein space of\nprobability distributions on R^d with finite second order moments. We include\ngeneral existence and consistency results as well as characterizations of\nbarycenters of probabilities that belong to (non necessarily elliptical)\nlocation and scatter familes. On these families, the effective computation of\nbarycenters and distances can be addressed through a consistent iterative\nalgorithm. Since, once a shape has been chosen, these computations just depend\non the locations and scatters, the theory can be applied to cover with great\ngenerality a wide consensus approach for location and scatter estimation or for\nobtaining confidence regions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 11:29:37 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 15:48:52 GMT"}, {"version": "v3", "created": "Thu, 11 May 2017 13:24:38 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["\u00c1lvarez-Esteban", "P. C.", ""], ["del Barrio", "E.", ""], ["Cuesta-Albertos", "J. A.", ""], ["Matr\u00e1n", "C.", ""]]}, {"id": "1511.05397", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford and Peter M. Aronow and Li Zeng and Jianghong Li", "title": "Identification of homophily and preferential recruitment in\n  respondent-driven sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a link-tracing procedure for surveying\nhidden or hard-to-reach populations in which subjects recruit other subjects\nvia their social network. There is significant research interest in detecting\nclustering or dependence of epidemiological traits in networks, but researchers\ndisagree about whether data from RDS studies can reveal it. Two distinct\nmechanisms account for dependence in traits of recruiters and recruitees in an\nRDS study: homophily, the tendency for individuals to share social ties with\nothers exhibiting similar characteristics, and preferential recruitment, in\nwhich recruiters do not recruit uniformly at random from their available\nalters. The different effects of network homophily and preferential recruitment\nin RDS studies have been a source of confusion in methodological research on\nRDS, and in empirical studies of the social context of health risk in hidden\npopulations. In this paper, we give rigorous definitions of homophily and\npreferential recruitment and show that neither can be measured precisely in\ngeneral RDS studies. We derive nonparametric identification regions for\nhomophily and preferential recruitment and show that these parameters are not\npoint identified unless the network takes a degenerate form. The results\nindicate that claims of homophily or recruitment bias measured from empirical\nRDS studies may not be credible. We apply our identification results to a study\ninvolving both a network census and RDS on a population of injection drug users\nin Hartford, CT.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 13:41:02 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Aronow", "Peter M.", ""], ["Zeng", "Li", ""], ["Li", "Jianghong", ""]]}, {"id": "1511.05433", "submitter": "Sylvain Sardy", "authors": "Caroline Giacobino, Sylvain Sardy, Jairo Diaz-Rodriguez, Nick\n  Hengartner", "title": "Quantile universal threshold for model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient recovery of a low-dimensional structure from high-dimensional data\nhas been pursued in various settings including wavelet denoising, generalized\nlinear models and low-rank matrix estimation. By thresholding some parameters\nto zero, estimators such as lasso, elastic net and subset selection allow to\nperform not only parameter estimation but also variable selection, leading to\nsparsity. Yet one crucial step challenges all these estimators: the choice of\nthe threshold parameter~$\\lambda$. If too large, important features are\nmissing; if too small, incorrect features are included.\n  Within a unified framework, we propose a new selection of $\\lambda$ at the\ndetection edge under the null model. To that aim, we introduce the concept of a\nzero-thresholding function and a null-thresholding statistic, that we\nexplicitly derive for a large class of estimators. The new approach has the\ngreat advantage of transforming the selection of $\\lambda$ from an unknown\nscale to a probabilistic scale with the simple selection of a probability\nlevel. Numerical results show the effectiveness of our approach in terms of\nmodel selection and prediction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 15:18:28 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 09:05:56 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 08:08:27 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Giacobino", "Caroline", ""], ["Sardy", "Sylvain", ""], ["Diaz-Rodriguez", "Jairo", ""], ["Hengartner", "Nick", ""]]}, {"id": "1511.05491", "submitter": "Pamela Llop", "authors": "Liliana Forzani, Rodrigo Garc\\'ia Arancibia, Pamela Llop, Diego\n  Tomassi", "title": "Supervised dimension reduction for ordinal predictors", "comments": "33 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications involving ordinal predictors, common approaches to reduce\ndimensionality are either extensions of unsupervised techniques such as\nprincipal component analysis, or variable selection procedures that rely on\nmodeling the regression function. In this paper, a supervised dimension\nreduction method tailored to ordered categorical predictors is introduced. It\nuses a model-based dimension reduction approach, inspired by extending\nsufficient dimension reductions to the context of latent Gaussian variables.\nThe reduction is chosen without modeling the response as a function of the\npredictors and does not impose any distributional assumption on the response or\non the response given the predictors. A likelihood-based estimator of the\nreduction is derived and an iterative expectation-maximization type algorithm\nis proposed to alleviate the computational load and thus make the method more\npractical. A regularized estimator, which simultaneously achieves variable\nselection and dimension reduction, is also presented. Performance of the\nproposed method is evaluated through simulations and a real data example for\nsocioeconomic index construction, comparing favorably to widespread use\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 17:58:44 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 17:06:15 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 15:21:37 GMT"}, {"version": "v4", "created": "Thu, 12 Oct 2017 13:52:14 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Forzani", "Liliana", ""], ["Arancibia", "Rodrigo Garc\u00eda", ""], ["Llop", "Pamela", ""], ["Tomassi", "Diego", ""]]}, {"id": "1511.05611", "submitter": "Thomas E Bartlett", "authors": "Thomas E. Bartlett", "title": "Co-modularity and Detection of Co-communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the notion of co-modularity, to co-cluster observations\nof bipartite networks into co-communities. The task of co-clustering is to\ngroup together nodes of one type, whose interaction with nodes of another type\nare the most similar. The novel measure of co-modularity is introduced to\nassess the strength of co-communities, as well as to arrange the representation\nof nodes and clusters for visualisation, and to define an objective function\nfor optimisation. We illustrate the power of our proposed methodology on\nsimulated data, as well as with examples from genomics and consumer-product\nreviews.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 23:15:19 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 19:48:49 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bartlett", "Thomas E.", ""]]}, {"id": "1511.05629", "submitter": "Delia Voronca Miss", "authors": "Delia C. Voronca, Mulugeta Gebregziabher, Valerie L. Durkalski, Lei\n  Liu, Leonard E. Egede", "title": "Marginalized Two Part Models for Generalized Gamma Family of\n  Distributions", "comments": "4 tables, 2 figures, 39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive continuous outcomes with a point mass at zero are prevalent in\nbiomedical research. To model the point mass at zero and to provide\nmarginalized covariate effect estimates, marginalized two part models (MTP)\nhave been developed for outcomes with lognormal and log skew normal\ndistributions. In this paper, we propose MTP models for outcomes from a\ngeneralized gamma (GG) family of distributions. In the proposed MTP-GG model,\nthe conditional mean from a two-part model with a three-parameter GG\ndistribution is parameterized to provide regression coefficients that have\nmarginal interpretation. MTP-gamma and MTP-Weibull are developed as special\ncases of MTP-GG. We derive marginal covariate effect estimators from each model\nand through simulations assess their finite sample operating characteristics in\nterms of bias, standard errors, 95% coverage, and rate of convergence. We\nillustrate the models using data sets from The Medical Expenditure Survey\n(MEPS) and from a randomized trial of addictive disorders and we provide SAS\ncode for implementation. The simulation results show that when the response\ndistribution is unknown or mis-specified, which is usually the case in real\ndata sets, the MTP-GG is preferable over other models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 00:40:51 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Voronca", "Delia C.", ""], ["Gebregziabher", "Mulugeta", ""], ["Durkalski", "Valerie L.", ""], ["Liu", "Lei", ""], ["Egede", "Leonard E.", ""]]}, {"id": "1511.05748", "submitter": "Massimo Ventrucci", "authors": "Massimo Ventrucci and H{\\aa}vard Rue", "title": "Penalized complexity priors for degrees of freedom in Bayesian P-splines", "comments": null, "journal-ref": null, "doi": "10.1177/1471082X16659154", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian P-splines assume an intrinsic Gaussian Markov random field prior on\nthe spline coefficients, conditional on a precision hyper-parameter $\\tau$.\nPrior elicitation of $\\tau$ is difficult. To overcome this issue we aim to\nbuilding priors on an interpretable property of the model, indicating the\ncomplexity of the smooth function to be estimated. Following this idea, we\npropose Penalized Complexity (PC) priors for the number of effective degrees of\nfreedom. We present the general ideas behind the construction of these new PC\npriors, describe their properties and show how to implement them in P-splines\nfor Gaussian data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 12:00:51 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 09:56:26 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Ventrucci", "Massimo", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1511.05925", "submitter": "Bruno Santos", "authors": "Bruno Santos and Heleno Bolfarine", "title": "Bayesian quantile regression analysis for continuous data with a\n  discrete component at zero", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show a Bayesian quantile regression method to response\nvariables with mixed discrete-continuous distribution with a point mass at\nzero, where these observations are believed to be left censored or true zeros.\nWe combine the information provided by the quantile regression analysis to\npresent a more complete description of the probability of being censored given\nthat the observed value is equal to zero, while also studying the conditional\nquantiles of the continuous part. We build up an Markov Chain Monte Carlo\nmethod from related models in the literature to obtain samples from the\nposterior distribution. We demonstrate the suitability of the model to analyze\nthis censoring probability with a simulated example and two applications with\nreal data. The first is a well known dataset from the econometrics literature\nabout women labor in Britain and the second considers the statistical analysis\nof expenditures with durable goods, considering information from Brazil.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:11:44 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Santos", "Bruno", ""], ["Bolfarine", "Heleno", ""]]}, {"id": "1511.05969", "submitter": "Alan Heavens", "authors": "Elena Sellentin and Alan F. Heavens", "title": "Parameter inference with estimated covariance matrices", "comments": "Matches accepted MNRAS letter", "journal-ref": null, "doi": "10.1093/mnrasl/slv190", "report-no": null, "categories": "astro-ph.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When inferring parameters from a Gaussian-distributed data set by computing a\nlikelihood, a covariance matrix is needed that describes the data errors and\ntheir correlations. If the covariance matrix is not known a priori, it may be\nestimated and thereby becomes a random object with some intrinsic uncertainty\nitself. We show how to infer parameters in the presence of such an estimated\ncovariance matrix, by marginalising over the true covariance matrix,\nconditioned on its estimated value. This leads to a likelihood function that is\nno longer Gaussian, but rather an adapted version of a multivariate\nt-distribution, which has the same numerical complexity as the multivariate\nGaussian. As expected, marginalisation over the true covariance matrix improves\ninference when compared with Hartlap et al.'s method, which uses an unbiased\nestimate of the inverse covariance matrix but still assumes that the likelihood\nis Gaussian.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 21:00:17 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 13:25:26 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Sellentin", "Elena", ""], ["Heavens", "Alan F.", ""]]}, {"id": "1511.06198", "submitter": "Kai Zhang", "authors": "Kai Zhang", "title": "Spherical Cap Packing Asymptotics and Rank-Extreme Detection", "comments": "14 pages; 1 figure. Accepted Jan 31, 2017 by IEEE Transactions on\n  Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2017.2700202", "report-no": null, "categories": "math.ST cs.IT math.IT physics.data-an stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the spherical cap packing problem with a probabilistic approach.\nSuch probabilistic considerations result in an asymptotic sharp universal\nuniform bound on the maximal inner product between any set of unit vectors and\na stochastically independent uniformly distributed unit vector. When the set of\nunit vectors are themselves independently uniformly distributed, we further\ndevelop the extreme value distribution limit of the maximal inner product,\nwhich characterizes its uncertainty around the bound.\n  As applications of the above asymptotic results, we derive (1) an asymptotic\nsharp universal uniform bound on the maximal spurious correlation, as well as\nits uniform convergence in distribution when the explanatory variables are\nindependently Gaussian distributed; and (2) an asymptotic sharp universal bound\non the maximum norm of a low-rank elliptically distributed vector, as well as\nrelated limiting distributions. With these results, we develop a fast detection\nmethod for a low-rank structure in high-dimensional Gaussian data without using\nthe spectrum information.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 15:08:10 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 23:52:16 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Zhang", "Kai", ""]]}, {"id": "1511.06400", "submitter": "In\\'es del Puerto", "authors": "Miguel Gonzalez, Carmen Minuesa, and Ines del Puerto", "title": "Minimum disparity estimation in controlled branching processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum disparity estimation in controlled branching processes is dealt with\nby assuming that the offspring law belongs to a general parametric family.\nUnder some regularity conditions it is proved that the minimum disparity\nestimators proposed -based on the nonparametric maximum likelihood estimator of\nthe offspring law when the entire family tree is observed- are consistent and\nasymptotic normally distributed. Moreover, it is discussed the robustness of\nthe estimators proposed. Through a simulated example, focussing on the minimum\nHellinger and negative exponential disparity estimators, it is shown that both\nare robust against outliers, being the negative exponential one also robust\nagainst inliers.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:45:10 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Gonzalez", "Miguel", ""], ["Minuesa", "Carmen", ""], ["del Puerto", "Ines", ""]]}, {"id": "1511.06417", "submitter": "Behnaz Pirzamanbein", "authors": "Behnaz Pirzamanbein (1 and 2), Johan Lindstr\\\"om (1), Anneli Poska (3\n  and 4) and Marie-Jos\\'e Gaillard (5) ((1) Centre for Mathematical Sciences,\n  Lund University, Sweden, (2) Centre for Environmental and Climate Research,\n  Lund University, Sweden, (3) Department of Physical Geography and Ecosystems\n  Analysis, Lund University, Sweden, (4) Institute of Geology, Tallinn\n  University of Technology, Estonia, (5) Department of Biology and\n  Environmental Sciences, Linnaeus University, Sweden)", "title": "Modelling Spatial Compositional Data: Reconstructions of past land cover\n  and uncertainties", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2018.03.005", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct a hierarchical model for spatial compositional\ndata, which is used to reconstruct past land-cover compositions (in terms of\nconiferous forest, broadleaved forest, and unforested/open land) for five time\nperiods during the past $6\\,000$ years over Europe. The model consists of a\nGaussian Markov Random Field (GMRF) with Dirichlet observations. A block\nupdated Markov chain Monte Carlo (MCMC), including an adaptive Metropolis\nadjusted Langevin step, is used to estimate model parameters. The sparse\nprecision matrix in the GMRF provides computational advantages leading to a\nfast MCMC algorithm. Reconstructions are obtained by combining pollen-based\nestimates of vegetation cover at a limited number of locations with scenarios\nof past deforestation and output from a dynamic vegetation model. To evaluate\nuncertainties in the predictions a novel way of constructing joint confidence\nregions for the entire composition at each prediction location is proposed. The\nhierarchical model's ability to reconstruct past land cover is evaluated\nthrough cross validation for all time periods, and by comparing reconstructions\nfor the recent past to a present day European forest map. The evaluation\nresults are promising and the model is able to capture known structures in past\nland-cover compositions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:09:15 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 09:30:15 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Pirzamanbein", "Behnaz", "", "1 and 2"], ["Lindstr\u00f6m", "Johan", "", "3\n  and 4"], ["Poska", "Anneli", "", "3\n  and 4"], ["Gaillard", "Marie-Jos\u00e9", ""]]}, {"id": "1511.06665", "submitter": "Fabian Spanhel", "authors": "Fabian Spanhel and Malte S. Kurz", "title": "The partial copula: Properties and associated dependence measures", "comments": null, "journal-ref": "Statistics & Probability Letters 119 (2016) 76 - 83", "doi": "10.1016/j.spl.2016.07.014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial correlation coefficient is a commonly used measure to assess the\nconditional dependence between two random variables. We provide a thorough\nexplanation of the partial copula, which is a natural generalization of the\npartial correlation coefficient, and investigate several of its properties. In\naddition, properties of some associated partial dependence measures are\nexamined.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:16:08 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Spanhel", "Fabian", ""], ["Kurz", "Malte S.", ""]]}, {"id": "1511.06733", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "On an inferential model construction using generalized associations", "comments": "18 pages, 4 figures", "journal-ref": "Journal of Statistical Planning and Inference, 2018, volume 195,\n  pages 105--115", "doi": "10.1016/j.jspi.2016.11.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inferential model (IM) approach, like fiducial and its generalizations,\ndepends on a representation of the data-generating process. Here, a particular\nvariation on the IM construction is considered, one based on generalized\nassociations. The resulting generalized IM is more flexible than the basic IM\nin that it does not require a complete specification of the data-generating\nprocess and is provably valid under mild conditions. Computation and\nmarginalization strategies are discussed, and two applications of this\ngeneralized IM approach are presented.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 19:29:04 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 12:41:11 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1511.06750", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla, Nicholas G. Polson and James G. Scott", "title": "A deconvolution path for mixtures", "comments": null, "journal-ref": "Electronic Journal of Statistics Volume 12, Number 1 (2018),\n  1717-1751", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of estimators for deconvolution in mixture models based on\na simple two-step \"bin-and-smooth\" procedure applied to histogram counts. The\nmethod is both statistically and computationally efficient: by exploiting\nrecent advances in convex optimization, we are able to provide a full\ndeconvolution path that shows the estimate for the mixing distribution across a\nrange of plausible degrees of smoothness, at far less cost than a full Bayesian\nanalysis. This enables practitioners to conduct a sensitivity analysis with\nminimal effort. This is especially important for applied data analysis, given\nthe ill-posed nature of the deconvolution problem. Our results establish the\nfavorable theoretical properties of our estimator and show that it offers\nstate-of-the-art performance when compared to benchmark methods across a range\nof scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 20:35:49 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 04:53:38 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 01:59:38 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1511.06813", "submitter": "Robin Evans", "authors": "Robin J. Evans and Thomas S. Richardson", "title": "Smooth, identifiable supermodels of discrete DAG models with latent\n  variables", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a parameterization of the discrete nested Markov model, which is a\nsupermodel that approximates DAG models (Bayesian network models) with latent\nvariables. Such models are widely used in causal inference and machine\nlearning. We explicitly evaluate their dimension, show that they are curved\nexponential families of distributions, and fit them to data. The\nparameterization avoids the irregularities and unidentifiability of latent\nvariable models. The parameters used are all fully identifiable and\ncausally-interpretable quantities.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 01:47:34 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 22:37:10 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Evans", "Robin J.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1511.06821", "submitter": "Xin Lu Tan", "authors": "Xin Lu Tan, Andreas Buja, and Zongming Ma", "title": "Kernel Additive Principal Components", "comments": "54 pages including appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive principal components (APCs for short) are a nonlinear generalization\nof linear principal components. We focus on smallest APCs to describe additive\nnonlinear constraints that are approximately satisfied by the data. Thus APCs\nfit data with implicit equations that treat the variables symmetrically, as\nopposed to regression analyses which fit data with explicit equations that\ntreat the data asymmetrically by singling out a response variable. We propose a\nregularized data-analytic procedure for APC estimation using kernel methods. In\ncontrast to existing approaches to APCs that are based on regularization\nthrough subspace restriction, kernel methods achieve regularization through\nshrinkage and therefore grant distinctive flexibility in APC estimation by\nallowing the use of infinite-dimensional functions spaces for searching APC\ntransformation while retaining computational feasibility. To connect population\nAPCs and kernelized finite-sample APCs, we study kernelized population APCs and\ntheir associated eigenproblems, which eventually lead to the establishment of\nconsistency of the estimated APCs. Lastly, we discuss an iterative algorithm\nfor computing kernelized finite-sample APCs.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 03:12:04 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Tan", "Xin Lu", ""], ["Buja", "Andreas", ""], ["Ma", "Zongming", ""]]}, {"id": "1511.07178", "submitter": "Moritz  Berger", "authors": "Moritz Berger and Gerhard Tutz", "title": "Detection of Uniform and Non-Uniform Differential Item Functioning by\n  Item Focussed Trees", "comments": "32 pages, 13 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of differential item functioning by use of the logistic modelling\napproach has a long tradition. One big advantage of the approach is that it can\nbe used to investigate non-uniform DIF as well as uniform DIF. The classical\napproach allows to detect DIF by distinguishing between multiple groups. We\npropose an alternative method that is a combination of recursive partitioning\nmethods (or trees) and logistic regression methodology to detect uniform and\nnon-uniform DIF in a nonparametric way. The output of the method are trees that\nvisualize in a simple way the structure of DIF in an item showing which\nvariables are interacting in which way when generating DIF. In addition we\nconsider a logistic regression method in which DIF can by induced by a vector\nof covariates, which may include categorical but also continuous covariates.\nThe methods are investigated in simulation studies and illustrated by two\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 11:17:05 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Berger", "Moritz", ""], ["Tutz", "Gerhard", ""]]}, {"id": "1511.07238", "submitter": "Yingbo Li", "authors": "Yingbo Li, Robert Lund, and Anuradha Hewaarachchi", "title": "Multiple Changepoint Detection with Partial Information on Changepoint\n  Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new minimum description length procedure to detect\nmultiple changepoints in time series data when some times are a priori thought\nmore likely to be changepoints. This scenario arises with temperature time\nseries homogenization pursuits, our focus here. Our Bayesian procedure\nconstructs a natural prior distribution for the situation, and is shown to\nestimate the changepoint locations consistently, with an optimal convergence\nrate. Our methods substantially improve changepoint detection power when prior\ninformation is available. The methods are also tailored to bivariate data,\nallowing changes to occur in one or both component series.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 14:32:38 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 21:34:54 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 04:56:55 GMT"}, {"version": "v4", "created": "Mon, 13 May 2019 14:43:04 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Li", "Yingbo", ""], ["Lund", "Robert", ""], ["Hewaarachchi", "Anuradha", ""]]}, {"id": "1511.07293", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Xiaorui Li", "title": "Sparse Recovery via Partial Regularization: Models, Theory and\n  Algorithms", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of sparse recovery, it is known that most of existing\nregularizers such as $\\ell_1$ suffer from some bias incurred by some leading\nentries (in magnitude) of the associated vector. To neutralize this bias, we\npropose a class of models with partial regularizers for recovering a sparse\nsolution of a linear system. We show that every local minimizer of these models\nis sufficiently sparse or the magnitude of all its nonzero entries is above a\nuniform constant depending only on the data of the linear system. Moreover, for\na class of partial regularizers, any global minimizer of these models is a\nsparsest solution to the linear system. We also establish some sufficient\nconditions for local or global recovery of the sparsest solution to the linear\nsystem, among which one of the conditions is weaker than the best known\nrestricted isometry property (RIP) condition for sparse recovery by $\\ell_1$.\nIn addition, a first-order feasible augmented Lagrangian (FAL) method is\nproposed for solving these models, in which each subproblem is solved by a\nnonmonotone proximal gradient (NPG) method. Despite the complication of the\npartial regularizers, we show that each proximal subproblem in NPG can be\nsolved as a certain number of one-dimensional optimization problems, which\nusually have a closed-form solution. We also show that any accumulation point\nof the sequence generated by FAL is a first-order stationary point of the\nmodels. Numerical results on compressed sensing and sparse logistic regression\ndemonstrate that the proposed models substantially outperform the widely used\nones in the literature in terms of solution quality.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 16:08:24 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Lu", "Zhaosong", ""], ["Li", "Xiaorui", ""]]}, {"id": "1511.07464", "submitter": "Aleksandar Mijatovic", "authors": "Aleksandar Mijatovic and Jure Vogrinc", "title": "On the Poisson equation for Metropolis-Hastings chains", "comments": "Presentation streamlined, new short proof of Proposition 3.2 in the\n  reversible case with other arguments essentially unchanged, 25 pages, no\n  figures, to appear in Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines an approximation scheme for a solution of the Poisson\nequation of a geometrically ergodic Metropolis-Hastings chain $\\Phi$. The\napproximations give rise to a natural sequence of control variates for the\nergodic average $S_k(F)=(1/k)\\sum_{i=1}^{k} F(\\Phi_i)$, where $F$ is the force\nfunction in the Poisson equation. The main result of the paper shows that the\nsequence of the asymptotic variances (in the CLTs for the control-variate\nestimators) converges to zero and gives a rate of this convergence. Numerical\nexamples in the case of a double-well potential are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 21:16:15 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 08:04:19 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Mijatovic", "Aleksandar", ""], ["Vogrinc", "Jure", ""]]}, {"id": "1511.07600", "submitter": "Michail Tsagris", "authors": "Michail Tsagris", "title": "A novel, divergence based, regression for compositional data", "comments": "This is a preprint of the paper accepted for publication in the\n  Proceedings of the 28th Panhellenic Statistics Conference, 15-18/4/2015,\n  Athens, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compositional data, an observation is a vector with non-negative\ncomponents which sum to a constant, typically 1. Data of this type arise in\nmany areas, such as geology, archaeology, biology, economics and political\nscience amongst others. The goal of this paper is to propose a new, divergence\nbased, regression modelling technique for compositional data. To do so, a\nrecently proved metric which is a special case of the Jensen-Shannon divergence\nis employed. A strong advantage of this new regression technique is that zeros\nare naturally handled. An example with real data and simulation studies are\npresented and are both compared with the log-ratio based regression suggested\nby Aitchison in 1986.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 07:49:04 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Tsagris", "Michail", ""]]}, {"id": "1511.07601", "submitter": "Michail Tsagris", "authors": "Konstantinos C. Fragkos, Michail Tsagris and Christos C. Frangos", "title": "Exploring the Distribution for the Estimator of Rosenthal's 'Fail-Safe'\n  Number of Unpublished Studies in Meta-analysis", "comments": "This is a preperint of the paper accepted for publication in\n  Communications in Statistics: Theory and Methods. arXiv admin note: text\n  overlap with arXiv:1509.01365", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper discusses the statistical distribution for the estimator of\nRosenthal's 'Fail-Safe' number NR, which is an estimator of unpublished studies\nin meta-analysis. We calculate the probability distribution function of NR.\nThis is achieved based on the Central Limit Theorem and the proposition that\ncertain components of the estimator NR follow a half normal distribution,\nderived from the standard normal distribution. Our proposed distributions are\nsupported by simulations and investigation of convergence.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 08:00:43 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Fragkos", "Konstantinos C.", ""], ["Tsagris", "Michail", ""], ["Frangos", "Christos C.", ""]]}, {"id": "1511.07662", "submitter": "Samuel Orso", "authors": "St\\'ephane Guerrier (1), Nabil Mili (2), Roberto Molinari (2), Samuel\n  Orso (2), Marco Avella-Medina (2) and Yanyuan Ma (3) ( (1) University of\n  Illinois at Urbana-Champaign, (2) University of Geneva, (3) University of\n  South Carolina )", "title": "A Paradigmatic Regression Algorithm for Gene Selection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Gene selection has become a common task in most gene expression\nstudies. The objective of such research is often to identify the smallest\npossible set of genes that can still achieve good predictive performance. The\nproblem of assigning tumours to a known class is a particularly important\nexample that has received considerable attention in the last ten years. Many of\nthe classification methods proposed recently require some form of\ndimension-reduction of the problem. These methods provide a single model as an\noutput and, in most cases, rely on the likelihood function in order to achieve\nvariable selection.\n  Results: We propose a prediction-based objective function that can be\ntailored to the requirements of practitioners and can be used to assess and\ninterpret a given problem. The direct optimization of such a function can be\nvery difficult because the problem is potentially discontinuous and nonconvex.\nWe therefore propose a general procedure for variable selection that resembles\nimportance sampling to explore the feature space. Our proposal compares\nfavorably with competing alternatives when applied to two cancer data sets in\nthat smaller models are obtained for better or at least comparable\nclassification errors. Furthermore by providing a set of selected models\ninstead of a single one, we construct a network of possible models for a target\nprediction accuracy level.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 11:40:58 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Mili", "Nabil", ""], ["Molinari", "Roberto", ""], ["Orso", "Samuel", ""], ["Avella-Medina", "Marco", ""], ["Ma", "Yanyuan", ""]]}, {"id": "1511.07839", "submitter": "Stephen Reid", "authors": "Stephen Reid, Jonathan Taylor, Robert Tibshirani", "title": "A general framework for estimation and inference from clusters of\n  features", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied statistical problems often come with pre-specified groupings to\npredictors. It is natural to test for the presence of simultaneous group-wide\nsignal for groups in isolation, or for multiple groups together. Classical\ntests for the presence of such signals rely either on tests for the omission of\nthe entire block of variables (the classical F-test) or on the creation of an\nunsupervised prototype for the group (either a group centroid or first\nprincipal component) and subsequent t-tests on these prototypes.\n  In this paper, we propose test statistics that aim for power improvements\nover these classical approaches. In particular, we first create group\nprototypes, with reference to the response, hopefully improving on the\nunsupervised prototypes, and then testing with likelihood ratio statistics\nincorporating only these prototypes. We propose a (potentially) novel model,\ncalled the \"prototype model\", which naturally models the two-step\nprototype-then-test procedure. Furthermore, we introduce an inferential schema\ndetailing the unique considerations for different combinations of prototype\nformation and univariate/multivariate testing models. The prototype model also\nsuggests new applications to estimation and prediction.\n  Prototype formation often relies on variable selection, which invalidates\nclassical Gaussian test theory. We use recent advances in selective inference\nto account for selection in the prototyping step and retain test validity.\nSimulation experiments suggest that our testing procedure enjoys more power\nthan do classical approaches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 19:30:36 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Reid", "Stephen", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1511.08029", "submitter": "Olcay Arslan Ph.D.", "authors": "Olcay Arslan", "title": "Penalized MM Regression Estimation with $L_{\\gamma }$ Penalty: A Robust\n  Version of Bridge Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bridge regression estimator generalizes both ridge regression and LASSO\nestimators. Since it minimizes the sum of squared residuals with a $L_{\\gamma\n}$ penalty, this estimator is typically not robust against outliers in the\ndata. There have been attempts to define robust versions of the bridge\nregression method, but while these proposed methods produce bridge regression\nestimators robust to outliers and heavy-tailed errors, they are not robust\nagainst leverage points. We propose a robust bridge regression estimation\nmethod combining MM and bridge regression estimation methods. The MM bridge\nregression estimator obtained from the proposed method is robust against\noutliers and leverage points. Furthermore, for appropriate choices of the\npenalty function, the proposed method is able to perform variable selection and\nparameter estimation simultaneously. Consistency, asymptotic normality, and\nsparsity of the MM bridge regression estimator are achieved. We propose an\nalgorithm to compute the MM bridge regression estimate. A simulation study and\na real data example are provided to demonstrate the performance of the MM\nbridge regression estimator for finite sample cases.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 12:10:15 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Arslan", "Olcay", ""]]}, {"id": "1511.08074", "submitter": "Denis Agniel", "authors": "Denis Agniel, Katherine P. Liao, Tianxi Cai", "title": "Estimation and testing for multiple regulation of multivariate mixed\n  outcomes", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable interest has recently been focused on studying multiple\nphenotypes simultaneously in both epidemiological and genomic studies, either\nto capture the multidimensionality of complex disorders or to understand shared\netiology of related disorders. We seek to identify {\\em multiple regulators} or\npredictors that are associated with multiple outcomes when these outcomes may\nbe measured on very different scales or composed of a mixture of continuous,\nbinary, and not-fully-observed elements. We first propose an estimation\ntechnique to put all effects on similar scales, and we induce sparsity on the\nestimated effects. We provide standard asymptotic results for this estimator\nand show that resampling can be used to quantify uncertainty in finite samples.\nWe finally provide a multiple testing procedure which can be geared\nspecifically to the types of multiple regulators of interest, and we establish\nthat, under standard regularity conditions, the familywise error rate will\napproach 0 as sample size diverges. Simulation results indicate that our\napproach can improve over unregularized methods both in reducing bias in\nestimation and improving power for testing.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 14:41:34 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Agniel", "Denis", ""], ["Liao", "Katherine P.", ""], ["Cai", "Tianxi", ""]]}, {"id": "1511.08190", "submitter": "Almut Veraart", "authors": "Ragnhild C. Noven, Almut E. D. Veraart, Axel Gandy", "title": "A latent trawl process model for extreme values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new model for characterising temporal dependence in\nexceedances above a threshold. The model is based on the class of trawl\nprocesses, which are stationary, infinitely divisible stochastic processes. The\nmodel for extreme values is constructed by embedding a trawl process in a\nhierarchical framework, which ensures that the marginal distribution is\ngeneralised Pareto, as expected from classical extreme value theory. We also\nconsider a modified version of this model that works with a wider class of\ngeneralised Pareto distributions, and has the advantage of separating marginal\nand temporal dependence properties. The model is illustrated by applications to\nenvironmental time series, and it is shown that the model offers considerable\nflexibility in capturing the dependence structure of extreme value data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 20:22:09 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 20:29:41 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 10:02:45 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Noven", "Ragnhild C.", ""], ["Veraart", "Almut E. D.", ""], ["Gandy", "Axel", ""]]}, {"id": "1511.08272", "submitter": "Suyan Tian", "authors": "Suyan Tian, Chi Wang, Howard H. Chang", "title": "Feature selection for longitudinal microarray data by adapting a pathway\n  analysis method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Feature selection and gene set analysis are of increasing\ninterest in bioinformatics. While these two approaches have been developed for\ndifferent purposes, we describe how some gene set analysis methods can be used\nto conduct feature selection. Here we adapt the gene set analysis method,\nsignificance analysis of microarray gene set reduction (SAMGSR), for feature\nselection, and propose two extensions-simple SAMGSR and two-level SAMGSR to\nidentify relevant features for longitudinal microarray data.\n  Results and Discussion: When applied to a real-world application, both simple\nand two-level SAMGSR work comparably well. Using simulated data, we further\ndemonstrate that both SAMGSR extensions have the ability to identify the true\nrelevant genes. If the relevant genes are not highly correlated with the\nirrelevant ones, the final models given by the two SAMGSR extensions are\nparsimonious as well.\n  Conclusions: By adapting SAMGSR for feature selection and applying the\nproposed algorithms on a longitudinal gene expression dataset, we demonstrate\nthat a gene set analysis method can be used for the purpose of feature\nselection. We believe this work paves the way for more research to bridge\nfeature selection and gene set analysis with the development of novel\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 02:31:33 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Tian", "Suyan", ""], ["Wang", "Chi", ""], ["Chang", "Howard H.", ""]]}, {"id": "1511.08404", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz, Elizabeth Colantuoni, Daniel F. Hanley, and Michael\n  Rosenblum", "title": "Improved Precision in the Analysis of Randomized Trials with Survival\n  Outcomes, without Assuming Proportional Hazards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new estimator of the restricted mean survival time in randomized\ntrials where there is right censoring that may depend on treatment and baseline\nvariables. The proposed estimator leverages prognostic baseline variables to\nobtain equal or better asymptotic precision compared to traditional estimators.\nUnder regularity conditions and random censoring within strata of treatment and\nbaseline variables, the proposed estimator has the following features: (i) it\nis interpretable under violations of the proportional hazards assumption; (ii)\nit is consistent and at least as precise as the Kaplan-Meier estimator under\nindependent censoring; (iii) it remains consistent under violations of\nindependent censoring (unlike the Kaplan-Meier estimator) when either the\ncensoring or survival distributions are estimated consistently; and (iv) it\nachieves the nonparametric efficiency bound when both of these distributions\nare consistently estimated. We illustrate the performance of our method using\nsimulations based on resampling data from a completed, phase 3 randomized\nclinical trial of a new surgical treatment for stroke; the proposed estimator\nachieves a 12% gain in relative efficiency compared to the Kaplan-Meier\nestimator. The proposed estimator has potential advantages over existing\napproaches for randomized trials with time-to-event outcomes, since existing\nmethods either rely on model assumptions that are untenable in many\napplications, or lack some of the efficiency and consistency properties\n(i)-(iv). We focus on estimation of the restricted mean survival time, but our\nmethods may be adapted to estimate any treatment effect measure defined as a\nsmooth contrast between the survival curves for each study arm. We provide R\ncode to implement the estimator.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 14:49:00 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 00:19:04 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Colantuoni", "Elizabeth", ""], ["Hanley", "Daniel F.", ""], ["Rosenblum", "Michael", ""]]}, {"id": "1511.08501", "submitter": "Ashkan Ertefaie", "authors": "Ashkan Ertefaie, Masoud Asgharian and David Stephens", "title": "Variable Selection in Causal Inference using a Simultaneous Penalization\n  Method", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.1283", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the causal adjustment setting, variable selection techniques based on one\nof either the outcome or treatment allocation model can result in the omission\nof confounders, which leads to bias, or the inclusion of spurious variables,\nwhich leads to variance inflation, in the propensity score. We propose a\nvariable selection method based on a penalized objective function which\nconsiders the outcome and treatment assignment models simultaneously. The\nproposed method facilitates confounder selection in high-dimensional settings.\nWe show that under regularity conditions our method attains the oracle\nproperty. The selected variables are used to form a doubly robust regression\nestimator of the treatment effect. We show that under some conditions our\nmethod attains the oracle property. Simulation results are presented and\neconomic growth data are analyzed. Specifically, we study the effect of life\nexpectancy as a measure of population health on the average growth rate of\ngross domestic product per capita.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 20:58:04 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Ertefaie", "Ashkan", ""], ["Asgharian", "Masoud", ""], ["Stephens", "David", ""]]}, {"id": "1511.08575", "submitter": "Samrat Mukhopadhyay", "authors": "Samrat Mukhopadhyay, Siddhartha Satpathi, and Mrityunjoy Chakraborty", "title": "A Modified Multiple OLS (m$^2$OLS) Algorithm for Signal Recovery in\n  Compressive Sensing", "comments": "15 pages, 7 figures, journal, added new material, changed few\n  figures, changed title, some minor changes in writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal least square (OLS) is an important sparse signal recovery\nalgorithm for compressive sensing, which enjoys superior probability of success\nover other well-known recovery algorithms under conditions of correlated\nmeasurement matrices. Multiple OLS (mOLS) is a recently proposed improved\nversion of OLS which selects multiple candidates per iteration by generalizing\nthe greedy selection principle used in OLS and enjoys faster convergence than\nOLS. In this paper, we present a refined version of the mOLS algorithm where at\neach step of the iteration, we first preselect a submatrix of the measurement\nmatrix suitably and then apply the mOLS computations to the chosen submatrix.\nSince mOLS now works only on a submatrix and not on the overall matrix,\ncomputations reduce drastically. Convergence of the algorithm, however,\nrequires ensuring passage of true candidates through the two stages of\npreselection and mOLS based selection successively. This paper presents\nconvergence conditions for both noisy and noise free signal models. The\nproposed algorithm enjoys faster convergence properties similar to mOLS, at a\nmuch reduced computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 07:39:05 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 13:15:14 GMT"}, {"version": "v3", "created": "Thu, 4 Jan 2018 14:16:00 GMT"}, {"version": "v4", "created": "Wed, 1 Aug 2018 07:46:02 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Mukhopadhyay", "Samrat", ""], ["Satpathi", "Siddhartha", ""], ["Chakraborty", "Mrityunjoy", ""]]}, {"id": "1511.08775", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck and Eric-Jan Wagenmakers", "title": "Adjusted Priors for Bayes Factors Involving Reparameterized Order\n  Constraints", "comments": null, "journal-ref": "Journal of Mathematical Psychology 73 (2016) 110-116", "doi": "10.1016/j.jmp.2016.05.004", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many psychological theories that are instantiated as statistical models imply\norder constraints on the model parameters. To fit and test such restrictions,\norder constraints of the form $\\theta_i \\leq \\theta_j$ can be reparameterized\nwith auxiliary parameters $\\eta\\in [0,1]$ to replace the original parameters by\n$\\theta_i = \\eta\\cdot\\theta_j$. This approach is especially common in\nmultinomial processing tree (MPT) modeling because the reparameterized, less\ncomplex model also belongs to the MPT class. Here, we discuss the importance of\nadjusting the prior distributions for the auxiliary parameters of a\nreparameterized model. This adjustment is important for computing the Bayes\nfactor, a model selection criterion that measures the evidence in favor of an\norder constraint by trading off model fit and complexity. We show that uniform\npriors for the auxiliary parameters result in a Bayes factor that differs from\nthe one that is obtained using a multivariate uniform prior on the\norder-constrained original parameters. As a remedy, we derive the adjusted\npriors for the auxiliary parameters of the reparameterized model. The practical\nrelevance of the problem is underscored with a concrete example using the\nmulti-trial pair-clustering model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 19:51:00 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 15:24:48 GMT"}, {"version": "v3", "created": "Sat, 7 May 2016 14:04:03 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Heck", "Daniel W.", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1511.08866", "submitter": "Joshua Loftus", "authors": "Joshua R. Loftus", "title": "Selective inference after cross-validation", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for performing inference on models chosen by\ncross-validation. When the test error being minimized in cross-validation is a\nresidual sum of squares it can be written as a quadratic form. This allows us\nto apply the inference framework in Loftus et al. (2015) for models determined\nby quadratic constraints to the model that minimizes CV test error. Our only\nrequirement on the model training pro- cedure is that its selection events are\nregions satisfying linear or quadratic constraints. This includes both Lasso\nand forward stepwise, which serve as our main examples throughout. We do not\nrequire knowledge of the error variance $\\sigma^2$. The procedures described\nhere are computationally intensive methods of selecting models adaptively and\nperforming inference for the selected model. Implementations are available in\nan R package.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 04:34:50 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Loftus", "Joshua R.", ""]]}, {"id": "1511.09149", "submitter": "Steven Thompson", "authors": "Steven K. Thompson", "title": "Fast Moving Sampling Designs in Temporal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a study related to this one I set up a temporal network simulation\nenvironment for evaluating network intervention strategies. A network\nintervention strategy consists of a sampling design to select nodes in the\nnetwork. An intervention is applied to nodes in the sample for the purpose of\nchanging the wider network in some desired way. The network intervention\nstrategies can represent natural agents such as viruses that spread in the\nnetwork, programs to prevent or reduce the virus spread, and the agency of\nindividual nodes, such as people, in forming and dissolving the links that\ncreate, maintain or change the network. The present paper examines idealized\nversions of the sampling designs used to that study. The purpose is to better\nunderstand the natural and human network designs in real situations and to\nprovide a simple inference of design-based properties that in turn measure\nproperties of the time-changing network. The designs use link tracing and\nsometimes other probabilistic procedures to add units to the sample and have an\nongoing attrition process by which units are removed from the sample.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 04:03:03 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Thompson", "Steven K.", ""]]}, {"id": "1511.09188", "submitter": "Huili Yuan", "authors": "Huili Yuan, Ruibin Xi, Chong Chen and Minghua Deng", "title": "Differential Network Analysis via the Lasso Penalized D-Trace Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biological networks often change under different environmental and genetic\nconditions. Understanding how these networks change becomes an important\nproblem in biological studies. In this paper, we model the network change as\nthe difference of two precision matrices and propose a novel loss function for\nestimating the precision matrix difference. Under a new irrepresentability\ncondition, we show that the new loss function with the lasso penalty can give\nconsistent estimates in high-dimensional setting for sub-Gaussian and\npolynomial-tailed distributions. An efficient algorithm is developed based on\nthe alternating direction method to solve the optimization problem. Simulation\nstudies and a real data analysis about colorectal cancer show that the proposed\nmethod outperforms other available methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 07:53:57 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 15:37:58 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yuan", "Huili", ""], ["Xi", "Ruibin", ""], ["Chen", "Chong", ""], ["Deng", "Minghua", ""]]}, {"id": "1511.09354", "submitter": "Oliver Ch\\'en", "authors": "Oliver Y. Ch\\'en, Ciprian M. Crainiceanu, Elizabeth L. Ogburn, Brian\n  S. Caffo, Tor D. Wager, Martin A. Lindquist", "title": "High-dimensional Multivariate Mediation: with Application to\n  Neuroimaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis has become an important tool in the behavioral sciences\nfor investigating the role of intermediate variables that lie in the path\nbetween a randomized treatment and an outcome variable. The influence of the\nintermediate variable on the outcome is often explored using structural\nequation models (SEMs), with model coefficients interpreted as possible\neffects. While there has been significant research on the topic in recent\nyears, little work has been done on mediation analysis when the intermediate\nvariable (mediator) is a high-dimensional vector. In this work we present a new\nmethod for exploratory mediation analysis in this setting called the directions\nof mediation (DMs). The first DM is defined as the linear combination of the\nelements of a high-dimensional vector of potential mediators that maximizes the\nlikelihood of the SEM. The subsequent DMs are defined as linear combinations of\nthe elements of the high-dimensional vector that are orthonormal to the\nprevious DMs and maximize the likelihood of the SEM. We provide an estimation\nalgorithm and establish the asymptotic properties of the obtained estimators.\nThis method is well suited for cases when many potential mediators are\nmeasured. Examples of high-dimensional potential mediators are brain images\ncomposed of hundreds of thousands of voxels, genetic variation measured at\nmillions of SNPs, or vectors of thousands of variables in large-scale\nepidemiological studies. We demonstrate the method using a functional magnetic\nresonance imaging (fMRI) study of thermal pain where we are interested in\ndetermining which brain locations mediate the relationship between the\napplication of a thermal stimulus and self-reported pain.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 15:43:34 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 02:29:58 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Ch\u00e9n", "Oliver Y.", ""], ["Crainiceanu", "Ciprian M.", ""], ["Ogburn", "Elizabeth L.", ""], ["Caffo", "Brian S.", ""], ["Wager", "Tor D.", ""], ["Lindquist", "Martin A.", ""]]}]