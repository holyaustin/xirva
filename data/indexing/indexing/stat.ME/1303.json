[{"id": "1303.0188", "submitter": "Abdollah Jalilian", "authors": "Yongtao Guan, Abdollah Jalilian, Rasmus Waagepetersen", "title": "Quasi-likelihood for Spatial Point Processes", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology), 77(3), 677-697, 2015", "doi": "10.1111/rssb.12083", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting regression models for intensity functions of spatial point processes\nis of great interest in ecological and epidemiological studies of association\nbetween spatially referenced events and geographical or environmental\ncovariates. When Cox or cluster process models are used to accommodate\nclustering not accounted for by the available covariates, likelihood based\ninference becomes computationally cumbersome due to the complicated nature of\nthe likelihood function and the associated score function. It is therefore of\ninterest to consider alternative more easily computable estimating functions.\nWe derive the optimal estimating function in a class of first-order estimating\nfunctions. The optimal estimating function depends on the solution of a certain\nFredholm integral equation which in practice is solved numerically. The\napproximate solution is equivalent to a quasi-likelihood for binary spatial\ndata and we therefore use the term quasi-likelihood for our optimal estimating\nfunction approach. We demonstrate in a simulation study and a data example that\nour quasi-likelihood method for spatial point processes is both statistically\nand computationally efficient.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 15:14:39 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Guan", "Yongtao", ""], ["Jalilian", "Abdollah", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "1303.0383", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Daniel W. Apley", "title": "Local Gaussian process approximation for large computer experiments", "comments": "29 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new approach to approximate emulation of large computer\nexperiments. By focusing expressly on desirable properties of the predictive\nequations, we derive a family of local sequential design schemes that\ndynamically define the support of a Gaussian process predictor based on a local\nsubset of the data. We further derive expressions for fast sequential updating\nof all needed quantities as the local designs are built-up iteratively. Then we\nshow how independent application of our local design strategy across the\nelements of a vast predictive grid facilitates a trivially parallel\nimplementation. The end result is a global predictor able to take advantage of\nmodern multicore architectures, while at the same time allowing for a\nnonstationary modeling feature as a bonus. We demonstrate our method on two\nexamples utilizing designs sized in the thousands, and tens of thousands of\ndata points. Comparisons are made to the method of compactly supported\ncovariances.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 12:56:45 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 22:04:18 GMT"}, {"version": "v3", "created": "Tue, 8 Apr 2014 13:28:46 GMT"}, {"version": "v4", "created": "Fri, 10 Oct 2014 13:16:56 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Apley", "Daniel W.", ""]]}, {"id": "1303.0426", "submitter": "Stephanie S Zhang", "authors": "Stephanie S. Zhang, Lawrence T. DeCarlo, and Zhiliang Ying", "title": "Non-identifiability, equivalence classes, and attribute-specific\n  classification in Q-matrix based Cognitive Diagnosis Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interest in recent years in Q-matrix based cognitive\ndiagnosis models. Parameter estimation and respondent classification under\nthese models may suffer due to identifiability issues. Non-identifiability can\nbe described by a partition separating attribute profiles into groups of those\nwith identical likelihoods. Marginal identifiability concerns the\nidentifiability of individual attributes. Maximum likelihood estimation of the\nproportion of respondents within each equivalence class is consistent, making\npossible a new measure of assessment quality reporting the proportion of\nrespondents for whom each individual attribute is marginally identifiable.\nArising from this is a new posterior-based classification method adjusting for\nnon-identifiability.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 20:36:16 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Zhang", "Stephanie S.", ""], ["DeCarlo", "Lawrence T.", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1303.0439", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos and Stephen G. Walker", "title": "On Bayesian Nonparametric Continuous Time Series Models", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a note on the use of Bayesian nonparametric mixture models for\ncontinuous time series. We identify a key requirement for such models, and then\nestablish that there is a single type of model which meets this requirement. As\nit turns out, the model is well known in multiple change-point problems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 00:16:44 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Karabatsos", "George", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1303.0449", "submitter": "Anjishnu  Banerjee", "authors": "Anjishnu Banerjee, Jared Murray, David B. Dunson", "title": "Bayesian learning of joint distributions of objects", "comments": "Appearing in Proceedings of the 16th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2013, Scottsdale, AZ, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There is increasing interest in broad application areas in defining flexible\njoint models for data having a variety of measurement scales, while also\nallowing data of complex types, such as functions, images and documents. We\nconsider a general framework for nonparametric Bayes joint modeling through\nmixture models that incorporate dependence across data types through a joint\nmixing measure. The mixing measure is assigned a novel infinite tensor\nfactorization (ITF) prior that allows flexible dependence in cluster allocation\nacross data types. The ITF prior is formulated as a tensor product of\nstick-breaking processes. Focusing on a convenient special case corresponding\nto a Parafac factorization, we provide basic theory justifying the flexibility\nof the proposed prior and resulting asymptotic properties. Focusing on ITF\nmixtures of product kernels, we develop a new Gibbs sampling algorithm for\nroutine implementation relying on slice sampling. The methods are compared with\nalternative joint mixture models based on Dirichlet processes and related\napproaches through simulations and real data applications.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 01:55:10 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Banerjee", "Anjishnu", ""], ["Murray", "Jared", ""], ["Dunson", "David B.", ""]]}, {"id": "1303.0670", "submitter": "Fabian Scheipl", "authors": "Fabian Scheipl, Thomas Kneib, Ludwig Fahrmeir", "title": "Penalized Likelihood and Bayesian Function Selection in Regression\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Challenging research in various fields has driven a wide range of\nmethodological advances in variable selection for regression models with\nhigh-dimensional predictors. In comparison, selection of nonlinear functions in\nmodels with additive predictors has been considered only more recently. Several\ncompeting suggestions have been developed at about the same time and often do\nnot refer to each other. This article provides a state-of-the-art review on\nfunction selection, focusing on penalized likelihood and Bayesian concepts,\nrelating various approaches to each other in a unified framework. In an\nempirical comparison, also including boosting, we evaluate several methods\nthrough applications to simulated and real data, thereby providing some\nguidance on their performance in practice.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 11:11:48 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Scheipl", "Fabian", ""], ["Kneib", "Thomas", ""], ["Fahrmeir", "Ludwig", ""]]}, {"id": "1303.1219", "submitter": "Ian Fellows", "authors": "Ian E. Fellows and Mark S. Handcock", "title": "Analysis of Partially Observed Networks via Exponential-family Random\n  Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential-family random network (ERN) models specify a joint representation\nof both the dyads of a network and nodal characteristics. This class of models\nallow the nodal characteristics to be modelled as stochastic processes,\nexpanding the range and realism of exponential-family approaches to network\nmodelling. In this paper we develop a theory of inference for ERN models when\nonly part of the network is observed, as well as specific methodology for\nmissing data, including non-ignorable mechanisms for network-based sampling\ndesigns and for latent class models. In particular, we consider data collected\nvia contact tracing, of considerable importance to infectious disease\nepidemiology and public health.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 23:15:34 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Fellows", "Ian E.", ""], ["Handcock", "Mark S.", ""]]}, {"id": "1303.1282", "submitter": "Cinzia Viroli", "authors": "Christian Hennig and Cinzia Viroli", "title": "Quantile-based classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile classifiers for potentially high-dimensional data are defined by\nclassifying an observation according to a sum of appropriately weighted\ncomponent-wise distances of the components of the observation to the\nwithin-class quantiles. An optimal percentage for the quantiles can be chosen\nby minimizing the misclassification error in the training sample.\n  It is shown that this is consistent, for $n \\to \\infty$, for the\nclassification rule with asymptotically optimal quantile, and that, under some\nassumptions, for $p\\to\\infty$ the probability of correct classification\nconverges to one. The role of skewness of the involved variables is discussed,\nwhich leads to an improved classifier.\n  The optimal quantile classifier performs very well in a comprehensive\nsimulation study and a real data set from chemistry (classification of\nbioaerosols) compared to nine other classifiers, including the support vector\nmachine and the recently proposed median-based classifier (Hall et al., 2009),\nwhich inspired the quantile classifier.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 09:27:06 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 11:56:45 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Hennig", "Christian", ""], ["Viroli", "Cinzia", ""]]}, {"id": "1303.1288", "submitter": "M{\\aa}ns Thulin", "authors": "M{\\aa}ns Thulin", "title": "The cost of using exact confidence intervals for a binomial proportion", "comments": null, "journal-ref": "Electronic Journal of Statistics, 8, 817-840 (2014)", "doi": "10.1214/14-EJS909", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When computing a confidence interval for a binomial proportion p one must\nchoose between using an exact interval, which has a coverage probability of at\nleast 1-{\\alpha} for all values of p, and a shorter approximate interval, which\nmay have lower coverage for some p but that on average has coverage equal to\n1-\\alpha. We investigate the cost of using the exact one and two-sided\nClopper--Pearson confidence intervals rather than shorter approximate\nintervals, first in terms of increased expected length and then in terms of the\nincrease in sample size required to obtain a desired expected length. Using\nasymptotic expansions, we also give a closed-form formula for determining the\nsample size for the exact Clopper--Pearson methods. For two-sided intervals,\nour investigation reveals an interesting connection between the frequentist\nClopper--Pearson interval and Bayesian intervals based on noninformative\npriors.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 10:00:25 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Thulin", "M\u00e5ns", ""]]}, {"id": "1303.1382", "submitter": "Won Chang", "authors": "Won Chang, Murali Haran, Roman Olson, Klaus Keller", "title": "Fast dimension-reduced climate model calibration and the effect of data\n  aggregation", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS733 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 649-673", "doi": "10.1214/14-AOAS733", "report-no": "IMS-AOAS-AOAS733", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How will the climate system respond to anthropogenic forcings? One approach\nto this question relies on climate model projections. Current climate\nprojections are considerably uncertain. Characterizing and, if possible,\nreducing this uncertainty is an area of ongoing research. We consider the\nproblem of making projections of the North Atlantic meridional overturning\ncirculation (AMOC). Uncertainties about climate model parameters play a key\nrole in uncertainties in AMOC projections. When the observational data and the\nclimate model output are high-dimensional spatial data sets, the data are\ntypically aggregated due to computational constraints. The effects of\naggregation are unclear because statistically rigorous approaches for model\nparameter inference have been infeasible for high-resolution data. Here we\ndevelop a flexible and computationally efficient approach using principal\ncomponents and basis expansions to study the effect of spatial data aggregation\non parametric and projection uncertainties. Our Bayesian reduced-dimensional\ncalibration approach allows us to study the effect of complicated error\nstructures and data-model discrepancies on our ability to learn about climate\nmodel parameters from high-dimensional data. Considering high-dimensional\nspatial observations reduces the effect of deep uncertainty associated with\nprior specifications for the data-model discrepancy. Also, using the\nunaggregated data results in sharper projections based on our climate model.\nOur computationally efficient approach may be widely applicable to a variety of\nhigh-dimensional computer model calibration problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 16:44:40 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2013 21:36:15 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2014 00:38:18 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2014 18:20:12 GMT"}, {"version": "v5", "created": "Thu, 31 Jul 2014 10:45:38 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Chang", "Won", ""], ["Haran", "Murali", ""], ["Olson", "Roman", ""], ["Keller", "Klaus", ""]]}, {"id": "1303.1436", "submitter": "Nanny Wermuth", "authors": "Nanny Wermuth and D.R. Cox", "title": "Concepts and a case study for a flexible class of graphical Markov\n  models", "comments": "21 pages, 7 figures, 7 tables; invited, refereed chapter in a book", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With graphical Markov models, one can investigate complex dependences,\nsummarize some results of statistical analyses with graphs and use these graphs\nto understand implications of well-fitting models. The models have a rich\nhistory and form an area that has been intensively studied and developed in\nrecent years. We give a brief review of the main concepts and describe in more\ndetail a flexible subclass of models, called traceable regressions. These are\nsequences of joint response regressions for which regression graphs permit one\nto trace and thereby understand pathways of dependence. We use these methods to\nreanalyze and interpret data from a prospective study of child development, now\nknown as the Mannheim Study of Children at Risk. The two related primary\nfeatures concern cognitive and motor development, at the age of 4.5 and 8 years\nof a child. Deficits in these features form a sequence of joint responses.\nSeveral possible risks are assessed at birth of the child and when the child\nreached age 3 months and 2 years.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 19:55:22 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Wermuth", "Nanny", ""], ["Cox", "D. R.", ""]]}, {"id": "1303.1788", "submitter": "Hae Kyung Im", "authors": "Heather E. Wheeler, Keston Aquino-Michaels, Eric R. Gamazon, Vassily\n  V. Trubetskoy, M. Eileen Dolan, R. Stephanie Huang, Nancy J. Cox, Hae Kyung\n  Im", "title": "Poly-Omic Prediction of Complex Traits: OmicKriging", "comments": null, "journal-ref": null, "doi": "10.1002/gepi.21808", "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-confidence prediction of complex traits such as disease risk or drug\nresponse is an ultimate goal of personalized medicine. Although genome-wide\nassociation studies have discovered thousands of well-replicated polymorphisms\nassociated with a broad spectrum of complex traits, the combined predictive\npower of these associations for any given trait is generally too low to be of\nclinical relevance. We propose a novel systems approach to complex trait\nprediction, which leverages and integrates similarity in genetic,\ntranscriptomic or other omics-level data. We translate the omic similarity into\nphenotypic similarity using a method called Kriging, commonly used in\ngeostatistics and machine learning. Our method called OmicKriging emphasizes\nthe use of a wide variety of systems-level data, such as those increasingly\nmade available by comprehensive surveys of the genome, transcriptome and\nepigenome, for complex trait prediction. Furthermore, our OmicKriging framework\nallows easy integration of prior information on the function of subsets of\nomics-level data from heterogeneous sources without the sometimes heavy\ncomputational burden of Bayesian approaches. Using seven disease datasets from\nthe Wellcome Trust Case Control Consortium (WTCCC), we show that OmicKriging\nallows simple integration of sparse and highly polygenic components yielding\ncomparable performance at a fraction of the computing time of a recently\npublished Bayesian sparse linear mixed model method. Using a cellular growth\nphenotype, we show that integrating mRNA and microRNA expression data\nsubstantially increases performance over either dataset alone. We also\nintegrate genotype and expression data to predict change in LDL cholesterol\nlevels after statin treatment and show that OmicKriging performs better than\nthe polygenic score method. We provide an R package to implement OmicKriging.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 19:26:33 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2013 15:42:08 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Wheeler", "Heather E.", ""], ["Aquino-Michaels", "Keston", ""], ["Gamazon", "Eric R.", ""], ["Trubetskoy", "Vassily V.", ""], ["Dolan", "M. Eileen", ""], ["Huang", "R. Stephanie", ""], ["Cox", "Nancy J.", ""], ["Im", "Hae Kyung", ""]]}, {"id": "1303.2236", "submitter": "Benjamin Guedj", "authors": "G\\'erard Biau and Aur\\'elie Fischer and Benjamin Guedj and James\n  Malley", "title": "COBRA: A Combined Regression Strategy", "comments": "42 pages", "journal-ref": "Journal of Multivariate Analysis (2016), vol. 146, 18--28", "doi": "10.1016/j.jmva.2015.04.007", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A new method for combining several initial estimators of the regression\nfunction is introduced. Instead of building a linear or convex optimized\ncombination over a collection of basic estimators $r_1,\\dots,r_M$, we use them\nas a collective indicator of the proximity between the training data and a test\nobservation. This local distance approach is model-free and very fast. More\nspecifically, the resulting nonparametric/nonlinear combined estimator is shown\nto perform asymptotically at least as well in the $L^2$ sense as the best\ncombination of the basic estimators in the collective. A companion R package\ncalled \\cobra (standing for COmBined Regression Alternative) is presented\n(downloadable on\n\\url{http://cran.r-project.org/web/packages/COBRA/index.html}). Substantial\nnumerical evidence is provided on both synthetic and real data sets to assess\nthe excellent performance and velocity of our method in a large variety of\nprediction problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 16:52:59 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2013 17:47:41 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2013 18:52:48 GMT"}, {"version": "v4", "created": "Thu, 23 May 2019 05:41:24 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Biau", "G\u00e9rard", ""], ["Fischer", "Aur\u00e9lie", ""], ["Guedj", "Benjamin", ""], ["Malley", "James", ""]]}, {"id": "1303.2316", "submitter": "Tsung-I Lin", "authors": "Tsung-I Lin, Paul D. McNicholas and Hsiu J. Ho", "title": "Capturing Patterns via Parsimonious t Mixture Models", "comments": "19 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exploits a simplified version of the mixture of multivariate\nt-factor analyzers (MtFA) for robust mixture modelling and clustering of\nhigh-dimensional data that frequently contain a number of outliers. Two classes\nof eight parsimonious t mixture models are introduced and computation of\nmaximum likelihood estimates of parameters is achieved using the alternating\nexpectation conditional maximization (AECM) algorithm. The usefulness of the\nmethodology is illustrated through applications of image compression and\ncompact facial representation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 12:08:26 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Lin", "Tsung-I", ""], ["McNicholas", "Paul D.", ""], ["Ho", "Hsiu J.", ""]]}, {"id": "1303.2797", "submitter": "Dimitris Rizopoulos", "authors": "Dimitris Rizopoulos, Laura A. Hatfield, Bradley P. Carlin and Johanna\n  J.M. Takkenberg", "title": "Combining Dynamic Predictions from Joint Models for Longitudinal and\n  Time-to-Event Data using Bayesian Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint modeling of longitudinal and time-to-event data is an active area\nof statistics research that has received a lot of attention in the recent\nyears. More recently, a new and attractive application of this type of models\nhas been to obtain individualized predictions of survival probabilities and/or\nof future longitudinal responses. The advantageous feature of these predictions\nis that they are dynamically updated as extra longitudinal responses are\ncollected for the subjects of interest, providing real time risk assessment\nusing all recorded information. The aim of this paper is two-fold. First, to\nhighlight the importance of modeling the association structure between the\nlongitudinal and event time responses that can greatly influence the derived\npredictions, and second, to illustrate how we can improve the accuracy of the\nderived predictions by suitably combining joint models with different\nassociation structures. The second goal is achieved using Bayesian model\naveraging, which, in this setting, has the very intriguing feature that the\nmodel weights are not fixed but they are rather subject- and time-dependent,\nimplying that at different follow-up times predictions for the same subject may\nbe based on different models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 07:39:31 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Rizopoulos", "Dimitris", ""], ["Hatfield", "Laura A.", ""], ["Carlin", "Bradley P.", ""], ["Takkenberg", "Johanna J. M.", ""]]}, {"id": "1303.2962", "submitter": "Pierre Latouche", "authors": "E. C\\^ome, P. Latouche", "title": "Model selection and clustering in stochastic block models with the exact\n  integrated complete data likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a mixture model used for the clustering\nof nodes in networks. It has now been employed for more than a decade to\nanalyze very different types of networks in many scientific fields such as\nBiology and social sciences. Because of conditional dependency, there is no\nanalytical expression for the posterior distribution over the latent variables,\ngiven the data and model parameters. Therefore, approximation strategies, based\non variational techniques or sampling, have been proposed for clustering.\nMoreover, two SBM model selection criteria exist for the estimation of the\nnumber K of clusters in networks but, again, both of them rely on some\napproximations. In this paper, we show how an analytical expression can be\nderived for the integrated complete data log likelihood. We then propose an\ninference algorithm to maximize this exact quantity. This strategy enables the\nclustering of nodes as well as the estimation of the number clusters to be\nperformed at the same time and no model selection criterion has to be computed\nfor various values of K. The algorithm we propose has a better computational\ncost than existing inference techniques for SBM and can be employed to analyze\nlarge networks with ten thousand nodes. Using toy and true data sets, we\ncompare our work with other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 17:37:00 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 10:46:24 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["C\u00f4me", "E.", ""], ["Latouche", "P.", ""]]}, {"id": "1303.3079", "submitter": "Jeffrey Regier", "authors": "Jeffrey C. Regier and Philip B. Stark", "title": "Mini-Minimax Uncertainty Quantification for Emulators", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification. 3-1 (2015), pp.\n  686-708", "doi": "10.1137/130917909", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider approximating a \"black box\" function $f$ by an emulator $\\hat{f}$\nbased on $n$ noiseless observations of $f$. Let $w$ be a point in the domain of\n$f$. How big might the error $|\\hat{f}(w) - f(w)|$ be? If $f$ could be\narbitrarily rough, this error could be arbitrarily large: we need some\nconstraint on $f$ besides the data. Suppose $f$ is Lipschitz with known\nconstant. We find a lower bound on the number of observations required to\nensure that for the best emulator $\\hat{f}$ based on the $n$ data, $|\\hat{f}(w)\n- f(w)| \\le \\epsilon$. But in general, we will not know whether $f$ is\nLipschitz, much less know its Lipschitz constant. Assume optimistically that\n$f$ is Lipschitz-continuous with the smallest constant consistent with the $n$\ndata. We find the maximum (over such regular $f$) of $|\\hat{f}(w) - f(w)|$ for\nthe best possible emulator $\\hat{f}$; we call this the \"mini-minimax\nuncertainty\" at $w$. In reality, $f$ might not be Lipschitz or---if it is---it\nmight not attain its Lipschitz constant on the data. Hence, the mini-minimax\nuncertainty at $w$ could be much smaller than $|\\hat{f}(w) - f(w)|$. But if the\nmini-minimax uncertainty is large, then---even if $f$ satisfies the optimistic\nregularity assumption---$|\\hat{f}(w) - f(w)|$ could be large, no matter how\ncleverly we choose $\\hat{f}$. For the Community Atmosphere Model, the maximum\n(over $w$) of the mini-minimax uncertainty based on a set of 1154~observations\nof $f$ is no smaller than it would be for a single observation of $f$ at the\ncentroid of the 21-dimensional parameter space. We also find lower confidence\nbounds for quantiles of the mini-minimax uncertainty and its mean over the\ndomain of $f$. For the Community Atmosphere Model, these lower confidence\nbounds are an appreciable fraction of the maximum.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 02:29:37 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 00:08:50 GMT"}, {"version": "v3", "created": "Sun, 7 Jul 2013 17:25:32 GMT"}, {"version": "v4", "created": "Tue, 8 Apr 2014 20:16:06 GMT"}, {"version": "v5", "created": "Mon, 14 Sep 2015 21:29:54 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Regier", "Jeffrey C.", ""], ["Stark", "Philip B.", ""]]}, {"id": "1303.3118", "submitter": "Johannes Schmidt-Hieber", "authors": "Johannes Schmidt-Hieber", "title": "On an estimator achieving the adaptive rate in nonparametric regression\n  under $L^p$-loss for all $1\\leq p \\leq \\infty$", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider nonparametric function estimation under $L^p$-loss. The minimax rate\nfor estimation of the regression function over a H\\\"older ball with smoothness\nindex $\\beta$ is $n^{-\\beta/(2\\beta+1)}$ if $1\\leq p<\\infty$ and $(n/\\log\nn)^{-\\beta/(2\\beta+1)}$ if $p=\\infty.$ There are many known procedures that\neither attain this rate for $p=\\infty$ but are suboptimal by a $\\log n$ factor\nin the case $p<\\infty$ or the other way around. In this article, we construct\nan estimator that simultaneously achieves the optimal rates under $L^p$-risk\nfor all $1\\leq p\\leq \\infty$ without prior knowledge of $\\beta.$ In contrast to\nclassical wavelet thresholding methods that kill small empirical wavelet\ncoefficients and keep large ones, it is essential for simultaneous adaptation\nthat on each resolution level, the largest empirical wavelet coefficients are\ntruncated. This leads to a completely different point of view on wavelet\nthresholding. The crucial part in the construction of the estimator is the size\nof the truncation level which is linked to the unknown smoothness index.\nAlthough estimation of the smoothness index is known to be a difficult task,\nthere is a data-driven choice of the truncation level that is sufficiently\nprecise for our purpose.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 09:52:42 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 11:16:19 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1303.3123", "submitter": "John Aston", "authors": "Yan Zhou, Adam M Johansen and John A D Aston", "title": "Towards Automatic Model Comparison: An Adaptive Sequential Monte Carlo\n  Approach", "comments": "31 pages; 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model comparison for the purposes of selection, averaging and validation is a\nproblem found throughout statistics. Within the Bayesian paradigm, these\nproblems all require the calculation of the posterior probabilities of models\nwithin a particular class. Substantial progress has been made in recent years,\nbut difficulties remain in the implementation of existing schemes. This paper\npresents adaptive sequential Monte Carlo (\\smc) sampling strategies to\ncharacterise the posterior distribution of a collection of models, as well as\nthe parameters of those models. Both a simple product estimator and a\ncombination of \\smc and a path sampling estimator are considered and existing\ntheoretical results are extended to include the path sampling variant. A novel\napproach to the automatic specification of distributions within \\smc algorithms\nis presented and shown to outperform the state of the art in this area. The\nperformance of the proposed strategies is demonstrated via an extensive\nempirical study. Comparisons with state of the art algorithms show that the\nproposed algorithms are always competitive, and often substantially superior to\nalternative techniques, at equal computational cost and considerably less\napplication-specific implementation effort.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 10:18:17 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 10:30:49 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Zhou", "Yan", ""], ["Johansen", "Adam M", ""], ["Aston", "John A D", ""]]}, {"id": "1303.3128", "submitter": "Chinghway Lim", "authors": "Chinghway Lim and Bin Yu", "title": "Estimation Stability with Cross Validation (ESCV)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is often used to select the regularization parameter in\nhigh dimensional problems. However, when applied to the sparse modeling method\nLasso, CV leads to models that are unstable in high-dimensions, and\nconsequently not suited for reliable interpretation. In this paper, we propose\na model-free criterion ESCV based on a new estimation stability (ES) metric and\nCV. Our proposed ESCV finds a locally ES-optimal model smaller than the CV\nchoice so that the it fits the data and also enjoys estimation stability\nproperty. We demonstrate that ESCV is an effective alternative to CV at a\nsimilar easily parallelizable computational cost. In particular, we compare the\ntwo approaches with respect to several performance measures when applied to the\nLasso on both simulated and real data sets. For dependent predictors common in\npractice, our main finding is that, ESCV cuts down false positive rates often\nby a large margin, while sacrificing little of true positive rates. ESCV\nusually outperforms CV in terms of parameter estimation while giving similar\nperformance as CV in terms of prediction. For the two real data sets from\nneuroscience and cell biology, the models found by ESCV are less than half of\nthe model sizes by CV. Judged based on subject knowledge, they are more\nplausible than those by CV as well. We also discuss some regularization\nparameter alignment issues that come up in both approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 10:47:42 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 09:36:50 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Lim", "Chinghway", ""], ["Yu", "Bin", ""]]}, {"id": "1303.3216", "submitter": "Alain Hauser", "authors": "Alain Hauser and Peter B\\\"uhlmann", "title": "Jointly interventional and observational data: estimation of\n  interventional Markov equivalence classes of directed acyclic graphs", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12071", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications we have both observational and (randomized)\ninterventional data. We propose a Gaussian likelihood framework for joint\nmodeling of such different data-types, based on global parameters consisting of\na directed acyclic graph (DAG) and correponding edge weights and error\nvariances. Thanks to the global nature of the parameters, maximum likelihood\nestimation is reasonable with only one or few data points per intervention. We\nprove consistency of the BIC criterion for estimating the interventional Markov\nequivalence class of DAGs which is smaller than the observational analogue due\nto increased partial identifiability from interventional data. Such an\nimprovement in identifiability has immediate implications for tighter bounds\nfor inferring causal effects. Besides methodology and theoretical derivations,\nwe present empirical results from real and simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 17:19:47 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Hauser", "Alain", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1303.3390", "submitter": "Steven Geinitz", "authors": "Steven Geinitz and Reinhard Furrer", "title": "Conjugate distributions in hierarchical Bayesian ANOVA for computational\n  efficiency and assessments of both practical and statistical significance", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing variability according to distinct factors in data is a fundamental\ntechnique of statistics. The method commonly regarded to as analysis of\nvariance (ANOVA) is, however, typically confined to the case where all levels\nof a factor are present in the data (i.e. the population of factor levels has\nbeen exhausted). Random and mixed effects models are used for more elaborate\ncases, but require distinct nomenclature, concepts and theory, as well as\ndistinct inferential procedures. Following a hierarchical Bayesian approach, a\ncomprehensive ANOVA framework is shown, which unifies the above statistical\nmodels, emphasizes practical rather than statistical significance, addresses\nissues of parameter identifiability for random effects, and provides\nstraightforward computational procedures for inferential steps. Although this\nis done in a rigorous manner the contents herein can be seen as ideological in\nsupporting a shift in the approach taken towards analysis of variance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 10:03:50 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Geinitz", "Steven", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1303.3574", "submitter": "Alexandre Janon", "authors": "Fabrice Gamboa (UMR CNRS 5219), Alexandre Janon (INRIA Grenoble\n  Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann, - M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques, SAF), Thierry Klein\n  (IMT), Agn\\`es Lagnoux (UMR CNRS 5219)", "title": "Sensitivity indices for multivariate outputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study a generalization of Sobol sensitivity indices for the\ncase of a vector output.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 19:50:07 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2013 15:11:10 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Gamboa", "Fabrice", "", "UMR CNRS 5219"], ["Janon", "Alexandre", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann, - M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques, SAF"], ["Klein", "Thierry", "", "IMT"], ["Lagnoux", "Agn\u00e8s", "", "UMR CNRS 5219"]]}, {"id": "1303.3750", "submitter": "Julian Faraway", "authors": "Julian Faraway", "title": "Regression with Distance Matrices", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": "10.1080/02664763.2014.909794", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data types that lie in metric spaces but not in vector spaces are difficult\nto use within the usual regression setting, either as the response and/or a\npredictor. We represent the information in these variables using distance\nmatrices which requires only the specification of a distance function. A\nlow-dimensional representation of such distance matrices can be obtained using\nmethods such as multidimensional scaling. Once these variables have been\nrepresented as scores, an internal model linking the predictors and the\nresponse can be developed using standard methods. We call scoring the\ntransformation from a new observation to a score while backscoring is a method\nto represent a score as an observation in the data space. Both methods are\nessential for prediction and explanation. We illustrate the methodology for\nshape data, unregistered curve data and correlation matrices using motion\ncapture data from an experiment to study the motion of children with cleft lip.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 12:10:25 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2013 14:15:01 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Faraway", "Julian", ""]]}, {"id": "1303.4121", "submitter": "Gery Geenens", "authors": "Gery Geenens", "title": "Probit transformation for kernel density estimation on the unit interval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel estimation of a probability density function supported on the unit\ninterval has proved difficult, because of the well known boundary bias issues a\nconventional kernel density estimator would necessarily face in this situation.\nTransforming the variable of interest into a variable whose density has\nunconstrained support, estimating that density, and obtaining an estimate of\nthe density of the original variable through back-transformation, seems a\nnatural idea to easily get rid of the boundary problems. In practice, however,\na simple and efficient implementation of this methodology is far from\nimmediate, and the few attempts found in the literature have been reported not\nto perform well. In this paper, the main reasons for this failure are\nidentified and an easy way to correct them is suggested. It turns out that\ncombining the transformation idea with local likelihood density estimation\nproduces viable density estimators, mostly free from boundary issues. Their\nasymptotic properties are derived, and a practical cross-validation bandwidth\nselection rule is devised. Extensive simulations demonstrate the excellent\nperformance of these estimators compared to their main competitors for a wide\nrange of density shapes. In fact, they turn out to be the best choice overall.\nFinally, they are used to successfully estimate a density of non-standard shape\nsupported on $[0,1]$ from a small-size real data sample.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2013 23:42:28 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Geenens", "Gery", ""]]}, {"id": "1303.5228", "submitter": "Robin  Lovelace", "authors": "Robin Lovelace, Dimitris Ballas", "title": "`Truncate, replicate, sample': a method for creating integer weights for\n  spatial microsimulation", "comments": "51 pages, 10 images (including supplementary information)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Iterative proportional fitting (IPF) is a widely used method for spatial\nmicrosimulation. The technique results in non-integer weights for individual\nrows of data. This is problematic for certain applications and has led many\nresearchers to favour combinatorial optimisation approaches such as simulated\nannealing. An alternative to this is `integerisation' of IPF weights: the\ntranslation of the continuous weight variable into a discrete number of unique\nor `cloned' individuals. We describe four existing methods of integerisation\nand present a new one. Our method --- `truncate, replicate, sample' (TRS) ---\nrecognises that IPF weights consist of both `replication weights' and\n`conventional weights', the effects of which need to be separated. The\nprocedure consists of three steps: 1) separate replication and conventional\nweights by truncation; 2) replication of individuals with positive integer\nweights; and 3) probabilistic sampling. The results, which are reproducible\nusing supplementary code and data published alongside this paper, show that TRS\nis fast, and more accurate than alternative approaches to integerisation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 11:04:10 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Lovelace", "Robin", ""], ["Ballas", "Dimitris", ""]]}, {"id": "1303.5525", "submitter": "Saeid Rezakhah", "authors": "N. Alemohammad, S. Rezakhah, S. H. Alizadeh", "title": "Markov Switching Component ARCH Model: Stability and Forecasting", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an extension of the Markov switching GARCH model where\nthe volatility in each state is a convex combination of two different GARCH\ncomponents with time varying weights. This model has the dynamic behavior to\ncapture the variants of shocks. The asymptotic behavior of the second moment is\ninvestigated and an appropriate upper bound for it is evaluated. The estimation\nof the parameters by using the Bayesian method via Gibbs sampling algorithm is\nstudied. Finally we illustrate the efficiency of the model by simulation and\nempirical analysis. We show that this model provides a much better forecast of\nthe volatility than the Markov switching GARCH model.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 06:21:42 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 10:35:51 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Alemohammad", "N.", ""], ["Rezakhah", "S.", ""], ["Alizadeh", "S. H.", ""]]}, {"id": "1303.5536", "submitter": "Saeid Rezakhah", "authors": "Akram Kohansal, Saeid Rezakhah", "title": "Testing Exponentiality Based on R\\'enyi Entropy With Progressively\n  Type-II Censored Data", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We express the joint R\\'enyi entropy of progressively censored order\nstatistics in terms of an incomplete integral of the hazard function, and\nprovide a simple estimate of the joint R\\'enyi entropy of progressively Type-II\ncensored data. Then we establish a goodness of fit test statistic based on the\nR\\'enyi Kullback-Leibler information with the progressively Type-II censored\ndata, and compare its performance with the leading test statistic. A Monte\nCarlo simulation study shows that the proposed test statistic shows better\npowers than the leading test statistic against the alternatives with monotone\nincreasing, monotone decreasing and nonmonotone hazard functions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 07:45:01 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Kohansal", "Akram", ""], ["Rezakhah", "Saeid", ""]]}, {"id": "1303.5973", "submitter": "Christian P. Robert", "authors": "Christian Robert (Universite Paris-Dauphine, University of Warwick,\n  and CREST)", "title": "On the Jeffreys-Lindley's paradox", "comments": "15 pages (second revision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the dual interpretation of the Jeffreys--Lindley's\nparadox associated with Bayesian posterior probabilities and Bayes factors,\nboth as a differentiation between frequentist and Bayesian statistics and as a\npointer to the difficulty of using improper priors while testing. We stress the\nconsiderable impact of this paradox on the foundations of both classical and\nBayesian statistics. While assessing existing resolutions of the paradox, we\nfocus on a critical viewpoint of the paradox discussed by Spanos (2013) in\nPhilosophy of Science.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 18:21:05 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 13:56:45 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2013 21:27:24 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Robert", "Christian", "", "Universite Paris-Dauphine, University of Warwick,\n  and CREST"]]}, {"id": "1303.6073", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene, Marta Alvarez and Luis Pericchi", "title": "A Robust Bayesian Dynamic Linear Model for Latin-American Economic Time\n  Series: \"The Mexico and Puerto Rico Cases\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional time series methodology requires at least a preliminary\ntransformation of the data to get stationarity. On the other hand, Robust\nBayesian Dynamic Models (RBDMs) do not assume a regular pattern or stability of\nthe underlying system but can include points of statement breaks. In this paper\nwe use RBDMs in order to account possible outliers and structural breaks in\nLatin-American economic time series. We work with important economic time\nseries from Puerto Rico and Mexico. We show by using a random walk model how\nRBDMs can be applied for detecting historic changes in the economic inflation\nof Mexico. Also, we model the Consumer Price Index (CPI), the Economic Activity\nIndex (EAI) and the total number of employments (TNE) economic time series in\nPuerto Rico using local linear trend and seasonal RBDMs with observational and\nstates variances. The results illustrate how the model accounts the structural\nbreaks for the historic recession periods in Puerto Rico.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 10:17:41 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 19:23:24 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Fuquene", "Jairo", ""], ["Alvarez", "Marta", ""], ["Pericchi", "Luis", ""]]}, {"id": "1303.6165", "submitter": "Tao Liu", "authors": "Tao Liu and Joseph W. Hogan", "title": "Inference about ATE from Observational Studies with Continuous Outcome\n  and Unmeasured Confounding", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For settings with a binary treatment and a binary outcome, instrumental\nvariables can be used to construct bounds on a causal treatment effect. With\ncontinuous outcomes, meaningful bounds are more difficult to obtain because the\ndomain of the outcome is typically unrestricted. In this paper, we combine an\ninstrumental variable and subjective assumptions in the context of an obser-\nvational cohort study of HIV-infected women to construct meaningful bounds on\nthe initial-stage causal effect of antiretroviral therapy on CD4 count. The\nsubjective assumptions are encoded in terms of the potential outcomes that are\nidentified by observed data as well as a sensitivity parameter that captures\nthe impact of unmeasured confounding. Measured confounding is adjusted using\nthe method of inverse probability weighting (IPW). With extra information from\nan IV, we quantify both the causal treatment effect and the degree of the\nunmea- sured confounding. We demonstrate our method by analyzing data from the\nHIV Epidemiology Research Study.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 15:22:01 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Liu", "Tao", ""], ["Hogan", "Joseph W.", ""]]}, {"id": "1303.6182", "submitter": "Stefan Siegert", "authors": "Stefan Siegert", "title": "Variance estimation for Brier Score decomposition", "comments": "10 pages, 2 figures, 2 tables", "journal-ref": null, "doi": "10.1002/qj.2228", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Brier Score is a widely-used criterion to assess the quality of\nprobabilistic predictions of binary events. The expectation value of the Brier\nScore can be decomposed into the sum of three components called reliability,\nresolution, and uncertainty which characterize different forecast attributes.\nGiven a dataset of forecast probabilities and corresponding binary\nverifications, these three components can be estimated empirically. Here,\npropagation of uncertainty is used to derive expressions that approximate the\nsampling variances of the estimated components. Variance estimates are provided\nfor both the traditional estimators, as well as for refined estimators that\ninclude a bias correction. Applications of the derived variance estimates to\nartificial data illustrate their validity, and application to a meteorological\nprediction problem illustrates a possible use case. The observed increase of\nvariance of the bias-corrected estimators is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 15:56:58 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 06:34:43 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Siegert", "Stefan", ""]]}, {"id": "1303.6199", "submitter": "Sonia Dias Mrs", "authors": "S\\'onia Dias and Paula Brito", "title": "Distribution and Symmetric Distribution Regression Model for\n  Histogram-Valued Variables", "comments": "49 pages 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histogram-valued variables are a particular kind of variables studied in\nSymbolic Data Analysis where to each entity under analysis corresponds a\ndistribution that may be represented by a histogram or by a quantile function.\nLinear regression models for this type of data are necessarily more complex\nthan a simple generalization of the classical model: the parameters cannot be\nnegative still the linear relationship between the variables must be allowed to\nbe either direct or inverse. In this work we propose a new linear regression\nmodel for histogram-valued variables that solves this problem, named\nDistribution and Symmetric Distribution Regression Model. To determine the\nparameters of this model it is necessary to solve a quadratic optimization\nproblem, subject to non-negativity constraints on the unknowns; the error\nmeasure between the predicted and observed distributions uses the Mallows\ndistance. As in classical analysis, the model is associated with a\ngoodness-of-fit measure whose values range between 0 and 1. Using the proposed\nmodel, applications with real and simulated data are presented.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 16:27:57 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Dias", "S\u00f3nia", ""], ["Brito", "Paula", ""]]}, {"id": "1303.6223", "submitter": "Rajen Shah", "authors": "Rajen Dinesh Shah, Nicolai Meinshausen", "title": "Random Intersection Trees", "comments": "23 pages, 4 figures", "journal-ref": "Journal of Machine Learning Research 15 (2014) 629-654", "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding interactions between variables in large and high-dimensional datasets\nis often a serious computational challenge. Most approaches build up\ninteraction sets incrementally, adding variables in a greedy fashion. The\ndrawback is that potentially informative high-order interactions may be\noverlooked. Here, we propose at an alternative approach for classification\nproblems with binary predictor variables, called Random Intersection Trees. It\nworks by starting with a maximal interaction that includes all variables, and\nthen gradually removing variables if they fail to appear in randomly chosen\nobservations of a class of interest. We show that informative interactions are\nretained with high probability, and the computational complexity of our\nprocedure is of order $p^\\kappa$ for a value of $\\kappa$ that can reach values\nas low as 1 for very sparse data; in many more general settings, it will still\nbeat the exponent $s$ obtained when using a brute force search constrained to\norder $s$ interactions. In addition, by using some new ideas based on min-wise\nhash schemes, we are able to further reduce the computational cost.\nInteractions found by our algorithm can be used for predictive modelling in\nvarious forms, but they are also often of interest in their own right as useful\ncharacterisations of what distinguishes a certain class from others.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 17:29:24 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Shah", "Rajen Dinesh", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1303.6447", "submitter": "Alexandre Janon", "authors": "Fabrice Gamboa (UMR CNRS 5219), Alexandre Janon (INRIA Grenoble\n  Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann, - M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques, SAF), Thierry Klein\n  (IMT), Agnes Lagnoux-Renaudie (IMT), Cl\\'ementine Prieur (INRIA Grenoble\n  Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann, - M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques), Cl\\'ementine Prieur\n  (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann, - M\\'ethodes\n  d'Analyse Stochastique des Codes et Traitements Num\\'eriques)", "title": "Statistical inference for Sobol pick freeze Monte Carlo method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many mathematical models involve input parameters, which are not precisely\nknown. Global sensitivity analysis aims to identify the parameters whose\nuncertainty has the largest impact on the variability of a quantity of interest\n(output of the model). One of the statistical tools used to quantify the\ninfluence of each input variable on the output is the Sobol sensitivity index.\nWe consider the statistical estimation of this index from a finite sample of\nmodel outputs. We study asymptotic and non-asymptotic properties of two\nestimators of Sobol indices. These properties are applied to significance tests\nand estimation by confidence intervals.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 12:03:48 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Gamboa", "Fabrice", "", "UMR CNRS 5219"], ["Janon", "Alexandre", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann, - M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques, SAF"], ["Klein", "Thierry", "", "IMT"], ["Lagnoux-Renaudie", "Agnes", "", "IMT"], ["Prieur", "Cl\u00e9mentine", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann, - M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques"], ["Prieur", "Cl\u00e9mentine", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann, - M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques"]]}, {"id": "1303.6454", "submitter": "Dimitris Kugiumtzis", "authors": "Dimitris Kugiumtzis", "title": "Partial Transfer Entropy on Rank Vectors", "comments": "21 pages, 6 figures, 3 tables, accepted in EPJ/ST", "journal-ref": null, "doi": "10.1140/epjst/e2013-01849-4", "report-no": null, "categories": "stat.ME cs.IT math.IT nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the evaluation of information flow in bivariate time series, information\nmeasures have been employed, such as the transfer entropy (TE), the symbolic\ntransfer entropy (STE), defined similarly to TE but on the ranks of the\ncomponents of the reconstructed vectors, and the transfer entropy on rank\nvectors (TERV), similar to STE but forming the ranks for the future samples of\nthe response system with regard to the current reconstructed vector. Here we\nextend TERV for multivariate time series, and account for the presence of\nconfounding variables, called partial transfer entropy on ranks (PTERV). We\ninvestigate the asymptotic properties of PTERV, and also partial STE (PSTE),\nconstruct parametric significance tests under approximations with Gaussian and\ngamma null distributions, and show that the parametric tests cannot achieve the\npower of the randomization test using time-shifted surrogates. Using\nsimulations on known coupled dynamical systems and applying parametric and\nrandomization significance tests, we show that PTERV performs better than PSTE\nbut worse than the partial transfer entropy (PTE). However, PTERV, unlike PTE,\nis robust to the presence of drifts in the time series and it is also not\naffected by the level of detrending.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 12:35:45 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Kugiumtzis", "Dimitris", ""]]}, {"id": "1303.6466", "submitter": "Jean-Bernard Salomond", "authors": "Jean-Bernard Salomond", "title": "Testing Un-Separated Hypotheses by Estimating a Distance", "comments": null, "journal-ref": null, "doi": "10.1214/17-BA1059", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a Bayesian answer to testing problems when the\nhypotheses are not well separated. The idea of the method is to study the\nposterior distribution of a discrepancy measure between the parameter and the\nmodel we want to test for. This is shown to be equivalent to a modification of\nthe testing loss. An advantage of this approach is that it can easily be\nadapted to complex hypotheses testing which are in general difficult to test\nfor. Asymptotic properties of the test can be derived from the asymptotic\nbehaviour of the posterior distribution of the discrepancy measure, and gives\ninsight on possible calibrations. In addition one can derive separation rates\nfor testing, which ensure the asymptotic frequentist optimality of our\nprocedures.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 12:57:54 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 08:47:47 GMT"}, {"version": "v3", "created": "Thu, 19 Feb 2015 14:51:47 GMT"}, {"version": "v4", "created": "Tue, 27 Jun 2017 09:56:37 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Salomond", "Jean-Bernard", ""]]}, {"id": "1303.6584", "submitter": "Christophe Ley", "authors": "Christophe Ley and Thomas Verdebout", "title": "Simple, asymptotically distribution-free, optimal tests for circular\n  reflective symmetry about a known median direction", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose optimal tests for circular reflective symmetry\nabout a fixed median direction. The distributions against which optimality is\nachieved are the so-called k-sine-skewed distributions of Umbach and\nJammalamadaka (2009). We first show that sequences of k-sine-skewed models are\nlocally and asymptotically normal in the vicinity of reflective symmetry.\nFollowing the Le Cam methodology, we then construct optimal (in the maximin\nsense) parametric tests for reflective symmetry, which we render\nsemi-parametric by a studentization argument. These asymptotically\ndistribution-free tests happen to be uniformly optimal (under any reference\ndensity) and are moreover of a very simple and intuitive form. They furthermore\nexhibit nice small sample properties, as we show through a Monte Carlo\nsimulation study. Our new tests also allow us to re-visit the famous red wood\nants data set of Jander (1957). We further show that one of the proposed\nparametric tests can as well serve as a test for uniformity against cardioid\nalternatives; this test coincides with the famous circular Rayleigh (1919) test\nfor uniformity which is thus proved to be (also) optimal against cardioid\nalternatives. Moreover, our choice of k-sine-skewed alternatives, which are the\ncircular analogues of the classical linear skew-symmetric distributions,\npermits us a Fisher singularity analysis \\`a la Hallin and Ley (2012) with the\nresult that only the prominent sine-skewed von Mises distribution suffers from\nthese inferential drawbacks. Finally, we conclude the paper by discussing the\nunspecified location case.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 18:13:08 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Ley", "Christophe", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1303.6598", "submitter": "Daniel Gervini", "authors": "Daniel Gervini and Patrick A. Carter", "title": "Warped Functional Analysis of Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an Analysis of Variance model for functional data that\nexplicitly incorporates phase variability through a time-warping component,\nallowing for a unified approach to estimation and inference in presence of\namplitude and time variability. The focus is on single-random-factor models but\nthe approach can be easily generalized to more complex ANOVA models. The\nbehavior of the estimators is studied by simulation, and an application to the\nanalysis of growth curves of flour beetles is presented. Although the model\nassumes a smooth latent process behind the observed trajectories, smoothness of\nthe observed data is not required; the method can be applied to the sparsely\nobserved data that is often encountered in longitudinal studies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 18:49:45 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 19:27:14 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2013 00:35:10 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Gervini", "Daniel", ""], ["Carter", "Patrick A.", ""]]}, {"id": "1303.6668", "submitter": "Scott Holan", "authors": "Aaron T. Porter, Scott H. Holan, Christopher K. Wikle, and Noel\n  Cressie", "title": "Spatial Fay-Herriot Models for Small Area Estimation with Functional\n  Covariates", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fay-Herriot (FH) model is widely used in small area estimation and uses\nauxiliary information to reduce estimation variance at undersampled locations.\nWe extend the type of covariate information used in the FH model to include\nfunctional covariates, such as social-media search loads or remote-sensing\nimages (e.g., in crop-yield surveys). The inclusion of these functional\ncovariates is facilitated through a two-stage dimension-reduction approach that\nincludes a Karhunen-Lo\\`{e}ve expansion followed by stochastic search variable\nselection. Additionally, the importance of modeling spatial autocorrelation has\nrecently been recognized in the FH model; our model utilizes the intrinsic\nconditional autoregressive class of spatial models in addition to functional\ncovariates. We demonstrate the effectiveness of our approach through simulation\nand analysis of data from the American Community Survey. We use Google Trends\nsearches over time as functional covariates to analyze relative changes in\nrates of percent household Spanish-speaking in the eastern half of the United\nStates.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 21:31:22 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2013 02:23:49 GMT"}, {"version": "v3", "created": "Fri, 9 May 2014 18:15:27 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Porter", "Aaron T.", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""], ["Cressie", "Noel", ""]]}, {"id": "1303.6700", "submitter": "Tatiana Tatarinova", "authors": "Tatiana Tatarinova, Michael Neely, Jay Bartroff, Michael van Guilder,\n  Walter Yamada, David Bayard, Roger Jelliffe, Robert Leary, Alyona Chubatiuk\n  and Alan Schumitzky", "title": "Two General Methods for Population Pharmacokinetic Modeling:\n  Non-Parametric Adaptive Grid and Non-Parametric Bayesian", "comments": null, "journal-ref": "Tatarinova et al, Journal of Pharmacokinetics and\n  Pharmacodynamics, 2013, vol. 40 no 1", "doi": "10.1007/s10928-013-9302-8", "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population pharmacokinetic (PK) modeling methods can be statistically\nclassified as either parametric or nonparametric (NP). Each classification can\nbe divided into maximum likelihood (ML) or Bayesian (B) approaches. In this\npaper we discuss the nonparametric case using both maximum likelihood and\nBayesian approaches. We present two nonparametric methods for estimating the\nunknown joint population distribution of model parameter values in a\npharmacokinetic/pharmacodynamic (PK/PD) dataset. The first method is the NP\nAdaptive Grid (NPAG). The second is the NP Bayesian (NPB) algorithm with a\nstick-breaking process to construct a Dirichlet prior. Our objective is to\ncompare the performance of these two methods using a simulated PK/PD dataset.\nOur results showed excellent performance of NPAG and NPB in a realistically\nsimulated PK study. This simulation allowed us to have benchmarks in the form\nof the true population parameters to compare with the estimates produced by the\ntwo methods, while incorporating challenges like unbalanced sample times and\nsample numbers as well as the ability to include the covariate of patient\nweight. We conclude that both NPML and NPB can be used in realistic PK/PD\npopulation analysis problems. The advantages of one versus the other are\ndiscussed in the paper. NPAG and NPB are implemented in R and freely available\nfor download within the Pmetrics package from www.lapk.org.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 23:04:41 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Tatarinova", "Tatiana", ""], ["Neely", "Michael", ""], ["Bartroff", "Jay", ""], ["van Guilder", "Michael", ""], ["Yamada", "Walter", ""], ["Bayard", "David", ""], ["Jelliffe", "Roger", ""], ["Leary", "Robert", ""], ["Chubatiuk", "Alyona", ""], ["Schumitzky", "Alan", ""]]}, {"id": "1303.7002", "submitter": "Giovanni Montana", "authors": "Christopher Minas, Edward Curry and Giovanni Montana", "title": "A Distance-Based Test of Association Between Paired Heterogeneous\n  Genomic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to rapid technological advances, a wide range of different measurements\ncan be obtained from a given biological sample including single nucleotide\npolymorphisms, copy number variation, gene expression levels, DNA methylation\nand proteomic profiles. Each of these distinct measurements provides the means\nto characterize a certain aspect of biological diversity, and a fundamental\nproblem of broad interest concerns the discovery of shared patterns of\nvariation across different data types. Such data types are heterogeneous in the\nsense that they represent measurements taken at very different scales or\ndescribed by very different data structures. We propose a distance-based\nstatistical test, the generalized RV (GRV) test, to assess whether there is a\ncommon and non-random pattern of variability between paired biological\nmeasurements obtained from the same random sample. The measurements enter the\ntest through distance measures which can be chosen to capture particular\naspects of the data. An approximate null distribution is proposed to compute\np-values in closed-form and without the need to perform costly Monte Carlo\npermutation procedures. Compared to the classical Mantel test for association\nbetween distance matrices, the GRV test has been found to be more powerful in a\nnumber of simulation settings. We also report on an application of the GRV test\nto detect biological pathways in which genetic variability is associated to\nvariation in gene expression levels in ovarian cancer samples, and present\nresults obtained from two independent cohorts.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 23:00:49 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Minas", "Christopher", ""], ["Curry", "Edward", ""], ["Montana", "Giovanni", ""]]}, {"id": "1303.7409", "submitter": "Tracy Ke", "authors": "Tracy Ke, Jianqing Fan and Yichao Wu", "title": "Homogeneity in Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the homogeneity of coefficients in high-dimensional\nregression, which extends the sparsity concept and is more general and suitable\nfor many applications. Homogeneity arises when one expects regression\ncoefficients corresponding to neighboring geographical regions or a similar\ncluster of covariates to be approximately the same. Sparsity corresponds to a\nspecial case of homogeneity with a known atom zero. In this article, we propose\na new method called clustering algorithm in regression via data-driven\nsegmentation (CARDS) to explore homogeneity. New mathematics are provided on\nthe gain that can be achieved by exploring homogeneity. Statistical properties\nof two versions of CARDS are analyzed. In particular, the asymptotic normality\nof our proposed CARDS estimator is established, which reveals better estimation\naccuracy for homogeneous parameters than that without homogeneity exploration.\nWhen our methods are combined with sparsity exploration, further efficiency can\nbe achieved beyond the exploration of sparsity alone. This provides additional\ninsights into the power of exploring low-dimensional strucuture in\nhigh-dimensional regression: homogeneity and sparsity. The newly developed\nmethod is further illustrated by simulation studies and applications to real\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 14:34:10 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Ke", "Tracy", ""], ["Fan", "Jianqing", ""], ["Wu", "Yichao", ""]]}]