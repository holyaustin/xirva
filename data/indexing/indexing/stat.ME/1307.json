[{"id": "1307.0056", "submitter": "Zuofeng Shang", "authors": "Zuofeng Shang, Ping Li", "title": "High-Dimensional Bayesian Inference in Nonparametric Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fully Bayesian approach is proposed for ultrahigh-dimensional nonparametric\nadditive models in which the number of additive components may be larger than\nthe sample size, though ideally the true model is believed to include only a\nsmall number of components. Bayesian approaches can conduct stochastic model\nsearch and fulfill flexible parameter estimation by stochastic draws. The\ntheory shows that the proposed model selection method has satisfactory\nproperties. For instance, when the hyperparameter associated with the model\nprior is correctly specified, the true model has posterior probability\napproaching one as the sample size goes to infinity; when this hyperparameter\nis incorrectly specified, the selected model is still acceptable since\nasymptotically it is proved to be nested in the true model. To enhance model\nflexibility, two new $g$-priors are proposed and their theoretical performance\nis examined. We also propose an efficient MCMC algorithm to handle the\ncomputational issues. Several simulation examples are provided to demonstrate\nthe computational advantages of our method.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 01:09:40 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2013 05:55:21 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2013 19:19:58 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Shang", "Zuofeng", ""], ["Li", "Ping", ""]]}, {"id": "1307.0170", "submitter": "Toshiya Hoshikawa", "authors": "Toshiya Hoshikawa", "title": "Mixture regression for observational data, with application to\n  functional regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regression analysis, suppose we suspect that there are several\nheterogeneous groups in the population that a sample represents. Mixture\nregression models have been applied to address such problems. By modeling the\nconditional distribution of the response given the covariate as a mixture, the\nsample can be clustered into groups and the individual regression models for\nthe groups can be estimated simultaneously. This approach treats the covariate\nas deterministic so that the covariate carries no information as to which group\nthe subject is likely to belong to. Although this assumption may be reasonable\nin experiments where the covariate is completely determined by the\nexperimenter, in observational data the covariate may behave differently across\nthe groups. Thus the model should also incorporate the heterogeneity of the\ncovariate, which allows us to estimate the membership of the subject from the\ncovariate.\n  In this paper, we consider a mixture regression model where the joint\ndistribution of the response and the covariate is modeled as a mixture. Given a\nnew observation of the covariate, this approach allows us to compute the\nposterior probabilities that the subject belongs to each group. Using these\nposterior probabilities, the prediction of the response can adaptively use the\ncovariate. We introduce an inference procedure for this approach and show its\nproperties concerning estimation and prediction. The model is explored for the\nfunctional covariate as well as the multivariate covariate. We present a\nreal-data example where our approach outperforms the traditional approach,\nusing the well-analyzed Berkeley growth study data.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2013 02:13:17 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Hoshikawa", "Toshiya", ""]]}, {"id": "1307.0238", "submitter": "Joseph Dureau", "authors": "Alexandros Beskos, Joseph Dureau, Konstantinos Kalogeropoulos", "title": "Bayesian Inference for partially observed SDEs Driven by Fractional\n  Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider continuous-time diffusion models driven by fractional Brownian\nmotion. Observations are assumed to possess a non-trivial likelihood given the\nlatent path. Due to the non-Markovianity and high-dimensionality of the latent\npaths, estimating posterior expectations is a computationally challenging\nundertaking. We present a reparameterization framework based on the Davies and\nHarte method for sampling stationary Gaussian processes and use this framework\nto construct a Markov chain Monte Carlo algorithm that allows computationally\nefficient Bayesian inference. The Markov chain Monte Carlo algorithm is based\non a version of hybrid Monte Carlo that delivers increased efficiency when\napplied on the high-dimensional latent variables arising in this context. We\nspecify the methodology on a stochastic volatility model allowing for memory in\nthe volatility increments through a fractional specification. The methodology\nis illustrated on simulated data and on the S&P500/VIX time series and is shown\nto be effective. Contrary to a long range dependence attribute of such models\noften assumed in the literature, with Hurst parameter larger than 1/2, the\nposterior distribution favours values smaller than 1/2, pointing towards medium\nrange dependence.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2013 20:13:54 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2013 13:03:03 GMT"}, {"version": "v3", "created": "Tue, 17 Feb 2015 22:34:14 GMT"}, {"version": "v4", "created": "Fri, 20 Feb 2015 07:28:48 GMT"}, {"version": "v5", "created": "Tue, 24 Mar 2015 18:31:42 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Beskos", "Alexandros", ""], ["Dureau", "Joseph", ""], ["Kalogeropoulos", "Konstantinos", ""]]}, {"id": "1307.0239", "submitter": "Mari Myllym\\\"aki", "authors": "Mari Myllym\\\"aki, Tom\\'as Mrkvicka, Pavel Grabarnik, Henri Seijo, Ute\n  Hahn", "title": "Global envelope tests for spatial processes", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology), 79 (2017): 381-404", "doi": "10.1111/rssb.12172", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Envelope tests are a popular tool in spatial statistics, where they are used\nin goodness-of-fit testing. These tests graphically compare an empirical\nfunction $T(r)$ with its simulated counterparts from the null model. However,\nthe type I error probability $\\alpha$ is conventionally controlled for a fixed\ndistance $r$ only, whereas the functions are inspected on an interval of\ndistances $I$. In this study, we propose two approaches related to Barnard's\nMonte Carlo test for building global envelope tests on $I$:(1) ordering the\nempirical and simulated functions based on their $r$-wise ranks among each\nother, and (2) the construction of envelopes for a deviation test. These new\ntests allow the a priori selection of the global $\\alpha$ and they yield\n$p$-values. We illustrate these tests using simulated and real point pattern\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2013 20:14:48 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 17:55:32 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2015 18:43:58 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2015 08:05:06 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Myllym\u00e4ki", "Mari", ""], ["Mrkvicka", "Tom\u00e1s", ""], ["Grabarnik", "Pavel", ""], ["Seijo", "Henri", ""], ["Hahn", "Ute", ""]]}, {"id": "1307.0252", "submitter": "Eric Bair", "authors": "Eric Bair", "title": "Semi-supervised clustering methods", "comments": "28 pages, 5 figures", "journal-ref": "WIREs Comp Stat, 2013, 5(5): 349-361", "doi": "10.1002/wics.1270", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis methods seek to partition a data set into homogeneous\nsubgroups. It is useful in a wide variety of applications, including document\nprocessing and modern genetics. Conventional clustering methods are\nunsupervised, meaning that there is no outcome variable nor is anything known\nabout the relationship between the observations in the data set. In many\nsituations, however, information about the clusters is available in addition to\nthe values of the features. For example, the cluster labels of some\nobservations may be known, or certain observations may be known to belong to\nthe same cluster. In other cases, one may wish to identify clusters that are\nassociated with a particular outcome variable. This review describes several\nclustering algorithms (known as \"semi-supervised clustering\" methods) that can\nbe applied in these situations. The majority of these methods are modifications\nof the popular k-means clustering method, and several of them will be described\nin detail. A brief description of some other semi-supervised clustering\nalgorithms is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 00:51:07 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Bair", "Eric", ""]]}, {"id": "1307.0326", "submitter": "Liang Li Dr.", "authors": "Liang Li, Wei Dong, Yindong Ji, Lang Tong", "title": "Spectral Clustering on Subspace for Parameter Estimation of Jump Linear\n  Models", "comments": "11 pages with 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The problem of estimating parameters of a deterministic jump or piecewise\nlinear model is considered. A subspace technique referred to as spectral\nclustering on subspace (SCS) algorithm is proposed to estimate a set of linear\nmodel parameters, the model input, and the set of switching epochs. The SCS\nalgorithm exploits a block diagonal structure of the system input subspace,\nwhich partitions the observation space into separate subspaces, each\ncorresponding to one and only one linear submodel. A spectral clustering\ntechnique is used to label the noisy observations for each submodel, which\ngenerates estimates of switching time epoches. A total least squares technique\nis used to estimate model parameters and the model input. It is shown that, in\nthe absence of observation noise, the SCS algorithm provides exact parameter\nidentification. At high signal to noise ratios, SCS attains a clairvoyant\nCram\\'{e}r-Rao bound computed by assuming the labeling of observation samples\nis perfect.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 10:48:39 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Li", "Liang", ""], ["Dong", "Wei", ""], ["Ji", "Yindong", ""], ["Tong", "Lang", ""]]}, {"id": "1307.0522", "submitter": "Saharon Rosset", "authors": "Ehud Aharoni and Saharon Rosset", "title": "Generalized Alpha Investing: Definitions, Optimality Results, and\n  Application to Public Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing prevalence and utility of large, public databases necessitates\nthe development of appropriate methods for controlling false discovery.\nMotivated by this challenge, we discuss the generic problem of testing a\npossibly infinite stream of null hypotheses. In this context, Foster and Stine\n(2008) suggested a novel method named Alpha Investing for controlling a false\ndiscovery measure known as mFDR. We develop a more general procedure for\ncontrolling mFDR, of which Alpha Investing is a special case. We show that in\ncommon, practical situations, the general procedure can be optimized to produce\nan expected reward optimal (ERO) version, which is more powerful than Alpha\nInvesting.\n  We then present the concept of quality preserving databases (QPD), originally\nintroduced in Aharoni et al. (2011), which formalizes efficient public database\nmanagement to simultaneously save costs and control false discovery. We show\nhow one variant of generalized alpha investing can be used to control mFDR in a\nQPD and lead to significant reduction in costs compared to naive approaches for\ncontrolling the Family-Wise Error Rate implemented in Aharoni et al. (2011).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 20:29:22 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Aharoni", "Ehud", ""], ["Rosset", "Saharon", ""]]}, {"id": "1307.0742", "submitter": "F Lau Mr", "authors": "F. Din-Houn Lau and Axel Gandy", "title": "RMCMC: A System for Updating Bayesian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system to update estimates from a sequence of probability distributions is\npresented. The aim of the system is to quickly produce estimates with a\nuser-specified bound on the Monte Carlo error. The estimates are based upon\nweighted samples stored in a database. The stored samples are maintained such\nthat the accuracy of the estimates and quality of the samples is satisfactory.\nThis maintenance involves varying the number of samples in the database and\nupdating their weights. New samples are generated, when required, by a Markov\nchain Monte Carlo algorithm. The system is demonstrated using a football league\nmodel that is used to predict the end of season table. Correctness of the\nestimates and their accuracy is shown in a simulation using a linear Gaussian\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 16:08:50 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 07:52:44 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2013 14:00:09 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2013 18:20:47 GMT"}, {"version": "v5", "created": "Thu, 6 Feb 2014 13:43:33 GMT"}, {"version": "v6", "created": "Tue, 27 May 2014 17:53:15 GMT"}, {"version": "v7", "created": "Tue, 3 Jun 2014 12:05:07 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Lau", "F. Din-Houn", ""], ["Gandy", "Axel", ""]]}, {"id": "1307.1164", "submitter": "Martin Lysy", "authors": "Martin Lysy and Natesh S. Pillai", "title": "Statistical Inference for Stochastic Differential Equations with Memory", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct a framework for doing statistical inference for\ndiscretely observed stochastic differential equations (SDEs) where the driving\nnoise has 'memory'. Classical SDE models for inference assume the driving noise\nto be Brownian motion, or \"white noise\", thus implying a Markov assumption. We\nfocus on the case when the driving noise is a fractional Brownian motion, which\nis a common continuous-time modeling device for capturing long-range memory.\nSince the likelihood is intractable, we proceed via data augmentation, adapting\na familiar discretization and missing data approach developed for the white\nnoise case. In addition to the other SDE parameters, we take the Hurst index to\nbe unknown and estimate it from the data. Posterior sampling is performed via a\nHybrid Monte Carlo algorithm on both the parameters and the missing data\nsimultaneously so as to improve mixing. We point out that, due to the\nlong-range correlations of the driving noise, careful discretization of the\nunderlying SDE is necessary for valid inference. Our approach can be adapted to\nother types of rough-path driving processes such as Gaussian \"colored\" noise.\nThe methodology is used to estimate the evolution of the memory parameter in US\nshort-term interest rates.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 21:24:33 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Lysy", "Martin", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1307.1379", "submitter": "Xiangping Hu", "authors": "Xiangping Hu, Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Multivariate Gaussian Random Fields Using Systems of Stochastic Partial\n  Differential Equations", "comments": "47 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new approach for constructing \\emph{multivariate} Gaussian\nrandom fields (GRFs) using systems of stochastic partial differential equations\n(SPDEs) has been introduced and applied to simulated data and real data. By\nsolving a system of SPDEs, we can construct multivariate GRFs. On the\ntheoretical side, the notorious requirement of non-negative definiteness for\nthe covariance matrix of the GRF is satisfied since the constructed covariance\nmatrices with this approach are automatically symmetric positive definite.\nUsing the approximate stochastic weak solutions to the systems of SPDEs,\nmultivariate GRFs are represented by multivariate Gaussian \\emph{Markov} random\nfields (GMRFs) with sparse precision matrices. Therefore, on the computational\nside, the sparse structures make it possible to use numerical algorithms for\nsparse matrices to do fast sampling from the random fields and statistical\ninference. Therefore, the \\emph{big-n} problem can also be partially resolved\nfor these models. These models out-preform existing multivariate GRF models on\na commonly used real dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 15:40:06 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2013 18:33:24 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Hu", "Xiangping", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1307.1384", "submitter": "Xiangping Hu", "authors": "Xiangping Hu, Finn Lindgren, Daniel Simpson and H{\\aa}vard Rue", "title": "Multivariate Gaussian Random Fields with Oscillating Covariance\n  Functions using Systems of Stochastic Partial Differential Equations", "comments": "40 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new approach for constructing \\emph{multivariate}\nGaussian random fields (GRFs) with oscillating covariance functions through\nsystems of stochastic partial differential equations (SPDEs). We discuss how to\nbuild systems of SPDEs that introduces oscillation characteristics in the\ncovariance functions of the multivariate GRFs. By choosing different\nparametrization of the equations, some GRFs can be made with oscillating\ncovariance functions but other fields can have Mat\\'ern covariance functions or\nclose to Mat\\'ern covariance functions. The multivariate GRFs constructed by\nsolving the systems of SPDEs automatically fulfill the hard requirement of\nnonnegative definiteness for the covariance functions. The approximate weak\nsolutions to the systems of SPDEs are used to represent the multivariate GRFs\nby multivariate Gaussian \\emph{Markov} random fields (GMRFs). Since the\nmultivariate GMRFs have sparse precision matrices (inverse of the covariance\nmatrices), numerical algorithms for sparse matrices can be applied to the\nprecision matrices for sampling and inference. Thus from a computational point\nof view, the \\emph{big-n} problem can be partially solved with these types of\nmodels. Another advantage of the method is that the oscillation in the\ncovariance function can be controlled directly by the parameters in the system\nof SPDEs. We show how to use this proposed approach with simulated data and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 15:53:28 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Hu", "Xiangping", ""], ["Lindgren", "Finn", ""], ["Simpson", "Daniel", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1307.1446", "submitter": "Kushal  Dey", "authors": "Kushal Kumar Dey, Sourabh Bhattacharya", "title": "A Brief Tutorial on Transformation Based Markov Chain Monte Carlo and\n  Optimal Scaling of the Additive Transformation", "comments": "61 pages, 22 figures, will appear in Brazilian Journal of Probability\n  and Statistics, presented at PCM Gold Medal Award presentations 2013 at\n  Indian Statistical Institute, Kolkata", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recently introduced Transformation-based Markov Chain Monte\nCarlo (TMCMC) (Dutta and Bhattacharya (2014)), a methodology that is designed\nto update all the parameters simultaneously using some simple deterministic\ntransformation of a onedimensional random variable drawn from some arbitrary\ndistribution on a relevant support. The additive transformation based TMCMC is\nsimilar in spirit to random walk Metropolis, except the fact that unlike the\nlatter, additive TMCMC uses a single draw from a onedimensional proposal\ndistribution to update the high-dimensional parameter. In this paper, we first\nprovide a brief tutorial on TMCMC, exploring its connections and contrasts with\nvarious available MCMC methods. Then we study the diffusion limits of additive\nTMCMC under various set-ups ranging from the product structure of the target\ndensity to the case where the target is absolutely continuous with respect to a\nGaussian measure; we also consider the additive TMCMC within Gibbs approach for\nall the above set-ups. These investigations lead to appropriate scaling of the\none-dimensional proposal density. We also show that the optimal acceptance rate\nof additive TMCMC is 0.439 under all the aforementioned set-ups, in contrast\nwith the well-established 0.234 acceptance rate associated with optimal random\nwalk Metropolis algorithms under the same set-ups. We also elucidate the\nramifications of our results and clear advantages of additive TMCMC over random\nwalk Metropolis with ample simulation studies and Bayesian analysis of a real,\nspatial dataset with which 160 unknowns are associated.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 18:40:02 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2013 15:24:30 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2015 05:01:14 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2015 18:24:32 GMT"}, {"version": "v5", "created": "Thu, 27 Oct 2016 18:25:59 GMT"}, {"version": "v6", "created": "Mon, 23 Jan 2017 14:43:42 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Dey", "Kushal Kumar", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1307.1493", "submitter": "Stefan Wager", "authors": "Stefan Wager, Sida Wang, and Percy Liang", "title": "Dropout Training as Adaptive Regularization", "comments": "11 pages. Advances in Neural Information Processing Systems (NIPS),\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout and other feature noising schemes control overfitting by artificially\ncorrupting the training data. For generalized linear models, dropout performs a\nform of adaptive regularization. Using this viewpoint, we show that the dropout\nregularizer is first-order equivalent to an L2 regularizer applied after\nscaling the features by an estimate of the inverse diagonal Fisher information\nmatrix. We also establish a connection to AdaGrad, an online learning\nalgorithm, and find that a close relative of AdaGrad operates by repeatedly\nsolving linear dropout-regularized problems. By casting dropout as\nregularization, we develop a natural semi-supervised algorithm that uses\nunlabeled data to create a better adaptive regularizer. We apply this idea to\ndocument classification tasks, and show that it consistently boosts the\nperformance of dropout training, improving on state-of-the-art results on the\nIMDB reviews dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 21:33:56 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 17:56:35 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Wager", "Stefan", ""], ["Wang", "Sida", ""], ["Liang", "Percy", ""]]}, {"id": "1307.1552", "submitter": "Alessio Farcomeni", "authors": "Alessio Farcomeni, Daria Scacciatelli", "title": "Heterogeneity and behavioral response in continuous time\n  capture-recapture, with application to street cannabis use in Italy", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS672 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2293-2314", "doi": "10.1214/13-AOAS672", "report-no": "IMS-AOAS-AOAS672", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general and flexible capture-recapture model in continuous time.\nOur model incorporates time-heterogeneity, observed and unobserved individual\nheterogeneity, and behavioral response to capture. Behavioral response can\npossibly have a delayed onset and a finite-time memory. Estimation of the\npopulation size is based on the conditional likelihood after use of the EM\nalgorithm. We develop an application to the estimation of the number of adult\ncannabinoid users in Italy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 09:05:26 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 13:50:19 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Farcomeni", "Alessio", ""], ["Scacciatelli", "Daria", ""]]}, {"id": "1307.1748", "submitter": "Tsung-I Lin", "authors": "Tsung-I Lin, Geoffrey J. McLachlan and Sharon X. Lee", "title": "Extending mixtures of factor models using the restricted multivariate\n  skew-normal distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of factor analyzers (MFA) model provides a powerful tool for\nanalyzing high-dimensional data as it can reduce the number of free parameters\nthrough its factor-analytic representation of the component covariance\nmatrices. This paper extends the MFA model to incorporate a restricted version\nof the multivariate skew-normal distribution to model the distribution of the\nlatent component factors, called mixtures of skew-normal factor analyzers\n(MSNFA). The proposed MSNFA model allows us to relax the need for the normality\nassumption for the latent factors in order to accommodate skewness in the\nobserved data. The MSNFA model thus provides an approach to model-based density\nestimation and clustering of high-dimensional data exhibiting asymmetric\ncharacteristics. A computationally feasible ECM algorithm is developed for\ncomputing the maximum likelihood estimates of the parameters. Model selection\ncan be made on the basis of three commonly used information-based criteria. The\npotential of the proposed methodology is exemplified through applications to\ntwo real examples, and the results are compared with those obtained from\nfitting the MFA model.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 04:51:34 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Lin", "Tsung-I", ""], ["McLachlan", "Geoffrey J.", ""], ["Lee", "Sharon X.", ""]]}, {"id": "1307.1864", "submitter": "Mingjun Yang", "authors": "Mingjun Yang, Lijiang Yang, Yiqin Gao, Hao Hu", "title": "Combine Umbrella Sampling with Integrated Tempering Method for Efficient\n  and Accurate Calculation of Free Energy Changes of Complex Energy Surface", "comments": "28 pages and 6 figures", "journal-ref": null, "doi": "10.1063/1.4887340", "report-no": null, "categories": "stat.ME q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Umbrella sampling is an efficient method for the calculation of free energy\nchanges of a system along well-defined reaction coordinates. However, when\nmultiple parallel channels along the reaction coordinate or hidden barriers in\ndirections perpendicular to the reaction coordinate exist, it is difficult for\nconventional umbrella sampling methods to generate sufficient sampling within\nlimited simulation time. Here we propose an efficient approach to combine\numbrella sampling with the integrated tempering sampling method. The umbrella\nsampling method is applied to conformational degrees of freedom which possess\nsignificant barriers and are chemically more relevant. The integrated tempering\nsampling method is employed to facilitate the sampling of other degrees of\nfreedom in which statistically non-negligible barriers may exist. The combined\nmethod is applied to two model systems and show significantly improved sampling\nefficiencies as compared to standalone conventional umbrella sampling or\nintegrated tempering sampling approaches. Therefore, the combined approach will\nbecome a very efficient method in the simulation of biomolecular processes\nwhich often involve sampling of complex rugged energy landscapes.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2013 13:49:21 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Yang", "Mingjun", ""], ["Yang", "Lijiang", ""], ["Gao", "Yiqin", ""], ["Hu", "Hao", ""]]}, {"id": "1307.1996", "submitter": "Jean-Marc F\\'edou", "authors": "Jean-Marc F\\'edou and Maria Jo\\~ao Rendas", "title": "Equitable $(d,m)$-edge designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses design of experiments for classifying the input factors\nof a multi-variate function into negligible, linear and other\n(non-linear/interaction) factors. We give constructive procedures for\ncompleting the definition of the clustered designs proposed Morris 1991, that\nbecome defined for arbitrary number of input factors and desired clusters'\nmultiplicity. Our work is based on a representation of subgraphs of the\nhyper-cube by polynomials that allows the formal verification of the designs'\nproperties. Ability to generate these designs in a systematic manner opens new\nperspectives for the characterisation of the behaviour of the function's\nderivatives over the input space that may offer increased discrimination.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 09:23:56 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["F\u00e9dou", "Jean-Marc", ""], ["Rendas", "Maria Jo\u00e3o", ""]]}, {"id": "1307.2614", "submitter": "Djalel Eddine Meskaldji", "authors": "Djalel Eddine Meskaldji and Jean-Philippe Thiran and Stephan\n  Morgenthaler", "title": "Optimality in multiple comparison procedures", "comments": "arXiv admin note: text overlap with arXiv:1112.4519", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When many (m) null hypotheses are tested with a single dataset, the control\nof the number of false rejections is often the principal consideration. Two\npopular controlling rates are the probability of making at least one false\ndiscovery (FWER) and the expected fraction of false discoveries among all\nrejections (FDR). Scaled multiple comparison error rates form a new family that\nbridges the gap between these two extremes. For example, the Scaled Expected\nValue (SEV) limits the number of false positives relative to an arbitrary\nincreasing function of the number of rejections, that is, E(FP/s(R)). We\ndiscuss the problem of how to choose in practice which procedure to use, with\nelements of an optimality theory, by considering the number of false rejections\nFP separately from the number of correct rejections TP. Using this framework we\nwill show how to choose an element in the new family mentioned above.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 22:24:19 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Meskaldji", "Djalel Eddine", ""], ["Thiran", "Jean-Philippe", ""], ["Morgenthaler", "Stephan", ""]]}, {"id": "1307.2668", "submitter": "Heng Lian", "authors": "Yuao Hu and Kaifeng Zhao and Heng Lian", "title": "Bayesian Quantile Regression for Partially Linear Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop a semiparametric Bayesian estimation and model\nselection approach for partially linear additive models in conditional quantile\nregression. The asymmetric Laplace distribution provides a mechanism for\nBayesian inferences of quantile regression models based on the check loss. The\nadvantage of this new method is that nonlinear, linear and zero function\ncomponents can be separated automatically and simultaneously during model\nfitting without the need of pre-specification or parameter tuning. This is\nachieved by spike-and-slab priors using two sets of indicator variables. For\nposterior inferences, we design an effective partially collapsed Gibbs sampler.\nSimulation studies are used to illustrate our algorithm. The proposed approach\nis further illustrated by applications to two real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 04:31:06 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Hu", "Yuao", ""], ["Zhao", "Kaifeng", ""], ["Lian", "Heng", ""]]}, {"id": "1307.2822", "submitter": "Antonio Canale", "authors": "Antonio Canale and David B. Dunson", "title": "Nonparametric Bayes modeling of count processes", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/ast037", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data on count processes arise in a variety of applications, including\nlongitudinal, spatial and imaging studies measuring count responses. The\nliterature on statistical models for dependent count data is dominated by\nmodels built from hierarchical Poisson components. The Poisson assumption is\nnot warranted in many applications, and hierarchical Poisson models make\nrestrictive assumptions about over-dispersion in marginal distributions. This\narticle proposes a class of nonparametric Bayes count process models, which are\nconstructed through rounding real-valued underlying processes. The proposed\nclass of models accommodates applications in which one observes separate\ncount-valued functional data for each subject under study. Theoretical results\non large support and posterior consistency are established, and computational\nalgorithms are developed using Markov chain Monte Carlo. The methods are\nevaluated via simulation studies and illustrated through application to\nlongitudinal tumor counts and asthma inhaler usage.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 15:14:41 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Canale", "Antonio", ""], ["Dunson", "David B.", ""]]}, {"id": "1307.2869", "submitter": "Gail Potter", "authors": "Gail E. Potter, Timo Smieszek, Kerstin Sailer", "title": "Modelling workplace contact networks: the effects of organizational\n  structure, architecture, and reporting errors on epidemic predictions", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face-to-face social contacts are potentially important transmission routes\nfor acute respiratory infections, and understanding the contact network can\nimprove our ability to predict, contain, and control epidemics. Although\nworkplaces are important settings for infectious disease transmission, few\nstudies have collected workplace contact data and estimated workplace contact\nnetworks. We use contact diaries, architectural distance measures, and\ninstitutional structures to estimate social contact networks within a Swiss\nresearch institute. Some contact reports were inconsistent, indicating\nreporting errors. We adjust for this with a latent variable model, jointly\nestimating the true (unobserved) network of contacts and duration-specific\nreporting probabilities. We find that contact probability decreases with\ndistance, and research group membership, role, and shared projects are strongly\npredictive of contact patterns. Estimated reporting probabilities were low only\nfor 0-5 minute contacts. Adjusting for reporting error changed the estimate of\nthe duration distribution, but did not change the estimates of covariate\neffects and had little effect on epidemic predictions. Our epidemic simulation\nstudy indicates that inclusion of network structure based on architectural and\norganizational structure data can improve the accuracy of epidemic forecasting\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 18:17:22 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 17:58:50 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 07:01:48 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Potter", "Gail E.", ""], ["Smieszek", "Timo", ""], ["Sailer", "Kerstin", ""]]}, {"id": "1307.3000", "submitter": "Thierry Huillet", "authors": "Thierry Huillet (LPTM), Servet Martinez", "title": "Occupancy distributions arising in sampling from Gibbs-Poisson abundance\n  models", "comments": "to appear in Journal of Statistical Physics", "journal-ref": null, "doi": "10.1007/s10955-013-0865-y", "report-no": null, "categories": "stat.ME cond-mat.stat-mech q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number $n$ of unseen species from a $k-$sample displaying only\n$p\\leq k$ distinct sampled species has received attention for long. It requires\na model of species abundance together with a sampling model. We start with a\ndiscrete model of iid stochastic species abundances, each with Gibbs-Poisson\ndistribution. A $k-$sample drawn from the $n-$species abundances vector is the\none obtained while conditioning it on summing to $k$% . We discuss the sampling\nformulae (species occupancy distributions, frequency of frequencies) in this\ncontext. We then develop some aspects of the estimation of $n$ problem from the\nsize $k$ of the sample and the observed value of $P_{n,k}$, the number of\ndistinct sampled species. It is shown that it always makes sense to study these\noccupancy problems from a Gibbs-Poisson abundance model in the context of a\npopulation with infinitely many species. From this extension, a parameter\n$\\gamma $ naturally appears, which is a measure of richness or diversity of\nspecies. We rederive the sampling formulae for a population with infinitely\nmany species, together with the distribution of the number $P_{k}$ of distinct\nsampled species. We investigate the estimation of $\\gamma $ problem from the\nsample size $k$ and the observed value of $P_{k}$. We then exhibit a large\nspecial class of Gibbs-Poisson distributions having the property that sampling\nfrom a discrete abundance model may equivalently be viewed as a sampling\nproblem from a random partition of unity, now in the continuum. When $n$ is\nfinite, this partition may be built upon normalizing $% n$ infinitely divisible\niid positive random variables by its partial sum. It is shown that the sampling\nprocess in the continuum should generically be biased on the total length\nappearing in the latter normalization. A construction with size-biased sampling\nfrom the ranked normalized jumps of a subordinator is also supplied, would the\nproblem under study present infinitely many species. We illustrate our point of\nview with many examples, some of which being new ones.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 07:40:40 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 06:18:43 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Huillet", "Thierry", "", "LPTM"], ["Martinez", "Servet", ""]]}, {"id": "1307.3214", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko and Grigory Sokolov and Wenyu Du", "title": "An Accurate Method for Determining the Pre-Change Run-Length\n  Distribution of the Generalized Shiryaev--Roberts Detection Procedure", "comments": "24 pages, 5 figures, 5 tables, accepted for publication in Sequential\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change-of-measure is a powerful technique used across statistics, probability\nand analysis. Particularly known as Wald's likelihood ratio identity, the\ntechnique enabled the proof of a number of exact and asymptotic optimality\nresults pertaining to the problem of quickest change-point detection. Within\nthe latter problem's context we apply the technique to develop a numerical\nmethod to compute the Generalized Shiryaev--Roberts (GSR) detection procedure's\npre-change Run-Length distribution. Specifically, the method is based on the\nintegral-equations approach and uses the collocation framework with the basis\nfunctions chosen so as to exploit a certain change-of-measure identity and a\nspecific martingale property of the GSR procedure's detection statistic. As a\nresult, the method's accuracy and robustness improve substantially, even though\nthe method's theoretical rate of convergence is shown to be merely quadratic. A\ntight upper bound on the method's error is supplied as well. The method is not\nrestricted to a particular data distribution or to a specific value of the GSR\ndetection statistic's \"headstart\". To conclude, we offer a case study to\ndemonstrate the proposed method at work, drawing particular attention to the\nmethod's accuracy and its robustness with respect to three factors: (a)\npartition size, (b) change magnitude, and (c) Average Run Length (ARL) to false\nalarm level. Specifically, assuming independent standard Gaussian observations\nundergoing a surge in the mean, we employ the method to study the GSR\nprocedure's Run-Length's pre-change distribution, its average (i.e., the usual\nARL to false alarm) and standard deviation. As expected from the theoretical\nanalysis, the method's high accuracy and robustness with respect to the\nforegoing three factors are confirmed experimentally. We also comment on\nextending the method to handle other performance measures and other procedures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 18:58:45 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 22:08:10 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""], ["Du", "Wenyu", ""]]}, {"id": "1307.3227", "submitter": "Aurelie Lozano C", "authors": "Aur\\'elie C. Lozano and Nicolai Meinshausen", "title": "Minimum Distance Estimation for Robust High-Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a minimum distance estimation method for robust regression in\nsparse high-dimensional settings. The traditional likelihood-based estimators\nlack resilience against outliers, a critical issue when dealing with\nhigh-dimensional noisy data. Our method, Minimum Distance Lasso (MD-Lasso),\ncombines minimum distance functionals, customarily used in nonparametric\nestimation for their robustness, with l1-regularization for high-dimensional\nregression. The geometry of MD-Lasso is key to its consistency and robustness.\nThe estimator is governed by a scaling parameter that caps the influence of\noutliers: the loss per observation is locally convex and close to quadratic for\nsmall squared residuals, and flattens for squared residuals larger than the\nscaling parameter. As the parameter approaches infinity, the estimator becomes\nequivalent to least-squares Lasso. MD-Lasso enjoys fast convergence rates under\nmild conditions on the model error distribution, which hold for any of the\nsolutions in a convexity region around the true parameter and in certain cases\nfor every solution. Remarkably, a first-order optimization method is able to\nproduce iterates very close to the consistent solutions, with geometric\nconvergence and regardless of the initialization. A connection is established\nwith re-weighted least-squares that intuitively explains MD-Lasso robustness.\nThe merits of our method are demonstrated through simulation and eQTL data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 19:41:00 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Lozano", "Aur\u00e9lie C.", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1307.3282", "submitter": "Anna Klimova", "authors": "Anna Klimova and Tamas Rudas", "title": "Iterative Scaling in Curved Exponential Families", "comments": "The paper has one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a generalized iterative proportional fitting procedure\nwhich can be used for maximum likelihood estimation in a special class of the\ngeneral log-linear model. The models in this class, called relational, apply to\nmultivariate discrete sample spaces which do not necessarily have a Cartesian\nproduct structure and may not contain an overall effect. When applied to the\ncell probabilities, the models without the overall effect are curved\nexponential families and the values of the sufficient statistics are reproduced\nby the MLE only up to a constant of proportionality. The paper shows that\nIterative Proportional Fitting, Generalized Iterative Scaling and Improved\nIterative Scaling, fail to work for such models. The algorithm proposed here is\nbased on iterated Bregman projections. As a by-product, estimates of the\nmultiplicative parameters are also obtained.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 22:07:17 GMT"}, {"version": "v2", "created": "Sat, 29 Mar 2014 16:39:34 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Klimova", "Anna", ""], ["Rudas", "Tamas", ""]]}, {"id": "1307.3286", "submitter": "Djalel Eddine Meskaldji", "authors": "Djalel Eddine Meskaldji and Patric Hagmann and Jean-Philippe Thiran\n  and Stephan Morgenthaler", "title": "Two step multiple comparisons procedures for positively dependent data\n  with application to detecting differences in human brain network topologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing positively dependent multiple hypotheses\nassuming that a prior information about the dependence structure is available.\nWe propose two-step multiple comparisons procedures that exploit the prior\ninformation of the dependence structure, without relying on strong assumptions.\nIn the first step, we group the tests into subsets where tests are supposed to\nbe positively dependent and in each of which we compute the standardized mean\nof the test scores. Given the subset mean scores or equivalently the subsets\np-values, we apply a first screening at a predefined threshold, which results\nin two types of subsets. Based on this typing, the original single test\np-values are modified such that they can be used in conjunction with any\nmultiple comparison procedure. We show by means of different simulation that\npower is gained with the proposed two-step methods, and compare it with\ntraditional multiple comparison procedures. As an illustration, our method is\napplied on real data comparing topological differences between two groups of\nhuman brain networks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 22:21:11 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Meskaldji", "Djalel Eddine", ""], ["Hagmann", "Patric", ""], ["Thiran", "Jean-Philippe", ""], ["Morgenthaler", "Stephan", ""]]}, {"id": "1307.3490", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni and J. Fraser Forbes", "title": "On-line Bayesian parameter estimation in general non-linear state-space\n  models: A tutorial and new results", "comments": "A condensed version of this article has been published in: Tulsyan,\n  A., Huang, B., Gopaluni, R.B., Forbes, J.F. \"On simultaneous on-line state\n  and parameter estimation in non-linear state-space models\". Journal of\n  Process Control, vol 23, no. 4, 2013", "journal-ref": "Journal of Process Control, vol 23, no. 4, 2013", "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-line estimation plays an important role in process control and monitoring.\nObtaining a theoretical solution to the simultaneous state-parameter estimation\nproblem for non-linear stochastic systems involves solving complex\nmulti-dimensional integrals that are not amenable to analytical solution. While\nbasic sequential Monte-Carlo (SMC) or particle filtering (PF) algorithms for\nsimultaneous estimation exist, it is well recognized that there is a need for\nmaking these on-line algorithms non-degenerate, fast and applicable to\nprocesses with missing measurements. To overcome the deficiencies in\ntraditional algorithms, this work proposes a Bayesian approach to on-line state\nand parameter estimation. Its extension to handle missing data in real-time is\nalso provided. The simultaneous estimation is performed by filtering an\nextended vector of states and parameters using an adaptive\nsequential-importance-resampling (SIR) filter with a kernel density estimation\nmethod. The approach uses an on-line optimization algorithm based on\nKullback-Leibler (KL) divergence to allow adaptation of the SIR filter for\ncombined state-parameter estimation. An optimal tuning rule to control the\nwidth of the kernel and the variance of the artificial noise added to the\nparameters is also proposed. The approach is illustrated through numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 15:30:38 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.3495", "submitter": "James Scott", "authors": "James G. Scott, Ryan C. Kelly, Matthew A. Smith, Pengcheng Zhou, and\n  Robert E. Kass", "title": "False discovery rate regression: an application to neural synchrony\n  detection in primary visual cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches for multiple testing begin with the assumption that all tests\nin a given study should be combined into a global false-discovery-rate\nanalysis. But this may be inappropriate for many of today's large-scale\nscreening problems, where auxiliary information about each test is often\navailable, and where a combined analysis can lead to poorly calibrated error\nrates within different subsets of the experiment. To address this issue, we\nintroduce an approach called false-discovery-rate regression that directly uses\nthis auxiliary information to inform the outcome of each test. The method can\nbe motivated by a two-groups model in which covariates are allowed to influence\nthe local false discovery rate, or equivalently, the posterior probability that\na given observation is a signal. This poses many subtle issues at the interface\nbetween inference and computation, and we investigate several variations of the\noverall approach. Simulation evidence suggests that: (1) when covariate effects\nare present, FDR regression improves power for a fixed false-discovery rate;\nand (2) when covariate effects are absent, the method is robust, in the sense\nthat it does not lead to inflated error rates. We apply the method to neural\nrecordings from primary visual cortex. The goal is to detect pairs of neurons\nthat exhibit fine-time-scale interactions, in the sense that they fire together\nmore often than expected due to chance. Our method detects roughly 50% more\nsynchronous pairs versus a standard FDR-controlling analysis. The companion R\npackage FDRreg implements all methods described in the paper.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 15:55:33 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2013 03:30:47 GMT"}, {"version": "v3", "created": "Sun, 8 Jun 2014 22:27:29 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Scott", "James G.", ""], ["Kelly", "Ryan C.", ""], ["Smith", "Matthew A.", ""], ["Zhou", "Pengcheng", ""], ["Kass", "Robert E.", ""]]}, {"id": "1307.3598", "submitter": "Paul McNicholas", "authors": "Irene Vrbik and Paul D. McNicholas", "title": "Fractionally-Supervised Classification", "comments": null, "journal-ref": null, "doi": "10.1007/s00357-015-9188-9", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, there are three species of classification: unsupervised,\nsupervised, and semi-supervised. Supervised and semi-supervised classification\ndiffer by whether or not weight is given to unlabelled observations in the\nclassification procedure. In unsupervised classification, or clustering, all\nobservations are unlabeled and hence full weight is given to unlabelled\nobservations. When some observations are unlabelled, it can be very difficult\nto \\textit{a~priori} choose the optimal level of supervision, and the\nconsequences of a sub-optimal choice can be non-trivial. A flexible\nfractionally-supervised approach to classification is introduced, where any\nlevel of supervision --- ranging from unsupervised to supervised --- can be\nattained. Our approach uses a weighted likelihood, wherein weights control the\nrelative role that labelled and unlabelled data have in building a classifier.\nA comparison between our approach and the traditional species is presented\nusing simulated and real data. Gaussian mixture models are used as a vehicle to\nillustrate our fractionally-supervised classification approach; however, it is\nbroadly applicable and variations on the postulated model can be easily made.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 00:41:37 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2013 16:15:30 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 05:29:13 GMT"}, {"version": "v4", "created": "Sun, 13 Sep 2015 18:04:19 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2015 18:16:39 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Vrbik", "Irene", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1307.3646", "submitter": "Tu Xu", "authors": "A. S. Hedayat, Junhui Wang and Tu Xu", "title": "On Minimum Clinically Important Difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical trials, minimum clinically important difference (MCID) has\nattracted increasing interest as an important supportive clinical and\nstatistical inference tool. Many estimation methods have been developed based\non various intuitions, while little theoretical justification has been\nestablished. This paper proposes a new estimation framework of MCID using both\ndiagnostic measurements and patient-reported outcomes (PROs). It first provides\na precise definition of population-based MCID so that estimating such a MCID\ncan be formulated as a large margin classification problem. The framework is\nthen extended to personalized MCID to allow individualized thresholding value\nfor patients whose clinical profiles may affect their PRO responses. More\nimportantly, we show that the proposed estimation framework is asymptotically\nconsistent, and a finite-sample upper bound is established for its prediction\naccuracy compared against the ideal MCID. The advantage of our proposed method\nis also demonstrated in a variety of simulated experiments as well as\napplications to two benchmark datasets and two phase-3 clinical trials.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 13:44:50 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 15:55:30 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Hedayat", "A. S.", ""], ["Wang", "Junhui", ""], ["Xu", "Tu", ""]]}, {"id": "1307.3719", "submitter": "Florian Maire", "authors": "Florian Maire, Randal Douc, Jimmy Olsson", "title": "Comparison of asymptotic variances of inhomogeneous Markov chains with\n  application to Markov chain Monte Carlo methods", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1209 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1483-1510", "doi": "10.1214/14-AOS1209", "report-no": "IMS-AOS-AOS1209", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the asymptotic variance of sample path averages for\ninhomogeneous Markov chains that evolve alternatingly according to two\ndifferent $\\pi$-reversible Markov transition kernels $P$ and $Q$. More\nspecifically, our main result allows us to compare directly the asymptotic\nvariances of two inhomogeneous Markov chains associated with different kernels\n$P_i$ and $Q_i$, $i\\in\\{0,1\\}$, as soon as the kernels of each pair $(P_0,P_1)$\nand $(Q_0,Q_1)$ can be ordered in the sense of lag-one autocovariance. As an\nimportant application, we use this result for comparing different\ndata-augmentation-type Metropolis-Hastings algorithms. In particular, we\ncompare some pseudo-marginal algorithms and propose a novel exact algorithm,\nreferred to as the random refreshment algorithm, which is more efficient, in\nterms of asymptotic variance, than the Grouped Independence Metropolis-Hastings\nalgorithm and has a computational complexity that does not exceed that of the\nMonte Carlo Within Metropolis algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 10:22:35 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 00:46:58 GMT"}, {"version": "v3", "created": "Fri, 21 Mar 2014 17:22:59 GMT"}, {"version": "v4", "created": "Thu, 14 Aug 2014 06:54:15 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Maire", "Florian", ""], ["Douc", "Randal", ""], ["Olsson", "Jimmy", ""]]}, {"id": "1307.3925", "submitter": "Saad AlMalki", "authors": "Saad J. Almalki", "title": "A reduced new modified Weibull distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a reduced version of the new modified Weibull (NMW)\ndistribution due to Almalki and Yuan \\cite{meNMW} in order to avoid some\nestimation problems. The number of parameters in the NMW distribution is five.\nThe number of parameters in the reduced version is three. We study mathematical\nproperties as well as maximum likelihood estimation of the reduced version.\nFour real data sets (two of them complete and the other two censored) are used\nto compare the flexibility of the reduced version versus the NMW distribution.\nIt is shown that the reduced version has the same desirable properties of the\nNMW distribution in spite of having two less parameters. The NMW distribution\ndid not provide a significantly better fit than the reduced version for any of\nthe four data sets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 13:13:45 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Almalki", "Saad J.", ""]]}, {"id": "1307.4394", "submitter": "Sebastian D\\\"ohler", "authors": "Sebastian D\\\"ohler", "title": "A sufficient criterion for control of generalised error rates in\n  multiple testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the work of Romano and Shaikh (2006) and Lehmann and Romano (2005)\nwe give a sufficient criterion for controlling generalised error rates for\narbitrarily dependent p-values. This criterion is formulated in terms of\nmatrices associated with the corresponding error rates and thus it is possible\nto view the corresponding critical constants as solutions of sets of certain\nlinear inequalities. This property can in some cases be used to improve the\npower of existing procedures by finding optimal solutions to an associated\nlinear programming problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 19:59:41 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["D\u00f6hler", "Sebastian", ""]]}, {"id": "1307.4625", "submitter": "Gyorgy Terdik DR", "authors": "E. Igl\\'oi and Gy. Terdik", "title": "When the bispectrum is real-valued", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let {X(t)} be a stationary time series with a.e. positive spectrum. Two\nconsequences of that the bispectrum of {X(t)} is real-valued but nonzero: 1) if\n{X(t)} is also linear, then it is reversible; 2) {X(t),} can not be causal\nlinear. A corollary of the first statement: if {X(t)} is linear, and the\nskewness of X(0) is nonzero, then third order reversibility implies\nreversibility. In this paper the notion of bispectrum is of a broader scope.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 13:42:23 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Igl\u00f3i", "E.", ""], ["Terdik", "Gy.", ""]]}, {"id": "1307.4626", "submitter": "Chao Wang", "authors": "Chao Wang, Heng Liu, Jian-Feng Yao, Richard A. Davis, Wai Keung Li", "title": "Self-excited Threshold Poisson Autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies theory and inference of an observation-driven model for\ntime series of counts. It is assumed that the observations follow a Poisson\ndistribution conditioned on an accompanying intensity process, which is\nequipped with a two-regime structure according to the magnitude of the lagged\nobservations. The model remedies one of the drawbacks of the Poisson\nautoregression model by allowing possibly negative correlation in the\nobservations. Classical Markov chain theory and Lyapunov's method are utilized\nto derive the conditions under which the process has a unique invariant\nprobability measure and to show a strong law of large numbers of the intensity\nprocess. Moreover the asymptotic theory of the maximum likelihood estimates of\nthe parameters is established. A simulation study and a real data application\nare considered, where the model is applied to the number of major earthquakes\nin the world.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 13:43:00 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Wang", "Chao", ""], ["Liu", "Heng", ""], ["Yao", "Jian-Feng", ""], ["Davis", "Richard A.", ""], ["Li", "Wai Keung", ""]]}, {"id": "1307.4765", "submitter": "Maxwell Grazier G'Sell", "authors": "Max Grazier G'Sell, Jonathan Taylor, Robert Tibshirani", "title": "Adaptive testing for the graphical lasso", "comments": "33 pages, 8 figures. Submitted to Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider tests of significance in the setting of the graphical lasso for\ninverse covariance matrix estimation. We propose a simple test statistic based\non a subsequence of the knots in the graphical lasso path. We show that this\nstatistic has an exponential asymptotic null distribution, under the null\nhypothesis that the model contains the true connected components.\n  Though the null distribution is asymptotic, we show through simulation that\nit provides a close approximation to the true distribution at reasonable sample\nsizes. Thus the test provides a simple, tractable test for the significance of\nnew edges as they are introduced into the model. Finally, we show connections\nbetween our results and other results for regularized regression, as well as\nextensions of our results to other correlation matrix based methods like\nsingle-linkage clustering.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 20:00:57 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2013 23:27:02 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["G'Sell", "Max Grazier", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1307.4834", "submitter": "Vakili Kaveh", "authors": "Kaveh Vakili and Eric Schmitt", "title": "Finding Regression Outliers With FastRCS", "comments": "23 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1301.2053", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Residual Congruent Subset (RCS) is a new method for finding outliers in\nthe linear regression setting. Like many other outlier detection procedures,\nRCS searches for a subset which minimizes a criterion. The difference is that\nthe new criterion was designed to be insensitive to the outliers. RCS is\nsupported by FastRCS, a fast regression and affine equivariant algorithm which\nwe also detail. Both an extensive simulation study and two real data\napplications show that FastRCS performs better than its competitors.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 05:24:34 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2013 07:26:56 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 00:09:19 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Vakili", "Kaveh", ""], ["Schmitt", "Eric", ""]]}, {"id": "1307.4860", "submitter": "Hazhir Homei", "authors": "Hazhir Homei", "title": "Directly Specifying the Power Semicircle Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new proof for a newly proved conjecture of Soltani and Roozegar (2012) is\nprovided; our proof does not make any use of the Stieltjes transform unlike the\nproof of Roozegar and Soltani (2013), and the distribution of power semicircle\nhas been directly specified, contrary to the authors' claim in (Roozegar and\nSoltani 2013).\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 08:16:15 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Homei", "Hazhir", ""]]}, {"id": "1307.4956", "submitter": "Steffen Lauritzen", "authors": "Therese Graversen and Steffen Lauritzen", "title": "Computational aspects of DNA mixture analysis", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-014-9451-7", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of DNA mixtures is known to pose computational\nchallenges due to the enormous state space of possible DNA profiles. We propose\na Bayesian network representation for genotypes, allowing computations to be\nperformed locally involving only a few alleles at each step. In addition, we\ndescribe a general method for computing the expectation of a product of\ndiscrete random variables using auxiliary variables and probability propagation\nin a Bayesian network, which in combination with the genotype network allows\nefficient computation of the likelihood function and various other quantities\nrelevant to the inference. Lastly, we introduce a set of diagnostic tools for\nassessing the adequacy of the model for describing a particular dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 14:19:12 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Graversen", "Therese", ""], ["Lauritzen", "Steffen", ""]]}, {"id": "1307.5231", "submitter": "Jim Griffin", "authors": "Jim E. Griffin and Philip J. Brown", "title": "Hierarchical sparsity priors for regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the increasingly important area of sparse regression problems\nwhere there are many variables and the effects of a large subset of these are\nnegligible. This paper describes the construction of hierarchical prior\ndistributions when the effects are considered related. These priors allow\ndependence between the regression coefficients and encourage related shrinkage\ntowards zero of different regression coefficients. The properties of these\npriors are discussed and applications to linear models with interactions and\ngeneralized additive models are used as illustrations. Ideas of heredity\nrelating different levels of interaction are encompassed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 14:32:46 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 11:01:16 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Griffin", "Jim E.", ""], ["Brown", "Philip J.", ""]]}, {"id": "1307.5339", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Daniela Witten, and Ali Shojaie", "title": "The Cluster Graphical Lasso for improved estimation of Gaussian\n  graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of estimating a Gaussian graphical model in the\nhigh-dimensional setting. The graphical lasso, which involves maximizing the\nGaussian log likelihood subject to an l1 penalty, is a well-studied approach\nfor this task. We begin by introducing a surprising connection between the\ngraphical lasso and hierarchical clustering: the graphical lasso in effect\nperforms a two-step procedure, in which (1) single linkage hierarchical\nclustering is performed on the variables in order to identify connected\ncomponents, and then (2) an l1-penalized log likelihood is maximized on the\nsubset of variables within each connected component. In other words, the\ngraphical lasso determines the connected components of the estimated network\nvia single linkage clustering. Unfortunately, single linkage clustering is\nknown to perform poorly in certain settings. Therefore, we propose the cluster\ngraphical lasso, which involves clustering the features using an alternative to\nsingle linkage clustering, and then performing the graphical lasso on the\nsubset of variables within each cluster. We establish model selection\nconsistency for this technique, and demonstrate its improved performance\nrelative to the graphical lasso in a simulation study, as well as in\napplications to an equities data set, a university webpage data set, and a gene\nexpression data set.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 21:19:11 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Tan", "Kean Ming", ""], ["Witten", "Daniela", ""], ["Shojaie", "Ali", ""]]}, {"id": "1307.5381", "submitter": "Sang-Yun Oh", "authors": "Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam", "title": "A convex pseudo-likelihood framework for high dimensional partial\n  correlation estimation with convergence guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse high dimensional graphical model selection is a topic of much interest\nin modern day statistics. A popular approach is to apply l1-penalties to either\n(1) parametric likelihoods, or, (2) regularized regression/pseudo-likelihoods,\nwith the latter having the distinct advantage that they do not explicitly\nassume Gaussianity. As none of the popular methods proposed for solving\npseudo-likelihood based objective functions have provable convergence\nguarantees, it is not clear if corresponding estimators exist or are even\ncomputable, or if they actually yield correct partial correlation graphs. This\npaper proposes a new pseudo-likelihood based graphical model selection method\nthat aims to overcome some of the shortcomings of current methods, but at the\nsame time retain all their respective strengths. In particular, we introduce a\nnovel framework that leads to a convex formulation of the partial covariance\nregression graph problem, resulting in an objective function comprised of\nquadratic forms. The objective is then optimized via a coordinate-wise\napproach. The specific functional form of the objective function facilitates\nrigorous convergence analysis leading to convergence guarantees; an important\nproperty that cannot be established using standard results, when the dimension\nis larger than the sample size, as is often the case in high dimensional\napplications. These convergence guarantees ensure that estimators are\nwell-defined under very general conditions, and are always computable. In\naddition, the approach yields estimators that have good large sample properties\nand also respect symmetry. Furthermore, application to simulated/real data,\ntiming comparisons and numerical convergence is demonstrated. We also present a\nnovel unifying framework that places all graphical pseudo-likelihood methods as\nspecial cases of a more general formulation, leading to important insights.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 07:01:20 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 05:27:44 GMT"}, {"version": "v3", "created": "Thu, 14 Aug 2014 21:15:28 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Khare", "Kshitij", ""], ["Oh", "Sang-Yun", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1307.5396", "submitter": "Nanny Wermuth", "authors": "Nanny Wermuth and Giovanni M. Marchetti", "title": "Star graphs induce tetrad correlations: for Gaussian as well as for\n  binary variables", "comments": "21 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tetrad correlations were obtained historically for Gaussian distributions\nwhen tasks are designed to measure an ability or attitude so that a single\nunobserved variable may generate the observed, linearly increasing dependences\namong the tasks. We connect such generating processes to a particular type of\ndirected graph, the star graph, and to the notion of traceable regressions.\nTetrad correlation conditions for the existence of a single latent variable are\nderived. These are needed for positive dependences not only in joint Gaussian\nbut also in joint binary distributions. Three applications with binary items\nare given.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 09:14:36 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 16:01:47 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2013 04:49:08 GMT"}, {"version": "v4", "created": "Tue, 11 Mar 2014 15:38:02 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Wermuth", "Nanny", ""], ["Marchetti", "Giovanni M.", ""]]}, {"id": "1307.5558", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Paul D. McNicholas and Ryan P. Browne", "title": "Mixtures of Common Skew-t Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.43", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of common skew-t factor analyzers model is introduced for\nmodel-based clustering of high-dimensional data. By assuming common component\nfactor loadings, this model allows clustering to be performed in the presence\nof a large number of mixture components or when the number of dimensions is too\nlarge to be well-modelled by the mixtures of factor analyzers model or a\nvariant thereof. Furthermore, assuming that the component densities follow a\nskew-t distribution allows robust clustering of skewed data. The alternating\nexpectation-conditional maximization algorithm is employed for parameter\nestimation. We demonstrate excellent clustering performance when our model is\napplied to real and simulated data.This paper marks the first time that skewed\ncommon factors have been used.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 19:18:39 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2013 15:56:34 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2013 21:28:57 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Murray", "Paula M.", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1307.5576", "submitter": "Suyan  Tian", "authors": "Suyan Tian and Mayte Su\\'arez-Fari\\~nas", "title": "Multi-TGDR: a regularization method for multi-class classification in\n  microarray experiments", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0078302", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background\n  With microarray technology becoming mature and popular, the selection and use\nof a small number of relevant genes for accurate classification of samples is a\nhot topic in the circles of biostatistics and bioinformatics. However, most of\nthe developed algorithms lack the ability to handle multiple classes, which\narguably a common application. Here, we propose an extension to an existing\nregularization algorithm called Threshold Gradient Descent Regularization\n(TGDR) to specifically tackle multi-class classification of microarray data.\nWhen there are several microarray experiments addressing the same/similar\nobjectives, one option is to use meta-analysis version of TGDR (Meta-TGDR),\nwhich considers the classification task as combination of classifiers with the\nsame structure/model while allowing the parameters to vary across studies.\nHowever, the original Meta-TGDR extension did not offer a solution to the\nprediction on independent samples. Here, we propose an explicit method to\nestimate the overall coefficients of the biomarkers selected by Meta-TGDR. This\nextension permits broader applicability and allows a comparison between the\npredictive performance of Meta-TGDR and TGDR using an independent testing set.\n  Results\n  Using real-world applications, we demonstrated the proposed multi-TGDR\nframework works well and the number of selected genes is less than the sum of\nall individualized binary TGDRs. Additionally, Meta-TGDR and TGDR on the\nbatch-effect adjusted pooled data approximately provided same results. By\nadding Bagging procedure in each application, the stability and good predictive\nperformance are warranted.\n  Conclusions\n  Compared with Meta-TGDR, TGDR is less computing time intensive, and requires\nno samples of all classes in each study. On the adjusted data, it has\napproximate same predictive performance with Meta-TGDR. Thus, it is highly\nrecommended.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 00:59:59 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Tian", "Suyan", ""], ["Su\u00e1rez-Fari\u00f1as", "Mayte", ""]]}, {"id": "1307.5636", "submitter": "Marloes H. Maathuis", "authors": "Marloes H. Maathuis, Diego Colombo", "title": "A generalized back-door criterion", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1295 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 1060-1088", "doi": "10.1214/14-AOS1295", "report-no": "IMS-AOS-AOS1295", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize Pearl's back-door criterion for directed acyclic graphs (DAGs)\nto more general types of graphs that describe Markov equivalence classes of\nDAGs and/or allow for arbitrarily many hidden variables. We also give easily\ncheckable necessary and sufficient graphical criteria for the existence of a\nset of variables that satisfies our generalized back-door criterion, when\nconsidering a single intervention and a single outcome variable. Moreover, if\nsuch a set exists, we provide an explicit set that fulfills the criterion. We\nillustrate the results in several examples. R-code is available in the\nR-package pcalg.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 09:54:01 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2013 13:13:40 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 07:04:21 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Maathuis", "Marloes H.", ""], ["Colombo", "Diego", ""]]}, {"id": "1307.5698", "submitter": "Yoann Altmann", "authors": "Yoann Altmann and Nicolas Dobigeon and Steve McLaughlin and Jean-Yves\n  Tourneret", "title": "Residual component analysis of hyperspectral images -- Application to\n  joint nonlinear unmixing and nonlinearity detection", "comments": "arXiv admin note: text overlap with arXiv:1304.2499", "journal-ref": null, "doi": "10.1109/TIP.2014.2312616", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a nonlinear mixing model for joint hyperspectral image\nunmixing and nonlinearity detection. The proposed model assumes that the pixel\nreflectances are linear combinations of known pure spectral components\ncorrupted by an additional nonlinear term, affecting the endmembers and\ncontaminated by an additive Gaussian noise. A Markov random field is considered\nfor nonlinearity detection based on the spatial structure of the nonlinear\nterms. The observed image is segmented into regions where nonlinear terms, if\npresent, share similar statistical properties. A Bayesian algorithm is proposed\nto estimate the parameters involved in the model yielding a joint nonlinear\nunmixing and nonlinearity detection algorithm. The performance of the proposed\nstrategy is first evaluated on synthetic data. Simulations conducted with real\ndata show the accuracy of the proposed unmixing and nonlinearity detection\nstrategy for the analysis of hyperspectral images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:35:28 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Altmann", "Yoann", ""], ["Dobigeon", "Nicolas", ""], ["McLaughlin", "Steve", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1307.5759", "submitter": "Moshe Adrian", "authors": "Moshe Adrian, Eric Bradlow, Peter Fader, Blake McShane", "title": "Count Models Based on Weibull Interarrival Times", "comments": "24 pages. Figures excluded. Please see published version in JBES", "journal-ref": "Journal of Business and Economic Statistics (2008), Volume 26, No.\n  3, 369-378", "doi": "10.1198/073500107000000278", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a generalized model for count data based upon an\nassumed Weibull interarrival process that nests the Poisson and negative\nbinomial models as special cases. In addition, we demonstrate that this new\nWeibull count model can model both over and underdispersed count data, allow\ncovariates to be introduced in a straightforward manner through the hazard\nfunction, and be computed in standard software.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 16:13:43 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Adrian", "Moshe", ""], ["Bradlow", "Eric", ""], ["Fader", "Peter", ""], ["McShane", "Blake", ""]]}, {"id": "1307.5806", "submitter": "Kyungchul Song", "authors": "Kyungchul Song", "title": "Semiparametric Models with Single-Index Nuisance Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many semiparametric models, the parameter of interest is identified\nthrough conditional expectations, where the conditioning variable involves a\nsingle-index that is estimated in the first step. Among the examples are sample\nselection models and propensity score matching estimators. When the first-step\nestimator follows cube-root asymptotics, no method of analyzing the asymptotic\nvariance of the second step estimator exists in the literature. This paper\nprovides nontrivial sufficient conditions under which the asymptotic variance\nis not affected by the first step single index estimator regardless of whether\nit is root-n or cube-root consistent. The finding opens a way to simple\ninference procedures in these models. Results from Monte Carlo simulations show\nthat the procedures perform well in finite samples.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 18:20:49 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Song", "Kyungchul", ""]]}, {"id": "1307.5875", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel", "title": "New Confidence Intervals and Bias Comparisons Show that Maximum\n  Likelihood Can Beat Multiple Imputation in Small Samples", "comments": "5 tables", "journal-ref": "Structural Equation Modeling, 23(3): 423-437 (2015)", "doi": "10.1080/10705511.2015.1047931", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing incomplete data, is it better to use multiple imputation (MI)\nor full information maximum likelihood (ML)? In large samples ML is clearly\nbetter, but in small samples ML's usefulness has been limited because ML\ncommonly uses normal test statistics and confidence intervals that require\nlarge samples. We propose small-sample t-based ML confidence intervals that\nhave good coverage and are shorter than t-based confidence intervals under MI.\nWe also show that ML point estimates are less biased and more efficient than MI\npoint estimates in small samples of bivariate normal data. With our new\nconfidence intervals, ML should be preferred over MI, even in small samples,\nwhenever both options are available.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 20:33:05 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2013 21:26:10 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2013 17:17:43 GMT"}, {"version": "v4", "created": "Tue, 20 Aug 2013 22:39:53 GMT"}, {"version": "v5", "created": "Thu, 5 Mar 2015 19:29:21 GMT"}, {"version": "v6", "created": "Fri, 1 May 2015 14:02:51 GMT"}, {"version": "v7", "created": "Thu, 20 Aug 2015 18:12:50 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["von Hippel", "Paul T.", ""]]}, {"id": "1307.5928", "submitter": "Andrew Gelman", "authors": "Andrew Gelman, Jessica Hwang and Aki Vehtari", "title": "Understanding predictive information criteria for Bayesian models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the Akaike, deviance, and Watanabe-Akaike information criteria from\na Bayesian perspective, where the goal is to estimate expected\nout-of-sample-prediction error using a biascorrected adjustment of\nwithin-sample error. We focus on the choices involved in setting up these\nmeasures, and we compare them in three simple examples, one theoretical and two\napplied. The contribution of this review is to put all these information\ncriteria into a Bayesian predictive context and to better understand, through\nsmall examples, how these methods can apply in practice.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 02:42:10 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Gelman", "Andrew", ""], ["Hwang", "Jessica", ""], ["Vehtari", "Aki", ""]]}, {"id": "1307.5929", "submitter": "Andrew Gelman", "authors": "Andrew Gelman and Keith O'Rourke", "title": "Convincing Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textbooks on statistics emphasize care and precision, via concepts such as\nreliability and validity in measurement, random sampling and treatment\nassignment in data collection, and causal identification and bias in\nestimation. But how do researchers decide what to believe and what to trust\nwhen choosing which statistical methods to use? How do they decide the\ncredibility of methods? Statisticians and statistical practitioners seem to\nrely on a sense of anecdotal evidence based on personal experience and on the\nattitudes of trusted colleagues. Authorship, reputation, and past experience\nare thus central to decisions about statistical procedures.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 02:45:44 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Gelman", "Andrew", ""], ["O'Rourke", "Keith", ""]]}, {"id": "1307.5996", "submitter": "Nicolas Dobigeon", "authors": "Qi Wei, Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Bayesian Fusion of Multi-Band Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Bayesian fusion technique for remotely sensed multi-band\nimages is presented. The observed images are related to the high spectral and\nhigh spatial resolution image to be recovered through physical degradations,\ne.g., spatial and spectral blurring and/or subsampling defined by the sensor\ncharacteristics. The fusion problem is formulated within a Bayesian estimation\nframework. An appropriate prior distribution exploiting geometrical\nconsideration is introduced. To compute the Bayesian estimator of the scene of\ninterest from its posterior distribution, a Markov chain Monte Carlo algorithm\nis designed to generate samples asymptotically distributed according to the\ntarget distribution. To efficiently sample from this high-dimension\ndistribution, a Hamiltonian Monte Carlo step is introduced in the Gibbs\nsampling strategy. The efficiency of the proposed fusion method is evaluated\nwith respect to several state-of-the-art fusion techniques. In particular, low\nspatial resolution hyperspectral and multispectral images are fused to produce\na high spatial resolution hyperspectral image.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 09:44:36 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 09:34:49 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Wei", "Qi", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1307.6081", "submitter": "Raffaella Calabrese", "authors": "Raffaella Calabrese, Giampiero Marra and Silvia Angela Osmetti", "title": "Bankruptcy Prediction of Small and Medium Enterprises Using a Flexible\n  Binary Generalized Extreme Value Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a binary regression accounting-based model for bankruptcy\nprediction of small and medium enterprises (SMEs). The main advantage of the\nmodel lies in its predictive performance in identifying defaulted SMEs. Another\nadvantage, which is especially relevant for banks, is that the relationship\nbetween the accounting characteristics of SMEs and response is not assumed a\npriori (e.g., linear, quadratic or cubic) and can be determined from the data.\nThe proposed approach uses the quantile function of the generalized extreme\nvalue distribution as link function as well as smooth functions of accounting\ncharacteristics to flexibly model covariate effects. Therefore, the usual\nassumptions in scoring models of symmetric link function and linear or\npre-specied covariate-response relationships are relaxed. Out-of-sample and\nout-of-time validation on Italian data shows that our proposal outperforms the\ncommonly used (logistic) scoring model for different default horizons.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 13:55:42 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 13:10:12 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Calabrese", "Raffaella", ""], ["Marra", "Giampiero", ""], ["Osmetti", "Silvia Angela", ""]]}, {"id": "1307.6254", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni and J. Fraser Forbes", "title": "Error analysis in Bayesian identification of non-linear state-space\n  models", "comments": "This article has been published in: Tulsyan, A, B. Huang, R.B.\n  Gopaluni and J.F. Forbes (2013). Bayesian identification of non-linear\n  state-space models: Part II- Error Analysis. In: Proceedings of the 10th IFAC\n  International Symposium on Dynamics and Control of Process Systems. Mumbai,\n  India", "journal-ref": "Proceedings of the 10th IFAC International Symposium on Dynamics\n  and Control of Process Systems. Mumbai, India, 2013", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, several methods based on sequential Monte Carlo\n(SMC) and Markov chain Monte Carlo (MCMC) have been proposed for Bayesian\nidentification of stochastic non-linear state-space models (SSMs). It is well\nknown that the performance of these simulation based identification methods\ndepends on the numerical approximations used in their design. We propose the\nuse of posterior Cram\\'er-Rao lower bound (PCRLB) as a mean square error (MSE)\nbound. Using PCRLB, a systematic procedure is developed to analyse the\nestimates delivered by Bayesian identification methods in terms of bias, MSE,\nand efficiency. The efficacy and utility of the proposed approach is\nillustrated through a numerical example.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 21:53:53 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.6258", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Swanand R. Khare, Biao Huang, R. Bhushan Gopaluni and\n  J. Fraser Forbes", "title": "Input design for Bayesian identification of non-linear state-space\n  models", "comments": "This article has been published in: Tulsyan, A, S.R. Khare, B. Huang,\n  R.B. Gopaluni and J.F. Forbes (2013). Bayesian identification of non-linear\n  state-space models: Part I- Input design. In: Proceedings of the 10th IFAC\n  International Symposium on Dynamics and Control of Process Systems. Mumbai,\n  India", "journal-ref": "Proceedings of the 10th IFAC International Symposium on Dynamics\n  and Control of Process Systems. Mumbai, India, 2013", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for designing optimal inputs for on-line Bayesian\nidentification of stochastic non-linear state-space models. The proposed method\nrelies on minimization of the posterior Cram\\'er Rao lower bound derived for\nthe model parameters, with respect to the input sequence. To render the\noptimization problem computationally tractable, the inputs are parametrized as\na multi-dimensional Markov chain in the input space. The proposed approach is\nillustrated through a simulation example.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 22:09:04 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Khare", "Swanand R.", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.6275", "submitter": "Daniel Zelterman", "authors": "Daniel Zelterman", "title": "A Two-Stage, Phase II Clinical Trial Design with Nested Criteria for\n  Early Stopping and Efficacy", "comments": "23 pages, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage design for a clinical trial with an early stopping\nrule for safety. We use different criteria to assess early stopping and\nefficacy. The early stopping rule is based on a criteria that can be determined\nmore quickly than that of efficacy. These separate criteria are also nested in\nthe sense that efficacy is a special case of, but not identical to, the early\nstopping criteria. The design readily allows for planning in terms of\nstatistical significance, power, and expected sample size necessary to assess\nan early stopping rule. This method is illustrated with a Phase II design\ncomparing patients treated for lung cancer with a novel drug combination to\nthose treated using historical control. In this example, the early stopping\nrule is based on the numbers of patients who exhibit progression-free survival\n(PFS) at 2 months post treatment follow-up and efficacy is judged by the number\nof patients who have PFS at 6 months.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 00:41:37 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Zelterman", "Daniel", ""]]}, {"id": "1307.6294", "submitter": "Hao Chen", "authors": "Hao Chen and Jerome H. Friedman", "title": "A new graph-based two-sample test for multivariate and object data", "comments": null, "journal-ref": "Journal of the American Statistical Association 2017, Vol. 112,\n  No. 517, 397-409, Theory and Methods", "doi": "10.1080/01621459.2016.1147356", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample tests for multivariate data and especially for non-Euclidean data\nare not well explored. This paper presents a novel test statistic based on a\nsimilarity graph constructed on the pooled observations from the two samples.\nIt can be applied to multivariate data and non-Euclidean data as long as a\ndissimilarity measure on the sample space can be defined, which can usually be\nprovided by domain experts. Existing tests based on a similarity graph lack\npower either for location or for scale alternatives. The new test utilizes a\ncommon pattern that was overlooked previously, and works for both types of\nalternatives. The test exhibits substantial power gains in simulation studies.\nIts asymptotic permutation null distribution is derived and shown to work well\nunder finite samples, facilitating its application to large data sets. The new\ntest is illustrated on two applications: The assessment of covariate balance in\na matched observational study, and the comparison of network data under\ndifferent conditions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 04:33:14 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2014 00:53:32 GMT"}, {"version": "v3", "created": "Thu, 14 Aug 2014 01:05:39 GMT"}, {"version": "v4", "created": "Mon, 16 Feb 2015 23:31:21 GMT"}, {"version": "v5", "created": "Fri, 17 Apr 2015 09:18:57 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Chen", "Hao", ""], ["Friedman", "Jerome H.", ""]]}, {"id": "1307.6417", "submitter": "Andreas Mayr", "authors": "Andreas Mayr and Matthias Schmid", "title": "Boosting the concordance index for survival data - a unified framework\n  to derive and evaluate biomarker combinations", "comments": "revised manuscript - added simulation study, additional results", "journal-ref": "PloS ONE 2014, 9(1): e84483", "doi": "10.1371/journal.pone.0084483", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of molecular signatures for the prediction of time-to-event\noutcomes is a methodologically challenging task in bioinformatics and\nbiostatistics. Although there are numerous approaches for the derivation of\nmarker combinations and their evaluation, the underlying methodology often\nsuffers from the problem that different optimization criteria are mixed during\nthe feature selection, estimation and evaluation steps. This might result in\nmarker combinations that are only suboptimal regarding the evaluation criterion\nof interest. To address this issue, we propose a unified framework to derive\nand evaluate biomarker combinations. Our approach is based on the concordance\nindex for time-to-event data, which is a non-parametric measure to quantify the\ndiscrimatory power of a prediction rule. Specifically, we propose a\ncomponent-wise boosting algorithm that results in linear biomarker combinations\nthat are optimal with respect to a smoothed version of the concordance index.\nWe investigate the performance of our algorithm in a large-scale simulation\nstudy and in two molecular data sets for the prediction of survival in breast\ncancer patients. Our numerical results show that the new approach is not only\nmethodologically sound but can also lead to a higher discriminatory power than\ntraditional approaches for the derivation of gene signatures.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 13:51:16 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 14:14:42 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Mayr", "Andreas", ""], ["Schmid", "Matthias", ""]]}, {"id": "1307.6437", "submitter": "Robert W. Johnson Jr", "authors": "Robert W. Johnson", "title": "Applications of the Beta Distribution Part 1: Transformation Group\n  Approach", "comments": "22 pages, 5 figures, 5 tables, minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A transformation group approach to the prior for the parameters of the beta\ndistribution is suggested which accounts for finite sets of data by imposing a\nlimit to the range of parameter values under consideration. The relationship\nbetween the beta distribution and the Poisson and gamma distributions in the\ncontinuum is explored, with an emphasis on the decomposition of the model into\nseparate estimates for size and shape. Use of the beta distribution in\nclassification and prediction problems is discussed, and the effect of the\nprior on the analysis of some well known examples from statistical genetics is\nexamined.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 14:44:06 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 21:21:09 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Johnson", "Robert W.", ""]]}, {"id": "1307.6442", "submitter": "Francisco Javier Rubio Dr.", "authors": "F. J. Rubio, B. Liseo", "title": "On the Independence Jeffreys prior for skew--symmetric models with\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Jeffreys prior of the skewness parameter of a general class of\nscalar skew--symmetric models. It is shown that this prior is symmetric about\n0, proper, and with tails $O(\\lambda^{-3/2})$ under mild regularity conditions.\nWe also calculate the independence Jeffreys prior for the case with unknown\nlocation and scale parameters. Sufficient conditions for the existence of the\ncorresponding posterior distribution are investigated for the case when the\nsampling model belongs to the family of skew--symmetric scale mixtures of\nnormal distributions. The usefulness of these results is illustrated using the\nskew--logistic model and two applications with real data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 14:57:51 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 22:14:33 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Rubio", "F. J.", ""], ["Liseo", "B.", ""]]}, {"id": "1307.6695", "submitter": "Nassim N. Taleb", "authors": "Nassim Nicholas Taleb", "title": "Where Do Thin Tails Come From?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature of heavy tails (typically) starts with a random walk and finds\nmechanisms that lead to fat tails under aggregation. We follow the inverse\nroute and show how starting with fat tails we get to thin-tails when deriving\nthe probability distribution of the response to a random variable. We introduce\na general dose-response curve and argue that the left and right-boundedness or\nsaturation of the response in natural things leads to thin-tails, even when the\n\"underlying\" random variable at the source of the exposure is fat-tailed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 10:56:16 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 00:58:55 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "1307.6701", "submitter": "Fabian Dunker", "authors": "Fabian Dunker, Jean-Pierre Florens, Thorsten Hohage, Jan Johannes,\n  Enno Mammen", "title": "Iterative Estimation of Solutions to Noisy Nonlinear Operator Equations\n  in Nonparametric Instrumental Regression", "comments": null, "journal-ref": "Journal of Econometrics , 2014, 178, 444-455", "doi": "10.1016/j.jeconom.2013.06.001", "report-no": null, "categories": "math.NA math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the solution of nonlinear integral equations with noisy\nintegral kernels as they appear in nonparametric instrumental regression. We\npropose a regularized Newton-type iteration and establish convergence and\nconvergence rate results. A particular emphasis is on instrumental regression\nmodels where the usual conditional mean assumption is replaced by a stronger\nindependence assumption. We demonstrate for the case of a binary instrument\nthat our approach allows the correct estimation of regression functions which\nare not identifiable with the standard model. This is illustrated in computed\nexamples with simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 11:25:11 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Dunker", "Fabian", ""], ["Florens", "Jean-Pierre", ""], ["Hohage", "Thorsten", ""], ["Johannes", "Jan", ""], ["Mammen", "Enno", ""]]}, {"id": "1307.7126", "submitter": "Grigory Sokolov", "authors": "Aleksey S. Polunchenko, Grigory Sokolov and Alexander G. Tartakovsky", "title": "Optimal Design and Analysis of the Exponentially Weighted Moving Average\n  Chart for Exponential Data", "comments": "28 pages, 5 figures, accepted for publication in the Sri Lankan\n  Journal of Applied Statistics. arXiv admin note: text overlap with\n  arXiv:1202.2849. text overlap with arXiv:1202.2849", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal design of the Exponentially Weighted Moving Average (EWMA)\nchart by a proper choice of the smoothing factor and the initial value\n(headstart) of the decision statistic. The particular problem addressed is that\nof quickest detection of an abrupt change in the parameter of a discrete-time\nexponential model. Both pre- and post-change parameter values are assumed\nknown, but the change-point is not known. For this change-point detection\nscenario, we examine the performance of the conventional one-sided EWMA chart\nwith respect to two optimality criteria: Pollak's minimax criterion associated\nwith the maximal conditional expected delay to detection and Shiryaev's\nmulti-cyclic setup associated with the stationary expected delay to detection.\nUsing the integral-equations approach, we derive the exact closed-form formulae\nfor all of the required performance measures. Based on these formulae we find\nthe optimal smoothing factor and headstart by solving the corresponding two\nbivariate constraint optimization problems. Finally, the performance of the\noptimized EWMA chart is compared against that of the Shiryaev--Roberts--$r$\nprocedure in the minimax setting, and against that of the original\nShiryaev--Roberts procedure in the multi-cyclic setting. The main conclusion is\nthat the EWMA chart, when fully optimized, turns out to be a very competitive\nprocedure, with performance nearly indistinguishable from that of the\nknown-to-be-best Shiryaev--Roberts--$r$ and Shiryaev--Roberts procedures.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 19:11:35 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 20:11:11 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 20:30:44 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1307.7140", "submitter": "Sertac Eroglu", "authors": "Sertac Eroglu", "title": "Parameters of the Menzerath-Altmann law: Statistical mechanical\n  interpretation as applied to a linguistic organization", "comments": "28 pages, 2 figures : Submitted to PHYSICA A", "journal-ref": null, "doi": "10.1007/s10955-014-1078-8", "report-no": null, "categories": "stat.ME cond-mat.stat-mech physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution behavior dictated by the Menzerath-Altmann (MA) law is\nfrequently encountered in linguistic and natural organizations at various\nstructural levels. The mathematical form of this empirical law comprises three\nfitting parameters whose values tend to be elusive, especially in\ninter-organizational studies. To allow interpretation of these parameters and\nbetter understand such distribution behavior, we present a statistical\nmechanical approach based on an analogy between the classical particles of a\nstatistical mechanical organization and the number of distinct words in a\ntextual organization. With this derivation, we achieve a transformed\n(generalized) form of the MA model, termed the statistical mechanical\nMenzerath-Altmann (SMMA) model. This novel transformed model consists of four\nparameters, one of which is a structure-dependent input parameter, and three of\nwhich are free-fitting parameters. Using distinct word data sets from two text\ncorpora, we verified that the SMMA model describes the same distribution as the\nMA model. We propose that the additional structure-dependent parameter of the\nSMMA model converts the three fitting parameters into structure-independent\nparameters. Moreover, the parameters of the SMMA model are associated with a\ncorresponding physical interpretation that can lead to characterization of an\norganization' s thermodynamic properties. We also propose that many\norganizations presenting MA law behavior, whether linguistic or not, can be\nexamined by the SMMA distribution model through the properly defined structural\ndegeneracy parameter and the energy associated states.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 11:12:24 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Eroglu", "Sertac", ""]]}, {"id": "1307.7203", "submitter": "Heejung Shim Heejung Shim", "authors": "Heejung Shim and Matthew Stephens", "title": "Wavelet-based genetic association analysis of functional phenotypes\n  arising from high-throughput sequencing assays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how genetic variants influence cellular-level processes is an\nimportant step towards understanding how they influence important\norganismal-level traits, or \"phenotypes\", including human disease\nsusceptibility. To this end scientists are undertaking large-scale genetic\nassociation studies that aim to identify genetic variants associated with\nmolecular and cellular phenotypes, such as gene expression, transcription\nfactor binding, or chromatin accessibility. These studies use high-throughput\nsequencing assays (e.g. RNA-seq, ChIP-seq, DNase-seq) to obtain high-resolution\ndata on how the traits vary along the genome in each sample. However, typical\nassociation analyses fail to exploit these high-resolution measurements,\ninstead aggregating the data at coarser resolutions, such as genes, or windows\nof fixed length. Here we develop and apply statistical methods that better\nexploit the high-resolution data. The key idea is to treat the sequence data as\nmeasuring an underlying \"function\" that varies along the genome, and then,\nbuilding on wavelet-based methods for functional data analysis, test for\nassociation between genetic variants and the underlying function. Applying\nthese methods to identify genetic variants associated with chromatin\naccessibility (dsQTLs) we find that they identify substantially more\nassociations than a simpler window-based analysis, and in total we identify 772\nnovel dsQTLs not identified by the original analysis.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 01:48:17 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Shim", "Heejung", ""], ["Stephens", "Matthew", ""]]}, {"id": "1307.7209", "submitter": "Robert Yuen", "authors": "Robert A. Yuen and Stilian Stoev", "title": "CRPS M-estimation for max-stable models", "comments": "23 pages, 1 figure, svjour3 document class", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable random fields provide canonical models for the dependence of\nmultivariate extremes. Inference with such models has been challenging due to\nthe lack of tractable likelihoods. In contrast, the finite dimensional\ncumulative distribution functions (CDFs) are often readily available and\nnatural to work with. Motivated by this fact, in this work we develop an\nM-estimation framework for max-stable models based on the continuous ranked\nprobability score (CRPS) of multivariate CDFs. We start by establishing\nconditions for the consistency and asymptotic normality of the CRPS-based\nestimators in a general context. We then implement them in the max-stable\nsetting and provide readily computable expressions for their asymptotic\ncovariance matrices. The resulting point and asymptotic confidence interval\nestimates are illustrated over popular simulated models. They enjoy accurate\ncoverages and offer an alternative to composite likelihood based methods.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 02:39:11 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Yuen", "Robert A.", ""], ["Stoev", "Stilian", ""]]}, {"id": "1307.7306", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Theodoros Tsiligkaridis, Alfred O Hero III", "title": "Kronecker Sum Decompositions of Space-Time Data", "comments": "5 pages, 8 figures, accepted to CAMSAP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the use of the space vs. time Kronecker product\ndecomposition in the estimation of covariance matrices for spatio-temporal\ndata. This decomposition imposes lower dimensional structure on the estimated\ncovariance matrix, thus reducing the number of samples required for estimation.\nTo allow a smooth tradeoff between the reduction in the number of parameters\n(to reduce estimation variance) and the accuracy of the covariance\napproximation (affecting estimation bias), we introduce a diagonally loaded\nmodification of the sum of kronecker products representation [1]. We derive a\nCramer-Rao bound (CRB) on the minimum attainable mean squared predictor\ncoefficient estimation error for unbiased estimators of Kronecker structured\ncovariance matrices. We illustrate the accuracy of the diagonally loaded\nKronecker sum decomposition by applying it to video data of human activity.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 20:43:21 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 14:20:39 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Tsiligkaridis", "Theodoros", ""], ["Hero", "Alfred O", "III"]]}, {"id": "1307.7383", "submitter": "Julie Josse", "authors": "Julie Josse and Susan Holmes", "title": "Measures of dependence between random vectors and tests of independence.\n  Literature review", "comments": "Incorporated new section on actual examples of data analyses", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple correlation coefficients between two variables have been generalized\nto measure association between two matrices in many ways. Coefficients such as\nthe RV coefficient, the distance covariance (dCov) coefficient and kernel based\ncoefficients have been adopted by different research communities. Scientists\nuse these coefficients to test whether two random vectors are linked. If they\nare, it is important to uncover what patterns exist in these associations.\n  We discuss the topic of measures of dependence between random vectors and\ntests of independence and show links between different approaches. We document\nsome of the interesting rediscoveries and lack of interconnection between\nbodies of literature. After providing definitions of the coefficients and\nassociated tests, we present the recent improvements that enhance their\nstatistical properties and ease of interpretation. We summarize multi-table\napproaches and provide scenarii where the indices can provide useful summaries\nof heterogeneous multi-block data.\n  We illustrate these different strategies on several examples of real data and\nsuggest directions for future research.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2013 17:22:47 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 16:12:46 GMT"}, {"version": "v3", "created": "Sun, 17 Aug 2014 21:25:27 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Josse", "Julie", ""], ["Holmes", "Susan", ""]]}, {"id": "1307.7536", "submitter": "Mette Langaas", "authors": "Mette Langaas, {\\O}yvind Bakke", "title": "Robust Methods for Disease-Genotype Association in Genetic Association\n  Studies: Calculate P-values Using Exact Conditional Enumeration instead of\n  Asymptotic Approximations", "comments": null, "journal-ref": null, "doi": "10.1515/sagmb-2013-0084", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetic association studies, detecting disease-genotype associations is a\nprimary goal. For most diseases, the underlying genetic model is unknown, and\nwe study seven robust test statistics for monotone association. For a given\ntest statistic, there are many ways to calculate a p-value, but in genetic\nassociation studies, calculations have predominantly been based on asymptotic\napproximations or on simulated permutations. We show that when the number of\npermutations tends to infinity, the permutation p-value approaches the exact\nconditional enumeration p-value, and further that calculating the latter\np-value is much more efficient than performing simulated permutations. We then\nanswer two research questions. (i) Which of the test statistics under study are\nthe most powerful for monotone genetic models? (ii) Based on test size, power,\nand computational considerations, should asymptotic approximations or exact\nconditional enumeration be used for calculating p-values? We have studied\ncase-control sample sizes with 500-5000 cases and 500-15000 controls, and\nsignificance levels from 5e-8 to 0.05, thus our results are applicable to\ngenetic association studies with only one genetic marker under study,\nintermediate follow-up studies, and genome wide association studies. We find\nthat if all monotone genetic models are of interest, the best performance is\nachieved for a test statistics based on the maximum over a range of\nCochrane-Armitage trend tests with different scores and for a constrained\nlikelihood ratio test. For significance levels below 0.05, asymptotic\napproximations may give a test size up to 20 times the nominal level, and\nshould therefore be used with caution. Further, calculating p-values based on\nexact conditional enumeration is a powerful, valid and computationally feasible\napproach, and we advocate its use in genetic association studies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 10:58:54 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Langaas", "Mette", ""], ["Bakke", "\u00d8yvind", ""]]}, {"id": "1307.7537", "submitter": "Mette Langaas", "authors": "Max Moldovan, Mette Langaas", "title": "Exact conditional p-values from arbitrary ranking of a sample space: An\n  application to genome-wide association studies", "comments": null, "journal-ref": "Advances in Systems Science and Applications (2014) Vol.14 No.1\n  76-83", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for computation of exact conditional efficiency robust\nenumeration p-values for detection of genotype--phenotype associations at a\nsingle bi-allelic genetic locus. Our method can be based on any arbitrary\nranking test statistics, such as efficiency robust test statistics or\nasymptotic p-values. The resulting p-values are exact conditional enumeration\np-values and satisfy the basic statistical validity property. Practically, the\nmethod allows performing statistically valid significance testing in genomic\nanalyses with unknown modes of inheritance at individual bi-allelic genetic\nloci -- the situation typical in genome-wide association studies. We provide an\nopen-source R code implementing the method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 10:59:45 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Moldovan", "Max", ""], ["Langaas", "Mette", ""]]}, {"id": "1307.7650", "submitter": "Johanna F. Ziegel", "authors": "Johanna F. Ziegel and Tilmann Gneiting", "title": "Copula Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose notions of calibration for probabilistic forecasts of general\nmultivariate quantities. Probabilistic copula calibration is a natural analogue\nof probabilistic calibration in the univariate setting. It can be assessed\nempirically by checking for the uniformity of the copula probability integral\ntransform (CopPIT), which is invariant under coordinate permutations and\ncoordinatewise strictly monotone transformations of the predictive distribution\nand the outcome. The CopPIT histogram can be interpreted as a generalization\nand variant of the multivariate rank histogram, which has been used to check\nthe calibration of ensemble forecasts. Climatological copula calibration is an\nanalogue of marginal calibration in the univariate setting. Methods and tools\nare illustrated in a simulation study and applied to compare raw numerical\nmodel and statistically postprocessed ensemble forecasts of bivariate wind\nvectors.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 17:23:19 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Ziegel", "Johanna F.", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1307.7667", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Jinlin Song", "title": "A Rejection Principle for Sequential Tests of Multiple Hypotheses\n  Controlling Familywise Error Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying approach to multiple testing procedures for sequential\n(or streaming) data by giving sufficient conditions for a sequential multiple\ntesting procedure to control the familywise error rate (FWER), extending to the\nsequential domain the work of Goeman and Solari (2010) who accomplished this\nfor fixed sample size procedures. Together we call these conditions the\n\"rejection principle for sequential tests,\" which we then apply to some\nexisting sequential multiple testing procedures to give simplified\nunderstanding of their FWER control. Next the principle is applied to derive\ntwo new sequential multiple testing procedures with provable FWER control, one\nfor testing hypotheses in order and another for closed testing. Examples of\nthese new procedures are given by applying them to a chromosome aberration data\nset and to finding the maximum safe dose of a treatment.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 18:05:00 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:58:58 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Bartroff", "Jay", ""], ["Song", "Jinlin", ""]]}, {"id": "1307.7682", "submitter": "Allan McRobie", "authors": "Allan McRobie", "title": "Probability-Matching Predictors for Extreme Extremes", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A location- and scale-invariant predictor is constructed which exhibits good\nprobability matching for extreme predictions outside the span of data drawn\nfrom a variety of (stationary) general distributions. It is constructed via the\nthree-parameter {\\mu, \\sigma, \\xi} Generalized Pareto Distribution (GPD). The\npredictor is designed to provide matching probability exactly for the GPD in\nboth the extreme heavy-tailed limit and the extreme bounded-tail limit, whilst\ngiving a good approximation to probability matching at all intermediate values\nof the tail parameter \\xi. The predictor is valid even for small sample sizes\nN, even as small as N = 3.\n  The main purpose of this paper is to present the somewhat lengthy derivations\nwhich draw heavily on the theory of hypergeometric functions, particularly the\nLauricella functions. Whilst the construction is inspired by the Bayesian\napproach to the prediction problem, it considers the case of vague prior\ninformation about both parameters and model, and all derivations are undertaken\nusing sampling theory.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 18:50:40 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["McRobie", "Allan", ""]]}, {"id": "1307.7721", "submitter": "Alfredo Lopez", "authors": "J\\'er\\'emie Bigot, Ra\\'ul Gouet, Thierry Klein, Alfredo L\\'opez", "title": "Geodesic PCA in the Wasserstein space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the method of Geodesic Principal Component Analysis (GPCA) on\nthe space of probability measures on the line, with finite second moment,\nendowed with the Wasserstein metric. We discuss the advantages of this\napproach, over a standard functional PCA of probability densities in the\nHilbert space of square-integrable functions. We establish the consistency of\nthe method by showing that the empirical GPCA converges to its population\ncounterpart, as the sample size tends to infinity. A key property in the study\nof GPCA is the isometry between the Wasserstein space and a closed convex\nsubset of the space of square-integrable functions, with respect to an\nappropriate measure. Therefore, we consider the general problem of PCA in a\nclosed convex subset of a separable Hilbert space, which serves as basis for\nthe analysis of GPCA and also has interest in its own right. We provide\nillustrative examples on simple statistical models, to show the benefits of\nthis approach for data analysis. The method is also applied to a real dataset\nof population pyramids.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 20:03:21 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2013 22:19:32 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 21:33:58 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", ""], ["Gouet", "Ra\u00fal", ""], ["Klein", "Thierry", ""], ["L\u00f3pez", "Alfredo", ""]]}, {"id": "1307.7784", "submitter": "Shu-Kay (Angus) Ng", "authors": "S.K. Ng, G.J. McLachlan, K. Wang, Z. Nagymanyoki, S. Liu, S.-W. Ng", "title": "Inference on differences between classes using cluster-specific\n  contrasts of mixed effects", "comments": "17 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of differentially expressed (DE) genes is one of the most\ncommonly studied problems in bioinformatics. For example, the identification of\nDE genes between distinct disease phenotypes is an important first step in\nunderstanding and developing treatment drugs for the disease. It can also\ncontribute significantly to the construction of a discriminant rule for\npredicting the class of origin of an unclassified tissue sample from a patient.\nWe present a novel approach to the problem of detecting DE genes that is based\non a test statistic formed as a weighted (normalized) cluster-specific contrast\nin the mixed effects of the mixture model used in the first instance to cluster\nthe gene profiles into a manageable number of clusters. The key factor in the\nformation of our test statistic is the use of gene-specific mixed effects in\nthe cluster-specific contrast. It thus means that the (soft) assignment of a\ngiven gene to a cluster is not crucial. This is because in addition to class\ndifferences between the (estimated) fixed effects terms for a cluster,\ngene-specific class differences also contribute to the cluster-specific\ncontributions to the final form of the test statistic. The proposed test\nstatistic can be used where the primary aim is to rank the genes in order of\nevidence against the null hypothesis of no DE. We also show how a P-value can\nbe calculated for each gene for use in multiple hypothesis testing where the\nintent is to control the false discovery rate (FDR) at some desired level. With\nthe use of real and simulated data sets, we show that the proposed\ncontrast-based approach outperforms other methods commonly used for the\ndetection of DE genes both in a ranking context with lower proportion of false\ndiscoveries and in a multiple hypothesis testing context with higher power for\na specified level of the FDR.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 02:05:30 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Ng", "S. K.", ""], ["McLachlan", "G. J.", ""], ["Wang", "K.", ""], ["Nagymanyoki", "Z.", ""], ["Liu", "S.", ""], ["Ng", "S. -W.", ""]]}, {"id": "1307.7830", "submitter": "Stefan Wager", "authors": "William Fithian and Stefan Wager", "title": "Semiparametric Exponential Families for Heavy-Tailed Data", "comments": "To appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semiparametric method for fitting the tail of a heavy-tailed\npopulation given a relatively small sample from that population and a larger\nsample from a related background population. We model the tail of the small\nsample as an exponential tilt of the better-observed large-sample tail, using a\nrobust sufficient statistic motivated by extreme value theory. In particular,\nour method induces an estimator of the small-population mean, and we give\ntheoretical and empirical evidence that this estimator outperforms methods that\ndo not use the background sample. We demonstrate substantial efficiency gains\nover competing methods in simulation and on data from a large controlled\nexperiment conducted by Facebook.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 05:44:41 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 02:08:16 GMT"}, {"version": "v3", "created": "Sun, 19 Oct 2014 21:09:11 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Fithian", "William", ""], ["Wager", "Stefan", ""]]}, {"id": "1307.7841", "submitter": "Wenxue Huang", "authors": "Wenxue Huang, Yong Shi and Xiaogang Wang", "title": "A nominal association matrix with feature selection for categorical data", "comments": "24 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1109.2553", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an informative probabilistic association matrix to measure a\nproportional local-to-global association of categories of one variable with\nanother categorical variable. Towards a probability based proportional\nprediction, the association matrix gives rise to the expected predictive\ndistribution of the first and second types of errors for a multinomial response\nvariable. In addition, the normalization of the diagonal of the matrix gives\nrise to an association vector, which provides the expected category accuracy\nlift rate distribution. A general scheme of global-to-global association\nmeasures with flexible weight vectors is further developed from the matrix. A\nhierarchy of equivalence relations defined by the association matrix and vector\nis shown. Applications to financial and survey data together with simulations\nresults are presented.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 06:53:17 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Huang", "Wenxue", ""], ["Shi", "Yong", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1307.7948", "submitter": "Kristi Kuljus", "authors": "Kristi Kuljus and J\\\"uri Lember", "title": "On the accuracy of the Viterbi alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a hidden Markov model, the underlying Markov chain is usually hidden.\nOften, the maximum likelihood alignment (Viterbi alignment) is used as its\nestimate. Although having the biggest likelihood, the Viterbi alignment can\nbehave very untypically by passing states that are at most unexpected. To avoid\nsuch situations, the Viterbi alignment can be modified by forcing it not to\npass these states. In this article, an iterative procedure for improving the\nViterbi alignment is proposed and studied. The iterative approach is compared\nwith a simple bunch approach where a number of states with low probability are\nall replaced at the same time. It can be seen that the iterative way of\nadjusting the Viterbi alignment is more efficient and it has several advantages\nover the bunch approach. The same iterative algorithm for improving the Viterbi\nalignment can be used in the case of peeping, that is when it is possible to\nreveal hidden states. In addition, lower bounds for classification\nprobabilities of the Viterbi alignment under different conditions on the model\nparameters are studied.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 12:40:16 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Kuljus", "Kristi", ""], ["Lember", "J\u00fcri", ""]]}, {"id": "1307.7963", "submitter": "Minh-Ngoc Tran", "authors": "David J Nott, Minh-Ngoc Tran, Anthony Y.C. Kuk and Robert Kohn", "title": "Efficient variational inference for generalized linear mixed models with\n  large datasets", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article develops a hybrid Variational Bayes algorithm that combines the\nmean-field and fixed-form Variational Bayes methods. The new estimation\nalgorithm can be used to approximate any posterior without relying on conjugate\npriors. We propose a divide and recombine strategy for the analysis of large\ndatasets, which partitions a large dataset into smaller pieces and then\ncombines the variational distributions that have been learnt in parallel on\neach separate piece using the hybrid Variational Bayes algorithm. The proposed\nmethod is applied to fitting generalized linear mixed models. The computational\nefficiency of the parallel and hybrid Variational Bayes algorithm is\ndemonstrated on several simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 13:11:47 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 11:00:15 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Nott", "David J", ""], ["Tran", "Minh-Ngoc", ""], ["Kuk", "Anthony Y. C.", ""], ["Kohn", "Robert", ""]]}, {"id": "1307.7975", "submitter": "Minh-Ngoc Tran", "authors": "Michael K. Pitt, Minh-Ngoc Tran, Marcel Scharth and Robert Kohn", "title": "On the existence of moments for high dimensional importance sampling", "comments": "33 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical results for importance sampling rely on the existence of certain\nmoments of the importance weights, which are the ratios between the proposal\nand target densities. In particular, a finite variance ensures square root\nconvergence and asymptotic normality of the importance sampling estimate, and\ncan be important for the reliability of the method in practice. We derive\nconditions for the existence of any required moments of the weights for\nGaussian proposals and show that these conditions are almost necessary and\nsufficient for a wide range of models with latent Gaussian components.\nImportant examples are time series and panel data models with measurement\ndensities which belong to the exponential family. We introduce practical and\nsimple methods for checking and imposing the conditions for the existence of\nthe desired moments. We develop a two component mixture proposal that allows us\nto flexibly adapt a given proposal density into a robust importance density.\nThese methods are illustrated on a wide range of models including generalized\nlinear mixed models, non-Gaussian nonlinear state space models and panel data\nmodels with autoregressive random effects.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 13:39:29 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Pitt", "Michael K.", ""], ["Tran", "Minh-Ngoc", ""], ["Scharth", "Marcel", ""], ["Kohn", "Robert", ""]]}, {"id": "1307.8136", "submitter": "Brian Kent", "authors": "Brian P. Kent, Alessandro Rinaldo, Timothy Verstynen", "title": "DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering", "comments": "28 pages, 9 figures, for associated software see\n  https://github.com/CoAxLab/DeBaCl", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The level set tree approach of Hartigan (1975) provides a probabilistically\nbased and highly interpretable encoding of the clustering behavior of a\ndataset. By representing the hierarchy of data modes as a dendrogram of the\nlevel sets of a density estimator, this approach offers many advantages for\nexploratory analysis and clustering, especially for complex and\nhigh-dimensional data. Several R packages exist for level set tree estimation,\nbut their practical usefulness is limited by computational inefficiency,\nabsence of interactive graphical capabilities and, from a theoretical\nperspective, reliance on asymptotic approximations. To make it easier for\npractitioners to capture the advantages of level set trees, we have written the\nPython package DeBaCl for DEnsity-BAsed CLustering. In this article we\nillustrate how DeBaCl's level set tree estimates can be used for difficult\nclustering tasks and interactive graphical data analysis. The package is\nintended to promote the practical use of level set trees through improvements\nin computational efficiency and a high degree of user customization. In\naddition, the flexible algorithms implemented in DeBaCl enjoy finite sample\naccuracy, as demonstrated in recent literature on density clustering. Finally,\nwe show the level set tree framework can be easily extended to deal with\nfunctional data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 20:19:26 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Kent", "Brian P.", ""], ["Rinaldo", "Alessandro", ""], ["Verstynen", "Timothy", ""]]}, {"id": "1307.8217", "submitter": "Gongjun Xu", "authors": "Gongjun Xu, Bodhisattva Sen and Zhiliang Ying", "title": "Bootstrapping a Change-Point Cox Model for Survival Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the (in)-consistency of various bootstrap methods for\nmaking inference on a change-point in time in the Cox model with right censored\nsurvival data. A criterion is established for the consistency of any bootstrap\nmethod. It is shown that the usual nonparametric bootstrap is inconsistent for\nthe maximum partial likelihood estimation of the change-point. A new\nmodel-based bootstrap approach is proposed and its consistency established.\nSimulation studies are carried out to assess the performance of various\nbootstrap schemes.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 03:44:56 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Xu", "Gongjun", ""], ["Sen", "Bodhisattva", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1307.8339", "submitter": "Ayodeji Akinduko Mr", "authors": "A. A. Akinduko, A. N. Gorban", "title": "Multiscale principal component analysis", "comments": "24 pages, 22 figures", "journal-ref": null, "doi": "10.1088/1742-6596/490/1/012081", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Principal component analysis (PCA) is an important tool in exploring data.\nThe conventional approach to PCA leads to a solution which favours the\nstructures with large variances. This is sensitive to outliers and could\nobfuscate interesting underlying structures. One of the equivalent definitions\nof PCA is that it seeks the subspaces that maximize the sum of squared pairwise\ndistances between data projections. This definition opens up more flexibility\nin the analysis of principal components which is useful in enhancing PCA. In\nthis paper we introduce scales into PCA by maximizing only the sum of pairwise\ndistances between projections for pairs of datapoints with distances within a\nchosen interval of values [l,u]. The resulting principal component\ndecompositions in Multiscale PCA depend on point (l,u) on the plane and for\neach point we define projectors onto principal components. Cluster analysis of\nthese projectors reveals the structures in the data at various scales. Each\nstructure is described by the eigenvectors at the medoid point of the cluster\nwhich represent the structure. We also use the distortion of projections as a\ncriterion for choosing an appropriate scale especially for data with outliers.\nThis method was tested on both artificial distribution of data and real data.\nFor data with multiscale structures, the method was able to reveal the\ndifferent structures of the data and also to reduce the effect of outliers in\nthe principal component analysis.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 14:46:32 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Akinduko", "A. A.", ""], ["Gorban", "A. N.", ""]]}]