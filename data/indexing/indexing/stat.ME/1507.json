[{"id": "1507.00033", "submitter": "Jeremie Houssineau", "authors": "Daniel S. Bryant and Emmanuel D. Delande and Steven Gehly and Jeremie\n  Houssineau and Daniel E. Clark and Brandon A. Jones", "title": "Spawning Models for the CPHD Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In its classical form, the Cardinalized Probability Hypothesis Density (CPHD)\nfilter does not model the appearance of new targets through spawning, yet there\nare applications for which spawning models more appropriately account for\nnewborn objects when compared to spontaneous birth models. In this paper, we\npropose a principled derivation of the CPHD filter with spawning from the\nFinite Set Statistics framework. A Gaussian Mixture implementation of the CPHD\nfilter with spawning is then presented, illustrated with three applicable\nspawning models on a simulated scenario involving two parent targets spawning a\ntotal of five objects. Results show that filter implementations with spawn\nmodels provide more accurate results when compared to a birth model\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 20:53:19 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2015 09:17:36 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Bryant", "Daniel S.", ""], ["Delande", "Emmanuel D.", ""], ["Gehly", "Steven", ""], ["Houssineau", "Jeremie", ""], ["Clark", "Daniel E.", ""], ["Jones", "Brandon A.", ""]]}, {"id": "1507.00108", "submitter": "Scott Sisson", "authors": "Boris Beranger and Simone A. Padoan and Scott A. Sisson", "title": "Models for extremal dependence derived from skew-symmetric families", "comments": "To appear in Scandinavian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skew-symmetric families of distributions such as the skew-normal and skew-$t$\nrepresent supersets of the normal and $t$ distributions, and they exhibit\nricher classes of extremal behaviour. By defining a non-stationary skew-normal\nprocess, which allows the easy handling of positive definite, non-stationary\ncovariance functions, we derive a new family of max-stable processes - the\nextremal-skew-$t$ process. This process is a superset of non-stationary\nprocesses that include the stationary extremal-$t$ processes. We provide the\nspectral representation and the resulting angular densities of the\nextremal-skew-$t$ process, and illustrate its practical implementation\n(Includes Supporting Information).\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 05:37:44 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 00:43:15 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2016 03:34:24 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Beranger", "Boris", ""], ["Padoan", "Simone A.", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1507.00171", "submitter": "Kevin Bleakley", "authors": "G\\'erard Biau (LSTA), Kevin Bleakley (LMO, SELECT), Benoit Cadre (ENS\n  Rennes, UEB, IRMAR)", "title": "The Statistical Performance of Collaborative Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of massive and complex data sets will require the\ndevelopment of algorithms that depend on distributed computing and\ncollaborative inference. Inspired by this, we propose a collaborative framework\nthat aims to estimate the unknown mean $\\theta$ of a random variable $X$. In\nthe model we present, a certain number of calculation units, distributed across\na communication network represented by a graph, participate in the estimation\nof $\\theta$ by sequentially receiving independent data from $X$ while\nexchanging messages via a stochastic matrix $A$ defined over the graph. We give\nprecise conditions on the matrix $A$ under which the statistical precision of\nthe individual units is comparable to that of a (gold standard) virtual\ncentralized estimate, even though each unit does not have access to all of the\ndata. We show in particular the fundamental role played by both the non-trivial\neigenvalues of $A$ and the Ramanujan class of expander graphs, which provide\nremarkable performance for moderate algorithmic cost.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 10:05:29 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA"], ["Bleakley", "Kevin", "", "LMO, SELECT"], ["Cadre", "Benoit", "", "ENS\n  Rennes, UEB, IRMAR"]]}, {"id": "1507.00181", "submitter": "Belinda Hernandez", "authors": "Belinda Hern\\'andez, Adrian E. Raftery, Stephen R. Pennington, Andrew\n  C. Parnell", "title": "Bayesian Additive Regression Trees using Bayesian Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Additive Regression Trees (BART) is a statistical sum of trees\nmodel. It can be considered a Bayesian version of machine learning tree\nensemble methods where the individual trees are the base learners. However for\ndata sets where the number of variables $p$ is large (e.g. $p>5,000$) the\nalgorithm can become prohibitively expensive, computationally.\n  Another method which is popular for high dimensional data is random forests,\na machine learning algorithm which grows trees using a greedy search for the\nbest split points. However, as it is not a statistical model, it cannot produce\nprobabilistic estimates or predictions.\n  We propose an alternative algorithm for BART called BART-BMA, which uses\nBayesian Model Averaging and a greedy search algorithm to produce a model which\nis much more efficient than BART for datasets with large $p$. BART-BMA\nincorporates elements of both BART and random forests to offer a model-based\nalgorithm which can deal with high-dimensional data.\n  We have found that BART-BMA can be run in a reasonable time on a standard\nlaptop for the \"small $n$ large $p$\" scenario which is common in many areas of\nbioinformatics. We showcase this method using simulated data and data from two\nreal proteomic experiments; one to distinguish between patients with\ncardiovascular disease and controls and another to classify agressive from\nnon-agressive prostate cancer. We compare our results to their main\ncompetitors.\n  Open source code written in R and Rcpp to run BART-BMA can be found at:\nhttps://github.com/BelindaHernandez/BART-BMA.git\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 10:58:46 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 14:08:27 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Hern\u00e1ndez", "Belinda", ""], ["Raftery", "Adrian E.", ""], ["Pennington", "Stephen R.", ""], ["Parnell", "Andrew C.", ""]]}, {"id": "1507.00280", "submitter": "David Hallac", "authors": "David Hallac, Jure Leskovec, Stephen Boyd", "title": "Network Lasso: Clustering and Optimization in Large Graphs", "comments": null, "journal-ref": null, "doi": "10.1145/2783258.2783313", "report-no": null, "categories": "cs.SI math.OC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization is an essential tool for modern data analysis, as it\nprovides a framework to formulate and solve many problems in machine learning\nand data mining. However, general convex optimization solvers do not scale\nwell, and scalable solvers are often specialized to only work on a narrow class\nof problems. Therefore, there is a need for simple, scalable algorithms that\ncan solve many common optimization problems. In this paper, we introduce the\n\\emph{network lasso}, a generalization of the group lasso to a network setting\nthat allows for simultaneous clustering and optimization on graphs. We develop\nan algorithm based on the Alternating Direction Method of Multipliers (ADMM) to\nsolve this problem in a distributed and scalable manner, which allows for\nguaranteed global convergence even on large graphs. We also examine a\nnon-convex extension of this approach. We then demonstrate that many types of\nproblems can be expressed in our framework. We focus on three in particular -\nbinary classification, predicting housing prices, and event detection in time\nseries data - comparing the network lasso to baseline approaches and showing\nthat it is both a fast and accurate method of solving large optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 16:25:38 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Hallac", "David", ""], ["Leskovec", "Jure", ""], ["Boyd", "Stephen", ""]]}, {"id": "1507.00420", "submitter": "Qi Zheng", "authors": "Qi Zheng, Limin Peng and Xuming He", "title": "Globally adaptive quantile regression with ultra-high dimensional data", "comments": "This paper has been withdrawn by the author due to a crucial proof\n  error in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression has become a valuable tool to analyze heterogeneous\ncovaraite-response associations that are often encountered in practice. The\ndevelopment of quantile regression methodology for high-dimensional covariates\nprimarily focuses on examination of model sparsity at a single or multiple\nquantile levels, which are typically pre-specified ad hoc by the users. The\nresulting models may be sensitive to the specific choices of the quantile\nlevels, leading to difficulties in interpretation and erosion of confidence in\nthe results. In this article, we propose a new penalization framework for\nquantile regression in the high-dimensional setting. We employ adaptive L1\npenalties, and more importantly, propose a uniform selector of the tuning\nparameter for a set of quantile levels to avoid some of the potential problems\nwith model selection at individual quantile levels. Our proposed approach\nachieves consistent shrinkage of regression quantile estimates across a\ncontinuous range of quantiles levels, enhancing the flexibility and robustness\nof the existing penalized quantile regression methods. Our theoretical results\ninclude the oracle rate of uniform convergence and weak convergence of the\nparameter estimators. We also use numerical studies to confirm our theoretical\nfindings and illustrate the practical utility of our proposal\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 03:50:48 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 18:10:07 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Zheng", "Qi", ""], ["Peng", "Limin", ""], ["He", "Xuming", ""]]}, {"id": "1507.00433", "submitter": "Mathias Drton", "authors": "Lina Lin, Mathias Drton, Ali Shojaie", "title": "Estimation of High-Dimensional Graphical Models Using Regularized Score\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are widely used to model stochastic dependences among large\ncollections of variables. We introduce a new method of estimating undirected\nconditional independence graphs based on the score matching loss, introduced by\nHyvarinen (2005), and subsequently extended in Hyvarinen (2007). The\nregularized score matching method we propose applies to settings with\ncontinuous observations and allows for computationally efficient treatment of\npossibly non-Gaussian exponential family models. In the well-explored Gaussian\nsetting, regularized score matching avoids issues of asymmetry that arise when\napplying the technique of neighborhood selection, and compared to existing\nmethods that directly yield symmetric estimates, the score matching approach\nhas the advantage that the considered loss is quadratic and gives piecewise\nlinear solution paths under $\\ell_1$ regularization. Under suitable\nirrepresentability conditions, we show that $\\ell_1$-regularized score matching\nis consistent for graph estimation in sparse high-dimensional settings. Through\nnumerical experiments and an application to RNAseq data, we confirm that\nregularized score matching achieves state-of-the-art performance in the\nGaussian case and provides a valuable tool for computationally efficient\nestimation in non-Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 05:48:12 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 17:26:42 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Lin", "Lina", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "1507.00720", "submitter": "Rajesh Ranganath", "authors": "Rajesh Ranganath, David Blei", "title": "Correlated Random Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop correlated random measures, random measures where the atom weights\ncan exhibit a flexible pattern of dependence, and use them to develop powerful\nhierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric\nmodels are usually built from completely random measures, a Poisson-process\nbased construction in which the atom weights are independent. Completely random\nmeasures imply strong independence assumptions in the corresponding\nhierarchical model, and these assumptions are often misplaced in real-world\nsettings. Correlated random measures address this limitation. They model\ncorrelation within the measure by using a Gaussian process in concert with the\nPoisson process. With correlated random measures, for example, we can develop a\nlatent feature model for which we can infer both the properties of the latent\nfeatures and their dependency pattern. We develop several other examples as\nwell. We study a correlated random measure model of pairwise count data. We\nderive an efficient variational inference algorithm and show improved\npredictive performance on large data sets of documents, web clicks, and\nelectronic health records.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 19:56:45 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 03:12:18 GMT"}, {"version": "v3", "created": "Wed, 9 Nov 2016 07:54:54 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Blei", "David", ""]]}, {"id": "1507.00747", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Zongming Ma, Matthew D. McHugh, Dylan S. Small", "title": "Nonparametric methods for doubly robust estimation of continuous\n  treatment effects", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12212", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous treatments (e.g., doses) arise often in practice, but many\navailable causal effect estimators are limited by either requiring parametric\nmodels for the effect curve, or by not allowing doubly robust covariate\nadjustment. We develop a novel kernel smoothing approach that requires only\nmild smoothness assumptions on the effect curve, and still allows for\nmisspecification of either the treatment density or outcome regression. We\nderive asymptotic properties and give a procedure for data-driven bandwidth\nselection. The methods are illustrated via simulation and in a study of the\neffect of nurse staffing on hospital readmissions penalties.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 20:16:03 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 15:00:24 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2016 20:10:49 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Ma", "Zongming", ""], ["McHugh", "Matthew D.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1507.00803", "submitter": "Guillaume Basse", "authors": "Guillaume W. Basse, Edoardo M. Airoldi", "title": "Model-assisted design of experiments in the presence of network\n  correlated outcomes", "comments": "56 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of how to assign treatment in a randomized\nexperiment, in which the correlation among the outcomes is informed by a\nnetwork available pre-intervention. Working within the potential outcome causal\nframework, we develop a class of models that posit such a correlation structure\namong the outcomes. Then we leverage these models to develop restricted\nrandomization strategies for allocating treatment optimally, by minimizing the\nmean square error of the estimated average treatment effect. Analytical\ndecompositions of the mean square error, due both to the model and to the\nrandomization distribution, provide insights into aspects of the optimal\ndesigns. In particular, the analysis suggests new notions of balance based on\nspecific network quantities, in addition to classical covariate balance. The\nresulting balanced, optimal restricted randomization strategies are still\ndesign unbiased, in situations where the model used to derive them does not\nhold. We illustrate how the proposed treatment allocation strategies improve on\nallocations that ignore the network structure, with extensive simulations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 01:44:51 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 21:14:41 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 15:14:40 GMT"}, {"version": "v4", "created": "Thu, 18 May 2017 14:30:09 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Basse", "Guillaume W.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1507.00955", "submitter": "Olga Kolchyna", "authors": "Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste", "title": "Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination", "comments": "32 pages, 5 figures", "journal-ref": "Handbook of Sentiment Analysis in Finance. Mitra, G. and Yu, X.\n  (Eds.). (2016). ISBN 1910571571", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 15:46:55 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 17:24:18 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 11:44:33 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kolchyna", "Olga", ""], ["Souza", "Tharsis T. P.", ""], ["Treleaven", "Philip", ""], ["Aste", "Tomaso", ""]]}, {"id": "1507.00965", "submitter": "Joaquin Ortega", "authors": "R. B\\'arcenas and J. Ortega and A. J. Quiroz", "title": "Quadratic forms of the empirical processes for the two sample problem\n  for functional data", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of quadratic forms of the empirical process for the two-sample\nproblem in the context of functional data is considered. The convergence of the\nfamily of statistics proposed to a Gaussian limit is established under metric\nentropy conditions for smooth functional data. The applicability of the\nproposed methodology is evaluated in examples.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 16:53:32 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["B\u00e1rcenas", "R.", ""], ["Ortega", "J.", ""], ["Quiroz", "A. J.", ""]]}, {"id": "1507.01173", "submitter": "Klaus Holst K", "authors": "Klaus K. Holst", "title": "Model Diagnostics Based on Cumulative Residuals: The R-package gof", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized linear model is widely used in all areas of applied\nstatistics and while correct asymptotic inference can be achieved under\nmisspecification of the distributional assumptions, a correctly specified mean\nstructure is crucial to obtain interpretable results. Usually the linearity and\nfunctional form of predictors are checked by inspecting various scatter plots\nof the residuals, however, the subjective task of judging these can be\nchallenging. In this paper we present an implementation of model diagnostics\nfor the generalized linear model as well as structural equation models, based\non aggregates of the residuals where the asymptotic behavior under the null is\nimitated by simulations. A procedure for checking the proportional hazard\nassumption in the Cox regression is also implemented.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 07:14:16 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Holst", "Klaus K.", ""]]}, {"id": "1507.01179", "submitter": "Robert Kohn", "authors": "Michael K. Pitt and Jamie Hall and Robert Kohn", "title": "Bayesian inference for latent factor GARCH models", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor GARCH models are difficult to estimate using Bayesian methods\nbecause standard Markov chain Monte Carlo samplers produce slowly mixing and\ninefficient draws from the posterior distributions of the model parameters.\nThis paper describes how to apply the particle Gibbs algorithm to estimate\nfactor GARCH models efficiently. The method has two advantages over previous\napproaches. First, it generalises in a straightfoward way to models with\nmultiple factors and to various members of the GARCH family. Second, it scales\nup well as the dimension of the o, bservation vector increases.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 08:47:24 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Pitt", "Michael K.", ""], ["Hall", "Jamie", ""], ["Kohn", "Robert", ""]]}, {"id": "1507.01182", "submitter": "Klaus Holst K", "authors": "Klaus K. Holst and Esben Budtz-J{\\o}rgensen and Gitte Moos Knudsen", "title": "A latent variable model with mixed binary and continuous response\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for obtaining maximum likelihood estimates in a model\nwith continuous and binary outcomes. Combinations of left and right censored\nobservations are also naturally modeled in this framework. The model and\nestimation procedure has been implemented in the R package lava.tobit.\n  The method is demonstrated on brain imaging and personality data where\nmeasurement error on predictor variables is handled in a latent variable\nframework. A simulation study is conducted comparing the small sample\nproperties of the MLE with a limited information estimator.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 09:29:50 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Holst", "Klaus K.", ""], ["Budtz-J\u00f8rgensen", "Esben", ""], ["Knudsen", "Gitte Moos", ""]]}, {"id": "1507.01296", "submitter": "Qing Zhou", "authors": "Qing Zhou and Seunghyun Min", "title": "Uncertainty Quantification Under Group Sparsity", "comments": "44 pages", "journal-ref": "Biometrika, 104: 613-632, 2017", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the uncertainty in penalized regression under group sparsity is\nan important open question. We establish, under a high-dimensional scaling, the\nasymptotic validity of a modified parametric bootstrap method for the group\nlasso, assuming a Gaussian error model and mild conditions on the design matrix\nand the true coefficients. Simulation of bootstrap samples provides\nsimultaneous inferences on large groups of coefficients. Through extensive\nnumerical comparisons, we demonstrate that our bootstrap method performs much\nbetter than popular competitors, highlighting its practical utility. The\ntheoretical result is generalized to other block norm penalization and\nsub-Gaussian errors, which further broadens the potential applications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 23:23:16 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 18:04:24 GMT"}, {"version": "v3", "created": "Sat, 10 Sep 2016 19:58:22 GMT"}, {"version": "v4", "created": "Sun, 4 Jun 2017 21:41:04 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhou", "Qing", ""], ["Min", "Seunghyun", ""]]}, {"id": "1507.01397", "submitter": "Sarah Lemler", "authors": "Agathe Guilloux (LSTA), Sarah Lemler (LaMME), Marie-Luce Taupin\n  (LaMME, Unit\\'e MIAJ)", "title": "Adaptive kernel estimation of the baseline function in the Cox model,\n  with high-dimensional covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to propose a novel kernel estimator of the\nbaseline function in a general high-dimensional Cox model, for which we derive\nnon-asymptotic rates of convergence. To construct our estimator, we first\nestimate the regression parameter in the Cox model via a Lasso procedure. We\nthen plug this estimator into the classical kernel estimator of the baseline\nfunction, obtained by smoothing the so-called Breslow estimator of the\ncumulative baseline function. We propose and study an adaptive procedure for\nselecting the bandwidth, in the spirit of Gold-enshluger and Lepski (2011). We\nstate non-asymptotic oracle inequalities for the final estimator, which reveal\nthe reduction of the rates of convergence when the dimension of the covariates\ngrows.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 11:30:27 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Guilloux", "Agathe", "", "LSTA"], ["Lemler", "Sarah", "", "LaMME"], ["Taupin", "Marie-Luce", "", "LaMME, Unit\u00e9 MIAJ"]]}, {"id": "1507.01404", "submitter": "J\\'er\\'emy Magnanensi", "authors": "J\\'er\\'emy Magnanensi, Fr\\'ed\\'eric Bertrand, Myriam Maumy-Bertrand,\n  Nicolas Meyer", "title": "A new Universal Resample Stable Bootstrap-based Stopping Criterion in\n  PLS Components Construction", "comments": "31 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": "1507.01404", "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new robust stopping criterion in Partial Least Squares\nRegressions (PLSR) components construction characterised by a high level of\nstability. This new criterion is defined as a universal one since it is\nsuitable both for PLSR and its extension to Generalized Linear Regressions\n(PLSGLR). This criterion is based on a non-parametric bootstrap process and has\nto be computed algorithmically. It allows to test each successive components on\na preset significant level alpha. In order to assess its performances and\nrobustness with respect to different noise levels, we perform intensive\ndatasets simulations, with a preset and known number of components to extract,\nboth in the case n>p (n being the number of subjects and p the number of\noriginal predictors), and for datasets with n<p. We then use t-tests to compare\nthe performance of our approach to some others classical criteria. The property\nof robustness is particularly tested through resampling processes on a real\nallelotyping dataset. Our conclusion is that our criterion presents also better\nglobal predictive performances, both in the PLSR and PLSGLR (Logistic and\nPoisson) frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 11:56:18 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Magnanensi", "J\u00e9r\u00e9my", ""], ["Bertrand", "Fr\u00e9d\u00e9ric", ""], ["Maumy-Bertrand", "Myriam", ""], ["Meyer", "Nicolas", ""]]}, {"id": "1507.01542", "submitter": "Jiannan Lu", "authors": "Jiannan Lu, Peng Ding and Tirthankar Dasgupta", "title": "Treatment Effects on Ordinal Outcomes: Causal Estimands and Sharp Bounds", "comments": "Accepted by the Journal of Education and Behavioral Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the causal effects of interventions on ordinal outcomes is an\nimportant objective of many educational and behavioral studies. Under the\npotential outcomes framework, we can define causal effects as comparisons\nbetween the potential outcomes under treatment and control. However,\nunfortunately, the average causal effect, often the parameter of interest, is\ndifficult to interpret for ordinal outcomes. To address this challenge, we\npropose to use two causal parameters, which are defined as the probabilities\nthat the treatment is beneficial and strictly beneficial for the experimental\nunits. However, although well-defined for any outcomes and of particular\ninterest for ordinal outcomes, the two aforementioned parameters depend on the\nassociation between the potential outcomes, and are therefore not identifiable\nfrom the observed data without additional assumptions. Echoing recent advances\nin the econometrics and biostatistics literature, we present the sharp bounds\nof the aforementioned causal parameters for ordinal outcomes, under fixed\nmarginal distributions of the potential outcomes. Because the causal estimands\nand their corresponding sharp bounds are based on the potential outcomes\nthemselves, the proposed framework can be flexibly incorporated into any chosen\nmodels of the potential outcomes, and are directly applicable to randomized\nexperiments, unconfounded observational studies, and randomized experiments\nwith noncompliance. We illustrate our methodology via numerical examples and\nthree real-life applications related to educational and behavioral research.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 17:09:19 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 01:10:12 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 07:29:33 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Lu", "Jiannan", ""], ["Ding", "Peng", ""], ["Dasgupta", "Tirthankar", ""]]}, {"id": "1507.01729", "submitter": "Jozef Barunik", "authors": "Jozef Barunik and Tomas Krehlik", "title": "Measuring the frequency dynamics of financial connectedness and systemic\n  risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.EC q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for measuring connectedness among financial\nvariables that arises due to heterogeneous frequency responses to shocks. To\nestimate connectedness in short-, medium-, and long-term financial cycles, we\nintroduce a framework based on the spectral representation of variance\ndecompositions. In an empirical application, we document the rich\ntime-frequency dynamics of volatility connectedness in US financial\ninstitutions. Economically, periods in which connectedness is created at high\nfrequencies are periods when stock markets seem to process information rapidly\nand calmly, and a shock to one asset in the system will have an impact mainly\nin the short term. When the connectedness is created at lower frequencies, it\nsuggests that shocks are persistent and are being transmitted for longer\nperiods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 09:44:30 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 08:59:00 GMT"}, {"version": "v3", "created": "Sat, 29 Apr 2017 15:04:16 GMT"}, {"version": "v4", "created": "Tue, 19 Dec 2017 17:40:15 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Barunik", "Jozef", ""], ["Krehlik", "Tomas", ""]]}, {"id": "1507.01778", "submitter": "David  Bolin", "authors": "David Bolin and Finn Lindgren", "title": "Quantifying the uncertainty of contour maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contour maps are widely used to display estimates of spatial fields. Instead\nof showing the estimated field, a contour map only shows a fixed number of\ncontour lines for different levels. However, despite the ubiquitous use of\nthese maps, the uncertainty associated with them has been given a surprisingly\nsmall amount of attention. We derive measures of the statistical uncertainty,\nor quality, of contour maps, and use these to decide an appropriate number of\ncontour lines, that relates to the uncertainty in the estimated spatial field.\nFor practical use in geostatistics and medical imaging, computational methods\nare constructed, that can be applied to Gaussian Markov random fields, and in\nparticular be used in combination with integrated nested Laplace approximations\nfor latent Gaussian models. The methods are demonstrated on simulated data and\nan application to temperature estimation is presented.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 12:27:55 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 17:04:59 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 09:34:25 GMT"}, {"version": "v4", "created": "Fri, 8 Jul 2016 11:15:15 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Bolin", "David", ""], ["Lindgren", "Finn", ""]]}, {"id": "1507.01822", "submitter": "Melanie Prague", "authors": "Melanie Prague, Rui Wang, Alisa Stephens, Eric Tchetgen Tchetgen and\n  Victor DeGruttola", "title": "Accounting for interactions and complex inter-subject dependency in\n  estimating treatment effect in cluster randomized trials with missing\n  outcomes", "comments": "27 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-parametric methods are often used for the estimation of intervention\neffects on correlated outcomes in cluster-randomized trials (CRTs). When\noutcomes are missing at random (MAR), Inverse Probability Weighted (IPW)\nmethods incorporating baseline covariates can be used to deal with informative\nmissingness. Also, augmented generalized estimating equations (AUG) correct for\nimbalance in baseline covariates but need to be extended for MAR outcomes.\nHowever, in the presence of interactions between treatment and baseline\ncovariates, neither method alone produces consistent estimates for the marginal\ntreatment effect if the model for interaction is not correctly specified. We\npropose an AUG-IPW estimator that weights by the inverse of the probability of\nbeing a complete case and allows different outcome models in each intervention\narm. This estimator is doubly robust (DR), it gives correct estimates whether\nthe missing data process or the outcome model is correctly specified. We\nconsider the problem of covariate interference which arises when the outcome of\nan individual may depend on covariates of other individuals. When interfering\ncovariates are not modeled, the DR property prevents bias as long as covariate\ninterference is not present simultaneously for the outcome and the missingness.\nAn R package is developed implementing the proposed method. An extensive\nsimulation study and an application to a CRT of HIV risk reduction-intervention\nin South Africa illustrate the method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 14:13:48 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 17:45:14 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 02:25:59 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2016 16:49:52 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Prague", "Melanie", ""], ["Wang", "Rui", ""], ["Stephens", "Alisa", ""], ["Tchetgen", "Eric Tchetgen", ""], ["DeGruttola", "Victor", ""]]}, {"id": "1507.01825", "submitter": "Erin Gabriel", "authors": "Erin E. Gabriel, Michael J. Daniels, M. Elizabeth Halloran", "title": "Comparing Biomarkers as Trial Level General Surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intermediate response measure that accurately predicts efficacy in a new\nsetting can reduce trial cost and time to product licensure. In this paper, we\ndefine a trial level general surrogate as a trial level intermediate response\nthat accurately predicts trial level clinical responses. Methods for evaluating\ntrial level general surrogates have been developed previously. Many methods in\nthe literature use trial level intermediate responses for prediction. However,\nall existing methods focus on surrogate evaluation and prediction in new\nsettings, rather than comparison of candidate trial level surrogates, and few\nformalize the use of cross validation to quantify the expected prediction\nerror. Our proposed method uses Bayesian non-parametric modeling and\ncross-validation to estimate the absolute prediction error for use in\nevaluating and comparing candidate trial level general surrogates. Simulations\nshow that our method performs well across a variety of scenarios. We use our\nmethod to evaluate and to compare candidate trial level general surrogates in\nseveral multi-national trials of a pentavalent rotavirus vaccine. We identify\ntwo immune measures that have potential value as trial level general surrogates\nand use the measures to predict efficacy in a trial with no clinical outcomes\nmeasured.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 14:34:19 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Gabriel", "Erin E.", ""], ["Daniels", "Michael J.", ""], ["Halloran", "M. Elizabeth", ""]]}, {"id": "1507.01835", "submitter": "Ram\\'on Flores", "authors": "Ram\\'on Flores, Rosa Lillo and Juan Romo", "title": "Homogeneity test for functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of functional data analysis, we propose new two sample tests\nfor homogeneity. Based on some well-known depth measures, we construct four\ndifferent statistics in order to measure distance between the two samples. A\nsimulation study is performed to check the efficiency of the tests when\nconfronted with shape and magnitude perturbation. Finally, we apply these tools\nto measure the homogeneity in some samples of real data, obtaining good results\nusing this new method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 15:06:34 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Flores", "Ram\u00f3n", ""], ["Lillo", "Rosa", ""], ["Romo", "Juan", ""]]}, {"id": "1507.01933", "submitter": "Zhixiang Lin", "authors": "Zhixiang Lin, Tao Wang, Can Yang and Hongyu Zhao", "title": "On Joint Estimation of Gaussian Graphical Models for Spatial and\n  Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we first propose a Bayesian neighborhood selection method to\nestimate Gaussian Graphical Models (GGMs). We show the graph selection\nconsistency of this method in the sense that the posterior probability of the\ntrue model converges to one. When there are multiple groups of data available,\ninstead of estimating the networks independently for each group, joint\nestimation of the networks may utilize the shared information among groups and\nlead to improved estimation for each individual network. Our method is extended\nto jointly estimate GGMs in multiple groups of data with complex structures,\nincluding spatial data, temporal data and data with both spatial and temporal\nstructures. Markov random field (MRF) models are used to efficiently\nincorporate the complex data structures. We develop and implement an efficient\nalgorithm for statistical inference that enables parallel computing. Simulation\nstudies suggest that our approach achieves better accuracy in network\nestimation compared with methods not incorporating spatial and temporal\ndependencies when there are shared structures among the networks, and that it\nperforms comparably well otherwise. Finally, we illustrate our method using the\nhuman brain gene expression microarray dataset, where the expression levels of\ngenes are measured in different brain regions across multiple time periods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 19:55:33 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Lin", "Zhixiang", ""], ["Wang", "Tao", ""], ["Yang", "Can", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1507.02074", "submitter": "Daniel Nevo", "authors": "Daniel Nevo and Ya'acov Ritov", "title": "On Bayesian robust regression with diverging number of predictors", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the robust regression model when the number of predictors\nand the number of observations grow in a similar rate. Theory for M-estimators\nin this regime has been recently developed by several authors [El Karoui et\nal., 2013, Bean et al., 2013, Donoho and Montanari, 2013].\n  Motivated by the inability of M-estimators to successfully estimate the\nEuclidean norm of the coefficient vector, we consider a Bayesian framework for\nthis model. We suggest a two-component mixture of normals prior for the\ncoefficients and develop a Gibbs sampler procedure for sampling from relevant\nposterior distributions, while utilizing a scale mixture of normal\nrepresentation for the error distribution . Unlike M-estimators, the proposed\nBayes estimator is consistent in the Euclidean norm sense. Simulation results\ndemonstrate the superiority of the Bayes estimator over traditional estimation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 09:20:50 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 20:59:56 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Nevo", "Daniel", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1507.02079", "submitter": "Roman Schefzik", "authors": "Roman Schefzik", "title": "A similarity-based implementation of the Schaake shuffle", "comments": null, "journal-ref": null, "doi": "10.1175/MWR-D-15-0227.1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary weather forecasts are typically based on ensemble prediction\nsystems, which consist of multiple runs of numerical weather prediction models\nthat vary with respect to in the initial conditions and/or the the\nparameterization of the atmosphere. Ensemble forecasts are frequently biased\nand show dispersion errors and thus need to be statistically postprocessed.\nHowever, current postprocessing approaches are often univariate and apply to a\nsingle weather quantity at a single location and for a single prediction\nhorizon only, thereby failing to account for potentially crucial dependence\nstructures. Non-parametric multivariate postprocessing methods based on\nempirical copulas, such as ensemble copula coupling or the Schaake shuffle, can\naddress this shortcoming. A specific implementation of the Schaake shuffle,\ncalled the SimSchaake approach, is introduced. The SimSchaake method aggregates\nunivariately postprocessed ensemble forecasts using dependence patterns from\npast observations. Specifically, the observations are taken from historical\ndates at which the ensemble forecasts resembled the current ensemble prediction\nwith respect to a specific similarity criterion. The SimSchaake ensemble\noutperforms all reference ensembles in an application to ensemble forecasts for\nsurface temperature from the European Centre for Medium-Range Weather\nForecasts.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 09:34:45 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Schefzik", "Roman", ""]]}, {"id": "1507.02485", "submitter": "Inder Tecuapetla-G\\'omez", "authors": "Inder Tecuapetla-G\\'omez and Axel Munk", "title": "Autocovariance estimation in regression with a discontinuous signal and\n  $m$-dependent errors: A difference-based approach", "comments": "41 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a class of difference-based estimators for the autocovariance in\nnonparametric regression when the signal is discontinuous (change-point\nregression), possibly highly fluctuating, and the errors form a stationary\n$m$-dependent process. These estimators circumvent the explicit pre-estimation\nof the unknown regression function, a task which is particularly challenging\nfor such signals. We provide explicit expressions for their mean squared errors\nwhen the signal function is piecewise constant (segment regression) and the\nerrors are Gaussian. Based on this we derive biased-optimized estimates which\ndo not depend on the particular (unknown) autocovariance structure. Notably,\nfor positively correlated errors, that part of the variance of our estimators\nwhich depends on the signal is minimal as well. Further, we provide sufficient\nconditions for $\\sqrt{n}$-consistency; this result is extended to piecewise\nHolder regression with non-Gaussian errors.\n  We combine our biased-optimized autocovariance estimates with a\nprojection-based approach and derive covariance matrix estimates, a method\nwhich is of independent interest. Several simulation studies as well as an\napplication to biophysical measurements complement this paper.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 12:44:07 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 18:58:17 GMT"}, {"version": "v3", "created": "Thu, 7 Jul 2016 15:41:00 GMT"}, {"version": "v4", "created": "Mon, 8 Aug 2016 14:47:43 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Tecuapetla-G\u00f3mez", "Inder", ""], ["Munk", "Axel", ""]]}, {"id": "1507.02493", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Michael Jansson, Whitney K. Newey", "title": "Inference in Linear Regression Models with Many Covariates and\n  Heteroskedasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear regression model is widely used in empirical work in Economics,\nStatistics, and many other disciplines. Researchers often include many\ncovariates in their linear model specification in an attempt to control for\nconfounders. We give inference methods that allow for many covariates and\nheteroskedasticity. Our results are obtained using high-dimensional\napproximations, where the number of included covariates are allowed to grow as\nfast as the sample size. We find that all of the usual versions of Eicker-White\nheteroskedasticity consistent standard error estimators for linear models are\ninconsistent under this asymptotics. We then propose a new heteroskedasticity\nconsistent standard error formula that is fully automatic and robust to both\n(conditional)\\ heteroskedasticity of unknown form and the inclusion of possibly\nmany covariates. We apply our findings to three settings: parametric linear\nmodels with many covariates, linear panel models with many fixed effects, and\nsemiparametric semi-linear models with many technical regressors. Simulation\nevidence consistent with our theoretical results is also provided. The proposed\nmethods are also illustrated with an empirical application.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 13:13:47 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 16:25:20 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Newey", "Whitney K.", ""]]}, {"id": "1507.02537", "submitter": "Thomas Opitz", "authors": "Thomas Opitz", "title": "Modeling asymptotically independent spatial extremes based on Laplace\n  random fields", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2016.01.001", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the modeling of threshold exceedances in asymptotically independent\nstochastic processes by constructions based on Laplace random fields. These are\ndefined as Gaussian random fields scaled with a stochastic variable following\nan exponential distribution. This framework yields useful asymptotic properties\nwhile remaining statistically convenient. Univariate distribution tails are of\nthe half exponential type and are part of the limiting generalized Pareto\ndistributions for threshold exceedances. After normalizing marginal tail\ndistributions in data, a standard Laplace field can be used to capture spatial\ndependence among extremes. Asymptotic properties of Laplace fields are explored\nand compared to the classical framework of asymptotic dependence. Multivariate\njoint tail decay rates for Laplace fields are slower than for Gaussian fields\nwith the same covariance structure; hence they provide more conservative\nestimates of very extreme joint risks while maintaining asymptotic\nindependence. Statistical inference is illustrated on extreme wind gusts in the\nNetherlands where a comparison to the Gaussian dependence model shows a better\ngoodness-of-fit in terms of Akaike's criterion. In this specific application we\nfit the well-adapted Weibull distribution as univariate tail model, such that\nthe normalization of univariate tail distributions can be done through a simple\npower transformation of data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 14:52:02 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 10:12:28 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Opitz", "Thomas", ""]]}, {"id": "1507.02608", "submitter": "Preetam Nandy", "authors": "Preetam Nandy, Alain Hauser and Marloes H. Maathuis", "title": "High-dimensional consistency in score-based and hybrid structure\n  learning", "comments": "37 pages, 5 figures, 41 pages supplement (available as an ancillary\n  file)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main approaches for learning Bayesian networks can be classified as\nconstraint-based, score-based or hybrid methods. Although high-dimensional\nconsistency results are available for constraint-based methods like the PC\nalgorithm, such results have not been proved for score-based or hybrid methods,\nand most of the hybrid methods have not even shown to be consistent in the\nclassical setting where the number of variables remains fixed and the sample\nsize tends to infinity. In this paper, we show that consistency of hybrid\nmethods based on greedy equivalence search (GES) can be achieved in the\nclassical setting with adaptive restrictions on the search space that depend on\nthe current state of the algorithm. Moreover, we prove consistency of GES and\nadaptively restricted GES (ARGES) in several sparse high-dimensional settings.\nARGES scales well to sparse graphs with thousands of variables and our\nsimulation study indicates that both GES and ARGES generally outperform the PC\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 17:31:52 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 13:14:02 GMT"}, {"version": "v3", "created": "Mon, 9 May 2016 11:46:51 GMT"}, {"version": "v4", "created": "Mon, 19 Jun 2017 18:47:46 GMT"}, {"version": "v5", "created": "Sat, 4 Nov 2017 20:36:18 GMT"}, {"version": "v6", "created": "Sat, 3 Feb 2018 17:38:36 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Nandy", "Preetam", ""], ["Hauser", "Alain", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1507.02646", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, Jonah Gabry", "title": "Pareto Smoothed Importance Sampling", "comments": "Minor revision: fixed some typos and updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance weighting is a general way to adjust Monte Carlo integration to\naccount for draws from the wrong distribution, but the resulting estimate can\nbe noisy when the importance ratios have a heavy right tail. This routinely\noccurs when there are aspects of the target distribution that are not well\ncaptured by the approximating distribution, in which case more stable estimates\ncan be obtained by modifying extreme importance ratios. We present a new method\nfor stabilizing importance weights using a generalized Pareto distribution fit\nto the upper tail of the distribution of the simulated importance ratios. The\nmethod, which empirically performs better than existing methods for stabilizing\nimportance sampling estimates, includes stabilized effective sample size\nestimates, Monte Carlo error estimates and convergence diagnostics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 18:43:28 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 18:37:17 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 11:30:37 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 08:34:49 GMT"}, {"version": "v5", "created": "Sat, 21 Oct 2017 08:37:46 GMT"}, {"version": "v6", "created": "Tue, 2 Jul 2019 13:56:16 GMT"}, {"version": "v7", "created": "Tue, 23 Feb 2021 10:07:05 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Vehtari", "Aki", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""], ["Yao", "Yuling", ""], ["Gabry", "Jonah", ""]]}, {"id": "1507.02741", "submitter": "Harrison Quick", "authors": "Harrison Quick and Lance A. Waller and Michele Casper", "title": "A Nonseparable Multivariate Space-Time Model for Analyzing County-Level\n  Heart Disease Death Rates by Race and Gender", "comments": null, "journal-ref": "Journal of the Royal Statistical Society; Series C, 67 (2018)\n  291-304", "doi": "10.1111/rssc.12215", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While death rates due to diseases of the heart have experienced a sharp\ndecline over the past 50 years, these diseases continue to be the leading cause\nof death in the United States, and the rate of decline varies by geographic\nlocation, race, and gender. We look to harness the power of hierarchical\nBayesian methods to obtain a clearer picture of the declines from county-level,\ntemporally varying heart disease death rates for men and women of different\nraces in the US. Specifically, we propose a nonseparable multivariate\nspatio-temporal Bayesian model which allows for group-specific temporal\ncorrelations and temporally-evolving covariance structures in the multivariate\nspatio-temporal component of the model. After verifying the effectiveness of\nour model via simulation, we apply our model to a dataset of over 200,000\ncounty-level heart disease death rates. In addition to yielding a superior fit\nthan other common approaches for handling such data, the richness of our model\nprovides insight into racial, gender, and geographic disparities underlying\nheart disease death rates in the US which are not permitted by more restrictive\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 23:23:43 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Quick", "Harrison", ""], ["Waller", "Lance A.", ""], ["Casper", "Michele", ""]]}, {"id": "1507.02751", "submitter": "Nina Golyandina E.", "authors": "Nikita Zvonarev and Nina Golyandina", "title": "Iterative algorithms for weighted and unweighted finite-rank time-series\n  approximations", "comments": null, "journal-ref": "Statistics and Its Interface, 2017, V.10, N1, P.5-18", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of time series approximation by series of finite rank is\nconsidered from the viewpoint of signal extraction. For signal estimation, a\nweighted least-squares method is applied to the trajectory matrix of the\nconsidered time series. Matrix weights are chosen to obtain equal or\napproximately equal weights in the equivalent problem of time-series\nleast-squares approximation. Several new methods are suggested and examined\ntogether with the Cadzow's iterative method. The questions of convergence,\ncomputational complexity, and accuracy are considered for the proposed methods.\nThe methods are compared on numeric examples.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 00:12:03 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Zvonarev", "Nikita", ""], ["Golyandina", "Nina", ""]]}, {"id": "1507.02971", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Mattias Villani and Robert Kohn", "title": "Scalable MCMC for Large Data Problems using Data Subsampling and the\n  Difference Estimator", "comments": "The content in this paper is now in arXiv:1404.4178, as a result of a\n  major revision of that paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic Markov Chain Monte Carlo (MCMC) algorithm to speed up\ncomputations for datasets with many observations. A key feature of our approach\nis the use of the highly efficient difference estimator from the survey\nsampling literature to estimate the log-likelihood accurately using only a\nsmall fraction of the data. Our algorithm improves on the $O(n)$ complexity of\nregular MCMC by operating over local data clusters instead of the full sample\nwhen computing the likelihood. The likelihood estimate is used in a\nPseudo-marginal framework to sample from a perturbed posterior which is within\n$O(m^{-1/2})$ of the true posterior, where $m$ is the subsample size. The\nmethod is applied to a logistic regression model to predict firm bankruptcy for\na large data set. We document a significant speed up in comparison to the\nstandard MCMC on the full dataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 17:10:33 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 11:49:19 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 00:33:01 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Quiroz", "Matias", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""]]}, {"id": "1507.03063", "submitter": "Panos Toulis", "authors": "Panos Toulis, David C. Parkes, Elery Pfeffer, James Zou", "title": "Incentive-Compatible Experimental Design", "comments": null, "journal-ref": null, "doi": "10.1145/2764468.2764525", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of experiments to evaluate treatments that are\nadministered by self-interested agents, each seeking to achieve the highest\nevaluation and win the experiment. For example, in an advertising experiment, a\ncompany wishes to evaluate two marketing agents in terms of their efficacy in\nviral marketing, and assign a contract to the winner agent. Contrary to\ntraditional experimental design, this problem has two new implications. First,\nthe experiment induces a game among agents, where each agent can select from\nmultiple versions of the treatment it administers. Second, the action of one\nagent -- selection of treatment version -- may affect the actions of another\nagent, with the resulting strategic interference complicating the evaluation of\nagents. An incentive-compatible experiment design is one with an equilibrium\nwhere each agent selects its natural action, which is the action that would\nmaximize the performance of the agent if there was no competition (e.g.,\nexpected number of conversions if agent was assigned the contract). Under a\ngeneral formulation of experimental design, we identify sufficient conditions\nthat guarantee incentive-compatible experiments. These conditions rely on the\nexistence of statistics that can estimate how agents would perform without\ncompetition, and their use in constructing score functions to evaluate the\nagents. In the setting with no strategic interference, we also study the power\nof the design, i.e., the probability that the best agent wins, and show how to\nimprove the power of incentive-compatible designs. From the technical side, our\ntheory uses a range of statistical methods such as hypothesis testing,\nvariance-stabilizing transformations and the Delta method, all of which rely on\nasymptotics.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 03:41:20 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Toulis", "Panos", ""], ["Parkes", "David C.", ""], ["Pfeffer", "Elery", ""], ["Zou", "James", ""]]}, {"id": "1507.03130", "submitter": "Yun Yang", "authors": "Yun Yang and Surya Tokdar", "title": "Joint estimation of quantile planes over arbitrary predictor spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the recent surge of interest in quantile regression, joint\nestimation of linear quantile planes remains a great challenge in statistics\nand econometrics. We propose a novel parametrization that characterizes any\ncollection of non-crossing quantile planes over arbitrarily shaped convex\npredictor domains in any dimension by means of unconstrained scalar, vector and\nfunction valued parameters. Statistical models based on this parametrization\ninherit a fast computation of the likelihood function, enabling penalized\nlikelihood or Bayesian approaches to model fitting. We introduce a complete\nBayesian methodology by using Gaussian process prior distributions on the\nfunction valued parameters and develop a robust and efficient Markov chain\nMonte Carlo parameter estimation. The resulting method is shown to offer\nposterior consistency under mild tail and regularity conditions. We present\nseveral illustrative examples where the new method is compared against existing\napproaches and is found to offer better accuracy, coverage and model fit.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 17:30:04 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Yang", "Yun", ""], ["Tokdar", "Surya", ""]]}, {"id": "1507.03133", "submitter": "Rahul Mazumder", "authors": "Dimitris Bertsimas, Angela King and Rahul Mazumder", "title": "Best Subset Selection via a Modern Optimization Lens", "comments": "This is a revised version (May, 2015) of the first submission in June\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last twenty-five years (1990-2014), algorithmic advances in integer\noptimization combined with hardware improvements have resulted in an\nastonishing 200 billion factor speedup in solving Mixed Integer Optimization\n(MIO) problems. We present a MIO approach for solving the classical best subset\nselection problem of choosing $k$ out of $p$ features in linear regression\ngiven $n$ observations. We develop a discrete extension of modern first order\ncontinuous optimization methods to find high quality feasible solutions that we\nuse as warm starts to a MIO solver that finds provably optimal solutions. The\nresulting algorithm (a) provides a solution with a guarantee on its\nsuboptimality even if we terminate the algorithm early, (b) can accommodate\nside constraints on the coefficients of the linear regression and (c) extends\nto finding best subset solutions for the least absolute deviation loss\nfunction. Using a wide variety of synthetic and real datasets, we demonstrate\nthat our approach solves problems with $n$ in the 1000s and $p$ in the 100s in\nminutes to provable optimality, and finds near optimal solutions for $n$ in the\n100s and $p$ in the 1000s in minutes. We also establish via numerical\nexperiments that the MIO approach performs better than {\\texttt {Lasso}} and\nother popularly used sparse learning procedures, in terms of achieving sparse\nsolutions with good predictive power.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 18:19:27 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["King", "Angela", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1507.03157", "submitter": "Sumit Kumar Ram", "authors": "Sumit Kumar Ram and Marta Molinas", "title": "Entropic Empirical Mode Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Empirical Mode Decomposition(EMD) is an adaptive data analysis technique for\nanalyzing nonlinear and nonstationary data[1]. EMD decomposes the original data\ninto a number of Intrinsic Mode Functions(IMFs)[1] for giving better physical\ninsight of the data. Permutation Entropy(PE) is a complexity measure[3]\nfunction which is widely used in the field of complexity theory for analyzing\nthe local complexity of time series. In this paper we are combining the\nconcepts of PE and EMD to resolve the mode mixing problem observed in\ndetermination of IMFs.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 21:24:46 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 18:43:25 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 13:52:05 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Ram", "Sumit Kumar", ""], ["Molinas", "Marta", ""]]}, {"id": "1507.03159", "submitter": "Alireza Mahani", "authors": "Alireza S. Mahani, Mansour T.A. Sharabiani", "title": "Combining matching and linear regression: Introducing a mathematical\n  framework and software for simulations, diagnostics and calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining matching and regression for causal inference provides\ndouble-robustness in removing treatment effect estimation bias due to\nconfounding variables. In most real-world applications, however, treatment and\ncontrol populations are not large enough for matching to achieve perfect or\nnear-perfect balance on all confounding variables and their\nnonlinear/interaction functions, leading to trade-offs. [this fact is\nindependent of regression, so a bit disjointed from first sentence.]\nFurthermore, variance is as important of a contributor as bias towards total\nerror in small samples, and must therefore be factored into the methodological\ndecisions. In this paper, we develop a mathematical framework for quantifying\nthe combined impact of matching and linear regression on bias and variance of\ntreatment effect estimation. The framework includes expressions for bias and\nvariance in a misspecified linear regression, theorems regarding impact of\nmatching on bias and variance, and a constrained bias estimation approach for\nquantifying misspecification bias and combining it with variance to arrive at\ntotal error. Methodological decisions can thus be based on minimization of this\ntotal error, given the practitioner's assumption/belief about an intuitive\nparameter, which we call `omitted R-squared'. The proposed methodology excludes\nthe outcome variable from analysis, thereby avoiding overfit creep and making\nit suitable for observational study designs. All core functions for bias and\nvariance calculation, as well as diagnostic tools for bias-variance trade-off\nanalysis, matching calibration, and power analysis are made available to\nresearchers and practitioners through an open-source R library, MatchLinReg.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 22:01:56 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Mahani", "Alireza S.", ""], ["Sharabiani", "Mansour T. A.", ""]]}, {"id": "1507.03496", "submitter": "Jos\\'e Luis Torrecilla", "authors": "Jos\\'e R. Berrendero, Antonio Cuevas and Jos\\'e L. Torrecilla", "title": "The mRMR variable selection method: a comparative study for functional\n  data", "comments": null, "journal-ref": "Journal of Statistical Computation and Simulation Volume 86, Issue\n  5, 2016", "doi": "10.1080/00949655.2015.1042378", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of variable selection methods is particularly appealing in\nstatistical problems with functional data. The obvious general criterion for\nvariable selection is to choose the `most representative' or `most relevant'\nvariables. However, it is also clear that a purely relevance-oriented criterion\ncould lead to select many redundant variables. The mRMR (minimum Redundance\nMaximum Relevance) procedure, proposed by Ding and Peng (2005) and Peng et al.\n(2005) is an algorithm to systematically perform variable selection, achieving\na reasonable trade-off between relevance and redundancy. In its original form,\nthis procedure is based on the use of the so-called mutual information\ncriterion to assess relevance and redundancy. Keeping the focus on functional\ndata problems, we propose here a modified version of the mRMR method, obtained\nby replacing the mutual information by the new association measure (called\ndistance correlation) suggested by Sz\\'ekely et al. (2007). We have also\nperformed an extensive simulation study, including 1600 functional experiments\n(100 functional models $\\times$ 4 sample sizes $\\times$ 4 classifiers) and\nthree real-data examples aimed at comparing the different versions of the mRMR\nmethodology. The results are quite conclusive in favor of the new proposed\nalternative.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 15:31:26 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Berrendero", "Jos\u00e9 R.", ""], ["Cuevas", "Antonio", ""], ["Torrecilla", "Jos\u00e9 L.", ""]]}, {"id": "1507.03833", "submitter": "Shih-Kang Chao", "authors": "Shih-Kang Chao, Wolfgang Karl H\\\"ardle, Ming Yuan", "title": "Factorisable Multitask Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multivariate quantile regression model with a factor structure is proposed\nto study data with many responses of interest. The factor structure is allowed\nto vary with the quantile levels, which makes our framework more flexible than\nthe classical factor models. The model is estimated with the nuclear norm\nregularization in order to accommodate the high dimensionality of data, but the\nincurred optimization problem can only be efficiently solved in an approximate\nmanner by off-the-shelf optimization methods. Such a scenario is often seen\nwhen the empirical risk is non-smooth or the numerical procedure involves\nexpensive subroutines such as singular value decomposition. To ensure that the\napproximate estimator accurately estimates the model, non-asymptotic bounds on\nerror of the the approximate estimator is established. For implementation, a\nnumerical procedure that provably marginalizes the approximate error is\nproposed. The merits of our model and the proposed numerical procedures are\ndemonstrated through Monte Carlo experiments and an application to finance\ninvolving a large pool of asset returns.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 12:59:33 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 21:41:25 GMT"}, {"version": "v3", "created": "Sat, 18 Jan 2020 17:25:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chao", "Shih-Kang", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Yuan", "Ming", ""]]}, {"id": "1507.04331", "submitter": "Heather Harrington", "authors": "Elizabeth Gross, Brent Davis, Kenneth L. Ho, Daniel J. Bates, Heather\n  A. Harrington", "title": "Numerical algebraic geometry for model selection and its application to\n  the life sciences", "comments": "References added, additional clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM math.AG math.NA q-bio.MN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers working with mathematical models are often confronted by the\nrelated problems of parameter estimation, model validation, and model\nselection. These are all optimization problems, well-known to be challenging\ndue to non-linearity, non-convexity and multiple local optima. Furthermore, the\nchallenges are compounded when only partial data is available. Here, we\nconsider polynomial models (e.g., mass-action chemical reaction networks at\nsteady state) and describe a framework for their analysis based on optimization\nusing numerical algebraic geometry. Specifically, we use probability-one\npolynomial homotopy continuation methods to compute all critical points of the\nobjective function, then filter to recover the global optima. Our approach\nexploits the geometric structures relating models and data, and we demonstrate\nits utility on examples from cell signaling, synthetic biology, and\nepidemiology.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 19:12:52 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 18:51:39 GMT"}, {"version": "v3", "created": "Fri, 1 Apr 2016 19:29:19 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Gross", "Elizabeth", ""], ["Davis", "Brent", ""], ["Ho", "Kenneth L.", ""], ["Bates", "Daniel J.", ""], ["Harrington", "Heather A.", ""]]}, {"id": "1507.04398", "submitter": "Jos\\'e Luis Torrecilla", "authors": "Jos\\'e R. Berrendero, Antonio Cuevas and Jos\\'e L. Torrecilla", "title": "On the use of reproducing kernel Hilbert spaces in functional\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The H\\'ajek-Feldman dichotomy establishes that two Gaussian measures are\neither mutually absolutely continuous with respect to each other (and hence\nthere is a Radon-Nikodym density for each measure with respect to the other\none) or mutually singular. Unlike the case of finite dimensional Gaussian\nmeasures, there are non-trivial examples of both situations when dealing with\nGaussian stochastic processes. This paper provides:\n  (a) Explicit expressions for the optimal (Bayes) rule and the minimal\nclassification error probability in several relevant problems of supervised\nbinary classification of mutually absolutely continuous Gaussian processes. The\napproach relies on some classical results in the theory of Reproducing Kernel\nHilbert Spaces (RKHS).\n  (b) An interpretation, in terms of mutual singularity, for the \"near perfect\nclassification\" phenomenon described by Delaigle and Hall (2012). We show that\nthe asymptotically optimal rule proposed by these authors can be identified\nwith the sequence of optimal rules for an approximating sequence of\nclassification problems in the absolutely continuous case.\n  (c) A new model-based method for variable selection in binary classification\nproblems, which arises in a very natural way from the explicit knowledge of the\nRN-derivatives and the underlying RKHS structure. Different classifiers might\nbe used from the selected variables. In particular, the classical, linear\nfinite-dimensional Fisher rule turns out to be consistent under some standard\nconditions on the underlying functional model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 21:43:09 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 19:06:25 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 16:04:50 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Berrendero", "Jos\u00e9 R.", ""], ["Cuevas", "Antonio", ""], ["Torrecilla", "Jos\u00e9 L.", ""]]}, {"id": "1507.04415", "submitter": "Guido Kuersteiner", "authors": "Jinyong Hahn, Guido Kuersteiner and Maurizio Mazzocco", "title": "Estimation with Aggregate Shocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregate shocks affect most households' and firms' decisions. Using three\nstylized models we show that inference based on cross-sectional data alone\ngenerally fails to correctly account for decision making of rational agents\nfacing aggregate uncertainty. We propose an econometric framework that\novercomes these problems by explicitly parameterizing the agents' inference\nproblem relative to aggregate shocks. Our framework and examples illustrate\nthat the cross-sectional and time-series aspects of the model are often\ninterdependent. Therefore, estimation of model parameters in the presence of\naggregate shocks requires the combined use of cross-sectional and time series\ndata. We provide easy-to-use formulas for test statistics and confidence\nintervals that account for the interaction between the cross-sectional and\ntime-series variation. Lastly, we perform Monte Carlo simulations that\nhighlight the properties of the proposed method and the risks of not properly\naccounting for the presence of aggregate shocks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 00:06:59 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 00:20:43 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 22:50:28 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Hahn", "Jinyong", ""], ["Kuersteiner", "Guido", ""], ["Mazzocco", "Maurizio", ""]]}, {"id": "1507.04528", "submitter": "Ilaria Bianchini", "authors": "Raffaele Argiento, Ilaria Bianchini and Alessandra Guglielmi", "title": "A priori truncation method for posterior sampling from homogeneous\n  normalized completely random measure mixture models", "comments": "32 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adopts a Bayesian nonparametric mixture model where the mixing\ndistribution belongs to the wide class of normalized homogeneous completely\nrandom measures. We propose a truncation method for the mixing distribution by\ndiscarding the weights of the unnormalized measure smaller than a threshold. We\nprove convergence in law of our approximation, provide some theoretical\nproperties and characterize its posterior distribution so that a blocked Gibbs\nsampler is devised. The versatility of the approximation is illustrated by two\ndifferent applications. In the first the normalized Bessel random measure,\nencompassing the Dirichlet process, is introduced; goodness of fit indexes show\nits good performances as mixing measure for density estimation. The second\ndescribes how to incorporate covariates in the support of the normalized\nmeasure, leading to a linear dependent model for regression and clustering.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 11:18:01 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Argiento", "Raffaele", ""], ["Bianchini", "Ilaria", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "1507.04544", "submitter": "Aki Vehtari", "authors": "Aki Vehtari and Andrew Gelman and Jonah Gabry", "title": "Practical Bayesian model evaluation using leave-one-out cross-validation\n  and WAIC", "comments": null, "journal-ref": "Statistics and Computing, 2017, Volume 27, Issue 5, pp 1413-1432", "doi": "10.1007/s11222-016-9696-4", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leave-one-out cross-validation (LOO) and the widely applicable information\ncriterion (WAIC) are methods for estimating pointwise out-of-sample prediction\naccuracy from a fitted Bayesian model using the log-likelihood evaluated at the\nposterior simulations of the parameter values. LOO and WAIC have various\nadvantages over simpler estimates of predictive error such as AIC and DIC but\nare less used in practice because they involve additional computational steps.\nHere we lay out fast and stable computations for LOO and WAIC that can be\nperformed using existing simulation draws. We introduce an efficient\ncomputation of LOO using Pareto-smoothed importance sampling (PSIS), a new\nprocedure for regularizing importance weights. Although WAIC is asymptotically\nequal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case\nwith weak priors or influential observations. As a byproduct of our\ncalculations, we also obtain approximate standard errors for estimated\npredictive errors and for comparing of predictive errors between two models. We\nimplement the computations in an R package called 'loo' and demonstrate using\nmodels fit with the Bayesian inference package Stan.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 12:25:54 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 10:28:11 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 17:39:03 GMT"}, {"version": "v4", "created": "Mon, 11 Jul 2016 14:58:15 GMT"}, {"version": "v5", "created": "Mon, 12 Sep 2016 18:34:18 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""], ["Gabry", "Jonah", ""]]}, {"id": "1507.04610", "submitter": "Adam Rothman", "authors": "Aaron J. Molstad and Adam J. Rothman", "title": "Indirect multivariate response linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of estimators of the multivariate response linear\nregression coefficient matrix that exploits the assumption that the response\nand predictors have a joint multivariate Normal distribution. This allows us to\nindirectly estimate the regression coefficient matrix through shrinkage\nestimation of the parameters of the inverse regression, or the conditional\ndistribution of the predictors given the responses. We establish a convergence\nrate bound for estimators in our class and we study two examples. The first\nexample estimator exploits an assumption that the inverse regression's\ncoefficient matrix is sparse. The second example estimator exploits an\nassumption that the inverse regression's coefficient matrix is rank deficient.\nThese estimators do not require the popular assumption that the forward\nregression coefficient matrix is sparse or has small Frobenius norm. Using\nsimulation studies, we show that our example estimators outperform relevant\ncompetitors for some data generating models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 15:12:09 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1507.04789", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss", "title": "A multi-resolution approximation for massive spatial datasets", "comments": "23 pages; to be published in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": "10.1080/01621459.2015.1123632", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated sensing instruments on satellites and aircraft have enabled the\ncollection of massive amounts of high-resolution observations of spatial fields\nover large spatial regions. If these datasets can be efficiently exploited,\nthey can provide new insights on a wide variety of issues. However, traditional\nspatial-statistical techniques such as kriging are not computationally feasible\nfor big datasets. We propose a multi-resolution approximation (M-RA) of\nGaussian processes observed at irregular locations in space. The M-RA process\nis specified as a linear combination of basis functions at multiple levels of\nspatial resolution, which can capture spatial structure from very fine to very\nlarge scales. The basis functions are automatically chosen to approximate a\ngiven covariance function, which can be nonstationary. All computations\ninvolving the M-RA, including parameter inference and prediction, are highly\nscalable for massive datasets. Crucially, the inference algorithms can also be\nparallelized to take full advantage of large distributed-memory computing\nenvironments. In comparisons using simulated data and a large satellite\ndataset, the M-RA outperforms a related state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 22:44:56 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 19:04:25 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Katzfuss", "Matthias", ""]]}, {"id": "1507.04905", "submitter": "Antonio D'Ambrosio Dr.", "authors": "Carmela Iorio, Gianluca Frasso, Antonio D'Ambrosio and Roberta\n  Siciliano", "title": "Boosted-Oriented Probabilistic Smoothing-Spline Clustering of Series", "comments": null, "journal-ref": null, "doi": null, "report-no": "STAD_REPORT_01_2015", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy clustering methods allow the objects to belong to several clusters\nsimultaneously, with different degrees of membership. However, a factor that\ninfluences the performance of fuzzy algorithms is the value of fuzzifier\nparameter. In this paper, we propose a fuzzy clustering procedure for data\n(time) series that does not depend on the definition of a fuzzifier parameter.\nIt comes from two approaches, theoretically motivated for unsupervised and\nsupervised classification cases, respectively. The first is the Probabilistic\nDistance (PD) clustering procedure. The second is the well known Boosting\nphilosophy. Our idea is to adopt a boosting prospective for unsupervised\nlearning problems, in particular we face with non hierarchical clustering\nproblems. The aim is to assign each instance (i.e. a series) of a data set to a\ncluster. We assume the representative instance of a given cluster (i.e. the\ncluster center) as a target instance, a loss function as a synthetic index of\nthe global performance and the probability of each instance to belong to a\ngiven cluster as the individual contribution of a given instance to the overall\nsolution. The global performance of the proposed method is investigated by\nvarious experiments.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:18:40 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 10:00:00 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Iorio", "Carmela", ""], ["Frasso", "Gianluca", ""], ["D'Ambrosio", "Antonio", ""], ["Siciliano", "Roberta", ""]]}, {"id": "1507.05021", "submitter": "Alain Durmus", "authors": "Alain Durmus (LTCI), Eric Moulines (CMAP)", "title": "Non-asymptotic convergence analysis for the Unadjusted Langevin\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a method to sample from a target distribution $\\pi$\nover $\\mathbb{R}^d$ having a positive density with respect to the Lebesgue\nmeasure, known up to a normalisation factor. This method is based on the Euler\ndiscretization of the overdamped Langevin stochastic differential equation\nassociated with $\\pi$. For both constant and decreasing step sizes in the Euler\ndiscretization, we obtain non-asymptotic bounds for the convergence to the\ntarget distribution $\\pi$ in total variation distance. A particular attention\nis paid to the dependency on the dimension $d$, to demonstrate the\napplicability of this method in the high dimensional setting. These bounds\nimprove and extend the results of (Dalalyan 2014).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 16:23:23 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 17:07:11 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 14:27:42 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Durmus", "Alain", "", "LTCI"], ["Moulines", "Eric", "", "CMAP"]]}, {"id": "1507.05128", "submitter": "Minyong Lee", "authors": "Minyong R. Lee, Art B. Owen", "title": "Single Nugget Kriging", "comments": null, "journal-ref": "Statistica Sinica 28 (2018), 649-669", "doi": "10.5705/ss.202016.0255", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method with better predictions at extreme values than the\nstandard method of Kriging. We construct our predictor in two ways: by\npenalizing the mean squared error through conditional bias and by penalizing\nthe conditional likelihood at the target function value. Our prediction\nexhibits robustness to the model mismatch in the covariance parameters, a\ndesirable feature for computer simulations with a restricted number of data\npoints. Applications on several functions show that our predictor is robust to\nthe non-Gaussianity of the function.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 22:46:59 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 03:36:11 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Lee", "Minyong R.", ""], ["Owen", "Art B.", ""]]}, {"id": "1507.05135", "submitter": "Jeng-Min Chiou", "authors": "Jane-Ling Wang, Jeng-Min Chiou, and Hans-Georg Mueller", "title": "Review of Functional Data Analysis", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of modern technology, more and more data are being recorded\ncontinuously during a time interval or intermittently at several discrete time\npoints. They are both examples of \"functional data\", which have become a\nprevailing type of data. Functional Data Analysis (FDA) encompasses the\nstatistical methodology for such data. Broadly interpreted, FDA deals with the\nanalysis and theory of data that are in the form of functions. This paper\nprovides an overview of FDA, starting with simple statistical notions such as\nmean and covariance functions, then covering some core techniques, the most\npopular of which is Functional Principal Component Analysis (FPCA). FPCA is an\nimportant dimension reduction tool and in sparse data situations can be used to\nimpute functional data that are sparsely observed. Other dimension reduction\napproaches are also discussed. In addition, we review another core technique,\nfunctional linear regression, as well as clustering and classification of\nfunctional data. Beyond linear and single or multiple index methods we touch\nupon a few nonlinear approaches that are promising for certain applications.\nThey include additive and other nonlinear functional regression models, such as\ntime warping, manifold learning, and dynamic modeling with empirical\ndifferential equations. The paper concludes with a brief discussion of future\ndirections.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 00:10:16 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Wang", "Jane-Ling", ""], ["Chiou", "Jeng-Min", ""], ["Mueller", "Hans-Georg", ""]]}, {"id": "1507.05179", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Hiromasa Tamae, Tatsuya Kubokawa", "title": "Bayesian Estimators for Small Area Models Shrinking Both Means and\n  Variances", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, 44, 150-167 (2017)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For small area estimation of area-level data, the Fay-Herriot model is\nextensively used as a model based method. In the Fay-Herriot model, it is\nconventionally assumed that the sampling variances are known whereas estimators\nof sampling variances are used in practice. Thus, the settings of knowing\nsampling variances are unrealistic and several methods are proposed to overcome\nthis problem. In this paper, we assume the situation where the direct\nestimators of the sampling variances are available as well as the sample means.\nUsing these information, we propose a Bayesian yet objective method producing\nshrinkage estimation of both means and variances in the Fay-Herriot model. We\nconsider the hierarchical structure for the sampling variances and we set\nuniform prior on model parameters to keep objectivity of the proposed model.\nFor validity of the posterior inference, we show under mild conditions that the\nposterior distribution is proper and has finite variances. We investigate the\nnumerical performance through simulation and empirical studies.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 12:36:28 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 05:05:15 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 04:13:51 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Tamae", "Hiromasa", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1507.05263", "submitter": "Domenico Ciuonzo", "authors": "Domenico Ciuonzo, Antonio De Maio, Danilo Orlando", "title": "A Unifying Framework for Adaptive Radar Detection in Homogeneous plus\n  Structured Interference-Part I: On the Maximal Invariant Statistic", "comments": "Submitted for journal publication", "journal-ref": "IEEE Transactions on Signal Processing, vol. 64, no. 11, pp.\n  2894-2906, Jun. 2016", "doi": "10.1109/TSP.2016.2519003", "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of adaptive multidimensional/multichannel\nsignal detection in homogeneous Gaussian disturbance with unknown covariance\nmatrix and structured deterministic interference. The aforementioned problem\ncorresponds to a generalization of the well-known Generalized Multivariate\nAnalysis of Variance (GMANOVA). In this first part of the work, we formulate\nthe considered problem in canonical form and, after identifying a desirable\ngroup of transformations for the considered hypothesis testing, we derive a\nMaximal Invariant Statistic (MIS) for the problem at hand. Furthermore, we\nprovide the MIS distribution in the form of a stochastic representation.\nFinally, strong connections to the MIS obtained in the open literature in\nsimpler scenarios are underlined.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 09:00:45 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Ciuonzo", "Domenico", ""], ["De Maio", "Antonio", ""], ["Orlando", "Danilo", ""]]}, {"id": "1507.05266", "submitter": "Domenico Ciuonzo", "authors": "Domenico Ciuonzo, Antonio De Maio, Danilo Orlando", "title": "A Unifying Framework for Adaptive Radar Detection in Homogeneous plus\n  Structured Interference-Part II: Detectors Design", "comments": "Submitted for journal publication", "journal-ref": "IEEE Transactions on Signal Processing, vol. 64, no. 11, pp.\n  2907-2919, Jun. 2016", "doi": "10.1109/TSP.2016.2519005", "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of adaptive multidimensional/multichannel\nsignal detection in homogeneous Gaussian disturbance with unknown covariance\nmatrix and structured (unknown) deterministic interference. The aforementioned\nproblem extends the well-known Generalized Multivariate Analysis of Variance\n(GMANOVA) tackled in the open literature. In a companion paper, we have\nobtained the Maximal Invariant Statistic (MIS) for the problem under\nconsideration, as an enabling tool for the design of suitable detectors which\npossess the Constant False-Alarm Rate (CFAR) property. Herein, we focus on the\ndevelopment of several theoretically-founded detectors for the problem under\nconsideration. First, all the considered detectors are shown to be function of\nthe MIS, thus proving their CFARness property. Secondly, coincidence or\nstatistical equivalence among some of them in such a general signal model is\nproved. Thirdly, strong connections to well-known simpler scenarios found in\nadaptive detection literature are established. Finally, simulation results are\nprovided for a comparison of the proposed receivers.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 09:23:10 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Ciuonzo", "Domenico", ""], ["De Maio", "Antonio", ""], ["Orlando", "Danilo", ""]]}, {"id": "1507.05286", "submitter": "Nina Golyandina E.", "authors": "Nina Golyandina and Alex Shlemov", "title": "Semi-nonparametric singular spectrum analysis with projection", "comments": null, "journal-ref": "Statistics and Its Interface, 2017, V.10, N1, P.47-57", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular spectrum analysis (SSA) is considered for decomposition of time\nseries into identifiable components. The Basic SSA method is nonparametric and\nconstructs an adaptive expansion based on singular value decomposition. The\ninvestigated modification is able to take into consideration a structure given\nin advance and therefore can be called semi-nonparametric. The approach called\nSSA with projection includes preliminary projections of rows and columns of the\nseries trajectory matrix to given subspaces. One application of SSA with\nprojection is the extraction of polynomial trends, e.g., a linear trend. It is\nshown that SSA with projection can extract polynomial trends much better than\nBasic SSA, especially for linear trends. Numerical examples including\ncomparison with the least-square approach to polynomial regression are\npresented.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 13:39:02 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Golyandina", "Nina", ""], ["Shlemov", "Alex", ""]]}, {"id": "1507.05315", "submitter": "Ulrike Schneider", "authors": "Karl Ewald and Ulrike Schneider", "title": "Uniformly Valid Confidence Sets Based on the Lasso", "comments": "Some typos corrected, updated references", "journal-ref": "Electron. J. Statist. 12 (2018), 1358-1387", "doi": "10.1214/18-EJS1425", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a linear regression model of fixed dimension $p \\leq n$, we construct\nconfidence regions for the unknown parameter vector based on the Lasso\nestimator that uniformly and exactly hold the prescribed in finite samples as\nwell as in an asymptotic setup. We thereby quantify estimation uncertainty as\nwell as the \"post-model selection error\" of this estimator. More concretely, in\nfinite samples with Gaussian errors and asymptotically in the case where the\nLasso estimator is tuned to perform conservative model selection, we derive\nexact formulas for computing the minimal coverage probability over the entire\nparameter space for a large class of shapes for the confidence sets, thus\nenabling the construction of valid confidence regions based on the Lasso\nestimator in these settings. The choice of shape for the confidence sets and\ncomparison with the confidence ellipse based on the least-squares estimator is\nalso discussed. Moreover, in the case where the Lasso estimator is tuned to\nenable consistent model selection, we give a simple confidence region with\nminimal coverage probability converging to one. Finally, we also treat the case\nof unknown error variance and present some ideas for extensions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 18:20:05 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 13:19:11 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 13:10:02 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Ewald", "Karl", ""], ["Schneider", "Ulrike", ""]]}, {"id": "1507.05366", "submitter": "Hau-tieng Wu", "authors": "Ingrid Daubechies, Yi Wang, Hau-tieng Wu", "title": "ConceFT: Concentration of Frequency and Time via a multitapered\n  synchrosqueezed transform", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2015.0193", "report-no": null, "categories": "math.ST cs.NA stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to determine the time-frequency content of\ntime-dependent signals consisting of multiple oscillatory components, with\ntime-varying amplitudes and instantaneous frequencies. Numerical experiments as\nwell as a theoretical analysis are presented to assess its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:05:14 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Daubechies", "Ingrid", ""], ["Wang", "Yi", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1507.05372", "submitter": "Hau-tieng Wu", "authors": "Yu-Ting Lin, Patrick Flandrin, Hau-tieng Wu", "title": "When interpolation-induced reflection artifact meets time-frequency\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While extracting the temporal dynamical features based on the time-frequency\nanalyses, like the reassignment and synchrosqueezing transform, attracts more\nand more interest in bio-medical data analysis, we should be careful about\nartifacts generated by interpolation schemes, in particular when the sampling\nrate is not significantly higher than the frequency of the oscillatory\ncomponent we are interested in. In this study, we formulate the problem called\nthe reflection effect and provide a theoretical justification of the statement.\nWe also show examples in the anesthetic depth analysis with clear but\nundesirable artifacts. The results show that the artifact associated with the\nreflection effect exists not only theoretically but practically. Its influence\nis pronounced when we apply the time-frequency analyses to extract the\ntime-varying dynamics hidden inside the signal. In conclusion, we have to\ncarefully deal with the artifact associated with the reflection effect by\nchoosing a proper interpolation scheme.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:47:36 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 03:26:46 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Lin", "Yu-Ting", ""], ["Flandrin", "Patrick", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1507.05529", "submitter": "Harrison Quick", "authors": "Harrison Quick, Scott H. Holan, and Christopher K. Wikle", "title": "Generating Partially Synthetic Geocoded Public Use Data with Decreased\n  Disclosure Risk Using Differential Smoothing", "comments": null, "journal-ref": "Journal of the Royal Statistical Society Series A, 181 (2018),\n  649-661", "doi": "10.1111/rssa.12360", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When collecting geocoded confidential data with the intent to disseminate,\nagencies often resort to altering the geographies prior to making data publicly\navailable due to data privacy obligations. An alternative to releasing\naggregated and/or perturbed data is to release multiply-imputed synthetic data,\nwhere sensitive values are replaced with draws from statistical models designed\nto capture important distributional features in the collected data. One issue\nthat has received relatively little attention, however, is how to handle\nspatially outlying observations in the collected data, as common spatial models\noften have a tendency to overfit these observations. The goal of this work is\nto bring this issue to the forefront and propose a solution, which we refer to\nas \"differential smoothing.\" After implementing our method on simulated data,\nhighlighting the effectiveness of our approach under various scenarios, we\nillustrate the framework using data consisting of sale prices of homes in San\nFrancisco.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 15:12:57 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Quick", "Harrison", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1507.05780", "submitter": "Samuel Livingstone", "authors": "Samuel Livingstone", "title": "Geometric ergodicity of the Random Walk Metropolis with\n  position-dependent proposal covariance", "comments": "13 pages including appendices, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Metropolis--Hastings method with proposal $\\mathcal{N}(x,\nhG(x)^{-1})$, where $x$ is the current state, and study its ergodicity\nproperties. We show that suitable choices of $G(x)$ can change these compared\nto the Random Walk Metropolis case $\\mathcal{N}(x, h\\Sigma)$, either for better\nor worse. We find that if the proposal variance is allowed to grow unboundedly\nin the tails of the distribution then geometric ergodicity can be established\nwhen the target distribution for the algorithm has tails that are heavier than\nexponential, but that the growth rate must be carefully controlled to prevent\nthe rejection rate approaching unity. We also illustrate that a judicious\nchoice of $G(x)$ can result in a geometrically ergodic chain when probability\nconcentrates on an ever narrower ridge in the tails, something that is not true\nfor the Random Walk Metropolis.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 10:49:33 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 10:09:40 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 11:32:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Livingstone", "Samuel", ""]]}, {"id": "1507.06032", "submitter": "Hongshuai Dai", "authors": "Chunhong Li, Dengxiang Huang, Hongshuai Dai, Xinxing Wei", "title": "Elastic Net Procedure for Partially Linear Models", "comments": "arXiv admin note: text overlap with arXiv:0908.1836 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection plays an important role in the high-dimensional data\nanalysis. However the high-dimensional data often induces the strongly\ncorrelated variables problem. In this paper, we propose Elastic Net procedure\nfor partially linear models and prove the group effect of its estimate. By a\nsimulation study, we show that the strongly correlated variables problem can be\nbetter handled by the Elastic Net procedure than Lasso, ALasso and Ridge. Based\non an empirical analysis, we can get that the Elastic Net procedure is\nparticularly useful when the number of predictors $p$ is much bigger than the\nsample size $n$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 02:08:23 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Li", "Chunhong", ""], ["Huang", "Dengxiang", ""], ["Dai", "Hongshuai", ""], ["Wei", "Xinxing", ""]]}, {"id": "1507.06090", "submitter": "Lixing Zhu", "authors": "Cuizhen Niu, Xu Guo, Lixing Zhu", "title": "Enhancements of nonparametric generalized likelihood ratio test:\n  Bias-correction and dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric generalized likelihood ratio test is popularly used for model\nchecking for regressions. However, there are two issues that may be the\nbarriers for its powerfulness. First, the bias term in its liming null\ndistribution causes the test not to well control type I error and thus Monte\nCarlo approximation for critical value determination is required. Second, it\nseverely suffers from the curse of dimensionality due to the use of\nmultivariate nonparametric function estimation. The purpose of this paper is\nthus two-fold: a bias-correction is suggested to this test and a dimension\nreduction-based model-adaptive enhancement is recommended to promote the power\nperformance. The proposed test still possesses the Wilks phenomenon, and the\ntest statistic can converge to its limit at a much faster rate and is much more\nsensitive to alternative models than the original nonparametric generalized\nlikelihood ratio test as if the dimension of covariates were one. Simulation\nstudies are conducted to evaluate the finite sample performance and to compare\nwith other popularly used tests. A real data analysis is conducted for\nillustration.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 07:50:26 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Niu", "Cuizhen", ""], ["Guo", "Xu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1507.06110", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Minh-Ngoc Tran, Mattias Villani, Robert Kohn", "title": "Speeding Up MCMC by Delayed Acceptance and Data Subsampling", "comments": "Accepted for publication in Journal of Computational and Graphical\n  Statistics", "journal-ref": null, "doi": "10.1080/10618600.2017.1307117", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of the Metropolis-Hastings (MH) algorithm arises from the\nrequirement of a likelihood evaluation for the full data set in each iteration.\nPayne and Mallick (2015) propose to speed up the algorithm by a delayed\nacceptance approach where the acceptance decision proceeds in two stages. In\nthe first stage, an estimate of the likelihood based on a random subsample\ndetermines if it is likely that the draw will be accepted and, if so, the\nsecond stage uses the full data likelihood to decide upon final acceptance.\nEvaluating the full data likelihood is thus avoided for draws that are unlikely\nto be accepted. We propose a more precise likelihood estimator which\nincorporates auxiliary information about the full data likelihood while only\noperating on a sparse set of the data. We prove that the resulting delayed\nacceptance MH is more efficient compared to that of Payne and Mallick (2015).\nThe caveat of this approach is that the full data set needs to be evaluated in\nthe second stage. We therefore propose to substitute this evaluation by an\nestimate and construct a state-dependent approximation thereof to use in the\nfirst stage. This results in an algorithm that (i) can use a smaller subsample\nm by leveraging on recent advances in Pseudo-Marginal MH (PMMH) and (ii) is\nprovably within $O(m^{-2})$ of the true posterior.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 09:25:21 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 11:52:10 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 23:26:05 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Quiroz", "Matias", ""], ["Tran", "Minh-Ngoc", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""]]}, {"id": "1507.06121", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic and Philippe Naveau", "title": "Detecting distributional changes in samples of independent block maxima\n  using probability weighted moments", "comments": "36 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of seasonal or annual block maxima is of interest in fields such\nas hydrology, climatology or meteorology. In connection with the celebrated\nmethod of block maxima, we study several tests that can be used to assess\nwhether the available series of maxima is identically distributed. It is\nassumed that block maxima are independent but not necessarily generalized\nextreme value distributed. The asymptotic null distributions of the test\nstatistics are investigated and the practical computation of approximate\np-values is addressed. Extensive Monte-Carlo simulations show the adequate\nfinite-sample behavior of the studied tests for a large number of realistic\ndata generating scenarios. Illustrations on several environmental datasets\nconclude the work.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 10:25:26 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 09:06:20 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 09:07:41 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Naveau", "Philippe", ""]]}, {"id": "1507.06201", "submitter": "Patrick Raanes", "authors": "Patrick N. Raanes and Alberto Carrassi and Laurent Bertino", "title": "Extending the square root method to account for additive forecast noise\n  in ensemble methods", "comments": null, "journal-ref": null, "doi": "10.1175/MWR-D-14-00375.1", "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A square root approach is considered for the problem of accounting for model\nnoise in the forecast step of the ensemble Kalman filter (EnKF) and related\nalgorithms. The primary aim is to replace the method of simulated,\npseudo-random, additive noise so as to eliminate the associated sampling\nerrors. The core method is based on the analysis step of ensemble square root\nfilters, and consists in the deterministic computation of a transform matrix.\nThe theoretical advantages regarding dynamical consistency are surveyed,\napplying equally well to the square root method in the analysis step. A\nfundamental problem due to the limited size of the ensemble subspace is\ndiscussed, and novel solutions that complement the core method are suggested\nand studied. Benchmarks from twin experiments with simple, low-order dynamics\nindicate improved performance over standard approaches such as additive,\nsimulated noise and multiplicative inflation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 14:18:55 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Raanes", "Patrick N.", ""], ["Carrassi", "Alberto", ""], ["Bertino", "Laurent", ""]]}, {"id": "1507.06336", "submitter": "Thomas House", "authors": "Thomas House", "title": "Hessian corrections to the Metropolis Adjusted Langevin Algorithm", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural method for the introduction of second-order derivatives of the log\nlikelihood into MCMC algorithms is introduced, based on Taylor expansion of the\nLangevin equation followed by exact solution of the truncated system.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 20:40:55 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["House", "Thomas", ""]]}, {"id": "1507.06597", "submitter": "Alexander Terenin", "authors": "Alexander Terenin and David Draper", "title": "Cox's Theorem and the Jaynesian Interpretation of Probability", "comments": "This work is withdrawn due to a critical error which we are unable to\n  repair without completely changing the framework. The first author deeply\n  regrets this error, which was committed when he was still obtaining his\n  master's degree and had yet to learn a proper degree of carefulness needed\n  when devising theoretical arguments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple proposed interpretations of probability theory: one such\ninterpretation is true-false logic under uncertainty. Cox's Theorem is a\nrepresentation theorem that states, under a certain set of axioms describing\nthe meaning of uncertainty, that every true-false logic under uncertainty is\nisomorphic to conditional probability theory. This result was used by Jaynes to\ndevelop a philosophical framework in which statistical inference under\nuncertainty should be conducted through the use of probability, via Bayes'\nRule. Unfortunately, most existing correct proofs of Cox's Theorem require\nrestrictive assumptions: for instance, many do not apply even to the simple\nexample of rolling a pair of fair dice. We offer a new axiomatization by\nreplacing various technical conditions with an axiom stating that our theory\nmust be consistent with respect to repeated events. We discuss the implications\nof our results, both for the philosophy of probability and for the philosophy\nof statistics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 18:21:22 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 04:00:32 GMT"}, {"version": "v3", "created": "Sat, 8 Feb 2020 14:28:49 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Terenin", "Alexander", ""], ["Draper", "David", ""]]}, {"id": "1507.06780", "submitter": "Jia Liu", "authors": "Jia Liu", "title": "An improved EM algorithm for solving MLE in constrained diffusion\n  kurtosis imaging of human brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The displacement distribution of a water molecular is characterized\nmathematically as Gaussianity without considering potential diffusion barriers\nand compartments. However, this is not true in real scenario: most biological\ntissues are comprised of cell membranes, various intracellular and\nextracellular spaces, and of other compartments, where the water diffusion is\nreferred to have a non-Gaussian distribution. Diffusion kurtosis imaging (DKI),\nrecently considered to be one sensitive biomarker, is an extension of diffusion\ntensor imaging, which quantifies the degree of non-Gaussianity of the\ndiffusion. This work proposes an efficient scheme of maximum likelihood\nestimation (MLE) in DKI: we start from the Rician noise model of the signal\nintensities. By augmenting a Von-Mises distributed latent phase variable, the\nRician likelihood is transformed to a tractable joint density without loss of\ngenerality. A fast computational method, an expectation-maximization (EM)\nalgorithm for MLE is proposed in DKI. To guarantee the physical relevance of\nthe diffusion kurtosis we apply the ternary quartic (TQ) parametrization to\nutilize its positivity, which imposes the upper bound to the kurtosis. A\nFisher-scoring method is used for achieving fast convergence of the individual\ndiffusion compartments. In addition, we use the barrier method to constrain the\nlower bound to the kurtosis. The proposed estimation scheme is conducted on\nboth synthetic and real data with an objective of healthy human brain. We\ncompared the method with the other popular ones with promising performance\nshown in the results.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 08:43:23 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Liu", "Jia", ""]]}, {"id": "1507.06792", "submitter": "Michael S{\\o}rensen", "authors": "Nina Munkholt Jakobsen and Michael S{\\o}rensen", "title": "Efficient estimation for diffusions sampled at high frequency over a\n  fixed time interval", "comments": "Published at http://projecteuclid.org/euclid.bj/1489737628 in the\n  Bernoulli Journal ( see\n  http://www.bernoulli-society.org/index.php/publications/bernoulli-journal/bernoulli-journal\n  ) by the International Statistical Institute/Bernoulli Society ( see\n  http://www.bernoulli-society.org )", "journal-ref": "Bernoulli 2017, Vol. 23, No. 3, 1874-1910", "doi": "10.3150/15-BEJ799", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric estimation for diffusion processes is considered for high\nfrequency observations over a fixed time interval. The processes solve\nstochastic differential equations with an unknown parameter in the diffusion\ncoefficient. We find easily verified conditions on approximate martingale\nestimating functions under which estimators are consistent, rate optimal, and\nefficient under high frequency (in-fill) asymptotics. The asymptotic\ndistributions of the estimators are shown to be normal variance-mixtures, where\nthe mixing distribution generally depends on the full sample path of the\ndiffusion process over the observation time interval. Utilising the concept of\nstable convergence, we also obtain the more easily applicable result that for a\nsuitable data dependent normalisation, the estimators converge in distribution\nto a standard normal distribution. The theory is illustrated by a simulation\nstudy comparing an efficient and a non-efficient estimating function for an\nergodic and a non-ergodic model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 09:33:14 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 15:22:13 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Jakobsen", "Nina Munkholt", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1507.06807", "submitter": "Andrew Golightly", "authors": "Gavin A. Whitaker, Andrew Golightly, Richard J. Boys and Chris\n  Sherlock", "title": "Bayesian inference for diffusion driven mixed-effects models", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) provide a natural framework for\nmodelling intrinsic stochasticity inherent in many continuous-time physical\nprocesses. When such processes are observed in multiple individuals or\nexperimental units, SDE driven mixed-effects models allow the quantification of\nbetween (as well as within) individual variation. Performing Bayesian inference\nfor such models, using discrete time data that may be incomplete and subject to\nmeasurement error is a challenging problem and is the focus of this paper. We\nextend a recently proposed MCMC scheme to include the SDE driven mixed-effects\nframework. Fundamental to our approach is the development of a novel construct\nthat allows for efficient sampling of conditioned SDEs that may exhibit\nnonlinear dynamics between observation times. We apply the resulting scheme to\nsynthetic data generated from a simple SDE model of orange tree growth, and\nreal data consisting of observations on aphid numbers recorded under a variety\nof different treatment regimes. In addition, we provide a systematic comparison\nof our approach with an inference scheme based on a tractable approximation of\nthe SDE, that is, the linear noise approximation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 10:52:44 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 13:52:01 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Whitaker", "Gavin A.", ""], ["Golightly", "Andrew", ""], ["Boys", "Richard J.", ""], ["Sherlock", "Chris", ""]]}, {"id": "1507.07050", "submitter": "Terrance Savitsky", "authors": "Terrance D. Savitsky, Daniell Toth", "title": "Bayesian Estimation Under Informative Sampling", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian analysis is increasingly popular for use in social science and other\napplication areas where the data are observations from an informative sample.\nAn informative sampling design leads to inclusion probabilities that are\ncorrelated with the response variable of interest. Model inference performed on\nthe observed sample taken from the population will be biased for the population\ngenerative model under informative sampling since the balance of information in\nthe sample data is different from that for the population. Typical approaches\nto account for an informative sampling design under Bayesian estimation are\noften difficult to implement because they require re-parameterization of the\nhypothesized generating model, or focus on design, rather than model-based,\ninference. We propose to construct a pseudo-posterior distribution that\nutilizes sampling weights based on the marginal inclusion probabilities to\nexponentiate the likelihood contribution of each sampled unit, which weights\nthe information in the sample back to the population. Our approach provides a\nnearly automated estimation procedure applicable to any model specified by the\ndata analyst for the population and retains the population model\nparameterization and posterior sampling geometry. We construct conditions on\nknown marginal and pairwise inclusion probabilities that define a class of\nsampling designs where $L_{1}$ consistency of the pseudo posterior is\nguaranteed. We demonstrate our method on an application concerning the Bureau\nof Labor Statistics Job Openings and Labor Turnover Survey.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 01:15:55 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 13:55:52 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2015 20:35:07 GMT"}, {"version": "v4", "created": "Fri, 3 Jun 2016 22:08:29 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Toth", "Daniell", ""]]}, {"id": "1507.07094", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "Unknown sparsity in compressed sensing: Denoising and inference", "comments": "The title of the previous tech report has been updated so that it\n  matches the published version. The published version contains additional\n  material", "journal-ref": "IEEE Transactions on Information Theory 62.9 (2016): 5145-5166", "doi": "10.1109/TIT.2016.2587772", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of Compressed Sensing (CS) asserts that an unknown signal\n$x\\in\\mathbb{R}^p$ can be accurately recovered from an underdetermined set of\n$n$ linear measurements with $n\\ll p$, provided that $x$ is sufficiently\nsparse. However, in applications, the degree of sparsity $\\|x\\|_0$ is typically\nunknown, and the problem of directly estimating $\\|x\\|_0$ has been a\nlongstanding gap between theory and practice. A closely related issue is that\n$\\|x\\|_0$ is a highly idealized measure of sparsity, and for real signals with\nentries not equal to 0, the value $\\|x\\|_0=p$ is not a useful description of\ncompressibility. In our previous conference paper [Lop13] that examined these\nproblems, we considered an alternative measure of \"soft\" sparsity,\n$\\|x\\|_1^2/\\|x\\|_2^2$, and designed a procedure to estimate\n$\\|x\\|_1^2/\\|x\\|_2^2$ that does not rely on sparsity assumptions.\n  The present work offers a new deconvolution-based method for estimating\nunknown sparsity, which has wider applicability and sharper theoretical\nguarantees. In particular, we introduce a family of entropy-based sparsity\nmeasures $s_q(x):=\\big(\\frac{\\|x\\|_q}{\\|x\\|_1}\\big)^{\\frac{q}{1-q}}$\nparameterized by $q\\in[0,\\infty]$. This family interpolates between\n$\\|x\\|_0=s_0(x)$ and $\\|x\\|_1^2/\\|x\\|_2^2=s_2(x)$ as $q$ ranges over $[0,2]$.\nFor any $q\\in (0,2]\\setminus\\{1\\}$, we propose an estimator $\\hat{s}_q(x)$\nwhose relative error converges at the dimension-free rate of $1/\\sqrt{n}$, even\nwhen $p/n\\to\\infty$. Our main results also describe the limiting distribution\nof $\\hat{s}_q(x)$, as well as some connections to Basis Pursuit Denosing, the\nLasso, deterministic measurement matrices, and inference problems in CS.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 11:17:03 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 23:06:31 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 09:52:03 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1507.07106", "submitter": "Minsuk Shin", "authors": "Minsuk Shin, Anirban Bhattacharya, and Valen E. Johnson", "title": "Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in\n  Ultrahigh-Dimensional Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection procedures based on nonlocal alternative prior\ndensities are extended to ultrahigh dimensional settings and compared to other\nvariable selection procedures using precision-recall curves. Variable selection\nprocedures included in these comparisons include methods based on $g$-priors,\nreciprocal lasso, adaptive lasso, scad, and minimax concave penalty criteria.\nThe use of precision-recall curves eliminates the sensitivity of our\nconclusions to the choice of tuning parameters. We find that Bayesian variable\nselection procedures based on nonlocal priors are competitive to all other\nprocedures in a range of simulation scenarios, and we subsequently explain this\nfavorable performance through a theoretical examination of their consistency\nproperties. When certain regularity conditions apply, we demonstrate that the\nnonlocal procedures are consistent for linear models even when the number of\ncovariates $p$ increases sub-exponentially with the sample size $n$. A model\nselection procedure based on Zellner's $g$-prior is also found to be\ncompetitive with penalized likelihood methods in identifying the true model,\nbut the posterior distribution on the model space induced by this method is\nmuch more dispersed than the posterior distribution induced on the model space\nby the nonlocal prior methods. We investigate the asymptotic form of the\nmarginal likelihood based on the nonlocal priors and show that it attains a\nunique term that cannot be derived from the other Bayesian model selection\nprocedures. We also propose a scalable and efficient algorithm called\nSimplified Shotgun Stochastic Search with Screening (S5) to explore the\nenormous model space, and we show that S5 dramatically reduces the computing\ntime without losing the capacity to search the interesting region in the model\nspace. The S5 algorithm is available in an \\verb R ~package {\\it BayesS5} on\n\\texttt{CRAN}.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 15:03:45 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 21:14:52 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 20:57:31 GMT"}, {"version": "v4", "created": "Wed, 18 Jan 2017 05:14:06 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Shin", "Minsuk", ""], ["Bhattacharya", "Anirban", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1507.07114", "submitter": "Luca Scrucca", "authors": "Luca Scrucca and Adrian E. Raftery", "title": "Improved initialisation of model-based clustering using Gaussian\n  hierarchical partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initialisation of the EM algorithm in model-based clustering is often\ncrucial. Various starting points in the parameter space often lead to different\nlocal maxima of the likelihood function and, so to different clustering\npartitions. Among the several approaches available in the literature,\nmodel-based agglomerative hierarchical clustering is used to provide initial\npartitions in the popular MCLUST R package. This choice is computationally\nconvenient and often yields good clustering partitions. However, in certain\ncircumstances, poor initial partitions may cause the EM algorithm to converge\nto a local maximum of the likelihood function. We propose several simple and\nfast refinements based on data transformations and illustrate them through data\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 15:54:12 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Scrucca", "Luca", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1507.07170", "submitter": "Yingbo Li", "authors": "Joyee Ghosh, Yingbo Li and Robin Mitra", "title": "On the Use of Cauchy Prior Distributions for Bayesian Logistic\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In logistic regression, separation occurs when a linear combination of the\npredictors can perfectly classify part or all of the observations in the\nsample, and as a result, finite maximum likelihood estimates of the regression\ncoefficients do not exist. Gelman et al. (2008) recommended independent Cauchy\ndistributions as default priors for the regression coefficients in logistic\nregression, even in the case of separation, and reported posterior modes in\ntheir analyses. As the mean does not exist for the Cauchy prior, a natural\nquestion is whether the posterior means of the regression coefficients exist\nunder separation. We prove theorems that provide necessary and sufficient\nconditions for the existence of posterior means under independent Cauchy priors\nfor the logit link and a general family of link functions, including the probit\nlink. We also study the existence of posterior means under multivariate Cauchy\npriors. For full Bayesian inference, we develop a Gibbs sampler based on\nPolya-Gamma data augmentation to sample from the posterior distribution under\nindependent Student-t priors including Cauchy priors, and provide a companion R\npackage in the supplement. We demonstrate empirically that even when the\nposterior means of the regression coefficients exist under separation, the\nmagnitude of the posterior samples for Cauchy priors may be unusually large,\nand the corresponding Gibbs sampler shows extremely slow mixing. While\nalternative algorithms such as the No-U-Turn Sampler in Stan can greatly\nimprove mixing, in order to resolve the issue of extremely heavy tailed\nposteriors for Cauchy priors under separation, one would need to consider\nlighter tailed priors such as normal priors or Student-t priors with degrees of\nfreedom larger than one.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 04:14:21 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 02:40:00 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Ghosh", "Joyee", ""], ["Li", "Yingbo", ""], ["Mitra", "Robin", ""]]}, {"id": "1507.07269", "submitter": "Shintaro Mori", "authors": "Masafumi Hino, Yosuke Irie, Masato Hisakado, Taiki Takahashi, Shintaro\n  Mori", "title": "Detection of phase transition in generalized P\\'olya urn in information\n  cascade experiment", "comments": "26 pages, 12 figures", "journal-ref": "J.Phys.Soc.Jpn. 85 (3), 034002-034013, 2016", "doi": "10.7566/JPSJ.85.034002", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of detecting a phase transition in a generalized P\\'olya\nurn in an information cascade experiment. The method is based on the asymptotic\nbehavior of the correlation $C(t)$ between the first subject's choice and the\n$t+1$-th subject's choice, the limit value of which, $c\\equiv \\lim_{t\\to\n\\infty}C(t)$, is the order parameter of the phase transition. To verify the\nmethod, we perform a voting experiment using two-choice questions. An urn X is\nchosen at random from two urns A and B, which contain red and blue balls in\ndifferent configurations. Subjects sequentially guess whether X is A or B using\ninformation about the prior subjects' choices and the color of a ball randomly\ndrawn from X. The color tells the subject which is X with probability $q$. We\nset $q\\in \\{5/9,6/9,7/9,8/9\\}$ by controlling the configurations of red and\nblue balls in A and B. The (average) lengths of the sequence of the subjects\nare 63, 63, 54.0, and 60.5 for $q\\in \\{5/9,6/9,7/9,8/9\\}$, respectively. We\ndescribe the sequential voting process by a nonlinear P\\'olya urn model. The\nmodel suggests the possibility of a phase transition when $q$ changes. We show\nthat $c>0\\,\\,\\,(=0)$ for $q=5/9,6/9\\,\\,\\,(7/9,8/9 )$ and detect the phase\ntransition using the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 00:01:30 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 06:01:05 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Hino", "Masafumi", ""], ["Irie", "Yosuke", ""], ["Hisakado", "Masato", ""], ["Takahashi", "Taiki", ""], ["Mori", "Shintaro", ""]]}, {"id": "1507.07271", "submitter": "Alex Reinhart", "authors": "Wesley Tansey, Alex Athey, Alex Reinhart, and James G. Scott", "title": "Multiscale spatial density smoothing: an application to large-scale\n  radiological survey and anomaly detection", "comments": "36 pages, 10 figures", "journal-ref": "Journal of the American Statistical Association, vol. 112 no. 519\n  (2017), pp. 1047-1063", "doi": "10.1080/01621459.2016.1276461", "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a spatially varying density function,\nmotivated by problems that arise in large-scale radiological survey and anomaly\ndetection. In this context, the density functions to be estimated are the\nbackground gamma-ray energy spectra at sites spread across a large geographical\narea, such as nuclear production and waste-storage sites, military bases,\nmedical facilities, university campuses, or the downtown of a city. Several\nchallenges combine to make this a difficult problem. First, the spectral\ndensity at any given spatial location may have both smooth and non-smooth\nfeatures. Second, the spatial correlation in these density functions is neither\nstationary nor locally isotropic. Finally, at some spatial locations, there is\nvery little data. We present a method called multiscale spatial density\nsmoothing that successfully addresses these challenges. The method is based on\nrecursive dyadic partition of the sample space, and therefore shares much in\ncommon with other multiscale methods, such as wavelets and P\\'olya-tree priors.\nWe describe an efficient algorithm for finding a maximum a posteriori (MAP)\nestimate that leverages recent advances in convex optimization for non-smooth\nfunctions.\n  We apply multiscale spatial density smoothing to real data collected on the\nbackground gamma-ray spectra at locations across a large university campus. The\nmethod exhibits state-of-the-art performance for spatial smoothing in density\nestimation, and it leads to substantial improvements in power when used in\nconjunction with existing methods for detecting the kinds of radiological\nanomalies that may have important consequences for public health and safety.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 00:41:16 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 16:02:04 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Tansey", "Wesley", ""], ["Athey", "Alex", ""], ["Reinhart", "Alex", ""], ["Scott", "James G.", ""]]}, {"id": "1507.07280", "submitter": "Rui Tuo", "authors": "Rui Tuo, C. F. Jeff Wu", "title": "Efficient Calibration for Imperfect Computer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer models contain unknown parameters which need to be estimated\nusing physical observations. Kennedy and O'Hagan (2001) shows that the\ncalibration method based on Gaussian process models proposed by Kennedy and\nO'Hagan (2001) may lead to unreasonable estimate for imperfect computer models.\nIn this work, we extend their study to calibration problems with stochastic\nphysical data. We propose a novel method, called the $L_2$ calibration, and\nshow its semiparametric efficiency. The conventional method of the ordinary\nleast squares is also studied. Theoretical analysis shows that it is consistent\nbut not efficient. Numerical examples show that the proposed method outperforms\nthe existing ones.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 02:38:35 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Tuo", "Rui", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1507.07323", "submitter": "Mike Tsionas", "authors": "Mike G. Tsionas", "title": "Bayesian analysis of multivariate stable distributions using\n  one-dimensional projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we take up Bayesian inference in general multivariate stable\ndistributions. We exploit the representation of Matsui and Takemura (2009) for\nunivariate projections, and the representation of the distributions in terms of\ntheir spectral measure. We present efficient MCMC schemes to perform the\ncomputations when the spectral measure is approximated discretely or, as we\npropose, by a normal distribution. Appropriate latent variables are introduced\nto implement MCMC. In relation to the discrete approximation, we propose\nefficient computational schemes based on the characteristic function.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 07:53:17 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Tsionas", "Mike G.", ""]]}, {"id": "1507.07394", "submitter": "Martine Barons Dr", "authors": "Jim Q. Smith, Martine J. Barons and Manuele Leonelli", "title": "Coherent Frameworks for Statistical Inference serving Integrating\n  Decision Support Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A subjective expected utility policy making centre, managing complex, dynamic\nsystems, needs to draw on the expertise of a variety of disparate panels of\nexperts and integrate this information coherently. To achieve this, diverse\nsupporting probabilistic models need to be networked together, the output of\none model providing the input to the next. In this paper we provide a\ntechnology for designing an integrating decision support system and to enable\nthe centre to explore and compare the efficiency of different candidate\npolicies. We develop a formal statistical methodology to underpin this tool. In\nparticular, we derive sufficient conditions that ensure inference remains\ncoherent before and after relevant evidence is accommodated into the system.\nThe methodology is illustrated throughout using examples drawn from two\ndecision support systems: one designed for nuclear emergency crisis management\nand the other to support policy makers in addressing the complex challenges of\nfood poverty in the UK.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 13:13:29 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 15:58:18 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Smith", "Jim Q.", ""], ["Barons", "Martine J.", ""], ["Leonelli", "Manuele", ""]]}, {"id": "1507.07466", "submitter": "Daniel Andr\\'es D\\'iaz Pach\\'on", "authors": "Daniel Andr\\'es D\\'iaz-Pach\\'on, Francisco J. P. Zimmermann, Luis\n  Alberto L\\'opez", "title": "$F$ tests for the strip-split plot design", "comments": "22 pages", "journal-ref": "REVISTA BRASILEIRA DE BIOMETRIA, [S.I.], v. 34, n. 2, p. 279-303,\n  June 2016", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present the structure of the $F$ tests, the variance\ncomponents and the approximate degrees of freedom for each of the eight\npossible mixed models of the strip-split plot design. We present an example to\nillustrate the model and compare it to more traditional settings like a\nthree-way factorial design and a split-split plot model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 16:03:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["D\u00edaz-Pach\u00f3n", "Daniel Andr\u00e9s", ""], ["Zimmermann", "Francisco J. P.", ""], ["L\u00f3pez", "Luis Alberto", ""]]}, {"id": "1507.07535", "submitter": "Ali Akbar Jafari", "authors": "Rasool Roozegar and Ali Akbar Jafari", "title": "On Bivariate Exponentiated Extended Weibull Family of Distributions", "comments": "arXiv admin note: text overlap with arXiv:1501.03528 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new class of bivariate distributions called the\nbivariate exponentiated extended Weibull distributions. The model introduced\nhere is of Marshall-Olkin type. This new class of bivariate distributions\ncontains several bivariate lifetime models. Some mathematical properties of the\nnew class of distributions are studied. We provide the joint and conditional\ndensity functions, the joint cumulative distribution function and the joint\nsurvival function. Special bivariate distributions are investigated in some\ndetail. The maximum likelihood estimators are obtained using the EM algorithm.\nWe illustrate the usefulness of the new class by means of application to two\nreal data sets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 19:24:20 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Roozegar", "Rasool", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1507.07902", "submitter": "Junmo Song", "authors": "Junmo Song, Dong-hyun Oh, and Jiwon Kang", "title": "Robust Estimation in Stochastic Frontier Models", "comments": "43 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a robust estimator for stochastic frontier models by\nintegrating the idea of Basu et al. [1998, Biometrika 85, 549-559] into such\nmodels. We verify that the suggested estimator is strongly consistent and\nasymptotic normal under regularity conditions and investigate robust\nproperties. We use a simulation study to demonstrate that the estimator has\nstrong robust properties with little loss in asymptotic efficiency relative to\nthe maximum likelihood estimator. A real data analysis is performed for\nillustrating the use of the estimator.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 19:10:07 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Song", "Junmo", ""], ["Oh", "Dong-hyun", ""], ["Kang", "Jiwon", ""]]}, {"id": "1507.08017", "submitter": "Marc G. Genton", "authors": "Marc G. Genton, William Kleiber", "title": "Cross-Covariance Functions for Multivariate Geostatistics", "comments": "Published at http://dx.doi.org/10.1214/14-STS487 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 147-163", "doi": "10.1214/14-STS487", "report-no": "IMS-STS-STS487", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuously indexed datasets with multiple variables have become ubiquitous\nin the geophysical, ecological, environmental and climate sciences, and pose\nsubstantial analysis challenges to scientists and statisticians. For many\nyears, scientists developed models that aimed at capturing the spatial behavior\nfor an individual process; only within the last few decades has it become\ncommonplace to model multiple processes jointly. The key difficulty is in\nspecifying the cross-covariance function, that is, the function responsible for\nthe relationship between distinct variables. Indeed, these cross-covariance\nfunctions must be chosen to be consistent with marginal covariance functions in\nsuch a way that the second-order structure always yields a nonnegative definite\ncovariance matrix. We review the main approaches to building cross-covariance\nmodels, including the linear model of coregionalization, convolution methods,\nthe multivariate Mat\\'{e}rn and nonstationary and space-time extensions of\nthese among others. We additionally cover specialized constructions, including\nthose designed for asymmetry, compact support and spherical domains, with a\nreview of physics-constrained models. We illustrate select models on a\nbivariate regional climate model output example for temperature and pressure,\nalong with a bivariate minimum and maximum temperature observational dataset;\nwe compare models by likelihood value as well as via cross-validation\nco-kriging studies. The article closes with a discussion of unsolved problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 04:52:25 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Genton", "Marc G.", ""], ["Kleiber", "William", ""]]}, {"id": "1507.08025", "submitter": "Sof\\'{i}a S. Villar", "authors": "Sof\\'ia S. Villar, Jack Bowden, James Wason", "title": "Multi-armed Bandit Models for the Optimal Design of Clinical Trials:\n  Benefits and Challenges", "comments": "Published at http://dx.doi.org/10.1214/14-STS504 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 199-215", "doi": "10.1214/14-STS504", "report-no": "IMS-STS-STS504", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandit problems (MABPs) are a special type of optimal control\nproblem well suited to model resource allocation under uncertainty in a wide\nvariety of contexts. Since the first publication of the optimal solution of the\nclassic MABP by a dynamic index rule, the bandit literature quickly diversified\nand emerged as an active research topic. Across this literature, the use of\nbandit models to optimally design clinical trials became a typical motivating\napplication, yet little of the resulting theory has ever been used in the\nactual design and analysis of clinical trials. To this end, we review two MABP\ndecision-theoretic approaches to the optimal allocation of treatments in a\nclinical trial: the infinite-horizon Bayesian Bernoulli MABP and the\nfinite-horizon variant. These models possess distinct theoretical properties\nand lead to separate allocation rules in a clinical trial design context. We\nevaluate their performance compared to other allocation rules, including fixed\nrandomization. Our results indicate that bandit approaches offer significant\nadvantages, in terms of assigning more patients to better treatments, and\nsevere limitations, in terms of their resulting statistical power. We propose a\nnovel bandit-based patient allocation rule that overcomes the issue of low\npower, thus removing a potential barrier for their use in practice.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 05:59:41 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Villar", "Sof\u00eda S.", ""], ["Bowden", "Jack", ""], ["Wason", "James", ""]]}, {"id": "1507.08041", "submitter": "El\\'{{\\i}}as Moreno", "authors": "El\\'ias Moreno, Javier Gir\\'on, George Casella", "title": "Posterior Model Consistency in Variable Selection as the Model Dimension\n  Grows", "comments": "Published at http://dx.doi.org/10.1214/14-STS508 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 228-241", "doi": "10.1214/14-STS508", "report-no": "IMS-STS-STS508", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the consistency analyses of Bayesian procedures for variable\nselection in regression refer to pairwise consistency, that is, consistency of\nBayes factors. However, variable selection in regression is carried out in a\ngiven class of regression models where a natural variable selector is the\nposterior probability of the models. In this paper we analyze the consistency\nof the posterior model probabilities when the number of potential regressors\ngrows as the sample size grows. The novelty in the posterior model consistency\nis that it depends not only on the priors for the model parameters through the\nBayes factor, but also on the model priors, so that it is a useful tool for\nchoosing priors for both models and model parameters. We have found that some\nclasses of priors typically used in variable selection yield posterior model\ninconsistency, while mixtures of these priors improve this undesirable\nbehavior. For moderate sample sizes, we evaluate Bayesian pairwise variable\nselection procedures by comparing their frequentist Type I and II error\nprobabilities. This provides valuable information to discriminate between the\npriors for the model parameters commonly used for variable selection.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 07:25:51 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Moreno", "El\u00edas", ""], ["Gir\u00f3n", "Javier", ""], ["Casella", "George", ""]]}, {"id": "1507.08069", "submitter": "Hiromasa Tamae", "authors": "Hiromasa Tamae and Tatsuya Kubokawa", "title": "Small Area Predictors with Dual Shrinkage of Means and Variances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper concerns small-area estimation in the Fay-Herriot type area-level\nmodel with random dispersions, which models the case that the sampling errors\nchange from area to area. The resulting Bayes estimator shrinks both means and\nvariances, but needs numerical computation to provide the estimates. In this\npaper, an approximated empirical Bayes (AEB) estimator with a closed form is\nsuggested. The model parameters are estimated via the moment method, and the\nmean squared error of the AEB is estimated via the single parametric bootstrap\nmethod. The benchmarked estimator and a second-order unbiased estimator of the\nmean squared error are also derived.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:07:08 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Tamae", "Hiromasa", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1507.08155", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family", "comments": "13 pages, 6 figures. IT-Dendrogram: An Effective Method to Visualize\n  the In-Tree structure by Dendrogram", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previously, we proposed a physically-inspired method to construct data points\ninto an effective in-tree (IT) structure, in which the underlying cluster\nstructure in the dataset is well revealed. Although there are some edges in the\nIT structure requiring to be removed, such undesired edges are generally\ndistinguishable from other edges and thus are easy to be determined. For\ninstance, when the IT structures for the 2-dimensional (2D) datasets are\ngraphically presented, those undesired edges can be easily spotted and\ninteractively determined. However, in practice, there are many datasets that do\nnot lie in the 2D Euclidean space, thus their IT structures cannot be\ngraphically presented. But if we can effectively map those IT structures into a\nvisualized space in which the salient features of those undesired edges are\npreserved, then the undesired edges in the IT structures can still be visually\ndetermined in a visualization environment. Previously, this purpose was reached\nby our method called IT-map. The outstanding advantage of IT-map is that\nclusters can still be found even with the so-called crowding problem in the\nembedding.\n  In this paper, we propose another method, called IT-Dendrogram, to achieve\nthe same goal through an effective combination of the IT structure and the\nsingle link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogram\ncan also effectively represent the IT structures in a visualization\nenvironment, whereas using another form, called the Dendrogram. IT-Dendrogram\ncan serve as another visualization method to determine the undesired edges in\nthe IT structures and thus benefit the IT-based clustering analysis. This was\ndemonstrated on several datasets with different shapes, dimensions, and\nattributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem,\nwhich could help users make more reliable cluster analysis in certain problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 14:22:13 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1507.08377", "submitter": "Weichen Wang", "authors": "Jianqing Fan, Han Liu and Weichen Wang", "title": "Large Covariance Estimation through Elliptical Factor Models", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a general Principal Orthogonal complEment Thresholding (POET)\nframework for large-scale covariance matrix estimation based on an approximate\nfactor model. A set of high level sufficient conditions for the procedure to\nachieve optimal rates of convergence under different matrix norms were brought\nup to better understand how POET works. Such a framework allows us to recover\nthe results for sub-Gaussian in a more transparent way that only depends on the\nconcentration properties of the sample covariance matrix. As a new theoretical\ncontribution, for the first time, such a framework allows us to exploit\nconditional sparsity covariance structure for the heavy-tailed data. In\nparticular, for the elliptical data, we proposed a robust estimator based on\nmarginal and multivariate Kendall's tau to satisfy these conditions. In\naddition, conditional graphical model was also studied under the same\nframework. The technical tools developed in this paper are of general interest\nto high dimensional principal component analysis. Thorough numerical results\nwere also provided to back up the developed theory.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 05:02:46 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Fan", "Jianqing", ""], ["Liu", "Han", ""], ["Wang", "Weichen", ""]]}, {"id": "1507.08383", "submitter": "Daniel Simpson", "authors": "Daniel Simpson, Finn Lindgren, H{\\aa}vard Rue", "title": "Beyond the Valley of the Covariance Function", "comments": "Published at http://dx.doi.org/10.1214/15-STS515 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 164-166", "doi": "10.1214/15-STS515", "report-no": "IMS-STS-STS515", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Cross-Covariance Functions for Multivariate Geostatistics\" by\nGenton and Kleiber [arXiv:1507.08017].\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 05:27:33 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1507.08386", "submitter": "Moreno Bevilacqua", "authors": "Moreno Bevilacqua, Amanda S. Hering, Emilio Porcu", "title": "On the Flexibility of Multivariate Covariance Models: Comment on the\n  Paper by Genton and Kleiber", "comments": "Published at http://dx.doi.org/10.1214/15-STS516 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 167-169", "doi": "10.1214/15-STS516", "report-no": "IMS-STS-STS516", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Cross-Covariance Functions for Multivariate Geostatistics\" by\nGenton and Kleiber [arXiv:1507.08017].\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 05:32:16 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Bevilacqua", "Moreno", ""], ["Hering", "Amanda S.", ""], ["Porcu", "Emilio", ""]]}, {"id": "1507.08401", "submitter": "Noel Cressie", "authors": "Noel Cressie, Sandy Burden, Walter Davis, Pavel N. Krivitsky, Payam\n  Mokhtarian, Thomas Suesse, Andrew Zammit-Mangion", "title": "Capturing Multivariate Spatial Dependence: Model, Estimate and then\n  Predict", "comments": "Published at http://dx.doi.org/10.1214/15-STS517 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 170-175", "doi": "10.1214/15-STS517", "report-no": "IMS-STS-STS517", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical processes rarely occur in isolation, rather they influence and\ninteract with one another. Thus, there is great benefit in modeling potential\ndependence between both spatial locations and different processes. It is the\ninteraction between these two dependencies that is the focus of Genton and\nKleiber's paper under discussion. We see the problem of ensuring that any\nmultivariate spatial covariance matrix is nonnegative definite as important,\nbut we also see it as a means to an end. That \"end\" is solving the scientific\nproblem of predicting a multivariate field. [arXiv:1507.08017].\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 07:01:08 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Cressie", "Noel", ""], ["Burden", "Sandy", ""], ["Davis", "Walter", ""], ["Krivitsky", "Pavel N.", ""], ["Mokhtarian", "Payam", ""], ["Suesse", "Thomas", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "1507.08403", "submitter": "Hao Zhang", "authors": "Hao Zhang, Wenxiang Cai", "title": "When Doesn't Cokriging Outperform Kriging?", "comments": "Published at http://dx.doi.org/10.1214/15-STS518 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 176-180", "doi": "10.1214/15-STS518", "report-no": "IMS-STS-STS518", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cokriging in theory should yield smaller or equal prediction\nvariance than kriging, this outperformance sometimes is hard to see in\npractice. This should motivate theoretical studies on cokriging. In general,\nthere is a lack of theoretical results for cokriging. In this work, we provide\nsome theoretical results to compare cokriging with kriging by examining some\nexplicit models and specific sampling schemes. [arXiv:1507.08017]\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 07:08:01 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Zhang", "Hao", ""], ["Cai", "Wenxiang", ""]]}, {"id": "1507.08405", "submitter": "Marc G. Genton", "authors": "Marc G. Genton, William Kleiber", "title": "Rejoinder of ``Cross-Covariance Functions for Multivariate\n  Geostatistics''", "comments": "Published at http://dx.doi.org/10.1214/15-STS519 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 181-183", "doi": "10.1214/15-STS519", "report-no": "IMS-STS-STS519", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of ``Cross-Covariance Functions for Multivariate Geostatistics'' by\nGenton and Kleiber [arXiv:1507.08017].\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 07:12:38 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Genton", "Marc G.", ""], ["Kleiber", "William", ""]]}, {"id": "1507.08612", "submitter": "Daniel Wegmann", "authors": "Athanasios Kousathanas, Christoph Leuenberger, Jonas Helfer, Mathieu\n  Quinodoz, Matthieu Foll, Daniel Wegmann", "title": "Likelihood-free inference in high-dimensional models", "comments": "Added supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods that bypass analytical evaluations of the likelihood function have\nbecome an indispensable tool for statistical inference in many fields of\nscience. These so-called likelihood-free methods rely on accepting and\nrejecting simulations based on summary statistics, which limits them to low\ndimensional models for which the absolute likelihood is large enough to result\nin manageable acceptance rates. To get around these issues, we introduce a\nnovel, likelihood-free Markov-Chain Monte Carlo (MCMC) method combining two key\ninnovations: updating only one parameter per iteration and accepting or\nrejecting this update based on subsets of statistics sufficient for this\nparameter. This increases acceptance rates dramatically, rendering this\napproach suitable even for models of very high dimensionality. We further\nderive that for linear models, a one dimensional combination of statistics per\nparameter is sufficient and can be found empirically with simulations. Finally,\nwe demonstrate that our method readily scales to models of very high\ndimensionality using both toy models as well as by jointly inferring the\neffective population size, the distribution of fitness effects of new mutations\n(DFE) and selection coefficients for each locus from data of a recent\nexperiment on the evolution of drug-resistance in Influenza.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 18:23:58 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 11:47:25 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Kousathanas", "Athanasios", ""], ["Leuenberger", "Christoph", ""], ["Helfer", "Jonas", ""], ["Quinodoz", "Mathieu", ""], ["Foll", "Matthieu", ""], ["Wegmann", "Daniel", ""]]}, {"id": "1507.08638", "submitter": "Najla Elhezzani", "authors": "Najla Saad Elhezzani", "title": "Heritability Estimation in Matrix-Variate Mixed models -- A Bayesian\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the emergence of genome-wide association studies (GWASs), estimation of\nthe narrow sense heritability explained by common single-nucleotide\npolymorphisms (SNPs) via linear mixed model approaches became widely used. As\nin most GWASs, most of the heritability analyses are performed using univariate\napproaches i.e. considering each phenotype independently. In this study, we\npropose a Bayesian matrix-variate mixed model that takes into account the\ngenetic correlation between phenotypes in addition to the genetic correlation\nbetween individuals which is usually modelled via a relatedness matrix. We\nshowed that when the relatedness matrix is estimated using all the genome-wide\nSNPs, our model is equivalent to a matrix normal regression with matrix normal\nprior on the effect sizes. Using real data we demonstrate that there is a boost\nin the heritability explained when phenotypes are jointly modelled (25-35%\nincrease). In fact based on their standard error, the joint modelling provides\nmore accurate estimates of the heritability over the univariate modelling.\nMoreover, our Bayesian approach provides slightly higher estimates of\nheritability compared to the maximum likelihood method. On the other hand,\nalthough our method performs less well in phenotype prediction, we note that an\ninitial imputation step relatively increases the prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 19:33:07 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Elhezzani", "Najla Saad", ""]]}, {"id": "1507.08645", "submitter": "Reza Solgi", "authors": "Luke Bornn, Neil Shephard and Reza Solgi", "title": "Moment conditions and Bayesian nonparametrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models phrased though moment conditions are central to much of modern\ninference. Here these moment conditions are embedded within a nonparametric\nBayesian setup. Handling such a model is not probabilistically straightforward\nas the posterior has support on a manifold. We solve the relevant issues,\nbuilding new probability and computational tools using Hausdorff measures to\nanalyze them on real and simulated data. These new methods which involve\nsimulating on a manifold can be applied widely, including providing Bayesian\nanalysis of quasi-likelihoods, linear and nonlinear regression, missing data\nand hierarchical models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 19:45:52 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 18:58:42 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Bornn", "Luke", ""], ["Shephard", "Neil", ""], ["Solgi", "Reza", ""]]}, {"id": "1507.08653", "submitter": "Aur\\'elien Nicosia", "authors": "Aur\\'elien Nicosia, Thierry Duchesne, Louis-Paul Rivest and Daniel\n  Fortin", "title": "A General Hidden State Random Walk Model for Animal Movement", "comments": "28 pages", "journal-ref": "Computational Statistics & Data Analysis, Volume 105, January\n  2017, Pages 76--95", "doi": "10.1016/j.csda.2016.07.009", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general hidden state random walk model to\ndescribe the movement of an animal that takes into account movement taxis with\nrespect to features of the environment. A circular-linear process models the\ndirection and distance between two consecutive localizations of the animal. A\nhidden process structure accounts for the animal's change in movement behavior.\nThe originality of the proposed approach is that several environmental targets\ncan be included in the directional model. An EM algorithm is devised to fit\nthis model and an application to the analysis of the movement of caribou in\nCanada's boreal forest is presented\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 19:58:35 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 19:17:17 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Nicosia", "Aur\u00e9lien", ""], ["Duchesne", "Thierry", ""], ["Rivest", "Louis-Paul", ""], ["Fortin", "Daniel", ""]]}, {"id": "1507.08689", "submitter": "Spencer Wheatley Mr.", "authors": "Spencer Wheatley and Didier Sornette", "title": "Multiple Outlier Detection in Samples with Exponential & Pareto Tails:\n  Redeeming the Inward Approach & Detecting Dragon Kings", "comments": "32 pages with 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the detection of multiple outliers in Exponential and Pareto\nsamples -- as well as general samples that have approximately Exponential or\nPareto tails, thanks to Extreme Value Theory. It is shown that a simple\n\"robust\" modification of common test statistics makes inward sequential testing\n-- formerly relegated within the literature since the introduction of outward\ntesting -- as powerful as, and potentially less error prone than, outward\ntests. Moreover, inward testing does not require the complicated type 1 error\ncontrol of outward tests. A variety of test statistics, employed in both block\nand sequential tests, are compared for their power and errors, in cases\nincluding no outliers, dispersed outliers (the classical slippage alternative),\nand clustered outliers (a case seldom considered). We advocate a density\nmixture approach for detecting clustered outliers. Tests are found to be highly\nsensitive to the correct specification of the main distribution\n(Exponential/Pareto), exposing high potential for errors in inference. Further,\nin five case studies -- financial crashes, nuclear power generation accidents,\nstock market returns, epidemic fatalities, and cities within countries --\nsignificant outliers are detected and related to the concept of \"Dragon King\"\nevents, defined as meaningful outliers of unique origin.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 21:03:50 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Wheatley", "Spencer", ""], ["Sornette", "Didier", ""]]}, {"id": "1507.08726", "submitter": "Jelena Bradic", "authors": "Jelena Bradic", "title": "Robustness in sparse linear models: relative efficiency based on robust\n  approximate message passing", "comments": "49 pages, 10 figures", "journal-ref": "Electronic Journal of Statistics, Volume 10, Number 2 (2016),\n  3894-3944", "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding efficiency in high dimensional linear models is a longstanding\nproblem of interest. Classical work with smaller dimensional problems dating\nback to Huber and Bickel has illustrated the benefits of efficient loss\nfunctions. When the number of parameters $p$ is of the same order as the sample\nsize $n$, $p \\approx n$, an efficiency pattern different from the one of Huber\nwas recently established. In this work, we consider the effects of model\nselection on the estimation efficiency of penalized methods. In particular, we\nexplore whether sparsity, results in new efficiency patterns when $p > n$. In\nthe interest of deriving the asymptotic mean squared error for regularized\nM-estimators, we use the powerful framework of approximate message passing. We\npropose a novel, robust and sparse approximate message passing algorithm\n(RAMP), that is adaptive to the error distribution. Our algorithm includes many\nnon-quadratic and non-differentiable loss functions. We derive its asymptotic\nmean squared error and show its convergence, while allowing $p, n, s \\to\n\\infty$, with $n/p \\in (0,1)$ and $n/s \\in (1,\\infty)$. We identify new\npatterns of relative efficiency regarding a number of penalized $M$ estimators,\nwhen $p$ is much larger than $n$. We show that the classical information bound\nis no longer reachable, even for light--tailed error distributions. We show\nthat the penalized least absolute deviation estimator dominates the penalized\nleast square estimator, in cases of heavy--tailed distributions. We observe\nthis pattern for all choices of the number of non-zero parameters $s$, both $s\n\\leq n$ and $s \\approx n$. In non-penalized problems where $s =p \\approx n$,\nthe opposite regime holds. Therefore, we discover that the presence of model\nselection significantly changes the efficiency patterns.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 01:31:24 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 00:27:46 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bradic", "Jelena", ""]]}, {"id": "1507.08727", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Large Scale Signal Detection: A Unified Perspective", "comments": "Online link of supplementary materials added; copyediting typos\n  corrected", "journal-ref": "Biometrics (2016), 72, 2, 325-334", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an overwhelmingly large literature and algorithms already available\non `large scale inference problems' based on different modeling techniques and\ncultures. Our primary goal in this paper is \\emph{not to add one more new\nmethodology} to the existing toolbox but instead (a) to clarify the mystery how\nthese different simultaneous inference methods are \\emph{connected}, (b) to\nprovide an alternative more intuitive derivation of the formulas that leads to\n\\emph{simpler} expressions, and (c) to develop a \\emph{unified} algorithm for\npractitioners. A detailed discussion on representation, estimation, inference,\nand model selection is given. Applications to a variety of real and simulated\ndatasets show promise. We end with several future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 01:37:02 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 18:40:36 GMT"}, {"version": "v3", "created": "Fri, 31 Mar 2017 14:10:50 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}]