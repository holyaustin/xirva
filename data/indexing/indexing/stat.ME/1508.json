[{"id": "1508.00127", "submitter": "Francesca Greselin", "authors": "Francesca Greselin and Ricardas Zitikis", "title": "Measuring economic inequality and risk: a unifying approach based on\n  personal gambles, societal preferences and references", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The underlying idea behind the construction of indices of economic inequality\nis based on measuring deviations of various portions of low incomes from\ncertain references or benchmarks, that could be point measures like population\nmean or median, or curves like the hypotenuse of the right triangle where every\nLorenz curve falls into. In this paper we argue that by appropriately choosing\npopulation-based references, called societal references, and distributions of\npersonal positions, called gambles, which are random, we can meaningfully unify\nclassical and contemporary indices of economic inequality, as well as various\nmeasures of risk. To illustrate the herein proposed approach, we put forward\nand explore a risk measure that takes into account the relativity of large\nrisks with respect to small ones.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 14:26:41 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Greselin", "Francesca", ""], ["Zitikis", "Ricardas", ""]]}, {"id": "1508.00286", "submitter": "Pierre Latouche", "authors": "Pierre Latouche, St\\'ephane Robin, Sarah Ouadah", "title": "Goodness of fit of logistic models for random graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is a natural and simple tool to understand how covariates\ncontribute to explain the topology of a binary network. Once the model fitted,\nthe practitioner is interested in the goodness-of-fit of the regression in\norder to check if the covariates are sufficient to explain the whole topology\nof the network and, if they are not, to analyze the residual structure. To\naddress this problem, we introduce a generic model that combines logistic\nregression with a network-oriented residual term. This residual term takes the\nform of the graphon function of a W-graph. Using a variational Bayes framework,\nwe infer the residual graphon by averaging over a series of blockwise constant\nfunctions. This approach allows us to define a generic goodness-of-fit\ncriterion, which corresponds to the posterior probability for the residual\ngraphon to be constant. Experiments on toy data are carried out to assess the\naccuracy of the procedure. Several networks from social sciences and ecology\nare studied to illustrate the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 21:02:32 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 14:21:58 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Latouche", "Pierre", ""], ["Robin", "St\u00e9phane", ""], ["Ouadah", "Sarah", ""]]}, {"id": "1508.00298", "submitter": "Kun Lu", "authors": "Hanfang Yang, Kun Lu, Xiang Lyu, and Feifang Hu", "title": "Two-Way Partial AUC and Its Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people evaluate the performance of a diagnostic test, it is important to\ncontrol both True Positive Rate (TPR) and False Positive Rate (FPR). In the\nliterature, most researchers propose the partial area under the ROC curve\n(pAUC) with restrictions on FPR to assess a binary classification system, which\nis named as FPR pAUC. It could be artificially designed to measure the area\ncontrolled by TPR and FPR, but is often misleading conceptually and\npractically. A new and intuitive method, named two-way pAUC, is provided in\nthis paper, which focuses directly on the partial area under the ROC curve with\nboth horizontal and vertical restrictions. We propose a nonparametric estimator\nof two-way pAUC, obtain its asymptotic normality properties and conduct the\nmeasure comparison by bootstrap method. Further, in order to evaluate possible\ncovariate effects on two-way pAUC, regression analysis framework is constructed\nand corresponding theoretical properties are established. Simulation and real\napplication are conducted to support our methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 01:44:34 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 21:08:50 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 01:43:21 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Yang", "Hanfang", ""], ["Lu", "Kun", ""], ["Lyu", "Xiang", ""], ["Hu", "Feifang", ""]]}, {"id": "1508.00436", "submitter": "Nathaniel Shiers", "authors": "Nathaniel Shiers, Piotr Zwiernik, John A. D. Aston, Jim Q. Smith", "title": "The correlation space of Gaussian latent tree models and model selection\n  without fitting", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a complete description of possible covariance matrices consistent\nwith a Gaussian latent tree model for any tree. We then present techniques for\nutilising these constraints to assess whether observed data is compatible with\nthat Gaussian latent tree model. Our method does not require us first to fit\nsuch a tree. We demonstrate the usefulness of the inverse-Wishart distribution\nfor performing preliminary assessments of tree-compatibility using\nsemialgebraic constraints. Using results from Drton et al. (2008) we then\nprovide the appropriate moments required for test statistics for assessing\nadherence to these equality constraints. These are shown to be effective even\nfor small sample sizes and can be easily adjusted to test either the entire\nmodel or only certain macrostructures hypothesized within the tree. We\nillustrate our exploratory tetrad analysis using a linguistic application and\nour confirmatory tetrad analysis using a biological application.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 14:49:30 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 23:36:59 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Shiers", "Nathaniel", ""], ["Zwiernik", "Piotr", ""], ["Aston", "John A. D.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1508.00459", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong, Yue Li, Zhaolei Zhang", "title": "Unsupervised Learning in Genome Informatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With different genomes available, unsupervised learning algorithms are\nessential in learning genome-wide biological insights. Especially, the\nfunctional characterization of different genomes is essential for us to\nunderstand lives. In this book chapter, we review the state-of-the-art\nunsupervised learning algorithms for genome informatics from DNA to MicroRNA.\n  DNA (DeoxyriboNucleic Acid) is the basic component of genomes. A significant\nfraction of DNA regions (transcription factor binding sites) are bound by\nproteins (transcription factors) to regulate gene expression at different\ndevelopment stages in different tissues. To fully understand genetics, it is\nnecessary of us to apply unsupervised learning algorithms to learn and infer\nthose DNA regions. Here we review several unsupervised learning methods for\ndeciphering the genome-wide patterns of those DNA regions.\n  MicroRNA (miRNA), a class of small endogenous non-coding RNA (RiboNucleic\nacid) species, regulate gene expression post-transcriptionally by forming\nimperfect base-pair with the target sites primarily at the 3$'$ untranslated\nregions of the messenger RNAs. Since the 1993 discovery of the first miRNA\n\\emph{let-7} in worms, a vast amount of studies have been dedicated to\nfunctionally characterizing the functional impacts of miRNA in a network\ncontext to understand complex diseases such as cancer. Here we review several\nrepresentative unsupervised learning frameworks on inferring miRNA regulatory\nnetwork by exploiting the static sequence-based information pertinent to the\nprior knowledge of miRNA targeting and the dynamic information of miRNA\nactivities implicated by the recently available large data compendia, which\ninterrogate genome-wide expression profiles of miRNAs and/or mRNAs across\nvarious cell conditions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:52:38 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Wong", "Ka-Chun", ""], ["Li", "Yue", ""], ["Zhang", "Zhaolei", ""]]}, {"id": "1508.00468", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "Evolutionary Algorithms: Concepts, Designs, and Applications in\n  Bioinformatics: Evolutionary Algorithms for Bioinformatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.GN q-bio.QM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since genetic algorithm was proposed by John Holland (Holland J. H., 1975) in\nthe early 1970s, the study of evolutionary algorithm has emerged as a popular\nresearch field (Civicioglu & Besdok, 2013). Researchers from various scientific\nand engineering disciplines have been digging into this field, exploring the\nunique power of evolutionary algorithms (Hadka & Reed, 2013). Many applications\nhave been successfully proposed in the past twenty years. For example,\nmechanical design (Lampinen & Zelinka, 1999), electromagnetic optimization\n(Rahmat-Samii & Michielssen, 1999), environmental protection (Bertini, Felice,\nMoretti, & Pizzuti, 2010), finance (Larkin & Ryan, 2010), musical orchestration\n(Esling, Carpentier, & Agon, 2010), pipe routing (Furuholmen, Glette, Hovin, &\nTorresen, 2010), and nuclear reactor core design (Sacco, Henderson,\nRios-Coelho, Ali, & Pereira, 2009). In particular, its function optimization\ncapability was highlighted (Goldberg & Richardson, 1987) because of its high\nadaptability to different function landscapes, to which we cannot apply\ntraditional optimization techniques (Wong, Leung, & Wong, 2009). Here we review\nthe applications of evolutionary algorithms in bioinformatics.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 16:05:34 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1508.00604", "submitter": "Terrance Savitsky", "authors": "Terrance D. Savitsky", "title": "Bayesian Nonparameteric Multiresolution Estimation for the American\n  Community Survey", "comments": "35 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical methods implemented for small area estimation focus on\nreducing the noise variation in published government official statistics by\nborrowing information among dependent response values. Even the most flexible\nmodels confine parameters defined at the finest scale to link to each data\nobservation in a one-to-one construction. We propose a Bayesian multiresolution\nformulation that utilizes an ensemble of observations at a variety of coarse\nscales in space and time to additively nest parameters we define at a finer\nscale, which serve as our focus for estimation. Our construction is motivated\nby and applied to the estimation of $1-$ year period employment levels, indexed\nby county, from statistics published at coarser areal domains and multi-year\nintervals in the American Community Survey (ACS). We construct a nonparametric\nmixture of Gaussian processes as the prior on a set of regression coefficients\nof county-indexed latent functions over multiple survey years. We evaluate a\nmodified Dirichlet process prior that incorporates county-year predictors as\nthe mixing measure. Each county-year parameter of a latent function is\nestimated from multiple coarse scale observations in space and time to which it\nlinks. The multiresolution formulation is evaluated on synthetic data and\napplied to the ACS.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 21:35:39 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Savitsky", "Terrance D.", ""]]}, {"id": "1508.00615", "submitter": "Terrance Savitsky", "authors": "Terrance D. Savitsky", "title": "Bayesian Nonparametric Functional Mixture Estimation for Time-Series\n  Data, With Application to Estimation of State Employment Totals", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The U.S. Bureau of Labor Statistics use monthly, by-state employment totals\nfrom the Current Population Survey (CPS) as a key input to develop employment\nestimates for counties within the states. The monthly CPS by-state totals,\nhowever, express high levels of volatility that compromise the accuracy of\nresulting estimates composed for the counties. Typically-employed models for\nsmall area estimation produce de-noised, state-level employment estimates by\nborrowing information over the survey months, but assume independence among the\ncollection of by-state time series, which is typically violated due to\nsimilarities in their underlying economies. We construct Gaussian process and\nGaussian Markov random field alternative functional prior specifications, each\nin a mixture of multivariate Gaussian distributions with a Dirichlet process\n(DP) mixing measure over the parameters of their covariance or precision\nmatrices. Our DP mixture of functions models allow the data to simultaneously\nestimate a dependence among the months and between states. A feature of our\nmodels is that those functions assigned to the same cluster are drawn from a\ndistribution with the same covariance parameters, so that they are similar, but\ndon't have to be identical. We compare the performances of our two alternatives\non synthetic data and apply them to recover de-noised, by-state CPS employment\ntotals for data from $2000-2013$.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 23:16:01 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Savitsky", "Terrance D.", ""]]}, {"id": "1508.00635", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Bayesian mixtures of spatial spline regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work relates the framework of model-based clustering for spatial\nfunctional data where the data are surfaces. We first introduce a Bayesian\nspatial spline regression model with mixed-effects (BSSR) for modeling spatial\nfunction data. The BSSR model is based on Nodal basis functions for spatial\nregression and accommodates both common mean behavior for the data through a\nfixed-effects part, and variability inter-individuals thanks to a\nrandom-effects part. Then, in order to model populations of spatial functional\ndata issued from heterogeneous groups, we integrate the BSSR model into a\nmixture framework. The resulting model is a Bayesian mixture of spatial spline\nregressions with mixed-effects (BMSSR) used for density estimation and\nmodel-based surface clustering. The models, through their Bayesian formulation,\nallow to integrate possible prior knowledge on the data structure and\nconstitute a good alternative to recent mixture of spatial spline regressions\nmodel estimated in a maximum likelihood framework via the\nexpectation-maximization (EM) algorithm. The Bayesian model inference is\nperformed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs\nsampler to infer the BSSR and the BMSSR models and apply them on simulated\nsurfaces and a real problem of handwritten digit recognition using the MNIST\ndata set. The obtained results highlight the potential benefit of the proposed\nBayesian approaches for modeling surfaces possibly dispersed in particular in\nclusters.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 01:29:49 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1508.00793", "submitter": "Konstantinos Perrakis", "authors": "Dimitris Fouskakis, Ioannis Ntzoufras, Konstantinos Perrakis", "title": "Power-Expected-Posterior Priors for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power-expected-posterior (PEP) prior provides an objective, automatic,\nconsistent and parsimonious model selection procedure. At the same time it\nresolves the conceptual and computational problems due to the use of imaginary\ndata. Namely, (i) it dispenses with the need to select and average across all\npossible minimal imaginary samples, and (ii) it diminishes the effect that the\nimaginary data have upon the posterior distribution. These attributes allow for\nlarge sample approximations, when needed, in order to reduce the computational\nburden under more complex models. In this work we generalize the applicability\nof the PEP methodology, focusing on the framework of generalized linear models\n(GLMs), by introducing two new PEP definitions which are in effect applicable\nto any general model setting. Hyper-prior extensions for the power parameter\nthat regulates the contribution of the imaginary data are introduced. We\nfurther study the validity of the predictive matching and of the model\nselection consistency, providing analytical proofs for the former and empirical\nevidence supporting the latter. For estimation of posterior model and inclusion\nprobabilities we introduce a tuning-free Gibbs-based variable selection\nsampler. Several simulation scenarios and one real life example are considered\nin order to evaluate the performance of the proposed methods compared to other\ncommonly used approaches based on mixtures of g-priors. Results indicate that\nthe GLM-PEP priors are more effective in the identification of sparse and\nparsimonious model formulations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 15:05:23 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 12:34:22 GMT"}, {"version": "v3", "created": "Mon, 15 Aug 2016 13:54:40 GMT"}, {"version": "v4", "created": "Fri, 29 Sep 2017 16:58:46 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Fouskakis", "Dimitris", ""], ["Ntzoufras", "Ioannis", ""], ["Perrakis", "Konstantinos", ""]]}, {"id": "1508.00934", "submitter": "Jingshu Wang", "authors": "Jingshu Wang, Art B. Owen", "title": "Admissibility in Partial Conjunction Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis combines results from multiple studies aiming to increase power\nin finding their common effect. It would typically reject the null hypothesis\nof no effect if any one of the studies shows strong significance. The partial\nconjunction null hypothesis is rejected only when at least $r$ of $n$ component\nhypotheses are non-null with $r = 1$ corresponding to a usual meta-analysis.\nCompared with meta-analysis, it can encourage replicable findings across\nstudies. A by-product of it when applied to different $r$ values is a\nconfidence interval of $r$ quantifying the proportion of non-null studies.\nBenjamini and Heller (2008) provided a valid test for the partial conjunction\nnull by ignoring the $r - 1$ smallest p-values and applying a valid\nmeta-analysis p-value to the remaining $n - r + 1$ p-values. We provide\nsufficient and necessary conditions of admissible combined p-value for the\npartial conjunction hypothesis among monotone tests. Non-monotone tests always\ndominate monotone tests but are usually too unreasonable to be used in\npractice. Based on these findings, we propose a generalized form of Benjamini\nand Heller's test which allows usage of various types of meta-analysis\np-values, and apply our method to an example in assessing replicable benefit of\nnew anticoagulants across subgroups of patients for stroke prevention.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 22:52:16 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 19:14:02 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 09:49:37 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Wang", "Jingshu", ""], ["Owen", "Art B.", ""]]}, {"id": "1508.00947", "submitter": "Bala Rajaratnam", "authors": "Bala Rajaratnam and Doug Sparks", "title": "MCMC-Based Inference in the Era of Big Data: A Fundamental Analysis of\n  the Convergence Complexity of High-Dimensional Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) lies at the core of modern Bayesian\nmethodology, much of which would be impossible without it. Thus, the\nconvergence properties of MCMCs have received significant attention, and in\nparticular, proving (geometric) ergodicity is of critical interest. Trust in\nthe ability of MCMCs to sample from modern-day high-dimensional posteriors,\nhowever, has been limited by a widespread perception that these chains\ntypically experience serious convergence problems. In this paper, we first\ndemonstrate that contemporary methods for obtaining convergence rates have\nserious limitations when the dimension grows. We then propose a framework for\nrigorously establishing the convergence behavior of commonly used\nhigh-dimensional MCMCs. In particular, we demonstrate theoretically the precise\nnature and severity of the convergence problems of popular MCMCs when\nimplemented in high dimensions, including phase transitions in the convergence\nrates in various $n$ and $p$ regimes, and a universality result across an\nentire spectrum of models. We also show that convergence problems effectively\neliminate the apparent safeguard of geometric ergodicity. We then demonstrate\ntheoretical principles by which MCMCs can be constructed and analyzed to yield\nbounded geometric convergence rates even as the dimension $p$ grows without\nbound. Additionally, we propose a diagnostic tool for establishing convergence.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 01:11:51 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 01:15:22 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Rajaratnam", "Bala", ""], ["Sparks", "Doug", ""]]}, {"id": "1508.01105", "submitter": "Ruiyan Luo", "authors": "Ruiyan Luo and Xin Qi", "title": "Signal extraction approach for sparse multivariate response regression", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider multivariate response regression models with high\ndimensional predictor variables. One way to model the correlation among the\nresponse variables is through the low rank decomposition of the coefficient\nmatrix, which has been considered by several papers for the high dimensional\npredictors. However, all these papers focus on the singular value decomposition\nof the coefficient matrix. Our target is the decomposition of the coefficient\nmatrix which leads to the best lower rank approximation to the regression\nfunction, the signal part in the response. Given any rank, this decomposition\nhas nearly the smallest expected prediction error among all approximations to\nthe the coefficient matrix with the same rank. To estimate the decomposition,\nwe formulate a penalized generalized eigenvalue problem to obtain the first\nmatrix in the decomposition and then obtain the second one by a least squares\nmethod. In the high-dimensional setting, we establish the oracle inequalities\nfor the estimates. Compared to the existing theoretical results, we have less\nrestrictions on the distribution of the noise vector in each observation and\nallow correlations among its coordinates. Our theoretical results do not depend\non the dimension of the multivariate response. Therefore, the dimension is\narbitrary and can be larger than the sample size and the dimension of the\npredictor. Simulation studies and application to real data show that the\nproposed method has good prediction performance and is efficient in dimension\nreduction for various reduced rank models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 15:29:15 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Luo", "Ruiyan", ""], ["Qi", "Xin", ""]]}, {"id": "1508.01113", "submitter": "Ruiyan Luo", "authors": "Ruiyan Luo and Xin Qi", "title": "Sparse Fisher's discriminant analysis with thresholded linear\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various regularized linear discriminant analysis (LDA) methods have been\nproposed to address the problems of the classic methods in high-dimensional\nsettings. Asymptotic optimality has been established for some of these methods\nin high dimension when there are only two classes. A major difficulty in\nproving asymptotic optimality for multiclass classification is that the\nclassification boundary is typically complicated and no explicit formula for\nclassification error generally exists when the number of classes is greater\nthan two. For the Fisher's LDA, one additional difficulty is that the\ncovariance matrix is also involved in the linear constraints. The main purpose\nof this paper is to establish asymptotic consistency and asymptotic optimality\nfor our sparse Fisher's LDA with thresholded linear constraints in the\nhigh-dimensional settings for arbitrary number of classes. To address the first\ndifficulty above, we provide asymptotic optimality and the corresponding\nconvergence rates in high-dimensional settings for a large family of linear\nclassification rules with arbitrary number of classes, and apply them to our\nmethod. To overcome the second difficulty, we propose a thresholding approach\nto avoid the estimate of the covariance matrix. We apply the method to the\nclassification problems for multivariate functional data through the wavelet\ntransformations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 15:55:58 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Luo", "Ruiyan", ""], ["Qi", "Xin", ""]]}, {"id": "1508.01126", "submitter": "Stanislav Volgushev", "authors": "Srijan Sengupta, Stanislav Volgushev, and Xiaofeng Shao", "title": "A subsampled double bootstrap for massive data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bootstrap is a popular and powerful method for assessing precision of\nestimators and inferential methods. However, for massive datasets which are\nincreasingly prevalent, the bootstrap becomes prohibitively costly in\ncomputation and its feasibility is questionable even with modern parallel\ncomputing platforms. Recently Kleiner, Talwalkar, Sarkar, and Jordan (2014)\nproposed a method called BLB (Bag of Little Bootstraps) for massive data which\nis more computationally scalable with little sacrifice of statistical accuracy.\nBuilding on BLB and the idea of fast double bootstrap, we propose a new\nresampling method, the subsampled double bootstrap, for both independent data\nand time series data. We establish consistency of the subsampled double\nbootstrap under mild conditions for both independent and dependent cases.\nMethodologically, the subsampled double bootstrap is superior to BLB in terms\nof running time, more sample coverage and automatic implementation with less\ntuning parameters for a given time budget. Its advantage relative to BLB and\nbootstrap is also demonstrated in numerical simulations and a data\nillustration.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 16:44:49 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Sengupta", "Srijan", ""], ["Volgushev", "Stanislav", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1508.01146", "submitter": "Michael Beyer", "authors": "Michael E. Beyer", "title": "Minimizing the CDF Path Length: A Novel Perspective on Uniformity and\n  Uncertainty of Bounded Distributions", "comments": "Draft Paper/Idea Sketch. 13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An index of uniformity is developed as an alternative to the maximum-entropy\nprinciple for selecting continuous, differentiable probability distributions\n$\\mathcal{P}$ subject to constraints $C$. The uniformity index developed in\nthis paper is motivated by the observation that among all differentiable\nprobability distributions defined on a finite interval $[a,b] \\in \\mathbb{R}$,\nit is the uniform probability distribution that minimizes the path length of\nthe associated cumulative distribution function $F_{\\mathcal{P}}$ on $[a,b]$.\nThis intuition is extended to situations where there are constraints on the\nallowable probability distributions. In particular, constraints on the first\nand second raw moments of a distribution are discussed in detail, including the\nanalytical form of the solutions and numerical studies of particular examples.\nThe resulting \"shortest path\" distributions are found to be decidedly more\nheavy-tailed than the associated maximum-entropy distributions, suggesting that\nentropy and \"CDF path length\" measure two different aspects of uncertainty for\nbounded distributions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 17:45:41 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 13:15:26 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Beyer", "Michael E.", ""]]}, {"id": "1508.01167", "submitter": "Elizabeth Roberto", "authors": "Elizabeth Roberto", "title": "The Divergence Index: A Decomposable Measure of Segregation and\n  Inequality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposition analysis is a critical tool for understanding the social and\nspatial dimensions of inequality, segregation, and diversity. In this paper, I\npropose a new measure - the Divergence Index - to address the need for a\ndecomposable measure of segregation. Although the Information Theory Index has\nbeen used to decompose segregation within and between communities, I argue that\nit measures relative diversity not segregation. I demonstrate the importance of\nthis conceptual distinction with two empirical analyses: I decompose\nsegregation and relative homogeneity in the Detroit metropolitan area, and I\nanalyze the relationship between the indexes in the 100 largest U.S. cities. I\nshow that it is problematic to interpret the Information Theory Index as a\nmeasure of segregation, especially when analyzing local-level results or any\ndecomposition of overall results. Segregation and diversity are important\naspects of residential differentiation, and it is critical that we study each\nconcept as the structure and stratification of the U.S. population becomes more\ncomplex.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 18:50:35 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 17:50:57 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Roberto", "Elizabeth", ""]]}, {"id": "1508.01217", "submitter": "Lorin Crawford", "authors": "Lorin Crawford, Kris C. Wood, Xiang Zhou, and Sayan Mukherjee", "title": "Bayesian Approximate Kernel Regression with Variable Selection", "comments": "22 pages, 3 figures, 3 tables; theory added; new simulations\n  presented; references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear kernel regression models are often used in statistics and machine\nlearning because they are more accurate than linear models. Variable selection\nfor kernel regression models is a challenge partly because, unlike the linear\nregression setting, there is no clear concept of an effect size for regression\ncoefficients. In this paper, we propose a novel framework that provides an\neffect size analog of each explanatory variable for Bayesian kernel regression\nmodels when the kernel is shift-invariant --- for example, the Gaussian kernel.\nWe use function analytic properties of shift-invariant reproducing kernel\nHilbert spaces (RKHS) to define a linear vector space that: (i) captures\nnonlinear structure, and (ii) can be projected onto the original explanatory\nvariables. The projection onto the original explanatory variables serves as an\nanalog of effect sizes. The specific function analytic property we use is that\nshift-invariant kernel functions can be approximated via random Fourier bases.\nBased on the random Fourier expansion we propose a computationally efficient\nclass of Bayesian approximate kernel regression (BAKR) models for both\nnonlinear regression and binary classification for which one can compute an\nanalog of effect sizes. We illustrate the utility of BAKR by examining two\nimportant problems in statistical genetics: genomic selection (i.e. phenotypic\nprediction) and association mapping (i.e. inference of significant variants or\nloci). State-of-the-art methods for genomic selection and association mapping\nare based on kernel regression and linear models, respectively. BAKR is the\nfirst method that is competitive in both settings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 20:40:11 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 14:37:03 GMT"}, {"version": "v3", "created": "Tue, 16 May 2017 18:22:14 GMT"}, {"version": "v4", "created": "Sat, 10 Jun 2017 00:57:28 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Crawford", "Lorin", ""], ["Wood", "Kris C.", ""], ["Zhou", "Xiang", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1508.01227", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over, Guido Knapp, Tim Friede", "title": "Hartung-Knapp-Sidik-Jonkman approach and its modification for\n  random-effects meta-analysis with few studies", "comments": "7 pages, 3 figures", "journal-ref": "BMC Medical Research Methodology, 15:99, 2015", "doi": "10.1186/s12874-015-0091-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND: Random-effects meta-analysis is commonly performed by first\nderiving an estimate of the between-study variation, the heterogeneity, and\nsubsequently using this as the basis for combining results, i.e., for\nestimating the effect, the figure of primary interest. The heterogeneity\nvariance estimate however is commonly associated with substantial uncertainty,\nespecially in contexts where there are only few studies available, such as in\nsmall populations and rare diseases.\n  METHODS: Confidence intervals and tests for the effect may be constructed via\na simple normal approximation, or via a Student-t distribution, using the\nHartung-Knapp-Sidik-Jonkman (HKSJ) approach, which additionally uses a refined\nestimator of variance of the effect estimator. The modified Knapp-Hartung\nmethod (mKH) applies an ad hoc correction and has been proposed to prevent\ncounterintuitive effects and to yield more conservative inference. We performed\na simulation study to investigate the behaviour of the standard HKSJ and\nmodified mKH procedures in a range of circumstances, with a focus on the common\ncase of meta-analysis based on only a few studies.\n  RESULTS: The standard HKSJ procedure works well when the treatment effect\nestimates to be combined are of comparable precision, but nominal error levels\nare exceeded when standard errors vary considerably between studies (e.g. due\nto variations in study size). Application of the modification on the other hand\nyields more conservative results with error rates closer to the nominal level.\nDifferences are most pronounced in the common case of few studies of varying\nsize or precision.\n  CONCLUSIONS: Use of the modified mKH procedure is recommended, especially\nwhen only a few studies contribute to the meta-analysis and the involved\nstudies' precisions (standard errors) vary.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 21:09:41 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 11:21:22 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 12:44:26 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Knapp", "Guido", ""], ["Friede", "Tim", ""]]}, {"id": "1508.01280", "submitter": "Zhou Fan", "authors": "Zhou Fan and Lester Mackey", "title": "Empirical Bayesian analysis of simultaneous changepoints in multiple\n  data sequences", "comments": "31 pages, 11 figures v3: Modify synthetic data comparisons based on\n  reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy number variations in cancer cells and volatility fluctuations in stock\nprices are commonly manifested as changepoints occurring at the same positions\nacross related data sequences. We introduce a Bayesian modeling framework,\nBASIC, that employs a changepoint prior to capture the co-occurrence tendency\nin data of this type. We design efficient algorithms to sample from and\nmaximize over the BASIC changepoint posterior and develop a Monte Carlo\nexpectation-maximization procedure to select prior hyperparameters in an\nempirical Bayes fashion. We use the resulting BASIC framework to analyze DNA\ncopy number variations in the NCI-60 cancer cell lines and to identify\nimportant events that affected the price volatility of S&P 500 stocks from 2000\nto 2009.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 04:42:37 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2016 00:48:27 GMT"}, {"version": "v3", "created": "Fri, 14 Apr 2017 03:08:27 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Fan", "Zhou", ""], ["Mackey", "Lester", ""]]}, {"id": "1508.01302", "submitter": "Francesco Donat", "authors": "Francesco Donat, Giampiero Marra", "title": "Discrete Responses in Bivariate Generalized Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conceptual framework for the analysis of dichotomous and ordinal\npolychotomous responses within a penalized multivariate Generalized Linear\nModel is introduced. The proposed structure allows for a rather flexible\npredictor specification through the inclusion of non-parametric and spatial\ncovariate effects, and the characterisation of the distribution of the\nstochastic model components with copulae of univariate marginals. Analytic\nderivations for the particular case of Gaussian marginals within a bivariate\nsystem of dichotomous outcomes are also provided, and the framework is\nsubsequently illustrated through the estimation of the HIV prevalence in Zambia\nusing the 2007 DHS dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 07:44:54 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Donat", "Francesco", ""], ["Marra", "Giampiero", ""]]}, {"id": "1508.01378", "submitter": "Whitney Newey", "authors": "Hidehiko Ichimura and Whitney K. Newey", "title": "The Influence Function of Semiparametric Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many useful parameters depend on nonparametric first steps. Examples include\ngames, dynamic discrete choice, average exact consumer surplus, and treatment\neffects. Often estimators of these parameters are asymptotically equivalent to\na sample average of an object referred to as the influence function. The\ninfluence function is useful in local policy analysis, in evaluating local\nsensitivity of estimators, constructing debiased machine learning estimators,\nin efficiency comparisons, and in formulating primitive regularity conditions\nfor asymptotic normality, We show that the influence function is a Gateaux\nderivative with respect to a smooth deviation evaluated at a point mass. This\nresult generalizes the classic Von Mises (1947) and Hampel (1974) calculation\nto estimators that depend on smooth nonparametric first steps. We give explicit\ninfluence functions for first steps that satisfy exogenous or endogenous\northogonality conditions. We use these results to generalize the omitted\nvariable bias formula for regression to policy analysis for and sensitivity to\nstructural changes. We apply this analysis and find no sensitivity to\nendogeneity of average equivalent variation estimates in a gasoline demand\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 12:54:28 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 13:46:50 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ichimura", "Hidehiko", ""], ["Newey", "Whitney K.", ""]]}, {"id": "1508.01451", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley, Christopher K. Wikle, Scott H. Holan", "title": "Spatio-Temporal Change of Support with Application to American Community\n  Survey Multi-Year Period Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present hierarchical Bayesian methodology to perform spatio-temporal\nchange of support (COS) for survey data with Gaussian sampling errors. This\nmethodology is motivated by the American Community Survey (ACS), which is an\nongoing survey administered by the U.S. Census Bureau that provides timely\ninformation on several key demographic variables. The ACS has published 1-year,\n3-year, and 5-year period-estimates, and margins of errors, for demographic and\nsocio-economic variables recorded over predefined geographies. The\nspatio-temporal COS methodology considered here provides data users with a way\nto estimate ACS variables on customized geographies and time periods, while\naccounting for sampling errors. Additionally, 3-year ACS period estimates are\nto be discontinued, and this methodology can provide predictions of ACS\nvariables for 3-year periods given the available period estimates. The\nmethodology is based on a spatio-temporal mixed effects model with a\nlow-dimensional spatio-temporal basis function representation, which provides\nmulti-resolution estimates through basis function aggregation in space and\ntime. This methodology includes a novel parameterization that uses a target\ndynamical process and recently proposed parsimonious Moran's I propagator\nstructures. Our approach is demonstrated through two applications using\npublic-use ACS estimates, and is shown to produce good predictions on a holdout\nset of 3-year period estimates.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 16:28:03 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 16:05:09 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Wikle", "Christopher K.", ""], ["Holan", "Scott H.", ""]]}, {"id": "1508.01605", "submitter": "Pallavi Basu", "authors": "Pallavi Basu, T. Tony Cai, Kiranmoy Das, and Wenguang Sun", "title": "Weighted False Discovery Rate Control in Large-Scale Multiple Testing", "comments": "Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of weights provides an effective strategy to incorporate prior domain\nknowledge in large-scale inference. This paper studies weighted multiple\ntesting in a decision-theoretic framework. We develop oracle and data-driven\nprocedures that aim to maximize the expected number of true positives subject\nto a constraint on the weighted false discovery rate. The asymptotic validity\nand optimality of the proposed methods are established. The results demonstrate\nthat incorporating informative domain knowledge enhances the interpretability\nof results and precision of inference. Simulation studies show that the\nproposed method controls the error rate at the nominal level, and the gain in\npower over existing methods is substantial in many settings. An application to\ngenome-wide association study is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 05:09:43 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 11:22:52 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Basu", "Pallavi", ""], ["Cai", "T. Tony", ""], ["Das", "Kiranmoy", ""], ["Sun", "Wenguang", ""]]}, {"id": "1508.01625", "submitter": "Koby Todros", "authors": "Koby Todros and Alfred O. Hero", "title": "Robust Multiple Signal Classification via Probability Measure\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new framework for robust multiple signal\nclassification (MUSIC). The proposed framework, called robust\nmeasure-transformed (MT) MUSIC, is based on applying a transform to the\nprobability distribution of the received signals, i.e., transformation of the\nprobability measure defined on the observation space. In robust MT-MUSIC, the\nsample covariance is replaced by the empirical MT-covariance. By judicious\nchoice of the transform we show that: 1) the resulting empirical MT-covariance\nis B-robust, with bounded influence function that takes negligible values for\nlarge norm outliers, and 2) under the assumption of spherically contoured noise\ndistribution, the noise subspace can be determined from the eigendecomposition\nof the MT-covariance. Furthermore, we derive a new robust measure-transformed\nminimum description length (MDL) criterion for estimating the number of\nsignals, and extend the MT-MUSIC framework to the case of coherent signals. The\nproposed approach is illustrated in simulation examples that show its\nadvantages as compared to other robust MUSIC and MDL generalizations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 07:12:18 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Todros", "Koby", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1508.01641", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Yuki Kawakubo and Kota Ogasawara", "title": "Small Area Estimation with Spatially Varying Natural Exponential\n  Families", "comments": "30 pages; to appear in Journal of Statistical Computational and\n  Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stage hierarchical models have been widely used in small area estimation\nto produce indirect estimates of areal means. When the areas are treated\nexchangeably and the model parameters are assumed to be the same over all\nareas, we might lose the efficiency in the presence of spatial heterogeneity.\nTo overcome this problem, we consider a two-stage area-level model based on\nnatural exponential family with spatially varying model parameters. We employ\ngeographically weighted regression approach to estimating the varying\nparameters and suggest a new empirical Bayes estimator of the areal mean. We\nalso discuss some related problems, including the mean squared error\nestimation, benchmarked estimation, and estimation in non-sampled areas. The\nperformance of the proposed method is evaluated through simulations and\napplications to two data sets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 09:03:42 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2017 10:41:18 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 01:13:36 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kawakubo", "Yuki", ""], ["Ogasawara", "Kota", ""]]}, {"id": "1508.01686", "submitter": "Jona Cederbaum", "authors": "Jona Cederbaum, Marianne Pouplier, Phil Hoole, and Sonja Greven", "title": "Functional Linear Mixed Models for Irregularly or Sparsely Sampled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimation approach to analyse correlated functional data which\nare observed on unequal grids or even sparsely. The model we use is a\nfunctional linear mixed model, a functional analogue of the linear mixed model.\nEstimation is based on dimension reduction via functional principal component\nanalysis and on mixed model methodology. Our procedure allows the decomposition\nof the variability in the data as well as the estimation of mean effects of\ninterest and borrows strength across curves. Confidence bands for mean effects\ncan be constructed conditional on estimated principal components. We provide\nR-code implementing our approach. The method is motivated by and applied to\ndata from speech production research.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 13:33:55 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Cederbaum", "Jona", ""], ["Pouplier", "Marianne", ""], ["Hoole", "Phil", ""], ["Greven", "Sonja", ""]]}, {"id": "1508.01695", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "Graphical tools for model-based mixture discriminant analysis", "comments": null, "journal-ref": "Advances in Data Analysis and Classification 8/2 (2014) 147-165", "doi": "10.1007/s11634-013-0147-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a methodology for visualizing on a dimension reduced\nsubspace the classification structure and the geometric characteristics induced\nby an estimated Gaussian mixture model for discriminant analysis. In\nparticular, we consider the case of mixture of mixture models with varying\nparametrization which allow for parsimonious models. The approach is an\nextension of an existing work on reducing dimensionality for model-based\nclustering based on Gaussian mixtures. Information on the dimension reduction\nsubspace is provided by the variation on class locations and, depending on the\nestimated mixture model, on the variation on class dispersions. Projections\nalong the estimated directions provide summary plots which help to visualize\nthe structure of the classes and their characteristics. A suitable modification\nof the method allows us to recover the most discriminant directions, i.e.,\nthose that show maximal separation among classes. The approach is illustrated\nusing simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 14:03:35 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "1508.01713", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "Dimension reduction for model-based clustering", "comments": null, "journal-ref": "Statistics and Computing 20/4 (2010) 471-484", "doi": "10.1007/s11222-009-9138-7", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dimension reduction method for visualizing the clustering\nstructure obtained from a finite mixture of Gaussian densities. Information on\nthe dimension reduction subspace is obtained from the variation on group means\nand, depending on the estimated mixture model, on the variation on group\ncovariances. The proposed method aims at reducing the dimensionality by\nidentifying a set of linear combinations, ordered by importance as quantified\nby the associated eigenvalues, of the original features which capture most of\nthe cluster structure contained in the data. Observations may then be projected\nonto such a reduced subspace, thus providing summary plots which help to\nvisualize the clustering structure. These plots can be particularly appealing\nin the case of high-dimensional data and noisy structure. The new constructed\nvariables capture most of the clustering information available in the data, and\nthey can be further reduced to improve clustering performance. We illustrate\nthe approach on both simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 14:54:03 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "1508.01782", "submitter": "Ali Akbar Jafari", "authors": "Saba Aghadoust and Kamel Abdollahnezhad and Farhad Yaghmaei and Ali\n  Akbar Jafari", "title": "Comparison of some Different Methods for Hypothesis Test of Means of\n  Log-normal Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-normal distribution is used to describe the positive data, that it\nhas skewed distribution with small mean and large variance. This distribution\nhas application in many sciences for example medicine, economics, biology and\nalimentary science, ect. Comparison of means of several log-normal populations\nalways has been in focus of researchers, but the test statistic are not easy to\nderive or extremely complicated for this comparisons. In this paper, the\ndifferent methods exist for this testing that we can point out F-test,\nlikelihood ratio test, generalized p-value approach and computational approach\ntest. In this line with help of simulation studies, in this methods we compare\nand evaluate size and power test.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 19:11:47 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Aghadoust", "Saba", ""], ["Abdollahnezhad", "Kamel", ""], ["Yaghmaei", "Farhad", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1508.01834", "submitter": "Bryant Chen", "authors": "Bryant Chen", "title": "Decomposition and Identification of Linear Structural Equation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of identifying linear structural\nequation models. We first extend the edge set half-trek criterion to cover a\nbroader class of models. We then show that any semi-Markovian linear model can\nbe recursively decomposed into simpler sub-models, resulting in improved\nidentification power. Finally, we show that, unlike the existing methods\ndeveloped for linear models, the resulting method subsumes the identification\nalgorithm of non-parametric models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 23:05:13 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Chen", "Bryant", ""]]}, {"id": "1508.01913", "submitter": "Tsagris Michail", "authors": "Michail Tsagris", "title": "Regression analysis with compositional data containing zero values", "comments": "The paper has been accepted for publication in the Chilean Journal of\n  Statistics. It consists of 12 pages with 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression analysis with compositional data containing zero values\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 15:01:49 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Tsagris", "Michail", ""]]}, {"id": "1508.01922", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder and Peter Radchenko", "title": "The Discrete Dantzig Selector: Estimating Sparse Linear Models via Mixed\n  Integer Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel high-dimensional linear regression estimator: the Discrete\nDantzig Selector, which minimizes the number of nonzero regression coefficients\nsubject to a budget on the maximal absolute correlation between the features\nand residuals. Motivated by the significant advances in integer optimization\nover the past 10-15 years, we present a Mixed Integer Linear Optimization\n(MILO) approach to obtain certifiably optimal global solutions to this\nnonconvex optimization problem. The current state of algorithmics in integer\noptimization makes our proposal substantially more computationally attractive\nthan the least squares subset selection framework based on integer quadratic\noptimization, recently proposed in [8] and the continuous nonconvex quadratic\noptimization framework of [33]. We propose new discrete first-order methods,\nwhich when paired with state-of-the-art MILO solvers, lead to good solutions\nfor the Discrete Dantzig Selector problem for a given computational budget. We\nillustrate that our integrated approach provides globally optimal solutions in\nsignificantly shorter computation times, when compared to off-the-shelf MILO\nsolvers. We demonstrate both theoretically and empirically that in a wide range\nof regimes the statistical properties of the Discrete Dantzig Selector are\nsuperior to those of popular $\\ell_{1}$-based approaches. We illustrate that\nour approach can handle problem instances with p = 10,000 features with\ncertifiable optimality making it a highly scalable combinatorial variable\nselection approach in sparse linear modeling.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 16:13:07 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 00:52:56 GMT"}, {"version": "v3", "created": "Thu, 19 Jan 2017 05:48:12 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""]]}, {"id": "1508.01939", "submitter": "Xi Luo", "authors": "Florentina Bunea, Christophe Giraud, Xi Luo, Martin Royer, Nicolas\n  Verzelen", "title": "Model Assisted Variable Clustering: Minimax-optimal Recovery and\n  Algorithms", "comments": "Maintext: 38 pages; supplementary information: 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering defines population level clusters relative to a model\nthat embeds notions of similarity. Algorithms tailored to such models yield\nestimated clusters with a clear statistical interpretation. We take this view\nhere and introduce the class of G-block covariance models as a background model\nfor variable clustering. In such models, two variables in a cluster are deemed\nsimilar if they have similar associations will all other variables. This can\narise, for instance, when groups of variables are noise corrupted versions of\nthe same latent factor. We quantify the difficulty of clustering data generated\nfrom a G-block covariance model in terms of cluster proximity, measured with\nrespect to two related, but different, cluster separation metrics. We derive\nminimax cluster separation thresholds, which are the metric values below which\nno algorithm can recover the model-defined clusters exactly, and show that they\nare different for the two metrics. We therefore develop two algorithms, COD and\nPECOK, tailored to G-block covariance models, and study their\nminimax-optimality with respect to each metric. Of independent interest is the\nfact that the analysis of the PECOK algorithm, which is based on a corrected\nconvex relaxation of the popular K-means algorithm, provides the first\nstatistical analysis of such algorithms for variable clustering. Additionally,\nwe contrast our methods with another popular clustering method, spectral\nclustering, specialized to variable clustering, and show that ensuring exact\ncluster recovery via this method requires clusters to have a higher separation,\nrelative to the minimax threshold. Extensive simulation studies, as well as our\ndata analyses, confirm the applicability of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 18:25:16 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 18:30:48 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 03:13:09 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 03:32:02 GMT"}, {"version": "v5", "created": "Thu, 13 Dec 2018 03:33:17 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Bunea", "Florentina", ""], ["Giraud", "Christophe", ""], ["Luo", "Xi", ""], ["Royer", "Martin", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1508.02186", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "Model-based SIR for dimension reduction", "comments": null, "journal-ref": "Computational Statistics and Data Analysis 55/11 (2011) pp.\n  3010-3026", "doi": "10.1016/j.csda.2011.05.006", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new dimension reduction method based on Gaussian finite mixtures is\nproposed as an extension to sliced inverse regression (SIR). The model-based\nSIR (MSIR) approach allows the main limitation of SIR to be overcome, i.e.,\nfailure in the presence of regression symmetric relationships, without the need\nto impose further assumptions. Extensive numerical studies are presented to\ncompare the new method with some of most popular dimension reduction methods,\nsuch as SIR, sliced average variance estimation, principal Hessian direction,\nand directional regression. MSIR appears sufficiently flexible to accommodate\nvarious regression functions, and its performance is comparable with or better,\nparticularly as sample size grows, than other available methods. Lastly, MSIR\nis illustrated with two real data examples about ozone concentration\nregression, and hand-written digit classification.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 09:31:01 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "1508.02498", "submitter": "Zeng Li", "authors": "Zeng Li and Jianfeng Yao", "title": "Testing the Sphericity of a covariance matrix when the dimension is much\n  larger than the sample size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the prominent sphericity test when the dimension $p$ is\nmuch lager than sample size $n$. The classical likelihood ratio test(LRT) is no\nlonger applicable when $p\\gg n$. Therefore a Quasi-LRT is proposed and\nasymptotic distribution of the test statistic under the null when\n$p/n\\rightarrow\\infty, n\\rightarrow\\infty$ is well established in this paper.\nMeanwhile, John's test has been found to possess the powerful {\\it\ndimension-proof} property, which keeps exactly the same limiting distribution\nunder the null with any $(n,p)$-asymptotic, i.e. $p/n\\rightarrow[0,\\infty]$,\n$n\\rightarrow\\infty$. All asymptotic results are derived for general population\nwith finite fourth order moment. Numerical experiments are implemented for\ncomparison.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 07:20:25 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 09:12:13 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 08:08:39 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Li", "Zeng", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1508.02502", "submitter": "Juho Piironen", "authors": "Juho Piironen and Aki Vehtari", "title": "Projection predictive variable selection using Stan+R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is additional material to our previous study comparing several\nstrategies for variable subset selection. Our recommended approach was to fit\nthe full model with all the candidate variables and best possible prior\ninformation, and perform the variable selection using the projection predictive\nframework. Here we give an example of performing such an analysis, using Stan\nfor fitting the model, and R for the variable selection.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 07:35:17 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Piironen", "Juho", ""], ["Vehtari", "Aki", ""]]}, {"id": "1508.02651", "submitter": "Konstantinos Spiliopoulos", "authors": "Alexandra Chronopoulou and Konstantinos Spiliopoulos", "title": "Sequential Monte Carlo for fractional Stochastic Volatility Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a fractional stochastic volatility model, that is a\nmodel in which the volatility may exhibit a long-range dependent or a\nrough/antipersistent behavior. We propose a dynamic sequential Monte Carlo\nmethodology that is applicable to both long memory and antipersistent processes\nin order to estimate the volatility as well as the unknown parameters of the\nmodel. We establish a central limit theorem for the state and parameter filters\nand we study asymptotic properties (consistency and asymptotic normality) for\nthe filter. We illustrate our results with a simulation study and we apply our\nmethod to estimating the volatility and the parameters of a long-range\ndependent model for S&P 500 data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 16:36:53 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 20:14:32 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Chronopoulou", "Alexandra", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1508.02749", "submitter": "Georg Mainik", "authors": "Georg Mainik", "title": "Risk aggregation with empirical margins: Latin hypercubes, empirical\n  copulas, and convergence of sum distributions", "comments": "Manuscript accepted in the Journal of Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2015.07.008", "report-no": null, "categories": "q-fin.RM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies convergence properties of multivariate distributions\nconstructed by endowing empirical margins with a copula. This setting includes\nLatin Hypercube Sampling with dependence, also known as the Iman--Conover\nmethod. The primary question addressed here is the convergence of the component\nsum, which is relevant to risk aggregation in insurance and finance.\n  This paper shows that a CLT for the aggregated risk distribution is not\navailable, so that the underlying mathematical problem goes beyond classic\nfunctional CLTs for empirical copulas. This issue is relevant to Monte-Carlo\nbased risk aggregation in all multivariate models generated by plugging\nempirical margins into a copula.\n  Instead of a functional CLT, this paper establishes strong uniform\nconsistency of the estimated sum distribution function and provides a\nsufficient criterion for the convergence rate $O(n^{-1/2})$ in probability.\nThese convergence results hold for all copulas with bounded densities. Examples\nwith unbounded densities include bivariate Clayton and Gauss copulas. The\nconvergence results are not specific to the component sum and hold also for any\nother componentwise non-decreasing aggregation function. On the other hand,\nconvergence of estimates for the joint distribution is much easier to prove,\nincluding CLTs.\n  Beyond Iman--Conover estimates, the results of this paper apply to\nmultivariate distributions obtained by plugging empirical margins into an exact\ncopula or by plugging exact margins into an empirical copula.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 21:14:49 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Mainik", "Georg", ""]]}, {"id": "1508.02803", "submitter": "Suprateek Kundu", "authors": "Suprateek Kundu, Minsuk Shin, Yichen Cheng, Ganiraju Manyam, Bani K.\n  Mallick, Veera Baladandayuthapani", "title": "Bayesian Variable Selection with Structure Learning: Applications in\n  Integrative Genomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant advances in biotechnology have allowed for simultaneous\nmeasurement of molecular data points across multiple genomic and transcriptomic\nlevels from a single tumor/cancer sample. This has motivated systematic\napproaches to integrate multi-dimensional structured datasets since cancer\ndevelopment and progression is driven by numerous co-ordinated molecular\nalterations and the interactions between them. We propose a novel two-step\nBayesian approach that combines a variable selection framework with integrative\nstructure learning between multiple sources of data. The structure learning in\nthe first step is accomplished through novel joint graphical models for\nheterogeneous (mixed scale) data allowing for flexible incorporation of prior\nknowledge. This structure learning subsequently informs the variable selection\nin the second step to identify groups of molecular features within and across\nplatforms associated with outcomes of cancer progression. The variable\nselection strategy adjusts for collinearity and multiplicity, and also has\ntheoretical justifications. We evaluate our methods through simulations and\napply them to a motivating genomic (DNA copy number and methylation) and\ntranscriptomic (mRNA expression) data for assessing important markers\nassociated with Glioblastoma progression.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 03:07:33 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Kundu", "Suprateek", ""], ["Shin", "Minsuk", ""], ["Cheng", "Yichen", ""], ["Manyam", "Ganiraju", ""], ["Mallick", "Bani K.", ""], ["Baladandayuthapani", "Veera", ""]]}, {"id": "1508.02925", "submitter": "Hao Han", "authors": "Hao Han, Wei Zhu", "title": "RCR: Robust Compound Regression for Robust Estimation of\n  Errors-in-Variables Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The errors-in-variables (EIV) regression model, being more realistic by\naccounting for measurement errors in both the dependent and the independent\nvariables, is widely adopted in applied sciences. The traditional EIV model\nestimators, however, can be highly biased by outliers and other departures from\nthe underlying assumptions. In this paper, we develop a novel nonparametric\nregression approach - the robust compound regression (RCR) analysis method for\nthe robust estimation of EIV models. We first introduce a robust and efficient\nestimator called least sine squares (LSS). Taking full advantage of both the\nnew LSS method and the compound regression analysis method developed in our own\ngroup, we subsequently propose the RCR approach as a generalization of those\ntwo, which provides a robust counterpart of the entire class of the maximum\nlikelihood estimation (MLE) solutions of the EIV model, in a 1-1 mapping.\nTechnically, our approach gives users the flexibility to select from a class of\nRCR estimates the optimal one with a predefined regression efficiency criterion\nsatisfied. Simulation studies and real-life examples are provided to illustrate\nthe effectiveness of the RCR approach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 14:19:24 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Han", "Hao", ""], ["Zhu", "Wei", ""]]}, {"id": "1508.02942", "submitter": "Hao Han", "authors": "Hao Han, Yeming Ma, Wei Zhu", "title": "Galton's Family Heights Data Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Galton's family heights data has been a preeminent historical dataset in\nregression analysis, on which the original model and basic results have\nsurvived the close scrutiny of statisticians for 125 years. However by\nrevisiting Galton's family data, we challenge whether Galton's classic model\nand his regression towards mean interpretation are proper. Using Galton's data\nas a benchmark for different regression methods, such as least squares,\northogonal regression, geometric mean regression, and least sine squares\nregression - a newly developed nonparametric robust regression approach, we\nelucidate that his regression model has fundamental drawbacks not only in\nvariable and model selection by \"transmuting\" women into men thus the simple\nlinear model, but also a strong bias in least squares regression leading to\notherwise alternative conclusions on the true relationships between the heights\nof the child and his or her parents.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 14:55:42 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Han", "Hao", ""], ["Ma", "Yeming", ""], ["Zhu", "Wei", ""]]}, {"id": "1508.02985", "submitter": "Gloria Gheno", "authors": "Gloria Gheno", "title": "The Causal Effects for a Causal Loglinear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the causality is important in many fields of research. I\npropose a causal theory to obtain the causal effects in a causal loglinear\nmodel. It calculates them using the odds ratio and Pearl's causal theory. The\neffects are calculated distinguishing between a simple mediation model (model\nwithout the multiplicative interaction effect) and a mediation model with the\nmultiplicative interaction effect. In both models it is possible also to\nanalyze the cell effect, which is a new interaction effect. Then in a causal\nloglinear model there are three interaction effects: multiplicative interaction\neffect, additive interaction effect and cell effect\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 16:43:45 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Gheno", "Gloria", ""]]}, {"id": "1508.03002", "submitter": "Rui Castro", "authors": "Ery Arias-Castro, Rui M. Castro, Ervin T\\'anczos, Meng Wang", "title": "Distribution-Free Detection of Structured Anomalies: Permutation and\n  Rank-Based Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scan statistic is by far the most popular method for anomaly detection,\nbeing popular in syndromic surveillance, signal and image processing, and\ntarget detection based on sensor networks, among other applications. The use of\nthe scan statistics in such settings yields a hypothesis testing procedure,\nwhere the null hypothesis corresponds to the absence of anomalous behavior. If\nthe null distribution is known, then calibration of a scan-based test is\nrelatively easy, as it can be done by Monte Carlo simulation. When the null\ndistribution is unknown, it is less straightforward. We investigate two\nprocedures. The first one is a calibration by permutation and the other is a\nrank-based scan test, which is distribution-free and less sensitive to\noutliers. Furthermore, the rank scan test requires only a one-time calibration\nfor a given data size making it computationally much more appealing. In both\ncases, we quantify the performance loss with respect to an oracle scan test\nthat knows the null distribution. We show that using one of these calibration\nprocedures results in only a very small loss of power in the context of a\nnatural exponential family. This includes the classical normal location model,\npopular in signal processing, and the Poisson model, popular in syndromic\nsurveillance. We perform numerical experiments on simulated data further\nsupporting our theory and also on a real dataset from genomics.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 17:15:35 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 22:58:42 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 09:49:37 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Castro", "Rui M.", ""], ["T\u00e1nczos", "Ervin", ""], ["Wang", "Meng", ""]]}, {"id": "1508.03179", "submitter": "Xin Zhou", "authors": "Xin Zhou, Nicole Mayer-Hamblett, Umer Khan and Michael R. Kosorok", "title": "Residual Weighted Learning for Estimating Individualized Treatment Rules", "comments": "48 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized medicine has received increasing attention among statisticians,\ncomputer scientists, and clinical practitioners. A major component of\npersonalized medicine is the estimation of individualized treatment rules\n(ITRs). Recently, Zhao et al. (2012) proposed outcome weighted learning (OWL)\nto construct ITRs that directly optimize the clinical outcome. Although OWL\nopens the door to introducing machine learning techniques to optimal treatment\nregimes, it still has some problems in performance. In this article, we propose\na general framework, called Residual Weighted Learning (RWL), to improve finite\nsample performance. Unlike OWL which weights misclassification errors by\nclinical outcomes, RWL weights these errors by residuals of the outcome from a\nregression fit on clinical covariates excluding treatment assignment. We\nutilize the smoothed ramp loss function in RWL, and provide a difference of\nconvex (d.c.) algorithm to solve the corresponding non-convex optimization\nproblem. By estimating residuals with linear models or generalized linear\nmodels, RWL can effectively deal with different types of outcomes, such as\ncontinuous, binary and count outcomes. We also propose variable selection\nmethods for linear and nonlinear rules, respectively, to further improve the\nperformance. We show that the resulting estimator of the treatment rule is\nconsistent. We further obtain a rate of convergence for the difference between\nthe expected outcome using the estimated ITR and that of the optimal treatment\nrule. The performance of the proposed RWL methods is illustrated in simulation\nstudies and in an analysis of cystic fibrosis clinical trial data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 11:41:52 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Zhou", "Xin", ""], ["Mayer-Hamblett", "Nicole", ""], ["Khan", "Umer", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1508.03262", "submitter": "Luke Keele", "authors": "Eric Freeman, Luke Keele, David Park, Julia Salzman, Brendan Weickert", "title": "The Plateau Problem in the Heteroskedastic Probit Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parameter determination for the heteroskedastic probit model, both in\nsimulated data and in actual data, we observe a failure of traditional local\nsearch methods to converge consistently to a single parameter vector, in\ncontrast to the typical situation for the regular probit model. We identify\nfeatures of the heteroskedastic probit log likelihood function that we argue\ntend to lead to this failure, and suggest ways to amend the local search\nmethods to remedy the problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 16:25:18 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Freeman", "Eric", ""], ["Keele", "Luke", ""], ["Park", "David", ""], ["Salzman", "Julia", ""], ["Weickert", "Brendan", ""]]}, {"id": "1508.03354", "submitter": "Andre Chalom", "authors": "Andr\\'e Chalom and Paulo In\\'acio de Knegt L\\'opez de Prado", "title": "Uncertainty analysis and composite hypothesis under the likelihood\n  paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The correct use and interpretation of models depends on several steps, two of\nwhich being the calibration by parameter estimation and the analysis of\nuncertainty. In the biological literature, these steps are seldom discussed\ntogether, but they can be seen as fitting pieces of the same puzzle. In\nparticular, analytical procedures for uncertainty estimation may be masking a\nhigh degree of uncertainty coming from a model with a stable structure, but\ninsufficient data.\n  Under a likelihoodist approach, the problem of uncertainty estimation is\nclosely related to the problem of composite hypothesis. In this paper, we\npresent a brief historical background on the statistical school of\nLikelihoodism, and examine the complex relations between the law of likelihood\nand the problem of composite hypothesis, together with the existing proposals\nfor coping with it. Then, we propose a new integrative methodology for the\nuncertainty estimation of models using the information in the collected data.\nWe argue that this methodology is intuitively appealing under a likelihood\nparadigm.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 20:40:58 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Chalom", "Andr\u00e9", ""], ["de Prado", "Paulo In\u00e1cio de Knegt L\u00f3pez", ""]]}, {"id": "1508.03365", "submitter": "Timothy Christensen", "authors": "Xiaohong Chen and Timothy M. Christensen", "title": "Optimal Sup-norm Rates and Uniform Inference on Nonlinear Functionals of\n  Nonparametric IV Regression", "comments": "This paper is a major extension of Sections 2 and 3 of our Cowles\n  Foundation Discussion Paper CFDP1923, Cemmap Working Paper CWP56/13 and arXiv\n  preprint arXiv:1311.0412 [math.ST]. Section 3 of the previous version of this\n  paper (dealing with data-driven choice of sieve dimension) is currently being\n  revised as a separate paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes several important contributions to the literature about\nnonparametric instrumental variables (NPIV) estimation and inference on a\nstructural function $h_0$ and its functionals. First, we derive sup-norm\nconvergence rates for computationally simple sieve NPIV (series 2SLS)\nestimators of $h_0$ and its derivatives. Second, we derive a lower bound that\ndescribes the best possible (minimax) sup-norm rates of estimating $h_0$ and\nits derivatives, and show that the sieve NPIV estimator can attain the minimax\nrates when $h_0$ is approximated via a spline or wavelet sieve. Our optimal\nsup-norm rates surprisingly coincide with the optimal root-mean-squared rates\nfor severely ill-posed problems, and are only a logarithmic factor slower than\nthe optimal root-mean-squared rates for mildly ill-posed problems. Third, we\nuse our sup-norm rates to establish the uniform Gaussian process strong\napproximations and the score bootstrap uniform confidence bands (UCBs) for\ncollections of nonlinear functionals of $h_0$ under primitive conditions,\nallowing for mildly and severely ill-posed problems. Fourth, as applications,\nwe obtain the first asymptotic pointwise and uniform inference results for\nplug-in sieve t-statistics of exact consumer surplus (CS) and deadweight loss\n(DL) welfare functionals under low-level conditions when demand is estimated\nvia sieve NPIV. Empiricists could read our real data application of UCBs for\nexact CS and DL functionals of gasoline demand that reveals interesting\npatterns and is applicable to other markets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 21:39:11 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 23:02:13 GMT"}, {"version": "v3", "created": "Sat, 29 Apr 2017 19:10:36 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chen", "Xiaohong", ""], ["Christensen", "Timothy M.", ""]]}, {"id": "1508.03498", "submitter": "Xin Yuan", "authors": "Xin Yuan, Hong Jiang, Gang Huang, Paul Wilford", "title": "Lensless Compressive Imaging", "comments": "37 pages, 10 figures. Submitted to SIAM Journal on Imaging Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a lensless compressive imaging architecture, which consists of an\naperture assembly and a single sensor, without using any lens. An anytime\nalgorithm is proposed to reconstruct images from the compressive measurements;\nthe algorithm produces a sequence of solutions that monotonically converge to\nthe true signal (thus, anytime). The algorithm is developed based on the\nsparsity of local overlapping patches (in the transformation domain) and\nstate-of-the-art results have been obtained. Experiments on real data\ndemonstrate that encouraging results are obtained by measuring about 10% (of\nthe image pixels) compressive measurements. The reconstruction results of the\nproposed algorithm are compared with the JPEG compression (based on file sizes)\nand the reconstructed image quality is close to the JPEG compression, in\nparticular at a high compression rate.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 13:34:10 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Yuan", "Xin", ""], ["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul", ""]]}, {"id": "1508.03534", "submitter": "Jonathan Friedemann Donges", "authors": "Jonathan F. Donges, Carl-Friedrich Schleussner, Jonatan F. Siegmund,\n  Reik V. Donner", "title": "Event coincidence analysis for quantifying statistical\n  interrelationships between event time series: on the role of flood events as\n  possible triggers of epidemic outbreaks", "comments": "18 pages, 4 figures", "journal-ref": "European Physical Journal Special Topics, 225(3), 471-487 (2016)", "doi": "10.1140/epjst/e2015-50233-y", "report-no": null, "categories": "physics.soc-ph physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying event time series is a powerful approach for analyzing the dynamics\nof complex dynamical systems in many fields of science. In this paper, we\ndescribe the method of event coincidence analysis to provide a framework for\nquantifying the strength, directionality and time lag of statistical\ninterrelationships between event series. Event coincidence analysis allows to\nformulate and test null hypotheses on the origin of the observed\ninterrelationships including tests based on Poisson processes or, more\ngenerally, stochastic point processes with a prescribed inter-event time\ndistribution and other higher-order properties. Applying the framework to\ncountry-level observational data yields evidence that flood events have acted\nas triggers of epidemic outbreaks globally since the 1950s. Facing projected\nfuture changes in the statistics of climatic extreme events, statistical\ntechniques such as event coincidence analysis will be relevant for\ninvestigating the impacts of anthropogenic climate change on human societies\nand ecosystems worldwide.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 21:23:25 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 08:15:24 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Donges", "Jonathan F.", ""], ["Schleussner", "Carl-Friedrich", ""], ["Siegmund", "Jonatan F.", ""], ["Donner", "Reik V.", ""]]}, {"id": "1508.03712", "submitter": "Ingo Steinwart", "authors": "Philipp Thomann, Ingo Steinwart, Nico Schmid", "title": "Towards an Axiomatic Approach to Hierarchical Clustering of Measures", "comments": null, "journal-ref": "Journal of Machine Learning Research. 16(Sep):1949-2002, 2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose some axioms for hierarchical clustering of probability measures\nand investigate their ramifications. The basic idea is to let the user\nstipulate the clusters for some elementary measures. This is done without the\nneed of any notion of metric, similarity or dissimilarity. Our main results\nthen show that for each suitable choice of user-defined clustering on\nelementary measures we obtain a unique notion of clustering on a large set of\ndistributions satisfying a set of additivity and continuity axioms. We\nillustrate the developed theory by numerous examples including some with and\nsome without a density.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 09:07:01 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Thomann", "Philipp", ""], ["Steinwart", "Ingo", ""], ["Schmid", "Nico", ""]]}, {"id": "1508.03747", "submitter": "Subhadeep Mukhopadhyay", "authors": "Scott Bruce, Zeda Li, Hsiang-Chieh Yang, and Subhadeep Mukhopadhyay", "title": "Nonparametric Distributed Learning Architecture for Big Data: Algorithm\n  and Applications", "comments": "The purpose of this paper is to answer the question: What is the\n  relevance of small-data-ideas in this big-data world? The bigger question is:\n  Should we make difficult things easy or easy things look difficult? The first\n  option will probably make some impact in the long-run, but the second one\n  will surely earn prestigious journal publications in short-run, IEEE\n  Transactions on Big Data (forthcoming). The first report came out in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dramatic increases in the size and complexity of modern datasets have made\ntraditional \"centralized\" statistical inference prohibitive. In addition to\ncomputational challenges associated with big data learning, the presence of\nnumerous data types (e.g. discrete, continuous, categorical, etc.) makes\nautomation and scalability difficult. A question of immediate concern is how to\ndesign a data-intensive statistical inference architecture without changing the\nbasic statistical modeling principles developed for \"small\" data over the last\ncentury. To address this problem, we present MetaLP, a flexible, distributed\nstatistical modeling framework.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 16:13:29 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 21:05:41 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2016 03:10:19 GMT"}, {"version": "v4", "created": "Sat, 2 Apr 2016 16:14:33 GMT"}, {"version": "v5", "created": "Mon, 26 Feb 2018 16:24:30 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Bruce", "Scott", ""], ["Li", "Zeda", ""], ["Yang", "Hsiang-Chieh", ""], ["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1508.03758", "submitter": "Maria DeYoreo", "authors": "Maria DeYoreo, Jerome P. Reiter, and D. Sunshine Hillygus", "title": "Bayesian Mixture Models With Focused Clustering for Mixed Ordinal and\n  Nominal Data", "comments": "Supplementary material available from the first author upon request", "journal-ref": null, "doi": "10.1214/16-BA1020", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some contexts, mixture models can fit certain variables well at the\nexpense of others in ways beyond the analyst's control. For example, when the\ndata include some variables with non-trivial amounts of missing values, the\nmixture model may fit the marginal distributions of the nearly and fully\ncomplete variables at the expense of the variables with high fractions of\nmissing data. Motivated by this setting, we present a mixture model for mixed\nordinal and nominal data that splits variables into two groups, focus variables\nand remainder variables. The model allows the analyst to specify a rich\nsub-model for the focus variables and a simpler sub-model for remainder\nvariables, yet still capture associations among the variables. Using\nsimulations, we illustrate advantages and limitations of focused clustering\ncompared to mixture models that do not distinguish variables. We apply the\nmodel to handle missing values in an analysis of the 2012 American National\nElection Study, estimating relationships among voting behavior, ideology, and\npolitical party affiliation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 17:45:24 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 20:27:33 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["DeYoreo", "Maria", ""], ["Reiter", "Jerome P.", ""], ["Hillygus", "D. Sunshine", ""]]}, {"id": "1508.03772", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo and Soumaila Dembele", "title": "Probabilistic, statistical and algorithmic aspects of the similarity of\n  texts and application to Gospels comparison", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental problem of similarity studies, in the frame of data-mining,\nis to examine and detect similar items in articles, papers, books, with huge\nsizes. In this paper, we are interested in the probabilistic, and the\nstatistical and the algorithmic aspects in studies of texts. We will be using\nthe approach of $k$\\textit{-shinglings}, a $k$\\textit{-shingling} being defined\nas a sequence of $k$ consecutive characters that are extracted from a text\n($k\\geq 1$ ). The main stake in this field is to find accurate and quick\nalgorithms to compute the similarity in short times. This will be achieved in\nusing approximation methods. The first approximation method is statistical and,\nis based on the theorem of Glivenko-Cantelli. The second is the banding\ntechnique. And the third concerns a modification of the algorithm proposed by\nRajaraman and al (% \\cite{AnandJeffrey}), denoted here as (RUM). The Jaccard\nindex is the one used in this paper. We finally illustrate these results of the\npaper on the four Gospels. The results are very conclusive.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 22:12:22 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Lo", "Gane Samb", ""], ["Dembele", "Soumaila", ""]]}, {"id": "1508.03808", "submitter": "Jakob Runge", "authors": "Jakob Runge", "title": "Quantifying information transfer and mediation along causal pathways in\n  complex systems", "comments": "20 pages, 6 figures", "journal-ref": "Phys. Rev. E 92, 062829 (2015)", "doi": "10.1103/PhysRevE.92.062829", "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of information transfer have become a popular approach to analyze\ninteractions in complex systems such as the Earth or the human brain from\nmeasured time series. Recent work has focused on causal definitions of\ninformation transfer excluding effects of common drivers and indirect\ninfluences. While the former clearly constitutes a spurious causality, the aim\nof the present article is to develop measures quantifying different notions of\nthe strength of information transfer along indirect causal paths, based on\nfirst reconstructing the multivariate causal network (\\emph{Tigramite}\napproach). Another class of novel measures quantifies to what extent different\nintermediate processes on causal paths contribute to an interaction mechanism\nto determine pathways of causal information transfer. A rigorous mathematical\nframework allows for a clear information-theoretic interpretation that can also\nbe related to the underlying dynamics as proven for certain classes of\nprocesses. Generally, however, estimates of information transfer remain hard to\ninterpret for nonlinearly intertwined complex systems. But, if experiments or\nmathematical models are not available, measuring pathways of information\ntransfer within the causal dependency structure allows at least for an\nabstraction of the dynamics. The measures are illustrated on a climatological\nexample to disentangle pathways of atmospheric flow over Europe.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 10:30:57 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 12:46:43 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Runge", "Jakob", ""]]}, {"id": "1508.03821", "submitter": "Mioara Alina Nicolaie", "authors": "M. A. Nicolaie and J.M.G. Taylor and C. Legrand", "title": "Vertical modeling: analysis of competing risks data with a cure\n  proportion", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the vertical modeling approach for the analysis of\nsurvival data with competing risks to incorporate a cured fraction in the\npopulation, that is, a proportion of the population for which none of the\ncompeting events can occur. The proposed method has three components: the\nproportion of cure, the risk of failure, irrespective of the cause, and the\nrelative risk of a certain cause of failure, given a failure occurred.\nCovariates may affect each of these components. An appealing aspect of the\nmethod is that it is a natural extension to competing risks of the\nsemi-parametric mixture cure model in ordinary survival analysis; thus, causes\nof failure are assigned only if a failure occurs. This contrasts with the\nexisting mixture cure model for competing risks of Larson and Dinse, which\nconditions at the onset on the future status presumably attained. Regression\nparameter estimates are obtained using an EM-algorithm. The performance of the\nestimators is evaluated in a simulation study. The method is illustrated using\na melanoma cancer data set.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 12:39:06 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Nicolaie", "M. A.", ""], ["Taylor", "J. M. G.", ""], ["Legrand", "C.", ""]]}, {"id": "1508.03828", "submitter": "Peter Rousseeuw", "authors": "Peter J. Rousseeuw and Mia Hubert", "title": "Statistical depth meets computational geometry: a short survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past two decades there has been a lot of interest in developing\nstatistical depth notions that generalize the univariate concept of ranking to\nmultivariate data. The notion of depth has also been extended to regression\nmodels and functional data. However, computing such depth functions as well as\ntheir contours and deepest points is not trivial. Techniques of computational\ngeometry appear to be well-suited for the development of such algorithms. Both\nthe statistical and the computational geometry communities have done much work\nin this direction, often in close collaboration. We give a short survey of this\nwork, focusing mainly on depth and multivariate medians, and end by listing\nsome other areas of statistics where computational geometry has been of great\nhelp in constructing efficient algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 14:26:42 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Rousseeuw", "Peter J.", ""], ["Hubert", "Mia", ""]]}, {"id": "1508.03852", "submitter": "Armeen Taeb", "authors": "Armeen Taeb, Venkat Chandrasekaran", "title": "Sufficient Dimension Reduction and Modeling Responses Conditioned on\n  Covariates: An Integrated Approach via Convex Optimization", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given observations of a collection of covariates and responses $(Y, X) \\in\n\\mathbb{R}^p \\times \\mathbb{R}^q$, sufficient dimension reduction (SDR)\ntechniques aim to identify a mapping $f: \\mathbb{R}^q \\rightarrow \\mathbb{R}^k$\nwith $k \\ll q$ such that $Y|f(X)$ is independent of $X$. The image $f(X)$\nsummarizes the relevant information in a potentially large number of covariates\n$X$ that influence the responses $Y$. In many contemporary settings, the number\nof responses $p$ is also quite large, in addition to a large number $q$ of\ncovariates. This leads to the challenge of fitting a succinctly parameterized\nstatistical model to $Y|f(X)$, which is a problem that is usually not addressed\nin a traditional SDR framework. In this paper, we present a computationally\ntractable convex relaxation based estimator for simultaneously (a) identifying\na linear dimension reduction $f(X)$ of the covariates that is sufficient with\nrespect to the responses, and (b) fitting several types of structured\nlow-dimensional models -- factor models, graphical models, latent-variable\ngraphical models -- to the conditional distribution of $Y|f(X)$. We analyze the\nconsistency properties of our estimator in a high-dimensional scaling regime.\nWe also illustrate the performance of our approach on a newsgroup dataset and\non a dataset consisting of financial asset prices.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 17:57:02 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 02:37:22 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Taeb", "Armeen", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1508.04073", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Richard G. Baraniuk", "title": "An Information-Theoretic Measure of Dependency Among Variables in Large\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximal information coefficient (MIC), which measures the amount of\ndependence between two variables, is able to detect both linear and non-linear\nassociations. However, computational cost grows rapidly as a function of the\ndataset size. In this paper, we develop a computationally efficient\napproximation to the MIC that replaces its dynamic programming step with a much\nsimpler technique based on the uniform partitioning of data grid. A variety of\nexperiments demonstrate the quality of our approximation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 16:00:30 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Mousavi", "Ali", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1508.04158", "submitter": "Claudio Fantacci", "authors": "Claudio Fantacci", "title": "Distributed multi-object tracking over sensor networks: a random finite\n  set approach", "comments": "Ph.D. thesis of Claudio Fantacci, Universit\\`a di Firenze,\n  Dipartimento di Ingegneria dell'Informazione (DINFO), Florence, Italy\n  Successfully defended on the 5th of March 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the present dissertation is to address distributed tracking over a\nnetwork of heterogeneous and geographically dispersed nodes (or agents) with\nsensing, communication and processing capabilities. Tracking is carried out in\nthe Bayesian framework and its extension to a distributed context is made\npossible via an information-theoretic approach to data fusion which exploits\nconsensus algorithms and the notion of Kullback-Leibler Average (KLA) of the\nProbability Density Functions (PDFs) to be fused. The first step toward\ndistributed tracking considers a single moving object. Consensus takes place in\neach agent for spreading information over the network so that each node can\ntrack the object. To achieve such a goal, consensus is carried out on the local\nsingle-object posterior distribution, which is the result of local data\nprocessing, in the Bayesian setting, exploiting the last available measurement\nabout the object. The next step is in the direction of distributed estimation\nof multiple moving objects. In order to model, in a rigorous and elegant way, a\npossibly time-varying number of objects present in a given area of interest,\nthe Random Finite Set (RFS) formulation is adopted since it provides the notion\nof probability density for multi-object states that allows to directly extend\nexisting tools in distributed estimation to multi-object tracking. The last\ntheoretical step of the present dissertation is toward distributed filtering\nwith the further requirement of unique object identities. To this end the\nlabeled RFS framework is adopted as it provides a tractable approach to the\nmulti-object Bayesian recursion. A generalization of the KLA to the labeled RFS\nframework, enables the development of novel consensus multi-object tracking\nfilters which are fully distributed, scalable and computationally efficient.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 21:54:01 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Fantacci", "Claudio", ""]]}, {"id": "1508.04178", "submitter": "Qingyuan Zhao", "authors": "Jingshu Wang, Qingyuan Zhao, Trevor Hastie, Art B. Owen", "title": "Confounder Adjustment in Multiple Hypothesis Testing", "comments": "The first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale studies in which thousands of significance tests are\nperformed simultaneously. In some of these studies, the multiple testing\nprocedure can be severely biased by latent confounding factors such as batch\neffects and unmeasured covariates that correlate with both primary variable(s)\nof interest (e.g. treatment variable, phenotype) and the outcome. Over the past\ndecade, many statistical methods have been proposed to adjust for the\nconfounders in hypothesis testing. We unify these methods in the same\nframework, generalize them to include multiple primary variables and multiple\nnuisance variables, and analyze their statistical properties. In particular, we\nprovide theoretical guarantees for RUV-4 and LEAPP, which correspond to two\ndifferent identification conditions in the framework: the first requires a set\nof \"negative controls\" that are known a priori to follow the null distribution;\nthe second requires the true non-nulls to be sparse. Two different estimators\nwhich are based on RUV-4 and LEAPP are then applied to these two scenarios. We\nshow that if the confounding factors are strong, the resulting estimators can\nbe asymptotically as powerful as the oracle estimator which observes the latent\nconfounding factors. For hypothesis testing, we show the asymptotic z-tests\nbased on the estimators can control the type I error. Numerical experiments\nshow that the false discovery rate is also controlled by the Benjamini-Hochberg\nprocedure when the sample size is reasonably large.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 23:45:02 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 03:37:10 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 03:28:20 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wang", "Jingshu", ""], ["Zhao", "Qingyuan", ""], ["Hastie", "Trevor", ""], ["Owen", "Art B.", ""]]}, {"id": "1508.04217", "submitter": "Yoshimasa Uematsu", "authors": "Yoshimasa Uematsu and Shinya Tanaka", "title": "Macroeconomic Forecasting and Variable Selection with a Very Large\n  Number of Predictors: A Penalized Regression Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies macroeconomic forecasting and variable selection using a\nfolded-concave penalized regression with a very large number of predictors. The\npenalized regression approach leads to sparse estimates of the regression\ncoefficients, and is applicable even if the dimensionality of the model is much\nlarger than the sample size. The first half of the paper discusses the\ntheoretical aspects of a folded-concave penalized regression when the model\nexhibits time series dependence. Specifically, we show the oracle inequality\nand the oracle property for ultrahigh-dimensional time-dependent regressors.\nThe latter half of the paper shows the validity of the penalized regression\nusing two motivating empirical applications. The first forecasts U.S. GDP with\nthe FRED-MD data using the MIDAS regression framework, where there are more\nthan 1000 covariates, while the sample size is at most 200. The second examines\nhow well the penalized regression screens the hidden portfolio with around 40\nstocks from more than 1800 potential stocks using NYSE stock price data. Both\napplications reveal that the penalized regression provides remarkable results\nin terms of forecasting performance and variable selection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 05:33:06 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 06:44:29 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Uematsu", "Yoshimasa", ""], ["Tanaka", "Shinya", ""]]}, {"id": "1508.04422", "submitter": "Vince Lyzinski", "authors": "Aren Jansen, Gregory Sell, Vince Lyzinski", "title": "Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural\n  Networks", "comments": "10 pages, 2 figures, 1 table, this paper is under consideration for\n  publication in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several popular graph embedding techniques for representation learning and\ndimensionality reduction rely on performing computationally expensive\neigendecompositions to derive a nonlinear transformation of the input data\nspace. The resulting eigenvectors encode the embedding coordinates for the\ntraining samples only, and so the embedding of novel data samples requires\nfurther costly computation. In this paper, we present a method for the\nout-of-sample extension of graph embeddings using deep neural networks (DNN) to\nparametrically approximate these nonlinear maps. Compared with traditional\nnonparametric out-of-sample extension methods, we demonstrate that the DNNs can\ngeneralize with equal or better fidelity and require orders of magnitude less\ncomputation at test time. Moreover, we find that unsupervised pretraining of\nthe DNNs improves optimization for larger network sizes, thus removing\nsensitivity to model selection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 19:47:31 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 16:07:53 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 15:50:41 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Jansen", "Aren", ""], ["Sell", "Gregory", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1508.04557", "submitter": "Elvira Di Nardo Ph.D.", "authors": "Elvira Di Nardo", "title": "On photon statistics parametrized by a non-central Wishart random matrix", "comments": "18 pages, in press in Journal of Statistical Planning and Inference,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to tackle parameter estimation of photocounting distributions,\npolykays of acting intensities are proposed as a new tool for computing photon\nstatistics. As unbiased estimators of cumulants, polykays are computationally\nfeasible thanks to a symbolic method recently developed in dealing with\nsequences of moments. This method includes the so-called method of moments for\nrandom matrices and results to be particularly suited to deal with convolutions\nor random summations of random vectors. The overall photocounting effect on a\ndeterministic number of pixels is introduced. A random number of pixels is also\nconsidered. The role played by spectral statistics of random matrices is\nhighlighted in approximating the overall photocounting distribution when acting\nintensities are modeled by a non-central Wishart random matrix. Generalized\ncomplete Bell polynomials are used in order to compute joint moments and joint\ncumulants of multivariate photocounters. Multivariate polykays can be\nsuccessfully employed in order to approximate the multivariate Mendel-Poisson\ntransform. Open problems are addressed at the end of the paper.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:11:49 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Di Nardo", "Elvira", ""]]}, {"id": "1508.04559", "submitter": "Przemys{\\l}aw Spurek", "authors": "Jacek Tabor, Przemys{\\l}aw Spurek, Konrad Kamieniecki, Marek \\'Smieja,\n  Krzysztof Misztal", "title": "Introduction to Cross-Entropy Clustering The R Package CEC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R Package CEC performs clustering based on the cross-entropy clustering\n(CEC) method, which was recently developed with the use of information theory.\nThe main advantage of CEC is that it combines the speed and simplicity of\n$k$-means with the ability to use various Gaussian mixture models and reduce\nunnecessary clusters. In this work we present a practical tutorial to CEC based\non the R Package CEC. Functions are provided to encompass the whole process of\nclustering.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:13:56 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Tabor", "Jacek", ""], ["Spurek", "Przemys\u0142aw", ""], ["Kamieniecki", "Konrad", ""], ["\u015amieja", "Marek", ""], ["Misztal", "Krzysztof", ""]]}, {"id": "1508.04837", "submitter": "Jay Beder", "authors": "Jay H. Beder and Wiebke S. Diestelkamp", "title": "Box-Hunter resolution in nonregular fractional factorial designs", "comments": "12 pages", "journal-ref": "Journal of Statistical Theory and Practice 3(4), 879--889, 2009", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In a 1961 paper, Box and Hunter defined the resolution of a regular\nfractional factorial design as a measure of the amount of aliasing in the\nfraction. They also indicated that the maximum resolution is equal to the\nminimum length of a defining word. The idea of a wordlength pattern has now\nbeen extended to nonregular designs by various authors, who show that the\nminimum generalized wordlength equals the maximum strength plus 1.\n  Minimum generalized wordlength is often taken as the definition of\nresolution. However, Box and Hunter's original definition, which does not\ndepend on wordlength, can be extended to nonregular designs if they are simple.\nThe purpose of this paper is to prove that the maximum Box-Hunter resolution\ndoes equal the maximum strength plus 1, and therefore equals the minimum\ngeneralized wordlength. Other approaches to resolution are briefly discussed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 00:07:35 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Beder", "Jay H.", ""], ["Diestelkamp", "Wiebke S.", ""]]}, {"id": "1508.04841", "submitter": "Jeremy Magland", "authors": "Jeremy F. Magland and Alex H. Barnett", "title": "Unimodal clustering using isotonic regression: ISO-SPLIT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A limitation of many clustering algorithms is the requirement to tune\nadjustable parameters for each application or even for each dataset. Some\ntechniques require an \\emph{a priori} estimate of the number of clusters while\ndensity-based techniques usually require a scale parameter. Other parametric\nmethods, such as mixture modeling, make assumptions about the underlying\ncluster distributions. Here we introduce a non-parametric clustering method\nthat does not involve tunable parameters and only assumes that clusters are\nunimodal, in the sense that they have a single point of maximal density when\nprojected onto any line, and that clusters are separated from one another by a\nseparating hyperplane of relatively lower density. The technique uses a\nnon-parametric variant of Hartigan's dip statistic using isotonic regression as\nthe kernel operation repeated at every iteration. We compare the method against\nk-means++, DBSCAN, and Gaussian mixture methods and show in simulations that it\nperforms better than these standard methods in many situations. The algorithm\nis suited for low-dimensional datasets with a large number of observations, and\nwas motivated by the problem of \"spike sorting\" in neural electrical\nrecordings. Source code is freely available.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 00:26:38 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 21:01:22 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Magland", "Jeremy F.", ""], ["Barnett", "Alex H.", ""]]}, {"id": "1508.05137", "submitter": "Martin Lysy", "authors": "Yifan Wang, Tian You, Martin Lysy", "title": "A Heteroscedastic Accelerated Failure Time Model for Survival Analysis", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric and semiparametric methods are commonly used in survival\nanalysis to mitigate the bias due to model misspecification. However, such\nmethods often cannot estimate upper-tail survival quantiles when a sizable\nproportion of the data are censored, in which case parametric likelihood-based\nestimators present a viable alternative. In this article, we extend a popular\nfamily of parametric survival models which make the Accelerated Failure Time\n(AFT) assumption to account for heteroscedasticity in the survival times. The\nconditional variances can depend on arbitrary covariates, thus adding\nconsiderable flexibility to the homoscedastic model. We present an\nExpectation-Conditional-Maximization (ECM) algorithm to efficiently compute the\nHAFT maximum likelihood estimator with right-censored data. The methodology is\napplied to the heavily censored data from a colon cancer clinical trial, for\nwhich a new type of highly stringent model residuals is proposed. Based on\nthese, the HAFT model was found to eliminate most outliers from its\nhomoscedastic counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 21:41:10 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 19:39:52 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Wang", "Yifan", ""], ["You", "Tian", ""], ["Lysy", "Martin", ""]]}, {"id": "1508.05178", "submitter": "Gael Martin Prof", "authors": "David T. Frazier, Gael M. Martin and Christian P. Robert", "title": "On Consistency of Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods have become increasingly\nprevalent of late, facilitating as they do the analysis of intractable, or\nchallenging, statistical problems. With the initial focus being primarily on\nthe practical import of ABC, exploration of its formal statistical properties\nhas begun to attract more attention. The aim of this paper is to establish\ngeneral conditions under which ABC methods are Bayesian consistent, in the\nsense of producing draws that yield a degenerate posterior distribution at the\ntrue parameter (vector) asymptotically (in the sample size). We derive\nconditions under which arbitrary summary statistics yield consistent inference\nin the Bayesian sense, with these conditions linked to identification of the\ntrue parameters. Using simple illustrative examples that have featured in the\nliterature, we demonstrate that identification, and hence consistency, is\nunlikely to be achieved in many cases, and propose a simple diagnostic\nprocedure that can indicate the presence of this problem. We also formally\nexplore the link between consistency and the use of auxiliary models within\nABC, and illustrate the subsequent results in the Lotka-Volterra predator-prey\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 05:41:41 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Frazier", "David T.", ""], ["Martin", "Gael M.", ""], ["Robert", "Christian P.", ""]]}, {"id": "1508.05214", "submitter": "Matthieu Wilhelm", "authors": "Matthieu Wilhelm, Luca Ded\\`e, Laura M. Sangalli, Pierre Wilhelm", "title": "IGS: an IsoGeometric approach for Smoothing on surfaces", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2015.12.028", "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an Isogeometric approach for smoothing on surfaces, namely\nestimating a function starting from noisy and discrete measurements. More\nprecisely, we aim at estimating functions lying on a surface represented by\nNURBS, which are geometrical representations commonly used in industrial\napplications. The estimation is based on the minimization of a penalized\nleast-square functional. The latter is equivalent to solve a 4th-order Partial\nDifferential Equation (PDE). In this context, we use Isogeometric Analysis\n(IGA) for the numerical approximation of such surface PDE, leading to an\nIsoGeometric Smoothing (IGS) method for fitting data spatially distributed on a\nsurface. Indeed, IGA facilitates encapsulating the exact geometrical\nrepresentation of the surface in the analysis and also allows the use of at\nleast globally $C^1-$continuous NURBS basis functions for which the 4th-order\nPDE can be solved using the standard Galerkin method. We show the performance\nof the proposed IGS method by means of numerical simulations and we apply it to\nthe estimation of the pressure coefficient, and associated aerodynamic force on\na winglet of the SOAR space shuttle.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 08:57:45 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2016 16:21:37 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Wilhelm", "Matthieu", ""], ["Ded\u00e8", "Luca", ""], ["Sangalli", "Laura M.", ""], ["Wilhelm", "Pierre", ""]]}, {"id": "1508.05314", "submitter": "Bojana Milo\\v{s}evi\\'c", "authors": "Bojana Milo\\v{s}evi\\'c", "title": "Asymptotic Efficiency of Goodness-of-fit Tests Based on Too-Lin\n  Characterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new class of uniformity tests is proposed. It is shown that\nthose tests are applicable to the cases of any simple null hypothesis as well\nas for the composite null hypothesis of rectangular distributions on arbitrary\nsupport. The asymptotic properties of test statistics are examined. The tests\nare compared with some standard and some recent uniformity tests. For each test\nthe Bahadur efficiencies against some common local alternatives are calculated.\nA class of locally optimal alternatives is found for each proposed test. The\npower study is also provided. Some applications in time series analysis are\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 15:37:57 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 12:28:51 GMT"}, {"version": "v3", "created": "Sun, 19 Aug 2018 20:26:12 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Milo\u0161evi\u0107", "Bojana", ""]]}, {"id": "1508.05406", "submitter": "Minjie Fan", "authors": "Minjie Fan", "title": "A Note on Spherical Needlets", "comments": "12 pages, 7 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with the traditional spherical harmonics, the spherical needlets are\na new generation of spherical wavelets that possess several attractive\nproperties. Their double localization in both spatial and frequency domains\nempowers them to easily and sparsely represent functions with small spatial\nscale features. This paper is divided into two parts. First, it reviews the\nspherical harmonics and discusses their limitations in representing functions\nwith small spatial scale features. To overcome the limitations, it introduces\nthe spherical needlets and their attractive properties. In the second part of\nthe paper, a Matlab package for the spherical needlets is presented. The\nproperties of the spherical needlets are demonstrated by several examples using\nthe package.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 20:50:49 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Fan", "Minjie", ""]]}, {"id": "1508.05431", "submitter": "Juhee Cho", "authors": "Juhee Cho, Donggyu Kim, Karl Rohe", "title": "Asymptotic Theory for Estimating the Singular Vectors and Values of a\n  Partially-observed Low Rank Matrix with Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion algorithms recover a low rank matrix from a small fraction\nof the entries, each entry contaminated with additive errors. In practice, the\nsingular vectors and singular values of the low rank matrix play a pivotal role\nfor statistical analyses and inferences. This paper proposes estimators of\nthese quantities and studies their asymptotic behavior. Under the setting where\nthe dimensions of the matrix increase to infinity and the probability of\nobserving each entry is identical, Theorem 4.1 gives the rate of convergence\nfor the estimated singular vectors; Theorem 4.3 gives a multivariate central\nlimit theorem for the estimated singular values. Even though the estimators use\nonly a partially observed matrix, they achieve the same rates of convergence as\nthe fully observed case. These estimators combine to form a consistent\nestimator of the full low rank matrix that is computed with a non-iterative\nalgorithm. In the cases studied in this paper, this estimator achieves the\nminimax lower bound in Koltchinskii et al. (2011). The numerical experiments\ncorroborate our theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 22:42:12 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 20:21:41 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cho", "Juhee", ""], ["Kim", "Donggyu", ""], ["Rohe", "Karl", ""]]}, {"id": "1508.05476", "submitter": "Vivian Viallon", "authors": "Edouard Ollier and Vivian Viallon", "title": "Regression modeling on stratified data with the lasso", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of regression models on strata defined using a\ncategorical covariate, in order to identify interactions between this\ncategorical covariate and the other predictors. A basic approach requires the\nchoice of a reference stratum. We show that the performance of a penalized\nversion of this approach depends on this arbitrary choice. We propose a refined\napproach that bypasses this arbitrary choice, at almost no additional\ncomputational cost. Regarding model selection consistency, our proposal mimics\nthe strategy based on an optimal and covariate-specific choice for the\nreference stratum. Results from an empirical study confirm that our proposal\ngenerally outperforms the basic approach in the identification and description\nof the interactions. An illustration on gene expression data is provided.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 07:33:37 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 19:02:14 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Ollier", "Edouard", ""], ["Viallon", "Vivian", ""]]}, {"id": "1508.05561", "submitter": "Boris Beranger", "authors": "Boris Beranger, Simone A. Padoan", "title": "Extreme Dependence Models", "comments": "To appear in Extreme Value Modelling and Risk Analysis: Methods and\n  Applications. Eds. D. Dey and J. Yan. Chapman & Hall/CRC Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme values of real phenomena are events that occur with low frequency,\nbut can have a large impact on real life. These are, in many practical\nproblems, high-dimensional by nature (e.g. Tawn, 1990; Coles and Tawn, 1991).\nTo study these events is of fundamental importance. For this purpose,\nprobabilistic models and statistical methods are in high demand. There are\nseveral approaches to modelling multivariate extremes as described in Falk et\nal. (2011), linked to some extent. We describe an approach for deriving\nmultivariate extreme value models and we illustrate the main features of some\nflexible extremal dependence models. We compare them by showing their utility\nwith a real data application, in particular analyzing the extremal dependence\namong several pollutants recorded in the city of Leeds, UK.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 03:02:36 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Beranger", "Boris", ""], ["Padoan", "Simone A.", ""]]}, {"id": "1508.05571", "submitter": "Kei Hirose", "authors": "Kei Hirose, Hironori Fujisawa, Jun Sese", "title": "Robust sparse Gaussian graphical modeling", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical modeling has been widely used to explore various network\nstructures, such as gene regulatory networks and social networks. We often use\na penalized maximum likelihood approach with the $L_1$ penalty for learning a\nhigh-dimensional graphical model. However, the penalized maximum likelihood\nprocedure is sensitive to outliers. To overcome this problem, we introduce a\nrobust estimation procedure based on the $\\gamma$-divergence. The proposed\nmethod has a redescending property, which is known as a desirable property in\nrobust statistics. The parameter estimation procedure is constructed using the\nMajorize-Minimization algorithm, which guarantees that the objective function\nmonotonically decreases at each iteration. Extensive simulation studies showed\nthat our procedure performed much better than the existing methods, in\nparticular, when the contamination ratio was large. Two real data analyses were\ncarried out to illustrate the usefulness of our proposed procedure.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 07:34:02 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 04:49:36 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2015 04:05:37 GMT"}, {"version": "v4", "created": "Mon, 12 Jun 2017 04:09:34 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Hirose", "Kei", ""], ["Fujisawa", "Hironori", ""], ["Sese", "Jun", ""]]}, {"id": "1508.05588", "submitter": "Federico G. Poloni", "authors": "Federico Poloni and Giacomo Sbrana", "title": "Multivariate trend-cycle extraction with the Hodrick-Prescott filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hodrick-Prescott filter represents one of the most popular method for\ntrend-cycle extraction in macroeconomic time series. In this paper we provide a\nmultivariate generalization of the Hodrick-Prescott filter, based on the\nseemingly unrelated time series approach. We first derive closed-form\nexpressions linking the signal-noise matrix ratio to the parameters of the\nVARMA representation of the model. We then show that the parameters can be\nestimated using a recently introduced method, called \"Moment Estimation Through\nAggregation (META)\". This method replaces the traditional multivariate\nlikelihood estimation with a procedure that requires estimating univariate\nprocesses only. This makes the estimation simpler, faster and better-behaved\nnumerically. We prove that our estimation method is consistent and\nasymptotically normal distributed for the proposed framework. Finally, we\npresent an empirical application focusing on the industrial production of\nseveral European countries.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 10:06:03 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Poloni", "Federico", ""], ["Sbrana", "Giacomo", ""]]}, {"id": "1508.05593", "submitter": "Thomas E Bartlett", "authors": "Thomas E. Bartlett, Adam M. Sykulski, Sofia C. Olhede, Jonathan M.\n  Lilly, and Jeffrey J. Early", "title": "A Power Variance Test for Nonstationarity in Complex-Valued Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for testing the hypothesis of nonstationarity in\ncomplex-valued signals. The implementation uses both the bootstrap and the Fast\nFourier Transform such that the algorithm can be efficiently implemented in\nO(NlogN) time, where N is the length of the observed signal. The test procedure\nexamines the second-order structure and contrasts the observed power variance -\ni.e. the variability of the instantaneous variance over time - with the\nexpected characteristics of stationary signals generated via the bootstrap\nmethod. Our algorithmic procedure is capable of learning different types of\nnonstationarity, such as jumps or strong sinusoidal components. We illustrate\nthe utility of our test and algorithm through application to turbulent flow\ndata from fluid dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 10:47:07 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 22:55:19 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Bartlett", "Thomas E.", ""], ["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "1508.05628", "submitter": "Nicolas Bousquet", "authors": "Shuai Fu and Mathieu Couplet and Nicolas Bousquet", "title": "An adaptive kriging method for solving nonlinear inverse statistical\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various industrial contexts, estimating the distribution of unobserved\nrandom vectors Xi from some noisy indirect observations H(Xi) + Ui is required.\nIf the relation between Xi and the quantity H(Xi), measured with the error Ui,\nis implemented by a CPU-consuming computer model H, a major practical\ndifficulty is to perform the statistical inference with a relatively small\nnumber of runs of H. Following Fu et al. (2014), a Bayesian statistical\nframework is considered to make use of possible prior knowledge on the\nparameters of the distribution of the Xi, which is assumed Gaussian. Moreover,\na Markov Chain Monte Carlo (MCMC) algorithm is carried out to estimate their\nposterior distribution by replacing H by a kriging metamodel build from a\nlimited number of simulated experiments. Two heuristics, involving two\ndifferent criteria to be optimized, are proposed to sequentially design these\ncomputer experiments in the limits of a given computational budget. The first\ncriterion is a Weighted Integrated Mean Square Error (WIMSE). The second one,\ncalled Expected Conditional Divergence (ECD), developed in the spirit of the\nStepwise Uncertainty Reduction (SUR) criterion, is based on the discrepancy\nbetween two consecutive approximations of the target posterior distribution.\nSeveral numerical comparisons conducted over a toy example then a motivating\nreal case-study show that such adaptive designs can significantly outperform\nthe classical choice of a maximin Latin Hypercube Design (LHD) of experiments.\nDealing with a major concern in hydraulic engineering, a particular emphasis is\nplaced upon the prior elicitation of the case-study, highlighting the overall\nfeasibility of the methodology. Faster convergences and manageability\nconsiderations lead to recommend the use of the ECD criterion in practical\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 16:20:14 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Fu", "Shuai", ""], ["Couplet", "Mathieu", ""], ["Bousquet", "Nicolas", ""]]}, {"id": "1508.05652", "submitter": "Dibyendu Bhaumik Mr.", "authors": "Dibyendu Bhaumik and Radhendushka Srivastava and Debasis Sengupta", "title": "Feature Sensitive and Automated Curve Registration", "comments": "43 Pages, 7 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets of functional data having a common underlying mean function\nbut different degrees of distortion in time measurements, we provide a method\nof estimating the time transformation necessary to align (or `register') them.\nWe prove that the proposed method is consistent under fairly general\nconditions. Simulation results show superiority of the performance of the\nproposed method over two existing methods. The proposed method is illustrated\nthrough the analysis of three paleoclimatic data sets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 19:55:13 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 15:01:05 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Bhaumik", "Dibyendu", ""], ["Srivastava", "Radhendushka", ""], ["Sengupta", "Debasis", ""]]}, {"id": "1508.05713", "submitter": "Lucia Modugno mrs", "authors": "Lucia Modugno and Simone Giannerini", "title": "The wild bootstrap for multilevel models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the performance of the most popular bootstrap schemes\nfor multilevel data. Also, we propose a modified version of the wild bootstrap\nprocedure for hierarchical data structures. The wild bootstrap does not require\nhomoscedasticity or assumptions on the distribution of the error processes.\nHence, it is a valuable tool for robust inference in a multilevel framework. We\nassess the finite size performances of the schemes through a Monte Carlo study.\nThe results show that for big sample sizes it always pays off to adopt an\nagnostic approach as the wild bootstrap outperforms other techniques.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 08:01:39 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Modugno", "Lucia", ""], ["Giannerini", "Simone", ""]]}, {"id": "1508.05740", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer and Johannes Elias and Michael H\\\"ohle", "title": "A space-time conditional intensity model for invasive meningococcal\n  disease occurrence", "comments": "Accepted Author Manuscript", "journal-ref": "Biometrics 68, 607--616", "doi": "10.1111/j.1541-0420.2011.01684.x", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel point process model continuous in space-time is proposed for\nquantifying the transmission dynamics of the two most common meningococcal\nantigenic sequence types observed in Germany 2002-2008. Modelling is based on\nthe conditional intensity function (CIF) which is described by a superposition\nof additive and multiplicative components. As an epidemiological interesting\nfinding, spread behaviour was shown to depend on type in addition to age: basic\nreproduction numbers were 0.25 (95% CI 0.19-0.34) and 0.11 (95% CI 0.07-0.17)\nfor types B:P1.7-2,4:F1-5 and C:P1.5,2:F3-3, respectively. Altogether, the\nproposed methodology represents a comprehensive and universal regression\nframework for the modelling, simulation and inference of self-exciting\nspatio-temporal point processes based on the CIF. Usability of the modelling in\nbiometric practice is promoted by an implementation in the R package\nsurveillance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 10:05:59 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Meyer", "Sebastian", ""], ["Elias", "Johannes", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "1508.05847", "submitter": "Meng Li", "authors": "Meng Li and Subhashis Ghosal", "title": "Bayesian Detection of Image Boundaries", "comments": null, "journal-ref": null, "doi": "10.1214/16-AOS1523", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting boundary of an image based on noisy observations is a fundamental\nproblem of image processing and image segmentation. For a $d$-dimensional image\n($d = 2, 3, \\ldots$), the boundary can often be described by a closed smooth\n$(d - 1)$-dimensional manifold. In this paper, we propose a nonparametric\nBayesian approach based on priors indexed by $\\mathbb{S}^{d - 1}$, the unit\nsphere in $\\mathbb{R}^d$. We derive optimal posterior contraction rates using\nGaussian processes or finite random series priors using basis functions such as\ntrigonometric polynomials for 2-dimensional images and spherical harmonics for\n3-dimensional images. For 2-dimensional images, we show a rescaled squared\nexponential Gaussian process on $\\mathbb{S}^1$ achieves four goals of\nguaranteed geometric restriction, (nearly) minimax rate optimal and adaptive to\nthe smoothness level, convenient for joint inference and computationally\nefficient. We conduct an extensive study of its reproducing kernel Hilbert\nspace, which may be of interest by its own and can also be used in other\ncontexts. Simulations confirm excellent performance of the proposed method and\nindicate its robustness under model misspecification at least under the\nsimulated settings.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 15:40:03 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 19:45:16 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 16:12:11 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Li", "Meng", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1508.05880", "submitter": "Sanvesh Srivastava", "authors": "Sanvesh Srivastava, Cheng Li, and David B. Dunson", "title": "Scalable Bayes via Barycenter in Wasserstein Space", "comments": "43 pages, 7 figures, and 11 tables. The updated revision will appear\n  in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divide-and-conquer based methods for Bayesian inference provide a general\napproach for tractable posterior inference when the sample size is large. These\nmethods divide the data into smaller subsets, sample from the posterior\ndistribution of parameters in parallel on all the subsets, and combine\nposterior samples from all the subsets to approximate the full data posterior\ndistribution. The smaller size of any subset compared to the full data implies\nthat posterior sampling on any subset is computationally more efficient than\nsampling from the true posterior distribution. Since the combination step takes\nnegligible time relative to sampling, posterior computations can be scaled to\nmassive data by dividing the full data into a sufficiently large number of data\nsubsets. One such approach relies on the geometry of posterior distributions\nestimated across different subsets and combines them through their barycenter\nin a Wasserstein space of probability measures. We provide theoretical\nguarantees on the accuracy of approximation that are valid in many\napplications. We show that the geometric method approximates the full data\nposterior distribution better than its competitors across diverse simulations\nand reproduces known results when applied to a movie ratings database.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 16:51:15 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 19:48:41 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 05:32:45 GMT"}, {"version": "v4", "created": "Wed, 20 Jun 2018 03:07:52 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Srivastava", "Sanvesh", ""], ["Li", "Cheng", ""], ["Dunson", "David B.", ""]]}, {"id": "1508.05918", "submitter": "Olanrewaju Akande", "authors": "Olanrewaju Akande, Fan Li and Jerome Reiter", "title": "An Empirical Comparison of Multiple Imputation Methods for Categorical\n  Data", "comments": null, "journal-ref": null, "doi": "10.1080/00031305.2016.1277158", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation is a common approach for dealing with missing values in\nstatistical databases. The imputer fills in missing values with draws from\npredictive models estimated from the observed data, resulting in multiple,\ncompleted versions of the database. Researchers have developed a variety of\ndefault routines to implement multiple imputation; however, there has been\nlimited research comparing the performance of these methods, particularly for\ncategorical data. We use simulation studies to compare repeated sampling\nproperties of three default multiple imputation methods for categorical data,\nincluding chained equations using generalized linear models, chained equations\nusing classification and regression trees, and a fully Bayesian joint\ndistribution based on Dirichlet Process mixture models. We base the simulations\non categorical data from the American Community Survey. In the circumstances of\nthis study, the results suggest that default chained equations approaches based\non generalized linear models are dominated by the default regression tree and\nBayesian mixture model approaches. They also suggest competing advantages for\nthe regression tree and Bayesian mixture model approaches, making both\nreasonable default engines for multiple imputation of categorical data. A\nsupplementary material for this article is available online.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 19:14:07 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 01:55:17 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Akande", "Olanrewaju", ""], ["Li", "Fan", ""], ["Reiter", "Jerome", ""]]}, {"id": "1508.05973", "submitter": "Zachary Weller", "authors": "Zachary D. Weller, Jennifer A. Hoeting", "title": "A Review of Nonparametric Hypothesis Tests of Isotropy Properties in\n  Spatial Data", "comments": "22 pages, 6 figures, 9 tables, appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of modeling spatially-referenced data is appropriately\nspecifying the covariance function of the random field. A practitioner working\nwith spatial data is presented a number of choices regarding the structure of\nthe dependence between observations. One of these choices is determining\nwhether or not an isotropic covariance function is appropriate. Isotropy\nimplies that spatial dependence does not depend on the direction of the spatial\nseparation between sampling locations. Misspecification of isotropy properties\n(directional dependence) can lead to misleading inferences, e.g., inaccurate\npredictions and parameter estimates. A researcher may use graphical\ndiagnostics, such as directional sample variograms, to decide whether the\nassumption of isotropy is reasonable. These graphical techniques can be\ndifficult to assess, open to subjective interpretations, and misleading.\nHypothesis tests of the assumption of isotropy may be more desirable. To this\nend, a number of tests of directional dependence have been developed using both\nthe spatial and spectral representations of random fields. We provide an\noverview of nonparametric methods available to test the hypotheses of isotropy\nand symmetry in spatial data. We summarize test properties, discuss important\nconsiderations and recommendations in choosing and implementing a test, compare\nseveral of the methods via a simulation study, and propose a number of open\nresearch questions. Several of the reviewed methods can be implemented in R\nusing our package spTest, available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 21:05:29 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 23:25:37 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Weller", "Zachary D.", ""], ["Hoeting", "Jennifer A.", ""]]}, {"id": "1508.05987", "submitter": "Yi Yang", "authors": "Yi Yang, Teng Zhang, Hui Zou", "title": "Flexible Expectile Regression in Reproducing Kernel Hilbert Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectile, first introduced by Newey and Powell (1987) in the econometrics\nliterature, has recently become increasingly popular in risk management and\ncapital allocation for financial institutions due to its desirable properties\nsuch as coherence and elicitability. The current standard tool for expectile\nregression analysis is the multiple linear expectile regression proposed by\nNewey and Powell in 1987. The growing applications of expectile regression\nmotivate us to develop a much more flexible nonparametric multiple expectile\nregression in a reproducing kernel Hilbert space. The resulting estimator is\ncalled KERE which has multiple advantages over the classical multiple linear\nexpectile regression by incorporating non-linearity, non-additivity and complex\ninteractions in the final estimator. The kernel learning theory of KERE is\nestablished. We develop an efficient algorithm inspired by\nmajorization-minimization principle for solving the entire solution path of\nKERE. It is shown that the algorithm converges at least at a linear rate.\nExtensive simulations are conducted to show the very competitive finite sample\nperformance of KERE. We further demonstrate the application of KERE by using\npersonal computer price data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 22:34:57 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 18:43:20 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Yang", "Yi", ""], ["Zhang", "Teng", ""], ["Zou", "Hui", ""]]}, {"id": "1508.06125", "submitter": "Qing Xiao", "authors": "Qing Xiao", "title": "A method for calculating quantile function and its further use for data\n  fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a polynomial transformation model based on Weibull\ndistribution, whereby the analytical representation of the quantile function\nfor many probability distributions can be obtained. Firstly, the target random\nvariable $x$ with specified distribution is expressed as a polynomial of a\nWeibull random variable $z$, the coefficients are conveniently determined by\nthe percentile matching method. Then, substituting $z$ with its quantile\nfunction $z=\\lambda [-ln(1-u)]^{1/k}$ gives the analytical expression of the\nquantile function of $x$. Furthermore, using the probability weighted moments\nmatching method, this polynomial transformation model can be used for data\nfitting. Through numerical experiment, it makes evident that the proposed model\nis capable of handling some distributions close to binomial which are difficult\nfor the extant approaches, and the quantile functions of various distributions\nare accurately approximated within the probit range $[10^{-4},1-10^{-4}]$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 12:22:01 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Xiao", "Qing", ""]]}, {"id": "1508.06303", "submitter": "Sinead Williamson", "authors": "Finale Doshi-Velez and Sinead A. Williamson", "title": "Restricted Indian Buffet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent feature models are a powerful tool for modeling data with\nglobally-shared features. Nonparametric exchangeable models such as the Indian\nBuffet Process offer modeling flexibility by letting the number of latent\nfeatures be unbounded. However, current models impose implicit distributions\nover the number of latent features per data point, and these implicit\ndistributions may not match our knowledge about the data. In this paper, we\ndemonstrate how the Restricted Indian Buffet Process circumvents this\nrestriction, allowing arbitrary distributions over the number of features in an\nobservation. We discuss several alternative constructions of the model and use\nthe insights gained to develop Markov Chain Monte Carlo and variational methods\nfor simulation and posterior inference.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 20:56:29 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Williamson", "Sinead A.", ""]]}, {"id": "1508.06313", "submitter": "Brett McClintock", "authors": "Ruth King, Brett T. McClintock, Darren Kidney, David Borchers", "title": "Capture-recapture abundance estimation using a semi-complete data\n  likelihood approach", "comments": null, "journal-ref": "Annals of Applied Statistics 2016, Vol. 10, No. 1, 264-285", "doi": "10.1214/15-AOAS890", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capture-recapture data are often collected when abundance estimation is of\ninterest. In the presence of unobserved individual heterogeneity, specified on\na continuous scale for the capture probabilities, the likelihood is not\ngenerally available in closed form, but expressible only as an analytically\nintractable integral. Model-fitting algorithms to estimate abundance most\nnotably include a numerical approximation for the likelihood or use of a\nBayesian data augmentation technique considering the complete data likelihood.\nWe consider a Bayesian hybrid approach, defining a \"semi-complete\" data\nlikelihood, composed of the product of a complete data likelihood component for\nindividuals seen at least once within the study and a marginal data likelihood\ncomponent for the individuals not seen within the study, approximated using\nnumerical integration. This approach combines the advantages of the two\ndifferent approaches, with the semi-complete likelihood component specified as\na single integral (over the dimension of the individual heterogeneity\ncomponent). In addition, the models can be fitted within BUGS/JAGS (commonly\nused for the Bayesian complete data likelihood approach) but with significantly\nimproved computational efficiency compared to the commonly used\nsuper-population data augmentation approaches (between about 10 and 77 times\nmore efficient in the two examples we consider). The semi-complete likelihood\napproach is flexible and applicable to a range of models, including spatially\nexplicit capture-recapture models. The model-fitting approach is applied to two\ndifferent datasets corresponding to the closed population model $M_h$ for\nsnowshoe hare data and a spatially explicit capture-recapture model applied to\ngibbon data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 21:38:42 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 16:58:24 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["King", "Ruth", ""], ["McClintock", "Brett T.", ""], ["Kidney", "Darren", ""], ["Borchers", "David", ""]]}, {"id": "1508.06321", "submitter": "Robert Staudte", "authors": "Luke A. Prendergast and Robert G. Staudte", "title": "When large n is not enough---Distribution-free Interval Estimators for\n  Ratios of Quantiles", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": "10.1007/s10888-017-9347-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ratios of sample percentiles or of quantiles based on a single sample are\noften published for skewed income data to illustrate aspects of income\ninequality, but distribution-free confidence intervals for such ratios are to\nour knowledge not in the literature. Here we derive and compare two\nlarge-sample methods for obtaining such intervals. They both require good\ndistribution-free estimates of the quantile density at the quantiles of\ninterest, and such estimates have recently become available. Simulation studies\nfor various sample sizes are carried out for Pareto, lognormal and exponential\ndistributions, as well as fitted generalized lambda distributions, to determine\nthe coverage probabilities and widths of the intervals. Robustness of the\nestimators to contamination or a positive proportion of zero incomes is\nexamined via influence functions. The motivating example is Australian\nhousehold income data where ratios of quantiles measure inequality, but of\ncourse these results apply equally to data from other countries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 22:45:40 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2015 23:58:57 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Prendergast", "Luke A.", ""], ["Staudte", "Robert G.", ""]]}, {"id": "1508.06359", "submitter": "Gang Cheng Dr.", "authors": "Gang Cheng, Sicong Wang, Yuhong Yang", "title": "Forecast Combination Under Heavy-Tailed Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecast combination has been proven to be a very important technique to\nobtain accurate predictions. In many applications, forecast errors exhibit\nheavy tail behaviors for various reasons. Unfortunately, to our knowledge,\nlittle has been done to deal with forecast combination for such situations. The\nfamiliar forecast combination methods such as simple average, least squares\nregression, or those based on variance-covariance of the forecasts, may perform\nvery poorly. In this paper, we propose two nonparametric forecast combination\nmethods to address the problem. One is specially proposed for the situations\nthat the forecast errors are strongly believed to have heavy tails that can be\nmodeled by a scaled Student's t-distribution; the other is designed for\nrelatively more general situations when there is a lack of strong or consistent\nevidence on the tail behaviors of the forecast errors due to shortage of data\nand/or evolving data generating process. Adaptive risk bounds of both methods\nare developed. Simulations and a real example show superior performance of the\nnew methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 04:19:47 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Cheng", "Gang", ""], ["Wang", "Sicong", ""], ["Yang", "Yuhong", ""]]}, {"id": "1508.06378", "submitter": "Yi Yang", "authors": "Yi Yang, Wei Qian, Hui Zou", "title": "Insurance Premium Prediction via Gradient Tree-Boosted Tweedie Compound\n  Poisson Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tweedie GLM is a widely used method for predicting insurance premiums.\nHowever, the structure of the logarithmic mean is restricted to a linear form\nin the Tweedie GLM, which can be too rigid for many applications. As a better\nalternative, we propose a gradient tree-boosting algorithm and apply it to\nTweedie compound Poisson models for pure premiums. We use a profile likelihood\napproach to estimate the index and dispersion parameters. Our method is capable\nof fitting a flexible nonlinear Tweedie model and capturing complex\ninteractions among predictors. A simulation study confirms the excellent\nprediction performance of our method. As an application, we apply our method to\nan auto insurance claim data and show that the new method is superior to the\nexisting methods in the sense that it generates more accurate premium\npredictions, thus helping solve the adverse selection issue. We have\nimplemented our method in a user-friendly R package that also includes a nice\nvisualization tool for interpreting the fitted model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 06:51:26 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 04:57:21 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 23:14:53 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Yang", "Yi", ""], ["Qian", "Wei", ""], ["Zou", "Hui", ""]]}, {"id": "1508.06433", "submitter": "Qing Xiao", "authors": "Qing Xiao", "title": "Generating correlated random vector by polynomial normal transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a polynomial normal transformation model, whereby various\nnon-normal probability distributions can be simulated by the standard normal\ndistribution. Two methods are presented to determine the coefficients of\npolynomial model: (1) probability weighted moment (PWM) matching (2) percentile\nmatching. Compared to the existing raw moment or L-moment matching, the\nproposed methods are more computationally convenient, and can be used to\nestimate the coefficients of polynomial model with a higher degree.\nFurthermore, for two correlated random variables, a polynomial equation is\nderived to estimate the equivalent correlation coefficient in standard normal\nspace, and random vector with non-normal marginal distributions and prescribed\ncorrelation matrix can be generated. Finally, numerical examples are worked to\ndemonstrate the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 10:18:55 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Xiao", "Qing", ""]]}, {"id": "1508.06452", "submitter": "Janne Hakkarainen", "authors": "Antti Solonen, Tiangang Cui, Janne Hakkarainen, and Youssef Marzouk", "title": "On dimension reduction in Gaussian filters", "comments": null, "journal-ref": null, "doi": "10.1088/0266-5611/32/4/045003", "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A priori dimension reduction is a widely adopted technique for reducing the\ncomputational complexity of stationary inverse problems. In this setting, the\nsolution of an inverse problem is parameterized by a low-dimensional basis that\nis often obtained from the truncated Karhunen-Loeve expansion of the prior\ndistribution. For high-dimensional inverse problems equipped with smoothing\npriors, this technique can lead to drastic reductions in parameter dimension\nand significant computational savings.\n  In this paper, we extend the concept of a priori dimension reduction to\nnon-stationary inverse problems, in which the goal is to sequentially infer the\nstate of a dynamical system. Our approach proceeds in an offline-online\nfashion. We first identify a low-dimensional subspace in the state space before\nsolving the inverse problem (the offline phase), using either the method of\n\"snapshots\" or regularized covariance estimation. Then this subspace is used to\nreduce the computational complexity of various filtering algorithms - including\nthe Kalman filter, extended Kalman filter, and ensemble Kalman filter - within\na novel subspace-constrained Bayesian prediction-and-update procedure (the\nonline phase). We demonstrate the performance of our new dimension reduction\napproach on various numerical examples. In some test cases, our approach\nreduces the dimensionality of the original problem by orders of magnitude and\nyields up to two orders of magnitude in computational savings.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 11:41:59 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 14:04:04 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 08:37:39 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Solonen", "Antti", ""], ["Cui", "Tiangang", ""], ["Hakkarainen", "Janne", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1508.06476", "submitter": "Tobias Michael Erhardt", "authors": "Tobias M. Erhardt and Claudia Czado", "title": "Standardized drought indices: A novel uni- and multivariate approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As drought is among the natural hazards which affects people and economies\nworldwide and often results in huge monetary losses sophisticated methods for\ndrought monitoring and decision making are needed. Several different approaches\nto quantify drought have been developed during past decades. However, most of\nthese drought indices suffer from different shortcomings and do not account for\nthe multiple driving factors which promote drought conditions and their\ninter-dependencies. We provide a novel methodology for the calculation of\n(multivariate) drought indices, which combines the advantages of existing\napproaches and omits their disadvantages. Moreover, our approach benefits from\nthe flexibility of vine copulas in modeling multivariate non-Gaussian\ninter-variable dependence structures. A three-variate data example is used in\norder to investigate drought conditions in Europe and to illustrate and reason\nthe different modeling steps. The data analysis shows the appropriateness of\nthe described methodology. Comparison to well-established drought indices shows\nthe benefits of our multivariate approach. The validity of the new methodology\nis verified by comparing the spatial extent of historic drought events based on\ndifferent drought indices. Further, we show that the assumption of non-Gaussian\ndependence structures is well-grounded in this real-world application.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 12:56:53 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Erhardt", "Tobias M.", ""], ["Czado", "Claudia", ""]]}, {"id": "1508.06523", "submitter": "Ulrike Schl\\\"agel", "authors": "Ulrike E. Schl\\\"agel and Mark A. Lewis", "title": "Robustness of movement models: can models bridge the gap between\n  temporal scales of data sets and behavioural processes?", "comments": "38 pages, 10 figures, submitted to Journal of Mathematical Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete-time random walks and their extensions are common tools for\nanalyzing animal movement data. In these analyses, resolution of temporal\ndiscretization is a critical feature. Ideally, a model both mirrors the\nrelevant temporal scale of the biological process of interest and matches the\ndata sampling rate. Challenges arise when resolution of data is too coarse due\nto technological constraints, or when we wish to extrapolate results or compare\nresults obtained from data with different resolutions. Drawing loosely on the\nconcept of robustness in statistics, we propose a rigorous mathematical\nframework for studying movement models' robustness against changes in temporal\nresolution. In this framework, we define varying levels of robustness as formal\nmodel properties, focusing on random walk models with spatially-explicit\ncomponent. With the new framework, we can investigate whether models can\nvalidly be applied to data across varying temporal resolutions and how we can\naccount for these different resolutions in statistical inference results. We\napply the new framework to movement-based resource selection models,\ndemonstrating both analytical and numerical calculations, as well as a Monte\nCarlo simulation approach. While exact robustness is rare, the concept of\napproximate robustness provides a promising new direction for analyzing\nmovement models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 15:03:20 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Schl\u00e4gel", "Ulrike E.", ""], ["Lewis", "Mark A.", ""]]}, {"id": "1508.06749", "submitter": "Torsten Hothorn", "authors": "Torsten Hothorn, Lisa M\\\"ost, Peter B\\\"uhlmann", "title": "Most Likely Transformations", "comments": "Accepted for publication by the Scandinavian Journal of Statistics\n  2017-06-19", "journal-ref": "Scandinavian Journal of Statistics (2018)", "doi": "10.1111/sjos.12291", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose and study properties of maximum likelihood estimators in the class\nof conditional transformation models. Based on a suitable explicit\nparameterisation of the unconditional or conditional transformation function,\nwe establish a cascade of increasingly complex transformation models that can\nbe estimated, compared and analysed in the maximum likelihood framework. Models\nfor the unconditional or conditional distribution function of any univariate\nresponse variable can be set-up and estimated in the same theoretical and\ncomputational framework simply by choosing an appropriate transformation\nfunction and parameterisation thereof. The ability to evaluate the distribution\nfunction directly allows us to estimate models based on the exact likelihood,\nespecially in the presence of random censoring or truncation. For discrete and\ncontinuous responses, we establish the asymptotic normality of the proposed\nestimators. A reference software implementation of maximum likelihood-based\nestimation for conditional transformation models allowing the same flexibility\nas the theory developed here was employed to illustrate the wide range of\npossible applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 08:26:51 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 09:38:21 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 12:06:01 GMT"}, {"version": "v4", "created": "Tue, 20 Jun 2017 08:47:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hothorn", "Torsten", ""], ["M\u00f6st", "Lisa", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1508.06803", "submitter": "Claus Ekstr{\\o}m", "authors": "Claus Thorn Ekstr{\\o}m and Thomas Alexander Gerds and Andreas Kryger\n  Jensen and Kasper Brink-Jensen", "title": "Sequential rank agreement methods for comparison of ranked lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The comparison of alternative rankings of a set of items is a general and\nprominent task in applied statistics. Predictor variables are ranked according\nto magnitude of association with an outcome, prediction models rank subjects\naccording to the personalized risk of an event, and genetic studies rank genes\naccording to their difference in gene expression levels. This article\nconstructs measures of the agreement of two or more ordered lists. We use the\nstandard deviation of the ranks to define a measure of agreement that both\nprovides an intuitive interpretation and can be applied to any number of lists\neven if some or all are incomplete or censored. The approach can identify\nchange-points in the agreement of the lists and the sequential changes of\nagreement as a function of the depth of the lists can be compared graphically\nto a permutation based reference set. The usefulness of these tools are\nillustrated using gene rankings, and using data from two Danish ovarian cancer\nstudies where we assess the within and between agreement of different\nstatistical classification methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 11:20:33 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Ekstr\u00f8m", "Claus Thorn", ""], ["Gerds", "Thomas Alexander", ""], ["Jensen", "Andreas Kryger", ""], ["Brink-Jensen", "Kasper", ""]]}, {"id": "1508.06845", "submitter": "Louis Aslett", "authors": "Louis J. M. Aslett, Pedro M. Esperan\\c{c}a, Chris C. Holmes", "title": "Encrypted statistical machine learning: new privacy preserving methods", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new statistical machine learning methods designed to learn on\nfully homomorphic encrypted (FHE) data. The introduction of FHE schemes\nfollowing Gentry (2009) opens up the prospect of privacy preserving statistical\nmachine learning analysis and modelling of encrypted data without compromising\nsecurity constraints. We propose tailored algorithms for applying extremely\nrandom forests, involving a new cryptographic stochastic fraction estimator,\nand na\\\"{i}ve Bayes, involving a semi-parametric model for the class decision\nboundary, and show how they can be used to learn and predict from encrypted\ndata. We demonstrate that these techniques perform competitively on a variety\nof classification data sets and provide detailed information about the\ncomputational practicalities of these and other FHE methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 13:06:55 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Aslett", "Louis J. M.", ""], ["Esperan\u00e7a", "Pedro M.", ""], ["Holmes", "Chris C.", ""]]}, {"id": "1508.06867", "submitter": "Shu Yang", "authors": "Shu Yang and Judith J. Lok", "title": "Doubly Robust Goodness-of-Fit Test of Coarse Structural Nested Mean\n  Models with Application to Initiating combination antiretroviral treatment in\n  HIV-Positive Patients", "comments": "25 pages, 0 figure", "journal-ref": "Biometrika (2016)", "doi": "10.1093/biomet/asw031", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarse Structural Nested Mean Models (SNMMs) provide useful tools to estimate\ntreatment effects from longitudinal observational data with time-dependent\nconfounders. Coarse SNMMs lead to a large class of estimators,within which an\noptimal estimator can be derived under the conditions of well-specified models\nfor the treatment effect, for treatment initiation, and for nuisance regression\noutcomes (Lok & Griner, 2015). The key assumption lies in a well-specified\nmodel for the treatment effect; however, there is no existing guidance to\nspecify the treatment effect model, and model misspecification leads to biased\nestimators, preventing valid inference. To test whether the treatment effect\nmodel matches the data well, we derive a goodness-of-fit (GOF) test procedure\nbased on overidentification restrictions tests (Sargan, 1958; Hansen, 1982). We\nshow that our GOF statistic is doubly-robust in the sense that with a correct\ntreatment effect model, if either the treatment initiation model or the\nnuisance regression outcome model is correctly specified, the GOF statistic has\na Chi-squared limiting distribution with degrees of freedom equal to the number\nof overidentification restrictions. We demonstrate the empirical relevance of\nour methods using simulation designs based on an actual dataset. In addition,\nwe apply the GOF test procedure to study how the initiation time of highly\nactive antiretroviral treatment (HAART) after infection predicts the one-year\ntreatment effect in HIV-positive patients with acute and early infection.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 14:20:42 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 16:26:00 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yang", "Shu", ""], ["Lok", "Judith J.", ""]]}, {"id": "1508.06886", "submitter": "Shu Yang", "authors": "Shu Yang and Zhengyuan Zhu", "title": "Semiparametric estimation of spectral density function for irregular\n  spatial data", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance structure of spatial processes is of fundamental\nimportance in spatial statistics. In the literature, several non-parametric and\nsemi-parametric methods have been developed to estimate the covariance\nstructure based on the spectral representation of covariance functions.\nHowever,they either ignore the high frequency properties of the spectral\ndensity, which are essential to determine the performance of interpolation\nprocedures such as Kriging, or lack of theoretical justification. We propose a\nnew semi-parametric method to estimate spectral densities of isotropic spatial\nprocesses with irregular observations. The spectral density function at low\nfrequencies is estimated using smoothing spline, while a parametric model is\nused for the spectral density at high frequencies, and the parameters are\nestimated by a method-of-moment approach based on empirical variograms at small\nlags. We derive the asymptotic bounds for bias and variance of the proposed\nestimator. The simulation study shows that our method outperforms the existing\nnon-parametric estimator by several performance criteria.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 14:53:21 GMT"}], "update_date": "2016-11-06", "authors_parsed": [["Yang", "Shu", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1508.06945", "submitter": "Shu Yang", "authors": "Shu Yang and Jae Kwang Kim", "title": "Fractional Imputation in Survey Sampling: A Comparative Review", "comments": "26 pages, 2 figures", "journal-ref": "Statistical Science (2016)", "doi": "10.1214/16-STS569", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional imputation (FI) is a relatively new method of imputation for\nhandling item nonresponse in survey sampling. In FI, several imputed values\nwith their fractional weights are created for each missing item. Each\nfractional weight represents the conditional probability of the imputed value\ngiven the observed data, and the parameters in the conditional probabilities\nare often computed by an iterative method such as EM algorithm. The underlying\nmodel for FI can be fully parametric, semiparametric, or nonparametric,\ndepending on plausibility of assumptions and the data structure.\n  In this paper, we give an overview of FI, introduce key ideas and methods to\nreaders who are new to the FI literature, and highlight some new development.\nWe also provide guidance on practical implementation of FI and valid\ninferential tools after imputation. We demonstrate the empirical performance of\nFI with respect to multiple imputation using a pseudo finite population\ngenerated from a sample in Monthly Retail Trade Survey in US Census Bureau.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 17:25:57 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yang", "Shu", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1508.06948", "submitter": "Shu Yang", "authors": "Shu Yang, Guido W. Imbens, Zhanglin Cui, Douglas Faries, and Zbigniew\n  Kadziola", "title": "Propensity Score Matching and Subclassification in Observational Studies\n  with Multi-level Treatments", "comments": "29 pages, 1 figure", "journal-ref": "Biometrics (2016)", "doi": "10.1111/biom.12505", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop new methods for estimating average treatment\neffects in observational studies, focusing on settings with more than two\ntreatment levels under unconfoundedness given pre-treatment variables. We\nemphasize subclassification and matching methods which have been found to be\neffective in the binary treatment literature and which are among the most\npopular methods in that setting. Whereas the literature has suggested that\nthese particular propensity-based methods do not naturally extend to the\nmulti-level treatment case, we show, using the concept of weak\nunconfoundedness, that adjusting for or matching on a scalar function of the\npre-treatment variables removes all biases associated with observed\npre-treatment variables. We apply the proposed methods to an analysis of the\neffect of treatments for fibromyalgia. We also carry out a simulation study to\nassess the finite sample performance of the methods relative to previously\nproposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 17:33:22 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 22:37:51 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yang", "Shu", ""], ["Imbens", "Guido W.", ""], ["Cui", "Zhanglin", ""], ["Faries", "Douglas", ""], ["Kadziola", "Zbigniew", ""]]}, {"id": "1508.06977", "submitter": "Shu Yang", "authors": "Shu Yang and Jae Kwang Kim", "title": "A note on multiple imputation for method of moments estimation", "comments": "8 pages, 0 figure", "journal-ref": "Biometrika (2016)", "doi": "10.1093/biomet/asv073", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation is a popular imputation method for general purpose\nestimation. Rubin(1987) provided an easily applicable formula for the variance\nestimation of multiple imputation. However, the validity of the multiple\nimputation inference requires the congeniality condition of Meng(1994), which\nis not necessarily satisfied for method of moments estimation. This paper\npresents the asymptotic bias of Rubin's variance estimator when the method of\nmoments estimator is used as a complete-sample estimator in the multiple\nimputation procedure. A new variance estimator based on over-imputation is\nproposed to provide asymptotically valid inference for method of moments\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 19:15:15 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yang", "Shu", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1508.07045", "submitter": "Shujie Ma", "authors": "Shujie Ma and Jian Huang", "title": "A concave pairwise fusion approach to subgroup analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important step in developing individualized treatment strategies is to\ncorrectly identify subgroups of a heterogeneous population, so that specific\ntreatment can be given to each subgroup. In this paper, we consider the\nsituation with samples drawn from a population consisting of subgroups with\ndifferent means, along with certain covariates. We propose a penalized approach\nfor subgroup analysis based on a regression model, in which heterogeneity is\ndriven by unobserved latent factors and thus can be represented by using\nsubject-specific intercepts. We apply concave penalty functions to pairwise\ndifferences of the intercepts. This procedure automatically divides the\nobservations into subgroups. We develop an alternating direction method of\nmultipliers algorithm with concave penalties to implement the proposed approach\nand demonstrate its convergence. We also establish the theoretical properties\nof our proposed estimator and determine the order requirement of the minimal\ndifference of signals between groups in order to recover them. These results\nprovide a sound basis for making statistical inference in subgroup analysis.\nOur proposed method is further illustrated by simulation studies and analysis\nof the Cleveland heart disease dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 22:12:21 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Ma", "Shujie", ""], ["Huang", "Jian", ""]]}, {"id": "1508.07155", "submitter": "Rui Tuo", "authors": "Rui Tuo and C. F. Jeff Wu", "title": "A theoretical framework for calibration in computer models:\n  parametrization, estimation and convergence properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration parameters in deterministic computer experiments are those\nattributes that cannot be measured or available in physical experiments.\nKennedy and O'Hagan \\cite{kennedy2001bayesian} suggested an approach to\nestimate them by using data from physical experiments and computer simulations.\nA theoretical framework is given which allows us to study the issues of\nparameter identifiability and estimation. We define the $L_2$-consistency for\ncalibration as a justification for calibration methods. It is shown that a\nsimplified version of the original KO method leads to asymptotically\n$L_2$-inconsistent calibration. This $L_2$-inconsistency can be remedied by\nmodifying the original estimation procedure. A novel calibration method, called\nthe $L_2$ calibration, is proposed and proven to be $L_2$-consistent and enjoys\noptimal convergence rate. A numerical example and some mathematical analysis\nare used to illustrate the source of the $L_2$-inconsistency problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 10:08:16 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Tuo", "Rui", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1508.07403", "submitter": "Daniel Taylor-Rodriguez", "authors": "Daniel Taylor-Rodriguez and Andrew Womack and Claudio Fuentes and\n  Nikolay Bliznyuk", "title": "Intrinsic Bayesian Analysis for Occupancy Models", "comments": "27 pages, 2 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occupancy models are typically used to determine the probability of a species\nbeing present at a given site while accounting for imperfect detection. The\nsurvey data underlying these models often include information on several\npredictors that could potentially characterize habitat suitability and species\ndetectability. Because these variables might not all be relevant, model\nselection techniques are necessary in this context. In practice, model\nselection is performed using the Akaike Information Criterion (AIC), as few\nother alternatives are available. This paper builds an objective Bayesian\nvariable selection framework for occupancy models through the intrinsic prior\nmethodology. The procedure incorporates priors on the model space that account\nfor test multiplicity and respect the polynomial hierarchy of the predictors\nwhen higher-order terms are considered. The methodology is implemented using a\nstochastic search algorithm that is able to thoroughly explore large spaces of\noccupancy models. The proposed strategy is entirely automatic and provides\ncontrol of false positives without sacrificing the discovery of truly\nmeaningful covariates. The performance of the method is evaluated and compared\nto AIC through a simulation study. The method is illustrated on two datasets\npreviously studied in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 05:40:39 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 20:29:47 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Taylor-Rodriguez", "Daniel", ""], ["Womack", "Andrew", ""], ["Fuentes", "Claudio", ""], ["Bliznyuk", "Nikolay", ""]]}, {"id": "1508.07448", "submitter": "Ryan Martin", "authors": "P. Richard Hahn, Ryan Martin, Stephen G. Walker", "title": "On recursive Bayesian predictive distributions", "comments": "22 pages, 3 figures, 3 tables", "journal-ref": "Journal of the American Statistical Association, 2018, volume 113,\n  number 523, pages 1085--1093", "doi": "10.1080/01621459.2017.1304219", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian framework is attractive in the context of prediction, but a fast\nrecursive update of the predictive distribution has apparently been out of\nreach, in part because Monte Carlo methods are generally used to compute the\npredictive. This paper shows that online Bayesian prediction is possible by\ncharacterizing the Bayesian predictive update in terms of a bivariate copula,\nmaking it unnecessary to pass through the posterior to update the predictive.\nIn standard models, the Bayesian predictive update corresponds to familiar\nchoices of copula but, in nonparametric problems, the appropriate copula may\nnot have a closed-form expression. In such cases, our new perspective suggests\na fast recursive approximation to the predictive density, in the spirit of\nNewton's predictive recursion algorithm, but without requiring evaluation of\nnormalizing constants. Consistency of the new algorithm is shown, and numerical\nexamples demonstrate its quality performance in finite-samples compared to\nfully Bayesian and kernel methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 13:27:06 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 12:23:26 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 15:27:53 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 18:26:12 GMT"}, {"version": "v5", "created": "Mon, 1 May 2017 02:20:25 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hahn", "P. Richard", ""], ["Martin", "Ryan", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1508.07511", "submitter": "Rebecca Yates Coley", "authors": "R. Yates Coley (1), Aaron J. Fisher (1), Mufaddal Mamawala (2), H.\n  Ballentine Carter (2), Kenneth J. Pienta (2), and Scott L. Zeger (1) ((1)\n  Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health,\n  Baltimore, USA, (2) The James Buchanan Brady Urological Institute, Johns\n  Hopkins Medical Institutions, Baltimore, USA)", "title": "A Bayesian Hierarchical Model for Prediction of Latent Health States\n  from Multiple Data Sources with Application to Active Surveillance of\n  Prostate Cancer", "comments": "25 pages (double-spaced, with references), 6 figures for primary\n  paper and 44 pages, 22 figures, 15 tables for appendix; Revision submittied\n  for review 5/16; Changes from original version include changed title,\n  additional simulations and figures, editing for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a Bayesian hierarchical model for predicting a\nlatent health state from longitudinal clinical measurements. Model development\nis motivated by the need to integrate multiple sources of data to improve\nclinical decisions about whether to remove or irradiate a patient's prostate\ncancer. Existing modeling approaches are extended to accommodate measurement\nerror in cancer state determinations based on biopsied tissue, clinical\nmeasurements possibly not missing at random, and informative partial\nobservation of the true state. The proposed model enables estimation of whether\nan individual's underlying prostate cancer is aggressive, requiring surgery\nand/or radiation, or indolent, permitting continued surveillance. These\nindividualized predictions can then be communicated to clinicians and patients\nto inform decision-making. We demonstrate the model with data from a cohort of\nlow risk prostate cancer patients at Johns Hopkins University and assess\npredictive accuracy among a subset for whom true cancer state is observed.\nSimulation studies confirm model performance and explore the impact of\nadjusting for informative missingness on true state predictions. R code and\nsimulated data available at\nhttps://github.com/rycoley/prediction-prostate-surveillance.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 22:42:24 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 04:13:52 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2016 20:46:24 GMT"}, {"version": "v4", "created": "Wed, 1 Jun 2016 20:01:24 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Coley", "R. Yates", ""], ["Fisher", "Aaron J.", ""], ["Mamawala", "Mufaddal", ""], ["Carter", "H. Ballentine", ""], ["Pienta", "Kenneth J.", ""], ["Zeger", "Scott L.", ""]]}, {"id": "1508.07634", "submitter": "Ali Akbar Jafari", "authors": "Saeid Tahmasebi and Ali Akbar Jafari", "title": "Generalized Gompertz-power series distributions", "comments": "Accepted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the generalized Gompertz-power series class of\ndistributions which is obtained by compounding generalized Gompertz and power\nseries distributions. This compounding procedure follows same way that was\npreviously carried out by Silva et al. (2013) and Barreto-Souza et al. (2011)\nin introducing the compound class of extended Weibull-power series distribution\nand the Weibull-geometric distribution, respectively. This distribution\ncontains several lifetime models such as generalized Gompertz, generalized\nGompertz-geometric, generalized Gompertz-poisson, generalized Gompertz-binomial\ndistribution, and generalized Gompertz-logarithmic distribution as special\ncases. The hazard rate function of the new class of distributions can be\nincreasing, decreasing and bathtub-shaped. We obtain several properties of this\ndistribution such as its probability density function, Shannon entropy, its\nmean residual life and failure rate functions, quantiles and moments. The\nmaximum likelihood estimation procedure via a EM-algorithm is presented, and\nsub-models of the distribution are studied in details.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 20:55:09 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Tahmasebi", "Saeid", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1508.07678", "submitter": "Shahriar Shariat", "authors": "Shahriar Shariat, Burkay Orten, Ali Dasdan", "title": "Online Model Evaluation in a Large-Scale Computational Advertising\n  Platform", "comments": "Accepted to ICDM2015", "journal-ref": "ICDM (2015) pp. 369 - 378", "doi": "10.1109/ICDM.2015.32", "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online media provides opportunities for marketers through which they can\ndeliver effective brand messages to a wide range of audiences. Advertising\ntechnology platforms enable advertisers to reach their target audience by\ndelivering ad impressions to online users in real time. In order to identify\nthe best marketing message for a user and to purchase impressions at the right\nprice, we rely heavily on bid prediction and optimization models. Even though\nthe bid prediction models are well studied in the literature, the equally\nimportant subject of model evaluation is usually overlooked. Effective and\nreliable evaluation of an online bidding model is crucial for making faster\nmodel improvements as well as for utilizing the marketing budgets more\nefficiently. In this paper, we present an experimentation framework for bid\nprediction models where our focus is on the practical aspects of model\nevaluation. Specifically, we outline the unique challenges we encounter in our\nplatform due to a variety of factors such as heterogeneous goal definitions,\nvarying budget requirements across different campaigns, high seasonality and\nthe auction-based environment for inventory purchasing. Then, we introduce\nreturn on investment (ROI) as a unified model performance (i.e., success)\nmetric and explain its merits over more traditional metrics such as\nclick-through rate (CTR) or conversion rate (CVR). Most importantly, we discuss\ncommonly used evaluation and metric summarization approaches in detail and\npropose a more accurate method for online evaluation of new experimental models\nagainst the baseline. Our meta-analysis-based approach addresses various\nshortcomings of other methods and yields statistically robust conclusions that\nallow us to conclude experiments more quickly in a reliable manner. We\ndemonstrate the effectiveness of our evaluation strategy on real campaign data\nthrough some experiments.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 04:11:50 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Shariat", "Shahriar", ""], ["Orten", "Burkay", ""], ["Dasdan", "Ali", ""]]}, {"id": "1508.07720", "submitter": "Saul Toscano-Palmerin", "authors": "Saul Toscano-Palmerin and Peter I. Frazier", "title": "Asymptotic Validity of the Bayes-Inspired Indifference Zone Procedure:\n  The Non-Normal Known Variance Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the indifference-zone (IZ) formulation of the ranking and\nselection problem in which the goal is to choose an alternative with the\nlargest mean with guaranteed probability, as long as the difference between\nthis mean and the second largest exceeds a threshold. Conservatism leads\nclassical IZ procedures to take too many samples in problems with many\nalternatives. The Bayes-inspired Indifference Zone (BIZ) procedure, proposed in\nFrazier (2014), is less conservative than previous procedures, but its proof of\nvalidity requires strong assumptions, specifically that samples are normal, and\nvariances are known with an integer multiple structure. In this paper, we show\nasymptotic validity of a slight modification of the original BIZ procedure as\nthe difference between the best alternative and the second best goes to\nzero,when the variances are known and finite, and samples are independent and\nidentically distributed, but not necessarily normal.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 08:46:07 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Toscano-Palmerin", "Saul", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1508.07825", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and Petro Kolesnyk and Ralf A. Wilke", "title": "Bi-log-concave distribution functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric statistics for distribution functions F or densities f=F' under\nqualitative shape constraints provides an interesting alternative to classical\nparametric or entirely nonparametric approaches. We contribute to this area by\nconsidering a new shape constraint: F is said to be bi-log-concave, if both\nlog(F) and log(1 - F) are concave. Many commonly considered distributions are\ncompatible with this constraint. For instance, any c.d.f. F with log-concave\ndensity f = F' is bi-log-concave. But in contrast to the latter constraint,\nbi-log-concavity allows for multimodal densities. We provide various\ncharacterizations. It is shown that combining any nonparametric confidence band\nfor F with the new shape-constraint leads to substantial improvements,\nparticularly in the tails. To pinpoint this, we show that these confidence\nbands imply non-trivial confidence bounds for arbitrary moments and the moment\ngenerating function of F.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 13:43:57 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 15:57:11 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 08:14:29 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2016 06:43:53 GMT"}, {"version": "v5", "created": "Fri, 28 Oct 2016 16:25:41 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Duembgen", "Lutz", ""], ["Kolesnyk", "Petro", ""], ["Wilke", "Ralf A.", ""]]}, {"id": "1508.07937", "submitter": "Vahed Maroufy", "authors": "Vahed Maroufy and Paul Marriott", "title": "Local and global robustness in conjugate Bayesian analysis", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the influence of perturbations of conjugate priors in\nBayesian inference. A perturbed prior is defined inside a larger family, local\nmixture models, and the effect on posterior inference is studied. The\nperturbation, in some sense, generalizes the linear perturbation studied in\n\\cite{Gustafson1996}. It is intuitive, naturally normalized and is flexible for\nstatistical applications. Both global and local sensitivity analyses are\nconsidered. A geometric approach is employed for optimizing the sensitivity\ndirection function, the difference between posterior means and the divergence\nfunction between posterior predictive models. All the sensitivity measure\nfunctions are defined on a convex space with non-trivial boundary which is\nshown to be a smooth manifold.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 18:09:05 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Maroufy", "Vahed", ""], ["Marriott", "Paul", ""]]}]