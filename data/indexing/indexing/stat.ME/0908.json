[{"id": "0908.0067", "submitter": "Michael Wood", "authors": "Michael Wood", "title": "Making statistical methods in management research more useful: some\n  suggestions from a case study", "comments": "27 pages, 2 figures. New version has amended title, revised abstract,\n  and the rest of the paper has been simplified", "journal-ref": "Slightly revised version published in Sage Open, vol 3, no 1, 2013", "doi": "10.1177/2158244013476873", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a critique of the methods used in a typical paper. This leads to\nthree broad conclusions about the conventional use of statistical methods.\nFirst, results are often reported in an unnecessarily obscure manner. Second,\nthe null hypothesis testing paradigm is deeply flawed: estimating the size of\neffects and citing confidence intervals or levels is usually better. Third,\nthere are several issues, independent of the particular statistical concepts\nemployed, which limit the value of any statistical approach: e.g. difficulties\nof generalizing to different contexts, and the weakness of some research in\nterms of the size of the effects found. The first two of these are easily\nremedied: I illustrate some of the possibilities by re-analyzing the data from\nthe case study article. The third means that in some contexts a statistical\napproach may not be worthwhile. My case study is a management paper, but\nsimilar problems arise in other social sciences. Keywords: Confidence,\nHypothesis testing, Null hypothesis significance tests, Philosophy of\nstatistics, Statistical methods, User-friendliness.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2009 11:09:25 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2010 13:21:46 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2012 08:53:19 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2012 12:45:00 GMT"}, {"version": "v5", "created": "Mon, 12 Nov 2012 15:47:32 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Wood", "Michael", ""]]}, {"id": "0908.0145", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina and Fred L. Mannering", "title": "Empirical assessment of the impact of highway design exceptions on the\n  frequency and severity of vehicle accidents", "comments": "24 pages, 1 figure, 2 tables, the final version is available in\n  Accident Analysis and Prevention", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compliance to standardized highway design criteria is considered essential to\nensure the roadway safety. However, for a variety of reasons, situations arise\nwhere exceptions to standard-design criteria are requested and accepted after\nreview. This research explores the impact that design exceptions have on the\naccident severity and accident frequency in Indiana. Data on accidents at\nroadway sites with and without design exceptions are used to estimate\nappropriate statistical models for the frequency and severity accidents at\nthese sites using some of the most recent statistical advances with mixing\ndistributions. The results of the modeling process show that presence of\napproved design exceptions has not had a statistically significant effect on\nthe average frequency or severity of accidents -- suggesting that current\nprocedures for granting design exceptions have been sufficiently rigorous to\navoid adverse safety impacts.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2009 18:04:24 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Malyshkina", "Nataliya V.", ""], ["Mannering", "Fred L.", ""]]}, {"id": "0908.0232", "submitter": "Fabio Rapallo", "authors": "Cristiano Bocci, Enrico Carlini, Fabio Rapallo", "title": "Geometry of diagonal-effect models for contingency tables", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study several types of diagonal-effect models for two-way\ncontingency tables in the framework of Algebraic Statistics. We use both toric\nmodels and mixture models to encode the different behavior of the diagonal\ncells. We compute the invariants of these models and we explore their\ngeometrical structure.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2009 11:50:54 GMT"}], "update_date": "2009-08-04", "authors_parsed": [["Bocci", "Cristiano", ""], ["Carlini", "Enrico", ""], ["Rapallo", "Fabio", ""]]}, {"id": "0908.0547", "submitter": "Lars Hansen", "authors": "Xioahong Chen, Lars Peter Hansen and Jose Scheinkman", "title": "Nonlinear Principal Components and Long-run Implications of Multivariate\n  Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a method for extracting nonlinear principal components (NPCs).\nThese NPCs maximize variation subject to smoothness and orthogonality\nconstraints; but we allow for a general class of constraints and multivariate\nprobability densities, including densities without compact support and even\ndensities with algebraic tails. We provide primitive sufficient conditions for\nthe existence of these NPCs. By exploiting the theory of continuous-time,\nreversible Markov diffusion processes, we give a different interpretation of\nthese NPCs and the smoothness constraints. When the diffusion matrix is used to\nenforce smoothness, the NPCs maximize long-run variation relative to the\noverall variation subject to orthogonality constraints. Moreover, the NPCs\nbehave as scalar autoregressions with heteroskedastic innovations; this\nsupports semiparametric identification and estimation of a multivariate\nreversible diffusion process and tests of the overidentifying restrictions\nimplied by such a process from low frequency data. We also explore implications\nfor stationary, possibly non-reversible diffusion processes. Finally, we\nsuggest a sieve method to estimate the NPCs from discretely-sampled data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2009 21:27:28 GMT"}], "update_date": "2009-08-06", "authors_parsed": [["Chen", "Xioahong", ""], ["Hansen", "Lars Peter", ""], ["Scheinkman", "Jose", ""]]}, {"id": "0908.0618", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Functional Partial Linear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When predicting scalar responses in the situation where the explanatory\nvariables are functions, it is sometimes the case that some functional\nvariables are related to responses linearly while other variables have more\ncomplicated relationships with the responses. In this paper, we propose a new\nsemi-parametric model to take advantage of both parametric and nonparametric\nfunctional modeling. Asymptotic properties of the proposed estimators are\nestablished and finite sample behavior is investigated through a small\nsimulation experiment.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2009 13:31:05 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2012 02:26:44 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "0908.1258", "submitter": "Eric Xing", "authors": "Steve Hanneke, Wenjie Fu, and Eric Xing", "title": "Discrete Temporal Models of Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of statistical models for social network evolution over\ntime, which represents an extension of Exponential Random Graph Models (ERGMs).\nMany of the methods for ERGMs are readily adapted for these models, including\nmaximum likelihood estimation algorithms. We discuss models of this type and\ntheir properties, and give examples, as well as a demonstration of their use\nfor hypothesis testing and classification. We believe our temporal ERG models\nrepresent a useful new framework for modeling time-evolving social networks,\nand rewiring networks from other domains such as gene regulation circuitry, and\ncommunication networks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2009 22:35:14 GMT"}], "update_date": "2009-08-11", "authors_parsed": [["Hanneke", "Steve", ""], ["Fu", "Wenjie", ""], ["Xing", "Eric", ""]]}, {"id": "0908.1980", "submitter": "David Degras", "authors": "David A. Degras", "title": "Simultaneous confidence bands for nonparametric regression with\n  functional data", "comments": "Accepted at Statistica Sinica (SS-09-207)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric regression in the context of functional data, that\nis, when a random sample of functions is observed on a fine grid. We obtain a\nfunctional asymptotic normality result allowing to build simultaneous\nconfidence bands (SCB) for various estimation and inference tasks. Two\napplications to a SCB procedure for the regression function and to a\ngoodness-of-fit test for curvilinear regression models are proposed. The first\none has improved accuracy upon the other available methods while the second can\ndetect local departures from a parametric shape, as opposed to the usual\ngoodness-of-fit tests which only track global departures. A numerical study of\nthe SCB procedures and an illustration with a speech data set are provided.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2009 20:44:44 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2009 12:48:12 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2010 21:05:00 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Degras", "David A.", ""]]}, {"id": "0908.2098", "submitter": "Krzysztof Latuszynski", "authors": "Krzysztof Latuszynski, Wojciech Niemiro", "title": "Rigorous confidence bounds for MCMC under a geometric drift condition", "comments": null, "journal-ref": "Journal of Complexity, 27(1), pp. 23-38, (2011)", "doi": "10.1016/j.jco.2010.07.003", "report-no": "CRiSM research report 09", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume a drift condition towards a small set and bound the mean square\nerror of estimators obtained by taking averages along a single trajectory of a\nMarkov chain Monte Carlo algorithm. We use these bounds to construct\nfixed-width nonasymptotic confidence intervals. For a possibly unbounded\nfunction $f:\\stany \\to R,$ let $I=\\int_{\\stany} f(x) \\pi(x) dx$ be the value of\ninterest and $\\hat{I}_{t,n}=(1/n)\\sum_{i=t}^{t+n-1}f(X_i)$ its MCMC estimate.\nPrecisely, we derive lower bounds for the length of the trajectory $n$ and\nburn-in time $t$ which ensure that $$P(|\\hat{I}_{t,n}-I|\\leq \\varepsilon)\\geq\n1-\\alpha.$$ The bounds depend only and explicitly on drift parameters, on the\n$V-$norm of $f,$ where $V$ is the drift function and on precision and\nconfidence parameters $\\varepsilon, \\alpha.$ Next we analyse an MCMC estimator\nbased on the median of multiple shorter runs that allows for sharper bounds for\nthe required total simulation cost. In particular the methodology can be\napplied for computing Bayesian estimators in practically relevant models. We\nillustrate our bounds numerically in a simple example.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2009 17:01:15 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Latuszynski", "Krzysztof", ""], ["Niemiro", "Wojciech", ""]]}, {"id": "0908.2372", "submitter": "Simon Guillotte", "authors": "Simon Guillotte, Fran\\c{c}ois Perron", "title": "Bayesian estimation of a bivariate copula using the Jeffreys prior", "comments": "Published in at http://dx.doi.org/10.3150/10-BEJ345 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2012, Vol. 18, No. 2, 496-519", "doi": "10.3150/10-BEJ345", "report-no": "IMS-BEJ-BEJ345", "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bivariate distribution with continuous margins can be uniquely decomposed\nvia a copula and its marginal distributions. We consider the problem of\nestimating the copula function and adopt a Bayesian approach. On the space of\ncopula functions, we construct a finite-dimensional approximation subspace that\nis parametrized by a doubly stochastic matrix. A major problem here is the\nselection of a prior distribution on the space of doubly stochastic matrices\nalso known as the Birkhoff polytope. The main contributions of this paper are\nthe derivation of a simple formula for the Jeffreys prior and showing that it\nis proper. It is known in the literature that for a complex problem like the\none treated here, the above results are difficult to obtain. The Bayes\nestimator resulting from the Jeffreys prior is then evaluated numerically via\nMarkov chain Monte Carlo methodology. A rather extensive simulation experiment\nis carried out. In many cases, the results favour the Bayes estimator over\nfrequentist estimators such as the standard kernel estimator and Deheuvels'\nestimator in terms of mean integrated squared error.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 15:33:41 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2009 15:46:22 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2012 12:53:41 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Guillotte", "Simon", ""], ["Perron", "Fran\u00e7ois", ""]]}, {"id": "0908.2503", "submitter": "Benoit Patra", "authors": "G\\'erard Biau (LSTA, PMA), Beno\\^it Patra (LSTA)", "title": "Sequential Quantile Prediction of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a broad range of potential applications, we address the quantile\nprediction problem of real-valued time series. We present a sequential quantile\nforecasting model based on the combination of a set of elementary nearest\nneighbor-type predictors called \"experts\" and show its consistency under a\nminimum of conditions. Our approach builds on the methodology developed in\nrecent years for prediction of individual sequences and exploits the quantile\nstructure as a minimizer of the so-called pinball loss function. We perform an\nin-depth analysis of real-world data sets and show that this nonparametric\nstrategy generally outperforms standard quantile prediction methods\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2009 07:16:28 GMT"}, {"version": "v2", "created": "Mon, 31 May 2010 07:04:18 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA, PMA"], ["Patra", "Beno\u00eet", "", "LSTA"]]}, {"id": "0908.2616", "submitter": "Assaf Oron", "authors": "Assaf P. Oron, David Azriel and Peter D. Hoff", "title": "Convergence of Nonparametric Long-Memory Phase I Designs", "comments": "New version uploaded. Lemma added that proves convergence of running\n  point estimates based on martingale theory. Also simulation study added", "journal-ref": "International Journal of Biostatistics, Volume 7, Issue 1, Article\n  39, 2011", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine nonparametric dose-finding designs that use toxicity estimates\nbased on all available data at each dose allocation decision. We prove that one\nsuch design family, called here \"interval design\", converges almost surely to\nthe maximum tolerated dose (MTD), if the MTD is the only dose level whose\ntoxicity rate falls within the pre-specified interval around the desired target\nrate. Another nonparametric family, called \"point design\", has a positive\nprobability of not converging. In a numerical sensitivity study, a diverse\nsample of dose-toxicity scenarios was randomly generated. On this sample, the\n\"interval design\" convergence conditions are met far more often than the\nconditions for one-parameter design convergence (the Shen-O'Quigley\nconditions), suggesting that the interval-design conditions are less\nrestrictive. Implications of these theoretical and numerical results for\nsmall-sample behavior of the designs, and for future research, are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2009 19:41:07 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2010 14:06:10 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Oron", "Assaf P.", ""], ["Azriel", "David", ""], ["Hoff", "Peter D.", ""]]}, {"id": "0908.2724", "submitter": "David Hardoon", "authors": "David R. Hardoon, John Shawe-Taylor", "title": "Sparse Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for solving Canonical Correlation Analysis (CCA) in\na sparse convex framework using a least squares approach. The presented method\nfocuses on the scenario when one is interested in (or limited to) a primal\nrepresentation for the first view while having a dual representation for the\nsecond view. Sparse CCA (SCCA) minimises the number of features used in both\nthe primal and dual projections while maximising the correlation between the\ntwo views. The method is demonstrated on two paired corpuses of English-French\nand English-Spanish for mate-retrieval. We are able to observe, in the\nmate-retreival, that when the number of the original features is large SCCA\noutperforms Kernel CCA (KCCA), learning the common semantic space from a sparse\nset of features.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2009 11:14:51 GMT"}], "update_date": "2009-08-20", "authors_parsed": [["Hardoon", "David R.", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "0908.2794", "submitter": "Jesus Enrique Garcia", "authors": "Jesus E. Garcia and Veronica A. Gonzalez-Lopez", "title": "A nonparametric independence test using random permutations", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new nonparametric test for the supposition of independence\nbetween two continuous random variables. The test is based on the size of the\nlongest increasing subsequence of a random permutation. We identified the\nindependence assumption between the two continuous variables with the space of\npermutation equipped with the uniform distribution and we show the exact\ndistribution of the statistic. We calculate the distribution for several sample\nsizes. Through a simulation study we estimate the power of our test for diverse\nalternative hypothesis under the null hypothesis of independence.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2009 18:07:29 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2009 00:46:49 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Garcia", "Jesus E.", ""], ["Gonzalez-Lopez", "Veronica A.", ""]]}, {"id": "0908.2804", "submitter": "Moritz Heene Dr.", "authors": "Peter H. Schonemann, Moritz Heene", "title": "Predictive validities: figures of merit or veils of deception?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ETS has recently released new estimates of validities of the GRE for\npredicting cumulative graduate GPA. They average in the middle thirties - twice\nas high as those previously reported by a number of independent investigators.\nIt is shown in the first part of this paper that this unexpected finding can be\ntraced to a flawed methodology that tends to inflate multiple correlation\nestimates, especially those of populations values near zero. Secondly, the\nissue of upward corrections of validity estimates for restriction of range is\ntaken up. It is shown that they depend on assumptions that are rarely met by\nthe data. Finally, it is argued more generally that conventional test theory,\nwhich is couched in terms of correlations and variances, is not only\nunnecessarily abstract but, more importantly, incomplete, since the practical\nutility of a test does not only depend on its validity, but also on base-rates\nand admission quotas. A more direct and conclusive method for gauging the\nutility of a test involves misclassification rates, and entirely dispenses with\nquestionable assumptions and post-hoc \"corrections\". On applying this approach\nto the GRE, it emerges (1) that the GRE discriminates against ethnic and\neconomic minorities, and (2) that it often produces more erroneous decisions\nthan a purely random admissions policy would.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2009 19:55:12 GMT"}], "update_date": "2009-08-20", "authors_parsed": [["Schonemann", "Peter H.", ""], ["Heene", "Moritz", ""]]}, {"id": "0908.2954", "submitter": "Swarnendu  Kar", "authors": "Swarnendu Kar, Kishan G. Mehrotra, Pramod K. Varshney", "title": "Approximation of Average Run Length of Moving Sum Algorithms Using\n  Multivariate Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the various procedures used to detect potential changes in a stochastic\nprocess the moving sum algorithms are very popular due to their intuitive\nappeal and good statistical performance. One of the important design parameters\nof a change detection algorithm is the expected interval between false\npositives, also known as the average run length (ARL). Computation of the ARL\nusually involves numerical procedures but in some cases it can be approximated\nusing a series involving multivariate probabilities. In this paper, we present\nan analysis of this series approach by providing sufficient conditions for\nconvergence and derive an error bound. Using simulation studies, we show that\nthe series approach is applicable to moving average and filtered derivative\nalgorithms. For moving average algorithms, we compare our results with\npreviously known bounds. We use two special cases to illustrate our\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2009 16:15:08 GMT"}], "update_date": "2009-08-21", "authors_parsed": [["Kar", "Swarnendu", ""], ["Mehrotra", "Kishan G.", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "0908.3163", "submitter": "Johannes Schmidt-Hieber", "authors": "Axel Munk and Johannes Schmidt-Hieber", "title": "Nonparametric estimation of the volatility function in a high-frequency\n  model corrupted by noise", "comments": "5 figures, corrected references, minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the models Y_{i,n}=\\int_0^{i/n}\n\\sigma(s)dW_s+\\tau(i/n)\\epsilon_{i,n}, and \\tilde\nY_{i,n}=\\sigma(i/n)W_{i/n}+\\tau(i/n)\\epsilon_{i,n}, i=1,...,n, where W_t\ndenotes a standard Brownian motion and \\epsilon_{i,n} are centered i.i.d.\nrandom variables with E(\\epsilon_{i,n}^2)=1 and finite fourth moment.\nFurthermore, \\sigma and \\tau are unknown deterministic functions and W_t and\n(\\epsilon_{1,n},...,\\epsilon_{n,n}) are assumed to be independent processes.\nBased on a spectral decomposition of the covariance structures we derive series\nestimators for \\sigma^2 and \\tau^2 and investigate their rate of convergence of\nthe MISE in dependence of their smoothness. To this end specific basis\nfunctions and their corresponding Sobolev ellipsoids are introduced and we show\nthat our estimators are optimal in minimax sense. Our work is motivated by\nmicrostructure noise models. Our major finding is that the microstructure noise\n\\epsilon_{i,n} introduces an additionally degree of ill-posedness of 1/2;\nirrespectively of the tail behavior of \\epsilon_{i,n}. The method is\nillustrated by a small numerical study.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2009 16:26:40 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2009 18:25:26 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2010 15:17:58 GMT"}], "update_date": "2010-04-07", "authors_parsed": [["Munk", "Axel", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "0908.3856", "submitter": "Simone Pigolotti", "authors": "Alberto Bernacchia, Simone Pigolotti", "title": "Self-consistent method for density estimation", "comments": "21 pages, 5 figures", "journal-ref": "Journal of the Royal Statistical Society: Series B, Volume 73(3)\n  pp 407-422, 2011", "doi": "10.1111/j.1467-9868.2011.00772.x", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of a density profile from experimental data points is a\nchallenging problem, usually tackled by plotting a histogram. Prior assumptions\non the nature of the density, from its smoothness to the specification of its\nform, allow the design of more accurate estimation procedures, such as Maximum\nLikelihood. Our aim is to construct a procedure that makes no explicit\nassumptions, but still providing an accurate estimate of the density. We\nintroduce the self-consistent estimate: the power spectrum of a candidate\ndensity is given, and an estimation procedure is constructed on the assumption,\nto be released \\emph{a posteriori}, that the candidate is correct. The\nself-consistent estimate is defined as a prior candidate density that precisely\nreproduces itself. Our main result is to derive the exact expression of the\nself-consistent estimate for any given dataset, and to study its properties.\nApplications of the method require neither priors on the form of the density\nnor the subjective choice of parameters. A cutoff frequency, akin to a bin size\nor a kernel bandwidth, emerges naturally from the derivation. We apply the\nself-consistent estimate to artificial data generated from various\ndistributions and show that it reaches the theoretical limit for the scaling of\nthe square error with the dataset size.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2009 16:23:07 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2009 08:31:40 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2010 18:28:25 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2010 16:53:58 GMT"}, {"version": "v5", "created": "Thu, 29 Jul 2010 16:23:07 GMT"}, {"version": "v6", "created": "Mon, 13 Dec 2010 13:20:04 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Bernacchia", "Alberto", ""], ["Pigolotti", "Simone", ""]]}, {"id": "0908.3882", "submitter": "Li Hsu Dr", "authors": "Pei Wang, Dennis L. Chao, Li Hsu", "title": "Learning networks from high dimensional binary data: An application to\n  genomic instability data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic instability, the propensity of aberrations in chromosomes, plays a\ncritical role in the development of many diseases. High throughput genotyping\nexperiments have been performed to study genomic instability in diseases. The\noutput of such experiments can be summarized as high dimensional binary\nvectors, where each binary variable records aberration status at one marker\nlocus. It is of keen interest to understand how these aberrations interact with\neach other. In this paper, we propose a novel method, \\texttt{LogitNet}, to\ninfer the interactions among aberration events. The method is based on\npenalized logistic regression with an extension to account for spatial\ncorrelation in the genomic instability data. We conduct extensive simulation\nstudies and show that the proposed method performs well in the situations\nconsidered. Finally, we illustrate the method using genomic instability data\nfrom breast cancer samples.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2009 18:57:48 GMT"}], "update_date": "2009-08-27", "authors_parsed": [["Wang", "Pei", ""], ["Chao", "Dennis L.", ""], ["Hsu", "Li", ""]]}, {"id": "0908.4262", "submitter": "George Moustakides", "authors": "Georgios Fellouris and George V. Moustakides", "title": "Decentralized Sequential Hypothesis Testing using Asynchronous\n  Communication", "comments": "13 double column pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a test for the problem of decentralized sequential hypothesis\ntesting, which is asymptotically optimum. By selecting a suitable sampling\nmechanism at each sensor, communication between sensors and fusion center is\nasynchronous and limited to 1-bit data. The proposed SPRT-like test turns out\nto be order-2 asymptotically optimum in the case of continuous time and\ncontinuous path signals, while in discrete time this strong asymptotic\noptimality property is preserved under proper conditions. If these conditions\ndo not hold, then we can show optimality of order-1. Simulations corroborate\nthe excellent performance characteristics of the test of interest.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2009 19:34:24 GMT"}], "update_date": "2009-08-31", "authors_parsed": [["Fellouris", "Georgios", ""], ["Moustakides", "George V.", ""]]}, {"id": "0908.4461", "submitter": "Hisayuki Hara", "authors": "Hisayuki Hara and Akimichi Takemura", "title": "Connecting tables with zero-one entries by a subset of a Markov basis", "comments": "18 pages", "journal-ref": "in Algebraic Methods in Statistics and Probability II,\n  Contemporary Mathematics, vol. 516, Amer. Math. Soc., Providence, RI, pp.\n  199-213, 2010", "doi": null, "report-no": null, "categories": "math.ST math.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss connecting tables with zero-one entries by a subset of a Markov\nbasis. In this paper, as a Markov basis we consider the Graver basis, which\ncorresponds to the unique minimal Markov basis for the Lawrence lifting of the\noriginal configuration. Since the Graver basis tends to be large, it is of\ninterest to clarify conditions such that a subset of the Graver basis, in\nparticular a minimal Markov basis itself, connects tables with zero-one\nentries. We give some theoretical results on the connectivity of tables with\nzero-one entries. We also study some common models, where a minimal Markov\nbasis for tables without the zero-one restriction does not connect tables with\nzero-one entries.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2009 06:25:21 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2009 01:27:25 GMT"}], "update_date": "2010-07-22", "authors_parsed": [["Hara", "Hisayuki", ""], ["Takemura", "Akimichi", ""]]}, {"id": "0908.4489", "submitter": "Nicolas Dobigeon", "authors": "Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Bayesian orthogonal component analysis for sparse representation", "comments": "Revised version. Accepted to IEEE Trans. Signal Processing", "journal-ref": "IEEE Trans. Signal Processing, vol. 58. no. 5, pp. 2675-2685, May\n  2010", "doi": "10.1109/TSP.2010.2041594", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of identifying a lower dimensional space\nwhere observed data can be sparsely represented. This under-complete dictionary\nlearning task can be formulated as a blind separation problem of sparse sources\nlinearly mixed with an unknown orthogonal mixing matrix. This issue is\nformulated in a Bayesian framework. First, the unknown sparse sources are\nmodeled as Bernoulli-Gaussian processes. To promote sparsity, a weighted\nmixture of an atom at zero and a Gaussian distribution is proposed as prior\ndistribution for the unobserved sources. A non-informative prior distribution\ndefined on an appropriate Stiefel manifold is elected for the mixing matrix.\nThe Bayesian inference on the unknown parameters is conducted using a Markov\nchain Monte Carlo (MCMC) method. A partially collapsed Gibbs sampler is\ndesigned to generate samples asymptotically distributed according to the joint\nposterior distribution of the unknown model parameters and hyperparameters.\nThese samples are then used to approximate the joint maximum a posteriori\nestimator of the sources and mixing matrix. Simulations conducted on synthetic\ndata are reported to illustrate the performance of the method for recovering\nsparse representations. An application to sparse coding on under-complete\ndictionary is finally investigated.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2009 09:44:06 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2009 21:33:07 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2010 17:46:54 GMT"}], "update_date": "2010-08-30", "authors_parsed": [["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}]