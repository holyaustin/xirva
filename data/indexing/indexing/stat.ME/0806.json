[{"id": "0806.0145", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen, Bin Yu", "title": "Lasso-type recovery of sparse representations for high-dimensional data", "comments": "Published in at http://dx.doi.org/10.1214/07-AOS582 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2009, Vol. 37, No. 1, 246-270", "doi": "10.1214/07-AOS582", "report-no": "IMS-AOS-AOS582", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasso is an attractive technique for regularization and variable\nselection for high-dimensional data, where the number of predictor variables\n$p_n$ is potentially much larger than the number of samples $n$. However, it\nwas recently discovered that the sparsity pattern of the Lasso estimator can\nonly be asymptotically identical to the true sparsity pattern if the design\nmatrix satisfies the so-called irrepresentable condition. The latter condition\ncan easily be violated in the presence of highly correlated variables. Here we\nexamine the behavior of the Lasso estimators if the irrepresentable condition\nis relaxed. Even though the Lasso cannot recover the correct sparsity pattern,\nwe show that the estimator is still consistent in the $\\ell_2$-norm sense for\nfixed designs under conditions on (a) the number $s_n$ of nonzero components of\nthe vector $\\beta_n$ and (b) the minimal singular values of design matrices\nthat are induced by selecting small subsets of variables. Furthermore, a rate\nof convergence result is obtained on the $\\ell_2$ error with an appropriate\nchoice of the smoothing parameter. The rate is shown to be optimal under the\ncondition of bounded maximal and minimal sparse eigenvalues. Our results imply\nthat, with high probability, all important variables are selected. The set of\nselected variables is a meaningful reduction on the original set of variables.\nFinally, our results are illustrated with the detection of closely adjacent\nfrequencies, a problem encountered in astrophysics.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2008 13:46:54 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2009 14:59:22 GMT"}], "update_date": "2009-03-02", "authors_parsed": [["Meinshausen", "Nicolai", ""], ["Yu", "Bin", ""]]}, {"id": "0806.0533", "submitter": "Herv\\'e Cardot", "authors": "Herve Cardot, Jan Johannes", "title": "Thresholding Projection Estimators in Functional Linear Models", "comments": "Revised version for J. of Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2009.03.001", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the regression function in functional\nlinear regression models by proposing a new type of projection estimators which\ncombine dimension reduction and thresholding. The introduction of a threshold\nrule allows to get consistency under broad assumptions as well as minimax rates\nof convergence under additional regularity hypotheses. We also consider the\nparticular case of Sobolev spaces generated by the trigonometric basis which\npermits to get easily mean squared error of prediction as well as estimators of\nthe derivatives of the regression function. We prove these estimators are\nminimax and rates of convergence are given for some particular cases.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2008 13:05:10 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2008 10:20:34 GMT"}], "update_date": "2009-12-19", "authors_parsed": [["Cardot", "Herve", ""], ["Johannes", "Jan", ""]]}, {"id": "0806.0582", "submitter": "Georgina Flesia MS", "authors": "O.H. Bustos, A.G. Flesia, A.C. Frery and M.M. Lucini", "title": "Sampling Spatially Correlated Clutter", "comments": "First draft", "journal-ref": "Communications in Statistics - Simulation and Computation, 2009,\n  2134--2151", "doi": "10.1080/03610910903249536", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated ${\\cal G}$ distributions can be used to describe the clutter seen\nin images obtained with coherent illumination, as is the case of B-scan\nultrasound, laser, sonar and synthetic aperture radar (SAR) imagery. These\ndistributions are derived using the square root of the generalized inverse\nGaussian distribution for the amplitude backscatter within the multiplicative\nmodel. A two-parameters particular case of the amplitude ${\\mathcal G}$\ndistribution, called ${\\mathcal G}_{A}^{0}$, constitutes a modeling improvement\nwith respect to the widespread ${\\mathcal K}_{A}$ distribution when fitting\nurban, forested and deforested areas in remote sensing data. This article deals\nwith the modeling and the simulation of correlated ${\\mathcal\nG}_{A}^{0}$-distributed random fields. It is accomplished by means of the\nInverse Transform method, applied to Gaussian random fields with spatial\ncorrelation. The main feature of this approach is its generality, since it\nallows the introduction of negative correlation values in the resulting\nprocess, necessary for the proper explanation of the shadowing effect in many\nSAR images.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2008 16:36:49 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Bustos", "O. H.", ""], ["Flesia", "A. G.", ""], ["Frery", "A. C.", ""], ["Lucini", "M. M.", ""]]}, {"id": "0806.0612", "submitter": "Juhyun Park", "authors": "Juhyun Park and Burkhardt Seifert", "title": "Local additive estimation", "comments": "34 pages, 5 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive models are popular in high--dimensional regression problems because\nof flexibility in model building and optimality in additive function\nestimation. Moreover, they do not suffer from the so-called {\\it curse of\ndimensionality} generally arising in nonparametric regression setting. Less\nknown is the model bias incurring from the restriction to the additive class of\nmodels. We introduce a new class of estimators that reduces additive model bias\nand at the same time preserves some stability of the additive estimator. This\nestimator is shown to partially relieve the dimensionality problem as well. The\nnew estimator is constructed by localizing the assumption of additivity and\nthus named {\\it local additive estimator}. Implementation can be easily made\nwith any standard software for additive regression. For detailed analysis we\nexplicitly use the smooth backfitting estimator by Mammen, Linton and Nielsen\n(1999).\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2008 19:17:17 GMT"}], "update_date": "2008-06-04", "authors_parsed": [["Park", "Juhyun", ""], ["Seifert", "Burkhardt", ""]]}, {"id": "0806.0899", "submitter": "Victor Patrangenaru", "authors": "V. Patrangenaru, X. Liu, S. Sugathadasa", "title": "A Nonparametric Approach to 3D Shape Analysis from Digital Camera Images\n  - I. in Memory of W.P. Dayawansa", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, for the first time, one develops a nonparametric methodology\nfor an analysis of shapes of configurations of landmarks on real 3D objects\nfrom regular camera photographs, thus making 3D shape analysis very accessible.\nA fundamental result in computer vision by Faugeras (1992), Hartley, Gupta and\nChang (1992) is that generically, a finite 3D configuration of points can be\nretrieved up to a projective transformation, from corresponding configurations\nin a pair of camera images. Consequently, the projective shape of a 3D\nconfiguration can be retrieved from two of its planar views. Given the inherent\nregistration errors, the 3D projective shape can be estimated from a sample of\nphotos of the scene containing that configuration. Projective shapes are here\nregarded as points on projective shape manifolds. Using large sample and\nnonparametric bootstrap methodology for extrinsic means on manifolds, one gives\nconfidence regions and tests for the mean projective shape of a 3D\nconfiguration from its 2D camera images.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2008 04:26:49 GMT"}], "update_date": "2008-06-14", "authors_parsed": [["Patrangenaru", "V.", ""], ["Liu", "X.", ""], ["Sugathadasa", "S.", ""]]}, {"id": "0806.1473", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan, Can Ceritoglu, M. Faisal Beg, Lei Wang, John C. Morris,\n  John G. Csernansky, Michael I. Miller, John Tilak Ratnanather", "title": "Analysis of Metric Distances and Volumes of Hippocampi Indicates\n  Different Morphometric Changes over Time in Dementia of Alzheimer Type and\n  Nondemented Subjects", "comments": null, "journal-ref": null, "doi": null, "report-no": "KU-EC-08-3", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we analyze the morphometry of hippocampus in subjects with\nvery mild dementia of Alzheimer's type (DAT) and nondemented controls and how\nit changes over a two-year period. Morphometric differences with respect to a\ntemplate hippocampus were measured by the metric distance obtained from the\nLarge Deformation Diffeomorphic Metric Mapping (LDDMM) algorithm which was\npreviously used to calculate dense one-to-one correspondence vector fields\nbetween the shapes. LDDMM assigns metric distances on the space of anatomical\nimages thereby allowing for the direct comparison and quantization of\nmorphometric changes. We use various statistical methods to compare the metric\ndistances in a cross-sectional and longitudinal manner. At baseline, the metric\ndistances for demented subjects are found not to be significantly different\nfrom those for nondemented subjects. At follow-up, the metric distances for\ndemented subjects were significantly larger compared to nondemented subjects.\nThe metric distances for demented subjects increased significantly from\nbaseline to follow-up but not for nondemented subjects. We also use the metric\ndistances in logistic regression for diagnostic discrimination of subjects. We\ncompare metric distances with the volumes and obtain similar results. In\nclassification, the model that uses volume, metric distance, and volume loss\nover time together performs better in detecting DAT. Thus, metric distances\nwith respect to a template computed via LDDMM can be a powerful tool in\ndetecting differences in shape in cross-sectional as well as longitudinal\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2008 15:09:39 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2008 18:14:26 GMT"}], "update_date": "2008-08-14", "authors_parsed": [["Ceyhan", "Elvan", ""], ["Ceritoglu", "Can", ""], ["Beg", "M. Faisal", ""], ["Wang", "Lei", ""], ["Morris", "John C.", ""], ["Csernansky", "John G.", ""], ["Miller", "Michael I.", ""], ["Ratnanather", "John Tilak", ""]]}, {"id": "0806.1578", "submitter": "Jiancheng Jiang", "authors": "Jiancheng Jiang, J. S. Marron", "title": "SiZer for Censored Density and Hazard Estimation", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2008_246", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SiZer method is extended to nonparametric hazard estimation and also to\ncensored density and hazard estimation. The new method allows quick, visual\nstatistical inference about the important issue of statistically significant\nincreases and decreases in the smooth curve estimate. This extension has\nrequired the opening of a new avenue of research on the interface between\nstatistical inference and scale space.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2008 06:08:18 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Jiang", "Jiancheng", ""], ["Marron", "J. S.", ""]]}, {"id": "0806.1652", "submitter": "Ulrike Schneider", "authors": "Benedikt M. P\\\"otscher, Ulrike Schneider", "title": "Confidence Sets Based on Penalized Maximum Likelihood Estimators in\n  Gaussian Regression", "comments": "second revision: new title, some comments added, proofs moved to\n  appendix", "journal-ref": "Electron. J. Statist. 4 (2010), 334-360", "doi": "10.1214/09-EJS523", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence intervals based on penalized maximum likelihood estimators such as\nthe LASSO, adaptive LASSO, and hard-thresholding are analyzed. In the\nknown-variance case, the finite-sample coverage properties of such intervals\nare determined and it is shown that symmetric intervals are the shortest. The\nlength of the shortest intervals based on the hard-thresholding estimator is\nlarger than the length of the shortest interval based on the adaptive LASSO,\nwhich is larger than the length of the shortest interval based on the LASSO,\nwhich in turn is larger than the standard interval based on the maximum\nlikelihood estimator. In the case where the penalized estimators are tuned to\npossess the `sparsity property', the intervals based on these estimators are\nlarger than the standard interval by an order of magnitude. Furthermore, a\nsimple asymptotic confidence interval construction in the `sparse' case, that\nalso applies to the smoothly clipped absolute deviation estimator, is\ndiscussed. The results for the known-variance case are shown to carry over to\nthe unknown-variance case in an appropriate asymptotic sense.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2008 13:31:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2009 13:47:06 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2010 17:33:42 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Schneider", "Ulrike", ""]]}, {"id": "0806.2208", "submitter": "Artur Lemonte", "authors": "Artur J. Lemonte, Silvia L. P. Ferrari and Francisco Cribari-Neto", "title": "Improved Likelihood Inference in Birnbaum-Saunders Regressions", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": "10.1016/j.csda.2009.11.017", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Birnbaum-Saunders regression model is commonly used in reliability\nstudies. We address the issue of performing inference in this class of models\nwhen the number of observations is small. We show that the likelihood ratio\ntest tends to be liberal when the sample size is small, and we obtain a\ncorrection factor which reduces the size distortion of the test. The correction\nmakes the error rate of he test vanish faster as the sample size increases. The\nnumerical results show that the modified test is more reliable in finite\nsamples than the usual likelihood ratio test. We also present an empirical\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2008 15:02:34 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2009 16:53:24 GMT"}], "update_date": "2009-11-25", "authors_parsed": [["Lemonte", "Artur J.", ""], ["Ferrari", "Silvia L. P.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "0806.2424", "submitter": "Kostas Alexandridis", "authors": "Kostas Alexandridis, Bryan C. Pijanowski", "title": "Developing Bayesian Information Entropy-based Techniques for Spatially\n  Explicit Model Assessment", "comments": "13 pages, 10 figures, 3 tables, 25 equations Submitted to IEEE Trans\n  Inf Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to explore and develop advanced spatial Bayesian\nassessment methods and techniques for land use modeling. The paper provides a\ncomprehensive guide for assessing additional informational entropy value of\nmodel predictions at the spatially explicit domain of knowledge, and proposes a\nfew alternative metrics and indicators for extracting higher-order information\ndynamics from simulation tournaments. A seven-county study area in\nSouth-Eastern Wisconsin (SEWI) has been used to simulate and assess the\naccuracy of historical land use changes (1963-1990) using artificial neural\nnetwork simulations of the Land Transformation Model (LTM). The use of the\nanalysis and the performance of the metrics helps: (a) understand and learn how\nwell the model runs fits to different combinations of presence and absence of\ntransitions in a landscape, not simply how well the model fits our given data;\n(b) derive (estimate) a theoretical accuracy that we would expect a model to\nassess under the presence of incomplete information and measurement; (c)\nunderstand the spatially explicit role and patterns of uncertainty in\nsimulations and model estimations, by comparing results across simulation runs;\n(d) compare the significance or estimation contribution of transitional\npresence and absence (change versus no change) to model performance, and the\ncontribution of the spatial drivers and variables to the explanatory value of\nour model; and (e) compare measurements of informational uncertainty at\ndifferent scales of spatial resolution.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2008 07:38:03 GMT"}], "update_date": "2008-06-17", "authors_parsed": [["Alexandridis", "Kostas", ""], ["Pijanowski", "Bryan C.", ""]]}, {"id": "0806.3248", "submitter": "Grigorios Pavliotis", "authors": "A.Papavasiliou, G.A. Pavliotis, A.M. Stuart", "title": "Maximum Likelihood Drift Estimation for Multiscale Diffusions", "comments": "36 pages, submitted to Stochastic Processes and Their Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of parameter estimation using maximum likelihood for\nfast/slow systems of stochastic differential equations. Our aim is to shed\nlight on the problem of model/data mismatch at small scales. We consider two\nclasses of fast/slow problems for which a closed coarse-grained equation for\nthe slow variables can be rigorously derived, which we refer to as averaging\nand homogenization problems. We ask whether, given data from the slow variable\nin the fast/slow system, we can correctly estimate parameters in the drift of\nthe coarse-grained equation for the slow variable, using maximum likelihood. We\nshow that, whereas the maximum likelihood estimator is asymptotically unbiased\nfor the averaging problem, for the homogenization problem maximum likelihood\nfails unless we subsample the data at an appropriate rate. An explicit formula\nfor the asymptotic error in the log likelihood function is presented. Our\ntheory is applied to two simple examples from molecular dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2008 17:28:16 GMT"}], "update_date": "2008-06-20", "authors_parsed": [["Papavasiliou", "A.", ""], ["Pavliotis", "G. A.", ""], ["Stuart", "A. M.", ""]]}, {"id": "0806.3286", "submitter": "Hugh A. Chipman", "authors": "Hugh A. Chipman, Edward I. George, Robert E. McCulloch", "title": "BART: Bayesian additive regression trees", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS285 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 1, 266-298", "doi": "10.1214/09-AOAS285", "report-no": "IMS-AOAS-AOAS285", "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian \"sum-of-trees\" model where each tree is constrained by\na regularization prior to be a weak learner, and fitting and inference are\naccomplished via an iterative Bayesian backfitting MCMC algorithm that\ngenerates samples from a posterior. Effectively, BART is a nonparametric\nBayesian regression approach which uses dimensionally adaptive random basis\nelements. Motivated by ensemble methods in general, and boosting algorithms in\nparticular, BART is defined by a statistical model: a prior and a likelihood.\nThis approach enables full posterior inference including point and interval\nestimates of the unknown regression function as well as the marginal effects of\npotential predictors. By keeping track of predictor inclusion frequencies, BART\ncan also be used for model-free variable selection. BART's many features are\nillustrated with a bake-off against competing methods on 42 different data\nsets, with a simulation experiment and on a drug discovery classification\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2008 21:00:03 GMT"}, {"version": "v2", "created": "Thu, 7 Oct 2010 09:21:37 GMT"}], "update_date": "2010-10-08", "authors_parsed": [["Chipman", "Hugh A.", ""], ["George", "Edward I.", ""], ["McCulloch", "Robert E.", ""]]}, {"id": "0806.3769", "submitter": "Tatiane Melo", "authors": "Tatiane F.N. Melo, Silvia L.P. Ferrari and Francisco Cribari-Neto", "title": "Improved testing inference in mixed linear models", "comments": "17 pages, 1 figure", "journal-ref": "Computational Statistics and Data Analysis, 53, (2009), 2573-2582", "doi": "10.1016/j.csda.2008.12.007", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed linear models are commonly used in repeated measures studies. They\naccount for the dependence amongst observations obtained from the same\nexperimental unit. Oftentimes, the number of observations is small, and it is\nthus important to use inference strategies that incorporate small sample\ncorrections. In this paper, we develop modified versions of the likelihood\nratio test for fixed effects inference in mixed linear models. In particular,\nwe derive a Bartlett correction to such a test and also to a test obtained from\na modified profile likelihood function. Our results generalize those in Zucker\net al. (Journal of the Royal Statistical Society B, 2000, 62, 827-838) by\nallowing the parameter of interest to be vector-valued. Additionally, our\nBartlett corrections allow for random effects nonlinear covariance matrix\nstructure. We report numerical evidence which shows that the proposed tests\ndisplay superior finite sample behavior relative to the standard likelihood\nratio test. An application is also presented and discussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2008 20:45:09 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2011 14:39:10 GMT"}], "update_date": "2011-08-05", "authors_parsed": [["Melo", "Tatiane F. N.", ""], ["Ferrari", "Silvia L. P.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "0806.3978", "submitter": "Vincent Vu", "authors": "Vincent Q. Vu, Bin Yu, Robert E. Kass", "title": "Information In The Non-Stationary Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information estimates such as the ``direct method'' of Strong et al. (1998)\nsidestep the difficult problem of estimating the joint distribution of response\nand stimulus by instead estimating the difference between the marginal and\nconditional entropies of the response. While this is an effective estimation\nstrategy, it tempts the practitioner to ignore the role of the stimulus and the\nmeaning of mutual information. We show here that, as the number of trials\nincreases indefinitely, the direct (or ``plug-in'') estimate of marginal\nentropy converges (with probability 1) to the entropy of the time-averaged\nconditional distribution of the response, and the direct estimate of the\nconditional entropy converges to the time-averaged entropy of the conditional\ndistribution of the response. Under joint stationarity and ergodicity of the\nresponse and stimulus, the difference of these quantities converges to the\nmutual information. When the stimulus is deterministic or non-stationary the\ndirect estimate of information no longer estimates mutual information, which is\nno longer meaningful, but it remains a measure of variability of the response\ndistribution across time.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2008 20:13:08 GMT"}, {"version": "v2", "created": "Fri, 18 Jul 2008 23:29:00 GMT"}], "update_date": "2008-07-19", "authors_parsed": [["Vu", "Vincent Q.", ""], ["Yu", "Bin", ""], ["Kass", "Robert E.", ""]]}, {"id": "0806.4730", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Ivan Fernandez-Val, Alfred Galichon", "title": "Improving Point and Interval Estimates of Monotone Functions by\n  Rearrangement", "comments": "24 pages, 4 figures, 3 tables", "journal-ref": "Biometrika (2009) 96 (3): 559-575", "doi": "10.1093/biomet/asp030", "report-no": null, "categories": "math.ST econ.EM math.FA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a target function is monotonic, namely, weakly increasing, and\nan available original estimate of this target function is not weakly\nincreasing. Rearrangements, univariate and multivariate, transform the original\nestimate to a monotonic estimate that always lies closer in common metrics to\nthe target function. Furthermore, suppose an original simultaneous confidence\ninterval, which covers the target function with probability at least\n$1-\\alpha$, is defined by an upper and lower end-point functions that are not\nweakly increasing. Then the rearranged confidence interval, defined by the\nrearranged upper and lower end-point functions, is shorter in length in common\nnorms than the original interval and also covers the target function with\nprobability at least $1-\\alpha$. We demonstrate the utility of the improved\npoint and interval estimates with an age-height growth chart example.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2008 00:49:46 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2008 13:08:58 GMT"}, {"version": "v3", "created": "Fri, 7 Nov 2008 15:40:51 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fernandez-Val", "Ivan", ""], ["Galichon", "Alfred", ""]]}]