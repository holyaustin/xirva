[{"id": "1411.0031", "submitter": "Jason Xu", "authors": "Jason Xu, Peter Guttorp, Midori Kato-Maeda, Vladimir N. Minin", "title": "Likelihood-Based Inference for Discretely Observed Birth-Death-Shift\n  Processes, with Applications to Evolution of Mobile Genetic Elements", "comments": "31 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time birth-death-shift (BDS) processes are frequently used in\nstochastic modeling, with many applications in ecology and epidemiology. In\nparticular, such processes can model evolutionary dynamics of transposable\nelements - important genetic markers in molecular epidemiology. Estimation of\nthe effects of individual covariates on the birth, death, and shift rates of\nthe process can be accomplished by analyzing patient data, but inferring these\nrates in a discretely and unevenly observed setting presents computational\nchallenges. We propose a mutli-type branching process approximation to BDS\nprocesses and develop a corresponding expectation maximization (EM) algorithm,\nwhere we use spectral techniques to reduce calculation of expected sufficient\nstatistics to low dimensional integration. These techniques yield an efficient\nand robust optimization routine for inferring the rates of the BDS process, and\napply more broadly to multi-type branching processes where rates can depend on\nmany covariates. After rigorously testing our methodology in simulation\nstudies, we apply our method to study intrapatient time evolution of IS6110\ntransposable element, a frequently used element during estimation of\nepidemiological clusters of Mycobacterium tuberculosis infections.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 21:46:04 GMT"}, {"version": "v2", "created": "Sat, 29 Nov 2014 23:02:50 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Xu", "Jason", ""], ["Guttorp", "Peter", ""], ["Kato-Maeda", "Midori", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1411.0045", "submitter": "Elizabeth Holmes", "authors": "Elizabeth Eli Holmes", "title": "Computation of Standardized Residuals for MARSS Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This report shows how to compute the variance of the joint conditional model\nand state residuals for multivariate autoregressive Gaussian state-space\n(MARSS) models. The MARSS model can be written: x(t)=Bx(t-1)+u+w(t),\ny(t)=Zx(t)+a+v(t), where w(t) and v(t) are multivariate normal error-terms with\nvariance-covariance matrices Q and R respectively. The joint conditional\nresiduals are the w(t) and v(t) conditioned on a set of, possibly incomplete,\ndata y. Harvey, Koopman and Penzer (1998) show a recursive algorithm for these\nresiduals. I show the equation for the residuals using the conditional\nvariances of the states and the conditional covariance between unobserved data\nand states. This allows one to compute the variance of un-observed residuals,\nwhich could be useful for leave-one-out cross-validation tests. I also show how\nto modify the Harvey et al algorithm in the case of missing values and how to\nmodify it to return the non-normalized conditional residuals.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 23:47:15 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 23:57:54 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Holmes", "Elizabeth Eli", ""]]}, {"id": "1411.0393", "submitter": "Hohsuk Noh", "authors": "Natalie Neumeyer, Hohsuk Noh and Ingrid Van Keilegom", "title": "Heteroscedastic semiparametric transformation models: estimation and\n  testing for validity", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a heteroscedastic transformation model, where the\ntransformation belongs to a parametric family of monotone transformations, the\nregression and variance function are modelled nonparametrically and the error\nis independent of the multidimensional covariates. In this model, we first\nconsider the estimation of the unknown components of the model, namely the\ntransformation parameter, regression and variance function and the distribution\nof the error. We show the asymptotic normality of the proposed estimators.\nSecond, we propose tests for the validity of the model, and establish the\nlimiting distribution of the test statistics under the null hypothesis. A\nbootstrap procedure is proposed to approximate the critical values of the\ntests. Finally, we carry out a simulation study to verify the small sample\nbehavior of the proposed estimators and tests.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 08:48:37 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 02:18:07 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Neumeyer", "Natalie", ""], ["Noh", "Hohsuk", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1411.0414", "submitter": "Johan Segers", "authors": "Anna Kiriliouk, Johan Segers, Michal Warchol", "title": "Nonparametric estimation of extremal dependence", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest to understand the dependence structure of a\nrandom vector not only in the center of its distribution but also in the tails.\nExtreme-value theory tackles the problem of modelling the joint tail of a\nmultivariate distribution by modelling the marginal distributions and the\ndependence structure separately. For estimating dependence at high levels, the\nstable tail dependence function and the spectral measure are particularly\nconvenient. These objects also lie at the basis of nonparametric techniques for\nmodelling the dependence among extremes in the max-domain of attraction\nsetting. In case of asymptotic independence, this setting is inadequate, and\nmore refined tail dependence coefficients exist, serving, among others, to\ndiscriminate between asymptotic dependence and independence. Throughout, the\nmethods are illustrated on financial data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 10:16:12 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Kiriliouk", "Anna", ""], ["Segers", "Johan", ""], ["Warchol", "Michal", ""]]}, {"id": "1411.0433", "submitter": "Beatriz Pateiro-L\\'opez", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, G\\'abor Lugosi and Beatriz\n  Pateiro-L\\'opez", "title": "Set estimation from reflected Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating a compact set $S\\subset \\mathbb{R}^d$ from\na trajectory of a reflected Brownian motion in $S$ with reflections on the\nboundary of $S$. We establish consistency and rates of convergence for various\nestimators of $S$ and its boundary. This problem has relevant applications in\necology in estimating the home range of an animal based on tracking data. There\nare a variety of studies on the habitat of animals that employ the notion of\nhome range. This paper offers theoretical foundations for a new methodology\nthat, under fairly unrestrictive shape assumptions, allows one to find flexible\nregions close to reality. The theoretical findings are illustrated on simulated\nand real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 11:24:20 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 14:30:08 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 09:57:19 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Lugosi", "G\u00e1bor", ""], ["Pateiro-L\u00f3pez", "Beatriz", ""]]}, {"id": "1411.0539", "submitter": "Tuomas Rajala", "authors": "Tuomas Rajala", "title": "A note on Bayesian logistic regression for spatial exponential family\n  Gibbs point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a very attractive logistic regression inference method for\nexponential family Gibbs spatial point processes was introduced. We combined it\nwith the technique of quadratic tangential variational approximation and\nderived a new Bayesian technique for analysing spatial point patterns. The\ntechnique is described in detail, and demonstrated on numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 15:59:29 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Rajala", "Tuomas", ""]]}, {"id": "1411.0560", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang, Antonio Punzo, Paul D. McNicholas, Salvatore\n  Ingrassia, and Ryan P. Browne", "title": "Multivariate response and parsimony for Gaussian cluster-weighted models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of parsimonious Gaussian cluster-weighted models is presented. This\nfamily concerns a multivariate extension to cluster-weighted modelling that can\naccount for correlations between multivariate responses. Parsimony is attained\nby constraining parts of an eigen-decomposition imposed on the component\ncovariance matrices. A sufficient condition for identifiability is provided and\nan expectation-maximization algorithm is presented for parameter estimation.\nModel performance is investigated on both synthetic and classical real data\nsets and compared with some popular approaches. Finally, accounting for linear\ndependencies in the presence of a linear regression structure is shown to offer\nbetter performance, vis-\\`{a}-vis clustering, over existing methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 16:57:39 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 15:05:55 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""], ["Ingrassia", "Salvatore", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1411.0564", "submitter": "Pierre Chainais", "authors": "Pierre Chainais and Aymeric Leray", "title": "Statistical performance analysis of a fast super-resolution technique\n  using noisy translations", "comments": "15 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the registration process is a key step for\nsuper-resolution reconstruction. In this work, we propose to use a\npiezoelectric system that is easily adaptable on all microscopes and telescopes\nfor controlling accurately their motion (down to nanometers) and therefore\nacquiring multiple images of the same scene at different controlled positions.\nThen a fast super-resolution algorithm \\cite{eh01} can be used for efficient\nsuper-resolution reconstruction. In this case, the optimal use of $r^2$ images\nfor a resolution enhancement factor $r$ is generally not enough to obtain\nsatisfying results due to the random inaccuracy of the positioning system. Thus\nwe propose to take several images around each reference position. We study the\nerror produced by the super-resolution algorithm due to spatial uncertainty as\na function of the number of images per position. We obtain a lower bound on the\nnumber of images that is necessary to ensure a given error upper bound with\nprobability higher than some desired confidence level.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 17:03:38 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chainais", "Pierre", ""], ["Leray", "Aymeric", ""]]}, {"id": "1411.0764", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal, David S. Matteson, and David Ruppert", "title": "A Bayesian Multivariate Functional Dynamic Linear Model", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2016.1165104", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian approach for modeling multivariate, dependent\nfunctional data. To account for the three dominant structural features in the\ndata--functional, time dependent, and multivariate components--we extend\nhierarchical dynamic linear models for multivariate time series to the\nfunctional data setting. We also develop Bayesian spline theory in a more\ngeneral constrained optimization framework. The proposed methods identify a\ntime-invariant functional basis for the functional observations, which is\nsmooth and interpretable, and can be made common across multivariate\nobservations for additional information sharing. The Bayesian framework permits\njoint estimation of the model parameters, provides exact inference (up to MCMC\nerror) on specific parameters, and allows generalized dependence structures.\nSampling from the posterior distribution is accomplished with an efficient\nGibbs sampling algorithm. We illustrate the proposed framework with two\napplications: (1) multi-economy yield curve data from the recent global\nrecession, and (2) local field potential brain signals in rats, for which we\ndevelop a multivariate functional time series approach for multivariate\ntime-frequency analysis. Supplementary materials, including R code and the\nmulti-economy yield curve data, are available online.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 02:43:06 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2015 23:40:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Matteson", "David S.", ""], ["Ruppert", "David", ""]]}, {"id": "1411.0807", "submitter": "A. P. Dawid", "authors": "A. P. Dawid", "title": "Discussion of \"On the Birnbaum Argument for the Strong Likelihood\n  Principle\"", "comments": "Published in at http://dx.doi.org/10.1214/14-STS470 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 240-241", "doi": "10.1214/14-STS470", "report-no": "IMS-STS-STS470", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deborah Mayo claims to have refuted Birnbaum's argument that the Likelihood\nPrinciple is a logical consequence of the Sufficiency and Conditionality\nPrinciples. However, this claim fails because her interpretation of the\nConditionality Principle is different from Birnbaum's. Birnbaum's proof cannot\nbe so readily dismissed. [arXiv:1302.7021]\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 07:09:01 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "A. P.", ""]]}, {"id": "1411.0808", "submitter": "Michael Evans", "authors": "Michael Evans", "title": "Discussion of \"On the Birnbaum Argument for the Strong Likelihood\n  Principle\"", "comments": "Published in at http://dx.doi.org/10.1214/14-STS471 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 242-246", "doi": "10.1214/14-STS471", "report-no": "IMS-STS-STS471", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Birnbaum's result, its relevance to statistical reasoning, Mayo's\nobjections and the result in [Electron. J. Statist. 7 (2013) 2645-2655] that\nthe proof of this result doesn't establish what is commonly believed.\n[arXiv:1302.7021]\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 07:12:15 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Evans", "Michael", ""]]}, {"id": "1411.0809", "submitter": "D. A. S. Fraser", "authors": "D. A. S. Fraser", "title": "Discussion: On Arguments Concerning Statistical Principles", "comments": "Published in at http://dx.doi.org/10.1214/14-STS473 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 252-253", "doi": "10.1214/14-STS473", "report-no": "IMS-STS-STS473", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"On the Birnbaum Argument for the Strong Likelihood Principle\"\nby Deborah G. Mayo [arXiv:1302.7021].\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 07:22:23 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Fraser", "D. A. S.", ""]]}, {"id": "1411.0810", "submitter": "Jan Hannig", "authors": "Jan Hannig", "title": "Discussion of \"On the Birnbaum Argument for the Strong Likelihood\n  Principle\"", "comments": "Published in at http://dx.doi.org/10.1214/14-STS474 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 254-258", "doi": "10.1214/14-STS474", "report-no": "IMS-STS-STS474", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this discussion we demonstrate that fiducial distributions provide a\nnatural example of an inference paradigm that does not obey Strong Likelihood\nPrinciple while still satisfying the Weak Conditionality Principle.\n[arXiv:1302.7021]\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 07:24:23 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Hannig", "Jan", ""]]}, {"id": "1411.0811", "submitter": "Jan F. Bj{\\o}rnstad", "authors": "Jan F. Bj{\\o}rnstad", "title": "Discussion of \"On the Birnbaum Argument for the Strong Likelihood\n  Principle\"", "comments": "Published in at http://dx.doi.org/10.1214/14-STS475 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 259-260", "doi": "10.1214/14-STS475", "report-no": "IMS-STS-STS475", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper by Mayo claims to provide a new clarification and critique of\nBirnbaum's argument for showing that sufficiency and conditionality principles\nimply the likelihood principle. However, much of the arguments go back to\narguments made thirty to forty years ago. Also, the main contention in the\npaper, that Birnbaum's arguments are not valid, seems to rest on a\nmisunderstanding. [arXiv:1302.7021]\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 07:27:26 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Bj\u00f8rnstad", "Jan F.", ""]]}, {"id": "1411.0812", "submitter": "Deborah G. Mayo", "authors": "Deborah G. Mayo", "title": "Rejoinder: \"On the Birnbaum Argument for the Strong Likelihood\n  Principle\"", "comments": "Published in at http://dx.doi.org/10.1214/14-STS482 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 261-266", "doi": "10.1214/14-STS482", "report-no": "IMS-STS-STS482", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of \"On the Birnbaum Argument for the Strong Likelihood Principle\"\nby Deborah G. Mayo [arXiv:1302.7021].\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 07:31:32 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Mayo", "Deborah G.", ""]]}, {"id": "1411.1151", "submitter": "Lan Jiang", "authors": "Lan Jiang and Fred J. Hickernell", "title": "Guaranteed Monte Carlo Methods for Bernoulli Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple Monte Carlo is a versatile computational method with a convergence\nrate of $O(n^{-1/2})$. It can be used to estimate the means of random variables\nwhose distributions are unknown. Bernoulli random variables, $Y$, are widely\nused to model success (failure) of complex systems. Here $Y=1$ denotes a\nsuccess (failure), and $p=\\mathbb{E}(Y)$ denotes the probability of that\nsuccess (failure). Another application of Bernoulli random variables is\n$Y=\\mathbb{1}_{R}(\\boldsymbol{X})$, where then $p$ is the probability of\n$\\boldsymbol{X}$ lying in the region $R$. This article explores how estimate\n$p$ to a prescribed absolute error tolerance, $\\varepsilon$, with a high level\nof confidence, $1-\\alpha$. The proposed algorithm automatically determines the\nnumber of samples of $Y$ needed to reach the prescribed error tolerance with\nthe specified confidence level by using Hoeffding's inequality. The algorithm\ndescribed here has been implemented in MATLAB and is part of the Guaranteed\nAutomatic Integration Library (GAIL).\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 04:59:22 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Jiang", "Lan", ""], ["Hickernell", "Fred J.", ""]]}, {"id": "1411.1168", "submitter": "Ting Yan", "authors": "Ting Yan", "title": "Ranking in the generalized Bradley-Terry models when the strong\n  connection condition fails", "comments": "16 pages", "journal-ref": "Communication in Statistics-Theory and Methods, 2016, 45(02),\n  344-358", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For nonbalanced paired comparisons, a wide variety of ranking methods have\nbeen proposed. One of the best popular methods is the Bradley-Terry model in\nwhich the ranking of a set of objects is decided by the maximum likelihood\nestimates (MLEs) of merits parameters. However, the existence of MLE for the\nBradley-Terry model and its generalized models to allow for tied observation or\nhome-field advantage or both to occur, crucially depends on the strong\nconnection condition on the directed graph constructed by a win-loss matrix.\nWhen this condition fails, the MLE does not exist and hence there is no\nsolution of ranking. In this paper, we propose an improved version of the\n$\\varepsilon$ singular perturbation proposed by Conner and Grant (2000), to\naddress this problem and extend it to the generalized Bradley-Terry models.\nSome necessary and sufficient conditions for the existence and uniqueness of\nthe penalized MLEs for these generalized Bradley-Terry-$\\varepsilon$ models are\nderived. Numerical studies show that the ranking is robust to the different\n$\\varepsilon$. We apply the proposed methods to the data of the 2008 NFL\nregular season.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 07:06:31 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Yan", "Ting", ""]]}, {"id": "1411.1194", "submitter": "Li Yin", "authors": "Li Yin and Xiaoqin Wang", "title": "Parametric Sequential Causal Inference in Point Parametrization", "comments": "33 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a sequence of treatments are assigned to influence an outcome of\ninterest that occurs after the last treatment. Between treatments there exist\ntime-dependent covariates that may be posttreatment variables of the earlier\ntreatments and confounders of the subsequent treatments. In this article, we\ndevelop a parametric approach to inference of the causal effect of the\ntreatment sequence on the outcome called the sequential causal effect. We\nconstruct a point parametrization for the conditional distribution of an\noutcome given all treatments and time-dependent covariates, in which the point\nparameters of interest are the point effects of treatments considered as\nsingle-point treatments. We (1) identify net effects of treatments by point\neffects of treatments, (2) express patterns of net effects of treatments by\nconstraints on point effects of treatments, and (3) show that all sequential\ncausal effects are determined by net effects of treatments. Accordingly we (1)\nestimate net effects of treatments through point effects of treatments by\nmaximum likelihood, (2) improve the estimation by constraints on point effects\nof treatments and assignment conditions of treatments, and (3) use the\nestimates of net effects of treatments to obtain those of sequential causal\neffects. As a result, we obtain unbiased consistent maximum-likelihood\nestimates of sequential causal effects even for long treatment sequences. For\nillustration of our method, we study the causal effects of various sequences of\nrecreational drugs on the CD4 count among HIV patients.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 09:03:44 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 12:21:47 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Yin", "Li", ""], ["Wang", "Xiaoqin", ""]]}, {"id": "1411.1351", "submitter": "Ehsan Zamanzade", "authors": "Ehsan Zamanzade and Amer Ibrahim Al-Omari", "title": "New ranked set sampling for estimating the population parameters", "comments": "This paper has been withdrawn by the first author due to many\n  notational errors in Section 2", "journal-ref": null, "doi": "10.15672/HJMS.20159213166", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new modification of ranked set sampling (RSS) is suggested,\nnamely; unified ranked set sampling (URSS) for estimating the population mean\nand variance. The performance of the empirical mean and variance estimators\nbased on URSS are compared with their counterparts in ranked set sampling and\nsimple random sampling (SRS) via Monte Carlo simulation. Simulation results\nindicate that the URSS estimators perform better than their counterparts using\nRSS and SRS designs when the ranking is perfect. When the ranking is imperfect,\nthe URSS estimators still are superior than their counterparts in ranked set\nsampling and simple random sampling methods. Finally, an illustrative example\nis provided to show the efficiency of the new method in practice.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 18:47:07 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 20:59:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2015 00:02:25 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Zamanzade", "Ehsan", ""], ["Al-Omari", "Amer Ibrahim", ""]]}, {"id": "1411.1352", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Alfred Hero III", "title": "Robust Kronecker Product PCA for Spatio-Temporal Covariance Estimation", "comments": "To appear in IEEE Transactions on Signal Processing 11 pgs", "journal-ref": null, "doi": "10.1109/TSP.2015.2472364", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kronecker PCA involves the use of a space vs. time Kronecker product\ndecomposition to estimate spatio-temporal covariances. In this work the\naddition of a sparse correction factor is considered, which corresponds to a\nmodel of the covariance as a sum of Kronecker products of low (separation) rank\nand a sparse matrix. This sparse correction extends the diagonally corrected\nKronecker PCA of [Greenewald et al 2013, 2014] to allow for sparse unstructured\n\"outliers\" anywhere in the covariance matrix, e.g. arising from variables or\ncorrelations that do not fit the Kronecker model well, or from sources such as\nsensor noise or sensor failure. We introduce a robust PCA-based algorithm to\nestimate the covariance under this model, extending the rearranged nuclear norm\npenalized LS Kronecker PCA approaches of [Greenewald et al 2014, Tsiligkaridis\net al 2013]. An extension to Toeplitz temporal factors is also provided,\nproducing a parameter reduction for temporally stationary measurement modeling.\nHigh dimensional MSE performance bounds are given for these extensions.\nFinally, the proposed extension of KronPCA is evaluated on both simulated and\nreal data coming from yeast cell cycle experiments. This establishes the\npractical utility of robust Kronecker PCA in biological and other applications.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 18:47:36 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 20:40:22 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2015 14:25:21 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2015 13:57:51 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Hero", "Alfred", "III"]]}, {"id": "1411.1437", "submitter": "Jian Li", "authors": "Jian Li, David Siegmund", "title": "Higher criticism: $p$-values and criticism", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1312 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 1323-1350", "doi": "10.1214/15-AOS1312", "report-no": "IMS-AOS-AOS1312", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares the higher criticism statistic (Donoho and Jin [Ann.\nStatist. 32 (2004) 962-994]), a modification of the higher criticism statistic\nalso suggested by Donoho and Jin, and two statistics of the Berk-Jones [Z.\nWahrsch. Verw. Gebiete 47 (1979) 47-59] type. New approximations to the\nsignificance levels of the statistics are derived, and their accuracy is\nstudied by simulations. By numerical examples it is shown that over a broad\nrange of sample sizes the Berk-Jones statistics have a better power function\nthan the higher criticism statistics to detect sparse mixtures. The\napplications suggested by Meinshausen and Rice [Ann. Statist. 34 (2006)\n373-393], to find lower confidence bounds for the number of false hypotheses,\nand by Jeng, Cai and Li [Biometrika 100 (2013) 157-172], to detect copy number\nvariants, are also studied.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 22:41:12 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2015 06:20:28 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 07:30:45 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Li", "Jian", ""], ["Siegmund", "David", ""]]}, {"id": "1411.1451", "submitter": "Scott Sisson", "authors": "Robert Erhardt and Scott A. Sisson", "title": "Modelling extremes using approximate Bayesian Computation", "comments": "To appear in Extreme Value Modelling and Risk Analysis: Methods and\n  Applications. Eds. D. Dey and J. Yan. Chapman & Hall/CRC Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the nature of their construction, many statistical models for extremes\nresult in likelihood functions that are computationally prohibitive to\nevaluate. This is consequently problematic for the purposes of likelihood-based\ninference. With a focus on the Bayesian framework, this chapter examines the\nuse of approximate Bayesian computation (ABC) techniques for the fitting and\nanalysis of statistical models for extremes. After introducing the ideas behind\nABC algorithms and methods, we demonstrate their application to extremal models\nin stereology and spatial extremes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 23:45:52 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Erhardt", "Robert", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1411.1594", "submitter": "Vivian Viallon", "authors": "Edouard Ollier and Vivian Viallon", "title": "Joint estimation of $K$ related regression models with simple $L_1$-norm\n  penalties", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach, along with refinements, based on $L_1$ penalties\nand aimed at jointly estimating several related regression models. Its main\ninterest is that it can be rewritten as a weighted lasso on a simple\ntransformation of the original data set. In particular, it does not need new\ndedicated algorithms and is ready to implement under a variety of regression\nmodels, {\\em e.g.}, using standard R packages. Moreover, asymptotic oracle\nproperties are derived along with preliminary non-asymptotic results,\nsuggesting good theoretical properties. Our approach is further compared with\nstate-of-the-art competitors under various settings on synthetic data: these\nempirical results confirm that our approach performs at least similarly to its\ncompetitors. As a final illustration, an analysis of road safety data is\nprovided.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 13:05:13 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Ollier", "Edouard", ""], ["Viallon", "Vivian", ""]]}, {"id": "1411.1715", "submitter": "Jing Lei", "authors": "Kehui Chen and Jing Lei", "title": "Network Cross-Validation for Determining the Number of Communities in\n  Network Data", "comments": "27 pages. Added further theoretical results and more references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model and its variants have been a popular tool in\nanalyzing large network data with community structures. In this paper we\ndevelop an efficient network cross-validation (NCV) approach to determine the\nnumber of communities, as well as to choose between the regular stochastic\nblock model and the degree corrected block model. The proposed NCV method is\nbased on a block-wise node-pair splitting technique, combined with an\nintegrated step of community recovery using sub-blocks of the adjacency matrix.\nWe prove that the probability of under selection vanishes as the number of node\nincreases, under mild conditions satisfied by a wide range of popular community\nrecovery algorithms. The solid performance of our method is also demonstrated\nin extensive simulations and a data example.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 19:44:33 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 17:28:17 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Chen", "Kehui", ""], ["Lei", "Jing", ""]]}, {"id": "1411.1869", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "Efficient estimation of high-dimensional multivariate normal copula\n  models with discrete spatial responses", "comments": "arXiv admin note: text overlap with arXiv:1304.0905", "journal-ref": "Stochastic Environmental Research and Risk Assessment, 2016,\n  30(2):493--505", "doi": "10.1007/s00477-015-1060-2", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributional transform (DT) is amongst the computational methods used\nfor estimation of high-dimensional multivariate normal copula models with\ndiscrete responses. Its advantage is that the likelihood can be derived\nconveniently under the theory for copula models with continuous margins, but\nthere has not been a clear analysis of the adequacy of this method. We\ninvestigate the small-sample and asymptotic efficiency of the method for\nestimating high-dimensional multivariate normal copula models with univariate\nBernoulli, Poisson, and negative binomial margins, and show that the DT\napproximation leads to biased estimates when there is more discretisation. For\na high-dimensional discrete response, we implement a maximum simulated\nlikelihood method, which is based on evaluating the multidimensional integrals\nof the likelihood with randomized quasi Monte Carlo methods. Efficiency\ncalculations show that our method is nearly as efficient as maximum likelihood\nfor fully specified high-dimensional multivariate normal copula models. Both\nmethods are illustrated with spatially aggregated count data sets, and it is\nshown that there is a substantial gain on efficiency via the maximum simulated\nlikelihood method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 09:48:51 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 19:21:24 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1411.1997", "submitter": "Barbara Engelhardt", "authors": "Chuan Gao, Shiwen Zhao, Ian C. McDowell, Christopher D. Brown, Barbara\n  E. Engelhardt", "title": "Differential gene co-expression networks via Bayesian biclustering\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying latent structure in large data matrices is essential for\nexploring biological processes. Here, we consider recovering gene co-expression\nnetworks from gene expression data, where each network encodes relationships\nbetween genes that are locally co-regulated by shared biological mechanisms. To\ndo this, we develop a Bayesian statistical model for biclustering to infer\nsubsets of co-regulated genes whose covariation may be observed in only a\nsubset of the samples. Our biclustering method, BicMix, has desirable\nproperties, including allowing overcomplete representations of the data,\ncomputational tractability, and jointly modeling unknown confounders and\nbiological signals. Compared with related biclustering methods, BicMix recovers\nlatent structure with higher precision across diverse simulation scenarios.\nFurther, we develop a method to recover gene co-expression networks from the\nestimated sparse biclustering matrices. We apply BicMix to breast cancer gene\nexpression data and recover a gene co-expression network that is differential\nacross ER+ and ER- samples.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 17:50:48 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Gao", "Chuan", ""], ["Zhao", "Shiwen", ""], ["McDowell", "Ian C.", ""], ["Brown", "Christopher D.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1411.2051", "submitter": "John Aston", "authors": "Ci-Ren Jiang, John A D Aston and Jane-Ling Wang", "title": "A functional approach to deconvolve dynamic neuroimaging data", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron Emission Tomography (PET) is an imaging technique which can be used\nto investigate chemical changes in human biological processes such as cancer\ndevelopment or neurochemical reactions. Most dynamic PET scans are currently\nanalyzed based on the assumption that linear first order kinetics can be used\nto adequately describe the system under observation. However, there has\nrecently been strong evidence that this is not the case. In order to provide an\nanalysis of PET data which is free from this compartmental assumption, we\npropose a nonparametric deconvolution and analysis model for dynamic PET data\nbased on functional principal component analysis. This yields flexibility in\nthe possible deconvolved functions while still performing well when a linear\ncompartmental model setup is the true data generating mechanism. As the\ndeconvolution needs to be performed on only a relative small number of basis\nfunctions rather than voxel by voxel in the entire 3-D volume, the methodology\nis both robust to typical brain imaging noise levels while also being\ncomputationally efficient. The new methodology is investigated through\nsimulations in both 1-D functions and 2-D images and also applied to a\nneuroimaging study whose goal is the quantification of opioid receptor\nconcentration in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 22:30:42 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Jiang", "Ci-Ren", ""], ["Aston", "John A D", ""], ["Wang", "Jane-Ling", ""]]}, {"id": "1411.2056", "submitter": "Ju Hyun Kim", "authors": "Ju Hyun Kim", "title": "Partial Identification of Distributional Parameters in Triangular\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I study partial identification of distributional parameters in triangular\nsystems. This model consists of a nonparametric outcome equation and a\nselection equation. This allows for general unobserved heterogeneity and\nselection on unobservables. The distributional parameters considered in this\npaper are the marginal distributions of potential outcomes, their joint\ndistribution, and the distribution of treatment effects. I investigate how\ndifferent types of plausible restrictions contribute to identifying these\nparameters. The restrictions I consider include stochastic dominance and\nquadrant dependence between unobservables and monotonicity between potential\noutcomes. My identification applies to the whole population without a full\nsupport condition on instrumental variables and does not rely on rank\nsimilarity. I also provide numerical examples to illustrate the identification\npower of each assumption.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 22:51:53 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kim", "Ju Hyun", ""]]}, {"id": "1411.2127", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser and Eric Tchetgen Tchetgen", "title": "Causal Inference with a Graphical Hierarchy of Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying causal parameters from observational data is fraught with\nsubtleties due to the issues of selection bias and confounding. In addition,\nmore complex questions of interest, such as effects of treatment on the treated\nand mediated effects may not always be identified even in data where treatment\nassignment is known and under investigator control, or may be identified under\none causal model but not another.\n  Increasingly complex effects of interest, coupled with a diversity of causal\nmodels in use resulted in a fragmented view of identification. This\nfragmentation makes it unnecessarily difficult to determine if a given\nparameter is identified (and in what model), and what assumptions must hold for\nthis to be the case. This, in turn, complicates the development of estimation\ntheory and sensitivity analysis procedures.\n  In this paper, we give a unifying view of a large class of causal effects of\ninterest in terms of a hierarchy of interventions, and show that identification\ntheory for this large class reduces to an identification theory of random\nvariables under interventions from this hierarchy. Moreover, we show that one\ntype of intervention in the hierarchy is naturally associated with queries\nidentified under the Finest Fully Randomized Causally Interpretable Structure\nTree Graph (FFRCISTG) model of Robins (via the extended g-formula), and another\nis naturally associated with queries identified under the Non-Parametric\nStructural Equation Model with Independent Errors (NPSEM-IE) of Pearl, via a\nmore general functional we call the edge g-formula.\n  Our results motivate the study of estimation theory for the edge g-formula,\nsince we show it arises both in mediation analysis, and in settings where\ntreatment assignment has unobserved causes, such as models associated with\nPearl's front-door criterion.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 15:29:54 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 15:24:57 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2015 21:57:36 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Shpitser", "Ilya", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1411.2158", "submitter": "Norbert Binkiewicz", "authors": "Norbert Binkiewicz, Joshua T. Vogelstein, and Karl Rohe", "title": "Covariate-assisted spectral clustering", "comments": "28 pages, 4 figures, includes substantial changes to theoretical\n  results", "journal-ref": "Biometrika, Volume 104, Issue 2, 1 June 2017, Pages 361-377", "doi": "10.1093/biomet/asx008", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological and social systems consist of myriad interacting units. The\ninteractions can be represented in the form of a graph or network. Measurements\nof these graphs can reveal the underlying structure of these interactions,\nwhich provides insight into the systems that generated the graphs. Moreover, in\napplications such as connectomics, social networks, and genomics, graph data\nare accompanied by contextualizing measures on each node. We utilize these node\ncovariates to help uncover latent communities in a graph, using a modification\nof spectral clustering. Statistical guarantees are provided under a joint\nmixture model that we call the node-contextualized stochastic blockmodel,\nincluding a bound on the mis-clustering rate. The bound is used to derive\nconditions for achieving perfect clustering. For most simulated cases,\ncovariate-assisted spectral clustering yields results superior to regularized\nspectral clustering without node covariates and to an adaptation of canonical\ncorrelation analysis. We apply our clustering method to large brain graphs\nderived from diffusion MRI data, using the node locations or neurological\nregion membership as covariates. In both cases, covariate-assisted spectral\nclustering yields clusters that are easier to interpret neurologically.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 20:14:59 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 19:14:01 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 04:51:04 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 04:07:22 GMT"}, {"version": "v5", "created": "Sun, 30 Oct 2016 04:22:47 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Binkiewicz", "Norbert", ""], ["Vogelstein", "Joshua T.", ""], ["Rohe", "Karl", ""]]}, {"id": "1411.2389", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, H. M. de Oliveira, L. R. Soares", "title": "On Filter Banks and Wavelets Based on Chebyshev Polynomials", "comments": "18 pages, 6 figures", "journal-ref": "Cintra, R. J. ; Oliveira, H. M. ; Soares, L. R. \"On Filter Banks\n  and Wavelets Based on Chebyshev Polynomials\". In: 7th WSEAS International\n  Conference on Circuits, 2003, Corfu Island, Greece, p. 195-200", "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new family of wavelets, named Chebyshev\nwavelets, which are derived from conventional first and second kind Chebyshev\npolynomials. Properties of Chebyshev filter banks are investigated, including\northogonality and perfect reconstruction conditions. Chebyshev wavelets have\ncompact support, their filters possess good selectivity, but they are not\northogonal. The convergence of the cascade algorithm of Chebyshev wavelets is\nproved by using properties of Markov chains. Computational implementation of\nthese wavelets and some clear-cut applications are presented. Proposed wavelets\nare suitable for signal denoising.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 11:46:51 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Cintra", "R. J.", ""], ["de Oliveira", "H. M.", ""], ["Soares", "L. R.", ""]]}, {"id": "1411.2497", "submitter": "Malcolm Farrow", "authors": "Kevin J. Wilson and Malcolm Farrow", "title": "Bayes linear kinematics in a dynamic Bayesian survival model", "comments": "A correction has been made to Figure 2. Section 6 (Simulations) has\n  been added. Some other minor changes have been made", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes linear kinematics and Bayes linear Bayes graphical models provide an\nextension of Bayes linear methods so that full conditional updates may be\ncombined with Bayes linear belief adjustment. In this paper we investigate the\napplication of this approach to a more complicated problem: namely survival\nanalysis with time-dependent covariate effects. We use a piecewise-constant\nhazard function with a prior in which covariate effects are correlated over\ntime. The need for computationally intensive methods is avoided and the\nrelatively simple structure facilitates interpretation. Our approach eliminates\nthe problem of non-commutativity which was observed in earlier work by\nGamerman. We apply the technique to data on survival times for leukemia\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 16:36:51 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 10:26:24 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Wilson", "Kevin J.", ""], ["Farrow", "Malcolm", ""]]}, {"id": "1411.2573", "submitter": "Armin Hatefi", "authors": "Armin Hatefi and Mohammad Jafari Jozani", "title": "Proportion estimation based on a partially rank ordered set sample with\n  multiple concomitants in a breast cancer study", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use partially rank-ordered set (PROS) sampling design with\nmultiple concomitants in a breast cancer study and propose a method to estimate\nthe proportion of patients with malignant (cancerous) breast tumours in a given\npopulation. Through extensive numerical studies, the performance of the\nestimator is evaluated under various concomitants with different ranking\npotentials (i.e., good, intermediate and bad) and tie-structures. We show that\nthe PROS estimator with multiple concomitants based on the ranking information\nprovided through some easy to obtain cytological characteristics that are\nassociated with the malignancy of breast tumours performs better than its\ncounterparts under simple random sampling (SRS) and ranked set sampling (RSS)\ndesigns with logistic regression models. As opposed to available RSS based\nmethods in the literature, our proposed methodology allows to declare ties\namong the ranks and does not rely on the existence of any specific regression\nmodel assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 20:27:06 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Hatefi", "Armin", ""], ["Jozani", "Mohammad Jafari", ""]]}, {"id": "1411.2578", "submitter": "K. Sham Bhat", "authors": "K. Sham Bhat, David S. Mebane, Curtis B. Storlie, and Priyadarshi\n  Mahapatra", "title": "Upscaling Uncertainty with Dynamic Discrepancy for a Multi-scale Carbon\n  Capture System", "comments": "27 pages, 13 figures, submitted to the Journal of the American\n  Statistical Association Revision on 12/16/14: corrected 6 figures to enhance\n  readability", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainties from model parameters and model discrepancy from small-scale\nmodels impact the accuracy and reliability of predictions of large-scale\nsystems. Inadequate representation of these uncertainties may result in\ninaccurate and overconfident predictions during scale-up to larger models.\nHence multiscale modeling efforts must quantify the effect of the propagation\nof uncertainties during upscaling. Using a Bayesian approach, we calibrate a\nsmall-scale solid sorbent model to Thermogravimetric (TGA) data on a functional\nprofile using chemistry-based priors. Crucial to this effort is the\nrepresentation of model discrepancy, which uses a Bayesian Smoothing Splines\n(BSS-ANOVA) framework. We use an intrusive uncertainty quantification (UQ)\napproach by including the discrepancy function within the chemical rate\nexpressions; resulting in a set of stochastic differential equations. Such an\napproach allows for easily propagating uncertainty by propagating the joint\nmodel parameter and discrepancy posterior into the larger-scale system of rate\nexpressions. The broad UQ framework presented here may have far-reaching impact\ninto virtually all areas of science where multiscale modeling is used.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 20:43:36 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 03:07:10 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Bhat", "K. Sham", ""], ["Mebane", "David S.", ""], ["Storlie", "Curtis B.", ""], ["Mahapatra", "Priyadarshi", ""]]}, {"id": "1411.2624", "submitter": "Theodore  Kypraios", "authors": "Edward S. Knock and Theodore Kypraios", "title": "Bayesian Non-Parametric Inference for Infectious Disease Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for Bayesian non-parametric estimation of the rate at\nwhich new infections occur assuming that the epidemic is partially observed.\nThe developed methodology relies on modelling the rate at which new infections\noccur as a function which only depends on time. Two different types of prior\ndistributions are proposed namely using step-functions and B-splines. The\nmethodology is illustrated using both simulated and real datasets and we show\nthat certain aspects of the epidemic such as seasonality and super-spreading\nevents are picked up without having to explicitly incorporate them into a\nparametric model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 21:26:10 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 10:10:42 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Knock", "Edward S.", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1411.2636", "submitter": "Philip Dawid", "authors": "A. P. Dawid, R. Murtas and M. Musio", "title": "Bounding the Probability of Causation in Mediation Analysis", "comments": "9 pages, 1 figure, 3 tables", "journal-ref": "In Topics on Methodological and Applied Statistical Inference,\n  edited by T. Di Battista, E. Moreno and W. Racugno. Springer (2016), 75-84", "doi": null, "report-no": null, "categories": "math.ST cs.AI stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given empirical evidence for the dependence of an outcome variable on an\nexposure variable, we can typically only provide bounds for the \"probability of\ncausation\" in the case of an individual who has developed the outcome after\nbeing exposed. We show how these bounds can be adapted or improved if further\ninformation becomes available. In addition to reviewing existing work on this\ntopic, we provide a new analysis for the case where a mediating variable can be\nobserved. In particular we show how the probability of causation can be bounded\nwhen there is no direct effect and no confounding.\n  Keywords: Causal inference, Mediation Analysis, Probability of Causation\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 21:48:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "A. P.", ""], ["Murtas", "R.", ""], ["Musio", "M.", ""]]}, {"id": "1411.2698", "submitter": "Barbara Engelhardt", "authors": "Shiwen Zhao and Chuan Gao and Sayan Mukherjee and Barbara E Engelhardt", "title": "Bayesian group latent factor analysis with structured sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models are the canonical statistical tool for exploratory\nanalyses of low-dimensional linear structure for an observation matrix with p\nfeatures across n samples. We develop a structured Bayesian group factor\nanalysis model that extends the factor model to multiple coupled observation\nmatrices; in the case of two observations, this reduces to a Bayesian model of\ncanonical correlation analysis. The main contribution of this work is to\ncarefully define a structured Bayesian prior that encourages both element-wise\nand column-wise shrinkage and leads to desirable behavior on high-dimensional\ndata. In particular, our model puts a structured prior on the joint factor\nloading matrix, regularizing at three levels, which enables element-wise\nsparsity and unsupervised recovery of latent factors corresponding to\nstructured variance across arbitrary subsets of the observations. In addition,\nour structured prior allows for both dense and sparse latent factors so that\ncovariation among either all features or only a subset of features can both be\nrecovered. We use fast parameter-expanded expectation-maximization for\nparameter estimation in this model. We validate our method on both simulated\ndata with substantial structure and real data, comparing against a number of\nstate-of-the-art approaches. These results illustrate useful properties of our\nmodel, including i) recovering sparse signal in the presence of dense effects;\nii) the ability to scale naturally to large numbers of observations; iii)\nflexible observation- and factor-specific regularization to recover factors\nwith a wide variety of sparsity levels and percentage of variance explained;\nand iv) tractable inference that scales to modern genomic and document data\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 04:50:32 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 17:24:03 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Zhao", "Shiwen", ""], ["Gao", "Chuan", ""], ["Mukherjee", "Sayan", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1411.2755", "submitter": "Chris Oates", "authors": "Chris J. Oates, Jim Q. Smith, Sach Mukherjee", "title": "Estimating causal structure using conditional DAG models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers inference of causal structure in a class of graphical\nmodels called \"conditional DAGs\". These are directed acyclic graph (DAG) models\nwith two kinds of variables, primary and secondary. The secondary variables are\nused to aid in estimation of causal relationships between the primary\nvariables. We give causal semantics for this model class and prove that, under\ncertain assumptions, the direction of causal influence is identifiable from the\njoint observational distribution of the primary and secondary variables. A\nscore-based approach is developed for estimation of causal structure using\nthese models and consistency results are established. Empirical results\ndemonstrate gains compared with formulations that treat all variables on an\nequal footing, or that ignore secondary variables. The methodology is motivated\nby applications in molecular biology and is illustrated here using simulated\ndata and in an analysis of proteomic data from the Cancer Genome Atlas.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 10:33:26 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Oates", "Chris J.", ""], ["Smith", "Jim Q.", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1411.2778", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck, Eric-Jan Wagenmakers, Richard D. Morey", "title": "Testing Order Constraints: Qualitative Differences Between Bayes Factors\n  and Normalized Maximum Likelihood", "comments": null, "journal-ref": "Statistics & Probability Letters 105 (2015) 157-162", "doi": "10.1016/j.spl.2015.06.014", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compared Bayes factors to normalized maximum likelihood for the simple\ncase of selecting between an order-constrained versus a full binomial model.\nThis comparison revealed two qualitative differences in testing order\nconstraints regarding data dependence and model preference.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 12:23:30 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 11:49:09 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2015 08:52:05 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Heck", "Daniel W.", ""], ["Wagenmakers", "Eric-Jan", ""], ["Morey", "Richard D.", ""]]}, {"id": "1411.2820", "submitter": "Sharon Lee", "authors": "Sharon X. Lee, Geoffrey J. McLachlan, Saumyadipta Pyne", "title": "Supervised Classification of Flow Cytometric Samples via the Joint\n  Clustering and Matching (JCM) Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the use of the Joint Clustering and Matching (JCM) procedure for\nthe supervised classification of a flow cytometric sample with respect to a\nnumber of predefined classes of such samples. The JCM procedure has been\nproposed as a method for the unsupervised classification of cells within a\nsample into a number of clusters and in the case of multiple samples, the\nmatching of these clusters across the samples. The two tasks of clustering and\nmatching of the clusters are performed simultaneously within the JCM framework.\nIn this paper, we consider the case where there is a number of distinct classes\nof samples whose class of origin is known, and the problem is to classify a new\nsample of unknown class of origin to one of these predefined classes. For\nexample, the different classes might correspond to the types of a particular\ndisease or to the various health outcomes of a patient subsequent to a course\nof treatment. We show and demonstrate on some real datasets how the JCM\nprocedure can be used to carry out this supervised classification task. A\nmixture distribution is used to model the distribution of the expressions of a\nfixed set of markers for each cell in a sample with the components in the\nmixture model corresponding to the various populations of cells in the\ncomposition of the sample. For each class of samples, a class template is\nformed by the adoption of random-effects terms to model the inter-sample\nvariation within a class. The classification of a new unclassified sample is\nundertaken by assigning the unclassified sample to the class that minimizes the\nKullback-Leibler distance between its fitted mixture density and each class\ndensity provided by the class templates.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 14:22:32 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""], ["Pyne", "Saumyadipta", ""]]}, {"id": "1411.3013", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth, Michael Habeck, Nabin K. Malakar, Asim M. Mubeen, Ben\n  Placek", "title": "Bayesian Evidence and Model Selection", "comments": "Arxiv version consists of 58 pages and 9 figures. Features theory,\n  numerical methods and four applications", "journal-ref": "Digital Signal Processing, 47:50-67 (2015)", "doi": "10.1016/j.dsp.2015.06.012", "report-no": null, "categories": "stat.ME astro-ph.IM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review the concepts of Bayesian evidence and Bayes factors,\nalso known as log odds ratios, and their application to model selection. The\ntheory is presented along with a discussion of analytic, approximate and\nnumerical techniques. Specific attention is paid to the Laplace approximation,\nvariational Bayes, importance sampling, thermodynamic integration, and nested\nsampling and its recent variants. Analogies to statistical physics, from which\nmany of these techniques originate, are discussed in order to provide readers\nwith deeper insights that may lead to new techniques. The utility of Bayesian\nmodel testing in the domain sciences is demonstrated by presenting four\nspecific practical examples considered within the context of signal processing\nin the areas of signal detection, sensor characterization, scientific model\nselection and molecular force characterization.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 23:08:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:13:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Habeck", "Michael", ""], ["Malakar", "Nabin K.", ""], ["Mubeen", "Asim M.", ""], ["Placek", "Ben", ""]]}, {"id": "1411.3062", "submitter": "Youngki Shin Youngki Shin", "authors": "Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin", "title": "Structural Change in Sparsity", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the high-dimensional sparse modeling literature, it has been crucially\nassumed that the sparsity structure of the model is homogeneous over the entire\npopulation. That is, the identities of important regressors are invariant\nacross the population and across the individuals in the collected sample. In\npractice, however, the sparsity structure may not always be invariant in the\npopulation, due to heterogeneity across different sub-populations. We consider\na general, possibly non-smooth M-estimation framework, allowing a possible\nstructural change regarding the identities of important regressors in the\npopulation. Our penalized M-estimator not only selects covariates but also\ndiscriminates between a model with homogeneous sparsity and a model with a\nstructural change in sparsity. As a result, it is not necessary to know or\npretest whether the structural change is present, or where it occurs. We derive\nasymptotic bounds on the estimation loss of the penalized M-estimators, and\nachieve the oracle properties. We also show that when there is a structural\nchange, the estimator of the threshold parameter is super-consistent. If the\nsignal is relatively strong, the rates of convergence can be further improved\nand asymptotic distributional properties of the estimators including the\nthreshold estimator can be established using an adaptive penalization. The\nproposed methods are then applied to quantile regression and logistic\nregression models and are illustrated via Monte Carlo experiments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 03:47:18 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 15:59:55 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Lee", "Sokbae", ""], ["Liao", "Yuan", ""], ["Seo", "Myung Hwan", ""], ["Shin", "Youngki", ""]]}, {"id": "1411.3070", "submitter": "Bo Jiang", "authors": "Bo Jiang, Chao Ye and Jun S. Liu", "title": "Bayesian nonparametric tests via sliced inverse modeling", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of independence and conditional independence tests\nbetween categorical covariates and a continuous response variable, which has an\nimmediate application in genetics. Instead of estimating the conditional\ndistribution of the response given values of covariates, we model the\nconditional distribution of covariates given the discretized response (aka\n\"slices\"). By assigning a prior probability to each possible discretization\nscheme, we can compute efficiently a Bayes factor (BF)-statistic for the\nindependence (or conditional independence) test using a dynamic programming\nalgorithm. Asymptotic and finite-sample properties such as power and null\ndistribution of the BF statistic are studied, and a stepwise variable selection\nmethod based on the BF statistic is further developed. We compare the BF\nstatistic with some existing classical methods and demonstrate its statistical\npower through extensive simulation studies. We apply the proposed method to a\nmouse genetics data set aiming to detect quantitative trait loci (QTLs) and\nobtain promising results.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 05:17:30 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 22:20:04 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 01:23:16 GMT"}, {"version": "v4", "created": "Sat, 2 May 2015 01:41:19 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Jiang", "Bo", ""], ["Ye", "Chao", ""], ["Liu", "Jun S.", ""]]}, {"id": "1411.3138", "submitter": "Federica Giardina", "authors": "Tom Britton and Federica Giardina", "title": "Introduction to statistical inference for infectious diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first introduce the general stochastic epidemic model for\nthe spread of infectious diseases. Then we give methods for inferring model\nparameters such as the basic reproduction number $R_0$ and vaccination coverage\n$v_c$ assuming different types of data from an outbreak such as final outbreak\ndetails and temporal data or observations from an ongoing outbreak. Both\nindividual heterogeneities and heterogeneous mixing are discussed. We also\nprovide an overview of statistical methods to perform parameter estimation for\nstochastic epidemic models. In the last section we describe the problem of\nearly outbreak detection in infectious disease surveillance and statistical\nmodels used for this purpose.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 11:06:12 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 08:19:04 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Britton", "Tom", ""], ["Giardina", "Federica", ""]]}, {"id": "1411.3174", "submitter": "Rapha\\\"el Huser", "authors": "Raphael Huser and Marc G. Genton", "title": "Non-Stationary Dependence Structures for Spatial Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes are natural models for spatial extremes because they\nprovide suitable asymptotic approximations to the distribution of maxima of\nrandom fields. In the recent past, several parametric families of stationary\nmax-stable models have been developed, and fitted to various types of data.\nHowever, a recurrent problem is the modeling of non-stationarity. In this\npaper, we develop non-stationary max-stable dependence structures in which\ncovariates can be easily incorporated. Inference is performed using pairwise\nlikelihoods, and its performance is assessed by an extensive simulation study\nbased on a non-stationary locally isotropic extremal $t$ model. Evidence that\nunknown parameters are well estimated is provided, and estimation of spatial\nreturn level curves is discussed. The methodology is demonstrated with\ntemperature maxima recorded over a complex topography. Models are shown to\nsatisfactorily capture extremal dependence.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 13:49:56 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 07:13:38 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Huser", "Raphael", ""], ["Genton", "Marc G.", ""]]}, {"id": "1411.3448", "submitter": "Rapha\\\"el Huser", "authors": "Rapha\\\"el Huser, Anthony C. Davison and Marc G. Genton", "title": "Likelihood estimators for multivariate extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main approach to inference for multivariate extremes consists in\napproximating the joint upper tail of the observations by a parametric family\narising in the limit for extreme events. The latter may be expressed in terms\nof componentwise maxima, high threshold exceedances or point processes,\nyielding different but related asymptotic characterizations and estimators. The\npresent paper clarifies the connections between the main likelihood estimators,\nand assesses their practical performance. We investigate their ability to\nestimate the extremal dependence structure and to predict future extremes,\nusing exact calculations and simulation, in the case of the logistic model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 05:43:07 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 03:25:51 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Huser", "Rapha\u00ebl", ""], ["Davison", "Anthony C.", ""], ["Genton", "Marc G.", ""]]}, {"id": "1411.3496", "submitter": "Mark van de Wiel", "authors": "Mark A. van de Wiel, Tonje G. Lien, Wina Verlaat, Wessel N. van\n  Wieringen, Saskia M. Wilting", "title": "Better prediction by use of co-data: Adaptive group-regularized ridge\n  regression", "comments": "15 pages, 2 figures. Supplementary Information available on first\n  author's web site", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many high-dimensional studies, additional information on the variables,\nlike (genomic) annotation or external p-values, is available. In the context of\nbinary and continuous prediction, we develop a method for adaptive\ngroup-regularized (logistic) ridge regression, which makes structural use of\nsuch 'co-data'. Here, 'groups' refer to a partition of the variables according\nto the co-data. We derive empirical Bayes estimates of group-specific\npenalties, which possess several nice properties: i) they are analytical; ii)\nthey adapt to the informativeness of the co-data for the data at hand; iii)\nonly one global penalty parameter requires tuning by cross-validation. In\naddition, the method allows use of multiple types of co-data at little extra\ncomputational effort.\n  We show that the group-specific penalties may lead to a larger distinction\nbetween `near-zero' and relatively large regression parameters, which\nfacilitates post-hoc variable selection. The method, termed GRridge, is\nimplemented in an easy-to-use R-package. It is demonstrated on two cancer\ngenomics studies, which both concern the discrimination of precancerous\ncervical lesions from normal cervix tissues using methylation microarray data.\nFor both examples, GRridge clearly improves the predictive performances of\nordinary logistic ridge regression and the group lasso. In addition, we show\nthat for the second study the relatively good predictive performance is\nmaintained when selecting only 42 variables.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 10:52:04 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 15:08:19 GMT"}, {"version": "v3", "created": "Mon, 18 May 2015 14:34:21 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["van de Wiel", "Mark A.", ""], ["Lien", "Tonje G.", ""], ["Verlaat", "Wina", ""], ["van Wieringen", "Wessel N.", ""], ["Wilting", "Saskia M.", ""]]}, {"id": "1411.3609", "submitter": "Jana Jure\\v{c}kov\\'{a}", "authors": "Jana Jure\\v{c}kov\\'a, Hira L. Koul, Radim Navr\\'atil, Jan Picek", "title": "Behavior of R-estimators under measurement errors", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ687 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 1093-1112", "doi": "10.3150/14-BEJ687", "report-no": "IMS-BEJ-BEJ687", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As was shown recently, the measurement errors in regressors affect only the\npower of the rank test, but not its critical region. Noting that, we study the\neffect of measurement errors on R-estimators in linear model. It is\ndemonstrated that while an R-estimator admits a local asymptotic bias, its bias\nsurprisingly depends only on the precision of measurements and does neither\ndepend on the chosen rank test score-generating function nor on the regression\nmodel error distribution. The R-estimators are numerically illustrated and\ncompared with the LSE and $L_1$ estimators in this situation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 16:39:47 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 08:04:36 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Jure\u010dkov\u00e1", "Jana", ""], ["Koul", "Hira L.", ""], ["Navr\u00e1til", "Radim", ""], ["Picek", "Jan", ""]]}, {"id": "1411.3688", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, Kody J.H. Law, Youssef M. Marzouk", "title": "Dimension-independent likelihood-informed MCMC", "comments": null, "journal-ref": "Journal of Computational Physics, 304, 109-137 (2016)", "doi": "10.1016/j.jcp.2015.10.008", "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Bayesian inference problems require exploring the posterior distribution\nof high-dimensional parameters that represent the discretization of an\nunderlying function. This work introduces a family of Markov chain Monte Carlo\n(MCMC) samplers that can adapt to the particular structure of a posterior\ndistribution over functions. Two distinct lines of research intersect in the\nmethods developed here. First, we introduce a general class of\noperator-weighted proposal distributions that are well defined on function\nspace, such that the performance of the resulting MCMC samplers is independent\nof the discretization of the function. Second, by exploiting local Hessian\ninformation and any associated low-dimensional structure in the change from\nprior to posterior distributions, we develop an inhomogeneous discretization\nscheme for the Langevin stochastic differential equation that yields\noperator-weighted proposals adapted to the non-Gaussian structure of the\nposterior. The resulting dimension-independent, likelihood-informed (DILI) MCMC\nsamplers may be useful for a large class of high-dimensional problems where the\ntarget probability measure has a density with respect to a Gaussian reference\nmeasure. Two nonlinear inverse problems are used to demonstrate the efficiency\nof these DILI samplers: an elliptic PDE coefficient inverse problem and path\nreconstruction in a conditioned diffusion.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 20:00:24 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2015 05:11:55 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cui", "Tiangang", ""], ["Law", "Kody J. H.", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1411.3690", "submitter": "Lei Sun", "authors": "David Soave, Andrew Paterson, Lisa Strug, Lei Sun", "title": "A novel joint location-scale testing framework for improved detection of\n  variants with main or interaction effects", "comments": "27 pages, 4 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and easy-to-implement joint location-scale association\ntesting procedure that can account for complex genetic architecture without\nexplicitly modeling interaction effects, and is suitable for large-scale\nwhole-genome scans and meta-analyses. We focus on Fisher's method and use it to\ncombine evidence from the standard location test and the more recent scale\ntest, and we describe its use for single-variant, gene-set and pathway\nassociation analyses.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 20:06:43 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Soave", "David", ""], ["Paterson", "Andrew", ""], ["Strug", "Lisa", ""], ["Sun", "Lei", ""]]}, {"id": "1411.3776", "submitter": "Shawn Mankad", "authors": "Donggeng Xia, Shawn Mankad, George Michailidis", "title": "Measuring Influence in Twitter Ecosystems using a Counting Process\n  Modeling Framework", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data extracted from social media platforms, such as Twitter, are both large\nin scale and complex in nature, since they contain both unstructured text, as\nwell as structured data, such as time stamps and interactions between users. A\nkey question for such platforms is to determine influential users, in the sense\nthat they generate interactions between members of the platform. Common\nmeasures used both in the academic literature and by companies that provide\nanalytics services are variants of the popular web-search PageRank algorithm\napplied to networks that capture connections between users. In this work, we\ndevelop a modeling framework using multivariate interacting counting processes\nto capture the detailed actions that users undertake on such platforms, namely\nposting original content, reposting and/or mentioning other users' postings.\nBased on the proposed model, we also derive a novel influence measure. We\ndiscuss estimation of the model parameters through maximum likelihood and\nestablish their asymptotic properties. The proposed model and the accompanying\ninfluence measure are illustrated on a data set covering a five year period of\nthe Twitter actions of the members of the US Senate, as well as mainstream news\norganizations and media personalities.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 02:09:12 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Xia", "Donggeng", ""], ["Mankad", "Shawn", ""], ["Michailidis", "George", ""]]}, {"id": "1411.3816", "submitter": "Peter Curran", "authors": "P.A. Curran (ICRAR/Curtin)", "title": "Monte Carlo error analyses of Spearman's rank test", "comments": "Unubmitted manuscript (comments welcome); 5 pages; Code available at\n  https://github.com/PACurran/MCSpearman/; Updated with ASCL ID", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM physics.data-an stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Spearman's rank correlation test is commonly used in astronomy to discern\nwhether a set of two variables are correlated or not. Unlike most other\nquantities quoted in astronomical literature, the Spearman's rank correlation\ncoefficient is generally quoted with no attempt to estimate the errors on its\nvalue. This is a practice that would not be accepted for those other\nquantities, as it is often regarded that an estimate of a quantity without an\nestimate of its associated uncertainties is meaningless. This manuscript\ndescribes a number of easily implemented, Monte Carlo based methods to estimate\nthe uncertainty on the Spearman's rank correlation coefficient, or more\nprecisely to estimate its probability distribution.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 07:40:48 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 05:01:04 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Curran", "P. A.", "", "ICRAR/Curtin"]]}, {"id": "1411.3904", "submitter": "Christoph Bandt", "authors": "Christoph Bandt", "title": "Autocorrelation type functions for big and dirty data series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One form of big data are signals - time series of consecutive values. In\nphysical experiments, billions of values can now be measured within a second.\nSignals of heart and brain in intensive care, as well as seismic waves, are\nmeasured with 100 up to 1000 Hz over hours, days or even years. A song of 3\nminutes on CD comprises 16 million values.\n  Music and seismic vibrations basically consist of harmonic oscillations for\nwhich classical tools like autocorrelation and spectrogram work well. This note\npresents similar tools for all kinds of rhythmic processes, with non-linear\ndistortion, artefacts, and outliers. Permutation entropy has been used in\nphysics, medicine, and engineering. Big data allow a detailed analysis of\nordinal patterns. As new version of permutation entropy, we define a distance\nto white noise consisting of four curious components. Applications to a variety\nof data are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 13:31:01 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 12:05:50 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Bandt", "Christoph", ""]]}, {"id": "1411.4158", "submitter": "Hongxiao Zhu", "authors": "Hongxiao Zhu, Nate Strawn and David B. Dunson", "title": "Bayesian Graphical Models for Multivariate Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models express conditional independence relationships among\nvariables. Although methods for vector-valued data are well established,\nfunctional data graphical models remain underdeveloped. We introduce a notion\nof conditional independence between random functions, and construct a framework\nfor Bayesian inference of undirected, decomposable graphs in the multivariate\nfunctional data context. This framework is based on extending Markov\ndistributions and hyper Markov laws from random variables to random processes,\nproviding a principled alternative to naive application of multivariate methods\nto discretized functional data. Markov properties facilitate the composition of\nlikelihoods and priors according to the decomposition of a graph. Our focus is\non Gaussian process graphical models using orthogonal basis expansions. We\npropose a hyper-inverse-Wishart-process prior for the covariance kernels of the\ninfinite coefficient sequences of the basis expansion, establish existence,\nuniqueness, strong hyper Markov property, and conjugacy. Stochastic search\nMarkov chain Monte Carlo algorithms are developed for posterior inference,\nassessed through simulations, and applied to a study of brain activity and\nalcoholism.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 15:50:11 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 19:30:41 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 20:11:39 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Zhu", "Hongxiao", ""], ["Strawn", "Nate", ""], ["Dunson", "David B.", ""]]}, {"id": "1411.4170", "submitter": "Baptiste Gregorutti", "authors": "Baptiste Gregorutti, Bertrand Michel, Philippe Saint-Pierre", "title": "Grouped variable importance with random forests and application to\n  multiple functional data analysis", "comments": null, "journal-ref": "Computational Statistics and Data Analysis, volume 90, 2015, pages\n  15-35", "doi": "10.1016/j.csda.2015.04.002", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of grouped variables using the random forest algorithm is\nconsidered. First a new importance measure adapted for groups of variables is\nproposed. Theoretical insights into this criterion are given for additive\nregression models. Second, an original method for selecting functional\nvariables based on the grouped variable importance measure is developed. Using\na wavelet basis, it is proposed to regroup all of the wavelet coefficients for\na given functional variable and use a wrapper selection algorithm with these\ngroups. Various other groupings which take advantage of the frequency and time\nlocalization of the wavelet basis are proposed. An extensive simulation study\nis performed to illustrate the use of the grouped importance measure in this\ncontext. The method is applied to a real life problem coming from aviation\nsafety.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 18:08:56 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 08:39:23 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Gregorutti", "Baptiste", ""], ["Michel", "Bertrand", ""], ["Saint-Pierre", "Philippe", ""]]}, {"id": "1411.4257", "submitter": "Riccardo Rastelli", "authors": "Marco Bertoletti, Nial Friel, Riccardo Rastelli", "title": "Choosing the number of clusters in a finite mixture model using an exact\n  Integrated Completed Likelihood criterion", "comments": "23 pages, to appear in Metron", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrated completed likelihood (ICL) criterion has proven to be a very\npopular approach in model-based clustering through automatically choosing the\nnumber of clusters in a mixture model. This approach effectively maximises the\ncomplete data likelihood, thereby including the allocation of observations to\nclusters in the model selection criterion. However for practical implementation\none needs to introduce an approximation in order to estimate the ICL. Our\ncontribution here is to illustrate that through the use of conjugate priors one\ncan derive an exact expression for ICL and so avoiding any approximation.\nMoreover, we illustrate how one can find both the number of clusters and the\nbest allocation of observations in one algorithmic framework. The performance\nof our algorithm is presented on several simulated and real examples.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 13:27:20 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 17:10:37 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Bertoletti", "Marco", ""], ["Friel", "Nial", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "1411.4277", "submitter": "Li Yin", "authors": "Li Yin and Xiaoqin Wang", "title": "Estimating Net Effects of Treatments in Treatment Sequence without the\n  Assumption of Strongly Ignorable Treatment Assignment", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.1194", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sequential causal inference, one estimates the causal net effect of\ntreatment in treatment sequence on an outcome after last treatment in the\npresence of time-dependent covariates between treatments, improves the\nestimation by the untestable assumption of strongly ignorable treatment\nassignment, and obtains consistent but non-genuine likelihood-based estimate.\nIn this article, we introduce the net effect of treatment as parameter for the\nconditional distribution of outcome given all treatments and time-dependent\ncovariates and show that it is equal to the causal net effect of treatment\nunder the assumption of strongly ignorable treatment assignment. As a result,\nwe can estimate the net effect of treatment and evaluate its causal\ninterpretation in two separate steps. The first step is fucus of this article\nwhile the second step can be accomplished by usual sensitivity analyses. We\nconstruct point parametrization for the conditional outcome distribution in\nwhich the parameters of interest are the point effects of single-point\ntreatments. With point parametrization and without the untestable assumption,\nwe estimate the net effect of treatment by maximum likelihood, improve the\nestimation by testable pattern of the net effect of treatment, and obtain\nunbiased consistent maximum-likelihood estimate for the net effect of treatment\nwith finite-dimensional pattern.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 16:59:13 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Yin", "Li", ""], ["Wang", "Xiaoqin", ""]]}, {"id": "1411.4411", "submitter": "Antonio Forcina", "authors": "Antonio Forcina, Davide Pellegrino", "title": "Ecological fallacy and covariates in the estimation of voters\n  transitions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple formulation of the conditions under which ecological bias\nshould be expected and argue that the bias will affect any method of ecological\ninference; our claim is supported by formal derivations and several examples\nwhere individual data are available. The conditions which we highlight imply\nthat, when they are violated, ecological bias cannot be avoided unless the a\nsuitable model for the effect of specific covariates is incorporated into\necological inference. We also detect situations where the ecological bias\ncannot be corrected even if the effect of covariates is incorporated into the\nmodel. In particular, when the association in the individual data is rather\nweak and certain transition probabilities are similar functions of a given\ncovariate, ecological inference methods may be unable to disentangle the\nindividual components from their aggregate. In any case, the value of the\ncovariates measured at the level of local units, when these are rather\nextensive, do not provide enough information on the within unit heterogeneity.\nOur findings are applied and tested on several data sets, both real and\nsimulated, where individual observations are available in addition to the\naggregated ones.,.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 10:08:57 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 09:53:25 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 16:20:08 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 07:07:45 GMT"}, {"version": "v5", "created": "Mon, 24 Sep 2018 15:09:01 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Forcina", "Antonio", ""], ["Pellegrino", "Davide", ""]]}, {"id": "1411.4564", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Natalya Pya and Simon N. Wood", "title": "A comparison of inferential methods for highly non-linear state space\n  models in ecology and epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly non-linear, chaotic or near chaotic, dynamic models are important in\nfields such as ecology and epidemiology: for example, pest species and diseases\noften display highly non-linear dynamics. However, such models are problematic\nfrom the point of view of statistical inference. The defining feature of\nchaotic and near chaotic systems is extreme sensitivity to small changes in\nsystem states and parameters, and this can interfere with inference. There are\ntwo main classes of methods for circumventing these difficulties: information\nreduction approaches, such as Approximate Bayesian Computation or Synthetic\nLikelihood and state space methods, such as Particle Markov chain Monte Carlo,\nIterated Filtering or Parameter Cascading. The purpose of this article is to\ncompare the methods, in order to reach conclusions about how to approach\ninference with such models in practice. We show that neither class of methods\nis universally superior to the other. We show that state space methods can\nsuffer multimodality problems in settings with low process noise or model\nmis-specification, leading to bias toward stable dynamics and high process\nnoise. Information reduction methods avoid this problem but, under the correct\nmodel and with sufficient process noise, state space methods lead to\nsubstantially sharper inference than information reduction methods. More\npractically, there are also differences in the tuning requirements of different\nmethods. Our overall conclusion is that model development and checking should\nprobably be performed using an information reduction method with low tuning\nrequirements, while for final inference it is likely to be better to switch to\na state space method, checking results against the information reduction\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:34:25 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 10:36:54 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Pya", "Natalya", ""], ["Wood", "Simon N.", ""]]}, {"id": "1411.4708", "submitter": "Charles R Doss", "authors": "Fadoua Balabdaoui and Charles R. Doss", "title": "Inference for a Two-Component Mixture of Symmetric Distributions under\n  Log-Concavity", "comments": "47 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we revisit the problem of estimating the unknown\nzero-symmetric distribution in a two-component location mixture model,\nconsidered in previous works, now under the assumption that the zero-symmetric\ndistribution has a log-concave density. When consistent estimators for the\nshift locations and mixing probability are used, we show that the nonparametric\nlog-concave Maximum Likelihood estimator (MLE) of both the mixed density and\nthat of the unknown zero-symmetric component are consistent in the Hellinger\ndistance. In case the estimators for the shift locations and mixing probability\nare $\\sqrt n$-consistent, we establish that these MLE's converge to the truth\nat the rate $n^{-2/5}$ in the $L_1$ distance. To estimate the shift locations\nand mixing probability, we use the estimators proposed by\n\\cite{hunteretal2007}. The unknown zero-symmetric density is efficiently\ncomputed using the \\proglang{R} package \\pkg{logcondens.mode}.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 01:29:01 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 21:54:18 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 02:41:49 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Doss", "Charles R.", ""]]}, {"id": "1411.4715", "submitter": "Yang Liu", "authors": "Yang Liu and Philip Kokic", "title": "Predictive Inference for Spatio-temporal Precipitation Data and Its\n  Extremes", "comments": "Under review at Journal of the American Statistical Association. 27\n  pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling of precipitation and its extremes is important for urban and\nagriculture planning purposes. We present a method for producing spatial\npredictions and measures of uncertainty for spatio-temporal data that is\nheavy-tailed and subject to substaintial skewness which often arise in\nmeasurements of many environmental processes, and we apply the method to\nprecipitation data in south-west Western Australia. A generalised hyperbolic\nBayesian hierarchical model is constructed for the intensity, frequency and\nduration of daily precipitation, including the extremes. Unlike models based on\nextreme value theory, which only model maxima of finite-sized blocks or\nexceedances above a large threshold, the proposed model uses all the data\navailable efficiently, and hence not only fits the extremes but also models the\nentire rainfall distribution. It captures spatial and temporal clustering, as\nwell as spatially and temporally varying volatility and skewness. The model\nassumes that the regional precipitation is driven by a latent process\ncharacterised by geographical and climatological covariates. Effects not fully\ndescribed by the covariates are captured by spatial and temporal structure in\nthe hierarchies. Inference is provided by MCMC using a Metropolis-Hastings\nalgorithm and spatial interpolation method, which provide a natural approach\nfor estimating uncertainty. Similarly both spatial and temporal predictions\nwith uncertainty can be produced with the model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 01:54:21 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Liu", "Yang", ""], ["Kokic", "Philip", ""]]}, {"id": "1411.4723", "submitter": "Raymond K. W. Wong", "authors": "Raymond K. W. Wong, Curtis B. Storlie, Thomas C. M. Lee", "title": "A Frequentist Approach to Computer Model Calibration", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the computer model calibration problem and provides a\ngeneral frequentist solution. Under the proposed framework, the data model is\nsemi-parametric with a nonparametric discrepancy function which accounts for\nany discrepancy between the physical reality and the computer model. In an\nattempt to solve a fundamentally important (but often ignored) identifiability\nissue between the computer model parameters and the discrepancy function, this\npaper proposes a new and identifiable parametrization of the calibration\nproblem. It also develops a two-step procedure for estimating all the relevant\nquantities under the new parameterization. This estimation procedure is shown\nto enjoy excellent rates of convergence and can be straightforwardly\nimplemented with existing software. For uncertainty quantification,\nbootstrapping is adopted to construct confidence regions for the quantities of\ninterest. The practical performance of the proposed methodology is illustrated\nthrough simulation examples and an application to a computational fluid\ndynamics model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 03:12:31 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 02:49:56 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Wong", "Raymond K. W.", ""], ["Storlie", "Curtis B.", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1411.4809", "submitter": "D. Michele  Cifarelli", "authors": "D.M. Cifarelli", "title": "Estimation of the regression slope by means of Gini's cograduation index", "comments": "Translation from Italian of the paper: Cifarelli, D.M. (1978). \"La\n  stima del coefficiente di regressione mediante l'indice di cograduazione di\n  Gini\"", "journal-ref": "Rivista di matematica per le scienze economiche e sociali (now:\n  Decisions in economics and finance). 1978, 1, 7-38", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simple linear model $$Y_i = \\alpha + \\beta \\, x_i + \\epsilon_i \\qquad\ni=1,2, \\ldots,N \\geq 2$$ is considered, where the $x_i$'s are given constants\nand $\\epsilon_1, \\epsilon_2 , \\ldots, \\epsilon_N$ are iid with continuous\ndistribution function $F$. An estimator of $\\beta$ is proposed, based on Gini's\nrank association coefficient $G(\\underline y;b)$ and defined as $\\tilde{\\beta}\n= \\frac 12 \\, \\left\\{ \\sup (b: G(\\underline y;b) >0) + \\right. $ $ \\left. \\inf\n(b: G(\\underline y;b) <0) \\right\\}.$ The properties of $\\tilde{\\beta}$ and of\nthe related confidence interval are studied. Some comparisons are given, in\nterms of asymptotic relative efficiency, with other estimators of $\\beta$\nincluding that obtained with the method of least squares.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 11:16:34 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Cifarelli", "D. M.", ""]]}, {"id": "1411.4899", "submitter": "Ehsan Zamanzade", "authors": "Ehsan Zamanzade, Nasser Reza Arghami and Michael Vock", "title": "Permutation-Based Tests of Perfect Ranking", "comments": null, "journal-ref": "Statistics & probability letters, 82, 3313-2220 (2012)", "doi": "10.1016/j.spl.2012.07.022", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve three tests of perfect ranking in ranked set sampling proposed by\nLi and Balakrishnan (2008) using a permutation approach. This simple way of\nextending all three concepts to comparisons across different cycles increases\nthe power. Two of the proposed tests are equivalent to tests from the\nliterature, which were derived differently and are therefore generalized by the\npermutation-based tests.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 16:31:39 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Zamanzade", "Ehsan", ""], ["Arghami", "Nasser Reza", ""], ["Vock", "Michael", ""]]}, {"id": "1411.5259", "submitter": "Patrick Kimes", "authors": "Patrick K. Kimes, Yufeng Liu, D. Neil Hayes and J. S. Marron", "title": "Statistical Significance for Hierarchical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis has proved to be an invaluable tool for the exploratory and\nunsupervised analysis of high dimensional datasets. Among methods for\nclustering, hierarchical approaches have enjoyed substantial popularity in\ngenomics and other fields for their ability to simultaneously uncover multiple\nlayers of clustering structure. A critical and challenging question in cluster\nanalysis is whether the identified clusters represent important underlying\nstructure or are artifacts of natural sampling variation. Few approaches have\nbeen proposed for addressing this problem in the context of hierarchical\nclustering, for which the problem is further complicated by the natural tree\nstructure of the partition, and the multiplicity of tests required to parse the\nlayers of nested clusters. In this paper, we propose a Monte Carlo based\napproach for testing statistical significance in hierarchical clustering which\naddresses these issues. The approach is implemented as a sequential testing\nprocedure guaranteeing control of the family-wise error rate. Theoretical\njustification is provided for our approach, and its power to detect true\nclustering structure is illustrated through several simulation studies and\napplications to two cancer gene expression datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 15:45:31 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Kimes", "Patrick K.", ""], ["Liu", "Yufeng", ""], ["Hayes", "D. Neil", ""], ["Marron", "J. S.", ""]]}, {"id": "1411.5279", "submitter": "Tim Hesterberg", "authors": "Tim Hesterberg", "title": "What Teachers Should Know about the Bootstrap: Resampling in the\n  Undergraduate Statistics Curriculum", "comments": "83 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I have three goals in this article: (1) To show the enormous potential of\nbootstrapping and permutation tests to help students understand statistical\nconcepts including sampling distributions, standard errors, bias, confidence\nintervals, null distributions, and P-values. (2) To dig deeper, understand why\nthese methods work and when they don't, things to watch out for, and how to\ndeal with these issues when teaching. (3) To change statistical practice---by\ncomparing these methods to common t tests and intervals, we see how inaccurate\nthe latter are; we confirm this with asymptotics. n >= 30 isn't enough---think\nn >= 5000. Resampling provides diagnostics, and more accurate alternatives.\nSadly, the common bootstrap percentile interval badly under-covers in small\nsamples; there are better alternatives. The tone is informal, with a few\nstories and jokes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 16:22:18 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Hesterberg", "Tim", ""]]}, {"id": "1411.5310", "submitter": "BaoLuo Sun", "authors": "BaoLuo Sun and Eric J. Tchetgen Tchetgen", "title": "On Inverse Probability Weighting for Nonmonotone Missing at Random Data", "comments": null, "journal-ref": "Journal of the American Statistical Association 113(2018) 369-379", "doi": "10.1080/01621459.2016.1256814", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of coherent missing data models to account for nonmonotone\nmissing at random (MAR) data by inverse probability weighting (IPW) remains to\ndate largely unresolved. As a consequence, IPW has essentially been restricted\nfor use only in monotone missing data settings. We propose a class of models\nfor nonmonotone missing data mechanisms that spans the MAR model, while\nallowing the underlying full data law to remain unrestricted. For parametric\nspecifications within the proposed class, we introduce an unconstrained maximum\nlikelihood estimator for estimating the missing data probabilities which can be\neasily implemented using existing software. To circumvent potential convergence\nissues with this procedure, we also introduce a Bayesian constrained approach\nto estimate the missing data process which is guaranteed to yield inferences\nthat respect all model restrictions. The efficiency of the standard IPW\nestimator is improved by incorporating information from incomplete cases\nthrough an augmented estimating equation which is optimal within a large class\nof estimating equations. We investigate the finite-sample properties of the\nproposed estimators in a simulation study and illustrate the new methodology in\nan application evaluating key correlates of preterm delivery for infants born\nto HIV infected mothers in Botswana, Africa.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 18:38:20 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2015 18:54:27 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sun", "BaoLuo", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1411.5404", "submitter": "Kevin Xu", "authors": "Kevin S. Xu", "title": "Stochastic Block Transition Models for Dynamic Networks", "comments": "To appear in proceedings of AISTATS 2015", "journal-ref": "Proceedings of the 18th International Conference on Artificial\n  Intelligence and Statistics (2015) 1079-1087", "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great interest in recent years on statistical models for\ndynamic networks. In this paper, I propose a stochastic block transition model\n(SBTM) for dynamic networks that is inspired by the well-known stochastic block\nmodel (SBM) for static networks and previous dynamic extensions of the SBM.\nUnlike most existing dynamic network models, it does not make a hidden Markov\nassumption on the edge-level dynamics, allowing the presence or absence of\nedges to directly influence future edge probabilities while retaining the\ninterpretability of the SBM. I derive an approximate inference procedure for\nthe SBTM and demonstrate that it is significantly better at reproducing\ndurations of edges in real social network data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 23:30:36 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 00:08:17 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Xu", "Kevin S.", ""]]}, {"id": "1411.5625", "submitter": "Erika  Gomes Goncalves", "authors": "Erika Gomes-Gon\\c{c}alves (UC3M), Henryk Gzyl (IESA) and Silvia\n  Mayoral (UC3M)", "title": "Two maxentropic approaches to determine the probability density of\n  compound risk losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present an application of two maxentropic procedures to determine the\nprobability density distribution of compound sums of random variables, using\nonly a finite number of empirically determined fractional moments. The two\nmethods are the Standard method of Maximum Entropy (SME), and the method of\nMaximum Entropy in the Mean (MEM). We shall verify that the reconstructions\nobtained satisfy a variety of statistical quality criteria, and provide good\nestimations of VaR and TVaR, which are important measures for risk management\npurposes. We analyze the performance and robustness of these two procedures in\nseveral numerical examples, in which the frequency of losses is Poisson and the\nindividual losses are lognormal random variables. As side product of the work,\nwe obtain a rather accurate description of the density of the compound random\nvariable. This is an extension of a previous application based on the Standard\nMaximum Entropy approach (SME) where the analytic form of the Laplace transform\nwas available to a case in which only observed or simulated data is used. These\napproaches are also used to develop a procedure to determine the distribution\nof the individual losses through the knowledge of the total loss. Then, in the\ncase of having only historical total losses, it is possible to decompound or\ndisaggregate the random sums in its frequency/severity distributions, through a\nprobabilistic inverse problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 18:04:26 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 14:07:30 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Gomes-Gon\u00e7alves", "Erika", "", "UC3M"], ["Gzyl", "Henryk", "", "IESA"], ["Mayoral", "Silvia", "", "UC3M"]]}, {"id": "1411.5653", "submitter": "Bani Mallick", "authors": "Richard D. Payne, Bani K. Mallick", "title": "Two-Stage Metropolis-Hastings for Tall Data", "comments": "To appear in Journal of Classification, Volume 35, Issue 1 (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the challenges presented by tall data problems\nassociated with Bayesian classification (specifically binary classification)\nand the existing methods to handle them. Current methods include parallelizing\nthe likelihood, subsampling, and consensus Monte Carlo. A new method based on\nthe two-stage Metropolis-Hastings algorithm is also proposed. The purpose of\nthis algorithm is to reduce the exact likelihood computational cost in the tall\ndata situation. In the first stage, a new proposal is tested by the approximate\nlikelihood based model. The full likelihood based posterior computation will be\nconducted only if the proposal passes the first stage screening. Furthermore,\nthis method can be adopted into the consensus Monte Carlo framework. The\ntwo-stage method is applied to logistic regression, hierarchical logistic\nregression, and Bayesian multivariate adaptive regression splines.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 19:48:12 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 15:56:21 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 21:17:17 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Payne", "Richard D.", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1411.5720", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola", "title": "Metric recovery from directed unweighted graphs", "comments": "Poster at NIPS workshop on networks. Submitted to AISTATS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze directed, unweighted graphs obtained from $x_i\\in \\mathbb{R}^d$ by\nconnecting vertex $i$ to $j$ iff $|x_i - x_j| < \\epsilon(x_i)$. Examples of\nsuch graphs include $k$-nearest neighbor graphs, where $\\epsilon(x_i)$ varies\nfrom point to point, and, arguably, many real world graphs such as\nco-purchasing graphs. We ask whether we can recover the underlying Euclidean\nmetric $\\epsilon(x_i)$ and the associated density $p(x_i)$ given only the\ndirected graph and $d$.\n  We show that consistent recovery is possible up to isometric scaling when the\nvertex degree is at least $\\omega(n^{2/(2+d)}\\log(n)^{d/(d+2)})$. Our estimator\nis based on a careful characterization of a random walk over the directed graph\nand the associated continuum limit. As an algorithm, it resembles the PageRank\ncentrality metric. We demonstrate empirically that the estimator performs well\non simulated examples as well as on real-world co-purchasing graphs even with a\nsmall number of points and degree scaling as low as $\\log(n)$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 23:16:09 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Sun", "Yi", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1411.5725", "submitter": "Wesley Brooks", "authors": "Wesley Brooks and Jun Zhu and Zudi Lu", "title": "Local Adaptive Grouped Regularization and its Oracle Properties for\n  Varying Coefficient Regression", "comments": "30 pages, one technical appendix, two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying coefficient regression is a flexible technique for modeling data\nwhere the coefficients are functions of some effect-modifying parameter, often\ntime or location in a certain domain. While there are a number of methods for\nvariable selection in a varying coefficient regression model, the existing\nmethods are mostly for global selection, which includes or excludes each\ncovariate over the entire domain. Presented here is a new local adaptive\ngrouped regularization (LAGR) method for local variable selection in spatially\nvarying coefficient linear and generalized linear regression. LAGR selects the\ncovariates that are associated with the response at any point in space, and\nsimultaneously estimates the coefficients of those covariates by tailoring the\nadaptive group Lasso toward a local regression model with locally linear\ncoefficient estimates. Oracle properties of the proposed method are established\nunder local linear regression and local generalized linear regression. The\nfinite sample properties of LAGR are assessed in a simulation study and for\nillustration, the Boston housing price data set is analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 23:42:21 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Brooks", "Wesley", ""], ["Zhu", "Jun", ""], ["Lu", "Zudi", ""]]}, {"id": "1411.5787", "submitter": "Zhenke Wu", "authors": "Zhenke Wu, Constantine E. Frangakis, Thomas A. Louis, and Daniel O.\n  Scharfstein", "title": "Estimation of Treatment Effects in Matched-Pair Cluster Randomized\n  Trials by Calibrating Covariate Imbalance between Clusters", "comments": "27 pages, 3 figures, 3 tables", "journal-ref": null, "doi": "10.1111/biom.12214", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address estimation of intervention effects in experimental designs in\nwhich (a) interventions are assigned at the cluster level; (b) clusters are\nselected to form pairs, matched on observed characteristics; and (c)\nintervention is assigned to one cluster at random within each pair. One goal of\npolicy interest is to estimate the average outcome if all clusters in all pairs\nare assigned control versus if all clusters in all pairs are assigned to\nintervention. In such designs, inference that ignores individual level\ncovariates can be imprecise because cluster-level assignment can leave\nsubstantial imbalance in the covariate distribution between experimental arms\nwithin each pair. However, most existing methods that adjust for covariates\nhave estimands that are not of policy interest. We propose a methodology that\nexplicitly balances the observed covariates among clusters in a pair, and\nretains the original estimand of interest. We demonstrate our approach through\nthe evaluation of the Guided Care program.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 08:05:54 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Wu", "Zhenke", ""], ["Frangakis", "Constantine E.", ""], ["Louis", "Thomas A.", ""], ["Scharfstein", "Daniel O.", ""]]}, {"id": "1411.5876", "submitter": "Kari Heine", "authors": "Kari Heine, Nick Whiteley, A. Taylan Cemgil and Hakan Guldas", "title": "Butterfly resampling: asymptotics for particle filters with constrained\n  interactions", "comments": "29 pages, supplementary material (46 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the elementary mechanism of sampling with replacement $N$ times\nfrom a weighted population of size $N$, by introducing auxiliary variables and\nconstraints on conditional independence characterised by modular congruence\nrelations. Motivated by considerations of parallelism, a convergence study\nreveals how sparsity of the mechanism's conditional independence graph is\nrelated to fluctuation properties of particle filters which use it for\nresampling, in some cases exhibiting exotic scaling behaviour. The proofs\ninvolve detailed combinatorial analysis of conditional independence graphs.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 14:03:24 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Heine", "Kari", ""], ["Whiteley", "Nick", ""], ["Cemgil", "A. Taylan", ""], ["Guldas", "Hakan", ""]]}, {"id": "1411.6144", "submitter": "Wesley Tansey", "authors": "Wesley Tansey and Oluwasanmi Koyejo and Russell A. Poldrack and James\n  G. Scott", "title": "False discovery rate smoothing", "comments": "Added misspecification analysis, added pathological scenario\n  discussions, additional comparisons, new graph fused lasso algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present false discovery rate smoothing, an empirical-Bayes method for\nexploiting spatial structure in large multiple-testing problems. FDR smoothing\nautomatically finds spatially localized regions of significant test statistics.\nIt then relaxes the threshold of statistical significance within these regions,\nand tightens it elsewhere, in a manner that controls the overall\nfalse-discovery rate at a given level. This results in increased power and\ncleaner spatial separation of signals from noise. The approach requires solving\na non-standard high-dimensional optimization problem, for which an efficient\naugmented-Lagrangian algorithm is presented. In simulation studies, FDR\nsmoothing exhibits state-of-the-art performance at modest computational cost.\nIn particular, it is shown to be far more robust than existing methods for\nspatially dependent multiple testing. We also apply the method to a data set\nfrom an fMRI experiment on spatial working memory, where it detects patterns\nthat are much more biologically plausible than those detected by standard\nFDR-controlling methods. All code for FDR smoothing is publicly available in\nPython and R.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 17:17:46 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 18:32:39 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tansey", "Wesley", ""], ["Koyejo", "Oluwasanmi", ""], ["Poldrack", "Russell A.", ""], ["Scott", "James G.", ""]]}, {"id": "1411.6219", "submitter": "Anirvan Chakraborty Mr.", "authors": "Anirvan Chakraborty and Probal Chaudhuri", "title": "Paired sample tests in infinite dimensional spaces", "comments": "21 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sign and the signed-rank tests for univariate data are perhaps the most\npopular nonparametric competitors of the t test for paired sample problems.\nThese tests have been extended in various ways for multivariate data in finite\ndimensional spaces. These extensions include tests based on spatial signs and\nsigned ranks, which have been studied extensively by Hannu Oja and his\ncoauthors. They showed that these tests are asymptotically more powerful than\nHotelling's $T^{2}$ test under several heavy tailed distributions. In this\npaper, we consider paired sample tests for data in infinite dimensional spaces\nbased on notions of spatial sign and spatial signed rank in such spaces. We\nderive their asymptotic distributions under the null hypothesis and under\nsequences of shrinking location shift alternatives. We compare these tests with\nsome mean based tests for infinite dimensional paired sample data. We show that\nfor shrinking location shift alternatives, the proposed tests are\nasymptotically more powerful than the mean based tests for some heavy tailed\ndistributions and even for some Gaussian distributions in infinite dimensional\nspaces. We also investigate the performance of different tests using some\nsimulated data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 10:07:50 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Chakraborty", "Anirvan", ""], ["Chaudhuri", "Probal", ""]]}, {"id": "1411.6370", "submitter": "Jun Zhu", "authors": "Jun Zhu, Jianfei Chen, Wenbo Hu, Bo Zhang", "title": "Big Learning with Bayesian Methods", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explosive growth in data and availability of cheap computing resources have\nsparked increasing interest in Big learning, an emerging subfield that studies\nscalable machine learning algorithms, systems, and applications with Big Data.\nBayesian methods represent one important class of statistic methods for machine\nlearning, with substantial recent developments on adaptive, flexible and\nscalable Bayesian learning. This article provides a survey of the recent\nadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,\nincluding nonparametric Bayesian methods for adaptively inferring model\ncomplexity, regularized Bayesian inference for improving the flexibility via\nposterior regularization, and scalable algorithms and systems based on\nstochastic subsampling and distributed computing for dealing with large-scale\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:28:51 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:07:26 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Jianfei", ""], ["Hu", "Wenbo", ""], ["Zhang", "Bo", ""]]}, {"id": "1411.6506", "submitter": "Daniele Durante", "authors": "Daniele Durante and David B. Dunson", "title": "Bayesian Inference and Testing of Group Differences in Brain Networks", "comments": null, "journal-ref": "Bayesian Analysis (2018). 13, 29-58", "doi": "10.1214/16-BA1030", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data are increasingly collected along with other variables of\ninterest. Our motivation is drawn from neurophysiology studies measuring brain\nconnectivity networks for a sample of individuals along with their membership\nto a low or high creative reasoning group. It is of paramount importance to\ndevelop statistical methods for testing of global and local changes in the\nstructural interconnections among brain regions across groups. We develop a\ngeneral Bayesian procedure for inference and testing of group differences in\nthe network structure, which relies on a nonparametric representation for the\nconditional probability mass function associated with a network-valued random\nvariable. By leveraging a mixture of low-rank factorizations, we allow simple\nglobal and local hypothesis testing adjusting for multiplicity. An efficient\nGibbs sampler is defined for posterior computation. We provide theoretical\nresults on the flexibility of the model and assess testing performance in\nsimulations. The approach is applied to provide novel insights on the\nrelationships between human brain networks and creativity.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:10:13 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 12:54:19 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 17:59:24 GMT"}, {"version": "v4", "created": "Sat, 30 Jan 2016 17:21:14 GMT"}, {"version": "v5", "created": "Wed, 17 Aug 2016 11:12:02 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "1411.6507", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Christian Hansen and\n  Damian Kozbur", "title": "Inference in High Dimensional Panel Models with an Application to Gun\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation and inference in panel data models with additive\nunobserved individual specific heterogeneity in a high dimensional setting. The\nsetting allows the number of time varying regressors to be larger than the\nsample size. To make informative estimation and inference feasible, we require\nthat the overall contribution of the time varying variables after eliminating\nthe individual specific heterogeneity can be captured by a relatively small\nnumber of the available variables whose identities are unknown. This\nrestriction allows the problem of estimation to proceed as a variable selection\nproblem. Importantly, we treat the individual specific heterogeneity as fixed\neffects which allows this heterogeneity to be related to the observed time\nvarying variables in an unspecified way and allows that this heterogeneity may\nbe non-zero for all individuals. Within this framework, we provide procedures\nthat give uniformly valid inference over a fixed subset of parameters in the\ncanonical linear fixed effects model and over coefficients on a fixed vector of\nendogenous variables in panel data instrumental variables models with fixed\neffects and many instruments. An input to developing the properties of our\nproposed procedures is the use of a variant of the Lasso estimator that allows\nfor a grouped data structure where data across groups are independent and\ndependence within groups is unrestricted. We provide formal conditions within\nthis structure under which the proposed Lasso variant selects a sparse model\nwith good approximation properties. We present simulation results in support of\nthe theoretical developments and illustrate the use of the methods in an\napplication aimed at estimating the effect of gun prevalence on crime rates.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:10:40 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""], ["Kozbur", "Damian", ""]]}, {"id": "1411.6512", "submitter": "Adrian Dobra", "authors": "Adrian Dobra", "title": "Graphical Modeling of Spatial Health Data", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on Gaussian graphical models (GGMs) contains two equally rich\nand equally significant domains of research efforts and interests. The first\nresearch domain relates to the problem of graph determination. That is, the\nunderlying graph is unknown and needs to be inferred from the data. The second\nresearch domain dominates the applications in spatial epidemiology. In this\ncontext GGMs are typically referred to as Gaussian Markov random fields\n(GMRFs). Here the underlying graph is assumed to be known: the vertices\ncorrespond to geographical areas, while the edges are associated with areas\nthat are considered to be neighbors of each other (e.g., if they share a\nborder). We introduce multi-way Gaussian graphical models that unify the\nstatistical approaches to inference for spatiotemporal epidemiology with the\nliterature on general GGMs. The novelty of the proposed work consists of the\naddition of the G-Wishart distribution to the substantial collection of\nstatistical tools used to model multivariate areal data. As opposed to fixed\ngraphs that describe geography, there is an inherent uncertainty related to\ngraph determination across the other dimensions of the data. Our new class of\nmethods for spatial epidemiology allow the simultaneous use of GGMs to\nrepresent known spatial dependencies and to determine unknown dependencies in\nthe other dimensions of the data. KEYWORDS: Gaussian graphical models, Gaussian\nMarkov random fields, spatiotemporal multivariate models\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:23:12 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Dobra", "Adrian", ""]]}, {"id": "1411.6529", "submitter": "Tiancheng Li", "authors": "Tiancheng Li", "title": "The Optimal Arbitrary-Proportional Finite-Set-Partitioning", "comments": null, "journal-ref": "Frontiers of Information Technology & Electronic Engineering,\n  Volume 16, Issue 11, pp 969-984 (2015)", "doi": "10.1631/FITEE.1500199", "report-no": null, "categories": "cs.NA stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the arbitrary-proportional finite-set-partitioning\nproblem which involves partitioning a finite set into multiple subsets with\nrespect to arbitrary nonnegative proportions. This is the core art of many\nfundamental problems such as determining quotas for different individuals of\ndifferent weights or sampling from a discrete-valued weighted sample set to get\na new identically distributed but non-weighted sample set (e.g. the resampling\nneeded in the particle filter). The challenge raises as the size of each subset\nmust be an integer while its unbiased expectation is often not. To solve this\nproblem, a metric (cost function) is defined on their discrepancies and\ncorrespondingly a solution is proposed to determine the sizes of each subsets,\ngaining the minimal bias. Theoretical proof and simulation demonstrations are\nprovided to demonstrate the optimality of the scheme in the sense of the\nproposed metric.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:56:28 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Li", "Tiancheng", ""]]}, {"id": "1411.6578", "submitter": "James Watson", "authors": "James Watson, Luis Nieto-Barajas and Chris Holmes", "title": "Characterising variation of nonparametric random probability measures\n  using the Kullback-Leibler divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the variation in Kullback-Leibler divergence between random\ndraws from some popular nonparametric processes and their baseline measure. In\nparticular we focus on the Dirichlet process, the P\\'olya tree and the\nfrequentist and Bayesian bootstrap. The results shed light on the support of\nthese nonparametric processes. Of particular note are results for finite\nP\\'olya trees that are used to model continuous random probability measures.\nOur results provide guidance for specifying the parameterisation of the P\\'olya\ntree process that allows for greater understanding while highlighting\nlimitations of the standard canonical choice of parameter settings.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 19:18:39 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Watson", "James", ""], ["Nieto-Barajas", "Luis", ""], ["Holmes", "Chris", ""]]}, {"id": "1411.6669", "submitter": "Michael Betancourt", "authors": "M.J. Betancourt, Simon Byrne, and Mark Girolami", "title": "Optimizing The Integrator Step Size for Hamiltonian Monte Carlo", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo can provide powerful inference in complex statistical\nproblems, but ultimately its performance is sensitive to various tuning\nparameters. In this paper we use the underlying geometry of Hamiltonian Monte\nCarlo to construct a universal optimization criteria for tuning the step size\nof the symplectic integrator crucial to any implementation of the algorithm as\nwell as diagnostics to monitor for any signs of invalidity. An immediate\noutcome of this result is that the suggested target average acceptance\nprobability of 0.651 can be relaxed to $0.6 \\lesssim a \\lesssim 0.9$ with\nlarger values more robust in practice.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 22:13:53 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 16:27:20 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Betancourt", "M. J.", ""], ["Byrne", "Simon", ""], ["Girolami", "Mark", ""]]}, {"id": "1411.6719", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Athina P. Petropulu", "title": "Asymptotically Optimal Discrete Time Nonlinear Filters From\n  Stochastically Convergent State Process Approximations", "comments": "EXTENDED version of an original paper published in the IEEE\n  Transactions on Signal Processing; 37 pages", "journal-ref": null, "doi": "10.1109/TSP.2015.2428220", "report-no": null, "categories": "math.ST cs.SY math.OC stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating optimal in the Minimum Mean Squared\nError (MMSE) sense nonlinear filters in a discrete time setting, exploiting\nproperties of stochastically convergent state process approximations. More\nspecifically, we consider a class of nonlinear, partially observable stochastic\nsystems, comprised by a (possibly nonstationary) hidden stochastic process (the\nstate), observed through another conditionally Gaussian stochastic process (the\nobservations). Under general assumptions, we show that, given an approximating\nprocess which, for each time step, is stochastically convergent to the state\nprocess, an approximate filtering operator can be defined, which converges to\nthe true optimal nonlinear filter of the state in a strong and well defined\nsense. In particular, the convergence is compact in time and uniform in a\ncompletely characterized measurable set of probability measure almost unity,\nalso providing a purely quantitative justification of Egoroff's Theorem for the\nproblem at hand. The results presented in this paper can form a common basis\nfor the analysis and characterization of a number of heuristic approaches for\napproximating optimal nonlinear filters, such as approximate grid based\ntechniques, known to perform well in a variety of applications.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 03:49:25 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 11:26:47 GMT"}, {"version": "v3", "created": "Sun, 1 May 2016 18:30:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1411.6927", "submitter": "Pavlo Mozharovskyi", "authors": "Rainer Dyckerhoff, Pavlo Mozharovskyi", "title": "Exact computation of the halfspace depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For computing the exact value of the halfspace depth of a point w.r.t. a data\ncloud of $n$ points in arbitrary dimension, a theoretical framework is\nsuggested. Based on this framework a whole class of algorithms can be derived.\nIn all of these algorithms the depth is calculated as the minimum over a finite\nnumber of depth values w.r.t. proper projections of the data cloud. Three\nvariants of this class are studied in more detail. All of these algorithms are\ncapable of dealing with data that are not in general position and even with\ndata that contain ties. As is shown by simulations, all proposed algorithms\nprove to be very efficient.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 17:20:06 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 15:21:03 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 15:31:38 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Dyckerhoff", "Rainer", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "1411.6948", "submitter": "Wenwen Zhang", "authors": "Wenwen Zhang and Wei-Yin Loh", "title": "PLUTO: Penalized Unbiased Logistic Regression Trees", "comments": "59 pages, 25 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm called PLUTO for building logistic regression\ntrees to binary response data. PLUTO can capture the nonlinear and interaction\npatterns in messy data by recursively partitioning the sample space. It fits a\nsimple or a multiple linear logistic regression model in each partition. PLUTO\nemploys the cyclical coordinate descent method for estimation of multiple\nlinear logistic regression models with elastic net penalties, which allows it\nto deal with high-dimensional data efficiently. The tree structure comprises a\ngraphical description of the data. Together with the logistic regression\nmodels, it provides an accurate classifier as well as a piecewise smooth\nestimate of the probability of \"success\". PLUTO controls selection bias by: (1)\nseparating split variable selection from split point selection; (2) applying an\nadjusted chi-squared test to find the split variable instead of exhaustive\nsearch. A bootstrap calibration technique is employed to further correct\nselection bias. Comparison on real datasets shows that on average, the multiple\nlinear PLUTO models predict more accurately than other algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 18:09:58 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Zhang", "Wenwen", ""], ["Loh", "Wei-Yin", ""]]}, {"id": "1411.7009", "submitter": "Shaan Qamar", "authors": "Shaan Qamar, Surya T. Tokdar", "title": "Additive Gaussian Process Regression", "comments": "28 pages; 9 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive-interactive regression has recently been shown to offer attractive\nminimax error rates over traditional nonparametric multivariate regression in a\nwide variety of settings, including cases where the predictor count is much\nlarger than the sample size and many of the predictors have important effects\non the response, potentially through complex interactions. We present a\nBayesian implementation of additive-interactive regression using an additive\nGaussian process (AGP) prior and develop an efficient Markov chain sampler that\nextends stochastic search variable selection in this setting. Careful prior and\nhyper-parameter specification are developed in light of performance and\ncomputational considerations, and key innovations address difficulties in\nexploring a joint posterior distribution over multiple subsets of high\ndimensional predictor inclusion vectors. The method offers state-of-the-art\nsupport and interaction recovery while improving dramatically over competitors\nin terms of prediction accuracy on a diverse set of simulated and real data.\nResults from real data studies provide strong evidence that the\nadditive-interactive framework is an attractive modeling platform for\nhigh-dimensional nonparametric regression.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 20:28:28 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Qamar", "Shaan", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1411.7013", "submitter": "Jocelyn Chi", "authors": "Jocelyn T. Chi, Eric C. Chi, Richard G. Baraniuk", "title": "$k$-POD: A Method for $k$-Means Clustering of Missing Data", "comments": "26 pages, 7 tables", "journal-ref": "The American Statistician 70(1):91-99, 2016", "doi": "10.1080/00031305.2015.1086685", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means algorithm is often used in clustering applications but its\nusage requires a complete data matrix. Missing data, however, is common in many\napplications. Mainstream approaches to clustering missing data reduce the\nmissing data problem to a complete data formulation through either deletion or\nimputation but these solutions may incur significant costs. Our $k$-POD method\npresents a simple extension of $k$-means clustering for missing data that works\neven when the missingness mechanism is unknown, when external information is\nunavailable, and when there is significant missingness in the data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 20:37:58 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 21:13:59 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 18:45:25 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Chi", "Jocelyn T.", ""], ["Chi", "Eric C.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1411.7342", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, Benno Kreuels, J\\\"urgen May, and Dylan S. Small", "title": "Full Matching Approach to Instrumental Variables Estimation with\n  Application to the Effect of Malaria on Stunting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous studies of the causal relationship between malaria and stunting\nhave been studies where potential confounders are controlled via\nregression-based methods, but these studies may have been biased by unobserved\nconfounders. Instrumental variables (IV) regression offers a way to control for\nunmeasured confounders where, in our case, the sickle cell trait can be used as\nan instrument. However, for the instrument to be valid, it may still be\nimportant to account for measured confounders. The most commonly used\ninstrumental variable regression method, two-stage least squares, relies on\nparametric assumptions on the effects of measured confounders to account for\nthem. Additionally, two-stage least squares lacks transparency with respect to\ncovariate balance and weighing of subjects and does not blind the researcher to\nthe outcome data. To address these drawbacks, we propose an alternative method\nfor IV estimation based on full matching. We evaluate our new procedure on\nsimulated data and real data concerning the causal effect of malaria on\nstunting among children. We estimate that the risk of stunting among children\nwith the sickle cell trait decrease by 0.22 times the average number of malaria\nepisodes prevented by the sickle cell trait, a substantial effect of malaria on\nstunting (p-value: 0.011, 95% CI: 0.044, 1).\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 19:25:12 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 18:25:41 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2015 17:27:05 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2015 19:49:59 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Kang", "Hyunseung", ""], ["Kreuels", "Benno", ""], ["May", "J\u00fcrgen", ""], ["Small", "Dylan S.", ""]]}, {"id": "1411.7405", "submitter": "Karl Rohe", "authors": "Karl Rohe", "title": "A note relating ridge regression and OLS p-values to preconditioned\n  sparse penalized regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the design matrix has orthonormal columns, \"soft thresholding\" the\nordinary least squares (OLS) solution produces the Lasso solution [Tibshirani,\n1996]. If one uses the Puffer preconditioned Lasso [Jia and Rohe, 2012], then\nthis result generalizes from orthonormal designs to full rank designs (Theorem\n1). Theorem 2 refines the Puffer preconditioner to make the Lasso select the\nsame model as removing the elements of the OLS solution with the largest\np-values. Using a generalized Puffer preconditioner, Theorem 3 relates ridge\nregression to the preconditioned Lasso; this result is for the high dimensional\nsetting, p > n. Where the standard Lasso is akin to forward selection [Efron et\nal., 2004], Theorems 1, 2, and 3 suggest that the preconditioned Lasso is more\nakin to backward elimination. These results hold for sparse penalties beyond\nl1; for a broad class of sparse and non-convex techniques (e.g. SCAD and MC+),\nthe results hold for all local minima.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 21:47:00 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 16:12:59 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Rohe", "Karl", ""]]}, {"id": "1411.7481", "submitter": "Dept Mathematics", "authors": "Valerie Poynor and Athanasios Kottas", "title": "Nonparametric Bayesian Inference for Mean Residual Life Functions in\n  Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean residual life function is a key functional for a survival\ndistribution. It has a practically useful interpretation as the expected\nremaining lifetime given survival up to a particular time point, and it also\ncharacterizes the survival distribution. However, it has received limited\nattention in terms of inference methods under a probabilistic modeling\nframework. We seek to provide general inference methodology for mean residual\nlife regression. Survival data often include a set of predictor variables for\nthe survival response distribution, and in many cases it is natural to include\nthe covariates as random variables into the modeling. We thus employ Dirichlet\nprocess mixture modeling for the joint stochastic mechanism of the covariates\nand survival responses. This approach implies a flexible model structure for\nthe mean residual life of the conditional response distribution, allowing\ngeneral shapes for mean residual life as a function of covariates given a\nspecific time point, as well as a function of time given particular values of\nthe covariate vector. To expand the scope of the modeling framework, we extend\nthe mixture model to incorporate dependence across experimental groups, such as\ntreatment and control groups. This extension is built from a dependent\nDirichlet process prior for the group-specific mixing distributions, with\ncommon locations and weights that vary across groups through latent bivariate\nBeta distributed random variables. We develop properties of the regression\nmodels, and discuss methods for prior specification and posterior inference.\nThe different components of the methodology are illustrated with simulated data\nexamples, and the model is also applied to a data set comprising right censored\nsurvival times.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 06:48:00 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 19:20:32 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 21:07:42 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Poynor", "Valerie", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1411.7694", "submitter": "Beatriz Sinova", "authors": "Beatriz Sinova and Stefan Van Aelst", "title": "On the consistency of a spatial-type interval-valued median for random\n  intervals", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample $d_\\theta$-median is a robust estimator of the central tendency or\nlocation of an interval-valued random variable. While the interval-valued\nsample mean can be highly influenced by outliers, this spatial-type\ninterval-valued median remains much more reliable. In this paper, we show that\nunder general conditions the sample $d_\\theta$-median is a strongly consistent\nestimator of the $d_\\theta$-median of an interval-valued random variable.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 20:28:58 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Sinova", "Beatriz", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "1411.7713", "submitter": "Sebastian Vollmer", "authors": "Sergios Agapiou and Gareth O. Roberts and Sebastian J. Vollmer", "title": "Unbiased Monte Carlo: posterior estimation for\n  intractable/infinite-dimensional models", "comments": "74pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general methodology for unbiased estimation for intractable\nstochastic models. We consider situations where the target distribution can be\nwritten as an appropriate limit of distributions, and where conventional\napproaches require truncation of such a representation leading to a systematic\nbias. For example, the target distribution might be representable as the\n$L^2$-limit of a basis expansion in a suitable Hilbert space; or alternatively\nthe distribution of interest might be representable as the weak limit of a\nsequence of random variables, as in MCMC. Our main motivation comes from\ninfinite-dimensional models which can be parame- terised in terms of a series\nexpansion of basis functions (such as that given by a Karhunen-Loeve\nexpansion). We consider schemes for direct unbiased estimation along such an\nexpansion, as well as those based on MCMC schemes which, due to their\ndimensionality, cannot be directly imple- mented, but which can be effectively\nestimated unbiasedly. For all our methods we give theory to justify the\nnumerical stability for robust Monte Carlo implementation, and in some cases we\nillustrate using simulations. Interestingly the computational efficiency of our\nmethods is usually comparable to simpler methods which are biased. Crucial to\nthe effectiveness of our proposed methodology is the construction of\nappropriate couplings, many of which resonate strongly with the Monte Carlo\nconstructions used in the coupling from the past algorithm and its variants.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 22:36:34 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Agapiou", "Sergios", ""], ["Roberts", "Gareth O.", ""], ["Vollmer", "Sebastian J.", ""]]}, {"id": "1411.7759", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Tatsuya Kubokawa", "title": "On Conditional Prediction Errors in Mixed Models with Application to\n  Small Area Estimation", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 2016", "doi": "10.1016/j.jmva.2016.02.009", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirical Bayes estimators in mixed models are useful for small area\nestimation in the sense of increasing precision of prediction for small area\nmeans, and one wants to know the prediction errors of the empirical Bayes\nestimators based on the data. This paper is concerned with conditional\nprediction errors in the mixed models instead of conventional unconditional\nprediction errors. In the mixed models based on natural exponential families\nwith quadratic variance functions, it is shown that the difference between the\nconditional and unconditional prediction errors is significant under\ndistributions far from normality. Especially for the binomial-beta mixed and\nthe Poisson-gamma mixed models, the leading terms in the conditional prediction\nerrors are, respectively, a quadratic concave function and an increasing\nfunction of the direct estimate in the small area, while the corresponding\nleading terms in the unconditional prediction errors are constants.\nSecond-order unbiased estimators of the conditional prediction errors are also\nderived and their performances are examined through simulation and empirical\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 05:53:31 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 04:07:34 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1411.7782", "submitter": "Anne Sabourin", "authors": "Anne Sabourin and Benjamin Renard", "title": "Combining regional estimation and historical floods: a multivariate\n  semi-parametric peaks-over-threshold model with censored data", "comments": null, "journal-ref": null, "doi": "10.1002/2015WR017320", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of extreme flood quantiles is challenging due to the relative\nscarcity of extreme data compared to typical target return periods. Several\napproaches have been developed over the years to face this challenge, including\nregional estimation and the use of historical flood data. This paper\ninvestigates the combination of both approaches using a multivariate\npeaks-over-threshold model, that allows estimating altogether the intersite\ndependence structure and the marginal distributions at each site. The joint\ndistribution of extremes at several sites is constructed using a\nsemi-parametric Dirichlet Mixture model. The existence of partially missing and\ncensored observations (historical data) is accounted for within a data\naugmentation scheme. This model is applied to a case study involving four\ncatchments in Southern France, for which historical data are available since\n1604. The comparison of marginal estimates from four versions of the model\n(with or without regionalizing the shape parameter; using or ignoring\nhistorical floods) highlights significant differences in terms of return level\nestimates. Moreover, the availability of historical data on several nearby\ncatchments allows investigating the asymptotic dependence properties of extreme\nfloods. Catchments display a a significant amount of asymptotic dependence,\ncalling for adapted multivariate statistical models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 09:00:14 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Sabourin", "Anne", ""], ["Renard", "Benjamin", ""]]}, {"id": "1411.7888", "submitter": "Theodore  Kypraios", "authors": "Philip D. O'Neill and Theodore Kypraios", "title": "Bayesian model choice via mixture distributions with application to\n  epidemics and population process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method for evaluating Bayes factors. The key idea is to\nintroduce a hypermodel in which the competing models are components of a\nmixture distribution. Inference for the mixing probabilities then yields\nestimates of the Bayes factors. Our motivation is the setting where the\nobserved data are a partially observed realisation of a stochastic population\nprocess, although the methods have far wider applicability. The methods allow\nfor missing data and for parameters to be shared between models. Illustrative\nexamples including epidemics, population processes and regression models are\ngiven, showing that the methods are competitive compared to existing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 14:50:48 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 21:00:42 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["O'Neill", "Philip D.", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1411.7919", "submitter": "Jing Ma", "authors": "Jing Ma, Ali Shojaie, George Michailidis", "title": "Network-Based Pathway Enrichment Analysis with Incomplete Network\n  Information", "comments": "45 pages, 3 figures, 23 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathway enrichment analysis has become a key tool for biomedical researchers\nto gain insight into the underlying biology of differentially expressed genes,\nproteins and metabolites. It reduces complexity and provides a system-level\nview of changes in cellular activity in response to treatments and/or in\ndisease states. Methods that use existing pathway network information have been\nshown to outperform simpler methods that only take into account pathway\nmembership. However, despite significant progress in understanding the\nassociation amongst members of biological pathways, and expansion of data bases\ncontaining information about interactions of biomolecules, the existing network\ninformation may be incomplete or inaccurate, and is not cell-type or disease\ncondition-specific. We propose a constrained network estimation framework that\ncombines network estimation based on cell- and condition-specific\nhigh-dimensional Omics data with interaction information from existing data\nbases. The resulting pathway topology information is subsequently used to\nprovide a framework for simultaneous testing of differences in expression\nlevels of pathway members, as well as their interactions. We study the\nasymptotic properties of the proposed network estimator and the test for\npathway enrichment, and investigate its small sample performance in simulated\nand real data settings.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 15:57:50 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 18:43:40 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Ma", "Jing", ""], ["Shojaie", "Ali", ""], ["Michailidis", "George", ""]]}, {"id": "1411.7934", "submitter": "Marianna Bolla CSc", "authors": "Marianna Bolla and Ahmed Elbanna", "title": "Estimating parameters of a multipartite loglinear graph model via the EM\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will amalgamate the Rash model (for rectangular binary tables) and the\nnewly introduced $\\alpha$-$\\beta$ models (for random undirected graphs) in the\nframework of a semiparametric probabilistic graph model. Our purpose is to give\na partition of the vertices of an observed graph so that the generated\nsubgraphs and bipartite graphs obey these models, where their strongly\nconnected parameters give multiscale evaluation of the vertices at the same\ntime. In this way, a heterogeneous version of the stochastic block model is\nbuilt via mixtures of loglinear models and the parameters are estimated with a\nspecial EM iteration. In the context of social networks, the clusters can be\nidentified with social groups and the parameters with attitudes of people of\none group towards people of the other, which attitudes depend on the cluster\nmemberships. The algorithm is applied to randomly generated and real-word data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 16:34:05 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 12:44:14 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2015 14:36:19 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Bolla", "Marianna", ""], ["Elbanna", "Ahmed", ""]]}, {"id": "1411.7955", "submitter": "Nicholas James", "authors": "Nicholas A. James, Arun Kejariwal, David S. Matteson", "title": "Leveraging Cloud Data to Mitigate User Experience from \"Breaking Bad\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Low latency and high availability of an app or a web service are key, amongst\nother factors, to the overall user experience (which in turn directly impacts\nthe bottomline). Exogenic and/or endogenic factors often give rise to breakouts\nin cloud data which makes maintaining high availability and delivering high\nperformance very challenging.\n  Although there exists a large body of prior research in breakout detection,\nexisting techniques are not suitable for detecting breakouts in cloud data\nowing to being not robust in the presence of anomalies.\n  To this end, we developed a novel statistical technique to automatically\ndetect breakouts in cloud data. In particular, the technique employs Energy\nStatistics to detect breakouts in both application as well as system metrics.\nFurther, the technique uses robust statistical metrics, viz., median, and\nestimates the statistical significance of a breakout through a permutation\ntest.\n  To the best of our knowledge, this is the first work which addresses breakout\ndetection in the presence of anomalies.\n  We demonstrate the efficacy of the proposed technique using production data\nand report Precision, Recall and F-measure measure. The proposed technique is\n3.5 times faster than a state-of-the-art technique for breakout detection and\nis being currently used on a daily basis at Twitter.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 17:57:35 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["James", "Nicholas A.", ""], ["Kejariwal", "Arun", ""], ["Matteson", "David S.", ""]]}]