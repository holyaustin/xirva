[{"id": "1608.00107", "submitter": "Scott Sisson", "authors": "Xin Zhang and Boris Beranger and Scott A. Sisson", "title": "Constructing Likelihood Functions for Interval-valued Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need for the ability to analyse interval-valued data.\nHowever, existing descriptive frameworks to achieve this ignore the process by\nwhich interval-valued data are typically constructed; namely by the aggregation\nof real-valued data generated from some underlying process. In this article we\ndevelop the foundations of likelihood based statistical inference for random\nintervals that directly incorporates the underlying generative procedure into\nthe analysis. That is, it permits the direct fitting of models for the\nunderlying real-valued data given only the random interval-valued summaries.\nThis generative approach overcomes several problems associated with existing\nmethods, including the rarely satisfied assumption of within-interval\nuniformity. The new methods are illustrated by simulated and real data\nanalyses.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 12:03:33 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 00:19:52 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Zhang", "Xin", ""], ["Beranger", "Boris", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1608.00264", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Stefano Favaro, Stephen G Walker", "title": "Frequency of Frequencies Distributions and Size Dependent Exchangeable\n  Random Partitions", "comments": "To appear in the Journal of the American Statistical Association\n  (Theory and Methods). 26 pages + 17 page supplement, 19 figures. arXiv admin\n  note: text overlap with arXiv:1410.3155", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the fundamental problem of modeling the frequency of frequencies\n(FoF) distribution, this paper introduces the concept of a cluster structure to\ndefine a probability function that governs the joint distribution of a random\ncount and its exchangeable random partitions. A cluster structure, naturally\narising from a completely random measure mixed Poisson process, allows the\nprobability distribution of the random partitions of a subset of a population\nto be dependent on the population size, a distinct and motivated feature that\nmakes it more flexible than a partition structure. This allows it to model an\nentire FoF distribution whose structural properties change as the population\nsize varies. A FoF vector can be simulated by drawing an infinite number of\nPoisson random variables, or by a stick-breaking construction with a finite\nrandom number of steps. A generalized negative binomial process model is\nproposed to generate a cluster structure, where in the prior the number of\nclusters is finite and Poisson distributed, and the cluster sizes follow a\ntruncated negative binomial distribution. We propose a simple Gibbs sampling\nalgorithm to extrapolate the FoF vector of a population given the FoF vector of\na sample taken without replacement from the population. We illustrate our\nresults and demonstrate the advantages of the proposed models through the\nanalysis of real text, genomic, and survey data.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 21:26:50 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Favaro", "Stefano", ""], ["Walker", "Stephen G", ""]]}, {"id": "1608.00354", "submitter": "Martin Spindler", "authors": "Victor Chernozhukov, Chris Hansen, Martin Spindler", "title": "hdm: High-Dimensional Metrics", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.01700", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article the package High-dimensional Metrics (\\texttt{hdm}) is\nintroduced. It is a collection of statistical methods for estimation and\nquantification of uncertainty in high-dimensional approximately sparse models.\nIt focuses on providing confidence intervals and significance testing for\n(possibly many) low-dimensional subcomponents of the high-dimensional parameter\nvector. Efficient estimators and uniformly valid confidence intervals for\nregression coefficients on target variables (e.g., treatment or policy\nvariable) in a high-dimensional approximately sparse regression model, for\naverage treatment effect (ATE) and average treatment effect for the treated\n(ATET), as well for extensions of these parameters to the endogenous setting\nare provided. Theory grounded, data-driven methods for selecting the\npenalization parameter in Lasso regressions under heteroscedastic and\nnon-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence\nintervals for regression coefficients of a high-dimensional sparse regression\nare implemented. Data sets which have been used in the literature and might be\nuseful for classroom demonstration and for testing new estimators are included.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 08:51:31 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Hansen", "Chris", ""], ["Spindler", "Martin", ""]]}, {"id": "1608.00476", "submitter": "Neeraj Bokde", "authors": "Neeraj Bokde, Kishore Kulat, Marcus W Beck, Gualberto Asencio-Cort\\'es", "title": "R package imputeTestbench to compare imputations methods for univariate\n  time series", "comments": null, "journal-ref": null, "doi": "10.32614/RJ-2018-024", "report-no": null, "categories": "stat.ME cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the R package imputeTestbench that provides a testbench\nfor comparing imputation methods for missing data in univariate time series.\nThe imputeTestbench package can be used to simulate the amount and type of\nmissing data in a complete dataset and compare filled data using different\nimputation methods. The user has the option to simulate missing data by\nremoving observations completely at random or in blocks of different sizes.\nSeveral default imputation methods are included with the package, including\nhistorical means, linear interpolation, and last observation carried forward.\nThe testbench is not limited to the default functions and users can add or\nremove additional methods using a simple two-step process. The testbench\ncompares the actual missing and imputed data for each method with different\nerror metrics, including RMSE, MAE, and MAPE. Alternative error metrics can\nalso be supplied by the user. The simplicity of use and significant reduction\nin time to compare imputation methods for missing data in univariate time\nseries is a significant advantage of the package. This paper provides an\noverview of the core functions, including a demonstration with examples.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 15:54:26 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 05:55:32 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Bokde", "Neeraj", ""], ["Kulat", "Kishore", ""], ["Beck", "Marcus W", ""], ["Asencio-Cort\u00e9s", "Gualberto", ""]]}, {"id": "1608.00528", "submitter": "Kory Johnson", "authors": "Kory D. Johnson, Dean P. Foster, Robert A. Stine", "title": "Impartial Predictive Modeling: Ensuring Group Fairness in Arbitrary\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness aware data mining (FADM) aims to prevent algorithms from\ndiscriminating against protected groups. The literature has come to an impasse\nas to what constitutes explainable variability as opposed to discrimination. We\ndemonstrate that fairness is achieved by ensuring \\emph{impartiality} with\nrespect to sensitive characteristics and provide a framework for impartiality\nby accounting for different perspectives on the data generating process. In\nparticular, fairness can only be precisely defined in a full-data scenario in\nwhich all covariates are observed. We then analyze how these models may be\nconservatively estimated via regression in partial-data settings. Decomposing\nthe regression estimates provides insights into previously unexplored\ndistinctions between explainable variability and discrimination. This framework\nyields a set of impartial estimates that are applicable in a wide variety of\nsituations and post-processing tools to correct estimates from arbitrary\nmodels. This effectively separates prediction and fairness goals.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:06:49 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 13:05:26 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 16:09:25 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Johnson", "Kory D.", ""], ["Foster", "Dean P.", ""], ["Stine", "Robert A.", ""]]}, {"id": "1608.00550", "submitter": "Ping Li", "authors": "Ping Li and Cun-Hui Zhang", "title": "Theory of the GMM Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop some theoretical results for a robust similarity measure named\n\"generalized min-max\" (GMM). This similarity has direct applications in machine\nlearning as a positive definite kernel and can be efficiently computed via\nprobabilistic hashing. Owing to the discrete nature, the hashed values can also\nbe used for efficient near neighbor search. We prove the theoretical limit of\nGMM and the consistency result, assuming that the data follow an elliptical\ndistribution, which is a very general family of distributions and includes the\nmultivariate $t$-distribution as a special case. The consistency result holds\nas long as the data have bounded first moment (an assumption which essentially\nholds for datasets commonly encountered in practice). Furthermore, we establish\nthe asymptotic normality of GMM. Compared to the \"cosine\" similarity which is\nroutinely adopted in current practice in statistics and machine learning, the\nconsistency of GMM requires much weaker conditions. Interestingly, when the\ndata follow the $t$-distribution with $\\nu$ degrees of freedom, GMM typically\nprovides a better measure of similarity than \"cosine\" roughly when $\\nu<8$\n(which is already very close to normal). These theoretical results will help\nexplain the recent success of GMM in learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:45:57 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1608.00607", "submitter": "Daniel Larremore", "authors": "Bailey K. Fosdick, Daniel B. Larremore, Joel Nishimura, Johan Ugander", "title": "Configuring Random Graph Models with Fixed Degree Sequences", "comments": "To appear in SIAM Review, June 2018. Code available at\n  github.com/joelnish/double-edge-swap-mcmc. v3 fixed minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.data-an physics.soc-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graph null models have found widespread application in diverse\nresearch communities analyzing network datasets, including social, information,\nand economic networks, as well as food webs, protein-protein interactions, and\nneuronal networks. The most popular family of random graph null models, called\nconfiguration models, are defined as uniform distributions over a space of\ngraphs with a fixed degree sequence. Commonly, properties of an empirical\nnetwork are compared to properties of an ensemble of graphs from a\nconfiguration model in order to quantify whether empirical network properties\nare meaningful or whether they are instead a common consequence of the\nparticular degree sequence. In this work we study the subtle but important\ndecisions underlying the specification of a configuration model, and\ninvestigate the role these choices play in graph sampling procedures and a\nsuite of applications. We place particular emphasis on the importance of\nspecifying the appropriate graph labeling (stub-labeled or vertex-labeled)\nunder which to consider a null model, a choice that closely connects the study\nof random graphs to the study of random contingency tables. We show that the\nchoice of graph labeling is inconsequential for studies of simple graphs, but\ncan have a significant impact on analyses of multigraphs or graphs with\nself-loops. The importance of these choices is demonstrated through a series of\nthree vignettes, analyzing network datasets under many different configuration\nmodels and observing substantial differences in study conclusions under\ndifferent models. We argue that in each case, only one of the possible\nconfiguration models is appropriate. While our work focuses on undirected\nstatic networks, it aims to guide the study of directed networks, dynamic\nnetworks, and all other network contexts that are suitably studied through the\nlens of random graph null models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 20:44:16 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 21:26:21 GMT"}, {"version": "v3", "created": "Tue, 10 Oct 2017 20:37:37 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Fosdick", "Bailey K.", ""], ["Larremore", "Daniel B.", ""], ["Nishimura", "Joel", ""], ["Ugander", "Johan", ""]]}, {"id": "1608.00623", "submitter": "Subhadeep Paul", "authors": "Subhadeep Paul and Yuguo Chen", "title": "Null Models and Community Detection in Multi-Layer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-layer networks are networks on a set of entities (nodes) with multiple\ntypes of relations (edges) among them where each type of relation/interaction\nis represented as a network layer. As with single layer networks, community\ndetection is an important task in multi-layer networks. A large group of\npopular community detection methods in networks are based on optimizing a\nquality function known as the modularity score, which is a measure of presence\nof modules or communities in networks. Hence a first step in community\ndetection is defining a suitable modularity score that is appropriate for the\nnetwork in question. Here we introduce several multi-layer network modularity\nmeasures under different null models of the network, motivated by empirical\nobservations in networks from a diverse field of applications. In particular we\ndefine the multi-layer configuration model, the multi-layer expected degree\nmodel and their various modifications as null models for multi-layer networks\nto derive different modularities. The proposed modularities are grouped into\ntwo categories. The first category, which is based on degree corrected\nmulti-layer stochastic block model, has the multi-layer expected degree model\nas their null model. The second category, which is based on multi-layer\nextensions of Newman-Girvan modularity, has the multi-layer configuration model\nas their null model. These measures are then optimized to detect the optimal\ncommunity assignment of nodes. We compare the effectiveness of the measures in\ncommunity detection in simulated networks and then apply them to four real\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:33:00 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 16:10:18 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Paul", "Subhadeep", ""], ["Chen", "Yuguo", ""]]}, {"id": "1608.00629", "submitter": "Yi Yang", "authors": "Chenglong Ye, Yi Yang and Yuhong Yang", "title": "Sparsity Oriented Importance Learning for High-dimensional Linear\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With now well-recognized non-negligible model selection uncertainty, data\nanalysts should no longer be satisfied with the output of a single final model\nfrom a model selection process, regardless of its sophistication. To improve\nreliability and reproducibility in model choice, one constructive approach is\nto make good use of a sound variable importance measure. Although interesting\nimportance measures are available and increasingly used in data analysis,\nlittle theoretical justification has been done. In this paper, we propose a new\nvariable importance measure, sparsity oriented importance learning (SOIL), for\nhigh-dimensional regression from a sparse linear modeling perspective by taking\ninto account the variable selection uncertainty via the use of a sensible model\nweighting. The SOIL method is theoretically shown to have the\ninclusion/exclusion property: When the model weights are properly around the\ntrue model, the SOIL importance can well separate the variables in the true\nmodel from the rest. In particular, even if the signal is weak, SOIL rarely\ngives variables not in the true model significantly higher important values\nthan those in the true model. Extensive simulations in several illustrative\nsettings and real data examples with guided simulations show desirable\nproperties of the SOIL importance in contrast to other importance measures.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 22:06:43 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Ye", "Chenglong", ""], ["Yang", "Yi", ""], ["Yang", "Yuhong", ""]]}, {"id": "1608.00696", "submitter": "Noureddine El Karoui", "authors": "Noureddine El Karoui and Elizabeth Purdom", "title": "Can we trust the bootstrap in high-dimension?", "comments": null, "journal-ref": null, "doi": null, "report-no": "Tech-report 824", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the performance of the bootstrap in high-dimensions for the\nsetting of linear regression, where $p<n$ but $p/n$ is not close to zero. We\nconsider ordinary least-squares as well as robust regression methods and adopt\na minimalist performance requirement: can the bootstrap give us good confidence\nintervals for a single coordinate of $\\beta$? (where $\\beta$ is the true\nregression vector).\n  We show through a mix of numerical and theoretical work that the bootstrap is\nfraught with problems. Both of the most commonly used methods of bootstrapping\nfor regression -- residual bootstrap and pairs bootstrap -- give very poor\ninference on $\\beta$ as the ratio $p/n$ grows. We find that the residuals\nbootstrap tend to give anti-conservative estimates (inflated Type I error),\nwhile the pairs bootstrap gives very conservative estimates (severe loss of\npower) as the ratio $p/n$ grows. We also show that the jackknife resampling\ntechnique for estimating the variance of $\\hat{\\beta}$ severely overestimates\nthe variance in high dimensions.\n  We contribute alternative bootstrap procedures based on our theoretical\nresults that mitigate these problems. However, the corrections depend on\nassumptions regarding the underlying data-generation model, suggesting that in\nhigh-dimensions it may be difficult to have universal, robust bootstrapping\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 05:10:39 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Karoui", "Noureddine El", ""], ["Purdom", "Elizabeth", ""]]}, {"id": "1608.00738", "submitter": "Qing Xiao", "authors": "Qing Xiao", "title": "Calculating correlation coefficient for Gaussian copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When Gaussian copula with linear correlation coefficient is used to model\ncorrelated random variables, one crucial issue is to determine a suitable\ncorrelation coefficient $\\rho_z$ in normal space for two variables with\ncorrelation coefficient $\\rho_x$. This paper attempts to address this problem.\nFor two continuous variables, the marginal transformation is approximated by a\nweighted sum of Hermite polynomials, then, with Mehler's formula, a polynomial\nof $\\rho_z$ is derived to approximate the function relationship between\n$\\rho_x$ and $\\rho_z$. If a discrete variable is involved, the marginal\ntransformation is decomposed into piecewise continuous ones, and $\\rho_x$ is\nexpressed as a polynomial of $\\rho_z$ by Taylor expansion. For a given\n$\\rho_x$, $\\rho_z$ can be efficiently determined by solving a polynomial\nequation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:58:42 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Xiao", "Qing", ""]]}, {"id": "1608.00818", "submitter": "Torben Martinussen", "authors": "T. Martinussen, S. Vansteelandt, E. J. Tchetgen Tchetgen, and D. M.\n  Zucker", "title": "Instrumental variables estimation of exposure effects on a time-to-event\n  response using structural cumulative survival models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of instrumental variables for estimating the effect of an exposure on\nan outcome is popular in econometrics, and increasingly so in epidemiology.\nThis increasing popularity may be attributed to the natural occurrence of\ninstrumental variables in observational studies that incorporate elements of\nrandomization, either by design or by nature (e.g., random inheritance of\ngenes). Instrumental variables estimation of exposure effects is well\nestablished for continuous outcomes and to some extent for binary outcomes. It\nis, however, largely lacking for time-to-event outcomes because of\ncomplications due to censoring and survivorship bias. In this paper, we make a\nnovel proposal under a class of structural cumulative survival models which\nparameterize time-varying effects of a point exposure directly on the scale of\nthe survival function; these models are essentially equivalent with a\nsemi-parametric variant of the instrumental variables additive hazards model.\nWe propose a class of recursive instrumental variable estimators for these\nexposure effects, and derive their large sample properties along with\ninferential tools. We examine the performance of the proposed method in\nsimulation studies and illustrate it in a Mendelian randomization study to\nevaluate the effect of diabetes on mortality using data from the Health and\nRetirement Study. We further use the proposed method to investigate potential\nbenefit from breast cancer screening on subsequent breast cancer mortality\nbased on the HIP-study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 13:43:06 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Martinussen", "T.", ""], ["Vansteelandt", "S.", ""], ["Tchetgen", "E. J. Tchetgen", ""], ["Zucker", "D. M.", ""]]}, {"id": "1608.00874", "submitter": "Fabrizio Leisen", "authors": "Jim Griffin and Fabrizio Leisen", "title": "Modelling and computation using NCoRM mixtures for density regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized compound random measures are flexible nonparametric priors for\nrelated distributions. We consider building general nonparametric regression\nmodels using normalized compound random measure mixture models. Posterior\ninference is made using a novel pseudo-marginal Metropolis-Hastings sampler for\nnormalized compound random measure mixture models. The algorithm makes use of a\nnew general approach to the unbiased estimation of Laplace functionals of\ncompound random measures (which includes completely random measures as a\nspecial case). The approach is illustrated on problems of density regression.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:47:42 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 10:41:56 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 10:21:58 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Griffin", "Jim", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1608.00948", "submitter": "Noureddine El Karoui", "authors": "Noureddine El Karoui and Elizabeth Purdom", "title": "The bootstrap, covariance matrices and PCA in moderate and\n  high-dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the properties of the bootstrap as a tool for inference\nconcerning the eigenvalues of a sample covariance matrix computed from an\n$n\\times p$ data matrix $X$. We focus on the modern framework where $p/n$ is\nnot close to 0 but remains bounded as $n$ and $p$ tend to infinity.\n  Through a mix of numerical and theoretical considerations, we show that the\nbootstrap is not in general a reliable inferential tool in the setting we\nconsider. However, in the case where the population covariance matrix is\nwell-approximated by a finite rank matrix, the bootstrap performs as it does in\nfinite dimension.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 19:31:49 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Karoui", "Noureddine El", ""], ["Purdom", "Elizabeth", ""]]}, {"id": "1608.01201", "submitter": "Fabio Rapallo", "authors": "Flavio Mignone, Fabio Rapallo", "title": "Detection of outlying proportions", "comments": "15 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new method for detecting outliers in a set of\nproportions. It is based on the construction of a suitable two-way contingency\ntable and on the application of an algorithm for the detection of outlying\ncells in such table. We exploit the special structure of the relevant\ncontingency table to increase the efficiency of the method. The main properties\nof our algorithm, together with a guide for the choice of the parameters, are\ninvestigated through simulations, and in simple cases some theoretical\njustifications are provided. Several examples on synthetic data and an example\nbased on pseudo-real data from biological experiments demonstrate the good\nperformances of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 14:23:07 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Mignone", "Flavio", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1608.01274", "submitter": "Daniel Kessler", "authors": "Daniel Kessler, Michael Angstadt, Chandra Sripada", "title": "Which Findings from the Functional Neuromaging Literature Can We Trust?", "comments": "All authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their recent \"Cluster Failure\" paper, Eklund and colleagues cast doubt on\nthe accuracy of a widely used statistical test in functional neuroimaging.\nHere, we leverage nonparametric methods that control the false discovery rate\nto offer more nuanced, quantitative guidance about which findings in the\nexisting literature can be trusted. We show that, in the task studies examined\nby Eklund et al., most clusters originally reported to be significant are\nindeed trustworthy by the false discovery rate benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 18:18:23 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Kessler", "Daniel", ""], ["Angstadt", "Michael", ""], ["Sripada", "Chandra", ""]]}, {"id": "1608.01337", "submitter": "KitIan Kou", "authors": "Cuiming Zou and Kit Ian Kou", "title": "Robust Signal Reconstruction Using the Prolate Spherical Wave Functions\n  and Maximum Correntropy Criterion", "comments": "16 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal Reconstruction is one of the most important problem in signal\nprocessing. This paper proposes a novel signal reconstruction method based on\nthe prolate spherical wave functions (PSWFs) and maximum correntropy criterion\n(MCC). The PSWFs are a kind of special functions, which have been proved having\ngood performance in signal reconstruction. However, the existing PSWFs based\nreconstruction methods only consider the mean square error (MSE) criterion as\nthe cost functions. The MSE criterion is sensitive to the non-Gaussian noise,\nsince it is builded up by the Gaussian assumption. Therefore, for the impulsive\nnoise or outliers, the MSE based reconstruction methods will lead to the large\nreconstruction error. The proposed MCC and PSWFs based robust signal\nreconstruction method can reduce the impact of large and non-Gaussian noise.\nThe experimental results on the synthetic signals show that the proposed method\ncan improve the MSE with notable gains in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 09:02:28 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Zou", "Cuiming", ""], ["Kou", "Kit Ian", ""]]}, {"id": "1608.01513", "submitter": "Lixing Zhu", "authors": "Libin Jin, Wangli Xu, Liping Zhu and Lixing Zhu", "title": "Penalized Maximum Likelihood Estimator for Skew Normal Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skew normal mixture models provide a more flexible framework than the popular\nnormal mixtures for modelling heterogeneous data with asymmetric behaviors. Due\nto the unboundedness of likelihood function and the divergency of shape\nparameters, the maximum likelihood estimators of the parameters of interest are\noften not well defined, leading to dissatisfactory inferential process. We put\nforward a proposal to deal with these issues simultaneously in the context of\npenalizing the likelihood function. The resulting penalized maximum likelihood\nestimator is proved to be strongly consistent when the putative order of\nmixture is equal to or larger than the true one. We also provide penalized\nEM-type algorithms to compute penalized estimators. Finite sample performances\nare examined by simulations and real data applications and the comparison to\nthe existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 12:36:59 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Jin", "Libin", ""], ["Xu", "Wangli", ""], ["Zhu", "Liping", ""], ["Zhu", "Lixing", ""]]}, {"id": "1608.01532", "submitter": "Martin Weidner", "authors": "Koen Jochmans, Martin Weidner", "title": "Fixed-Effect Regressions on Network Data", "comments": "26 pages main text (including bibliography), 15 pages supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers inference on fixed effects in a linear regression model\nestimated from network data. An important special case of our setup is the\ntwo-way regression model. This is a workhorse technique in the analysis of\nmatched data sets, such as employer-employee or student-teacher panel data. We\nformalize how the structure of the network affects the accuracy with which the\nfixed effects can be estimated. This allows us to derive sufficient conditions\non the network for consistent estimation and asymptotically-valid inference to\nbe possible. Estimation of moments is also considered. We allow for general\nnetworks and our setup covers both the dense and sparse case. We provide\nnumerical results for the estimation of teacher value-added models and\nregressions with occupational dummies.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 13:44:45 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 13:54:30 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 17:33:57 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2019 13:25:37 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Jochmans", "Koen", ""], ["Weidner", "Martin", ""]]}, {"id": "1608.01566", "submitter": "Tom Reynkens", "authors": "Tom Reynkens, Roel Verbelen, Jan Beirlant, Katrien Antonio", "title": "Modelling Censored Losses Using Splicing: a Global Fit Strategy With\n  Mixed Erlang and Extreme Value Distributions", "comments": null, "journal-ref": "Insurance Math. Econom. 77 (2017) 65-77", "doi": "10.1016/j.insmatheco.2017.08.005", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In risk analysis, a global fit that appropriately captures the body and the\ntail of the distribution of losses is essential. Modelling the whole range of\nthe losses using a standard distribution is usually very hard and often\nimpossible due to the specific characteristics of the body and the tail of the\nloss distribution. A possible solution is to combine two distributions in a\nsplicing model: a light-tailed distribution for the body which covers light and\nmoderate losses, and a heavy-tailed distribution for the tail to capture large\nlosses. We propose a splicing model with a mixed Erlang (ME) distribution for\nthe body and a Pareto distribution for the tail. This combines the flexibility\nof the ME distribution with the ability of the Pareto distribution to model\nextreme values. We extend our splicing approach for censored and/or truncated\ndata. Relevant examples of such data can be found in financial risk analysis.\nWe illustrate the flexibility of this splicing model using practical examples\nfrom risk measurement.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 14:55:47 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 15:59:30 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 14:34:18 GMT"}, {"version": "v4", "created": "Fri, 11 Aug 2017 14:55:27 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Reynkens", "Tom", ""], ["Verbelen", "Roel", ""], ["Beirlant", "Jan", ""], ["Antonio", "Katrien", ""]]}, {"id": "1608.01593", "submitter": "Thomas Nagler", "authors": "Thibault Vatter, Thomas Nagler", "title": "Generalized Additive Models for Pair-Copula Constructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pair-copula constructions are flexible dependence models that use bivariate\ncopulas as building blocks. In this paper, we use generalized additive models\nto extend them by allowing covariates effects. Borrowing ideas from a\ntraditionally univariate context, we let each pair-copula parameter depend\ndirectly on the covariates in a parametric, semiparametric or nonparametric\nway. We propose a sequential estimation method that we study by simulation, and\napply it to investigate the time-varying dependence structure between the\nintraday returns on four major foreign exchange rates. An R package, a script\nreproducing the results in this article, and additional simulation results are\nprovided as supplementary material.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 16:04:38 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 18:03:48 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Vatter", "Thibault", ""], ["Nagler", "Thomas", ""]]}, {"id": "1608.01736", "submitter": "Xiaowen Dai", "authors": "Xiaowen Dai, Zhen Yan, Maozai Tian, Manlai Tang", "title": "Quantile Regression for General Spatial Panel Data Models with Fixed\n  Effects", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the quantile regression model with both individual fixed\neffect and time period effect for general spatial panel data. Instrumental\nvariable quantile regression estimators will be proposed. Asymptotic properties\nof the proposed estimators will be developed. Simulations are conducted to\nstudy the performance of the proposed method. We will illustrate our\nmethodologies using a cigarettes demand data set.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 01:46:27 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Dai", "Xiaowen", ""], ["Yan", "Zhen", ""], ["Tian", "Maozai", ""], ["Tang", "Manlai", ""]]}, {"id": "1608.01739", "submitter": "Xiaowen Dai", "authors": "Xiaowen Dai, Shaoyang Li, Maozai Tian", "title": "Quantile Regression for Partially Linear Varying Coefficient Spatial\n  Autoregressive Models", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the quantile regression approach for partially linear\nspatial autoregressive models with possibly varying coefficients. B-spline is\nemployed for the approximation of varying coefficients. The instrumental\nvariable quantile regression approach is employed for parameter estimation. The\nrank score tests are developed for hypotheses on the coefficients, including\nthe hypotheses on the non-varying coefficients and the constancy of the varying\ncoefficients. The asymptotic properties of the proposed estimators and test\nstatistics are both established. Monte Carlo simulations are conducted to study\nthe finite sample performance of the proposed method. Analysis of a real data\nexample is presented for illustration.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 01:54:59 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Dai", "Xiaowen", ""], ["Li", "Shaoyang", ""], ["Tian", "Maozai", ""]]}, {"id": "1608.01787", "submitter": "Peng Ding", "authors": "Peng Ding, Tirthankar Dasgupta", "title": "A randomization-based perspective of analysis of variance: a test\n  statistic robust to treatment effect heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher randomization tests for Neyman's null hypothesis of no average\ntreatment effects are considered in a finite population setting associated with\ncompletely randomized experiments with more than two treatments. The\nconsequences of using the $F$ statistic to conduct such a test are examined\nboth theoretically and computationally, and it is argued that under treatment\neffect heterogeneity, use of the $F$ statistic in the Fisher randomization test\ncan severely inflate the type I error under Neyman's null hypothesis. An\nalternative test statistic is proposed, its asymptotic distributions under\nFisher's and Neyman's null hypotheses are derived, and its advantages\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 07:41:54 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 23:26:55 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Ding", "Peng", ""], ["Dasgupta", "Tirthankar", ""]]}, {"id": "1608.02014", "submitter": "Wesley Pegden", "authors": "Maria Chikina, Alan Frieze, Wesley Pegden", "title": "Assessing significance in a Markov chain without mixing", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new statistical test to detect that a presented state of a\nreversible Markov chain was not chosen from a stationary distribution. In\nparticular, given a value function for the states of the Markov chain, we would\nlike to demonstrate rigorously that the presented state is an outlier with\nrespect to the values, by establishing a $p$-value for observations we make\nabout the state under the null hypothesis that it was chosen uniformly at\nrandom.\n  A simple heuristic used in practice is to sample ranks of states from long\nrandom trajectories on the Markov chain, and compare these to the rank of the\npresented state; if the presented state is a $0.1\\%$-outlier compared to the\nsampled ranks (i.e., its rank is in the bottom $0.1\\%$ of sampled ranks) then\nthis should correspond to a $p$-value of $0.001$. This test is not rigorous,\nhowever, without good bounds on the mixing time of the Markov chain, as one\nmust argue that the observed states on the trajectory approximate the\nstationary distribution.\n  Our test is the following: given the presented state in the Markov chain,\ntake a random walk from the presented state for any number of steps. We prove\nthat observing that the presented state is an $\\varepsilon$-outlier on the walk\nis significant at $p=\\sqrt {2\\varepsilon}$, under the null hypothesis that the\nstate was chosen from a stationary distribution. Our result assumes nothing\nabout the structure of the Markov chain beyond reversibility, and we construct\nexamples to show that significance at $p\\approx\\sqrt \\varepsilon$ is\nessentially best possible in general. We illustrate the use of our test with a\npotential application to the rigorous detection of gerrymandering in\nCongressional districtings.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 20:24:39 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 15:26:48 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Chikina", "Maria", ""], ["Frieze", "Alan", ""], ["Pegden", "Wesley", ""]]}, {"id": "1608.02067", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Qiwei Yao, Wen Zhou", "title": "Testing for high-dimensional white noise using maximum\n  cross-correlations", "comments": null, "journal-ref": "Biometrika 2017, Vol. 104, No. 1, 111-127", "doi": "10.1093/biomet/asw066", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new omnibus test for vector white noise using the maximum\nabsolute auto-correlations and cross-correlations of the component series.\nBased on the newly established approximation by the $L_\\infty$-norm of a normal\nrandom vector, the critical value of the test can be evaluated by bootstrapping\nfrom a multivariate normal distribution. In contrast to the conventional white\nnoise test, the new method is proved to be valid for testing the departure from\nnon-IID white noise. We illustrate the accuracy and the power of the proposed\ntest by simulation, which also shows that the new test outperforms several\ncommonly used methods including, for example, the Lagrange multiplier test and\nthe multivariate Box-Pierce portmanteau tests especially when the dimension of\ntime series is high in relation to the sample size. The numerical results also\nindicate that the performance of the new test can be further enhanced when it\nis applied to the pre-transformed data obtained via the time series principal\ncomponent analysis proposed by Chang, Guo and Yao (2014). The proposed\nprocedures have been implemented in an R-package HDtest and is available online\nat CRAN.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 04:51:35 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 09:34:15 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 01:51:22 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Chang", "Jinyuan", ""], ["Yao", "Qiwei", ""], ["Zhou", "Wen", ""]]}, {"id": "1608.02148", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson, Sergey Voronin, Steven L. Brunton, J. Nathan\n  Kutz", "title": "Randomized Matrix Decompositions using R", "comments": null, "journal-ref": "Journal of Statistical Software. May 2019, Volume 89, Issue 11", "doi": "10.18637/jss.v089.i11", "report-no": null, "categories": "stat.CO cs.MS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix decompositions are fundamental tools in the area of applied\nmathematics, statistical computing, and machine learning. In particular,\nlow-rank matrix decompositions are vital, and widely used for data analysis,\ndimensionality reduction, and data compression. Massive datasets, however, pose\na computational challenge for traditional algorithms, placing significant\nconstraints on both memory and processing power. Recently, the powerful concept\nof randomness has been introduced as a strategy to ease the computational load.\nThe essential idea of probabilistic algorithms is to employ some amount of\nrandomness in order to derive a smaller matrix from a high-dimensional data\nmatrix. The smaller matrix is then used to compute the desired low-rank\napproximation. Such algorithms are shown to be computationally efficient for\napproximating matrices with low-rank structure. We present the \\proglang{R}\npackage rsvd, and provide a tutorial introduction to randomized matrix\ndecompositions. Specifically, randomized routines for the singular value\ndecomposition, (robust) principal component analysis, interpolative\ndecomposition, and CUR decomposition are discussed. Several examples\ndemonstrate the routines, and show the computational advantage over other\nmethods implemented in R.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 19:47:48 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 20:03:57 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 23:01:44 GMT"}, {"version": "v4", "created": "Sun, 1 Apr 2018 21:26:59 GMT"}, {"version": "v5", "created": "Tue, 26 Nov 2019 23:33:14 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Voronin", "Sergey", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1608.02158", "submitter": "Adler Perotte", "authors": "Rajesh Ranganath and Adler Perotte and No\\'emie Elhadad and David Blei", "title": "Deep Survival Analysis", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electronic health record (EHR) provides an unprecedented opportunity to\nbuild actionable tools to support physicians at the point of care. In this\npaper, we investigate survival analysis in the context of EHR data. We\nintroduce deep survival analysis, a hierarchical generative approach to\nsurvival analysis. It departs from previous approaches in two primary ways: (1)\nall observations, including covariates, are modeled jointly conditioned on a\nrich latent structure; and (2) the observations are aligned by their failure\ntime, rather than by an arbitrary time zero as in traditional survival\nanalysis. Further, it (3) scalably handles heterogeneous (continuous and\ndiscrete) data types that occur in the EHR. We validate deep survival analysis\nmodel by stratifying patients according to risk of developing coronary heart\ndisease (CHD). Specifically, we study a dataset of 313,000 patients\ncorresponding to 5.5 million months of observations. When compared to the\nclinically validated Framingham CHD risk score, deep survival analysis is\nsignificantly superior in stratifying patients according to their risk.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 22:18:18 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 14:08:02 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Perotte", "Adler", ""], ["Elhadad", "No\u00e9mie", ""], ["Blei", "David", ""]]}, {"id": "1608.02209", "submitter": "Daniele Durante", "authors": "Daniele Durante, Nabanita Mukherjee, Rebecca C. Steorts", "title": "Bayesian Learning of Dynamic Multilayer Networks", "comments": null, "journal-ref": "Journal of Machine Learning Research (2017). 18, 1-29", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of networks is being collected in a growing number of fields,\nincluding disease transmission, international relations, social interactions,\nand others. As data streams continue to grow, the complexity associated with\nthese highly multidimensional connectivity data presents novel challenges. In\nthis paper, we focus on the time-varying interconnections among a set of actors\nin multiple contexts, called layers. Current literature lacks flexible\nstatistical models for dynamic multilayer networks, which can enhance quality\nin inference and prediction by efficiently borrowing information within each\nnetwork, across time, and between layers. Motivated by this gap, we develop a\nBayesian nonparametric model leveraging latent space representations. Our\nformulation characterizes the edge probabilities as a function of shared and\nlayer-specific actors positions in a latent space, with these positions\nchanging in time via Gaussian processes. This representation facilitates\ndimensionality reduction and incorporates different sources of information in\nthe observed data. In addition, we obtain tractable procedures for posterior\ncomputation, inference, and prediction. We provide theoretical results on the\nflexibility of our model. Our methods are tested on simulations and infection\nstudies monitoring dynamic face-to-face contacts among individuals in multiple\ndays, where we perform better than current methods in inference and prediction.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 12:34:47 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 19:04:50 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Mukherjee", "Nabanita", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1608.02273", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Shreya Kangovi, Nandita Mitra", "title": "Estimating scaled treatment effects with multiple outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical study designs, the aim is often to learn about the effects of a\ntreatment or intervention on a single outcome; in many modern studies, however,\ndata on multiple outcomes are collected and it is of interest to explore\neffects on multiple outcomes simultaneously. Such designs can be particularly\nuseful in patient-centered research, where different outcomes might be more or\nless important to different patients. In this paper we propose scaled effect\nmeasures (via potential outcome notation) that translate effects on multiple\noutcomes to a common scale, using mean-variance and median-interquartile-range\n-based standardizations. We present efficient, nonparametric, doubly robust\nmethods for estimating these scaled effects (and weighted average summary\nmeasures), and for testing the null hypothesis that treatment affects all\noutcomes equally. We also discuss methods for exploring how treatment effects\ndepend on covariates (i.e., effect modification). In addition to describing\nefficiency theory for our estimands and the asymptotic behavior of our\nestimators, we illustrate the methods in a simulation study and a data\nanalysis. Importantly, and in contrast to much of the literature concerning\neffects on multiple outcomes, our methods are nonparametric and can be used not\nonly in randomized trials to yield increased efficiency, but also in\nobservational studies with high-dimensional covariates to reduce confounding\nbias.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 22:11:34 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 14:09:08 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 22:47:03 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Kangovi", "Shreya", ""], ["Mitra", "Nandita", ""]]}, {"id": "1608.02333", "submitter": "Juha Karvanen", "authors": "Juha Karvanen and Mikko J. Sillanp\\\"a\\\"a", "title": "Prioritizing covariates in the planning of future studies in the\n  meta-analytic framework", "comments": null, "journal-ref": "Biometrical Journal, Volume 59, Issue 1, Pages 110-125, 2017", "doi": "10.1002/bimj.201600067", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science can be seen as a sequential process where each new study augments\nevidence to the existing knowledge. To have the best prospects to make an\nimpact in this process, a new study should be designed optimally taking into\naccount the previous studies and other prior information. We propose a formal\napproach for the covariate prioritization, i.e., the decision about the\ncovariates to be measured in a new study. The decision criteria can be based on\nconditional power, change of the p-value, change in lower confidence limit,\nKullback-Leibler divergence, Bayes factors, Bayesian false discovery rate or\ndifference between prior and posterior expectation. The criteria can be also\nused for decisions on the sample size. As an illustration, we consider\ncovariate prioritization based on genome-wide association studies for\nC-reactive protein levels and make suggestions on the genes to be studied\nfurther.\n  keywords: design; evidence-based medicine; meta-analysis; power; scientific\nmethod\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 06:51:29 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Karvanen", "Juha", ""], ["Sillanp\u00e4\u00e4", "Mikko J.", ""]]}, {"id": "1608.02740", "submitter": "Luca Rossini", "authors": "Monica Billio, Roberto Casarin, Luca Rossini", "title": "Bayesian nonparametric sparse VAR models", "comments": "Forthcoming in \"Journal of Econometrics\" ---- Revised Version of the\n  paper \"Bayesian nonparametric Seemingly Unrelated Regression Models\" ----\n  Supplementary Material available on request", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional vector autoregressive (VAR) models require a large number of\nparameters to be estimated and may suffer of inferential problems. We propose a\nnew Bayesian nonparametric (BNP) Lasso prior (BNP-Lasso) for high-dimensional\nVAR models that can improve estimation efficiency and prediction accuracy. Our\nhierarchical prior overcomes overparametrization and overfitting issues by\nclustering the VAR coefficients into groups and by shrinking the coefficients\nof each group toward a common location. Clustering and shrinking effects\ninduced by the BNP-Lasso prior are well suited for the extraction of causal\nnetworks from time series, since they account for some stylized facts in\nreal-world networks, which are sparsity, communities structures and\nheterogeneity in the edges intensity. In order to fully capture the richness of\nthe data and to achieve a better understanding of financial and macroeconomic\nrisk, it is therefore crucial that the model used to extract network accounts\nfor these stylized facts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 09:30:37 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 12:42:16 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 09:14:23 GMT"}, {"version": "v4", "created": "Mon, 31 Jul 2017 10:48:01 GMT"}, {"version": "v5", "created": "Thu, 10 May 2018 07:16:42 GMT"}, {"version": "v6", "created": "Mon, 29 Oct 2018 13:27:43 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Billio", "Monica", ""], ["Casarin", "Roberto", ""], ["Rossini", "Luca", ""]]}, {"id": "1608.02812", "submitter": "Arman Kheirati Roonizi", "authors": "Arman Kheirati Roonizi", "title": "A Nonlinear Differential Equation for Generating Warping Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given set of functions $y_i(t)$ and $x(t)$ such that $y_i(t) = a_i\nx\\left[h_i(t)\\right]$ with $a_i$ being an unknown amplitude with low changes in\ntime (or $\\frac{\\Delta a_i}{a^2_i} << 1$) and $h_i(t)$ an unknown warping\nfunction, the paper shows that $h_i(t)$ can be described using a non-linear\ndifferential equation. The differential equation then can be utilized to\nestimate the warping function $h_i(t)$ using a nonlinear least-squares\noptimization. This differential equation can also be useful for reducing and\nanalyzing phase variability in data sequences. Results, obtained on synthetic\ncurves, showed that the proposed method is effective in aligning the curves.\nThe obtained aligned curves exhibit variation only in amplitude, and phase\nvariation can be removed efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 10:32:22 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 10:26:20 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 17:08:28 GMT"}, {"version": "v4", "created": "Fri, 15 May 2020 06:51:47 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Roonizi", "Arman Kheirati", ""]]}, {"id": "1608.02990", "submitter": "Carlo Berzuini Professor", "authors": "Carlo Berzuini, Hui Guo, Stephen Burgess and Luisa Bernardinelli", "title": "Bayesian Mendelian Randomization", "comments": "21 pages, 6 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our Bayesian approach to Mendelian Randomisation uses multiple instruments to\nassess the putative causal effect of an exposure on an outcome. The approach is\nrobust to violations of the (untestable) Exclusion Restriction condition, and\nhence it does not require instruments to be independent of the outcome\nconditional on the exposure and on the confounders of the exposure-outcome\nrelationship. The Bayesian approach offers a rigorous handling of the\nuncertainty (e.g. about the estimated instrument-exposure associations),\nfreedom from asymptotic approximations of the null distribution and the\npossibility to elaborate the model in any direction of scientific relevance. We\nillustrate the last feature with the aid of a study of the metabolic mediators\nof the disease-inducing effects of obesity, where we elaborate the model to\ninvestigate whether the causal effect of interest interacts with a covariate.\nThe proposed model contains a vector of unidentifiable parameters, $\\beta$,\nwhose $j$th element represents the pleiotropic (i.e., not mediated by the\nexposure) component of the association of instrument $j$ with the outcome. We\ndeal with the incomplete identifiability by assuming that the pleiotropic\neffect of some instruments is null, or nearly so, formally by imposing on\n$\\beta$ Carvalho's horseshoe shrinkage prior, in such a way that different\ncomponents of $\\beta$ are subjected to different degrees of shrinking,\nadaptively and in accord with the compatibility of each individual instrument\nwith the hypothesis of no pleiotropy. This prior requires a minimal input from\nthe user. We present the results of a simulation study into the performance of\nthe proposed method under different types of pleiotropy and sample sizes.\nComparisons with the performance of the weighted median estimator are made.\nChoice of the prior and inference via Markov chain Monte Carlo are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 22:11:46 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 15:15:53 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 15:28:25 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Berzuini", "Carlo", ""], ["Guo", "Hui", ""], ["Burgess", "Stephen", ""], ["Bernardinelli", "Luisa", ""]]}, {"id": "1608.03012", "submitter": "Alexander Petersen", "authors": "Alexander Petersen and Hans-Georg M\\\"uller", "title": "Fr\\'echet Regression for Random Objects with Euclidean Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly, statisticians are faced with the task of analyzing complex data\nthat are non-Euclidean and specifically do not lie in a vector space. To\naddress the need for statistical methods for such data, we introduce the\nconcept of Fr\\'echet regression. This is a general approach to regression when\nresponses are complex random objects in a metric space and predictors are in\n$\\mathcal{R}^p$, achieved by extending the classical concept of a Fr\\'echet\nmean to the notion of a conditional Fr\\'echet mean. We develop generalized\nversions of both global least squares regression and local weighted least\nsquares smoothing. The target quantities are appropriately defined population\nversions of global and local regression for response objects in a metric space.\nWe derive asymptotic rates of convergence for the corresponding fitted\nregressions using observed data to the population targets under suitable\nregularity conditions by applying empirical process methods. For the special\ncase of random objects that reside in a Hilbert space, such as regression\nmodels with vector predictors and functional data as responses, we obtain a\nlimit distribution. The proposed methods have broad applicability. Illustrative\nexamples include responses that consist of probability distributions and\ncorrelation matrices, and we demonstrate both global and local Fr\\'echet\nregression for demographic and brain imaging data. Local Fr\\'echet regression\nis also illustrated via a simulation with response data which lie on the\nsphere.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 00:41:03 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 20:56:13 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 20:05:40 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Petersen", "Alexander", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1608.03069", "submitter": "Meng Hwee Victor Ong", "authors": "Victor M-H. Ong, David J. Nott, Minh-Ngoc Tran, Scott A. Sisson,\n  Christopher C. Drovandi", "title": "Variational Bayes with Synthetic Likelihood", "comments": "30 pages; 7 figures", "journal-ref": "Stat.Comput. 28 (2018) 971-988", "doi": "10.1007/s11222-017-9773-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic likelihood is an attractive approach to likelihood-free inference\nwhen an approximately Gaussian summary statistic for the data, informative for\ninference about the parameters, is available. The synthetic likelihood method\nderives an approximate likelihood function from a plug-in normal density\nestimate for the summary statistic, with plug-in mean and covariance matrix\nobtained by Monte Carlo simulation from the model. In this article, we develop\nalternatives to Markov chain Monte Carlo implementations of Bayesian synthetic\nlikelihoods with reduced computational overheads. Our approach uses stochastic\ngradient variational inference methods for posterior approximation in the\nsynthetic likelihood context, employing unbiased estimates of the log\nlikelihood. We compare the new method with a related likelihood free\nvariational inference technique in the literature, while at the same time\nimproving the implementation of that approach in a number of ways. These new\nalgorithms are feasible to implement in situations which are challenging for\nconventional approximate Bayesian computation (ABC) methods, in terms of the\ndimensionality of the parameter and summary statistic.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 07:55:02 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Ong", "Victor M-H.", ""], ["Nott", "David J.", ""], ["Tran", "Minh-Ngoc", ""], ["Sisson", "Scott A.", ""], ["Drovandi", "Christopher C.", ""]]}, {"id": "1608.03109", "submitter": "Jingyi Jessica Li", "authors": "Xin Tong, Yang Feng, Jingyi Jessica Li", "title": "Neyman-Pearson (NP) classification algorithms and NP receiver operating\n  characteristics (NP-ROC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many binary classification applications such as disease diagnosis and spam\ndetection, practitioners often face great needs to control type I errors (i.e.,\nthe conditional probability of misclassifying a class 0 observation as class 1)\nso that it remains below a desired threshold. To address this need, the\nNeyman-Pearson (NP) classification paradigm is a natural choice; it minimizes\ntype II error (i.e., the conditional probability of misclassifying a class 1\nobservation as class 0) while enforcing an upper bound, $\\alpha$, on the type I\nerror. Although the NP paradigm has a century-long history in hypothesis\ntesting, it has not been well recognized and implemented in classification\nschemes. Common practices that directly limit the empirical type I error to no\nmore than $\\alpha$ do not satisfy the type I error control objective because\nthe resulting classifiers are still likely to have type I errors much larger\nthan $\\alpha$. As a result, the NP paradigm has not been properly implemented\nfor many classification scenarios in practice. In this work, we develop the\nfirst umbrella algorithm that implements the NP paradigm for all scoring-type\nclassification methods, including popular methods such as logistic regression,\nsupport vector machines and random forests. Powered by this umbrella algorithm,\nwe propose a novel graphical tool for NP classification methods: NP receiver\noperating characteristic (NP-ROC) bands, motivated by the popular receiver\noperating characteristic (ROC) curves. NP-ROC bands will help choose $\\alpha$\nin a data adaptive way and compare different NP classifiers. We demonstrate the\nuse and properties of the NP umbrella algorithm and NP-ROC bands, available in\nthe R package nproc, through simulation and real data case studies.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 10:03:12 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 08:38:16 GMT"}, {"version": "v3", "created": "Wed, 27 Sep 2017 17:11:50 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Tong", "Xin", ""], ["Feng", "Yang", ""], ["Li", "Jingyi Jessica", ""]]}, {"id": "1608.03154", "submitter": "Almut Veraart", "authors": "Almut E. D. Veraart", "title": "Modelling, simulation and inference for multivariate time series of\n  counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new continuous-time modelling framework for\nmultivariate time series of counts which have an infinitely divisible marginal\ndistribution. The model is based on a mixed moving average process driven by\nL\\'{e}vy noise - called a trawl process - where the serial correlation and the\ncross-sectional dependence are modelled independently of each other. Such\nprocesses can exhibit short or long memory. We derive a stochastic simulation\nalgorithm and a statistical inference method for such processes. The new\nmethodology is then applied to high frequency financial data, where we\ninvestigate the relationship between the number of limit order submissions and\ndeletions in a limit order book.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 12:57:29 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Veraart", "Almut E. D.", ""]]}, {"id": "1608.03686", "submitter": "Zemin Zheng", "authors": "Mohammad Taha Bahadori, Zemin Zheng, Yan Liu, Jinchi Lv", "title": "Scalable Interpretable Multi-Response Regression via SEED", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse reduced-rank regression is an important tool to uncover meaningful\ndependence structure between large numbers of predictors and responses in many\nbig data applications such as genome-wide association studies and social media\nanalysis. Despite the recent theoretical and algorithmic advances, scalable\nestimation of sparse reduced-rank regression remains largely unexplored. In\nthis paper, we suggest a scalable procedure called sequential estimation with\neigen-decomposition (SEED) which needs only a single top-$r$ singular value\ndecomposition to find the optimal low-rank and sparse matrix by solving a\nsparse generalized eigenvalue problem. Our suggested method is not only\nscalable but also performs simultaneous dimensionality reduction and variable\nselection. Under some mild regularity conditions, we show that SEED enjoys nice\nsampling properties including consistency in estimation, rank selection,\nprediction, and model selection. Numerical studies on synthetic and real data\nsets show that SEED outperforms the state-of-the-art approaches for large-scale\nmatrix estimation problem.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 06:19:02 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Bahadori", "Mohammad Taha", ""], ["Zheng", "Zemin", ""], ["Liu", "Yan", ""], ["Lv", "Jinchi", ""]]}, {"id": "1608.03706", "submitter": "Xu He", "authors": "Xu He", "title": "Rotated sphere packing designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of space-filling designs called rotated sphere packing\ndesigns for computer experiments. The approach starts from the asymptotically\noptimal positioning of identical balls that covers the unit cube. Properly\nscaled, rotated, translated and extracted, such designs are excellent in\nmaximin distance criterion, low in discrepancy, good in projective uniformity\nand thus useful in both prediction and numerical integration purposes. We\nprovide a fast algorithm to construct such designs for any numbers of\ndimensions and points with R codes available online. Theoretical and numerical\nresults are also provided.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 08:09:21 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["He", "Xu", ""]]}, {"id": "1608.03769", "submitter": "Daniel Simpson", "authors": "Jon Wakefield, Daniel Simpson, and Jessica Godwin", "title": "Spatial Modeling, with Application to Complex Survey Data: Discussion of\n  \"Model-based Geostatistics for Prevalence Mapping in Low-Resource Settings\",\n  by Diggle and Giorgi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevalence mapping in low resource settings is an increasingly important\nendeavor to guide policy making and to spatially and temporally characterize\nthe burden of disease. We will focus our discussion on consideration of the\ncomplex design when analyzing survey data, and on spatial modeling. With\nrespect to the former, we consider two approaches: direct use of the weights,\nand a model-based approach using a spatial model to acknowledge clustering. For\nthe latter we consider continuously indexed Markovian Gaussian random field\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 12:16:14 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Wakefield", "Jon", ""], ["Simpson", "Daniel", ""], ["Godwin", "Jessica", ""]]}, {"id": "1608.03787", "submitter": "Haakon Bakka", "authors": "Haakon Bakka, Jarno Vanhatalo, Janine Illian, Daniel Simpson,\n  H{\\aa}vard Rue", "title": "Non-stationary Gaussian models with physical barriers", "comments": "The new version contains major changes and new materials, including a\n  much more appropriate proof of existence of solution to the SPDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical tools in spatial statistics are stationary models, like the\nMat\\'ern field. However, in some applications there are boundaries, holes, or\nphysical barriers in the study area, e.g. a coastline, and stationary models\nwill inappropriately smooth over these features, requiring the use of a\nnon-stationary model.\n  We propose a new model, the Barrier model, which is different from the\nestablished methods as it is not based on the shortest distance around the\nphysical barrier, nor on boundary conditions. The Barrier model is based on\nviewing the Mat\\'ern correlation, not as a correlation function on the shortest\ndistance between two points, but as a collection of paths through a\nSimultaneous Autoregressive (SAR) model. We then manipulate these local\ndependencies to cut off paths that are crossing the physical barriers. To make\nthe new SAR well behaved, we formulate it as a stochastic partial differential\nequation (SPDE) that can be discretised to represent the Gaussian field, with a\nsparse precision matrix that is automatically positive definite.\n  The main advantage with the Barrier model is that the computational cost is\nthe same as for the stationary model. The model is easy to use, and can deal\nwith both sparse data and very complex barriers, as shown in an application in\nthe Finnish Archipelago Sea. Additionally, the Barrier model is better at\nreconstructing the modified Horseshoe test function than the standard models\nused in R-INLA.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 13:32:13 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 11:48:02 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 16:00:04 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bakka", "Haakon", ""], ["Vanhatalo", "Jarno", ""], ["Illian", "Janine", ""], ["Simpson", "Daniel", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1608.03855", "submitter": "Zaid Sawlan", "authors": "Marco Iglesias, Zaid Sawlan, Marco Scavino, Raul Tempone, Christopher\n  Wood", "title": "Bayesian inferences of the thermal properties of a wall using\n  temperature and heat flux measurements", "comments": null, "journal-ref": "International Journal of Heat and Mass Transfer 116C (2018) pp.\n  417-431", "doi": "10.1016/j.ijheatmasstransfer.2017.09.022", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of the thermal properties of walls is essential for accurate\nbuilding energy simulations that are needed to make effective energy-saving\npolicies. These properties are usually investigated through in-situ\nmeasurements of temperature and heat flux over extended time periods. The\none-dimensional heat equation with unknown Dirichlet boundary conditions is\nused to model the heat transfer process through the wall. In [F. Ruggeri, Z.\nSawlan, M. Scavino, R. Tempone, A hierarchical Bayesian setting for an inverse\nproblem in linear parabolic PDEs with noisy boundary conditions, Bayesian\nAnalysis 12 (2) (2017) 407--433], it was assessed the uncertainty about the\nthermal diffusivity parameter using different synthetic data sets. In this\nwork, we adapt this methodology to an experimental study conducted in an\nenvironmental chamber, with measurements recorded every minute from temperature\nprobes and heat flux sensors placed on both sides of a solid brick wall over a\nfive-day period. The observed time series are locally averaged, according to a\nsmoothing procedure determined by the solution of a criterion function\noptimization problem, to fit the required set of noise model assumptions.\nTherefore, after preprocessing, we can reasonably assume that the temperature\nand the heat flux measurements have stationary Gaussian noise and we can avoid\nworking with full covariance matrices. The results show that our technique\nreduces the bias error of the estimated parameters when compared to other\napproaches. Finally, we compute the information gain under two experimental\nsetups to recommend how the user can efficiently determine the duration of the\nmeasurement campaign and the range of the external temperature oscillation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 17:28:50 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 08:05:10 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 10:51:06 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Iglesias", "Marco", ""], ["Sawlan", "Zaid", ""], ["Scavino", "Marco", ""], ["Tempone", "Raul", ""], ["Wood", "Christopher", ""]]}, {"id": "1608.03913", "submitter": "William Weimin Yoo", "authors": "William Weimin Yoo, Subhashis Ghosal", "title": "Bayesian mode and maximum estimation and accelerated rates of\n  contraction", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the mode and maximum of an unknown\nregression function in the presence of noise. We adopt the Bayesian approach by\nusing tensor-product B-splines and endowing the coefficients with Gaussian\npriors. In the usual fixed-in-advanced sampling plan, we establish posterior\ncontraction rates for mode and maximum and show that they coincide with the\nminimax rates for this problem. To quantify estimation uncertainty, we\nconstruct credible sets for these two quantities that have high coverage\nprobabilities with optimal sizes. If one is allowed to collect data\nsequentially, we further propose a Bayesian two-stage estimation procedure,\nwhere a second stage posterior is built based on samples collected within a\ncredible set constructed from a first stage posterior. Under appropriate\nconditions on the radius of this credible set, we can accelerate optimal\ncontraction rates from the fixed-in-advanced setting to the minimax sequential\nrates. A simulation experiment shows that our Bayesian two-stage procedure\noutperforms single-stage procedure and also slightly improves upon a\nnon-Bayesian two-stage procedure.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 21:43:59 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 23:16:48 GMT"}, {"version": "v3", "created": "Sat, 12 Aug 2017 14:26:30 GMT"}, {"version": "v4", "created": "Thu, 15 Mar 2018 15:53:16 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Yoo", "William Weimin", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1608.03991", "submitter": "Mingyuan Zhou", "authors": "Siamak Zamani Dadaneh, Xiaoning Qian, Mingyuan Zhou", "title": "BNP-Seq: Bayesian Nonparametric Differential Expression Analysis of\n  Sequencing Count Data", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform differential expression analysis of high-throughput sequencing\ncount data under a Bayesian nonparametric framework, removing sophisticated\nad-hoc pre-processing steps commonly required in existing algorithms. We\npropose to use the gamma (beta) negative binomial process, which takes into\naccount different sequencing depths using sample-specific negative binomial\nprobability (dispersion) parameters, to detect differentially expressed genes\nby comparing the posterior distributions of gene-specific negative binomial\ndispersion (probability) parameters. These model parameters are inferred by\nborrowing statistical strength across both the genes and samples. Extensive\nexperiments on both simulated and real-world RNA sequencing count data show\nthat the proposed differential expression analysis algorithms clearly\noutperform previously proposed ones in terms of the areas under both the\nreceiver operating characteristic and precision-recall curves.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 15:16:56 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 20:04:42 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Dadaneh", "Siamak Zamani", ""], ["Qian", "Xiaoning", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1608.04039", "submitter": "Nan Zou", "authors": "Nan Zou and Dimitris N. Politis", "title": "Bootstrap Seasonal Unit Root Test under Periodic Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both seasonal unit roots and periodic variation can be prevalent in seasonal\ndata. When testing seasonal unit roots under periodic variation, the validity\nof the existing methods, such as the HEGY test, remains unknown. This paper\nanalyzes the behavior of the augmented HEGY test and the unaugmented HEGY test\nunder periodic variation. It turns out that the asymptotic null distributions\nof the HEGY statistics testing the single roots at $1$ or $-1$ when there is\nperiodic variation are identical to the asymptotic null distributions when\nthere is no periodic variation. On the other hand, the asymptotic null\ndistributions of the statistics testing any coexistence of roots at $1$, $-1$,\n$i$, or $-i$ when there is periodic variation are non-standard and are\ndifferent from the asymptotic null distributions when there is no periodic\nvariation. Therefore, when periodic variation exists, HEGY tests are not\ndirectly applicable to the joint tests for any concurrence of seasonal unit\nroots. As a remedy, bootstrap is proposed; in particular, the augmented HEGY\ntest with seasonal independent and identically distributed (iid) bootstrap and\nthe unaugmented HEGY test with seasonal block bootstrap are implemented. The\nconsistency of these bootstrap procedures is established. The finite-sample\nbehavior of these bootstrap tests is illustrated via simulation and prevails\nover their competitors'. Finally, these bootstrap tests are applied to detect\nthe seasonal unit roots in various economic time series.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 00:20:33 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 03:54:43 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 17:35:59 GMT"}, {"version": "v4", "created": "Sun, 22 Sep 2019 01:35:11 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zou", "Nan", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "1608.04167", "submitter": "Promit Ghosal Mr.", "authors": "Promit Ghosal and Bodhisattva Sen", "title": "On Univariate Convex Regression", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find the local rate of convergence of the least squares estimator (LSE) of\na one dimensional convex regression function when (a) a certain number of\nderivatives vanish at the point of interest, and (b) the true regression\nfunction is locally affine. In each case we derive the limiting distribution of\nthe LSE and its derivative. The pointwise limiting distributions depend on the\nsecond and third derivatives at 0 of the \"invelope function\" of the integral of\na two-sided Brownian motion with polynomial drifts. We also investigate the\ninconsistency of the LSE and the unboundedness of its derivative at the\nboundary of the domain of the covariate space. An estimator of the argmin of\nthe convex regression function is proposed and its asymptotic distribution is\nderived. Further, we present some new results on the characterization of the\nconvex LSE that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 01:34:17 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 16:00:21 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 20:03:47 GMT"}, {"version": "v4", "created": "Wed, 16 Nov 2016 01:47:33 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ghosal", "Promit", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1608.04341", "submitter": "Wendy Chan", "authors": "Wendy Chan", "title": "Partial Identification of Treatment Effects for Generalizability", "comments": "Presented at SREE 2016, Washington, D.C", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods to improve generalizations from nonrandom samples typically\ninvoke assumptions such as the strong ignorability of sample selection that are\noften controversial in practice to derive point estimates. Rather than focus on\nthe point estimate based inferences, this article considers inferences on\npartially identified estimates from fewer and weaker assumptions. We extend\npartial identification methods to causal generalization with nonrandom samples\nby using a cluster randomized trial in education. Bounds on the population\naverage treatment effect are derived under four cases, two under no assumptions\non the data, and two that assume bounded sample variation and monotonicity of\nresponse. This approach is amenable to incorporating population data frames to\ntighten bounds on the population average treatment effect. Under the\nassumptions of bounded sample variation and monotonicity, the interval\nestimates of the average treatment effect provide sufficiently informative\nbounds to rule out large treatment effects, which are consistent with the point\nestimates from the experimental study. This illustrates that partial\nidentification methods can provide an alternative perspective to causal\ngeneralization in the absence of strong ignorability of sample selection.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:46:14 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 17:08:06 GMT"}, {"version": "v3", "created": "Thu, 5 Jan 2017 04:50:51 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Chan", "Wendy", ""]]}, {"id": "1608.04478", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke", "title": "A Geometrical Approach to Topic Model Estimation", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and the Singular Value Decomposition (SVD) is a potentially\nuseful tool for learning such a low-rank matrix. However, the connection\nbetween this low-rank matrix and the singular vectors of the text corpus matrix\nare usually complicated and hard to spell out, so how to use SVD for learning\ntopic models faces challenges. In this paper, we overcome the challenge by\nrevealing a surprising insight: there is a low-dimensional simplex structure\nwhich can be viewed as a bridge between the low-rank matrix of interest and the\nSVD of the text corpus matrix, and allows us to conveniently reconstruct the\nformer using the latter. Such an insight motivates a new SVD approach to\nlearning topic models, which we analyze with delicate random matrix theory and\nderive the rate of convergence. We support our methods and theory numerically,\nusing both simulated data and real data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 04:31:52 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Ke", "Zheng Tracy", ""]]}, {"id": "1608.04615", "submitter": "Joseph Futoma", "authors": "Joseph Futoma, Mark Sendak, C. Blake Cameron, Katherine Heller", "title": "Scalable Modeling of Multivariate Longitudinal Data for Prediction of\n  Chronic Kidney Disease Progression", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of the future trajectory of a disease is an important challenge\nfor personalized medicine and population health management. However, many\ncomplex chronic diseases exhibit large degrees of heterogeneity, and\nfurthermore there is not always a single readily available biomarker to\nquantify disease severity. Even when such a clinical variable exists, there are\noften additional related biomarkers routinely measured for patients that may\nbetter inform the predictions of their future disease state. To this end, we\npropose a novel probabilistic generative model for multivariate longitudinal\ndata that captures dependencies between multivariate trajectories. We use a\nGaussian process based regression model for each individual trajectory, and\nbuild off ideas from latent class models to induce dependence between their\nmean functions. We fit our method using a scalable variational inference\nalgorithm to a large dataset of longitudinal electronic patient health records,\nand find that it improves dynamic predictions compared to a recent state of the\nart method. Our local accountable care organization then uses the model\npredictions during chart reviews of high risk patients with chronic kidney\ndisease.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 14:30:07 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Futoma", "Joseph", ""], ["Sendak", "Mark", ""], ["Cameron", "C. Blake", ""], ["Heller", "Katherine", ""]]}, {"id": "1608.04674", "submitter": "Eric Chi", "authors": "Bethany Lusch, Eric C. Chi, J. Nathan Kutz", "title": "Shape Constrained Tensor Decompositions using Sparse Representations in\n  Over-Complete Libraries", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider $N$-way data arrays and low-rank tensor factorizations where the\ntime mode is coded as a sparse linear combination of temporal elements from an\nover-complete library. Our method, Shape Constrained Tensor Decomposition\n(SCTD) is based upon the CANDECOMP/PARAFAC (CP) decomposition which produces\n$r$-rank approximations of data tensors via outer products of vectors in each\ndimension of the data. By constraining the vector in the temporal dimension to\nknown analytic forms which are selected from a large set of candidate\nfunctions, more readily interpretable decompositions are achieved and analytic\ntime dependencies discovered. The SCTD method circumvents traditional {\\em\nflattening} techniques where an $N$-way array is reshaped into a matrix in\norder to perform a singular value decomposition. A clear advantage of the SCTD\nalgorithm is its ability to extract transient and intermittent phenomena which\nis often difficult for SVD-based methods. We motivate the SCTD method using\nseveral intuitively appealing results before applying it on a number of\nhigh-dimensional, real-world data sets in order to illustrate the efficiency of\nthe algorithm in extracting interpretable spatio-temporal modes. With the rise\nof data-driven discovery methods, the decomposition proposed provides a viable\ntechnique for analyzing multitudes of data in a more comprehensible fashion.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:00:48 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Lusch", "Bethany", ""], ["Chi", "Eric C.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1608.04698", "submitter": "Dan Garant", "authors": "Dan Garant, David Jensen", "title": "Evaluating Causal Models by Comparing Interventional Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant method for evaluating the quality of causal models is to\nmeasure the graphical accuracy of the learned model structure. We present an\nalternative method for evaluating causal models that directly measures the\naccuracy of estimated interventional distributions. We contrast such\ndistributional measures with structural measures, such as structural Hamming\ndistance and structural intervention distance, showing that structural measures\noften correspond poorly to the accuracy of estimated interventional\ndistributions. We use a number of real and synthetic datasets to illustrate\nvarious scenarios in which structural measures provide misleading results with\nrespect to algorithm selection and parameter tuning, and we recommend that\ndistributional measures become the new standard for evaluating causal models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 18:32:24 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Garant", "Dan", ""], ["Jensen", "David", ""]]}, {"id": "1608.04700", "submitter": "Antoine Zambelli", "authors": "Antoine Zambelli", "title": "A Data-Driven Approach to Estimating the Number of Clusters in\n  Hierarchical Clustering", "comments": "6 pages, 7 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new methods for estimating the number of clusters in a\nhierarchical clustering framework in the hopes of creating a fully automated\nprocess with no human intervention. The methods are completely data-driven and\nrequire no input from the researcher, and as such are fully automated. They are\nquite easy to implement and not computationally intensive in the least. We\nanalyze performance on several simulated data sets and the Biobase Gene\nExpression Set, comparing our methods to the established Gap statistic and\nElbow methods and outperforming both in multi-cluster scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 18:35:09 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Zambelli", "Antoine", ""]]}, {"id": "1608.04717", "submitter": "Philip Ernst", "authors": "Philip Ernst, Robin Pemantle, Ville Satopaa, and Lyle Ungar", "title": "Bayesian aggregation of two forecasts in the partial information\n  framework", "comments": "21 pages, 5 figures in Statistics and Probability Letters (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We generalize the results of \\cite{SPU, SJPU} by showing how the Gaussian\naggregator may be computed in a setting where parameter estimation is not\nrequired. We proceed to provide an explicit formula for a \"one-shot\"\naggregation problem with two forecasters.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 19:28:21 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Ernst", "Philip", ""], ["Pemantle", "Robin", ""], ["Satopaa", "Ville", ""], ["Ungar", "Lyle", ""]]}, {"id": "1608.04867", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet (IRMAR), Wilfried Do Paco", "title": "Exact balanced random imputation for sample survey data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveys usually suffer from non-response, which decreases the effective\nsample size. Item non-response is typically handled by means of some form of\nrandom imputation if we wish to preserve the distribution of the imputed\nvariable. This leads to an increased variability due to the imputation\nvariance, and several approaches have been proposed for reducing this\nvariability. Balanced imputation consists in selecting residuals at random at\nthe imputation stage, in such a way that the imputation variance of the\nestimated total is eliminated or at least significantly reduced. In this work,\nwe propose an implementation of balanced random imputation which enables to\nfully eliminate the imputation variance. Following the approach in Cardot et\nal. (2013), we consider a regularized imputed estimator of a total and of a\ndistribution function, and we prove that they are consistent under the proposed\nimputation method. Some simulation results support our findings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 06:22:16 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 09:10:56 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Chauvet", "Guillaume", "", "IRMAR"], ["Paco", "Wilfried Do", ""]]}, {"id": "1608.04958", "submitter": "Isabel Fulcher", "authors": "Isabel R. Fulcher, Eric Tchetgen Tchetgen, Paige L. Williams", "title": "Mediation Analysis for Censored Survival Data under an Accelerated\n  Failure Time Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in causal mediation analysis have formalized conditions for\nestimating direct and indirect effects in various contexts. These approaches\nhave been extended to a number of models for survival outcomes including\naccelerated failure time (AFT) models which are widely used in a broad range of\nhealth applications given their intuitive interpretation. In this setting, it\nhas been suggested that under standard assumptions, the \"difference\" and\n\"product\" methods produce equivalent estimates of the indirect effect of\nexposure on the survival outcome. We formally show that these two methods may\nproduce substantially different estimates in the presence of censoring or\ntruncation, due to a form of model misspecification. Specifically, we establish\nthat while the product method remains valid under standard assumptions in the\npresence of independent censoring, the difference method can be biased in the\npresence of such censoring whenever the error distribution of the AFT model\nfails to be collapsible upon marginalizing over the mediator. This will\ninvariably be the case for most choices of mediator and outcome error\ndistributions. A notable exception arises in case of normal mediator-normal\noutcome where we show consistency of both difference and product estimators in\nthe presence of independent censoring. These results are confirmed in\nsimulation studies and two data applications.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 13:28:31 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 02:59:03 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 14:25:05 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Fulcher", "Isabel R.", ""], ["Tchetgen", "Eric Tchetgen", ""], ["Williams", "Paige L.", ""]]}, {"id": "1608.05012", "submitter": "Peter Rousseeuw", "authors": "Peter J. Rousseeuw, Jakob Raymaekers and Mia Hubert", "title": "A Measure of Directional Outlyingness with Applications to Image Data\n  and Video", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 2018, Vol. 27,\n  345-359", "doi": "10.1080/10618600.2017.1366912", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data covers a wide range of data types. They all have in common\nthat the observed objects are functions of of a univariate argument (e.g. time\nor wavelength) or a multivariate argument (say, a spatial position). These\nfunctions take on values which can in turn be univariate (such as the\nabsorbance level) or multivariate (such as the red/green/blue color levels of\nan image). In practice it is important to be able to detect outliers in such\ndata. For this purpose we introduce a new measure of outlyingness that we\ncompute at each gridpoint of the functions' domain. The proposed Directional\nOutlyingness} (DO) measure accounts for skewness in the data and only requires\nO(n) computation time per direction. We derive the influence function of the DO\nand compute a cutoff for outlier detection. The resulting heatmap and\nfunctional outlier map reflect local and global outlyingness of a function. To\nillustrate the performance of the method on real data it is applied to spectra,\nMRI images, and video surveillance data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 16:23:26 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 13:58:04 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Rousseeuw", "Peter J.", ""], ["Raymaekers", "Jakob", ""], ["Hubert", "Mia", ""]]}, {"id": "1608.05058", "submitter": "Vartan Choulakian", "authors": "V Choulakian", "title": "Globally Homogenous Mixture Components and Local Heterogeneity of Rank\n  Data", "comments": "50 pages , 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional methods of finding mixture components of rank data are mostly\nbased on distance and latent class models; these models may exhibit the\nphenomenon of masking of groups of small sizes; probably due to the spherical\nnature of rank data. Our approach diverges from the traditional methods; it is\ndirectional and uses a logical principle, the law of contradiction. We discuss\nthe concept of a mixture for rank data essentially in terms of the notion of\nglobal homogeneity of its group components. Local heterogeneities may appear\nonce the group components of the mixture have been discovered. This is done via\nthe exploratory analysis of rank data by taxicab correspondence analysis with\nthe nega coding: If the first factor is an affine function of the Borda count,\nthen we say that the rank data are globally homogenous, and local\nheterogeneities may appear on the consequent factors; otherwise, the rank data\neither are globally homogenous with outliers, or a mixture of globally\nhomogenous groups. Also we introduce a new coefficient of global homogeneity,\nGHC. GHC is based on the first taxicab dispersion measure: it takes values\nbetween 0 and 100\\%, so it is easily interpretable. GHC measures the extent of\ncrossing of scores of voters between two or three blocks seriation of the items\nwhere the Borda count statistic provides consensus ordering of the items on the\nfirst axis. Examples are provided. Key words: Preferences; rankings; Borda\ncount; global homogeneity coefficient; nega coding; law of contradiction;\nmixture; outliers; taxicab correspondence analysis; masking.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 19:29:38 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Choulakian", "V", ""]]}, {"id": "1608.05142", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, Blaise Melly, and Kaspar\n  W\\\"uthrich", "title": "Generic Inference on Quantile and Quantile Effect Functions for Discrete\n  Outcomes", "comments": "38 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile and quantile effect functions are important tools for descriptive\nand causal analyses due to their natural and intuitive interpretation. Existing\ninference methods for these functions do not apply to discrete random\nvariables. This paper offers a simple, practical construction of simultaneous\nconfidence bands for quantile and quantile effect functions of possibly\ndiscrete random variables. It is based on a natural transformation of\nsimultaneous confidence bands for distribution functions, which are readily\navailable for many problems. The construction is generic and does not depend on\nthe nature of the underlying problem. It works in conjunction with parametric,\nsemiparametric, and nonparametric modeling methods for observed and\ncounterfactual distributions, and does not depend on the sampling scheme. We\napply our method to characterize the distributional impact of insurance\ncoverage on health care utilization and obtain the distributional decomposition\nof the racial test score gap. We find that universal insurance coverage\nincreases the number of doctor visits across the entire distribution, and that\nthe racial test score gap is small at early ages but grows with age due to\nsocio economic factors affecting child development especially at the top of the\ndistribution. These are new, interesting empirical findings that complement\nprevious analyses that focused on mean effects only. In both applications, the\noutcomes of interest are discrete rendering existing inference methods invalid\nfor obtaining uniform confidence bands for observed and counterfactual quantile\nfunctions and for their difference -- the quantile effects functions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 00:46:37 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 03:05:16 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 02:42:06 GMT"}, {"version": "v4", "created": "Sun, 24 Jun 2018 17:13:31 GMT"}, {"version": "v5", "created": "Fri, 31 Aug 2018 02:19:14 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Melly", "Blaise", ""], ["W\u00fcthrich", "Kaspar", ""]]}, {"id": "1608.05289", "submitter": "Anower Hossain", "authors": "Anower Hossain, Karla Diaz-Ordaz and Jonathan W. Bartlett", "title": "Missing binary outcomes under covariate dependent missingness in cluster\n  randomised trials", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing outcomes are a commonly occurring problem for cluster randomised\ntrials, which can lead to biased and inefficient inference if ignored or\nhandled inappropriately. Two approaches for analysing such trials are\ncluster-level analysis and individual-level analysis. In this study, we\nassessed the performance of unadjusted cluster-level analysis, baseline\ncovariate adjusted cluster-level analysis, random effects logistic regression\n(RELR) and generalised estimating equations (GEE) when binary outcomes are\nmissing under a baseline covariate dependent missingness mechanism. Missing\noutcomes were handled using complete records analysis (CRA) and multilevel\nmultiple imputation (MMI). We analytically show that cluster-level analyses for\nestimating risk ratio (RR) using complete records are valid if the true data\ngenerating model has log link and the intervention groups have the same\nmissingness mechanism and the same covariate effect in the outcome model. We\nperformed a simulation study considering four different scenarios, depending on\nwhether the missingness mechanisms are the same or different between the\nintervention groups and whether there is an interaction between intervention\ngroup and baseline covariate in the outcome model. Based on the simulation\nstudy and analytical results, we give guidance on the conditions under which\neach approach is valid.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 15:14:44 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Hossain", "Anower", ""], ["Diaz-Ordaz", "Karla", ""], ["Bartlett", "Jonathan W.", ""]]}, {"id": "1608.05406", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel", "title": "How many imputations do you need? A two-stage calculation using a\n  quadratic rule", "comments": "17 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using multiple imputation, users often want to know how many imputations\nthey need. An old answer is that 2 to 10 imputations usually suffice, but this\nrecommendation only addresses the efficiency of point estimates. You may need\nmore imputations if, in addition to efficient point estimates, you also want\nstandard error (SE) estimates that would not change (much) if you imputed the\ndata again. For replicable SE estimates, the required number of imputations\nincreases quadratically with the fraction of missing information (not linearly,\nas previous studies have suggested). I recommend a two-stage procedure in which\nyou conduct a pilot analysis using a small-to-moderate number of imputations,\nthen use the results to calculate the number of imputations that are needed for\na final analysis whose SE estimates will have the desired level of\nreplicability. I implement the two-stage procedure using a new Stata command\ncalled how_many_imputations (available from SSC) and a new SAS macro called\n%mi_combine (available from the website missingdata.org).\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 16:57:37 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 03:07:44 GMT"}, {"version": "v3", "created": "Fri, 12 Jan 2018 04:55:59 GMT"}, {"version": "v4", "created": "Mon, 15 Jan 2018 04:57:06 GMT"}, {"version": "v5", "created": "Wed, 17 Jan 2018 22:04:43 GMT"}, {"version": "v6", "created": "Fri, 19 Jan 2018 12:28:04 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["von Hippel", "Paul T.", ""]]}, {"id": "1608.05428", "submitter": "Wagner Hugo Bonat Bonat W. H.", "authors": "Wagner Bonat and Jesus Olivero and Maria Grande-Vega and Miguel\n  F\\'arfan and John Fa", "title": "Modelling the covariance structure in marginal multivariate count\n  models: Hunting in Bioko Island", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible statistical modelling framework to deal with\nmultivariate count data along with longitudinal and repeated measures\nstructures. The covariance structure for each response variable is defined in\nterms of a covariance link function combined with a matrix linear predictor\ninvolving known matrices. To specify the joint covariance matrix for the\nmultivariate response vector the generalized Kronecker product is employed. The\ncount nature of the data is taken into account by means of the power dispersion\nfunction associated with the Poisson-Tweedie distribution. Furthermore, the\nscore information criterion is extended for selecting the components of the\nmatrix linear predictor. We analyse a dataset consisting of prey animals (the\nmain hunted species, the blue duiker \\textit{Philantomba monticola} and other\ntaxa) shot or snared for bushmeat by $52$ commercial hunters over a $33$-month\nperiod in Pico Basil\\'e, Bioko Island, Equatorial Guinea. By taking into\naccount the severely unbalanced repeated measures and longitudinal structures\ninduced by the hunters and a set of potential covariates (which in turn affect\nthe mean and covariance structures), our method can be used to indicate whether\nthere was statistical evidence of a decline in blue duikers and other species\nhunted during the study period. Determining whether observed drops in the\nnumber of animals hunted are indeed true is crucial to assess whether species\ndepletion effects are taking place in exploited areas anywhere in the world. We\nsuggest that our method can be used to more accurately understand the\ntrajectories of animals hunted for commercial or subsistence purposes, and\nestablish clear policies to ensure sustainable hunting practices.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 20:39:25 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Bonat", "Wagner", ""], ["Olivero", "Jesus", ""], ["Grande-Vega", "Maria", ""], ["F\u00e1rfan", "Miguel", ""], ["Fa", "John", ""]]}, {"id": "1608.05460", "submitter": "Gane Samb Lo", "authors": "Diam Ba, Cheikh Tidiane Seck, Gane Samb Lo", "title": "Asymptotic confidence bands for copulas based on the transformation\n  kernel estimator", "comments": "5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish asymptotic simultaneous confidence bands for the\ntransformation kernel estimator of copulas introduced in Omelka et al.(2009).\nTo this aim, we prove a uniform in bandwidth law of the iterated logarithm for\nthe maximal deviation of this estimator from its expectation, under smoothness\nconditions on the copula function. We also study the bias, which tends\nasymptotically and uniformly to zero with the same precise rate. Some\nsimulation experiments are finally provided to support our results\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 00:39:01 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Ba", "Diam", ""], ["Seck", "Cheikh Tidiane", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1608.05465", "submitter": "Rob Tibshirani", "authors": "Leying Guan, Zhou Fan and Robert Tibshirani", "title": "Regularization for supervised learning via the \"hubNet\" procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for supervised learning. The hubNet procedure fits a\nhub-based graphical model to the predictors, to estimate the amount of\n\"connection\" that each predictor has with other predictors. This yields a set\nof predictor weights that are then used in a regularized regression such as the\nlasso or elastic net. The resulting procedure is easy to implement, can\nsometimes yields higher prediction accuracy that the lasso, and can give\ninsights into the underlying structure of the predictors. HubNet can also be\ngeneralized seamlessly to other supervised problems such as regularized\nlogistic regression (and other GLMs), Cox's proportional hazards model, and\nnonlinear procedures such as random forests and boosting. We prove some\nrecovery results under a specialized model and illustrate the method on real\nand simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 00:48:41 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Guan", "Leying", ""], ["Fan", "Zhou", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1608.05515", "submitter": "Alan Huang", "authors": "Nanxi Zhang, Alan Huang", "title": "Profile likelihood ratio tests for parameter inferences in generalized\n  single-index models", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A profile likelihood ratio test is proposed for inferences on the index\ncoefficients in generalized single-index models. Key features include its\nsimplicity in implementation, invariance against parametrization, and\nexhibiting substantially less bias than standard Wald-tests in finite-sample\nsettings. Moreover, the R routine to carry out the profile likelihood ratio\ntest is demonstrated to be over two orders of magnitude faster than the\nrecently proposed generalized likelihood ratio test based on kernel regression.\nThe advantages of the method are demonstrated on various simulations and a data\nanalysis example.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 07:11:45 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 01:29:32 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 14:07:10 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zhang", "Nanxi", ""], ["Huang", "Alan", ""]]}, {"id": "1608.05565", "submitter": "Bruno Sudret", "authors": "Roland Sch\\\"obi and Bruno Sudret", "title": "Uncertainty propagation of p-boxes using sparse polynomial chaos\n  expansions", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.03.021", "report-no": "RSUQ-2016-009", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern engineering, physical processes are modelled and analysed using\nadvanced computer simulations, such as finite element models. Furthermore,\nconcepts of reliability analysis and robust design are becoming popular, hence,\nmaking efficient quantification and propagation of uncertainties an important\naspect. In this context, a typical workflow includes the characterization of\nthe uncertainty in the input variables. In this paper, input variables are\nmodelled by probability-boxes (p-boxes), accounting for both aleatory and\nepistemic uncertainty. The propagation of p-boxes leads to p-boxes of the\noutput of the computational model. A two-level meta-modelling approach is\nproposed using non-intrusive sparse polynomial chaos expansions to surrogate\nthe exact computational model and, hence, to facilitate the uncertainty\nquantification analysis. The capabilities of the proposed approach are\nillustrated through applications using a benchmark analytical function and two\nrealistic engineering problem settings. They show that the proposed two-level\napproach allows for an accurate estimation of the statistics of the response\nquantity of interest using a small number of evaluations of the exact\ncomputational model. This is crucial in cases where the computational costs are\ndominated by the runs of high-fidelity computational models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 10:49:43 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 08:53:40 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Sch\u00f6bi", "Roland", ""], ["Sudret", "Bruno", ""]]}, {"id": "1608.05583", "submitter": "Alison Parton", "authors": "Alison Parton, Paul G. Blackwell, Anna Skarin", "title": "Bayesian inference for continuous time animal movement based on steps\n  and turns", "comments": "8 pages, 2 figures, BAYSM 2016", "journal-ref": "Bayesian Statistics in Action: BAYSM 2016, Florence, Italy, June\n  19-21 (2017) Springer vol.194", "doi": "10.1007/978-3-319-54084-9", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although animal locations gained via GPS, etc. are typically observed on a\ndiscrete time scale, movement models formulated in continuous time are\npreferable in order to avoid the struggles experienced in discrete time when\nfaced with irregular observations or the prospect of comparing analyses on\ndifferent time scales. A class of models able to emulate a range of movement\nideas are defined by representing movement as a combination of stochastic\nprocesses describing both speed and bearing. A method for Bayesian inference\nfor such models is described through the use of a Markov chain Monte Carlo\napproach. Such inference relies on an augmentation of the animal's locations in\ndiscrete time that have been observed with error, with a more detailed movement\npath gained via simulation techniques. Analysis on real data on an individual\nreindeer (Rangifer tarandus) illustrates the presented methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 12:41:33 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 14:11:42 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Parton", "Alison", ""], ["Blackwell", "Paul G.", ""], ["Skarin", "Anna", ""]]}, {"id": "1608.05606", "submitter": "Clemence Leyrat", "authors": "Clemence Leyrat, Shaun R. Seaman, Ian R. White, Ian Douglas, Liam\n  Smeeth, Joseph Kim, Matthieu Resche-Rigon, James R. Carpenter and Elizabeth\n  J. Williamson", "title": "Propensity score analysis with partially observed confounders: how\n  should multiple imputation be used?", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse probability of treatment weighting (IPTW) is a popular propensity\nscore (PS)-based approach to estimate causal effects in observational studies\nat risk of confounding bias. A major issue when estimating the PS is the\npresence of partially observed covariates. Multiple imputation (MI) is a\nnatural approach to handle missing data on covariates, but its use in the PS\ncontext raises three important questions: (i) should we apply Rubin's rules to\nthe IPTW treatment effect estimates or to the PS estimates themselves? (ii)\ndoes the outcome have to be included in the imputation model? (iii) how should\nwe estimate the variance of the IPTW estimator after MI? We performed a\nsimulation study focusing on the effect of a binary treatment on a binary\noutcome with three confounders (two of them partially observed). We used MI\nwith chained equations to create complete datasets and compared three ways of\ncombining the results: combining treatment effect estimates (MIte); combining\nthe PS across the imputed datasets (MIps); or combining the PS parameters and\nestimating the PS of the average covariates across the imputed datasets\n(MIpar). We also compared the performance of these methods to complete case\n(CC) analysis and the missingness pattern (MP) approach, a method which uses a\ndifferent PS model for each pattern of missingness. We also studied empirically\nthe consistency of these 3 MI estimators. Under a missing at random (MAR)\nmechanism, CC and MP analyses were biased in most cases when estimating the\nmarginal treatment effect, whereas MI approaches had good performance in\nreducing bias as long as the outcome was included in the imputation model.\nHowever, only MIte was unbiased in all the studied scenarios and Rubin's rules\nprovided good variance estimates for MIte.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 14:05:58 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Leyrat", "Clemence", ""], ["Seaman", "Shaun R.", ""], ["White", "Ian R.", ""], ["Douglas", "Ian", ""], ["Smeeth", "Liam", ""], ["Kim", "Joseph", ""], ["Resche-Rigon", "Matthieu", ""], ["Carpenter", "James R.", ""], ["Williamson", "Elizabeth J.", ""]]}, {"id": "1608.05670", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "Barbora Pe\\v{s}tov\\'a and Michal Pe\\v{s}ta", "title": "Change Point in Panel Data with Small Fixed Panel Size: Ratio and\n  Non-Ratio Test Statistics", "comments": "arXiv admin note: substantial text overlap with arXiv:1509.01291", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal is to develop and, consequently, compare stochastic methods for\ndetection whether a structural change in panel data occurred at some unknown\ntime or not. Panel data of our interest consist of a moderate or relatively\nlarge number of panels, while the panels contain a small number of\nobservations. Testing procedures to detect a possible common change in means of\nthe panels are established. Ratio and non-ratio type test statistics are\nconsidered. Their asymptotic distributions under the no change null hypothesis\nare derived. Moreover, we prove the consistency of the tests under the\nalternative. The main advantage of the ratio type statistics compared to the\nnon-ratio ones is that the variance of the observations neither has to be known\nnor estimated. A simulation study reveals that the proposed ratio statistic\noutperforms the non-ratio one by keeping the significance level under the null,\nmainly when stronger dependence within the panel is taken into account.\nHowever, the non-ratio statistic rejects the null in the simulations more often\nthan it should, which yields higher power compared to the ratio statistic.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 16:55:06 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Pe\u0161tov\u00e1", "Barbora", ""], ["Pe\u0161ta", "Michal", ""]]}, {"id": "1608.05815", "submitter": "Antony Overstall", "authors": "Antony M. Overstall, James M. McGree, Christopher C. Drovandi", "title": "An approach for finding fully Bayesian optimal designs using\n  normal-based approximations to loss functions", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of decision-theoretic Bayesian optimal designs is complicated\nby the significant computational challenge of minimising an analytically\nintractable expected loss function over a, potentially, high-dimensional design\nspace. A new general approach for approximately finding Bayesian optimal\ndesigns is proposed which uses computationally efficient normal-based\napproximations to posterior summaries to aid in approximating the expected\nloss. This new approach is demonstrated on illustrative, yet challenging,\nexamples including hierarchical models for blocked experiments, and\nexperimental aims of parameter estimation and model discrimination. Where\npossible, the results of the proposed methodology are compared, both in terms\nof performance and computing time, to results from using computationally more\nexpensive, but potentially more accurate, Monte Carlo approximations. Moreover\nthe methodology is also applied to problems where the use of Monte Carlo\napproximations is computationally infeasible.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 13:00:42 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 15:45:22 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Overstall", "Antony M.", ""], ["McGree", "James M.", ""], ["Drovandi", "Christopher C.", ""]]}, {"id": "1608.05913", "submitter": "Toby Kenney", "authors": "Toby Kenney and Hong Gu", "title": "The Adequate Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a fundamental disconnect between what is tested in a model adequacy\ntest, and what we would like to test. The usual approach is to test the null\nhypothesis \"Model M is the true model.\" However, Model M is never the true\nmodel. A model might still be useful even if we have enough data to reject it.\nIn this paper, we present a technique to assess the adequacy of a model from\nthe philosophical standpoint that we know the model is not true, but we want to\nknow if it is useful.\n  Our solution to this problem is to measure the parameter uncertainty in our\nestimates caused by the model uncertainty. We use bootstrap inference on\nsamples of a smaller size, for which the model cannot be rejected. We use a\nmodel adequacy test to choose a bootstrap size with limited probability of\nrejecting the model and perform inference for samples of this size based on a\nnonparametric bootstrap. Our idea is that if we base our inference on a sample\nsize at which we do not reject the model, then we should be happy with this\ninference, because we would have been confident in it if our original dataset\nhad been this size.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 10:35:10 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Kenney", "Toby", ""], ["Gu", "Hong", ""]]}, {"id": "1608.05973", "submitter": "Yi-An Ma", "authors": "Yi-An Ma and Emily B. Fox and Tianqi Chen and Lei Wu", "title": "Irreversible Samplers from Jump and Continuous Markov Processes", "comments": null, "journal-ref": "Stat. Comput. (2018) 1-26", "doi": "10.1007/s11222-018-9802-x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose irreversible versions of the Metropolis Hastings\n(MH) and Metropolis adjusted Langevin algorithm (MALA) with a main focus on the\nlatter. For the former, we show how one can simply switch between different\nproposal and acceptance distributions upon rejection to obtain an irreversible\njump sampler (I-Jump). The resulting algorithm has a simple implementation akin\nto MH, but with the demonstrated benefits of irreversibility. We then show how\nthe previously proposed MALA method can also be extended to exploit\nirreversible stochastic dynamics as proposal distributions in the I-Jump\nsampler. Our experiments explore how irreversibility can increase the\nefficiency of the samplers in different situations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 17:40:09 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 19:58:21 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 16:31:22 GMT"}, {"version": "v4", "created": "Thu, 1 Feb 2018 09:10:33 GMT"}, {"version": "v5", "created": "Mon, 12 Mar 2018 23:59:03 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Ma", "Yi-An", ""], ["Fox", "Emily B.", ""], ["Chen", "Tianqi", ""], ["Wu", "Lei", ""]]}, {"id": "1608.06076", "submitter": "Fabio Clementi", "authors": "F. Clementi, M. Gallegati", "title": "New economic windows on income and wealth: The k-generalized family of\n  distributions", "comments": "LaTeX2e; 14 pages with 3 figures", "journal-ref": "Journal of Social and Economic Statistics, Vol: 6, Issue: 1,\n  Summer 2017, pp: 1-15", "doi": null, "report-no": null, "categories": "q-fin.GN physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, the distribution of income and wealth has been\ndeteriorating in many countries, leading to increased inequalities within and\nbetween societies. This tendency has revived the interest in the subject\ngreatly, yet it still receives very little attention within the realm of\nmainstream economic thinking. One reason for this is that the basic paradigm of\n\"standard economics\", the representative-agent General Equilibrium framework,\nis badly equipped to cope with distributional issues. Here we argue that when\nthe economy is treated as a complex system composed of many heterogeneous\ninteracting agents who give rise to emergent phenomena, to address the main\nstylized facts of income/wealth distribution requires leaving the toolbox of\nmainstream economics in favour of alternative approaches. The \"k-generalized\"\nfamily of income/wealth distributions, building on the categories of\ncomplexity, is an example of how advances in the field can be achieved within\nnew interdisciplinary research contexts.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 08:12:49 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Clementi", "F.", ""], ["Gallegati", "M.", ""]]}, {"id": "1608.06143", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi and Aurora Maccarone and Aongus McCarthy and Steve\n  McLaughlin and Gerald S. Buller", "title": "Object Depth Profile and Reflectivity Restoration from Sparse\n  Single-Photon Data Acquired in Underwater Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two new algorithms for the joint restoration of depth and\nreflectivity (DR) images constructed from time-correlated single-photon\ncounting (TCSPC) measurements. Two extreme cases are considered: (i) a reduced\nacquisition time that leads to very low photon counts and (ii) a highly\nattenuating environment (such as a turbid medium) which makes the reflectivity\nestimation more difficult at increasing range. Adopting a Bayesian approach,\nthe Poisson distributed observations are combined with prior distributions\nabout the parameters of interest, to build the joint posterior distribution.\nMore precisely, two Markov random field (MRF) priors enforcing spatial\ncorrelations are assigned to the DR images. Under some justified assumptions,\nthe restoration problem (regularized likelihood) reduces to a convex\nformulation with respect to each of the parameters of interest. This problem is\nfirst solved using an adaptive Markov chain Monte Carlo (MCMC) algorithm that\napproximates the minimum mean square parameter estimators. This algorithm is\nfully automatic since it adjusts the parameters of the MRFs by maximum marginal\nlikelihood estimation. However, the MCMC-based algorithm exhibits a relatively\nlong computational time. The second algorithm deals with this issue and is\nbased on a coordinate descent algorithm. Results on single-photon depth data\nfrom laboratory based underwater measurements demonstrate the benefit of the\nproposed strategy that improves the quality of the estimated DR images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 12:23:40 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Maccarone", "Aurora", ""], ["McCarthy", "Aongus", ""], ["McLaughlin", "Steve", ""], ["Buller", "Gerald S.", ""]]}, {"id": "1608.06174", "submitter": "David Gunawan", "authors": "D. Gunawan and M.-N. Tran and K. Suzuki and J. Dick and R. Kohn", "title": "Computationally Efficient Bayesian Estimation of High Dimensional\n  Copulas with Discrete and Mixed Margins", "comments": "63 pages and 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating copulas with discrete marginal distributions is challenging,\nespecially in high dimensions, because computing the likelihood contribution of\neach observation requires evaluating $2^{J}$ terms, with $J$ the number of\ndiscrete variables. Currently, data augmentation methods are used to carry out\ninference for discrete copula and, in practice, the computation becomes\ninfeasible when $J$ is large. Our article proposes two new fast Bayesian\napproaches for estimating high dimensional copulas with discrete margins, or a\ncombination of discrete and continuous margins. Both methods are based on\nrecent advances in Bayesian methodology that work with an unbiased estimate of\nthe likelihood rather than the likelihood itself, and our key observation is\nthat we can estimate the likelihood of a discrete copula unbiasedly with much\nless computation than evaluating the likelihood exactly or with current\nsimulation methods that are based on augmenting the model with latent\nvariables. The first approach builds on the pseudo marginal method that allows\nMarkov chain Monte Carlo simulation from the posterior distribution using only\nan unbiased estimate of the likelihood. The second approach is based on a\nVariational Bayes approximation to the posterior and also uses an unbiased\nestimate of the likelihood. We show that Monte Carlo and randomised quasi Monte\nCarlo methods can be used with both approaches to reduce the variability of the\nestimate of the likelihood, and hence enable us to carry out Bayesian inference\nfor high values of $J$ for some classes of copulas where the computation was\npreviously too expensive. Our article also introduces {\\em a correlated quasi\nrandom number pseudo marginal} approach into the literature. The methodology is\nillustrated through several real and simulated data examples.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 14:14:14 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 11:38:24 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 06:22:05 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Gunawan", "D.", ""], ["Tran", "M. -N.", ""], ["Suzuki", "K.", ""], ["Dick", "J.", ""], ["Kohn", "R.", ""]]}, {"id": "1608.06189", "submitter": "Wang Shaoxin", "authors": "Shaoxin Wang and Hu Yang and Chaoli Yao", "title": "On the penalized maximum likelihood estimation of high-dimensional\n  approximate factor model", "comments": "27 pages,8 tables, accepted by Computational Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we mainly focus on the penalized maximum likelihood estimation\n(MLE) of the high-dimensional approximate factor model. Since the current\nestimation procedure can not guarantee the positive definiteness of the error\ncovariance matrix, by reformulating the estimation of error covariance matrix\nand based on the lagrangian duality, we propose an accelerated proximal\ngradient (APG) algorithm to give a positive definite estimate of the error\ncovariance matrix. Combined the APG algorithm with EM method, a new estimation\nprocedure is proposed to estimate the high-dimensional approximate factor\nmodel. The new method not only gives positive definite estimate of error\ncovariance matrix but also improves the efficiency of estimation for the\nhigh-dimensional approximate factor model. Although the proposed algorithm can\nnot guarantee a global unique solution, it enjoys a desirable non-increasing\nproperty. The efficiency of the new algorithm on estimation and forecasting is\nalso investigated via simulation and real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 15:04:04 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 06:00:49 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wang", "Shaoxin", ""], ["Yang", "Hu", ""], ["Yao", "Chaoli", ""]]}, {"id": "1608.06196", "submitter": "Mason A. Porter", "authors": "Marya Bazzi, Lucas G. S. Jeub, Alex Arenas, Sam D. Howison, and Mason\n  A. Porter", "title": "A Framework for the Construction of Generative Models for Mesoscale\n  Structure in Multilayer Networks", "comments": "The abstract in the arXiv field is a slightly shortened version of\n  the abstract because of the character-count limit", "journal-ref": "Phys. Rev. Research 2, 023100 (2020)", "doi": "10.1103/PhysRevResearch.2.023100", "report-no": null, "categories": "cs.SI cond-mat.stat-mech nlin.AO physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks allow one to represent diverse and coupled connectivity\npatterns --- e.g., time-dependence, multiple subsystems, or both --- that arise\nin many applications and which are difficult or awkward to incorporate into\nstandard network representations. In the study of multilayer networks, it is\nimportant to investigate mesoscale (i.e., intermediate-scale) structures, such\nas dense sets of nodes known as communities, to discover network features that\nare not apparent at the microscale or the macroscale. The ill-defined nature of\nmesoscale structure and its ubiquity in empirical networks make it crucial to\ndevelop generative models that can produce the features that one encounters in\nempirical networks. Key purposes of such generative models include generating\nsynthetic networks with empirical properties of interest, benchmarking\nmesoscale-detection methods and algorithms, and inferring structure in\nempirical multilayer networks. In this paper, we introduce a framework for the\nconstruction of generative models for mesoscale structures in multilayer\nnetworks. Our framework provides a standardized set of generative models,\ntogether with an associated set of principles from which they are derived, for\nstudies of mesoscale structures in multilayer networks. It unifies and\ngeneralizes many existing models for mesoscale structures in fully-ordered\n(e.g., temporal) and unordered (e.g., multiplex) multilayer networks. One can\nalso use it to construct generative models for mesoscale structures in\npartially-ordered multilayer networks (e.g., networks that are both temporal\nand multiplex). Our framework has the ability to produce many features of\nempirical multilayer networks, and it explicitly incorporates a user-specified\ndependency structure between layers.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 15:33:16 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 01:16:28 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 09:15:00 GMT"}, {"version": "v4", "created": "Sat, 10 Aug 2019 21:41:13 GMT"}, {"version": "v5", "created": "Wed, 11 Dec 2019 16:38:45 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bazzi", "Marya", ""], ["Jeub", "Lucas G. S.", ""], ["Arenas", "Alex", ""], ["Howison", "Sam D.", ""], ["Porter", "Mason A.", ""]]}, {"id": "1608.06383", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Softplus Regressions and Convex Polytopes", "comments": "33 pages + 12 page appendix, 15 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To construct flexible nonlinear predictive distributions, the paper\nintroduces a family of softplus function based regression models that convolve,\nstack, or combine both operations by convolving countably infinite stacked\ngamma distributions, whose scales depend on the covariates. Generalizing\nlogistic regression that uses a single hyperplane to partition the covariate\nspace into two halves, softplus regressions employ multiple hyperplanes to\nconstruct a confined space, related to a single convex polytope defined by the\nintersection of multiple half-spaces or a union of multiple convex polytopes,\nto separate one class from the other. The gamma process is introduced to\nsupport the convolution of countably infinite (stacked) covariate-dependent\ngamma distributions. For Bayesian inference, Gibbs sampling derived via novel\ndata augmentation and marginalization techniques is used to deconvolve and/or\ndemix the highly complex nonlinear predictive distribution. Example results\ndemonstrate that softplus regressions provide flexible nonlinear decision\nboundaries, achieving classification accuracies comparable to that of kernel\nsupport vector machine while requiring significant less computation for\nout-of-sample prediction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 05:15:25 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1608.06663", "submitter": "Cheng Ouyang", "authors": "Ryan Martin, Cheng Ouyang, Francois Domagni", "title": "Efficient posterior inference on the volatility of a jump diffusion\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jump diffusion processes are widely used to model asset prices over time,\nmainly for their ability to capture complex discontinuous behavior, but\ninference on the model parameters remains a challenge. Here our goal is\nposterior inference on the volatility coefficient of the diffusion part of the\nprocess based on discrete samples. A Bayesian approach requires specification\nof a model for the jump part of the process, prior distributions for the\ncorresponding parameters, and computation of the joint posterior. Since the\nvolatility coefficient is our only interest, it would be desirable to avoid the\nmodeling and computational costs associated with the jump part of the process.\nTowards this, we consider a {\\em purposely misspecified model} that ignores the\njump part entirely. We work out precisely the asymptotic behavior of the\nBayesian posterior under the misspecified model, propose some simple\nmodifications to correct for the effects of misspecification, and demonstrate\nthat our modified posterior inference on the volatility is efficient in the\nsense that its asymptotic variance equals the no-jumps model Cram\\'er--Rao\nbound.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 22:38:24 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 23:14:02 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Martin", "Ryan", ""], ["Ouyang", "Cheng", ""], ["Domagni", "Francois", ""]]}, {"id": "1608.06791", "submitter": "Ryan Martin", "authors": "Ryan Martin and Yi Lin", "title": "Exact prior-free probabilistic inference in a class of non-regular\n  models", "comments": "11 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of standard statistical methods, such as maximum likelihood, is often\njustified based on their asymptotic properties. For suitably regular models,\nthis theory is standard but, when the model is non-regular, e.g., the support\ndepends on the parameter, these asymptotic properties may be difficult to\nassess. Recently, an inferential model (IM) framework has been developed that\nprovides valid prior-free probabilistic inference without the need for\nasymptotic justification. In this paper, we construct an IM for a class of\nhighly non-regular models with parameter-dependent support. This construction\nrequires conditioning, which is facilitated through the solution of a\nparticular differential equation. We prove that the plausibility intervals\nderived from this IM are exact confidence intervals, and we demonstrate their\nefficiency in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 12:27:45 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Martin", "Ryan", ""], ["Lin", "Yi", ""]]}, {"id": "1608.06802", "submitter": "Sebastian Lerch", "authors": "Fabian Kr\\\"uger, Sebastian Lerch, Thordis L. Thorarinsdottir, Tilmann\n  Gneiting", "title": "Predictive Inference Based on Markov Chain Monte Carlo Output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian inference, predictive distributions are typically in the form of\nsamples generated via Markov chain Monte Carlo (MCMC) or related algorithms. In\nthis paper, we conduct a systematic analysis of how to make and evaluate\nprobabilistic forecasts from such simulation output. Based on proper scoring\nrules, we develop a notion of consistency that allows to assess the adequacy of\nmethods for estimating the stationary distribution underlying the simulation\noutput. We then provide asymptotic results that account for the salient\nfeatures of Bayesian posterior simulators, and derive conditions under which\nchoices from the literature satisfy our notion of consistency. Importantly,\nthese conditions depend on the scoring rule being used, such that the choices\nof approximation method and scoring rule are intertwined. While the logarithmic\nrule requires fairly stringent conditions, the continuous ranked probability\nscore (CRPS) yields consistent approximations under minimal assumptions. These\nresults are illustrated in a simulation study and an economic data example.\nOverall, mixture-of-parameters approximations which exploit the parametric\nstructure of Bayesian models perform particularly well. Under the CRPS, the\nempirical distribution function is a simple and appealing alternative option.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 13:06:00 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 08:38:57 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 17:06:24 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 09:49:44 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2020 15:37:58 GMT"}, {"version": "v6", "created": "Wed, 24 Jun 2020 09:52:44 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Kr\u00fcger", "Fabian", ""], ["Lerch", "Sebastian", ""], ["Thorarinsdottir", "Thordis L.", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1608.06888", "submitter": "Wagner Hugo Bonat Bonat W. H.", "authors": "Wagner H. Bonat and Bent J{\\o}rgensen and C\\'elestin C. Kokonendji and\n  John Hinde and Clarice G. B. Dem\\'etrio", "title": "Extended Poisson-Tweedie: properties and regression models for count\n  data", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of discrete generalized linear models based on the\nclass of Poisson-Tweedie factorial dispersion models with variance of the form\n$\\mu + \\phi\\mu^p$, where $\\mu$ is the mean, $\\phi$ and $p$ are the dispersion\nand Tweedie power parameters, respectively. The models are fitted by using an\nestimating function approach obtained by combining the quasi-score and Pearson\nestimating functions for estimation of the regression and dispersion\nparameters, respectively. This provides a flexible and efficient regression\nmethodology for a comprehensive family of count models including Hermite,\nNeyman Type A, P\\'olya-Aeppli, negative binomial and Poisson-inverse Gaussian.\nThe estimating function approach allows us to extend the Poisson-Tweedie\ndistributions to deal with underdispersed count data by allowing negative\nvalues for the dispersion parameter $\\phi$. Furthermore, the Poisson-Tweedie\nfamily can automatically adapt to highly skewed count data with excessive\nzeros, without the need to introduce zero-inflated or hurdle components, by the\nsimple estimation of the power parameter. Thus, the proposed models offer a\nunified framework to deal with under, equi, overdispersed, zero-inflated and\nheavy-tailed count data. The computational implementation of the proposed\nmodels is fast, relying only on a simple Newton scoring algorithm. Simulation\nstudies showed that the estimating function approach provides unbiased and\nconsistent estimators for both regression and dispersion parameters. We\nhighlight the ability of the Poisson-Tweedie distributions to deal with count\ndata through a consideration of dispersion, zero-inflated and heavy tail\nindices, and illustrate its application with four data analyses. We provide an\n\\texttt{R} implementation and the data sets as supplementary materials.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 16:26:52 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 12:06:14 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Bonat", "Wagner H.", ""], ["J\u00f8rgensen", "Bent", ""], ["Kokonendji", "C\u00e9lestin C.", ""], ["Hinde", "John", ""], ["Dem\u00e9trio", "Clarice G. B.", ""]]}, {"id": "1608.06903", "submitter": "Shovan Chowdhury Shovan Chowdhury", "authors": "Shovan Chowdhury and Amarjit Kundu", "title": "Stochastic Comparison of Parallel Systems with Log-Lindley Distributed\n  Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study stochastic comparisons of parallel systems having\nlog-Lindley distributed components. These comparisons are carried out with\nrespect to reversed hazard rate and likelihood ratio ordering.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 17:17:48 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Chowdhury", "Shovan", ""], ["Kundu", "Amarjit", ""]]}, {"id": "1608.07014", "submitter": "Yanglei Song", "authors": "Yanglei Song, Georgios Fellouris", "title": "Sequential multiple testing with generalized error control: an\n  asymptotic optimality theory", "comments": "Accepted in Annals of Statistics", "journal-ref": "the Annals of Statistics 2019, Vol. 47, No. 3, 1776-1803", "doi": "10.1214/18-AOS1737", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sequential multiple testing problem is considered under two generalized\nerror metrics. Under the first one, the probability of at least $k$ mistakes,\nof any kind, is controlled. Under the second, the probabilities of at least\n$k_1$ false positives and at least $k_2$ false negatives are simultaneously\ncontrolled. For each formulation, the optimal expected sample size is\ncharacterized, to a first-order asymptotic approximation as the error\nprobabilities go to 0, and a novel multiple testing procedure is proposed and\nshown to be asymptotically efficient under every signal configuration. These\nresults are established when the data streams for the various hypotheses are\nindependent and each local log-likelihood ratio statistic satisfies a certain\nStrong Law of Large Numbers. In the special case of i.i.d. observations in each\nstream, the gains of the proposed sequential procedures over fixed-sample size\nschemes are quantified.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 04:08:15 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 00:38:33 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 13:06:21 GMT"}, {"version": "v4", "created": "Mon, 18 Jun 2018 13:40:41 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Song", "Yanglei", ""], ["Fellouris", "Georgios", ""]]}, {"id": "1608.07048", "submitter": "Tore Selland Kleppe", "authors": "Janne Mannseth, Tore Selland Kleppe, Hans J. Skaug", "title": "On the application of higher order symplectic integrators in Hamiltonian\n  Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the construction of new symplectic numerical integration schemes\nto be used in Hamiltonian Monte Carlo and study their efficiency. Two\nintegration schemes from Blanes et al. (2014), and a new scheme based on\noptimal acceptance probability, are considered as candidates to the commonly\nused leapfrog method. All integration schemes are tested within the framework\nof the No-U-Turn sampler (NUTS), both for a logistic regression model and a\nstudent $t$-model. The results show that the leapfrog method is inferior to all\nthe new methods both in terms of asymptotic expected acceptance probability for\na model problem and the and efficient sample size per computing time for the\nrealistic models.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 08:23:04 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Mannseth", "Janne", ""], ["Kleppe", "Tore Selland", ""], ["Skaug", "Hans J.", ""]]}, {"id": "1608.07173", "submitter": "Merle Behr", "authors": "Merle Behr, Chris Holmes, and Axel Munk", "title": "Multiscale Blind Source Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new methodology for statistical recovery of single linear\nmixtures of piecewise constant signals (sources) with unknown mixing weights\nand change points in a multiscale fashion. We show exact recovery within an\n$\\epsilon$-neighborhood of the mixture when the sources take only values in a\nknown finite alphabet. Based on this we provide the SLAM (Separates Linear\nAlphabet Mixtures) estimators for the mixing weights and sources. For Gaussian\nerror, we obtain uniform confidence sets and optimal rates (up to log-factors)\nfor all quantities. SLAM is efficiently computed as a nonconvex optimization\nproblem by a dynamic program tailored to the finite alphabet assumption. Its\nperformance is investigated in a simulation study. Finally, it is applied to\nassign copy-number aberrations from genetic sequencing data to different clones\nand to estimate their proportions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 14:30:59 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 16:20:36 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 09:39:56 GMT"}, {"version": "v4", "created": "Wed, 30 Aug 2017 10:11:20 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Behr", "Merle", ""], ["Holmes", "Chris", ""], ["Munk", "Axel", ""]]}, {"id": "1608.07180", "submitter": "Federico Ricciardi", "authors": "Federico Ricciardi, Alessandra Mattei and Fabrizia Mealli", "title": "Bayesian Inference for Sequential Treatments under Latent Sequential\n  Ignorability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on causal inference for longitudinal treatments, where units are\nassigned to treatments at multiple time points, aiming to assess the effect of\ndifferent treatment sequences on an outcome observed at a final point. A common\nassumption in similar studies is Sequential Ignorability (SI): treatment\nassignment at each time point is assumed independent of future potential\noutcomes given past observed outcomes and covariates. SI is questionable when\ntreatment participation depends on individual choices, and treatment assignment\nmay depend on unobservable quantities associated with future outcomes. We rely\non Principal Stratification to formulate a relaxed version of SI: Latent\nSequential Ignorability (LSI) assumes that treatment assignment is\nconditionally independent on future potential outcomes given past treatments,\ncovariates and principal stratum membership, a latent variable defined by the\njoint value of observed and missing intermediate outcomes. We evaluate SI and\nLSI, using theoretical arguments and simulation studies to investigate the\nperformance of the two assumptions when one holds and inference is conducted\nunder both. Simulations show that when SI does not hold, inference performed\nunder SI leads to misleading conclusions. Conversely, LSI generally leads to\ncorrect posterior distributions, irrespective of which assumption holds.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 14:44:07 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 11:36:56 GMT"}, {"version": "v3", "created": "Sun, 12 May 2019 11:39:41 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ricciardi", "Federico", ""], ["Mattei", "Alessandra", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "1608.07204", "submitter": "Junyong Park", "authors": "Iris Ivy Gauran, Junyong Park, Johan Lim, DoHwan Park, John Zylstra,\n  Thomas Peterson, Maricel Kann, John Spouge", "title": "Empirical Null Estimation using Discrete Mixture Distributions and its\n  Application to Protein Domain Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent mutation studies, analyses based on protein domain positions are\ngaining popularity over gene-centric approaches since the latter have\nlimitations in considering the functional context that the position of the\nmutation provides. This presents a large-scale simultaneous inference problem,\nwith hundreds of hypothesis tests to consider at the same time. This paper aims\nto select significant mutation counts while controlling a given level of Type I\nerror via False Discovery Rate (FDR) procedures. One main assumption is that\nthere exists a cut-off value such that smaller counts than this value are\ngenerated from the null distribution. We present several data-dependent methods\nto determine the cut-off value. We also consider a two-stage procedure based on\nscreening process so that the number of mutations exceeding a certain value\nshould be considered as significant mutations. Simulated and protein domain\ndata sets are used to illustrate this procedure in estimation of the empirical\nnull using a mixture of discrete distributions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 16:10:39 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Gauran", "Iris Ivy", ""], ["Park", "Junyong", ""], ["Lim", "Johan", ""], ["Park", "DoHwan", ""], ["Zylstra", "John", ""], ["Peterson", "Thomas", ""], ["Kann", "Maricel", ""], ["Spouge", "John", ""]]}, {"id": "1608.07482", "submitter": "Jun Li", "authors": "Ping-Shou Zhong, Jun Li", "title": "Test for Temporal Homogeneity of Means in High-dimensional Longitudinal\n  Data", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of testing temporal homogeneity of\n$p$-dimensional population mean vectors from the repeated measurements of $n$\nsubjects over $T$ times. To cope with the challenges brought by\nhigh-dimensional longitudinal data, we propose a test statistic that takes into\naccount not only the \"large $p$, large $T$ and small $n$\" situation, but also\nthe complex temporospatial dependence. The asymptotic distribution of the\nproposed test statistic is established under mild conditions. When the null\nhypothesis of temporal homogeneity is rejected, we further propose a binary\nsegmentation method shown to be consistent for multiple change-point\nidentification. Simulation studies and an application to fMRI data are provided\nto demonstrate the performance of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 15:01:36 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Zhong", "Ping-Shou", ""], ["Li", "Jun", ""]]}, {"id": "1608.07618", "submitter": "Bailey Fosdick", "authors": "Bailey K. Fosdick, Tyler H. McCormick, Thomas Brendan Murphy, Tin Lok\n  James Ng and Ted Westling", "title": "Multiresolution network models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing statistical and machine learning tools for social network\nanalysis focus on a single level of analysis. Methods designed for clustering\noptimize a global partition of the graph, whereas projection based approaches\n(e.g. the latent space model in the statistics literature) represent in rich\ndetail the roles of individuals. Many pertinent questions in sociology and\neconomics, however, span multiple scales of analysis. Further, many questions\ninvolve comparisons across disconnected graphs that will, inevitably be of\ndifferent sizes, either due to missing data or the inherent heterogeneity in\nreal-world networks. We propose a class of network models that represent\nnetwork structure on multiple scales and facilitate comparison across graphs\nwith different numbers of individuals. These models differentially invest\nmodeling effort within subgraphs of high density, often termed communities,\nwhile maintaining a parsimonious structure between said subgraphs. We show that\nour model class is projective, highlighting an ongoing discussion in the social\nnetwork modeling literature on the dependence of inference paradigms on the\nsize of the observed graph. We illustrate the utility of our method using data\non household relations from Karnataka, India.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 22:14:37 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 20:49:46 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 06:21:11 GMT"}, {"version": "v4", "created": "Tue, 26 Sep 2017 15:28:39 GMT"}, {"version": "v5", "created": "Thu, 5 Jul 2018 16:12:33 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Fosdick", "Bailey K.", ""], ["McCormick", "Tyler H.", ""], ["Murphy", "Thomas Brendan", ""], ["Ng", "Tin Lok James", ""], ["Westling", "Ted", ""]]}, {"id": "1608.07885", "submitter": "Li Ma", "authors": "Li Ma and Jialiang Mao", "title": "Fisher exact scanning for dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method---called Fisher exact scanning (FES)---for testing and\nidentifying variable dependency that generalizes Fisher's exact test on\n$2\\times 2$ contingency tables to $R\\times C$ contingency tables and continuous\nsample spaces. FES proceeds through scanning over the sample space using\nwindows in the form of $2\\times 2$ tables of various sizes, and on each window\ncompleting a Fisher's exact test. Based on a factorization of Fisher's\nmultivariate hypergeometric (MHG) likelihood into the product of the univariate\nhypergeometric likelihoods, we show that there exists a coarse-to-fine,\nsequential generative representation for the MHG model in the form of a\nBayesian network, which in turn implies the mutual independence (up to\ndeviation due to discreteness) among the Fisher's exact tests completed under\nFES. This allows an exact characterization of the joint null distribution of\nthe $p$-values and gives rise to an effective inference recipe through simple\nmultiple testing procedures such as \\v{S}id\\'{a}k and Bonferroni corrections,\neliminating the need for resampling. In addition, FES can characterize\ndependency through reporting significant windows after multiple testing\ncontrol. The computational complexity of FES is approximately linear in the\nsample size, which along with the avoidance of resampling makes it ideal for\nanalyzing massive data sets. We use extensive numerical studies to illustrate\nthe work of FES and compare it to several state-of-the-art methods for testing\ndependency in both statistical and computational performance. Finally, we apply\nFES to analyzing a microbiome data set and further investigate its relationship\nwith other popular dependency metrics in that context.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 01:45:24 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 19:36:08 GMT"}, {"version": "v3", "created": "Thu, 20 Oct 2016 18:56:34 GMT"}, {"version": "v4", "created": "Mon, 1 May 2017 19:46:21 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Ma", "Li", ""], ["Mao", "Jialiang", ""]]}, {"id": "1608.07921", "submitter": "Genya Kobayashi Mr.", "authors": "Genya Kobayashi and Kota Ogasawara", "title": "Bayesian Nonparametric Instrumental Variable Regression Approach to\n  Quantile Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study extends the Bayesian nonparametric instrumental variable\nregression model to determine the structural effects of covariates on the\nconditional quantile of the response variable. The error distribution is\nnonparametrically modelled using a Dirichlet mixture of bivariate normal\ndistributions. The mean functions include the smooth effects of the covariates\nrepresented using the spline functions in an additive manner. The conditional\nvariance of the second-stage error is also modelled using the spline functions\nsuch that it varies smoothly with the covariates. Accordingly, the proposed\nmodel allows for considerable flexibility in the shape of the quantile function\nwhile correcting for an endogeneity effect. The posterior inference for the\nproposed model is based on the Markov chain Monte Carlo method that requires no\nMetropolis-Hastings update. The approach is demonstrated using simulated and\nreal data on the death rate in Japan during the inter-war period.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 06:18:05 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Kobayashi", "Genya", ""], ["Ogasawara", "Kota", ""]]}, {"id": "1608.08076", "submitter": "Andrew Correia", "authors": "Andrew W. Correia", "title": "Bayesian Sequentially Monitored Multi-arm Experiments with Multiple\n  Comparison Adjustments", "comments": "25 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments play a major role in data-driven decision making\nacross many different fields and disciplines. In medicine, for example,\nrandomized controlled trials (RCTs) are the backbone of clinical trial\nmethodology for testing the efficacy of new drugs and therapies versus existing\ntreatments or placebo. In business and marketing, randomized experiments are\ntypically referred to as A/B tests when there are only two arms, or variants,\nin the experiment, and as multivariate A/B tests when there are more than two\narms. Typical applications of A/B tests include comparing the effectiveness of\ndifferent ad campaigns, evaluating how people respond to different website\nlayouts, or comparing different customer subpopulations to each other.\n  This paper focuses on multivariate A/B testing from a digital marketing\nperspective, and presents a method for the sequential monitoring of such\nexperiments while accounting for the issue of multiple comparisons. In adapting\nand combining the methods of two previous works, the method presented herein is\nstraightforward to implement using standard statistical software and performs\nquite well in various simulation studies, exhibiting better power and smaller\naverage sample sizes than comparable methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 14:39:58 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Correia", "Andrew W.", ""]]}, {"id": "1608.08096", "submitter": "Gerd Rosenkranz", "authors": "Gerd K. Rosenkranz", "title": "Clinical trials with rescue medication applied according to a\n  deterministic rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials in specific indications require the administration of rescue\nmedication in case a patient does not sufficiently respond to investigational\ntreatment. The application of additional treatment on an as needed basis causes\nproblems to the analysis and interpretation of the results of these studies\nsince the effect of the investigational treatment can be confounded by the\nadditional medication. Following-up all patients until study end and capturing\nall data is not fully addressing the issue. We present an analysis that takes\ncare of the fact that rescue is a study outcome and not a covariate when rescue\nmedication is administered according to a deterministic rule. This approach\nallows to clearly define a biological effect. For normally distributed\nlongitudinal data a practically unbiased estimator of the biological effect can\nbe obtained. The results are compared to an ITT analysis and an analysis on all\npatients not receiving rescue.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 15:03:05 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Rosenkranz", "Gerd K.", ""]]}, {"id": "1608.08126", "submitter": "Esa Ollila", "authors": "Esa Ollila, Ilya Soloveychik, David E. Tyler and Ami Wiesel", "title": "Simultaneous penalized M-estimation of covariance matrices using\n  geodesically convex optimization", "comments": "Submitted to Journal of Multivariate Analysis (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common assumption when sampling $p$-dimensional observations from $K$\ndistinct group is the equality of the covariance matrices. In this paper, we\npropose two penalized $M$-estimation approaches for the estimation of the\ncovariance or scatter matrices under the broader assumption that they may\nsimply be close to each other, and hence roughly deviate from some positive\ndefinite \"center\". The first approach begins by generating a pooled\n$M$-estimator of scatter based on all the data, followed by a penalised\n$M$-estimator of scatter for each group, with the penalty term chosen so that\nthe individual scatter matrices are shrunk towards the pooled scatter matrix.\nIn the second approach, we minimize the sum of the individual group\n$M$-estimation cost functions together with an additive joint penalty term\nwhich enforces some similarity between the individual scatter estimators, i.e.\nshrinkage towards a mutual center. In both approaches, we utilize the concept\nof geodesic convexity to prove the existence and uniqueness of the penalized\nsolution under general conditions. We consider three specific penalty functions\nbased on the Euclidean, the Riemannian, and the Kullback-Leibler distances. In\nthe second approach, the distance based penalties are shown to lead to\nestimators of the mutual center that are related to the arithmetic, the\nRiemannian and the harmonic means of positive definite matrices, respectively.\nA penalty based on an ellipticity measure is also considered which is\nparticularly useful for shape matrix estimators. Fixed point equations are\nderived for each penalty function and the benefits of the estimators are\nillustrated in regularized discriminant analysis problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 16:11:08 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Ollila", "Esa", ""], ["Soloveychik", "Ilya", ""], ["Tyler", "David E.", ""], ["Wiesel", "Ami", ""]]}, {"id": "1608.08199", "submitter": "Dimosthenis Tsagkrasoulis", "authors": "Dimosthenis Tsagkrasoulis, Pirro Hysi, Tim Spector, Giovanni Montana", "title": "Heritability maps of human face morphology through large-scale automated\n  three-dimensional phenotyping", "comments": "21 pages, 5 figures Updated content", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human face is a complex trait under strong genetic control, as evidenced\nby the striking visual similarity between twins. Nevertheless, heritability\nestimates of facial traits have often been surprisingly low or difficult to\nreplicate. Furthermore, the construction of facial phenotypes that correspond\nto naturally perceived facial features remains largely a mystery. We present\nhere a large-scale heritability study of face geometry that aims to address\nthese issues. High-resolution, three-dimensional facial models have been\nacquired on a cohort of $952$ twins recruited from the TwinsUK registry, and\nprocessed through a novel landmarking workflow, GESSA (Geodesic Ensemble\nSurface Sampling Algorithm). The algorithm places thousands of landmarks\nthroughout the facial surface and automatically establishes point-wise\ncorrespondence across faces. These landmarks enabled us to intuitively\ncharacterize facial geometry at a fine level of detail through curvature\nmeasurements, yielding accurate heritability maps of the human face\n(www.heritabilitymaps.info).\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:53:58 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 15:32:06 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Tsagkrasoulis", "Dimosthenis", ""], ["Hysi", "Pirro", ""], ["Spector", "Tim", ""], ["Montana", "Giovanni", ""]]}, {"id": "1608.08291", "submitter": "Mike Ludkovski", "authors": "Mike Ludkovski, Jimmy Risk, Howard Zail", "title": "Gaussian Process Models for Mortality Rates and Improvement Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Gaussian process (\"GP\") framework for modeling mortality rates\nand mortality improvement factors. GP regression is a nonparametric,\ndata-driven approach for determining the spatial dependence in mortality rates\nand jointly smoothing raw rates across dimensions, such as calendar year and\nage. The GP model quantifies uncertainty associated with smoothed historical\nexperience and generates full stochastic trajectories for out-of-sample\nforecasts. Our framework is well suited for updating projections when newly\navailable data arrives, and for dealing with \"edge\" issues where credibility is\nlower. We present a detailed analysis of Gaussian process model performance for\nUS mortality experience based on the CDC datasets. We investigate the\ninteraction between mean and residual modeling, Bayesian and non-Bayesian GP\nmethodologies, accuracy of in-sample and out-of-sample forecasting, and\nstability of model parameters. We also document the general decline, along with\nstrong age-dependency, in mortality improvement factors over the past few\nyears, contrasting our findings with the Society of Actuaries (\"SOA\") MP-2014\nand -2015 models that do not fully reflect these recent trends.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 00:55:11 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 06:55:35 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 22:48:30 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ludkovski", "Mike", ""], ["Risk", "Jimmy", ""], ["Zail", "Howard", ""]]}, {"id": "1608.08318", "submitter": "Yuan Liao", "authors": "Yuan Liao", "title": "A Note on Choosing the Threshold for Large Covariance Estimations in\n  Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note shows that for i.i.d. data, estimating large covariance matrices in\nfactor models can be casted using a simple plug-in method to choose the\nthreshold: $$\n\\mu_{jl}=\\frac{c_0}{\\sqrt{n}}\\Phi^{-1}(1-\\frac{\\alpha}{2p^2})\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\hat\nu_{ji}^2\\hat u_{li}^2}.$$ This is motivated by the tuning parameter suggested\nby Belloni et al. (2012) in the lasso literature. It also leads to the minimax\nrate of convergence of the large covariance matrix estimator. Previously, the\nminimaxity is achievable only when $n=o(p\\log p)$ by Fan et al. (2013), and now\nthis condition is weakened to $n=o(p^2\\log p)$. Here $n$ denotes the sample\nsize and $p$ denotes the dimension.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 04:07:22 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Liao", "Yuan", ""]]}, {"id": "1608.08323", "submitter": "Satoshi Aoki", "authors": "Satoshi Aoki and Takayuki Hibi", "title": "Markov bases for two-way change-point models of ladder determinantal\n  tables", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate a fitting of a statistical model to given data, calculating a\nconditional $p$ value by a Markov chain Monte Carlo method is one of the\neffective approaches. For this purpose, a Markov basis plays an important role\nbecause it guarantees the connectivity of the chain for unbiasedness of the\nestimation, and therefore is investigated in various settings such as\nincomplete tables or subtable sum constraints. In this paper, we consider the\ntwo-way change-point model for the ladder determinantal table, which is an\nextension of these two previous works. Our main result is based on the theory\nof Groebner basis for the distributive lattice. We give a numerical example for\nactual data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 04:35:18 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 03:56:39 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Aoki", "Satoshi", ""], ["Hibi", "Takayuki", ""]]}, {"id": "1608.08340", "submitter": "Sahil Agarwal", "authors": "Sahil Agarwal, John S. Wettlaufer", "title": "The statistical properties of sea ice velocity fields", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": "10.1175/JCLI-D-16-0653.1", "report-no": null, "categories": "physics.ao-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By arguing that the surface pressure field over the Arctic Ocean can be\ntreated as an isotropic, stationary, homogeneous, Gaussian random field,\nThorndike estimated a number of covariance functions from two years of data\n(1979 and 1980). Given the active interest in changes of general circulation\nquantities and indices in the polar regions during the recent few decades, the\nspatial correlations in sea ice velocity fields are of particular interest. It\nis thus natural to ask; \"how persistent are these correlations?\" To this end, a\nmulti-fractal stochastic treatment is developed to analyze observed Arctic sea\nice velocity fields from satellites and buoys for the period 1978 - 2015.\nHaving previously found that the Arctic Equivalent Ice Extent (EIE) has a white\nnoise structure on annual to bi-annual time scales, the connection between EIE\nand ice motion is assessed. The long-term stationarity of the spatial\ncorrelation structure of the velocity fields, and the robustness of their white\nnoise structure on multiple time scales is demonstrated, which (a) combine to\nexplain the white noise characteristics of the EIE on annual to biannual time\nscales, and (b) explain why the fluctuations in the ice velocity are\nproportional to fluctuations in the geostrophic winds on time scales of days to\nmonths. Moreover, it is shown that the statistical structure of these two\nquantities is commensurate from days up to years, which may be related to the\nincreasing prevalence of free drift in the ice pack.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 06:19:28 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 16:50:12 GMT"}, {"version": "v3", "created": "Tue, 20 Dec 2016 20:19:23 GMT"}, {"version": "v4", "created": "Thu, 9 Mar 2017 13:25:35 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Agarwal", "Sahil", ""], ["Wettlaufer", "John S.", ""]]}, {"id": "1608.08347", "submitter": "Minh-Ngoc Tran", "authors": "Dao Thanh Tung and Minh-Ngoc Tran and Tran Manh Cuong", "title": "Bayesian Adaptive Lasso with Variational Bayes for Variable Selection in\n  High-dimensional Generalized Linear Mixed Models", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a full Bayesian treatment for simultaneous\nfixed-effect selection and parameter estimation in high-dimensional generalized\nlinear mixed models. The approach consists of using a Bayesian adaptive Lasso\npenalty for signal-level adaptive shrinkage and a fast Variational Bayes scheme\nfor estimating the posterior mode of the coefficients. The proposed approach\noffers several advantages over the existing methods, for example, the adaptive\nshrinkage parameters are automatically incorporated, no Laplace approximation\nstep is required to integrate out the random effects. The performance of our\napproach is illustrated on several simulated and real data examples. The\nalgorithm is implemented in the R package glmmvb and is made available online.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 07:29:03 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Tung", "Dao Thanh", ""], ["Tran", "Minh-Ngoc", ""], ["Cuong", "Tran Manh", ""]]}, {"id": "1608.08468", "submitter": "Gregor Kastner", "authors": "Gregor Kastner", "title": "Sparse Bayesian time-varying covariance estimation in many dimensions", "comments": null, "journal-ref": "Journal of Econometrics 210(1), 98-115 (2019)", "doi": "10.1016/j.jeconom.2018.11.007", "report-no": null, "categories": "stat.ME econ.EM q-fin.PM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the curse of dimensionality in dynamic covariance estimation by\nmodeling the underlying co-volatility dynamics of a time series vector through\nlatent time-varying stochastic factors. The use of a global-local shrinkage\nprior for the elements of the factor loadings matrix pulls loadings on\nsuperfluous factors towards zero. To demonstrate the merits of the proposed\nframework, the model is applied to simulated data as well as to daily\nlog-returns of 300 S&P 500 members. Our approach yields precise correlation\nestimates, strong implied minimum variance portfolio performance and superior\nforecasting accuracy in terms of log predictive scores when compared to typical\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 14:16:54 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 13:43:38 GMT"}, {"version": "v3", "created": "Sat, 11 Nov 2017 11:28:19 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Kastner", "Gregor", ""]]}, {"id": "1608.08485", "submitter": "Michela Baccini", "authors": "Michela Baccini, Alessandra Mattei, Fabrizia Mealli, Pier Alberto\n  Bertazzi, Michele Carugno", "title": "Potential outcome approach to causal inference in assessing the short\n  term impact of air pollution on mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The opportunity to assess short term impact of air pollution relies on the\ncausal interpretation of the exposure-outcome association, but up to now few\nstudies explicitly faced this issue within a causal inference framework. In\nthis paper, we reformulated the problem of assessing the short term impact of\nair pollution on health using the potential outcome approach to causal\ninference. We focused on the impact of high daily levels of PM10 on mortality\nwithin two days from the exposure in the metropolitan area of Milan (Italy),\nduring the period 2003-2006. After defining the number of attributable deaths\nin terms of difference between potential outcomes, we used the estimated\npropensity score to match each high exposure-day with a day with similar\nbackground characteristics but lower PM10 level. Then, we estimated the impact\nby comparing mortality between matched days. We found that during the study\nperiod daily exposures larger than 40 microgram per cubic meter were\nresponsible of 1079 deaths (116; 2042). The impact was more evident among the\nelderly than in the younger classes of age. The propensity score matching\nturned out to be an appealing method to assess historical impacts in this\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 14:57:53 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Baccini", "Michela", ""], ["Mattei", "Alessandra", ""], ["Mealli", "Fabrizia", ""], ["Bertazzi", "Pier Alberto", ""], ["Carugno", "Michele", ""]]}, {"id": "1608.08825", "submitter": "Nikolai Dokuchaev", "authors": "Nikolai Dokuchaev, Lin-Yee Hin", "title": "On predictability of ultra short AR(1) sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses short term forecast of ultra short AR(1) sequences (4 to\n6 terms only) with a single structural break at an unknown time and of unknown\nsign and magnitude. As prediction of autoregressive processes requires\nestimated coefficients, the efficiency of which relies on the large sample\nproperties of the estimator, it is a common perception that prediction is\npractically impossible for such short series with structural break. However, we\nobtain a heuristic result that some universal predictors represented in the\nfrequency domain allow certain predictability based on these ultra short\nsequences. The predictors that we use are universal in a sense that they are\nnot oriented on particular types of autoregressions and do not require explicit\nmodelling of structural break. The shorter the sequence, the better the\none-step-ahead forecast performance of the smoothed predicting kernel. If the\nstructural break entails a model parameter switch from negative to positive\nvalue, the forecast performance of the smoothed predicting kernel is better\nthan that of the linear predictor that utilize AR(1) coefficient estimated from\nthe ultra short sequence without taking the structural break into account\nregardless whether the innovation terms in the learning sequences are\nconstructed from independent and identically distributed random Gaussian or\nGamma variables, scaled pseudo-uniform variables, or first-order\nauto-correlated Gaussian process.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 12:39:28 GMT"}], "update_date": "2016-09-04", "authors_parsed": [["Dokuchaev", "Nikolai", ""], ["Hin", "Lin-Yee", ""]]}, {"id": "1608.08873", "submitter": "Jonathan Rosenblatt", "authors": "Jonathan D. Rosenblatt, Yuval Benjamini, Roee Gilron, Roy Mukamel,\n  Jelle J. Goeman", "title": "Better-Than-Chance Classification for Signal Detection", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxz035", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimated accuracy of a classifier is a random quantity with variability.\nA common practice in supervised machine learning, is thus to test if the\nestimated accuracy is significantly better than chance level. This method of\nsignal detection is particularly popular in neuroimaging and genetics. We\nprovide evidence that using a classifier's accuracy as a test statistic can be\nan underpowered strategy for finding differences between populations, compared\nto a bona-fide statistical test. It is also computationally more demanding than\na statistical test. Via simulation, we compare test statistics that are based\non classification accuracy, to others based on multivariate test statistics. We\nfind that probability of detecting differences between two distributions is\nlower for accuracy based statistics. We examine several candidate causes for\nthe low power of accuracy tests. These causes include: the discrete nature of\nthe accuracy test statistic, the type of signal accuracy tests are designed to\ndetect, their inefficient use of the data, and their regularization. When the\npurposes of the analysis is not signal detection, but rather, the evaluation of\na particular classifier, we suggest several improvements to increase power. In\nparticular, to replace V-fold cross validation with the Leave-One-Out\nBootstrap.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 14:15:38 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 06:45:38 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Rosenblatt", "Jonathan D.", ""], ["Benjamini", "Yuval", ""], ["Gilron", "Roee", ""], ["Mukamel", "Roy", ""], ["Goeman", "Jelle J.", ""]]}, {"id": "1608.08941", "submitter": "Sigrunn Holbek Sorbye", "authors": "Sigrunn Holbek S{\\o}rbye and H{\\aa}vard Rue", "title": "Penalised complexity priors for stationary autoregressive processes", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autoregressive process of order $p$ (AR($p$)) is a central model in time\nseries analysis. A Bayesian approach requires the user to define a prior\ndistribution for the coefficients of the AR($p$) model. Although it is easy to\nwrite down some prior, it is not at all obvious how to understand and interpret\nthe prior, to ensure that it behaves according to the users prior knowledge. In\nthis paper, we approach this problem using the recently developed ideas of\npenalised complexity (PC) priors. These priors have important properties like\nrobustness and invariance to reparameterisations, as well as a clear\ninterpretation. A PC prior is computed based on specific principles, where\nmodel component complexity is penalised in terms of deviation from simple base\nmodel formulations. In the AR(1) case, we discuss two natural base model\nchoices, corresponding to either independence in time or no change in time. The\nlatter case is illustrated in a survival model with possible time-dependent\nfrailty. For higher-order processes, we propose a sequential approach, where\nthe base model for AR($p$) is the corresponding AR($p-1$) model expressed using\nthe partial autocorrelations. The properties of the new prior are compared with\nthe reference prior in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:01:10 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["S\u00f8rbye", "Sigrunn Holbek", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1608.08968", "submitter": "Amir Sepehri", "authors": "Amir Sepehri", "title": "The Bayesian SLOPE", "comments": "19 pages, 4 figures, R software available online", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SLOPE estimates regression coefficients by minimizing a regularized\nresidual sum of squares using a sorted-$\\ell_1$-norm penalty. The SLOPE\ncombines testing and estimation in regression problems. It exhibits suitable\nvariable selection and prediction properties, as well as minimax optimality.\nThis paper introduces the Bayesian SLOPE procedure for linear regression. The\nclassical SLOPE estimate is the posterior mode in the normal regression problem\nwith an appropriate prior on the coefficients. The Bayesian SLOPE considers the\nfull Bayesian model and has the advantage of offering credible sets and\nstandard error estimates for the parameters. Moreover, the hierarchical\nBayesian framework allows for full Bayesian and empirical Bayes treatment of\nthe penalty coefficients; whereas it is not clear how to choose these\ncoefficients when using the SLOPE on a general design matrix. A direct\ncharacterization of the posterior is provided which suggests a Gibbs sampler\nthat does not involve latent variables. An efficient hybrid Gibbs sampler for\nthe Bayesian SLOPE is introduced. Point estimation using the posterior mean is\nhighlighted, which automatically facilitates the Bayesian prediction of future\nobservations. These are demonstrated on real and synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:55:52 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 07:55:12 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Sepehri", "Amir", ""]]}]