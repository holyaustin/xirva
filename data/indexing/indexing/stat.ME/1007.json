[{"id": "1007.0238", "submitter": "Wagner Barreto-Souza", "authors": "Wagner Barreto-Souza and Hassan S. Bakouch", "title": "A new lifetime model with decreasing failure rate", "comments": null, "journal-ref": null, "doi": "10.1080/02331888.2011.595489", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new lifetime distribution by compounding\nexponential and Poisson-Lindley distributions, named exponential\nPoisson-Lindley distribution. Several properties are derived, such as density,\nfailure rate, mean lifetime, moments, order statistics and R\\'enyi entropy.\nFurthermore, estimation by maximum likelihood and inference for large sample\nare discussed. The paper is motivated by two applications to real data sets and\nwe hope that this model be able to attract wider applicability in survival and\nreliability.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2010 19:33:07 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Bakouch", "Hassan S.", ""]]}, {"id": "1007.0357", "submitter": "Dimitris Kugiumtzis", "authors": "Dimitris Kugiumtzis", "title": "Transfer Entropy on Rank Vectors", "comments": "9 pages, 7 figures, accepted in Journal of Nonlinear Systems and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.IT math.IT physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer entropy (TE) is a popular measure of information flow found to\nperform consistently well in different settings. Symbolic transfer entropy\n(STE) is defined similarly to TE but on the ranks of the components of the\nreconstructed vectors rather than the reconstructed vectors themselves. First,\nwe correct STE by forming the ranks for the future samples of the response\nsystem with regard to the current reconstructed vector. We give the grounds for\nthis modified version of STE, which we call Transfer Entropy on Rank Vectors\n(TERV). Then we propose to use more than one step ahead in the formation of the\nfuture of the response in order to capture the information flow from the\ndriving system over a longer time horizon. To assess the performance of STE, TE\nand TERV in detecting correctly the information flow we use receiver operating\ncharacteristic (ROC) curves formed by the measure values in the two coupling\ndirections computed on a number of realizations of known weakly coupled\nsystems. We also consider different settings of state space reconstruction,\ntime series length and observational noise. The results show that TERV indeed\nimproves STE and in some cases performs better than TE, particularly in the\npresence of noise, but overall TE gives more consistent results. The use of\nmultiple steps ahead improves the accuracy of TE and TERV.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2010 13:03:09 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Kugiumtzis", "Dimitris", ""]]}, {"id": "1007.0394", "submitter": "Dimitris Kugiumtzis", "authors": "Ioannis Vlachos and Dimitris Kugiumtzis", "title": "Non-uniform state space reconstruction and coupling detection", "comments": "21 pages, 11 figures, to be published in Physical Review E", "journal-ref": null, "doi": "10.1103/PhysRevE.82.016207", "report-no": null, "categories": "nlin.CD cs.IT math.IT physics.data-an q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the state space reconstruction from multiple time series\nderived from continuous and discrete systems and propose a method for building\nembedding vectors progressively using information measure criteria regarding\npast, current and future states. The embedding scheme can be adapted for\ndifferent purposes, such as mixed modelling, cross-prediction and Granger\ncausality. In particular we apply this method in order to detect and evaluate\ninformation transfer in coupled systems. As a practical application, we\ninvestigate in records of scalp epileptic EEG the information flow across brain\nareas.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2010 16:41:37 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Vlachos", "Ioannis", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1007.0571", "submitter": "Vikram Krishnamurthy", "authors": "Vikram Krishnamurthy", "title": "Quickest Detection with Social Learning: Interaction of local and global\n  decision makers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider how local and global decision policies interact in stopping time\nproblems such as quickest time change detection. Individual agents make myopic\nlocal decisions via social learning, that is, each agent records a private\nobservation of a noisy underlying state process, selfishly optimizes its local\nutility and then broadcasts its local decision. Given these local decisions,\nhow can a global decision maker achieve quickest time change detection when the\nunderlying state changes according to a phase-type distribution? The paper\npresents four results. First, using Blackwell dominance of measures, it is\nshown that the optimal cost incurred in social learning based quickest\ndetection is always larger than that of classical quickest detection. Second,\nit is shown that in general the optimal decision policy for social learning\nbased quickest detection is characterized by multiple thresholds within the\nspace of Bayesian distributions. Third, using lattice programming and\nstochastic dominance, sufficient conditions are given for the optimal decision\npolicy to consist of a single linear hyperplane, or, more generally, a\nthreshold curve. Estimation of the optimal linear approximation to this\nthreshold curve is formulated as a simulation-based stochastic optimization\nproblem. Finally, the paper shows that in multi-agent sensor management with\nquickest detection, where each agent views the world according to its prior,\nthe optimal policy has a similar structure to social learning.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2010 17:06:38 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2011 09:01:38 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2012 18:55:34 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Krishnamurthy", "Vikram", ""]]}, {"id": "1007.1193", "submitter": "Hamdi Raissi", "authors": "Valentin Patilea and Hamdi Ra\\\"issi", "title": "Adaptive estimation of vector autoregressive models with time-varying\n  variance: application to testing linear causality in mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Vector AutoRegressive (VAR) models where the innovations could be\nunconditionally heteroscedastic and serially dependent are considered. The\nvolatility structure is deterministic and quite general, including breaks or\ntrending variances as special cases. In this framework we propose Ordinary\nLeast Squares (OLS), Generalized Least Squares (GLS) and Adaptive Least Squares\n(ALS) procedures. The GLS estimator requires the knowledge of the time-varying\nvariance structure while in the ALS approach the unknown variance is estimated\nby kernel smoothing with the outer product of the OLS residuals vectors.\nDifferent bandwidths for the different cells of the time-varying variance\nmatrix are also allowed. We derive the asymptotic distribution of the proposed\nestimators for the VAR model coefficients and compare their properties. In\nparticular we show that the ALS estimator is asymptotically equivalent to the\ninfeasible GLS estimator. This asymptotic equivalence is obtained uniformly\nwith respect to the bandwidth(s) in a given range and hence justifies\ndata-driven bandwidth rules. Using these results we build Wald tests for the\nlinear Granger causality in mean which are adapted to VAR processes driven by\nerrors with a non stationary volatility. It is also shown that the commonly\nused standard Wald test for the linear Granger causality in mean is potentially\nunreliable in our framework. Monte Carlo experiments illustrate the use of the\ndifferent estimation approaches for the analysis of VAR models with stable\ninnovations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 17:31:14 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2010 13:17:48 GMT"}], "update_date": "2010-07-09", "authors_parsed": [["Patilea", "Valentin", ""], ["Ra\u00efssi", "Hamdi", ""]]}, {"id": "1007.1434", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro, Emmanuel J. Cand\\`es, Yaniv Plan", "title": "Global testing under sparse alternatives: ANOVA, multiple comparisons\n  and the higher criticism", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS910 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 5, 2533-2556", "doi": "10.1214/11-AOS910", "report-no": "IMS-AOS-AOS910", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for the significance of a subset of regression coefficients in a\nlinear model, a staple of statistical analysis, goes back at least to the work\nof Fisher who introduced the analysis of variance (ANOVA). We study this\nproblem under the assumption that the coefficient vector is sparse, a common\nsituation in modern high-dimensional settings. Suppose we have $p$ covariates\nand that under the alternative, the response only depends upon the order of\n$p^{1-\\alpha}$ of those, $0\\le\\alpha\\le1$. Under moderate sparsity levels, that\nis, $0\\le\\alpha\\le1/2$, we show that ANOVA is essentially optimal under some\nconditions on the design. This is no longer the case under strong sparsity\nconstraints, that is, $\\alpha>1/2$. In such settings, a multiple comparison\nprocedure is often preferred and we establish its optimality when\n$\\alpha\\geq3/4$. However, these two very popular methods are suboptimal, and\nsometimes powerless, under moderately strong sparsity where $1/2<\\alpha<3/4$.\nWe suggest a method based on the higher criticism that is powerful in the whole\nrange $\\alpha>1/2$. This optimality property is true for a variety of designs,\nincluding the classical (balanced) multi-way designs and more modern \"$p>n$\"\ndesigns arising in genetics and signal processing. In addition to the standard\nfixed effects model, we establish similar results for a random effects model\nwhere the nonzero coefficients of the regression vector are normally\ndistributed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2010 18:21:59 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 14:57:43 GMT"}], "update_date": "2012-02-24", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Cand\u00e8s", "Emmanuel J.", ""], ["Plan", "Yaniv", ""]]}, {"id": "1007.2876", "submitter": "Russell Lyons", "authors": "Russell Lyons", "title": "The Spread of Evidence-Poor Medicine via Flawed Social-Network Analysis", "comments": "16 pp, 2 figures", "journal-ref": "Statistics, Politics, and Policy: (2011) Vol. 2 : Iss. 1, Article\n  2: http://www.bepress.com/spp/vol2/iss1/2", "doi": "10.2202/2151-7509.1024", "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chronic widespread misuse of statistics is usually inadvertent, not\nintentional. We find cautionary examples in a series of recent papers by\nChristakis and Fowler that advance statistical arguments for the transmission\nvia social networks of various personal characteristics, including obesity,\nsmoking cessation, happiness, and loneliness. Those papers also assert that\nsuch influence extends to three degrees of separation in social networks. We\nshall show that these conclusions do not follow from Christakis and Fowler's\nstatistical analyses. In fact, their studies even provide some evidence against\nthe existence of such transmission. The errors that we expose arose, in part,\nbecause the assumptions behind the statistical procedures used were\ninsufficiently examined, not only by the authors, but also by the reviewers.\nOur examples are instructive because the practitioners are highly reputed,\ntheir results have received enormous popular attention, and the journals that\npublished their studies are among the most respected in the world. An\neducational bonus emerges from the difficulty we report in getting our critique\npublished. We discuss the relevance of this episode to understanding\nstatistical literacy and the role of scientific review, as well as to reforming\nstatistics education.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2010 21:39:07 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 22:04:44 GMT"}, {"version": "v3", "created": "Thu, 5 May 2011 20:40:46 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Lyons", "Russell", ""]]}, {"id": "1007.3196", "submitter": "Rossi Hassad", "authors": "Rossi A. Hassad", "title": "Toward improving the quality of doctoral education: A focus on\n  statistics, research methods, and dissertation supervision", "comments": "4 Pages, Refereed Conference Publication", "journal-ref": "In C. Reading (Ed.), Data and context in statistics education:\n  Towards an evidence-based society. Proceedings of the Eighth International\n  Conference on Teaching Statistics (2010), Ljubljana, Slovenia. International\n  Statistical Institute", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doctoral education (PhD) in the USA has long been characterized as being in a\ncrisis, yet empirical research to identify possible determinants is limited, in\nparticular, faculty competence has received only scant research attention. This\nstudy ascertained from students, faculty and consultants, their concerns about\nthe teaching of statistics and research (including dissertation supervision).\nThe responses encompass the curriculum, pedagogy, content knowledge, support,\nand accountability. The current U.S. doctoral education model needs to be\nsystematically reviewed toward assessing its relevance to the changing needs of\nthe disciplines and the job market. In this regard, the almost universal\nemphasis on evidence-based practice, especially in the disciplines of health\nand behavioral sciences must be given major consideration. Reform initiatives\nmust also address the roles and qualifications of dissertation committee\nmembers (including consultants), the composition of the dissertation committee,\nand training geared toward preparing and certifying faculty to serve as\ndissertation committee members.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2010 16:13:07 GMT"}], "update_date": "2010-07-20", "authors_parsed": [["Hassad", "Rossi A.", ""]]}, {"id": "1007.3205", "submitter": "Rossi Hassad", "authors": "Rossi A. Hassad", "title": "Reflection on Training, Experience, and Introductory Statistics: A\n  Mini-Survey of Tertiary Level Statistics Instructors", "comments": "4 Pages, Refereed Conference Publication", "journal-ref": "The Proceedings of ICOTS-7 (International Conference on the\n  Teaching of Statistics). IASE (International Association for Statistical\n  Education), 2006", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instructors of statistics who teach non-statistics majors possess varied\nacademic backgrounds, and hence it is reasonable to expect variability in their\ncontent knowledge, and pedagogical approach. The aim of this study was to\ndetermine the specific course(s) that contributed mostly to instructors'\nunderstanding of statistics. Courses reported were described as advanced or\ngraduate level, and classified as application-based, math, multivariate,\nprobability, and research. The majority, 9 (56%) attributed their understanding\nof statistics to either an application-based or research course, and of those,\n7 (44%) reported negative feelings about their introductory courses. These\nfindings underscore the importance of authentic activities, and constructivist\npedagogy toward facilitating statistical literacy. Research is needed to\ndetermine the effect of instructors' academic preparation on their knowledge,\nattitudes, and practices.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2010 16:42:44 GMT"}], "update_date": "2010-07-20", "authors_parsed": [["Hassad", "Rossi A.", ""]]}, {"id": "1007.3230", "submitter": "Sean Simpson", "authors": "Sean L. Simpson, Satoru Hayasaka, and Paul J. Laurienti", "title": "Exponential Random Graph Modeling for Complex Brain Networks", "comments": null, "journal-ref": "PLoS ONE 2011: 6(5), e20039 (doi:10.1371/journal.pone.0020039)", "doi": "10.1371/journal.pone.0020039", "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential random graph models (ERGMs), also known as p* models, have been\nutilized extensively in the social science literature to study complex networks\nand how their global structure depends on underlying structural components.\nHowever, the literature on their use in biological networks (especially brain\nnetworks) has remained sparse. Descriptive models based on a specific feature\nof the graph (clustering coefficient, degree distribution, etc.) have dominated\nconnectivity research in neuroscience. Corresponding generative models have\nbeen developed to reproduce one of these features. However, the complexity\ninherent in whole-brain network data necessitates the development and use of\ntools that allow the systematic exploration of several features simultaneously\nand how they interact to form the global network architecture. ERGMs provide a\nstatistically principled approach to the assessment of how a set of interacting\nlocal brain network features gives rise to the global structure. We illustrate\nthe utility of ERGMs for modeling, analyzing, and simulating complex\nwhole-brain networks with network data from normal subjects. We also provide a\nfoundation for the selection of important local features through the\nimplementation and assessment of three selection approaches: a traditional\np-value based backward selection approach, an information criterion approach\n(AIC), and a graphical goodness of fit (GOF) approach. The graphical GOF\napproach serves as the best method given the scientific interest in being able\nto capture and reproduce the structure of fitted brain networks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2010 18:16:26 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2011 20:31:29 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Simpson", "Sean L.", ""], ["Hayasaka", "Satoru", ""], ["Laurienti", "Paul J.", ""]]}, {"id": "1007.3376", "submitter": "Holger Dette", "authors": "Stanislav Volgushev, Holger Dette", "title": "Nonparametric quantile regression for twice censored data", "comments": "46 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of nonparametric quantile regression for twice\ncensored data. Two new estimates are presented, which are constructed by\napplying concepts of monotone rearrangements to estimates of the conditional\ndistribution function. The proposed methods avoid the problem of crossing\nquantile curves. Weak uniform consistency and weak convergence is established\nfor both estimates and their finite sample properties are investigated by means\nof a simulation study. As a by-product, we obtain a new result regarding the\nweak convergence of the Beran estimator for right censored data on the maximal\npossible domain, which is of its own interest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2010 09:39:03 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2012 06:06:42 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Volgushev", "Stanislav", ""], ["Dette", "Holger", ""]]}, {"id": "1007.3823", "submitter": "Judith Rousseau", "authors": "Judith Rousseau, Nicolas Chopin, Brunero Liseo", "title": "Bayesian nonparametric estimation of the spectral density of a long or\n  intermediate memory Gaussian process", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS955 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 2, 964-995", "doi": "10.1214/11-AOS955", "report-no": "IMS-AOS-AOS955", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stationary Gaussian process is said to be long-range dependent (resp.,\nanti-persistent) if its spectral density $f(\\lambda)$ can be written as\n$f(\\lambda)=|\\lambda|^{-2d}g(|\\lambda|)$, where $0<d<1/2$ (resp., $-1/2<d<0$),\nand $g$ is continuous and positive. We propose a novel Bayesian nonparametric\napproach for the estimation of the spectral density of such processes. We prove\nposterior consistency for both $d$ and $g$, under appropriate conditions on the\nprior distribution. We establish the rate of convergence for a general class of\npriors and apply our results to the family of fractionally exponential priors.\nOur approach is based on the true likelihood and does not resort to Whittle's\napproximation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2010 08:25:07 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2012 08:58:29 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Rousseau", "Judith", ""], ["Chopin", "Nicolas", ""], ["Liseo", "Brunero", ""]]}, {"id": "1007.4013", "submitter": "Roger Koenker", "authors": "Roger Koenker, Ivan Mizera", "title": "Quasi-concave density estimation", "comments": "Published in at http://dx.doi.org/10.1214/10-AOS814 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2010, Vol. 38, No. 5, 2998-3027", "doi": "10.1214/10-AOS814", "report-no": "IMS-AOS-AOS814", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation of a log-concave probability density is\nformulated as a convex optimization problem and shown to have an equivalent\ndual formulation as a constrained maximum Shannon entropy problem. Closely\nrelated maximum Renyi entropy estimators that impose weaker concavity\nrestrictions on the fitted density are also considered, notably a minimum\nHellinger discrepancy estimator that constrains the reciprocal of the\nsquare-root of the density to be concave. A limiting form of these estimators\nconstrains solutions to the class of quasi-concave densities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2010 21:28:38 GMT"}, {"version": "v2", "created": "Mon, 15 Nov 2010 07:04:10 GMT"}], "update_date": "2010-11-16", "authors_parsed": [["Koenker", "Roger", ""], ["Mizera", "Ivan", ""]]}, {"id": "1007.4148", "submitter": "Andrey Shabalin", "authors": "Andrey Shabalin and Andrew Nobel", "title": "Reconstruction of a Low-rank Matrix in the Presence of Gaussian Noise", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of reconstruction of a low-rank matrix\nobserved with additive Gaussian noise. First we show that under mild\nassumptions (about the prior distribution of the signal matrix) we can restrict\nour attention to reconstruction methods that are based on the singular value\ndecomposition of the observed matrix and act only on its singular values\n(preserving the singular vectors). Then we determine the effect of noise on the\nSVD of low-rank matrices by building a connection between matrix reconstruction\nproblem and spiked population model in random matrix theory. Based on this\nknowledge, we propose a new reconstruction method, called RMT, that is designed\nto reverse the effect of the noise on the singular values of the signal matrix\nand adjust for its effect on the singular vectors. With an extensive simulation\nstudy we show that the proposed method outperform even oracle versions of both\nsoft and hard thresholding methods and closely matches the performance of a\ngeneral oracle scheme.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2010 15:13:23 GMT"}], "update_date": "2010-07-26", "authors_parsed": [["Shabalin", "Andrey", ""], ["Nobel", "Andrew", ""]]}, {"id": "1007.4169", "submitter": "Donald Percival", "authors": "Michael J. Keim and Donald B. Percival", "title": "Assessing Characteristic Scales Using Wavelets", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characteristic scale is a notion that pervades the geophysical sciences, but\nit has no widely accepted precise definition. The wavelet transform decomposes\na time series into coefficients that are associated with different scales. The\nvariance of these coefficients can be used to decompose the variance of the\ntime series across different scales. A practical definition for characteristic\nscale can be formulated in terms of peaks in plots of the wavelet variance\nversus scale. This paper presents basic theory for characteristic scales based\nupon the discrete wavelet transform, proposes a natural estimator for these\nscales and provides a large sample theory for this estimator that permits the\nconstruction of confidence intervals for a true unknown characteristic scale.\nComputer experiments are presented that demonstrate the efficacy of the large\nsample theory for finite sample sizes. Examples of characteristic scale\nestimation are given for global temperature records, coherent structures in\nriver flows, the Madden-Julian oscillation in an atmospheric time series and\ntransects of one type of Arctic sea ice.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2010 16:23:35 GMT"}], "update_date": "2010-07-26", "authors_parsed": [["Keim", "Michael J.", ""], ["Percival", "Donald B.", ""]]}, {"id": "1007.4259", "submitter": "Wicher Bergsma", "authors": "Wicher Bergsma, Angelos Dassios", "title": "A consistent test of independence based on a sign covariance related to\n  Kendall's tau", "comments": "Published in at http://dx.doi.org/10.3150/13-BEJ514 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2014, Vol. 20, No. 2, 1006-1028", "doi": "10.3150/13-BEJ514", "report-no": "IMS-BEJ-BEJ514", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular ways to test for independence of two ordinal random\nvariables are by means of Kendall's tau and Spearman's rho. However, such tests\nare not consistent, only having power for alternatives with ``monotonic''\nassociation. In this paper, we introduce a natural extension of Kendall's tau,\ncalled $\\tau^*$, which is non-negative and zero if and only if independence\nholds, thus leading to a consistent independence test. Furthermore,\nnormalization gives a rank correlation which can be used as a measure of\ndependence, taking values between zero and one. A comparison with alternative\nmeasures of dependence for ordinal random variables is given, and it is shown\nthat, in a well-defined sense, $\\tau ^*$ is the simplest, similarly to\nKendall's tau being the simplest of ordinal measures of monotone association.\nSimulation studies show our test compares well with the alternatives in terms\nof average $p$-values.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2010 09:07:50 GMT"}, {"version": "v2", "created": "Fri, 12 Nov 2010 18:52:24 GMT"}, {"version": "v3", "created": "Mon, 24 Jan 2011 18:06:52 GMT"}, {"version": "v4", "created": "Mon, 17 Dec 2012 20:53:40 GMT"}, {"version": "v5", "created": "Mon, 31 Dec 2012 13:11:01 GMT"}, {"version": "v6", "created": "Fri, 14 Mar 2014 07:04:12 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Bergsma", "Wicher", ""], ["Dassios", "Angelos", ""]]}, {"id": "1007.4532", "submitter": "Christopher Yau", "authors": "Christopher Yau, Christopher C. Holmes", "title": "A decision-theoretic approach for segmental classification", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS657 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1814-1835", "doi": "10.1214/13-AOAS657", "report-no": "IMS-AOAS-AOAS657", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with statistical methods for the segmental\nclassification of linear sequence data where the task is to segment and\nclassify the data according to an underlying hidden discrete state sequence.\nSuch analysis is commonplace in the empirical sciences including genomics,\nfinance and speech processing. In particular, we are interested in answering\nthe following question: given data $y$ and a statistical model $\\pi(x,y)$ of\nthe hidden states $x$, what should we report as the prediction $\\hat{x}$ under\nthe posterior distribution $\\pi (x|y)$? That is, how should you make a\nprediction of the underlying states? We demonstrate that traditional approaches\nsuch as reporting the most probable state sequence or most probable set of\nmarginal predictions can give undesirable classification artefacts and offer\nlimited control over the properties of the prediction. We propose a decision\ntheoretic approach using a novel class of Markov loss functions and report\n$\\hat{x}$ via the principle of minimum expected loss (maximum expected\nutility). We demonstrate that the sequence of minimum expected loss under the\nMarkov loss function can be enumerated exactly using dynamic programming\nmethods and that it offers flexibility and performance improvements over\nexisting techniques. The result is generic and applicable to any probabilistic\nmodel on a sequence, such as Hidden Markov models, change point or product\npartition models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2010 19:03:11 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2013 08:43:45 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Yau", "Christopher", ""], ["Holmes", "Christopher C.", ""]]}, {"id": "1007.4740", "submitter": "Nicolas Bousquet", "authors": "Nicolas Bousquet", "title": "Elicitation of Weibull priors", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on expert opinions, informative prior elicitation for the common\nWeibull lifetime distribution usually presents some difficulties since it\nrequires to elicit a two-dimensional joint prior. We consider here a\nreliability framework where the available expert information states directly in\nterms of prior predictive values (lifetimes) and not parameter values, which\nare less intuitive. The novelty of our procedure is to weigh the expert\ninformation by the size m of a virtual sample yielding a similar information,\nthe prior being seen as a reference posterior. Thus, the prior calibration by\nthe Bayesian analyst, who has to moderate the subjective information with\nrespect to the data information, is made simple. A main result is the full\ntractability of the prior under mild conditions, despite the conjugation issues\nencountered with the Weibull distribution. Besides, m is a practical focus\npoint for discussion between analysts and experts, and a helpful parameter for\nleading sensitivity studies and reducing the potential imbalance in posterior\nselection between Bayesian Weibull models, which can be due to favoring\narbitrarily a prior. The calibration of m is discussed and a real example is\ntreated along the paper.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2010 14:46:16 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2010 08:52:42 GMT"}, {"version": "v3", "created": "Thu, 21 Oct 2010 16:59:47 GMT"}], "update_date": "2010-10-22", "authors_parsed": [["Bousquet", "Nicolas", ""]]}, {"id": "1007.5516", "submitter": "Korbinian Strimmer", "authors": "Verena Zuber and Korbinian Strimmer", "title": "High-dimensional regression and variable selection using CAR scores", "comments": "25 pages, 3 figures, 9 tables", "journal-ref": "Statistical Applications in Genetics and Molecular Biology 2011,\n  Vol. 10, Iss. 1, Article 34", "doi": "10.2202/1544-6115.1730", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is a difficult problem that is particularly challenging in\nthe analysis of high-dimensional genomic data. Here, we introduce the CAR\nscore, a novel and highly effective criterion for variable ranking in linear\nregression based on Mahalanobis-decorrelation of the explanatory variables. The\nCAR score provides a canonical ordering that encourages grouping of correlated\npredictors and down-weights antagonistic variables. It decomposes the\nproportion of variance explained and it is an intermediate between marginal\ncorrelation and the standardized regression coefficient. As a population\nquantity, any preferred inference scheme can be applied for its estimation.\nUsing simulations we demonstrate that variable selection by CAR scores is very\neffective and yields prediction errors and true and false positive rates that\ncompare favorably with modern regression techniques such as elastic net and\nboosting. We illustrate our approach by analyzing data concerned with diabetes\nprogression and with the effect of aging on gene expression in the human brain.\nThe R package \"care\" implementing CAR score regression is available from CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jul 2010 18:47:09 GMT"}, {"version": "v2", "created": "Mon, 9 Aug 2010 23:07:15 GMT"}, {"version": "v3", "created": "Mon, 27 Sep 2010 16:08:30 GMT"}, {"version": "v4", "created": "Fri, 15 Apr 2011 11:59:15 GMT"}, {"version": "v5", "created": "Thu, 7 Jul 2011 09:24:48 GMT"}, {"version": "v6", "created": "Tue, 19 Jul 2011 02:35:53 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Zuber", "Verena", ""], ["Strimmer", "Korbinian", ""]]}]