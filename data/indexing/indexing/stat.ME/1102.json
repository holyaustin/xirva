[{"id": "1102.0059", "submitter": "Donghui Yan", "authors": "Donghui Yan, Pei Wang, Michael Linden, Beatrice Knudsen, Timothy\n  Randolph", "title": "Statistical methods for tissue array images - algorithmic scoring and\n  co-training", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS543 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1280-1305", "doi": "10.1214/12-AOAS543", "report-no": "IMS-AOAS-AOAS543", "categories": "stat.ME cs.CE cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in tissue microarray technology have allowed\nimmunohistochemistry to become a powerful medium-to-high throughput analysis\ntool, particularly for the validation of diagnostic and prognostic biomarkers.\nHowever, as study size grows, the manual evaluation of these assays becomes a\nprohibitive limitation; it vastly reduces throughput and greatly increases\nvariability and expense. We propose an algorithm - Tissue Array Co-Occurrence\nMatrix Analysis (TACOMA) - for quantifying cellular phenotypes based on\ntextural regularity summarized by local inter-pixel relationships. The\nalgorithm can be easily trained for any staining pattern, is absent of\nsensitive tuning parameters and has the ability to report salient pixels in an\nimage that contribute to its score. Pathologists' input via informative\ntraining patches is an important aspect of the algorithm that allows the\ntraining for any specific marker or cell type. With co-training, the error rate\nof TACOMA can be reduced substantially for a very small training sample (e.g.,\nwith size 30). We give theoretical insights into the success of co-training via\nthinning of the feature set in a high-dimensional setting when there is\n\"sufficient\" redundancy among the features. TACOMA is flexible, transparent and\nprovides a scoring process that can be evaluated with clarity and confidence.\nIn a study based on an estrogen receptor (ER) marker, we show that TACOMA is\ncomparable to, or outperforms, pathologists' performance in terms of accuracy\nand repeatability.\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 02:08:00 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 09:20:39 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Yan", "Donghui", ""], ["Wang", "Pei", ""], ["Linden", "Michael", ""], ["Knudsen", "Beatrice", ""], ["Randolph", "Timothy", ""]]}, {"id": "1102.0299", "submitter": "Lianfen Qian", "authors": "Lianfen Qian", "title": "Fisher information matrix for three-parameter exponentiated-Weibull\n  distribution under type II censoring", "comments": "13 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the three-parameter exponentiated Weibull family under\ntype II censoring. It first graphically illustrates the shape property of the\nhazard function. Then, it proposes a simple algorithm for computing the maximum\nlikelihood estimator and derives the Fisher information matrix. The latter one\nis represented through a single integral in terms of hazard function, hence it\nsolves the problem of computation difficulty in constructing inference for the\nmaximum likelihood estimator. Real data analysis is conducted to illustrate the\neffect of censoring rate on the maximum likelihood estimation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 21:26:07 GMT"}], "update_date": "2011-02-03", "authors_parsed": [["Qian", "Lianfen", ""]]}, {"id": "1102.0369", "submitter": "Georgios Fellouris", "authors": "Georgios Fellouris", "title": "Asymptotically optimal parameter estimation under communication\n  constraints", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1035 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 2239-2265", "doi": "10.1214/12-AOS1035", "report-no": "IMS-AOS-AOS1035", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parameter estimation problem is considered, in which dispersed sensors\ntransmit to the statistician partial information regarding their observations.\nThe sensors observe the paths of continuous semimartingales, whose drifts are\nlinear with respect to a common parameter. A novel estimating scheme is\nsuggested, according to which each sensor transmits only one-bit messages at\nstopping times of its local filtration. The proposed estimator is shown to be\nconsistent and, for a large class of processes, asymptotically optimal, in the\nsense that its asymptotic distribution is the same as the exact distribution of\nthe optimal estimator that has full access to the sensor observations. These\nproperties are established under an asymptotically low rate of communication\nbetween the sensors and the statistician. Thus, despite being asymptotically\nefficient, the proposed estimator requires minimal transmission activity, which\nis a desirable property in many applications. Finally, the case of discrete\nsampling at the sensors is studied when their underlying processes are\nindependent Brownian motions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Feb 2011 06:55:47 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2011 15:53:22 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2012 07:17:34 GMT"}, {"version": "v4", "created": "Wed, 18 Jul 2012 16:35:16 GMT"}, {"version": "v5", "created": "Thu, 31 Jan 2013 06:30:45 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Fellouris", "Georgios", ""]]}, {"id": "1102.0470", "submitter": "Meili Baragatti", "authors": "Meili Baragatti (IML), Denys Pommeret (IML)", "title": "A study of variable selection using g-prior distribution with ridge\n  parameter", "comments": null, "journal-ref": "Computational Statistics and Data Analysis 56, 6 (2012) 1920-1934", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bayesian stochastic search variable selection framework, a common\nprior distribution for the regression coefficients is the g-prior of Zellner\n(1986). However, there are two standard cases in which the associated\ncovariance matrix does not exist, and the conventional prior of Zellner can not\nbe used: if the number of observations is lower than the number of variables\n(large p and small n paradigm), or if some variables are linear combinations of\nothers. In such situations a prior distribution derived from the prior of\nZellner can be used, by introducing a ridge parameter. This prior introduced by\nGupta and Ibrahim (2007) is a flexible and simple adaptation of the g-prior. In\nthis paper we study the influence of the ridge parameter on the selection of\nvariables. A simple way to choose the associated hyper-parameters is proposed.\nThe method is valid for any generalized linear mixed model and we focus on the\ncase of probit mixed models when some variables are linear combinations of\nothers. The method is applied to both simulated and real datasets obtained from\nAffymetrix microarray experiments. Results are compared to those obtained with\nthe Bayesian Lasso.\n", "versions": [{"version": "v1", "created": "Wed, 2 Feb 2011 16:11:24 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2011 20:28:50 GMT"}, {"version": "v3", "created": "Mon, 30 May 2011 12:31:07 GMT"}, {"version": "v4", "created": "Mon, 14 Nov 2011 07:43:03 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Baragatti", "Meili", "", "IML"], ["Pommeret", "Denys", "", "IML"]]}, {"id": "1102.0598", "submitter": "Georgios Fellouris Dr.", "authors": "Alexandra Chronopoulou and Georgios Fellouris", "title": "Optimal sequential change-detection for fractional diffusion-type\n  processes", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting an abrupt change in the distribution of\na sequentially observed stochastic process. We establish the optimality of the\nCUSUM test with respect to a modified version of Lorden's criterion for\narbitrary processes with continuous paths and apply this general result to the\nspecial case of fractional diffusion-type processes. As a by-product, we show\nthat the CUSUM test optimizes Lorden's original criterion when a fractional\nBrownian motion with Hurst index H adopts a polynomial drift term with exponent\nH + 1/2 after the change.\n", "versions": [{"version": "v1", "created": "Thu, 3 Feb 2011 03:58:25 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2012 18:40:21 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Chronopoulou", "Alexandra", ""], ["Fellouris", "Georgios", ""]]}, {"id": "1102.0927", "submitter": "Fabio Rapallo", "authors": "Fabio Rapallo", "title": "Outliers and patterns of outliers in contingency tables with Algebraic\n  Statistics", "comments": "24 pages, several examples and comments added in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a definition of pattern of outliers in contingency\ntables within a model-based framework. In particular, we make use of log-linear\nmodels and exact goodness-of-fit tests to specify the notions of outlier and\npattern of outliers. The language and some techniques from Algebraic Statistics\nare essential tools to make the definition clear and easily applicable. Some\nnumerical examples show how to use our definitions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 14:38:50 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2011 07:40:41 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Rapallo", "Fabio", ""]]}, {"id": "1102.1191", "submitter": "Richard Samworth", "authors": "Yining Chen and Richard J. Samworth", "title": "Smoothed log-concave maximum likelihood estimation with applications", "comments": "29 pages, 3 figures", "journal-ref": "Statist. Sinica. 23 (2013), 1373-1398", "doi": "10.5705/ss.2011.224", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the smoothed log-concave maximum likelihood estimator of a\nprobability distribution on $\\mathbb{R}^d$. This is a fully automatic\nnonparametric density estimator, obtained as a canonical smoothing of the\nlog-concave maximum likelihood estimator. We demonstrate its attractive\nfeatures both through an analysis of its theoretical properties and a\nsimulation study. Moreover, we use our methodology to develop a new test of\nlog-concavity, and show how the estimator can be used as an intermediate stage\nof more involved procedures, such as constructing a classifier or estimating a\nfunctional of the density. Here again, the use of these procedures can be\njustified both on theoretical grounds and through its finite sample\nperformance, and we illustrate its use in a breast cancer diagnosis\n(classification) problem.\n", "versions": [{"version": "v1", "created": "Sun, 6 Feb 2011 19:48:40 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2011 11:04:35 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2011 16:12:58 GMT"}, {"version": "v4", "created": "Sun, 10 Jun 2012 16:56:10 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Chen", "Yining", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1102.1796", "submitter": "Alexandre Lung-Yut-Fong", "authors": "Alexandre Lung-Yut-Fong (LTCI), C\\'eline L\\'evy-Leduc (LTCI), Olivier\n  Capp\\'e (LTCI)", "title": "Robust Retrospective Multiple Change-point Estimation for Multivariate\n  Data", "comments": "submitted to IEEE Workshop on Statistical Signal Processing 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric statistical procedure for detecting multiple\nchange-points in multidimensional signals. The method is based on a test\nstatistic that generalizes the well-known Kruskal-Wallis procedure to the\nmultivariate setting. The proposed approach does not require any knowledge\nabout the distribution of the observations and is parameter-free. It is\ncomputationally efficient thanks to the use of dynamic programming and can also\nbe applied when the number of change-points is unknown. The method is shown\nthrough simulations to be more robust than alternatives, particularly when\nfaced with atypical distributions (e.g., with outliers), high noise levels\nand/or high-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Feb 2011 07:38:55 GMT"}, {"version": "v2", "created": "Thu, 10 Feb 2011 20:07:46 GMT"}], "update_date": "2011-02-11", "authors_parsed": [["Lung-Yut-Fong", "Alexandre", "", "LTCI"], ["L\u00e9vy-Leduc", "C\u00e9line", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"]]}, {"id": "1102.2233", "submitter": "Weidong Liu", "authors": "Tony Cai, Weidong Liu and Xi Luo", "title": "A Constrained L1 Minimization Approach to Sparse Precision Matrix\n  Estimation", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A constrained L1 minimization method is proposed for estimating a sparse\ninverse covariance matrix based on a sample of $n$ iid $p$-variate random\nvariables. The resulting estimator is shown to enjoy a number of desirable\nproperties. In particular, it is shown that the rate of convergence between the\nestimator and the true $s$-sparse precision matrix under the spectral norm is\n$s\\sqrt{\\log p/n}$ when the population distribution has either exponential-type\ntails or polynomial-type tails. Convergence rates under the elementwise\n$L_{\\infty}$ norm and Frobenius norm are also presented. In addition, graphical\nmodel selection is considered. The procedure is easily implementable by linear\nprogramming. Numerical performance of the estimator is investigated using both\nsimulated and real data. In particular, the procedure is applied to analyze a\nbreast cancer dataset. The procedure performs favorably in comparison to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 21:01:18 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Cai", "Tony", ""], ["Liu", "Weidong", ""], ["Luo", "Xi", ""]]}, {"id": "1102.2237", "submitter": "Weidong Liu", "authors": "Tony Cai and Weidong Liu", "title": "Adaptive Thresholding for Sparse Covariance Matrix Estimation", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider estimation of sparse covariance matrices and\npropose a thresholding procedure which is adaptive to the variability of\nindividual entries. The estimators are fully data driven and enjoy excellent\nperformance both theoretically and numerically. It is shown that the estimators\nadaptively achieve the optimal rate of convergence over a large class of sparse\ncovariance matrices under the spectral norm. In contrast, the commonly used\nuniversal thresholding estimators are shown to be sub-optimal over the same\nparameter spaces. Support recovery is also discussed. The adaptive thresholding\nestimators are easy to implement. Numerical performance of the estimators is\nstudied using both simulated and real data. Simulation results show that the\nadaptive thresholding estimators uniformly outperform the universal\nthresholding estimators. The method is also illustrated in an analysis on a\ndataset from a small round blue-cell tumors microarray experiment. A supplement\nto this paper which contains additional technical proofs is available online.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 21:11:01 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Cai", "Tony", ""], ["Liu", "Weidong", ""]]}, {"id": "1102.2322", "submitter": "Matthew Sperrin", "authors": "Matthew Sperrin and Iain Buchan", "title": "Modelling time to event with observations made at arbitrary times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new methods of analysing time to event data via extended\nversions of the proportional hazards and accelerated failure time (AFT) models.\nIn many time to event studies, the time of first observation is arbitrary, in\nthe sense that no risk modifying event occurs. This is particularly common in\nepidemiological studies. We show formally that, in these situations, it is not\nsensible to take the first observation as the time origin, either in AFT or\nproportional hazards type models. Instead, we advocate using age of the subject\nas the time scale. We account for the fact that baseline observations may be\nmade at different ages in different patients via a two stage procedure. First,\nwe marginally regress any potentially age-varying covariates against age,\nretaining the residuals. These residuals are then used as covariates in the\nfitting of either an AFT model or a proportional hazards model. We call the\nprocedures residual accelerated failure time (RAFT) regression and residual\nproportional hazards (RPH) regression respectively. We compare standard AFT\nwith RAFT, and demonstrate superior predictive ability of RAFT in real\nexamples. In epidemiology, this has real implications in terms of risk\ncommunication to both patients and policy makers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Feb 2011 10:51:10 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Sperrin", "Matthew", ""], ["Buchan", "Iain", ""]]}, {"id": "1102.2773", "submitter": "Nancy L. Garcia", "authors": "Ronaldo Dias, Nancy L. Garcia and Alexandra M. Schmidt", "title": "A Hierarchical Model for Aggregated Functional Data", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas of science one aims to estimate latent sub-population mean\ncurves based only on observations of aggregated population curves. By\naggregated curves we mean linear combination of functional data that cannot be\nobserved individually. We assume that several aggregated curves with linear\nindependent coefficients are available. More specifically, we assume each\naggregated curve is an independent partial realization of a Gaussian process\nwith mean modeled through a weighted linear combination of the disaggregated\ncurves. We model the mean of the Gaussian processes as a smooth function\napproximated by a function belonging to a finite dimensional space ${\\cal H}_K$\nwhich is spanned by $K$ B-splines basis functions. We explore two different\nspecifications of the covariance function of the Gaussian process: one that\nassumes a constant variance across the domain of the process, and a more\ngeneral variance structure which is itself modelled as a smooth function,\nproviding a nonstationary covariance function. Inference procedure is performed\nfollowing the Bayesian paradigm allowing experts' opinion to be considered when\nestimating the disaggregated curves. Moreover, it naturally provides the\nuncertainty associated with the parameters estimates and fitted values. Our\nmodel is suitable for a wide range of applications. We concentrate on two\ndifferent real examples: calibration problem for NIR spectroscopy data and an\nanalysis of distribution of energy among different type of consumers.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 13:56:42 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Dias", "Ronaldo", ""], ["Garcia", "Nancy L.", ""], ["Schmidt", "Alexandra M.", ""]]}, {"id": "1102.2774", "submitter": "Dan L. Nicolae", "authors": "Dan L. Nicolae, Xiao-Li Meng, Augustine Kong", "title": "Quantifying the Fraction of Missing Information for Hypothesis Testing\n  in Statistical and Genetic Studies", "comments": "Published in at http://dx.doi.org/10.1214/07-STS244 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 287-312", "doi": "10.1214/07-STS244", "report-no": "IMS-STS-STS244", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical studies rely on hypothesis testing procedures applied to data\nsets with missing information. An important part of the analysis is to\ndetermine the impact of the missing data on the performance of the test, and\nthis can be done by properly quantifying the relative (to complete data) amount\nof available information. The problem is directly motivated by applications to\nstudies, such as linkage analyses and haplotype-based association projects,\ndesigned to identify genetic contributions to complex diseases. In the genetic\nstudies the relative information measures are needed for the experimental\ndesign, technology comparison, interpretation of the data, and for\nunderstanding the behavior of some of the inference tools. The central\ndifficulties in constructing such information measures arise from the multiple,\nand sometimes conflicting, aims in practice. For large samples, we show that a\nsatisfactory, likelihood-based general solution exists by using appropriate\nforms of the relative Kullback--Leibler information, and that the proposed\nmeasures are computationally inexpensive given the maximized likelihoods with\nthe observed data. Two measures are introduced, under the null and alternative\nhypothesis respectively. We exemplify the measures on data coming from mapping\nstudies on the inflammatory bowel disease and diabetes. For small-sample\nproblems, which appear rather frequently in practice and sometimes in disguised\nforms (e.g., measuring individual contributions to a large study), the robust\nBayesian approach holds great promise, though the choice of a general-purpose\n\"default prior\" is a very challenging problem.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 13:57:10 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Nicolae", "Dan L.", ""], ["Meng", "Xiao-Li", ""], ["Kong", "Augustine", ""]]}, {"id": "1102.2981", "submitter": "Guido Consonni", "authors": "Guido Consonni, Piero Veronese", "title": "Compatibility of Prior Specifications Across Linear Models", "comments": "Published in at http://dx.doi.org/10.1214/08-STS258 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 332-353", "doi": "10.1214/08-STS258", "report-no": "IMS-STS-STS258", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model comparison requires the specification of a prior distribution\non the parameter space of each candidate model. In this connection two concerns\narise: on the one hand the elicitation task rapidly becomes prohibitive as the\nnumber of models increases; on the other hand numerous prior specifications can\nonly exacerbate the well-known sensitivity to prior assignments, thus producing\nless dependable conclusions. Within the subjective framework, both difficulties\ncan be counteracted by linking priors across models in order to achieve\nsimplification and compatibility; we discuss links with related objective\napproaches. Given an encompassing, or full, model together with a prior on its\nparameter space, we review and summarize a few procedures for deriving priors\nunder a submodel, namely marginalization, conditioning, and Kullback--Leibler\nprojection. These techniques are illustrated and discussed with reference to\nvariable selection in linear models adopting a conventional $g$-prior;\ncomparisons with existing standard approaches are provided. Finally, the\nrelative merits of each procedure are evaluated through simulated and real data\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 07:11:29 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Consonni", "Guido", ""], ["Veronese", "Piero", ""]]}, {"id": "1102.2982", "submitter": "Hani Doss", "authors": "Hani Doss", "title": "Comment: Quantifying Information Loss in Survival Studies", "comments": "Published in at http://dx.doi.org/10.1214/08-STS244B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 313-317", "doi": "10.1214/08-STS244B", "report-no": "IMS-STS-STS244B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on \"Quantifying the Fraction of Missing Information for Hypothesis\nTesting in Statistical and Genetic Studies\" [arXiv:1102.2774]\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 07:33:28 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Doss", "Hani", ""]]}, {"id": "1102.2987", "submitter": "I-Shou Chang", "authors": "I-Shou Chang, Chung-Hsing Chen, Li-Chu Chien, Chao A. Hsiung", "title": "Comment: Quantifying the Fraction of Missing Information for Hypothesis\n  Testing in Statistical and Genetic Studies", "comments": "Published in at http://dx.doi.org/10.1214/08-STS244C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 318-320", "doi": "10.1214/08-STS244C", "report-no": "IMS-STS-STS244C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on \"Quantifying the Fraction of Missing Information for Hypothesis\nTesting in Statistical and Genetic Studies\" [arXiv:1102.2774]\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 08:07:56 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Chang", "I-Shou", ""], ["Chen", "Chung-Hsing", ""], ["Chien", "Li-Chu", ""], ["Hsiung", "Chao A.", ""]]}, {"id": "1102.2993", "submitter": "Tian Zheng", "authors": "Tian Zheng, Shaw-Hwa Lo", "title": "Comment: Quantifying the Fraction of Missing Information for Hypothesis\n  Testing in Statistical and Genetic Studies", "comments": "Published in at http://dx.doi.org/10.1214/08-STS244A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 321-324", "doi": "10.1214/08-STS244A", "report-no": "IMS-STS-STS244A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on \"Quantifying the Fraction of Missing Information for Hypothesis\nTesting in Statistical and Genetic Studies\" [arXiv:1102.2774]\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 08:30:06 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Zheng", "Tian", ""], ["Lo", "Shaw-Hwa", ""]]}, {"id": "1102.3005", "submitter": "Dan L. Nicolae", "authors": "Dan L. Nicolae, Xiao-Li Meng, Augustine Kong", "title": "Rejoinder: Quantifying the Fraction of Missing Information for\n  Hypothesis Testing in Statistical and Genetic Studies", "comments": "Published in at http://dx.doi.org/10.1214/08-STS244REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 325-331", "doi": "10.1214/08-STS244REJ", "report-no": "IMS-STS-STS244REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"Quantifying the Fraction of Missing Information for Hypothesis\nTesting in Statistical and Genetic Studies\" [arXiv:1102.2774]\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 09:18:18 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Nicolae", "Dan L.", ""], ["Meng", "Xiao-Li", ""], ["Kong", "Augustine", ""]]}, {"id": "1102.3074", "submitter": "Genevera Allen", "authors": "Genevera I. Allen and Logan Grosenick and Jonathan Taylor", "title": "A Generalized Least Squares Matrix Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variables in many massive high-dimensional data sets are structured, arising\nfor example from measurements on a regular grid as in imaging and time series\nor from spatial-temporal measurements as in climate studies. Classical\nmultivariate techniques ignore these structural relationships often resulting\nin poor performance. We propose a generalization of the singular value\ndecomposition (SVD) and principal components analysis (PCA) that is appropriate\nfor massive data sets with structured variables or known two-way dependencies.\nBy finding the best low rank approximation of the data with respect to a\ntransposable quadratic norm, our decomposition, entitled the Generalized least\nsquares Matrix Decomposition (GMD), directly accounts for structural\nrelationships. As many variables in high-dimensional settings are often\nirrelevant or noisy, we also regularize our matrix decomposition by adding\ntwo-way penalties to encourage sparsity or smoothness. We develop fast\ncomputational algorithms using our methods to perform generalized PCA (GPCA),\nsparse GPCA, and functional GPCA on massive data sets. Through simulations and\na whole brain functional MRI example we demonstrate the utility of our\nmethodology for dimension reduction, signal recovery, and feature selection\nwith high-dimensional structured data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 14:02:47 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2011 16:52:50 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2012 17:59:55 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Allen", "Genevera I.", ""], ["Grosenick", "Logan", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1102.3592", "submitter": "Ryan Martin", "authors": "Ryan Martin, Jayanta K. Ghosh", "title": "Stochastic Approximation and Newton's Estimate of a Mixing Distribution", "comments": "Published in at http://dx.doi.org/10.1214/08-STS265 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 365-382", "doi": "10.1214/08-STS265", "report-no": "IMS-STS-STS265", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical problems involve mixture models and the need for\ncomputationally efficient methods to estimate the mixing distribution has\nincreased dramatically in recent years. Newton [Sankhya Ser. A 64 (2002)\n306--322] proposed a fast recursive algorithm for estimating the mixing\ndistribution, which we study as a special case of stochastic approximation\n(SA). We begin with a review of SA, some recent statistical applications, and\nthe theory necessary for analysis of a SA algorithm, which includes Lyapunov\nfunctions and ODE stability theory. Then standard SA results are used to prove\nconsistency of Newton's estimate in the case of a finite mixture. We also\npropose a modification of Newton's algorithm that allows for estimation of an\nadditional unknown parameter in the model, and prove its consistency.\n", "versions": [{"version": "v1", "created": "Thu, 17 Feb 2011 14:37:43 GMT"}], "update_date": "2011-02-18", "authors_parsed": [["Martin", "Ryan", ""], ["Ghosh", "Jayanta K.", ""]]}, {"id": "1102.3768", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang, Michael I. Jordan", "title": "Multiway Spectral Clustering: A Margin-Based Perspective", "comments": "Published in at http://dx.doi.org/10.1214/08-STS266 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 383-403", "doi": "10.1214/08-STS266", "report-no": "IMS-STS-STS266", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a broad class of clustering procedures in which an\nintractable combinatorial optimization formulation of clustering is \"relaxed\"\ninto a tractable eigenvector problem, and in which the relaxed solution is\nsubsequently \"rounded\" into an approximate discrete solution to the original\nproblem. In this paper we present a novel margin-based perspective on multiway\nspectral clustering. We show that the margin-based perspective illuminates both\nthe relaxation and rounding aspects of spectral clustering, providing a unified\nanalysis of existing algorithms and guiding the design of new algorithms. We\nalso present connections between spectral clustering and several other topics\nin statistics, specifically minimum-variance clustering, Procrustes analysis\nand Gaussian intrinsic autoregression.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 07:20:24 GMT"}], "update_date": "2011-02-21", "authors_parsed": [["Zhang", "Zhihua", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1102.3773", "submitter": "William F. Rosenberger", "authors": "William F. Rosenberger, Oleksandr Sverdlov", "title": "Handling Covariates in the Design of Clinical Trials", "comments": "Published in at http://dx.doi.org/10.1214/08-STS269 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 404-419", "doi": "10.1214/08-STS269", "report-no": "IMS-STS-STS269", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a split in the statistics community about the need for taking\ncovariates into account in the design phase of a clinical trial. There are many\nadvocates of using stratification and covariate-adaptive randomization to\npromote balance on certain known covariates. However, balance does not always\npromote efficiency or ensure more patients are assigned to the better\ntreatment. We describe these procedures, including model-based procedures, for\nincorporating covariates into the design of clinical trials, and give examples\nwhere balance, efficiency and ethical considerations may be in conflict. We\nadvocate a new class of procedures, covariate-adjusted response-adaptive (CARA)\nrandomization procedures that attempt to optimize both efficiency and ethical\nconsiderations, while maintaining randomization. We review all these\nprocedures, present a few new simulation studies, and conclude with our\nphilosophy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 07:58:49 GMT"}], "update_date": "2011-02-21", "authors_parsed": [["Rosenberger", "William F.", ""], ["Sverdlov", "Oleksandr", ""]]}, {"id": "1102.3780", "submitter": "Francisco J. Samaniego", "authors": "Francisco J. Samaniego", "title": "A Conversation with Myles Hollander", "comments": "Published in at http://dx.doi.org/10.1214/07-STS248 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 420-438", "doi": "10.1214/07-STS248", "report-no": "IMS-STS-STS248", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myles Hollander was born in Brooklyn, New York, on March 21, 1941. He\ngraduated from Carnegie Mellon University in 1961 with a B.S. in mathematics.\nIn the fall of 1961, he entered the Department of Statistics, Stanford\nUniversity, earning his M.S. in statistics in 1962 and his Ph.D. in statistics\nin 1965. He joined the Department of Statistics, Florida State University in\n1965 and retired on May 31, 2007, after 42 years of service. He was department\nchair for nine years 1978-1981, 1999-2005. He was named Professor Emeritus at\nFlorida State upon retirement in 2007. Hollander served as Editor of the\nJournal of the American Statistical Association, Theory and Methods, 1994-1996,\nand was an Associate Editor for that journal from 1985 until he became Theory\nand Methods Editor-Elect in 1993. He also served on the editorial boards of the\nJournal of Nonparametric Statistics (1993-1997; 2003-2005) and Lifetime Data\nAnalysis (1994-2007). Hollander has published over 100 papers on nonparametric\nstatistics, survival analysis, reliability theory, biostatistics, probability\ntheory, decision theory, Bayesian statistics and multivariate analysis. He is\ngrateful for the generous research support he has received throughout his\ncareer, most notably from the Office of Naval Research, the U.S. Air Force\nOffice of Scientific Research, and the National Institutes of Health.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 08:37:33 GMT"}], "update_date": "2011-02-21", "authors_parsed": [["Samaniego", "Francisco J.", ""]]}, {"id": "1102.3851", "submitter": "Pierre Courrieu", "authors": "Pierre Courrieu (LPC), Arnaud Rey (LPC)", "title": "Missing Data Imputation and Corrected Statistics for Large-Scale\n  Behavioral Databases", "comments": "Behavior Research Methods (2011) in press", "journal-ref": null, "doi": "10.3758/s13428-011-0071-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new methodology to solve problems resulting from\nmissing data in large-scale item performance behavioral databases. Useful\nstatistics corrected for missing data are described, and a new method of\nimputation for missing data is proposed. This methodology is applied to the DLP\ndatabase recently published by Keuleers et al. (2010), which allows us to\nconclude that this database fulfills the conditions of use of the method\nrecently proposed by Courrieu et al. (2011) to test item performance models.\nTwo application programs in Matlab code are provided for the imputation of\nmissing data in databases, and for the computation of corrected statistics to\ntest models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 15:17:54 GMT"}], "update_date": "2011-02-21", "authors_parsed": [["Courrieu", "Pierre", "", "LPC"], ["Rey", "Arnaud", "", "LPC"]]}, {"id": "1102.4110", "submitter": "Eric F. Lock", "authors": "Eric F. Lock, Katherine A. Hoadley, J. S. Marron, Andrew B. Nobel", "title": "Joint and individual variation explained (JIVE) for integrated analysis\n  of multiple data types", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS597 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 523-542", "doi": "10.1214/12-AOAS597", "report-no": "IMS-AOAS-AOAS597", "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in several fields now requires the analysis of data sets in which\nmultiple high-dimensional types of data are available for a common set of\nobjects. In particular, The Cancer Genome Atlas (TCGA) includes data from\nseveral diverse genomic technologies on the same cancerous tumor samples. In\nthis paper we introduce Joint and Individual Variation Explained (JIVE), a\ngeneral decomposition of variation for the integrated analysis of such data\nsets. The decomposition consists of three terms: a low-rank approximation\ncapturing joint variation across data types, low-rank approximations for\nstructured variation individual to each data type, and residual noise. JIVE\nquantifies the amount of joint variation between data types, reduces the\ndimensionality of the data and provides new directions for the visual\nexploration of joint and individual structures. The proposed method represents\nan extension of Principal Component Analysis and has clear advantages over\npopular two-block methods such as Canonical Correlation Analysis and Partial\nLeast Squares. A JIVE analysis of gene expression and miRNA data on\nGlioblastoma Multiforme tumor samples reveals gene-miRNA associations and\nprovides better characterization of tumor types. Data and software are\navailable at https://genome.unc.edu/jive/\n", "versions": [{"version": "v1", "created": "Sun, 20 Feb 2011 23:52:20 GMT"}, {"version": "v2", "created": "Tue, 28 May 2013 06:15:04 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Lock", "Eric F.", ""], ["Hoadley", "Katherine A.", ""], ["Marron", "J. S.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1102.4399", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano and Sadanori Konishi", "title": "Semi-supervised logistic discrimination for functional data", "comments": "21 pages, 7 figures", "journal-ref": "Bulletin of Informatics and Cybernetics 44 (2012) 1-15", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class classification methods based on both labeled and unlabeled\nfunctional data sets are discussed. We present a semi-supervised logistic model\nfor classification in the context of functional data analysis. Unknown\nparameters in our proposed model are estimated by regularization with the help\nof EM algorithm. A crucial point in the modeling procedure is the choice of a\nregularization parameter involved in the semi-supervised functional logistic\nmodel. In order to select the adjusted parameter, we introduce model selection\ncriteria from information-theoretic and Bayesian viewpoints. Monte Carlo\nsimulations and a real data analysis are given to examine the effectiveness of\nour proposed modeling strategy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 03:43:24 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2011 06:31:40 GMT"}, {"version": "v3", "created": "Mon, 28 May 2012 11:05:51 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Kawano", "Shuichi", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1102.4432", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (University Paris-Dauphine), Jean-Marie Cornuet\n  (INRA, Montpellier), Jean-Michel Marin (I3M, Montpellier) and Natesh Pillai\n  (Harvard University)", "title": "Lack of confidence in ABC model choice", "comments": "9 pages, 7 figures, 1 table, second revision, submitted to the\n  Proceedings of the National Academy of Sciences, extension of arXiv:1101.5091", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) have become a essential tool for the\nanalysis of complex stochastic models. Earlier, Grelaud et al. (2009) advocated\nthe use of ABC for Bayesian model choice in the specific case of Gibbs random\nfields, relying on a inter-model sufficiency property to show that the\napproximation was legitimate. Having implemented ABC-based model choice in a\nwide range of phylogenetic models in the DIY-ABC software (Cornuet et al.,\n2008), we now present theoretical background as to why a generic use of ABC for\nmodel choice is ungrounded, since it depends on an unknown amount of\ninformation loss induced by the use of insufficient summary statistics. The\napproximation error of the posterior probabilities of the models under\ncomparison may thus be unrelated with the computational effort spent in running\nan ABC algorithm. We then conclude that additional empirical verifications of\nthe performances of the ABC procedure as those available in DIYABC are\nnecessary to conduct model choice.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 08:58:59 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2011 13:25:31 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2011 08:16:05 GMT"}, {"version": "v4", "created": "Mon, 20 Jun 2011 21:12:27 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Robert", "Christian P.", "", "University Paris-Dauphine"], ["Cornuet", "Jean-Marie", "", "INRA, Montpellier"], ["Marin", "Jean-Michel", "", "I3M, Montpellier"], ["Pillai", "Natesh", "", "Harvard University"]]}, {"id": "1102.4434", "submitter": "Kaspar Rufibach", "authors": "Kaspar Rufibach", "title": "Selection models with monotone weight functions in meta analysis", "comments": "15 pages, 2 figures. Some minor changes according to reviewer\n  comments", "journal-ref": "Biom. J. (2011), 53(4), 689-704", "doi": "10.1002/bimj.201000240", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publication bias, the fact that studies identified for inclusion in a meta\nanalysis do not represent all studies on the topic of interest, is commonly\nrecognized as a threat to the validity of the results of a meta analysis. One\nway to explicitly model publication bias is via selection models or weighted\nprobability distributions. We adopt the nonparametric approach initially\nintroduced by Dear (1992) but impose that the weight function $w$ is monotonely\nnon-increasing as a function of the $p$-value. Since in meta analysis one\ntypically only has few studies or \"observations\", regularization of the\nestimation problem seems sensible. In addition, virtually all parametric weight\nfunctions proposed so far in the literature are in fact decreasing. We discuss\nhow to estimate a decreasing weight function in the above model and illustrate\nthe new methodology on two well-known examples. The new approach potentially\noffers more insight in the selection process than other methods and is more\nflexible than parametric approaches. Some basic properties of the\nlog-likelihood function and computation of a $p$-value quantifying the evidence\nagainst the null hypothesis of a constant weight function are indicated. In\naddition, we provide an approximate selection bias adjusted profile likelihood\nconfidence interval for the treatment effect. The corresponding software and\nthe datasets used to illustrate it are provided as the R package selectMeta.\nThis enables full reproducibility of the results in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 09:00:35 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2011 07:11:15 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Rufibach", "Kaspar", ""]]}, {"id": "1102.4599", "submitter": "Maciej Kurant", "authors": "Maciej Kurant, Athina Markopoulou, Patrick Thiran", "title": "Towards Unbiased BFS Sampling", "comments": "BFS, RDS, graph traversal, sampling bias correction", "journal-ref": "arXiv:1004.1729, 2010", "doi": null, "report-no": null, "categories": "cs.SI cs.NI stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Breadth First Search (BFS) is a widely used approach for sampling large\nunknown Internet topologies. Its main advantage over random walks and other\nexploration techniques is that a BFS sample is a plausible graph on its own,\nand therefore we can study its topological characteristics. However, it has\nbeen empirically observed that incomplete BFS is biased toward high-degree\nnodes, which may strongly affect the measurements. In this paper, we first\nanalytically quantify the degree bias of BFS sampling. In particular, we\ncalculate the node degree distribution expected to be observed by BFS as a\nfunction of the fraction f of covered nodes, in a random graph RG(pk) with an\narbitrary degree distribution pk. We also show that, for RG(pk), all commonly\nused graph traversal techniques (BFS, DFS, Forest Fire, Snowball Sampling, RDS)\nsuffer from exactly the same bias. Next, based on our theoretical analysis, we\npropose a practical BFS-bias correction procedure. It takes as input a\ncollected BFS sample together with its fraction f. Even though RG(pk) does not\ncapture many graph properties common in real-life graphs (such as\nassortativity), our RG(pk)-based correction technique performs well on a broad\nrange of Internet topologies and on two large BFS samples of Facebook and Orkut\nnetworks. Finally, we consider and evaluate a family of alternative correction\nprocedures, and demonstrate that, although they are unbiased for an arbitrary\ntopology, their large variance makes them far less effective than the\nRG(pk)-based technique.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 20:19:40 GMT"}], "update_date": "2011-02-23", "authors_parsed": [["Kurant", "Maciej", ""], ["Markopoulou", "Athina", ""], ["Thiran", "Patrick", ""]]}, {"id": "1102.4803", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Detection of objects in noisy images and site percolation on square\n  lattices", "comments": "This paper first appeared as EURANDOM Report 2009-035 on November 11,\n  2009. Link to the paper at the EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2009/035-report.pdf Link to the abstract\n  at EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2009/035-abstract.pdf", "journal-ref": null, "doi": null, "report-no": "EURANDOM Report 2009-035", "categories": "math.ST cs.CV math.PR stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic method for detection of objects in noisy\nimages. The method uses results from percolation and random graph theories. We\npresent an algorithm that allows to detect objects of unknown shapes in the\npresence of random noise. Our procedure substantially differs from\nwavelets-based algorithms. The algorithm has linear complexity and exponential\naccuracy and is appropriate for real-time systems. We prove results on\nconsistency and algorithmic complexity of our procedure.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 17:28:21 GMT"}], "update_date": "2011-02-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.4811", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Robust nonparametric detection of objects in noisy images", "comments": "This paper initially appeared in 2010 as EURANDOM Report 2010-049.\n  Link to the abstract at EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/049-abstract.pdf Link to the paper at\n  EURANDOM repository: http://www.eurandom.tue.nl/reports/2010/049-report.pdf", "journal-ref": "Journal of Nonparametric Statistics (2013), Vol. 25(2), pp.\n  409-426", "doi": "10.1080/10485252.2012.759570", "report-no": "EURANDOM Report 2010-049", "categories": "math.ST math-ph math.MP math.PR stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel statistical hypothesis testing method for detection of\nobjects in noisy images. The method uses results from percolation theory and\nrandom graph theory. We present an algorithm that allows to detect objects of\nunknown shapes in the presence of nonparametric noise of unknown level and of\nunknown distribution. No boundary shape constraints are imposed on the object,\nonly a weak bulk condition for the object's interior is required. The algorithm\nhas linear complexity and exponential accuracy and is appropriate for real-time\nsystems. In this paper, we develop further the mathematical formalism of our\nmethod and explore important connections to the mathematical theory of\npercolation and statistical physics. We prove results on consistency and\nalgorithmic complexity of our testing procedure. In addition, we address not\nonly an asymptotic behavior of the method, but also a finite sample performance\nof our test.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 18:21:58 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.4816", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Computationally efficient algorithms for statistical image processing.\n  Implementation in R", "comments": "This paper initially appeared in 2010 as EURANDOM Report 2010-053.\n  Link to EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/053-report.pdf Link to the abstract\n  at EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/053-abstract.pdf", "journal-ref": null, "doi": null, "report-no": "EURANDOM Report 2010-053", "categories": "stat.CO cs.CV stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the series of our earlier papers on the subject, we proposed a novel\nstatistical hypothesis testing method for detection of objects in noisy images.\nThe method uses results from percolation theory and random graph theory. We\ndeveloped algorithms that allowed to detect objects of unknown shapes in the\npresence of nonparametric noise of unknown level and of unknown distribution.\nNo boundary shape constraints were imposed on the objects, only a weak bulk\ncondition for the object's interior was required. Our algorithms have linear\ncomplexity and exponential accuracy. In the present paper, we describe an\nimplementation of our nonparametric hypothesis testing method. We provide a\nprogram that can be used for statistical experiments in image processing. This\nprogram is written in the statistical programming language R.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 18:46:56 GMT"}], "update_date": "2011-02-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.4820", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Multiple testing, uncertainty and realistic pictures", "comments": "This paper initially appeared in January 2011 as EURANDOM Report\n  2011-004. Link to the abstract at EURANDOM Repository:\n  http://www.eurandom.tue.nl/reports/2011/004-abstract.pdf Link to the paper at\n  EURANDOM Repository: http://www.eurandom.tue.nl/reports/2011/004-report.pdf", "journal-ref": null, "doi": null, "report-no": "EURANDOM Report 2011-004", "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical detection of grayscale objects in noisy images. The\nobject of interest is of unknown shape and has an unknown intensity, that can\nbe varying over the object and can be negative. No boundary shape constraints\nare imposed on the object, only a weak bulk condition for the object's interior\nis required. We propose an algorithm that can be used to detect grayscale\nobjects of unknown shapes in the presence of nonparametric noise of unknown\nlevel. Our algorithm is based on a nonparametric multiple testing procedure. We\nestablish the limit of applicability of our method via an explicit,\nclosed-form, non-asymptotic and nonparametric consistency bound. This bound is\nvalid for a wide class of nonparametric noise distributions. We achieve this by\nproving an uncertainty principle for percolation on finite lattices.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 19:05:16 GMT"}], "update_date": "2011-02-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.5014", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Randomized algorithms for statistical image analysis and site\n  percolation on square lattices", "comments": "Submitted for publication on December 11, 2009", "journal-ref": null, "doi": "10.1111/stan.12010", "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic method for detection of objects in noisy\nimages. The method uses results from percolation and random graph theories. We\npresent an algorithm that allows to detect objects of unknown shapes in the\npresence of random noise. The algorithm has linear complexity and exponential\naccuracy and is appropriate for real-time systems. We prove results on\nconsistency and algorithmic complexity of our procedure.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 15:20:41 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.5019", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich and Patrick Laurie Davies", "title": "Unsupervised nonparametric detection of unknown objects in noisy images\n  based on percolation theory", "comments": "Added references, updated terminology, changed the order of the\n  authors and the title. arXiv admin note: substantial text overlap with\n  arXiv:1102.4803, arXiv:1102.5014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an unsupervised, nonparametric, and scalable statistical learning\nmethod for detection of unknown objects in noisy images. The method uses\nresults from percolation theory and random graph theory. We present an\nalgorithm that allows to detect objects of unknown shapes and sizes in the\npresence of nonparametric noise of unknown level. The noise density is assumed\nto be unknown and can be very irregular. The algorithm has linear complexity\nand exponential accuracy and is appropriate for real-time systems. We prove\nstrong consistency and scalability of our method in this setup with minimal\nassumptions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 15:37:09 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 13:51:22 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""], ["Davies", "Patrick Laurie", ""]]}, {"id": "1102.5021", "submitter": "Nevio Dubbini", "authors": "Nevio Dubbini", "title": "Causality as a unifying approach between activation and connectivity\n  analysis of fMRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper indicates causality as the tool that unifies the analysis of both\nactivations and connectivity of brain areas, obtained with fMRI data. Causality\nanalysis is commonly applied to study connectivity, so this work focuses on\ndemonstrating that also the detection of activations can be handled with a\ncausality analysis. We test our method on finger tapping data, in which GLM and\nGranger Causality approaches are compared in finding activations. Granger\ncausality not only performs the task well, but indeed we obtained a better\nlocalization (i.e. precision) of activations. As a result we claim that\ncausality must be the main tool to investigate activations, since it is a\nmeasure of \"how much\" the stimulus influences the BOLD signal, and since it\nunifies connectivity and activations analysis under the same area.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 15:55:27 GMT"}], "update_date": "2011-02-25", "authors_parsed": [["Dubbini", "Nevio", ""]]}, {"id": "1102.5063", "submitter": "Animashree Anandkumar", "authors": "Animashree Anandkumar, Avinatan Hassidim, Jonathan Kelner", "title": "Topology Discovery of Sparse Random Graphs With Few Participants", "comments": "A shorter version appears in ACM SIGMETRICS 2011. This version is\n  scheduled to appear in J. on Random Structures and Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of topology discovery of sparse random graphs using\nend-to-end random measurements (e.g., delay) between a subset of nodes,\nreferred to as the participants. The rest of the nodes are hidden, and do not\nprovide any information for topology discovery. We consider topology discovery\nunder two routing models: (a) the participants exchange messages along the\nshortest paths and obtain end-to-end measurements, and (b) additionally, the\nparticipants exchange messages along the second shortest path. For scenario\n(a), our proposed algorithm results in a sub-linear edit-distance guarantee\nusing a sub-linear number of uniformly selected participants. For scenario (b),\nwe obtain a much stronger result, and show that we can achieve consistent\nreconstruction when a sub-linear number of uniformly selected nodes\nparticipate. This implies that accurate discovery of sparse random graphs is\ntractable using an extremely small number of participants. We finally obtain a\nlower bound on the number of participants required by any algorithm to\nreconstruct the original random graph up to a given edit distance. We also\ndemonstrate that while consistent discovery is tractable for sparse random\ngraphs using a small number of participants, in general, there are graphs which\ncannot be discovered by any algorithm even with a significant number of\nparticipants, and with the availability of end-to-end information along all the\npaths between the participants.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 18:54:26 GMT"}, {"version": "v2", "created": "Mon, 28 Feb 2011 18:18:59 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2012 01:31:30 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Hassidim", "Avinatan", ""], ["Kelner", "Jonathan", ""]]}, {"id": "1102.5088", "submitter": "Grant Izmirlian", "authors": "Grant Izmirlian", "title": "Estimation of the relative risk following group sequential procedure\n  based upon the weighted log-rank statistic", "comments": null, "journal-ref": "Statistics and Its Interface. 7 (2014) 27-42", "doi": "10.4310/SII.2014.v7.n1.a4", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper we consider a group sequentially monitored trial on a survival\nendpoint, monitored using a weighted log-rank (WLR) statistic with\ndeterministic weight function. We introduce a summary statistic in the form of\na weighted average logged relative risk and show that if there is no sign\nchange in the instantaneous logged relative risk, there always exists a\nbijection between the WLR statistic and the weighted average logged relative\nrisk. We show that this bijection can be consistently estimated at each\nanalysis under a suitable shape assumption, for which we have listed two\npossibilities. We indicate how to derive a design-adjusted p-value and\nconfidence interval and suggest how to apply the bias-correction method.\nFinally, we document several decisions made in the design of the NLST interim\nanalysis plan and in reporting its results on the primary endpoint.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 20:58:53 GMT"}, {"version": "v2", "created": "Fri, 25 Feb 2011 18:35:21 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Izmirlian", "Grant", ""]]}, {"id": "1102.5390", "submitter": "Anna Klimova", "authors": "Anna Klimova, Tam\\'as Rudas, Adrian Dobra", "title": "Relational models for contingency tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers general multiplicative models for complete and incomplete\ncontingency tables that generalize log-linear and several other models and are\nentirely coordinate free. Sufficient conditions of the existence of maximum\nlikelihood estimates under these models are given, and it is shown that the\nusual equivalence between multinomial and Poisson likelihoods holds if and only\nif an overall effect is present in the model. If such an effect is not assumed,\nthe model becomes a curved exponential family and a related mixed\nparameterization is given that relies on non-homogeneous odds ratios. Several\nexamples are presented to illustrate the properties and use of such models.\n", "versions": [{"version": "v1", "created": "Sat, 26 Feb 2011 06:10:02 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2011 16:55:37 GMT"}], "update_date": "2011-03-04", "authors_parsed": [["Klimova", "Anna", ""], ["Rudas", "Tam\u00e1s", ""], ["Dobra", "Adrian", ""]]}, {"id": "1102.5431", "submitter": "Mohamed Boutahar", "authors": "Mohamed Boutahar (GREQAM)", "title": "Testing for change in mean of heteroskedastic time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a Lagrange Multiplier-type test (LM) to detect\nchange in the mean of time series with heteroskedasticity of unknown form. We\nderive the limiting distribution under the null, and prove the consistency of\nthe test against the alternative of either an abrupt or smooth changes in the\nmean. We perform also some Monte Carlo simulations to analyze the size\ndistortion and the power of the proposed test. We conclude that for moderate\nsample size, the test has a good performance. We finally carry out an empirical\napplication using the daily closing level of the S&P 500 stock index, in order\nto illustrate the usefulness of the proposed test.\n", "versions": [{"version": "v1", "created": "Sat, 26 Feb 2011 17:55:44 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Boutahar", "Mohamed", "", "GREQAM"]]}, {"id": "1102.5496", "submitter": "Ronny Luss", "authors": "Ronny Luss, Saharon Rosset, Moni Shahar", "title": "Efficient regularized isotonic regression with application to gene--gene\n  interaction search", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS504 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 253-283", "doi": "10.1214/11-AOAS504", "report-no": "IMS-AOAS-AOAS504", "categories": "stat.ME cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isotonic regression is a nonparametric approach for fitting monotonic models\nto data that has been widely studied from both theoretical and practical\nperspectives. However, this approach encounters computational and statistical\noverfitting issues in higher dimensions. To address both concerns, we present\nan algorithm, which we term Isotonic Recursive Partitioning (IRP), for isotonic\nregression based on recursively partitioning the covariate space through\nsolution of progressively smaller \"best cut\" subproblems. This creates a\nregularized sequence of isotonic models of increasing model complexity that\nconverges to the global isotonic regression solution. The models along the\nsequence are often more accurate than the unregularized isotonic regression\nmodel because of the complexity control they offer. We quantify this complexity\ncontrol through estimation of degrees of freedom along the path. Success of the\nregularized models in prediction and IRPs favorable computational properties\nare demonstrated through a series of simulated and real data experiments. We\ndiscuss application of IRP to the problem of searching for gene--gene\ninteractions and epistasis, and demonstrate it on data from genome-wide\nassociation studies of three common diseases.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 12:13:52 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2012 06:23:50 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Luss", "Ronny", ""], ["Rosset", "Saharon", ""], ["Shahar", "Moni", ""]]}, {"id": "1102.5509", "submitter": "Leo Lahti", "authors": "Leo Lahti", "title": "Probabilistic analysis of the human transcriptome with side information", "comments": "Doctoral thesis. 103 pages, 11 figures", "journal-ref": "TKK Dissertations in Information and Computer Science TKK-ICS-D19.\n  Aalto University School of Science and Technology, Department of Information\n  and Computer Science, Espoo, Finland, 2010", "doi": null, "report-no": "TKK-ICS-D19", "categories": "stat.ML cs.CE q-bio.GN q-bio.MN q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Understanding functional organization of genetic information is a major\nchallenge in modern biology. Following the initial publication of the human\ngenome sequence in 2001, advances in high-throughput measurement technologies\nand efficient sharing of research material through community databases have\nopened up new views to the study of living organisms and the structure of life.\nIn this thesis, novel computational strategies have been developed to\ninvestigate a key functional layer of genetic information, the human\ntranscriptome, which regulates the function of living cells through protein\nsynthesis. The key contributions of the thesis are general exploratory tools\nfor high-throughput data analysis that have provided new insights to\ncell-biological networks, cancer mechanisms and other aspects of genome\nfunction.\n  A central challenge in functional genomics is that high-dimensional genomic\nobservations are associated with high levels of complex and largely unknown\nsources of variation. By combining statistical evidence across multiple\nmeasurement sources and the wealth of background information in genomic data\nrepositories it has been possible to solve some the uncertainties associated\nwith individual observations and to identify functional mechanisms that could\nnot be detected based on individual measurement sources. Statistical learning\nand probabilistic models provide a natural framework for such modeling tasks.\nOpen source implementations of the key methodological contributions have been\nreleased to facilitate further adoption of the developed methods by the\nresearch community.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 14:11:30 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Lahti", "Leo", ""]]}, {"id": "1102.5541", "submitter": "Giorgos Sermaidis", "authors": "Giorgos Sermaidis, Omiros Papaspiliopoulos, Gareth O. Roberts, Alex\n  Beskos, Paul Fearnhead", "title": "Markov chain Monte Carlo for exact inference for diffusions", "comments": "23 pages, 6 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop exact Markov chain Monte Carlo methods for discretely-sampled,\ndirectly and indirectly observed diffusions. The qualification \"exact\" refers\nto the fact that the invariant and limiting distribution of the Markov chains\nis the posterior distribution of the parameters free of any discretisation\nerror. The class of processes to which our methods directly apply are those\nwhich can be simulated using the most general to date exact simulation\nalgorithm. The article introduces various methods to boost the performance of\nthe basic scheme, including reparametrisations and auxiliary Poisson sampling.\nWe contrast both theoretically and empirically how this new approach compares\nto irreducible high frequency imputation, which is the state-of-the-art\nalternative for the class of processes we consider, and we uncover intriguing\nconnections. All methods discussed in the article are tested on typical\nexamples.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 19:37:01 GMT"}, {"version": "v2", "created": "Thu, 3 May 2012 10:36:19 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Sermaidis", "Giorgos", ""], ["Papaspiliopoulos", "Omiros", ""], ["Roberts", "Gareth O.", ""], ["Beskos", "Alex", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1102.5721", "submitter": "Peter Hoff", "authors": "Peter D. Hoff and Xiaoyue Niu", "title": "A covariance regression model", "comments": "A version of this article will appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical regression analysis relates the expectation of a response variable\nto a linear combination of explanatory variables. In this article, we propose a\ncovariance regression model that parameterizes the covariance matrix of a\nmultivariate response vector as a parsimonious quadratic function of\nexplanatory variables. The approach is analogous to the mean regression model,\nand is similar to a factor analysis model in which the factor loadings depend\non the explanatory variables. Using a random-effects representation, parameter\nestimation for the model is straightforward using either an EM-algorithm or an\nMCMC approximation via Gibbs sampling. The proposed methodology provides a\nsimple but flexible representation of heteroscedasticity across the levels of\nan explanatory variable, improves estimation of the mean function and gives\nbetter calibrated prediction regions when compared to a homoscedastic model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 18:03:52 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Hoff", "Peter D.", ""], ["Niu", "Xiaoyue", ""]]}]