[{"id": "1705.00289", "submitter": "Faustino Prieto", "authors": "Jos\\'e Mar\\'ia Sarabia, Emilio G\\'omez-D\\'eniz, Faustino Prieto,\n  Vanesa Jord\\'a", "title": "Aggregation of Dependent Risks in Mixtures of Exponential Distributions\n  and Extensions", "comments": "This is a preprint (34 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of the sum of dependent risks is a crucial aspect in\nactuarial sciences, risk management and in many branches of applied\nprobability. In this paper, we obtain analytic expressions for the probability\ndensity function (pdf) and the cumulative distribution function (cdf) of\naggregated risks, modeled according to a mixture of exponential distributions.\nWe first review the properties of the multivariate mixture of exponential\ndistributions, to then obtain the analytical formulation for the pdf and the\ncdf for the aggregated distribution. We study in detail some specific families\nwith Pareto (Sarabia et al, 2016), Gamma, Weibull and inverse Gaussian mixture\nof exponentials (Whitmore and Lee, 1991) claims. We also discuss briefly the\ncomputation of risk measures, formulas for the ruin probability (Albrecher et\nal., 2011) and the collective risk model. An extension of the basic model based\non mixtures of gamma distributions is proposed, which is one of the suggested\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 09:30:52 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Sarabia", "Jos\u00e9 Mar\u00eda", ""], ["G\u00f3mez-D\u00e9niz", "Emilio", ""], ["Prieto", "Faustino", ""], ["Jord\u00e1", "Vanesa", ""]]}, {"id": "1705.00296", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Michael S{\\o}rensen, Kanti V. Mardia,\n  Thomas Hamelryck", "title": "Langevin diffusions on the torus: estimation and applications", "comments": "29 pages, 8 figures, 5 tables", "journal-ref": "Statistics and Computing, 29:1-22, 2019", "doi": "10.1007/s11222-017-9790-2", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce stochastic models for continuous-time evolution of angles and\ndevelop their estimation. We focus on studying Langevin diffusions with\nstationary distributions equal to well-known distributions from directional\nstatistics, since such diffusions can be regarded as toroidal analogues of the\nOrnstein-Uhlenbeck process. Their likelihood function is a product of\ntransition densities with no analytical expression, but that can be calculated\nby solving the Fokker-Planck equation numerically through adequate schemes. We\npropose three approximate likelihoods that are computationally tractable: (i) a\nlikelihood based on the stationary distribution; (ii) toroidal adaptations of\nthe Euler and Shoji-Ozaki pseudo-likelihoods; (iii) a likelihood based on a\nspecific approximation to the transition density of the wrapped normal process.\nA simulation study compares, in dimensions one and two, the approximate\ntransition densities to the exact ones, and investigates the empirical\nperformance of the approximate likelihoods. Finally, two diffusions are used to\nmodel the evolution of the backbone angles of the protein G (PDB identifier\n1GB1) during a molecular dynamics simulation. The software package sdetorus\nimplements the estimation methods and applications presented in the paper.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 11:04:13 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 22:38:56 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 11:03:03 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 09:43:12 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["S\u00f8rensen", "Michael", ""], ["Mardia", "Kanti V.", ""], ["Hamelryck", "Thomas", ""]]}, {"id": "1705.00351", "submitter": "Cornelis Potgieter", "authors": "F. Lombard, Douglas M. Hawkins and Cornelis Potgieter", "title": "Nonparametric Cusum Charts for Angular Data with Applications in Health\n  Science and Astrophysics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops non-parametric rotation invariant CUSUMs suited to the\ndetection of changes in the mean direction as well as changes in the\nconcentration parameter of angular data. The properties of the CUSUMs are\nillustrated by theoretical calculations, Monte Carlo simulation and application\nto sequentially observed angular data from health science and astrophysics.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 17:58:02 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 15:32:33 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Lombard", "F.", ""], ["Hawkins", "Douglas M.", ""], ["Potgieter", "Cornelis", ""]]}, {"id": "1705.00395", "submitter": "Lingzhou Xue", "authors": "Wei Luo, Lingzhou Xue, Jiawei Yao and Xiufan Yu", "title": "Inverse Moment Methods for Sufficient Forecasting using High-Dimensional\n  Predictors", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider forecasting a single time series using a large number of\npredictors in the presence of a possible nonlinear forecast function. Assuming\nthat the predictors affect the response through the latent factors, we propose\nto first conduct factor analysis and then apply sufficient dimension reduction\non the estimated factors, to derive the reduced data for subsequent\nforecasting. Using directional regression and the inverse third-moment method\nin the stage of sufficient dimension reduction, the proposed methods can\ncapture the non-monotone effect of factors on the response. We also allow a\ndiverging number of factors and only impose general regularity conditions on\nthe distribution of factors, avoiding the undesired time reversibility of the\nfactors by the latter. These make the proposed methods fundamentally more\napplicable than the sufficient forecasting method in Fan et al. (2017). The\nproposed methods are demonstrated in both simulation studies and an empirical\nstudy of forecasting monthly macroeconomic data from 1959 to 2016. Also, our\ntheory contributes to the literature of sufficient dimension reduction, as it\nincludes an invariance result, a path to perform sufficient dimension reduction\nunder the high-dimensional setting without assuming sparsity, and the\ncorresponding order-determination procedure.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 01:10:35 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 04:56:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Luo", "Wei", ""], ["Xue", "Lingzhou", ""], ["Yao", "Jiawei", ""], ["Yu", "Xiufan", ""]]}, {"id": "1705.00506", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Dylan S. Small", "title": "Paradoxes in instrumental variable studies with missing data and\n  one-sided noncompliance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in instrumental variable studies for instrument values to be\nmissing, for example when the instrument is a genetic test in Mendelian\nrandomization studies. In this paper we discuss two apparent paradoxes that\narise in so-called single consent designs where there is one-sided\nnoncompliance, i.e., where unencouraged units cannot access treatment. The\nfirst paradox is that, even under a missing completely at random assumption, a\ncomplete-case analysis is biased when knowledge of one-sided noncompliance is\ntaken into account; this is not the case when such information is disregarded.\nThis occurs because incorporating information about one-sided noncompliance\ninduces a dependence between the missingness and treatment. The second paradox\nis that, although incorporating such information does not lead to efficiency\ngains without missing data, the story is different when instrument values are\nmissing: there, incorporating such information changes the efficiency bound,\nallowing possible efficiency gains. This is because some of the missing values\ncan be filled in, based on the fact that anyone who received treatment must\nhave been encouraged by the instrument (since the unencouraged cannot access\ntreatment).\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 13:19:30 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 00:15:03 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1705.00634", "submitter": "Prasad Chalasani", "authors": "Prasad Chalasani, Ari Buchalter, Jaynth Thiagarajan, Ezra Winston", "title": "Counterfactual-based Incrementality Measurement in a Digital Ad-Buying\n  Platform", "comments": "44 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of measuring the true incremental effectiveness of a digital\nadvertising campaign is of increasing importance to marketers. With a large and\nincreasing percentage of digital advertising delivered via\nDemand-Side-Platforms (DSPs) executing campaigns via Real-Time-Bidding (RTB)\nauctions and programmatic approaches, a measurement solution that satisfies\nboth advertiser concerns and the constraints of a DSP is of particular\ninterest.\n  MediaMath (a DSP) has developed the first practical, statistically sound\nrandomization-based methodology for causal ad effectiveness (or Ad Lift)\nmeasurement by a DSP (or similar digital advertising execution system that may\nnot have full control over the advertising transaction mechanisms). We describe\nour solution and establish its soundness within the causal framework of\ncounterfactuals and potential outcomes, and present a Gibbs-sampling procedure\nfor estimating confidence intervals around the estimated Ad Lift. We also\naddress practical complications (unique to the digital advertising setting)\nthat stem from the fact that digital advertising is targeted and measured via\nidentifiers (e.g., cookies, mobile advertising IDs) that may not be stable over\ntime. One such complication is the repeated occurrence of identifiers, leading\nto interference among observations. Another is due to the possibility of\nmultiple identifiers being associated with the same consumer, leading to\n\"contamination\" with some of their identifiers being assigned to the Treatment\ngroup and others to the Control group. Complications such as these have\nseverely impaired previous efforts to derive accurate measurements of lift in\npractice.\n  In contrast to a few other papers on the subject, this paper has an\nexpository aim as well, and provides a rigorous, self-contained, and\nreadily-implementable treatment of all relevant concepts.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 18:01:01 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 17:30:55 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Chalasani", "Prasad", ""], ["Buchalter", "Ari", ""], ["Thiagarajan", "Jaynth", ""], ["Winston", "Ezra", ""]]}, {"id": "1705.00644", "submitter": "Adam Smith", "authors": "A.D. Smith, B. Hofner, J.E. Osenkowski, T. Allison, G. Sadoti, S.R.\n  McWilliams, P.W.C. Paton", "title": "Spatiotemporal modelling of sea duck abundance: implications for marine\n  spatial planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective marine spatial plans require information on the distribution and\nabundance of biological resources that are potentially vulnerable to\nanthropogenic change. In North America, spatially-explicit abundance estimates\nof marine birds are necessary to assess potential impacts from planned offshore\nwind energy developments (OWED). Sea ducks are particularly relevant in this\ncontext as populations of most North American species are below historic levels\nand European studies suggest OWEDs. We modelled species occupancy using a\ngeneralized additive model and conditional abundance with generalized additive\nmodels for location, scale, and shape; the models were subsequently combined to\nestimate unconditional abundance. We demonstrate this flexible, model-based\napproach using sea ducks (Common Eider [Somateria mollissima], Black Scoter\n[Melanitta americana], Surf Scoter [M. perspicillata], White-winged Scoter [M.\ndeglandi], and Long-tailed Duck [Clangula hyemalis]) in Nantucket Sound,\nMassachusetts, USA, which supports some of the largest concentrations of\nwintering sea ducks in eastern North America and where a 454-MW OWED is\nproposed. Our approach to species distribution and abundance modelling offers\nseveral useful features including (1) the ability to model all conditional\ndistribution parameters as a function of covariates, (2) integrated variable\nreduction and selection among many covariates, (3) integrated model selection,\nand (4) the efficient incorporation of smooth effects to capture spatiotemporal\ntrends poorly explained by other covariates. This modelling approach should\nprove useful for marine spatial planners in siting OWEDs while considering key\nhabitats and areas potentially vulnerable to anthropogenic stressors. Moreover,\nthe approach is equally suitable for terrestrial or aquatic systems.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 18:13:51 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Smith", "A. D.", ""], ["Hofner", "B.", ""], ["Osenkowski", "J. E.", ""], ["Allison", "T.", ""], ["Sadoti", "G.", ""], ["McWilliams", "S. R.", ""], ["Paton", "P. W. C.", ""]]}, {"id": "1705.00689", "submitter": "Tuomas Rajala", "authors": "Tuomas Rajala, David Murrell and Sofia Olhede", "title": "Detecting multivariate interactions in spatial point patterns with Gibbs\n  models and variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for detecting significant interactions in very large\nmultivariate spatial point patterns. This methodology develops high dimensional\ndata understanding in the point process setting. The method is based on\nmodelling the patterns using a flexible Gibbs point process model to directly\ncharacterise point-to-point interactions at different spatial scales. By using\nthe Gibbs framework significant interactions can also be captured at small\nscales. Subsequently, the Gibbs point process is fitted using a\npseudo-likelihood approximation, and we select significant interactions\nautomatically using the group lasso penalty with this likelihood approximation.\nThus we estimate the multivariate interactions stably even in this setting. We\ndemonstrate the feasibility of the method with a simulation study and show its\npower by applying it to a large and complex rainforest plant population data\nset of 83 species.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:59:01 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 16:30:46 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Rajala", "Tuomas", ""], ["Murrell", "David", ""], ["Olhede", "Sofia", ""]]}, {"id": "1705.00951", "submitter": "Nicholas Horton", "authors": "Ian R. White and James Carpenter and Nicholas J. Horton", "title": "A mean score method for sensitivity analysis to departures from the\n  missing at random assumption in randomised trials", "comments": "pre-publication (author version) in press, Statistica Sinica", "journal-ref": null, "doi": "10.5705/ss.202016.0308", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most analyses of randomised trials with incomplete outcomes make untestable\nassumptions and should therefore be subjected to sensitivity analyses. However,\nmethods for sensitivity analyses are not widely used. We propose a mean score\napproach for exploring global sensitivity to departures from missing at random\nor other assumptions about incomplete outcome data in a randomised trial. We\nassume a single outcome analysed under a generalised linear model. One or more\nsensitivity parameters, specified by the user, measure the degree of departure\nfrom missing at random in a pattern mixture model. Advantages of our method are\nthat its sensitivity parameters are relatively easy to interpret and so can be\nelicited from subject matter experts; it is fast and non-stochastic; and its\npoint estimate, standard error and confidence interval agree perfectly with\nstandard methods when particular values of the sensitivity parameters make\nthose standard methods appropriate. We illustrate the method using data from a\nmental health trial.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 13:15:46 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["White", "Ian R.", ""], ["Carpenter", "James", ""], ["Horton", "Nicholas J.", ""]]}, {"id": "1705.00998", "submitter": "Yixin Wang", "authors": "Yixin Wang, Jos\\'e R. Zubizarreta", "title": "Minimal Dispersion Approximately Balancing Weights: Asymptotic\n  Properties and Practical Considerations", "comments": "41 pages", "journal-ref": null, "doi": "10.1093/biomet/asz050", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighting methods are widely used to adjust for covariates in observational\nstudies, sample surveys, and regression settings. In this paper, we study a\nclass of recently proposed weighting methods which find the weights of minimum\ndispersion that approximately balance the covariates. We call these weights\n\"minimal weights\" and study them under a common optimization framework. The key\nobservation is the connection between approximate covariate balance and\nshrinkage estimation of the propensity score. This connection leads to both\ntheoretical and practical developments. From a theoretical standpoint, we\ncharacterize the asymptotic properties of minimal weights and show that, under\nstandard smoothness conditions on the propensity score function, minimal\nweights are consistent estimates of the true inverse probability weights. Also,\nwe show that the resulting weighting estimator is consistent, asymptotically\nnormal, and semiparametrically efficient. From a practical standpoint, we\npresent a finite sample oracle inequality that bounds the loss incurred by\nbalancing more functions of the covariates than strictly needed. This\ninequality shows that minimal weights implicitly bound the number of active\ncovariate balance constraints. We finally provide a tuning algorithm for\nchoosing the degree of approximate balance in minimal weights. We conclude the\npaper with four empirical studies that suggest approximate balance is\npreferable to exact balance, especially when there is limited overlap in\ncovariate distributions. In these studies, we show that the root mean squared\nerror of the weighting estimator can be reduced by as much as a half with\napproximate balance.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 14:31:32 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 03:37:40 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 02:35:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Yixin", ""], ["Zubizarreta", "Jos\u00e9 R.", ""]]}, {"id": "1705.01024", "submitter": "Yinchu Zhu", "authors": "Yinchu Zhu and Jelena Bradic", "title": "A projection pursuit framework for testing general high-dimensional\n  hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a framework for testing general hypothesis in\nhigh-dimensional models where the number of variables may far exceed the number\nof observations. Existing literature has considered less than a handful of\nhypotheses, such as testing individual coordinates of the model parameter.\nHowever, the problem of testing general and complex hypotheses remains widely\nopen. We propose a new inference method developed around the hypothesis\nadaptive projection pursuit framework, which solves the testing problems in the\nmost general case. The proposed inference is centered around a new class of\nestimators defined as $l_1$ projection of the initial guess of the unknown onto\nthe space defined by the null. This projection automatically takes into account\nthe structure of the null hypothesis and allows us to study formal inference\nfor a number of long-standing problems. For example, we can directly conduct\ninference on the sparsity level of the model parameters and the minimum signal\nstrength. This is especially significant given the fact that the former is a\nfundamental condition underlying most of the theoretical development in\nhigh-dimensional statistics, while the latter is a key condition used to\nestablish variable selection properties. Moreover, the proposed method is\nasymptotically exact and has satisfactory power properties for testing very\ngeneral functionals of the high-dimensional parameters. The simulation studies\nlend further support to our theoretical claims and additionally show excellent\nfinite-sample size and power properties of the proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 15:30:54 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1705.01204", "submitter": "Marianna Pensky", "authors": "Marianna Pensky and Teng Zhang", "title": "Spectral clustering in the dynamic stochastic block model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we studied a Dynamic Stochastic Block Model (DSBM)\nunder the assumptions that the connection probabilities, as functions of time,\nare smooth and that at most $s$ nodes can switch their class memberships\nbetween two consecutive time points. We estimate the edge probability tensor by\na kernel-type procedure and extract the group memberships of the nodes by\nspectral clustering. The procedure is computationally viable, adaptive to the\nunknown smoothness of the functional connection probabilities, to the rate $s$\nof membership switching and to the unknown number of clusters. In addition, it\nis accompanied by non-asymptotic guarantees for the precision of estimation and\nclustering.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 23:55:26 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Pensky", "Marianna", ""], ["Zhang", "Teng", ""]]}, {"id": "1705.01216", "submitter": "Dexter Cahoy", "authors": "Dexter Cahoy and Sharifa Minkabo", "title": "Inference for three-parameter M-Wright distributions with applications", "comments": null, "journal-ref": "Model Assisted Statistics and Applications, vol. 12, no. 2, pp.\n  115-125, 2017", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose point estimators for the three-parameter (location, scale, and the\nfractional parameter) variant distributions generated by a Wright function. We\nalso provide uncertainty quantification procedures for the proposed point\nestimators under certain conditions. The class of densities includes the\nthree-parameter one-sided and the three-parameter symmetric bimodal $M$-Wright\nfamily of distributions. The one-sided family naturally generalizes the Airy\nand half-normal models. The symmetric class includes the symmetric Airy and\nnormal or Gaussian densities. The proposed interval estimator for the scale\nparameter outperformed the estimator derived in \\cite{cah12} when the location\nparameter is zero. We obtain the asymptotic covariance structure for the scale\nand fractional parameter estimators, which allows estimation of the\ncorrelation. The coverage probabilities of the interval estimators slightly\ndepend on the proposed location parameter estimators. For the symmetric case,\nthe sample mean (or median) is favored than the median (or mean) when the\nfractional parameter is greater (or lesser) than 0.39106 in terms of their\nasymptotic relative efficiency. The estimation algorithms were tested using\nsynthetic data and were compared with their bootstrap counterparts. The\nproposed inference procedures were demonstrated on age and height data.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 01:24:16 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 19:08:41 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Cahoy", "Dexter", ""], ["Minkabo", "Sharifa", ""]]}, {"id": "1705.01282", "submitter": "Antonio Parisi", "authors": "Antonio Parisi and Brunero Liseo", "title": "Objective Bayesian analysis for the multivariate skew-t model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a Bayesian analysis of the p-variate skew-t model, providing a new\nparameterization, a set of non-informative priors and a sampler specifically\ndesigned to explore the posterior density of the model parameters. Extensions,\nsuch as the multivariate regression model with skewed errors and the stochastic\nfrontiers model, are easily accommodated. A novelty introduced in the paper is\ngiven by the extension of the bivariate skew-normal model given in Liseo &\nParisi (2013) to a more realistic p-variate skew-t model. We also introduce the\nR package mvst, which allows to estimate the multivariate skew-t model.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 07:29:10 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Parisi", "Antonio", ""], ["Liseo", "Brunero", ""]]}, {"id": "1705.01308", "submitter": "Eric Adjakossa", "authors": "Eric Adjakossa (LPMA, UAC), Gr\\'egory Nuel (LPMA)", "title": "Fixed effects selection in the linear mixed-effects model using adaptive\n  ridge procedure for L0 penalty performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the selection of fixed effects along with the\nestimation of fixed effects, random effects and variance components in the\nlinear mixed-effects model. We introduce a selection procedure based on an\nadaptive ridge (AR) penalty of the profiled likelihood, where the covariance\nmatrix of the random effects is Cholesky factorized. This selection procedure\nis intended to both low and high-dimensional settings where the number of fixed\neffects is allowed to grow exponentially with the total sample size, yielding\ntechnical difficulties due to the non-convex optimization problem induced by L0\npenalties. Through extensive simulation studies, the procedure is compared to\nthe LASSO selection and appears to enjoy the model selection consistency as\nwell as the estimation consistency.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 08:51:43 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 12:40:08 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Adjakossa", "Eric", "", "LPMA, UAC"], ["Nuel", "Gr\u00e9gory", "", "LPMA"]]}, {"id": "1705.01340", "submitter": "Fabio Rapallo", "authors": "Fabio Rapallo and Maria Piera Rogantin", "title": "Algebraic characterization of regular fractions under level permutations", "comments": "23 pages, including two appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the behavior of the fractions of a factorial design\nunder permutations of the factor levels. We focus on the notion of regular\nfraction and we introduce methods to check whether a given symmetric orthogonal\narray can or can not be transformed into a regular fraction by means of\nsuitable permutations of the factor levels. The proposed techniques take\nadvantage of the complex coding of the factor levels and of some tools from\npolynomial algebra. Several examples are described, mainly involving factors\nwith five levels.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 10:07:15 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Rapallo", "Fabio", ""], ["Rogantin", "Maria Piera", ""]]}, {"id": "1705.01449", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh", "title": "Robust Inference under the Beta Regression Model with Application to\n  Health Care Studies", "comments": "Author pre-print", "journal-ref": "Statistical Methods in Medical Research, 2017, online published.\n  doi:10.1177/0962280217738142", "doi": "10.1177/0962280217738142", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data on rates, percentages or proportions arise frequently in many different\napplied disciplines like medical biology, health care, psychology and several\nothers. In this paper, we develop a robust inference procedure for the beta\nregression model which is used to describe such response variables taking\nvalues in $(0, 1)$ through some related explanatory variables. In relation to\nthe beta regression model, the issue of robustness has been largely ignored in\nthe literature so far. The existing maximum likelihood based inference has\nserious lack of robustness against outliers in data and generate drastically\ndifferent (erroneous) inference in presence of data contamination. Here, we\ndevelop the robust minimum density power divergence estimator and a class of\nrobust Wald-type tests for the beta regression model along with several\napplications. We derive their asymptotic properties and describe their\nrobustness theoretically through the influence function analyses. Finite sample\nperformances of the proposed estimators and tests are examined through suitable\nsimulation studies and real data applications in the context of health care and\npsychology. Although we primarily focus on the beta regression models with a\nfixed dispersion parameter, some indications are also provided for extension to\nthe variable dispersion beta regression models with an application.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 14:34:48 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 14:38:25 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Ghosh", "Abhik", ""]]}, {"id": "1705.01505", "submitter": "Peter Green", "authors": "Peter J. Green", "title": "Introduction to finite mixtures", "comments": "14 pages, 7 figures, A chapter prepared for the forthcoming Handbook\n  of Mixture Analysis. V2 corrects a small but important typographical error,\n  and makes other minor edits; V3 makes further minor corrections and updates\n  following review; V4 corrects algorithmic details in sec 4.1 and 4.2, and\n  removes typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models have been around for over 150 years, as an intuitively simple\nand practical tool for enriching the collection of probability distributions\navailable for modelling data. In this chapter we describe the basic ideas of\nthe subject, present several alternative representations and perspectives on\nthese models, and discuss some of the elements of inference about the unknowns\nin the models. Our focus is on the simplest set-up, of finite mixture models,\nbut we discuss also how various simplifying assumptions can be relaxed to\ngenerate the rich landscape of modelling and inference ideas traversed in the\nrest of this book.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 16:44:04 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 17:29:17 GMT"}, {"version": "v3", "created": "Sat, 21 Oct 2017 10:33:37 GMT"}, {"version": "v4", "created": "Sat, 5 May 2018 17:53:12 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Green", "Peter J.", ""]]}, {"id": "1705.01654", "submitter": "Andrii Babii", "authors": "Andrii Babii and Jean-Pierre Florens", "title": "Are Unobservables Separable?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to assume in empirical research that observables and\nunobservables are additively separable, especially, when the former are\nendogenous. This is done because it is widely recognized that identification\nand estimation challenges arise when interactions between the two are allowed\nfor. Starting from a nonseparable IV model, where the instrumental variable is\nindependent of unobservables, we develop a novel nonparametric test of\nseparability of unobservables. The large-sample distribution of the test\nstatistics is nonstandard and relies on a novel Donsker-type central limit\ntheorem for the empirical distribution of nonparametric IV residuals, which may\nbe of independent interest. Using a dataset drawn from the 2015 US Consumer\nExpenditure Survey, we find that the test rejects the separability in Engel\ncurves for most of the commodities.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 23:33:34 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 18:22:41 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 23:20:43 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 02:54:33 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Babii", "Andrii", ""], ["Florens", "Jean-Pierre", ""]]}, {"id": "1705.01677", "submitter": "Stefan Wager", "authors": "Guido Imbens and Stefan Wager", "title": "Optimized Regression Discontinuity Designs", "comments": "Review of Economics and Statistics, forthcoming", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of regression discontinuity methods for causal\ninference in observational studies has led to a proliferation of different\nestimating strategies, most of which involve first fitting non-parametric\nregression models on both sides of a treatment assignment boundary and then\nreporting plug-in estimates for the effect of interest. In applications,\nhowever, it is often difficult to tune the non-parametric regressions in a way\nthat is well calibrated for the specific target of inference; for example, the\nmodel with the best global in-sample fit may provide poor estimates of the\ndiscontinuity parameter. In this paper, we propose an alternative method for\nestimation and statistical inference in regression discontinuity designs that\nuses numerical convex optimization to directly obtain the finite-sample-minimax\nlinear estimator for the regression discontinuity parameter, subject to bounds\non the second derivative of the conditional response function. Given a bound on\nthe second derivative, our proposed method is fully data-driven, and provides\nuniform confidence intervals for the regression discontinuity parameter with\nboth discrete and continuous running variables. The method also naturally\nextends to the case of multiple running variables.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 02:26:40 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 00:17:37 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 18:02:26 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Imbens", "Guido", ""], ["Wager", "Stefan", ""]]}, {"id": "1705.01715", "submitter": "Ting Yan", "authors": "Ting Yan", "title": "Directed Networks with a Differentially Private Bi-degree Sequence", "comments": "21 pages, 3 figures, minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a lot of approaches are developed to release network data with a\ndifferentially privacy guarantee, inference using noisy data in many network\nmodels is still unknown or not properly explored. In this paper, we release the\nbi-degree sequences of directed networks using the Laplace mechanism and use\nthe $p_0$ model for inferring the degree parameters. The $p_0$ model is an\nexponential random graph model with the bi-degree sequence as its exclusively\nsufficient statistic. We show that the estimator of the parameter without the\ndenoised process is asymptotically consistent and normally distributed. This is\ncontrast sharply with some known results that valid inference such as the\nexistence and consistency of the estimator needs the denoised process. Along\nthe way, a new phenomenon is revealed in which an additional variance factor\nappears in the asymptotic variance of the estimator when the noise becomes\nlarge. Further, we propose an efficient algorithm for finding the closet point\nlying in the set of all graphical bi-degree sequences under the global $L_1$\noptimization problem. Numerical studies demonstrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 06:47:30 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 09:29:18 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 07:35:26 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 02:16:07 GMT"}, {"version": "v5", "created": "Fri, 29 Nov 2019 03:14:29 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Yan", "Ting", ""]]}, {"id": "1705.01788", "submitter": "Carlos Matr\\'an", "authors": "E. del Barrio, J.A. Cuesta-Albertos and C. Matr\\'an", "title": "An optimal transportation approach for assessing almost stochastic order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When stochastic dominance $F\\leq_{st}G$ does not hold, we can improve\nagreement to stochastic order by suitably trimming both distributions. In this\nwork we consider the $L_2-$Wasserstein distance, $\\mathcal W_2$, to stochastic\norder of these trimmed versions. Our characterization for that distance\nnaturally leads to consider a $\\mathcal W_2$-based index of disagreement with\nstochastic order, $\\varepsilon_{\\mathcal W_2}(F,G)$. We provide asymptotic\nresults allowing to test $H_0: \\varepsilon_{\\mathcal W_2}(F,G)\\geq\n\\varepsilon_0$ vs $H_a: \\varepsilon_{\\mathcal W_2}(F,G)<\\varepsilon_0$, that,\nunder rejection, would give statistical guarantee of almost stochastic\ndominance. We include a simulation study showing a good performance of the\nindex under the normal model.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 10:46:23 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["del Barrio", "E.", ""], ["Cuesta-Albertos", "J. A.", ""], ["Matr\u00e1n", "C.", ""]]}, {"id": "1705.01875", "submitter": "Valery Terebizh", "authors": "V.Yu. Terebizh", "title": "Statistical approach to linear inverse problems", "comments": "21 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:math-ph/0412036", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main features of the statistical approach to inverse problems are\ndescribed on the example of a linear model with additive noise. The approach\ndoes not use any Bayesian hypothesis regarding an unknown object; instead, the\nstandard statistical requirements for the procedure for finding a desired\nobject estimate are presented. In this way, it is possible to obtain stable and\nefficient inverse solutions in the framework of classical statistical theory.\nThe exact representation is given for the feasible region of inverse solutions,\ni.e., the set of inverse estimates that are in agreement, in the statistical\nsense, with the data and available a priory information. The typical feasible\nregion has the form of an extremely elongated hole ellipsoid, the orientation\nand shape of which are determined by the Fisher information matrix. It is the\nspectrum of the Fisher matrix that provides an exhaustive description of the\nstability of the inverse problem under consideration. The method of\nconstructing a nonlinear filter close to the optimal Kolmogorov-Wiener filter\nis presented.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 06:26:38 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Terebizh", "V. Yu.", ""]]}, {"id": "1705.02037", "submitter": "Chul Moon", "authors": "Chul Moon, Noah Giansiracusa, and Nicole A. Lazar", "title": "Persistence Terrace for Topological Inference of Point Cloud Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.AT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Topological data analysis (TDA) is a rapidly developing collection of methods\nfor studying the shape of point cloud and other data types. One popular\napproach, designed to be robust to noise and outliers, is to first use a\nsmoothing function to convert the point cloud into a manifold and then apply\npersistent homology to a Morse filtration. A significant challenge is that this\nsmoothing process involves the choice of a parameter and persistent homology is\nhighly sensitive to that choice; moreover, important scale information is lost.\nWe propose a novel topological summary plot, called a persistence terrace, that\nincorporates a wide range of smoothing parameters and is robust, multi-scale,\nand parameter-free. This plot allows one to isolate distinct topological\nsignals that may have merged for any fixed value of the smoothing parameter,\nand it also allows one to infer the size and point density of the topological\nfeatures. We illustrate our method in some simple settings where noise is a\nserious issue for existing frameworks and then we apply it to a real data set\nby counting muscle fibers in a cross-sectional image.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 22:26:54 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 16:09:28 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Moon", "Chul", ""], ["Giansiracusa", "Noah", ""], ["Lazar", "Nicole A.", ""]]}, {"id": "1705.02069", "submitter": "Jin Xu", "authors": "Jin Xu, Cui Xiong, Rongji Mu", "title": "A Bayesian Stochastic Approximation Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the goal of improving the efficiency of small sample design, we\npropose a novel Bayesian stochastic approximation method to estimate the root\nof a regression function. The method features adaptive local modelling and\nnonrecursive iteration. Strong consistency of the Bayes estimator is obtained.\nSimulation studies show that our method is superior in finite-sample\nperformance to Robbins--Monro type procedures. Extensions to searching for\nextrema and a version of generalized multivariate quantile are presented.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 02:47:17 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Xu", "Jin", ""], ["Xiong", "Cui", ""], ["Mu", "Rongji", ""]]}, {"id": "1705.02344", "submitter": "Jakob Knollm\\\"uller", "authors": "Jakob Knollm\\\"uller, Torsten A. En{\\ss}lin", "title": "Noisy independent component analysis of auto-correlated components", "comments": null, "journal-ref": "Phys. Rev. E 96, 042114 (2017)", "doi": "10.1103/PhysRevE.96.042114", "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an q-bio.QM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for the separation of superimposed, independent,\nauto-correlated components from noisy multi-channel measurement. The presented\nmethod simultaneously reconstructs and separates the components, taking all\nchannels into account and thereby increases the effective signal-to-noise ratio\nconsiderably, allowing separations even in the high noise regime.\nCharacteristics of the measurement instruments can be included, allowing for\napplication in complex measurement situations. Independent posterior samples\ncan be provided, permitting error estimates on all desired quantities. Using\nthe concept of information field theory, the algorithm is not restricted to any\ndimensionality of the underlying space or discretization scheme thereof.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 18:00:04 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 10:17:59 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Knollm\u00fcller", "Jakob", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1705.02372", "submitter": "Zongming Ma", "authors": "Zhuang Ma, Zongming Ma", "title": "Exploration of Large Networks with Covariates via Fast and Universal\n  Latent Space Model Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent space models are effective tools for statistical modeling and\nexploration of network data. These models can effectively model real world\nnetwork characteristics such as degree heterogeneity, transitivity, homophily,\netc. Due to their close connection to generalized linear models, it is also\nnatural to incorporate covariate information in them. The current paper\npresents two universal fitting algorithms for networks with edge covariates:\none based on nuclear norm penalization and the other based on projected\ngradient descent. Both algorithms are motivated by maximizing likelihood for a\nspecial class of inner-product models while working simultaneously for a wide\nrange of different latent space models, such as distance models, which allow\nlatent vectors to affect edge formation in flexible ways. These fitting\nmethods, especially the one based on projected gradient descent, are fast and\nscalable to large networks. We obtain their rates of convergence for both\ninner-product models and beyond. The effectiveness of the modeling approach and\nfitting algorithms is demonstrated on five real world network datasets for\ndifferent statistical tasks, including community detection with and without\nedge covariates, and network assisted learning.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 19:24:52 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 15:33:40 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Ma", "Zhuang", ""], ["Ma", "Zongming", ""]]}, {"id": "1705.02391", "submitter": "Yael Grushka-Cockayne", "authors": "Kenneth C. Lichtendahl Jr., Yael Grushka-Cockayne, Victor Richmond R.\n  Jose, and Robert L. Winkler", "title": "Bayesian Ensembles of Binary-Event Forecasts: When Is It Appropriate to\n  Extremize or Anti-Extremize?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many organizations face critical decisions that rely on forecasts of binary\nevents. In these situations, organizations often gather forecasts from multiple\nexperts or models and average those forecasts to produce a single aggregate\nforecast. Because the average forecast is known to be underconfident, methods\nhave been proposed that create an aggregate forecast more extreme than the\naverage forecast. But is it always appropriate to extremize the average\nforecast? And if not, when is it appropriate to anti-extremize (i.e., to make\nthe aggregate forecast less extreme)? To answer these questions, we introduce a\nclass of optimal aggregators. These aggregators are Bayesian ensembles because\nthey follow from a Bayesian model of the underlying information experts have.\nEach ensemble is a generalized additive model of experts' probabilities that\nfirst transforms the experts' probabilities into their corresponding\ninformation states, then linearly combines these information states, and\nfinally transforms the combined information states back into the probability\nspace. Analytically, we find that these optimal aggregators do not always\nextremize the average forecast, and when they do, they can run counter to\nexisting methods. On two publicly available datasets, we demonstrate that these\nnew ensembles are easily fit to real forecast data and are more accurate than\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 20:39:00 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 12:39:42 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Lichtendahl", "Kenneth C.", "Jr."], ["Grushka-Cockayne", "Yael", ""], ["Jose", "Victor Richmond R.", ""], ["Winkler", "Robert L.", ""]]}, {"id": "1705.02441", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Yinchu Zhu", "title": "Comments on `High-dimensional simultaneous inference with the bootstrap'", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide comments on the article \"High-dimensional simultaneous inference\nwith the bootstrap\" by Ruben Dezeure, Peter Buhlmann and Cun-Hui Zhang.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 04:15:06 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Bradic", "Jelena", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1705.02459", "submitter": "Alexander Luedtke", "authors": "Alexander R. Luedtke, Oleg Sofrygin, Mark J. van der Laan, Marco\n  Carone", "title": "Sequential Double Robustness in Right-Censored Longitudinal Models", "comments": "Version 2 Version 1: May 6, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider estimating the G-formula for the counterfactual mean outcome under a\ngiven treatment regime in a longitudinal study. Bang and Robins provided an\nestimator for this quantity that relies on a sequential regression formulation\nof this parameter. This approach is doubly robust in that it is consistent if\neither the outcome regressions or the treatment mechanisms are consistently\nestimated. We define a stronger notion of double robustness, termed sequential\ndouble robustness, for estimators of the longitudinal G-formula. The definition\nemerges naturally from a more general definition of sequential double\nrobustness for the outcome regression estimators. An outcome regression\nestimator is sequentially doubly robust (SDR) if, at each subsequent time\npoint, either the outcome regression or the treatment mechanism is consistently\nestimated. This form of robustness is exactly what one would anticipate is\nattainable by studying the remainder term of a first-order expansion of the\nG-formula parameter. We show that a particular implementation of an existing\nprocedure is SDR. We also introduce a novel SDR estimator, whose development\ninvolves a novel translation of ideas used in targeted minimum loss-based\nestimation to the infinite-dimensional setting.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 08:06:44 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 02:23:31 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Luedtke", "Alexander R.", ""], ["Sofrygin", "Oleg", ""], ["van der Laan", "Mark J.", ""], ["Carone", "Marco", ""]]}, {"id": "1705.02511", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung, Ying Hung, William Rittase, Cheng Zhu, C. F. Jeff Wu", "title": "A generalized Gaussian process model for computer experiments with\n  binary time series", "comments": "49 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Gaussian observations such as binary responses are common in some\ncomputer experiments. Motivated by the analysis of a class of cell adhesion\nexperiments, we introduce a generalized Gaussian process model for binary\nresponses, which shares some common features with standard GP models. In\naddition, the proposed model incorporates a flexible mean function that can\ncapture different types of time series structures. Asymptotic properties of the\nestimators are derived, and an optimal predictor as well as its predictive\ndistribution are constructed. Their performance is examined via two simulation\nstudies. The methodology is applied to study computer simulations for cell\nadhesion experiments. The fitted model reveals important biological information\nin repeated cell bindings, which is not directly observable in lab experiments.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 18:04:52 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 03:19:33 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 20:36:01 GMT"}, {"version": "v4", "created": "Mon, 24 Sep 2018 20:58:43 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Sung", "Chih-Li", ""], ["Hung", "Ying", ""], ["Rittase", "William", ""], ["Zhu", "Cheng", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1705.02679", "submitter": "Adam Kashlak", "authors": "Adam B Kashlak, Linglong Kong", "title": "Nonasymptotic estimation and support recovery for high dimensional\n  sparse covariance matrices", "comments": "33 pages, 3 figures, 6 tables", "journal-ref": "Stat (2020) e316", "doi": "10.1002/sta4.316", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for nonasymptotic covariance matrix estimation\nmaking use of concentration inequality-based confidence sets. We specify this\nframework for the estimation of large sparse covariance matrices through\nincorporation of past thresholding estimators with key emphasis on support\nrecovery. This technique goes beyond past results for thresholding estimators\nby allowing for a wide range of distributional assumptions beyond merely\nsub-Gaussian tails. This methodology can furthermore be adapted to a wide range\nof other estimators and settings. The usage of nonasymptotic dimension-free\nconfidence sets yields good theoretical performance. Through extensive\nsimulations, it is demonstrated to have superior performance when compared with\nother such methods. In the context of support recovery, we are able to specify\na false positive rate and optimize to maximize the true recoveries.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 18:47:49 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 23:05:09 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 15:56:34 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kashlak", "Adam B", ""], ["Kong", "Linglong", ""]]}, {"id": "1705.02716", "submitter": "Wen-Ting Wang", "authors": "Wen-Ting Wang, Hsin-Cheng Huang", "title": "Regularized Spatial Maximum Covariance Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In climate and atmospheric research, many phenomena involve more than one\nmeteorological spatial processes covarying in space. To understand how one\nprocess is affected by another, maximum covariance analysis (MCA) is commonly\napplied. However, the patterns obtained from MCA may sometimes be difficult to\ninterpret. In this paper, we propose a regularization approach to promote\nspatial features in dominant coupled patterns by introducing smoothness and\nsparseness penalties while accounting for their orthogonalities. We develop an\nefficient algorithm to solve the resulting optimization problem by using the\nalternating direction method of multipliers. The effectiveness of the proposed\nmethod is illustrated by several numerical examples, including an application\nto study how precipitations in east Africa are affected by sea surface\ntemperatures in the Indian Ocean.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 00:48:42 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Wang", "Wen-Ting", ""], ["Huang", "Hsin-Cheng", ""]]}, {"id": "1705.03088", "submitter": "Shrijita Bhattacharya", "authors": "Shrijita Bhattacharya, Michael Kallitsis, Stilian Stoev", "title": "Trimming the Hill estimator: robustness, optimality and adaptivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a trimmed version of the Hill estimator for the index of a\nheavy-tailed distribution, which is robust to perturbations in the extreme\norder statistics. In the ideal Pareto setting, the estimator is essentially\nfinite-sample efficient among all unbiased estimators with a given strict upper\nbreak-down point. For general heavy-tailed models, we establish the asymptotic\nnormality of the estimator under second order conditions and discuss its\nminimax optimal rate in the Hall class. We introduce the so-called trimmed Hill\nplot, which can be used to select the number of top order statistics to trim.\nWe also develop an automatic, data-driven procedure for the choice of trimming.\nThis results in a new type of robust estimator that can {\\em adapt} to the\nunknown level of contamination in the extremes. As a by-product we also obtain\na methodology for identifying extreme outliers in heavy tailed data. The\ncompetitive performance of the trimmed Hill and adaptive trimmed Hill\nestimators is illustrated with simulations.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 21:07:12 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 18:20:17 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bhattacharya", "Shrijita", ""], ["Kallitsis", "Michael", ""], ["Stoev", "Stilian", ""]]}, {"id": "1705.03130", "submitter": "Paul McNicholas", "authors": "Yang Tang, Ryan P. Browne and Paul D. McNicholas", "title": "Flexible Clustering for High-Dimensional Data via Mixtures of Joint\n  Generalized Hyperbolic Models", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.177", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of joint generalized hyperbolic distributions (MJGHD) is introduced\nfor asymmetric clustering for high-dimensional data. The MJGHD approach takes\ninto account the cluster-specific subspace, thereby limiting the number of\nparameters to estimate while also facilitating visualization of results.\nIdentifiability is discussed, and a multi-cycle ECM algorithm is outlined for\nparameter estimation. The MJGHD approach is illustrated on two real data sets,\nwhere the Bayesian information criterion is used for model selection.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 00:25:32 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 18:04:08 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Tang", "Yang", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1705.03134", "submitter": "Paul McNicholas", "authors": "Yang Tang and Paul D. McNicholas", "title": "Clustering Airbnb Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, online customer reviews increasingly exert influence on\nconsumers' decision when booking accommodation online. The renewal importance\nto the concept of word-of mouth is reflected in the growing interests in\ninvestigating consumers' experience by analyzing their online reviews through\nthe process of text mining and sentiment analysis. A clustering approach is\ndeveloped for Boston Airbnb reviews submitted in the English language and\ncollected from 2009 to 2016. This approach is based on a mixture of latent\nvariable models, which provides an appealing framework for handling clustered\nbinary data. We address here the problem of discovering meaningful segments of\nconsumers that are coherent from both the underlying topics and the sentiment\nbehind the reviews. A penalized mixture of latent traits approach is developed\nto reduce the number of parameters and identify variables that are not\ninformative for clustering. The introduction of component-specific rate\nparameters avoids the over-penalization that can occur when inferring a shared\nrate parameter on clustered data. We divided the guests into four groups --\nproperty driven guests, host driven guests, guests with recent overall negative\nstay and guests with some negative experiences.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 00:53:36 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 00:13:54 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 23:34:08 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1705.03422", "submitter": "Rui Tuo", "authors": "Rui Tuo", "title": "Adjustments to Computer Models via Projected Kernel Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of model parameters in computer simulations is an important\ntopic in computer experiments. We propose a new method, called the projected\nkernel calibration method, to estimate these model parameters. The proposed\nmethod is proven to be asymptotic normal and semi-parametric efficient. As a\nfrequentist method, the proposed method is as efficient as the $L_2$\ncalibration method proposed by Tuo and Wu [Ann. Statist. 43 (2015) 2331-2352].\nOn the other hand, the proposed method has a natural Bayesian version, which\nthe $L_2$ method does not have. This Bayesian version allows users to calculate\nthe credible region of the calibration parameters without using a large sample\napproximation. We also show that, the inconsistency problem of the calibration\nmethod proposed by Kennedy and O'Hagan [J. R. Stat. Soc. Ser. B. Stat.\nMethodol. 63 (2001) 425-464] can be rectified by a simple modification of the\nkernel matrix.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 16:49:08 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Tuo", "Rui", ""]]}, {"id": "1705.03566", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Spatial Random Sampling: A Structure-Preserving Data Sketching Tool", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2017.2723472", "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random column sampling is not guaranteed to yield data sketches that preserve\nthe underlying structures of the data and may not sample sufficiently from\nless-populated data clusters. Also, adaptive sampling can often provide\naccurate low rank approximations, yet may fall short of producing descriptive\ndata sketches, especially when the cluster centers are linearly dependent.\nMotivated by that, this paper introduces a novel randomized column sampling\ntool dubbed Spatial Random Sampling (SRS), in which data points are sampled\nbased on their proximity to randomly sampled points on the unit sphere. The\nmost compelling feature of SRS is that the corresponding probability of\nsampling from a given data cluster is proportional to the surface area the\ncluster occupies on the unit sphere, independently from the size of the cluster\npopulation. Although it is fully randomized, SRS is shown to provide\ndescriptive and balanced data representations. The proposed idea addresses a\npressing need in data science and holds potential to inspire many novel\napproaches for analysis of big data.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 23:31:15 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 23:19:02 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1705.03594", "submitter": "Bradley Price", "authors": "Bradley S. Price and Charles J. Geyer and Adam J. Rothman", "title": "Automatic Response Category Combination in Multinomial Logistic\n  Regression", "comments": "19 Pages, 10 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method that simultaneously fits the\nmultinomial logistic regression model and combines subsets of the response\ncategories. The penalty is non differentiable when pairs of columns in the\noptimization variable are equal. This encourages pairwise equality of these\ncolumns in the estimator, which corresponds to response category combination.\nWe use an alternating direction method of multipliers algorithm to compute the\nestimator and we discuss the algorithm's convergence. Prediction and model\nselection are also addressed.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 03:02:21 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Price", "Bradley S.", ""], ["Geyer", "Charles J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1705.03604", "submitter": "Emre Demirkaya", "authors": "Yingying Fan, Emre Demirkaya, Jinchi Lv", "title": "Nonuniformity of P-values Can Occur Early in Diverging Dimensions", "comments": "23 pages including 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the joint significance of covariates is of fundamental importance\nin a wide range of applications. To this end, p-values are frequently employed\nand produced by algorithms that are powered by classical large-sample\nasymptotic theory. It is well known that the conventional p-values in Gaussian\nlinear model are valid even when the dimensionality is a non-vanishing fraction\nof the sample size, but can break down when the design matrix becomes singular\nin higher dimensions or when the error distribution deviates from Gaussianity.\nA natural question is when the conventional p-values in generalized linear\nmodels become invalid in diverging dimensions. We establish that such a\nbreakdown can occur early in nonlinear models. Our theoretical\ncharacterizations are confirmed by simulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 04:47:29 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Fan", "Yingying", ""], ["Demirkaya", "Emre", ""], ["Lv", "Jinchi", ""]]}, {"id": "1705.03655", "submitter": "Matteo Iacopini", "authors": "Roberto Casarin and Matteo Iacopini and Luca Rossini", "title": "Discussion on \"Sparse graphs using exchangeable random measures\" by F.\n  Caron and E. B. Fox", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion on \"Sparse graphs using exchangeable random measures\" by F. Caron\nand E. B. Fox. In this discussion we contribute to the analysis of the GGP\nmodel as compared to the Erd\u007fos-Renyi (ER) and the preferential attachment (AB)\nmodels, using different measures such as number of connected components, global\nclustering coefficient, assortativity coefficient and share of nodes in the\ncore.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 08:24:51 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Casarin", "Roberto", ""], ["Iacopini", "Matteo", ""], ["Rossini", "Luca", ""]]}, {"id": "1705.03659", "submitter": "Luca Rossini", "authors": "Roberto Casarin and Lorenzo Frattarolo and Luca Rossini", "title": "Discussion on \"Random-projection ensemble classification\" by T. Cannings\n  and R. Samworth", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion on \"Random-projection ensemble classification\" by T. Cannings and\nR. Samworth. We believe that the proposed approach can find many applications\nin economics such as credit scoring (e.g. Altman (1968)) and can be extended to\nmore general type of classifiers. In this discussion we would like to draw\nauthors attention to the copula-based discriminant analysis (Han et al. (2013)\nand He et al. (2016)).\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 08:40:18 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Casarin", "Roberto", ""], ["Frattarolo", "Lorenzo", ""], ["Rossini", "Luca", ""]]}, {"id": "1705.03695", "submitter": "Lazhar Benkhelifa", "authors": "Lazhar Benkhelifa", "title": "Log-Lindley generated family of distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generator of univariate continuous distributions, with two additional\nparameters, called the Log-Lindley generated family is introduced. Some special\ndistributions in the new family are presented. Some mathematical properties of\nthe new family are studied. The maximum likelihood method to estimate model\nparameters is employed. The potentiality of the new generator is illustrated\nusing a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 10:50:44 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Benkhelifa", "Lazhar", ""]]}, {"id": "1705.03727", "submitter": "Christian P. Robert", "authors": "Gilles Celeux (Inria Saclay-Ile-de-France), Jack Jewson (University of\n  Warwick), Julie Josse (Ecole Polytechnique), Jean-Michel Marin (Universite de\n  Montpellier) and Christian P. Robert (Universite Paris Dauphine)", "title": "Some discussions on the Read Paper \"Beyond subjective and objective in\n  statistics\" by A. Gelman and C. Hennig", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is a collection of several discussions of the paper \"Beyond\nsubjective and objective in statistics\", read by A. Gelman and C. Hennig to the\nRoyal Statistical Society on April 12, 2017, and to appear in the Journal of\nthe Royal Statistical Society, Series A.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 12:35:04 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Celeux", "Gilles", "", "Inria Saclay-Ile-de-France"], ["Jewson", "Jack", "", "University of\n  Warwick"], ["Josse", "Julie", "", "Ecole Polytechnique"], ["Marin", "Jean-Michel", "", "Universite de\n  Montpellier"], ["Robert", "Christian P.", "", "Universite Paris Dauphine"]]}, {"id": "1705.03827", "submitter": "Daniela Marella", "authors": "Pier Luigi Conti and Daniela Marella and Fulvia Mecatti and Federico\n  Andreis", "title": "A unified principled framework for resampling based on\n  pseudo-populations: asymptotic theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a class of resampling techniques for finite populations under\ncomplex sampling design is introduced. The basic idea on which it rests is a\ntwo-step procedure consisting in : (i) constructing a pseudo-population on the\nbasis of sample data; (ii) drawing a sample from the predicted population\naccording to an appropriate resampling design. From a logical point of view,\nthis approach is essentially based on the plug-in principle by Efron, at the\n\"sampling design level\". Theoretical justifications based on large sample\ntheory are provided. New approaches to construct pseudo-populations based on\nvarious forms of calibrations are proposed. Finally, a simulation study is\nperformed.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:47:08 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 13:05:34 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Conti", "Pier Luigi", ""], ["Marella", "Daniela", ""], ["Mecatti", "Fulvia", ""], ["Andreis", "Federico", ""]]}, {"id": "1705.03864", "submitter": "Daniele Durante", "authors": "Daniele Durante, Antonio Canale and Tommaso Rigon", "title": "A nested expectation-maximization algorithm for latent class models with\n  covariates", "comments": null, "journal-ref": "Statistics & Probability Letters (2019). 146, 97-103", "doi": "10.1016/j.spl.2018.10.015", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a nested EM routine for latent class models with covariates which\nallows maximization of the full-model log-likelihood and, differently from\ncurrent methods, guarantees monotone log-likelihood sequences along with\nimproved convergence rates.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 17:29:10 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 15:27:13 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 17:19:16 GMT"}, {"version": "v4", "created": "Tue, 16 Jan 2018 17:58:27 GMT"}, {"version": "v5", "created": "Thu, 2 Aug 2018 14:37:31 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Durante", "Daniele", ""], ["Canale", "Antonio", ""], ["Rigon", "Tommaso", ""]]}, {"id": "1705.03918", "submitter": "Raiden Hasegawa", "authors": "Raiden B. Hasegawa, Sameer K. Deshpande, Dylan S. Small, Paul R.\n  Rosenbaum", "title": "Causal Inference with Two Versions of Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal effects are commonly defined as comparisons of the potential outcomes\nunder treatment and control, but this definition is threatened by the\npossibility that the treatment or control condition is not well-defined,\nexisting instead in more than one version. A simple, widely applicable analysis\nis proposed to address the possibility that the treatment or control condition\nexists in two versions with two different treatment effects. This analysis\nloses no power in the main comparison of treatment and control, provides\nadditional information about version effects, and controls the family-wise\nerror rate in several comparisons. The method is motivated and illustrated\nusing an on-going study of the possibility that repeated head trauma in high\nschool football causes an increase in risk of early on-set dementia.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 18:41:07 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 20:33:33 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Hasegawa", "Raiden B.", ""], ["Deshpande", "Sameer K.", ""], ["Small", "Dylan S.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1705.03983", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Shuheng Zhou, Alfred Hero III", "title": "Tensor Graphical Lasso (TeraLasso)", "comments": "accepted to JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a multi-way tensor generalization of the Bigraphical\nLasso (BiGLasso), which uses a two-way sparse Kronecker-sum multivariate-normal\nmodel for the precision matrix to parsimoniously model conditional dependence\nrelationships of matrix-variate data based on the Cartesian product of graphs.\nWe call this generalization the {\\bf Te}nsor g{\\bf ra}phical Lasso (TeraLasso).\nWe demonstrate using theory and examples that the TeraLasso model can be\naccurately and scalably estimated from very limited data samples of high\ndimensional variables with multiway coordinates such as space, time and\nreplicates. Statistical consistency and statistical rates of convergence are\nestablished for both the BiGLasso and TeraLasso estimators of the precision\nmatrix and estimators of its support (non-sparsity) set, respectively. We\npropose a scalable composite gradient descent algorithm and analyze the\ncomputational convergence rate, showing that the composite gradient descent\nalgorithm is guaranteed to converge at a geometric rate to the global minimizer\nof the TeraLasso objective function. Finally, we illustrate the TeraLasso using\nboth simulation and experimental data from a meteorological dataset, showing\nthat we can accurately estimate precision matrices and recover meaningful\nconditional dependency graphs from high dimensional complex datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 01:33:37 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 03:48:33 GMT"}, {"version": "v3", "created": "Fri, 2 Jun 2017 03:03:39 GMT"}, {"version": "v4", "created": "Mon, 23 Sep 2019 01:08:11 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Zhou", "Shuheng", ""], ["Hero", "Alfred", "III"]]}, {"id": "1705.04136", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Tatsuya Kubokawa", "title": "Adaptively Transformed Mixed Model Prediction of General Finite\n  Population Parameters", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For estimating area-specific parameters (quantities) in a finite population,\na mixed model prediction approach is attractive. However, this approach\nstrongly depends on the normality assumption of the response values although we\noften encounter a non-normal case in practice. In such a case, transforming\nobservations to make them suitable for normality assumption is a useful tool,\nbut the problem of selecting suitable transformation still remains open. To\novercome the difficulty, we here propose a new empirical best predicting method\nby using a parametric family of transformations to estimate a suitable\ntransformation based on the data. We suggest a simple estimating method for\ntransformation parameters based on the profile likelihood function, which\nachieves consistency under some conditions on transformation functions. For\nmeasuring variability of point prediction, we construct an empirical Bayes\nconfidence interval of the population parameter of interest. Through simulation\nstudies, we investigate numerical performance of the proposed methods. Finally,\nwe apply the proposed method to synthetic income data in Spanish provinces in\nwhich the resulting estimates indicate that the commonly used\nlog-transformation would not be appropriate.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 12:31:57 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 03:35:41 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 07:56:23 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1705.04141", "submitter": "Nicholas Polson Prof", "authors": "Nozer D. Singpurwalla, Nicholas G. Polson and Refik Soyer", "title": "From Least Squares to Signal Processing and Particle Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De Facto, signal processing is the interpolation and extrapolation of a\nsequence of observations viewed as a realization of a stochastic process. Its\nrole in applied statistics ranges from scenarios in forecasting and time series\nanalysis, to image reconstruction, machine learning, and the degradation\nmodeling for reliability assessment. A general solution to the problem of\nfiltering and prediction entails some formidable mathematics. Efforts to\ncircumvent the mathematics has resulted in the need for introducing more\nexplicit descriptions of the underlying process. One such example, and a\nnoteworthy one, is the Kalman Filter Model, which is a special case of state\nspace models or what statisticians refer to as Dynamic Linear Models.\nImplementing the Kalman Filter Model in the era of \"big and high velocity\nnon-Gaussian data\" can pose computational challenges with respect to efficiency\nand timeliness. Particle filtering is a way to ease such computational burdens.\nThe purpose of this paper is to trace the historical evolution of this\ndevelopment from its inception to its current state, with an expository focus\non two versions of the particle filter, namely, the propagate first-update next\nand the update first-propagate next version. By way of going beyond a pure\nreview, this paper also makes transparent the importance and the role of a less\nrecognized principle, namely the principle of conditionalization, in filtering\nand prediction based on Bayesian methods. Furthermore, the paper also\narticulates the philosophical underpinnings of the filtering and prediction\nset-up, a matter that needs to ne made explicit, and Yule's decomposition of a\nrandom variable in terms of a sequence of innovations.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 12:58:10 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Singpurwalla", "Nozer D.", ""], ["Polson", "Nicholas G.", ""], ["Soyer", "Refik", ""]]}, {"id": "1705.04219", "submitter": "Pierre Carmier Ph.D.", "authors": "Pierre Carmier, Olexiy Kyrgyzov, Paul-Henry Courn\\`ede", "title": "A critical analysis of resampling strategies for the regularized\n  particle filter", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the performance of different resampling strategies for the\nregularized particle filter regarding parameter estimation. We show in\nparticular, building on analytical insight obtained in the linear Gaussian\ncase, that resampling systematically can prevent the filtered density from\nconverging towards the true posterior distribution. We discuss several means to\novercome this limitation, including kernel bandwidth modulation, and provide\nevidence that the resulting particle filter clearly outperforms traditional\nbootstrap particle filters. Our results are supported by numerical simulations\non a linear textbook example, the logistic map and a non-linear plant growth\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 14:47:30 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Carmier", "Pierre", ""], ["Kyrgyzov", "Olexiy", ""], ["Courn\u00e8de", "Paul-Henry", ""]]}, {"id": "1705.04312", "submitter": "Alexej Gossmann", "authors": "Alexej Gossmann, Pascal Zille, Vince Calhoun, and Yu-Ping Wang", "title": "FDR-Corrected Sparse Canonical Correlation Analysis with Applications to\n  Imaging Genomics", "comments": "- Clarification of the definition of FDR for CCA in Section III;\n  results unchanged. - Corrected typos. - Added IEEE copyright notice for the\n  accepted article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the number of false discoveries is presently one of the most\npressing issues in the life sciences. It is of especially great importance for\nmany applications in neuroimaging and genomics, where datasets are typically\nhigh-dimensional, which means that the number of explanatory variables exceeds\nthe sample size. The false discovery rate (FDR) is a criterion that can be\nemployed to address that issue. Thus it has gained great popularity as a tool\nfor testing multiple hypotheses. Canonical correlation analysis (CCA) is a\nstatistical technique that is used to make sense of the cross-correlation of\ntwo sets of measurements collected on the same set of samples (e.g., brain\nimaging and genomic data for the same mental illness patients), and sparse CCA\nextends the classical method to high-dimensional settings. Here we propose a\nway of applying the FDR concept to sparse CCA, and a method to control the FDR.\nThe proposed FDR correction directly influences the sparsity of the solution,\nadapting it to the unknown true sparsity level. Theoretical derivation as well\nas simulation studies show that our procedure indeed keeps the FDR of the\ncanonical vectors below a user-specified target level. We apply the proposed\nmethod to an imaging genomics dataset from the Philadelphia Neurodevelopmental\nCohort. Our results link the brain connectivity profiles derived from brain\nactivity during an emotion identification task, as measured by functional\nmagnetic resonance imaging (fMRI), to the corresponding subjects' genomic data.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:57:40 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 01:28:42 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 00:12:42 GMT"}, {"version": "v4", "created": "Sat, 23 Jun 2018 04:45:44 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gossmann", "Alexej", ""], ["Zille", "Pascal", ""], ["Calhoun", "Vince", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1705.04366", "submitter": "Fang Liu", "authors": "Fang Liu", "title": "Assessment of Bayesian Expected Power via Bayesian Bootstrap", "comments": null, "journal-ref": "Statistics in Medicine 2018 Oct 30;37(24):3471-3485", "doi": "10.1002/sim.7826", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian expected power (BEP) has become increasingly popular in sample\nsize determination and assessment of the probability of success (POS) for a\nfuture trial. The BEP takes into consideration the uncertainty around the\nparameters assumed by a power analysis and is thus more robust compared to the\ntraditional power that assumes a single set of parameters. Current methods for\nassessing BEP are often based in a parametric framework by imposing a model on\nthe pilot data to derive and sample from the posterior distributions of the\nparameters. Implementation of the model-based approaches can be analytically\nchallenging and computationally costly especially for multivariate data sets;\nit also runs the risk of generating misleading BEP if the model is\nmis-specified. We propose an approach based on the Bayesian bootstrap technique\n(BBS) to simulate future trials in the presence of individual-level pilot data,\nbased on which the empirical BEP can be calculated. The BBS approach is\nmodel-free with no assumptions about the distribution of the prior data and\ncircumvents the analytical and computational complexity associated with\nobtaining the posterior distribution of the parameters. Information from\nmultiple pilot studies is also straightforward to combine. We also propose the\ndouble bootstrap (BS2), a frequentist counterpart to the BBS, that shares\nsimilar properties and achieves the same goal as the BBS for BEP assessment.\nSimulation studies and case studies are presented to demonstrate the\nimplementation of the BBS and BS2 techniques and to compare the BEP results\nwith model-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 19:38:14 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Liu", "Fang", ""]]}, {"id": "1705.04436", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee, Jaeyong Lee and Sarat C. Dass", "title": "Inference for Differential Equation Models using Relaxation via\n  Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical regression models whose mean functions are represented by\nordinary differential equations (ODEs) can be used to describe phenomenons\ndynamical in nature, which are abundant in areas such as biology, climatology\nand genetics. The estimation of parameters of ODE based models is essential for\nunderstanding its dynamics, but the lack of an analytical solution of the ODE\nmakes the parameter estimation challenging. The aim of this paper is to propose\na general and fast framework of statistical inference for ODE based models by\nrelaxation of the underlying ODE system. Relaxation is achieved by a properly\nchosen numerical procedure, such as the Runge-Kutta, and by introducing\nadditive Gaussian noises with small variances. Consequently, filtering methods\ncan be applied to obtain the posterior distribution of the parameters in the\nBayesian framework. The main advantage of the proposed method is computation\nspeed. In a simulation study, the proposed method was at least 14 times faster\nthan the other methods. Theoretical results which guarantee the convergence of\nthe posterior of the approximated dynamical system to the posterior of true\nmodel are presented. Explicit expressions are given that relate the order and\nthe mesh size of the Runge-Kutta procedure to the rate of convergence of the\napproximated posterior as a function of sample size.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 03:52:15 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Lee", "Jaeyong", ""], ["Dass", "Sarat C.", ""]]}, {"id": "1705.04506", "submitter": "Ian White", "authors": "Ian R. White and Royes Joseph and Nicky Best", "title": "A causal modelling framework for reference-based imputation and tipping\n  point analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the \"de facto\" or effectiveness estimand in a\nrandomised placebo-controlled or standard-of-care-controlled drug trial with\nquantitative outcome, where participants who discontinue an investigational\ntreatment are not followed up thereafter. Carpenter et al (2013) proposed\nreference-based imputation methods which use a reference arm to inform the\ndistribution of post-discontinuation outcomes and hence to inform an imputation\nmodel. However, the reference-based imputation methods were not formally\njustified. We present a causal model which makes an explicit assumption in a\npotential outcomes framework about the maintained causal effect of treatment\nafter discontinuation. We show that the \"jump to reference\", \"copy reference\"\nand \"copy increments in reference\" reference-based imputation methods, with the\ncontrol arm as the reference arm, are special cases of the causal model with\nspecific assumptions about the causal treatment effect. Results from simulation\nstudies are presented. We also show that the causal model provides a flexible\nand transparent framework for a tipping point sensitivity analysis in which we\nvary the assumptions made about the causal effect of discontinued treatment. We\nillustrate the approach with data from two longitudinal clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 10:52:35 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["White", "Ian R.", ""], ["Joseph", "Royes", ""], ["Best", "Nicky", ""]]}, {"id": "1705.04518", "submitter": "Patrick Rubin-Delanchy Dr", "authors": "Patrick Rubin-Delanchy, Carey E. Priebe and Minh Tang", "title": "Consistency of adjacency spectral embedding for the mixed membership\n  stochastic blockmodel", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixed membership stochastic blockmodel is a statistical model for a\ngraph, which extends the stochastic blockmodel by allowing every node to\nrandomly choose a different community each time a decision of whether to form\nan edge is made. Whereas spectral analysis for the stochastic blockmodel is\nincreasingly well established, theory for the mixed membership case is\nconsiderably less developed. Here we show that adjacency spectral embedding\ninto $\\mathbb{R}^k$, followed by fitting the minimum volume enclosing convex\n$k$-polytope to the $k-1$ principal components, leads to a consistent estimate\nof a $k$-community mixed membership stochastic blockmodel. The key is to\nidentify a direct correspondence between the mixed membership stochastic\nblockmodel and the random dot product graph, which greatly facilitates\ntheoretical analysis. Specifically, a $2 \\rightarrow \\infty$ norm and central\nlimit theorem for the random dot product graph are exploited to respectively\nshow consistency and partially correct the bias of the procedure.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 11:33:30 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Rubin-Delanchy", "Patrick", ""], ["Priebe", "Carey E.", ""], ["Tang", "Minh", ""]]}, {"id": "1705.04557", "submitter": "Andreas Jakobsson", "authors": "Shiwen Lei, Andreas Jakobsson, Zhiqin Zhao", "title": "CFAR Adaptive Matched Detector for Target Detection in Non-Gaussian\n  Noise With Inverse Gamma Texture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an adaptive matched detector of a signal corrupted\nby a non-Gaussian noise with an inverse gamma texture. The detector is formed\nusing a set of secondary data measurements, and is analytically shown to have a\nconstant false alarm rate. The analytic performance is validated using Monte\nCarlo simulations, and the proposed detector is shown to offer preferable\nperformance as compared to the related one-step generalized likelihood ratio\ntest (1S-GLRT) and the adaptive subspace detector (ASD).\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 13:10:57 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Lei", "Shiwen", ""], ["Jakobsson", "Andreas", ""], ["Zhao", "Zhiqin", ""]]}, {"id": "1705.04765", "submitter": "Matthew Masten", "authors": "Matthew A. Masten and Alexandre Poirier", "title": "Inference on Breakdown Frontiers", "comments": "65 pages. 26 page supplemental appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of baseline assumptions, a breakdown frontier is the boundary\nbetween the set of assumptions which lead to a specific conclusion and those\nwhich do not. In a potential outcomes model with a binary treatment, we\nconsider two conclusions: First, that ATE is at least a specific value (e.g.,\nnonnegative) and second that the proportion of units who benefit from treatment\nis at least a specific value (e.g., at least 50\\%). For these conclusions, we\nderive the breakdown frontier for two kinds of assumptions: one which indexes\nrelaxations of the baseline random assignment of treatment assumption, and one\nwhich indexes relaxations of the baseline rank invariance assumption. These\nclasses of assumptions nest both the point identifying assumptions of random\nassignment and rank invariance and the opposite end of no constraints on\ntreatment selection or the dependence structure between potential outcomes.\nThis frontier provides a quantitative measure of robustness of conclusions to\nrelaxations of the baseline point identifying assumptions. We derive\n$\\sqrt{N}$-consistent sample analog estimators for these frontiers. We then\nprovide two asymptotically valid bootstrap procedures for constructing lower\nuniform confidence bands for the breakdown frontier. As a measure of\nrobustness, estimated breakdown frontiers and their corresponding confidence\nbands can be presented alongside traditional point estimates and confidence\nintervals obtained under point identifying assumptions. We illustrate this\napproach in an empirical application to the effect of child soldiering on\nwages. We find that sufficiently weak conclusions are robust to simultaneous\nfailures of rank invariance and random assignment, while some stronger\nconclusions are fairly robust to failures of rank invariance but not\nnecessarily to relaxations of random assignment.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 22:49:27 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 18:56:34 GMT"}, {"version": "v3", "created": "Sat, 2 Feb 2019 22:34:13 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Masten", "Matthew A.", ""], ["Poirier", "Alexandre", ""]]}, {"id": "1705.04784", "submitter": "Jianfeng Yao", "authors": "Weiming Li and Jianfeng Yao", "title": "On structure testing for component covariance matrices of a\n  high-dimensional mixture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By studying the family of $p$-dimensional scale mixtures, this paper shows\nfor the first time a non trivial example where the eigenvalue distribution of\nthe corresponding sample covariance matrix {\\em does not converge} to the\ncelebrated Mar\\v{c}enko-Pastur law. A different and new limit is found and\ncharacterized. The reasons of failure of the Mar\\v{c}enko-Pastur limit in this\nsituation are found to be a strong dependence between the $p$-coordinates of\nthe mixture. Next, we address the problem of testing whether the mixture has a\nspherical covariance matrix. To analize the traditional John's type test we\nestablish a novel and general CLT for linear statistics of eigenvalues of the\nsample covariance matrix. It is shown that the John's test and its recent\nhigh-dimensional extensions both fail for high-dimensional mixtures, precisely\ndue to the different spectral limit above. As a remedy, a new test procedure is\nconstructed afterwards for the sphericity hypothesis. This test is then applied\nto identify the covariance structure in model-based clustering. It is shown\nthat the test has much higher power than the widely used ICL and BIC criteria\nin detecting non spherical component covariance matrices of a high-dimensional\nmixture.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 03:27:08 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Weiming", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1705.04821", "submitter": "Shibin Zhang", "authors": "Shibin Zhang and Xin M. Tu", "title": "Tests for comparing time-invariant and time-varying spectra based on the\n  Anderson-Darling statistic", "comments": "23 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on periodogram-ratios of two univariate time series at different\nfrequency points, two tests are proposed for comparing their spectra. One is an\nAnderson-Darling-like statistic for testing the equality of two time-invariant\nspectra. The other is the maximum of Anderson-Darling-like statistics for\ntesting the equality of two spectra no matter that they are time-invariant and\ntime-varying. Both of two tests are applicable for independent or dependent\ntime series. Several simulation examples show that the proposed statistics\noutperform those that are also based on periodogram-ratios but constructed by\nthe Pearson-like statistics.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 13:31:49 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 13:53:26 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 00:33:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhang", "Shibin", ""], ["Tu", "Xin M.", ""]]}, {"id": "1705.04854", "submitter": "Anna Freni-Sterrantino", "authors": "Anna Freni-Sterrantino and Massimo Ventrucci and H{\\aa}vard Rue", "title": "A note on intrinsic Conditional Autoregressive models for disconnected\n  graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we discuss (Gaussian) intrinsic conditional autoregressive (CAR)\nmodels for disconnected graphs, with the aim of providing practical guidelines\nfor how these models should be defined, scaled and implemented. We show how\nthese suggestions can be implemented in two examples on disease mapping.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 16:50:26 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Freni-Sterrantino", "Anna", ""], ["Ventrucci", "Massimo", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1705.05125", "submitter": "Jian Shi", "authors": "Z. Wang, J. Q. Shi, and Y. Lee", "title": "Extended T-process Regression Models", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression (GPR) model has been widely used to fit data when\nthe regression function is unknown and its nice properties have been well\nestablished. In this article, we introduce an extended t-process regression\n(eTPR) model, a nonlinear model which allows a robust best linear unbiased\npredictor (BLUP). Owing to its succinct construction, it inherits many\nattractive properties from the GPR model, such as having closed forms of\nmarginal and predictive distributions to give an explicit form for robust\nprocedures, and easy to cope with large dimensional covariates with an\nefficient implementation. Properties of the robustness are studied. Simulation\nstudies and real data applications show that the eTPR model gives a robust fit\nin the presence of outliers in both input and output spaces and has a good\nperformance in prediction, compared with other existed methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 09:13:34 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Wang", "Z.", ""], ["Shi", "J. Q.", ""], ["Lee", "Y.", ""]]}, {"id": "1705.05181", "submitter": "Ilaria Bianchini", "authors": "Ilaria Bianchini, Alessandra Guglielmi and Fernando A. Quintana", "title": "Determinantal point process mixtures via spectral density approach", "comments": "42 pages (including Supplementary Material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider mixture models where location parameters are a priori encouraged\nto be well separated. We explore a class of determinantal point process (DPP)\nmixture models, which provide the desired notion of separation or repulsion.\nInstead of using the rather restrictive case where analytical results are\navailable, we adopt a spectral representation from which approximations to the\nDPP intensity functions can be readily computed. For the sake of concreteness\nthe presentation focuses on a power exponential spectral density, but the\nproposed approach is in fact quite general. We later extend our model to\nincorporate covariate information in the likelihood and also in the assignment\nto mixture components, yielding a trade-off between repulsiveness of locations\nin the mixtures and attraction among subjects with similar covariates. We\ndevelop full Bayesian inference, and explore model properties and posterior\nbehavior using several simulation scenarios and data illustrations.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 12:23:15 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Bianchini", "Ilaria", ""], ["Guglielmi", "Alessandra", ""], ["Quintana", "Fernando A.", ""]]}, {"id": "1705.05265", "submitter": "Seyoung Park", "authors": "Seyoung Park, Kerby Shedden, and Shuheng Zhou", "title": "Non-separable covariance models for spatio-temporal data, with\n  applications to neural encoding analysis", "comments": "48 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural encoding studies explore the relationships between measurements of\nneural activity and measurements of a behavior that is viewed as a response to\nthat activity. The coupling between neural and behavioral measurements is\ntypically imperfect and difficult to measure.To enhance our ability to\nunderstand neural encoding relationships, we propose that a behavioral\nmeasurement may be decomposable as a sum of two latent components, such that\nthe direct neural influence and prediction is primarily localized to the\ncomponent which encodes temporal dependence. For this purpose, we propose to\nuse a non-separable Kronecker sum covariance model to characterize the\nbehavioral data as the sum of terms with exclusively trial-wise, and\nexclusively temporal dependencies. We then utilize a corrected form of Lasso\nregression in combination with the nodewise regression approach for estimating\nthe conditional independence relationships between and among variables for each\ncomponent of the behavioral data, where normality is necessarily assumed. We\nprovide the rate of convergence for estimating the precision matrices\nassociated with the temporal as well as spatial components in the Kronecker sum\nmodel. We illustrate our methods and theory using simulated data, and data from\na neural encoding study of hawkmoth flight; we demonstrate that the neural\nencoding signal for hawkmoth wing strokes is primarily localized to a latent\ncomponent with temporal dependence, which is partially obscured by a second\ncomponent with trial-wise dependencies.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:26:53 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Park", "Seyoung", ""], ["Shedden", "Kerby", ""], ["Zhou", "Shuheng", ""]]}, {"id": "1705.05391", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Aaditya Ramdas, Michael I. Jordan, Martin J.\n  Wainwright", "title": "Optimal Rates and Tradeoffs in Multiple Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a central topic in statistics, but despite\nabundant work on the false discovery rate (FDR) and the corresponding Type-II\nerror concept known as the false non-discovery rate (FNR), a fine-grained\nunderstanding of the fundamental limits of multiple testing has not been\ndeveloped. Our main contribution is to derive a precise non-asymptotic tradeoff\nbetween FNR and FDR for a variant of the generalized Gaussian sequence model.\nOur analysis is flexible enough to permit analyses of settings where the\nproblem parameters vary with the number of hypotheses $n$, including various\nsparse and dense regimes (with $o(n)$ and $\\mathcal{O}(n)$ signals). Moreover,\nwe prove that the Benjamini-Hochberg algorithm as well as the Barber-Cand\\`{e}s\nalgorithm are both rate-optimal up to constants across these regimes.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 18:00:25 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Ramdas", "Aaditya", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1705.05431", "submitter": "Thomas Nagler", "authors": "Thomas Nagler", "title": "Asymptotic analysis of the jittering kernel density estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jittering estimators are nonparametric function estimators for mixed data.\nThey extend arbitrary estimators from the continuous setting by adding random\nnoise to discrete variables. We give an in-depth analysis of the jittering\nkernel density estimator, which reveals several appealing properties. The\nestimator is strongly consistent, asymptotically normal, and unbiased for\ndiscrete variables. It converges at minimax-optimal rates, which are\nestablished as a by-product of our analysis. To understand the effect of adding\nnoise, we further study its asymptotic efficiency and finite sample bias in the\nunivariate discrete case. Simulations show that the estimator is competitive on\nfinite samples. The analysis suggests that similar properties can be expected\nfor other jittering estimators.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 20:29:39 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 17:18:32 GMT"}, {"version": "v3", "created": "Sat, 11 Nov 2017 13:42:12 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Nagler", "Thomas", ""]]}, {"id": "1705.05543", "submitter": "Sen Zhao", "authors": "Sen Zhao, Daniela Witten, Ali Shojaie", "title": "In Defense of the Indefensible: A Very Naive Approach to\n  High-Dimensional Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great deal of interest has recently focused on conducting inference on the\nparameters in a high-dimensional linear model.\n  In this paper, we consider a simple and very na\\\"{i}ve two-step procedure for\nthis task, in which we (i) fit a lasso model in order to obtain a subset of the\nvariables, and (ii) fit a least squares model on the lasso-selected set.\nConventional statistical wisdom tells us that we cannot make use of the\nstandard statistical inference tools for the resulting least squares model\n(such as confidence intervals and $p$-values), since we peeked at the data\ntwice: once in running the lasso, and again in fitting the least squares model.\nHowever, in this paper, we show that under a certain set of assumptions, with\nhigh probability, the set of variables selected by the lasso is identical to\nthe one selected by the noiseless lasso and is hence deterministic.\nConsequently, the na\\\"{i}ve two-step approach can yield asymptotically valid\ninference. We utilize this finding to develop the \\emph{na\\\"ive confidence\ninterval}, which can be used to draw inference on the regression coefficients\nof the model selected by the lasso, as well as the \\emph{na\\\"ive score test},\nwhich can be used to test the hypotheses regarding the full-model regression\ncoefficients.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 06:05:18 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 21:06:24 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 03:16:30 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Zhao", "Sen", ""], ["Witten", "Daniela", ""], ["Shojaie", "Ali", ""]]}, {"id": "1705.05618", "submitter": "Jian Shi", "authors": "Chunzheng Cao and Jian Qing Shi and Youngjo Lee", "title": "Robust functional regression model for marginal mean and\n  subject-specific inferences", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce flexible robust functional regression models, using various\nheavy-tailed processes, including a Student $t$-process. We propose efficient\nalgorithms in estimating parameters for the marginal mean inferences and in\npredicting conditional means as well interpolation and extrapolation for the\nsubject-specific inferences. We develop bootstrap prediction intervals for\nconditional mean curves. Numerical studies show that the proposed model\nprovides robust analysis against data contamination or distribution\nmisspecification, and the proposed prediction intervals maintain the nominal\nconfidence levels. A real data application is presented as an illustrative\nexample.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 09:52:35 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Cao", "Chunzheng", ""], ["Shi", "Jian Qing", ""], ["Lee", "Youngjo", ""]]}, {"id": "1705.05677", "submitter": "Pierre-Andr\\'e Maugis", "authors": "Pierre-Andr\\'e G. Maugis and Sofia C. Olhede and Patrick J. Wolfe", "title": "Topology reveals universal features for network comparison", "comments": "95 pages, 10 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topology of any complex system is key to understanding its structure and\nfunction. Fundamentally, algebraic topology guarantees that any system\nrepresented by a network can be understood through its closed paths. The length\nof each path provides a notion of scale, which is vitally important in\ncharacterizing dominant modes of system behavior. Here, by combining topology\nwith scale, we prove the existence of universal features which reveal the\ndominant scales of any network. We use these features to compare several\ncanonical network types in the context of a social media discussion which\nevolves through the sharing of rumors, leaks and other news. Our analysis\nenables for the first time a universal understanding of the balance between\nloops and tree-like structure across network scales, and an assessment of how\nthis balance interacts with the spreading of information online. Crucially, our\nresults allow networks to be quantified and compared in a purely model-free way\nthat is theoretically sound, fully automated, and inherently scalable.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:28:13 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Maugis", "Pierre-Andr\u00e9 G.", ""], ["Olhede", "Sofia C.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1705.05752", "submitter": "Guillaume Basse", "authors": "Guillaume Basse and Edoardo Airoldi", "title": "Limitations of design-based causal inference and A/B testing under\n  arbitrary and network interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments on a network often involve interference between\nconnected units; i.e., a situation in which an individual's treatment can\naffect the response of another individual. Current approaches to deal with\ninterference, in theory and in practice, often make restrictive assumptions on\nits structure---for instance, assuming that interference is local---even when\nusing otherwise nonparametric inference strategies. This reliance on explicit\nrestrictions on the interference mechanism suggests a shared intuition that\ninference is impossible without any assumptions on the interference structure.\nIn this paper, we begin by formalizing this intuition in the context of a\nclassical nonparametric approach to inference, referred to as design-based\ninference of causal effects. Next, we show how, always in the context of\ndesign-based inference, even parametric structural assumptions that allow the\nexistence of unbiased estimators, cannot guarantee a decreasing variance even\nin the large sample limit. This lack of concentration in large samples is often\nobserved empirically, in randomized experiments in which interference of some\nform is expected to be present. This result has direct consequences for the\ndesign and analysis of large experiments---for instance, in online social\nplatforms---where the belief is that large sample sizes automatically guarantee\nsmall variance. More broadly, our results suggest that although strategies for\ncausal inference in the presence of interference borrow their formalism and\nmain concepts from the traditional causal inference literature, much of the\nintuition from the no-interference case do not easily transfer to the\ninterference setting.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:07:51 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Basse", "Guillaume", ""], ["Airoldi", "Edoardo", ""]]}, {"id": "1705.05877", "submitter": "Dominik M\\\"uller", "authors": "Dominik M\\\"uller, Claudia Czado", "title": "Selection of Sparse Vine Copulas in High Dimensions with the Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel structure selection method for high dimensional (d > 100)\nsparse vine copulas. Current sequential greedy approaches for structure\nselection require calculating spanning trees in hundreds of dimensions and\nfitting the pair copulas and their parameters iteratively throughout the\nstructure selection process. Our method uses a connection between the vine and\nstructural equation models (SEMs). The later can be estimated very fast using\nthe Lasso, also in very high dimensions, to obtain sparse models. Thus, we\nobtain a structure estimate independently of the chosen pair copulas and\nparameters. Additionally, we define the novel concept of regularization paths\nfor R-vine matrices. It relates sparsity of the vine copula model in terms of\nindependence copulas to a penalization coefficient in the structural equation\nmodels. We illustrate our approach and provide many numerical examples. These\ninclude simulations and data applications in high dimensions, showing the\nsuperiority of our approach to other existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 19:00:18 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["M\u00fcller", "Dominik", ""], ["Czado", "Claudia", ""]]}, {"id": "1705.05938", "submitter": "Charlotte Jones-Todd", "authors": "Charlotte M. Jones-Todd, Peter Caie, Janine Illian, Ben C. Stevenson,\n  Anne Savage, David J. Harrison, James L. Bown", "title": "Unusual structures inherent in point pattern data predict colon cancer\n  patient survival", "comments": "20 pages including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer patient diagnosis and prognosis is informed by assessment of\nmorphological properties observed in patient tissue. Pathologists normally\ncarry out this assessment, yet advances in computational image analysis provide\nopportunities for quantitative assessment of tissue. A key aspect of that\nquantitative assessment is the development of algorithms able to link image\ndata to patient survival. Here, we develop a point process methodology able to\ndescribe patterns in cell distribution within cancerous tissue samples. In\nparticular, we consider the Palm intensities of two Neyman Scott point\nprocesses, and a void process developed herein to reflect the spatial\npatterning of the cells. An approximate-likelihood technique is taken in order\nto fit point process models to patient data and the predictive performance of\neach model is determined. We demonstrate that based solely on the spatial\narrangement of cells we are able to predict patient survival.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 22:00:47 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Jones-Todd", "Charlotte M.", ""], ["Caie", "Peter", ""], ["Illian", "Janine", ""], ["Stevenson", "Ben C.", ""], ["Savage", "Anne", ""], ["Harrison", "David J.", ""], ["Bown", "James L.", ""]]}, {"id": "1705.05967", "submitter": "Fan Yang", "authors": "Fan Yang, Fengshuo Zhang", "title": "Community Detection for Multilayer Heterogeneous Network", "comments": "We were not aware of the fact that similar ideas have already\n  appeared in the literature. We decide to withdraw the manuscript and\n  apologize for any confusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world networks consist of multiple types of nodes with edges that\nare heterogeneous in nature. However, most of the existing work for community\ndetection only focused on homogeneous network consisting of a single layer. In\nthis paper, we propose a modified Degree-Corrected Stochastic Model (DCBM) for\nmodeling multilayer heterogeneous network. We develop a spectral clustering\nmethod that can unify the information contained in each sub-network, and\ndemonstrate its efficiency to detect communities on simulated data and on\nAuthorship/Citation network data. As a by-product, we present a novel algorithm\ncalled BiScore for clustering bipartite network under DCBM, and show that under\nmild conditions BiScore is guaranteed to yield consistent results.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 01:27:16 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 21:42:47 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Yang", "Fan", ""], ["Zhang", "Fengshuo", ""]]}, {"id": "1705.06168", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier,\n  Ulrike von Luxburg", "title": "Two-Sample Tests for Large Random Graphs Using Network Statistics", "comments": "To be presented in COLT 2017 (author sequence, funding details and\n  minor typos updated in version 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a two-sample hypothesis testing problem, where the distributions\nare defined on the space of undirected graphs, and one has access to only one\nobservation from each model. A motivating example for this problem is comparing\nthe friendship networks on Facebook and LinkedIn. The practical approach to\nsuch problems is to compare the networks based on certain network statistics.\nIn this paper, we present a general principle for two-sample hypothesis testing\nin such scenarios without making any assumption about the network generation\nprocess. The main contribution of the paper is a general formulation of the\nproblem based on concentration of network statistics, and consequently, a\nconsistent two-sample test that arises as the natural solution for this\nproblem. We also show that the proposed test is minimax optimal for certain\nnetwork statistics.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 14:05:59 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 13:27:07 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Gutzeit", "Maurilio", ""], ["Carpentier", "Alexandra", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1705.06259", "submitter": "Daniel Gervini", "authors": "Daniel Gervini and Tyler J. Baur", "title": "Joint models for grid point and response processes in longitudinal and\n  functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of the grid points at which a response function is observed\nin longitudinal or functional data applications is often informative and not\nindependent of the response process. In this paper we introduce a covariation\nmodel to estimate and make inferences about this interrelation, by treating the\ndata as replicated realizations of a marked point process. We derive maximum\nlikelihood estimators, the asymptotic distribution of the estimators, and study\nthe estimators' behavior by simulation. We apply the model to an online auction\ndata set and show that there is a strong correlation between bidding patterns\nand price trajectories.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 16:55:59 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 14:17:32 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 15:25:53 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Gervini", "Daniel", ""], ["Baur", "Tyler J.", ""]]}, {"id": "1705.06261", "submitter": "Matthias Killiches", "authors": "Matthias Killiches and Claudia Czado", "title": "A D-vine copula based model for repeated measurements extending linear\n  mixed models with homogeneous correlation structure", "comments": "28 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for unbalanced longitudinal data, where the univariate\nmargins can be selected arbitrarily and the dependence structure is described\nwith the help of a D-vine copula. We show that our approach is an extremely\nflexible extension of the widely used linear mixed model if the correlation is\nhomogeneous over the considered individuals. As an alternative to joint\nmaximum-likelihood a sequential estimation approach for the D-vine copula is\nprovided and validated in a simulation study. The model can handle missing\nvalues without being forced to discard data. Since conditional distributions\nare known analytically, we easily make predictions for future events. For model\nselection we adjust the Bayesian information criterion to our situation. In an\napplication to heart surgery data our model performs clearly better than\ncompeting linear mixed models.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:03:41 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Killiches", "Matthias", ""], ["Czado", "Claudia", ""]]}, {"id": "1705.06634", "submitter": "Gaonyalelwe Maribe Mr", "authors": "Jan Beirlant and Gaonyalelwe Maribe and Andrehette Verster", "title": "Penalized bias reduction in extreme value estimation for censored\n  Pareto-type data, and long-tailed insurance applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subject of tail estimation for randomly censored data from a heavy tailed\ndistribution receives growing attention, motivated by applications for instance\nin actuarial statistics. The bias of the available estimators of the extreme\nvalue index can be substantial and depends strongly on the amount of censoring.\nWe review the available estimators, propose a new bias reduced estimator, and\nshow how shrinkage estimation can help to keep the MSE under control. A\nbootstrap algorithm is proposed to construct confidence intervals. We compare\nthese new proposals with the existing estimators through simulation. We\nconclude this paper with a detailed study of a long-tailed car insurance\nportfolio, which typically exhibit heavy censoring.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 15:01:54 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Beirlant", "Jan", ""], ["Maribe", "Gaonyalelwe", ""], ["Verster", "Andrehette", ""]]}, {"id": "1705.06679", "submitter": "Minh-Ngoc Tran", "authors": "David Gunawan, Minh-Ngoc Tran, Robert Kohn", "title": "Fast Inference for Intractable Likelihood Problems using Variational\n  Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is a popular estimation method for Bayesian inference.\nHowever, most existing VB algorithms are restricted to cases where the\nlikelihood is tractable, which precludes their use in many important\nsituations. Tran et al. (2017) extend the scope of application of VB to cases\nwhere the likelihood is intractable but can be estimated unbiasedly, and name\nthe method Variational Bayes with Intractable Likelihood (VBIL). This paper\npresents a version of VBIL, named Variational Bayes with Intractable\nLog-Likelihood (VBILL), that is useful for cases as Big Data and Big Panel Data\nmodels, where unbiased estimators of the gradient of the log-likelihood are\navailable. We demonstrate that such estimators can be easily obtained in many\nBig Data applications. The proposed method is exact in the sense that, apart\nfrom an extra Monte Carlo error which can be controlled, it is able to produce\nestimators as if the true likelihood, or full-data likelihood, is used. In\nparticular, we develop a computationally efficient approach, based on data\nsubsampling and the MapReduce programming technique, for analyzing massive\ndatasets which cannot fit into the memory of a single desktop PC. We illustrate\nthe method using several simulated datasets and a big real dataset based on the\narrival time status of U. S. airlines.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 16:31:29 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Gunawan", "David", ""], ["Tran", "Minh-Ngoc", ""], ["Kohn", "Robert", ""]]}, {"id": "1705.06732", "submitter": "Francisco Traversaro Prof.", "authors": "Francisco Traversaro, Francisco Redelico", "title": "Confidence Intervals and Hypothesis Testing for the Permutation Entropy\n  with an application to Epilepsy", "comments": null, "journal-ref": null, "doi": "10.1016/j.cnsns.2017.10.013", "report-no": null, "categories": "stat.ME physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonlinear dynamics, and to a lesser extent in other fields, a widely used\nmeasure of complexity is the Permutation Entropy. But there is still no known\nmethod to determine the accuracy of this measure. There has been little\nresearch on the statistical properties of this quantity that characterize time\nseries. The literature describes some resampling methods of quantities used in\nnonlinear dynamics - as the largest Lyapunov exponent - but all of these seems\nto fail. In this contribution we propose a parametric bootstrap methodology\nusing a symbolic representation of the time series in order to obtain the\ndistribution of the Permutation Entropy estimator. We perform several time\nseries simulations given by well known stochastic processes: the 1=f? noise\nfamily, and show in each case that the proposed accuracy measure is as\nefficient as the one obtained by the frequentist approach of repeating the\nexperiment. The complexity of brain electrical activity, measured by the\nPermutation Entropy, has been extensively used in epilepsy research for\ndetection in dynamical changes in electroencephalogram (EEG) signal with no\nconsideration of the variability of this complexity measure. An application of\nthe parametric bootstrap methodology is used to compare normal and pre-ictal\nEEG signals.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 03:17:02 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Traversaro", "Francisco", ""], ["Redelico", "Francisco", ""]]}, {"id": "1705.06760", "submitter": "Valerie Robert", "authors": "Valerie Robert, Yann Vasseur, Vincent Brault", "title": "Comparing high dimensional partitions, with the Coclustering Adjusted\n  Rand Index", "comments": "52 pages", "journal-ref": null, "doi": "10.1007/s00357-020-09379-w", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the simultaneous clustering of rows and columns of a matrix and\nmore particularly the ability to measure the agreement between two\nco-clustering partitions. The new criterion we developed is based on the\nAdjusted Rand Index and is called the Co-clustering Adjusted Rand Index named\nCARI. We also suggest new improvements to existing criteria such as the\nClassification Error which counts the proportion of misclassified cells and the\nExtended Normalized Mutual Information criterion which is a generalization of\nthe criterion based on mutual information in the case of classic\nclassifications. We study these criteria with regard to some desired properties\nderiving from the co-clustering context. Experiments on simulated and real\nobserved data are proposed to compare the behavior of these criteria.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 18:26:59 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 13:12:37 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 06:30:55 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Robert", "Valerie", ""], ["Vasseur", "Yann", ""], ["Brault", "Vincent", ""]]}, {"id": "1705.06772", "submitter": "Yun-Jhong Wu", "authors": "Yun-Jhong Wu, Elizaveta Levina, Ji Zhu", "title": "Generalized linear models with low rank effects for network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a useful representation for data on connections between units of\ninterests, but the observed connections are often noisy and/or include missing\nvalues. One common approach to network analysis is to treat the network as a\nrealization from a random graph model, and estimate the underlying edge\nprobability matrix, which is sometimes referred to as network denoising. Here\nwe propose a generalized linear model with low rank effects to model network\nedges. This model can be applied to various types of networks, including\ndirected and undirected, binary and weighted, and it can naturally utilize\nadditional information such as node and/or edge covariates. We develop an\nefficient projected gradient ascent algorithm to fit the model, establish\nasymptotic consistency, and demonstrate empirical performance of the method on\nboth simulated and real networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 19:15:41 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Wu", "Yun-Jhong", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1705.06808", "submitter": "Kinjal Basu", "authors": "Kinjal Basu and Souvik Ghosh", "title": "Adaptive Rate of Convergence of Thompson Sampling for Gaussian Process\n  Optimization", "comments": "25 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of global optimization of a function over a\ncontinuous domain. In our setup, we can evaluate the function sequentially at\npoints of our choice and the evaluations are noisy. We frame it as a\ncontinuum-armed bandit problem with a Gaussian Process prior on the function.\nIn this regime, most algorithms have been developed to minimize some form of\nregret. In this paper, we study the convergence of the sequential point $x^t$\nto the global optimizer $x^*$ for the Thompson Sampling approach. Under some\nassumptions and regularity conditions, we prove concentration bounds for $x^t$\nwhere the probability that $x^t$ is bounded away from $x^*$ decays\nexponentially fast in $t$. Moreover, the result allows us to derive adaptive\nconvergence rates depending on the function structure.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 21:36:40 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 00:54:57 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 17:51:02 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2020 19:20:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Basu", "Kinjal", ""], ["Ghosh", "Souvik", ""]]}, {"id": "1705.06917", "submitter": "Bojana Milo\\v{s}evi\\'c", "authors": "V. Bo\\v{z}in, B. Milo\\v{s}evi\\'c, Ya.Yu. Nikitin, M. Obradovi\\'c", "title": "New characterization based symmetry tests", "comments": null, "journal-ref": null, "doi": "10.1007/s40840-018-0680-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two new symmetry tests, of integral and Kolmogorov type, based on the\ncharacterization by squares of linear statistics are proposed.\n  The test statistics are related to the family of degenerate U-statistics.\nTheir asymptotic properties are explored. The maximal eigenvalue, needed for\nthe derivation of their logarithmic tail behavior, was calculated or\napproximated using techniques from the theory of linear operators and the\nperturbation theory.\n  The quality of the tests is assessed using the approximate Bahadur efficiency\nas well as the simulated powers. The tests are shown to be comparable with some\nrecent and classical tests of symmetry.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:55:21 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 14:47:01 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 13:24:18 GMT"}, {"version": "v4", "created": "Wed, 28 Feb 2018 08:18:34 GMT"}, {"version": "v5", "created": "Thu, 27 Sep 2018 15:34:02 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Bo\u017ein", "V.", ""], ["Milo\u0161evi\u0107", "B.", ""], ["Nikitin", "Ya. Yu.", ""], ["Obradovi\u0107", "M.", ""]]}, {"id": "1705.07265", "submitter": "Sudipto Banerjee", "authors": "Sudipto Banerjee", "title": "High-Dimensional Bayesian Geostatistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing capabilities of Geographic Information Systems (GIS) and\nuser-friendly software, statisticians today routinely encounter geographically\nreferenced data containing observations from a large number of spatial\nlocations and time points. Over the last decade, hierarchical spatiotemporal\nprocess models have become widely deployed statistical tools for researchers to\nbetter understand the complex nature of spatial and temporal variability.\nHowever, fitting hierarchical spatiotemporal models often involves expensive\nmatrix computations with complexity increasing in cubic order for the number of\nspatial locations and temporal points. This renders such models unfeasible for\nlarge data sets. This article offers a focused review of two methods for\nconstructing well-defined highly scalable spatiotemporal stochastic processes.\nBoth these processes can be used as \"priors\" for spatiotemporal random fields.\nThe first approach constructs a low-rank process operating on a\nlower-dimensional subspace. The second approach constructs a Nearest-Neighbor\nGaussian Process (NNGP) that ensures sparse precision matrices for its finite\nrealizations. Both processes can be exploited as a scalable prior embedded\nwithin a rich hierarchical modeling framework to deliver full Bayesian\ninference. These approaches can be described as model-based solutions for big\nspatiotemporal datasets. The models ensure that the algorithmic complexity has\n$\\sim n$ floating point operations (flops), where $n$ the number of spatial\nlocations (per iteration). We compare these methods and provide some insight\ninto their methodological underpinnings.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 06:39:21 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Banerjee", "Sudipto", ""]]}, {"id": "1705.07278", "submitter": "Biswa Sengupta", "authors": "Gerald K Cooray and Richard Rosch and Torsten Baldeweg and Louis\n  Lemieux and Karl Friston and Biswa Sengupta", "title": "Bayesian Belief Updating of Spatiotemporal Seizure Dynamics", "comments": "ICML 2017 Time Series Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epileptic seizure activity shows complicated dynamics in both space and time.\nTo understand the evolution and propagation of seizures spatially extended sets\nof data need to be analysed. We have previously described an efficient\nfiltering scheme using variational Laplace that can be used in the Dynamic\nCausal Modelling (DCM) framework [Friston, 2003] to estimate the temporal\ndynamics of seizures recorded using either invasive or non-invasive electrical\nrecordings (EEG/ECoG). Spatiotemporal dynamics are modelled using a partial\ndifferential equation -- in contrast to the ordinary differential equation used\nin our previous work on temporal estimation of seizure dynamics [Cooray, 2016].\nWe provide the requisite theoretical background for the method and test the\nensuing scheme on simulated seizure activity data and empirical invasive ECoG\ndata. The method provides a framework to assimilate the spatial and temporal\ndynamics of seizure activity, an aspect of great physiological and clinical\nimportance.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 08:06:05 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:27:16 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cooray", "Gerald K", ""], ["Rosch", "Richard", ""], ["Baldeweg", "Torsten", ""], ["Lemieux", "Louis", ""], ["Friston", "Karl", ""], ["Sengupta", "Biswa", ""]]}, {"id": "1705.07463", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Athina P. Petropulu", "title": "Spatially Controlled Relay Beamforming: $2$-Stage Optimal Policies", "comments": "68 pages, 10 figures, this work constitutes an extended\n  preprint/version of a two part paper (soon to be) submitted for publication\n  to the IEEE Transactions on Signal Processing in Spring/Summer 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of enhancing Quality-of-Service (QoS) in power constrained,\nmobile relay beamforming networks, by optimally and dynamically controlling the\nmotion of the relaying nodes, is considered, in a dynamic channel environment.\nWe assume a time slotted system, where the relays update their positions before\nthe beginning of each time slot. Modeling the wireless channel as a Gaussian\nspatiotemporal stochastic field, we propose a novel $2$-stage stochastic\nprogramming problem formulation for optimally specifying the positions of the\nrelays at each time slot, such that the expected QoS of the network is\nmaximized, based on causal Channel State Information (CSI) and under a total\nrelay transmit power budget. This results in a schema where, at each time slot,\nthe relays, apart from optimally beamforming to the destination, also\noptimally, predictively decide their positions at the next time slot, based on\ncausally accumulated experience. Exploiting either the Method of Statistical\nDifferentials, or the multidimensional Gauss-Hermite Quadrature Rule, the\nstochastic program considered is shown to be approximately equivalent to a set\nof simple subproblems, which are solved in a distributed fashion, one at each\nrelay. Optimality and performance of the proposed spatially controlled system\nare also effectively assessed, under a rigorous technical framework; strict\noptimality is rigorously demonstrated via the development of a version of the\nFundamental Lemma of Stochastic Control, and, performance-wise, it is shown\nthat, quite interestingly, the optimal average network QoS exhibits an\nincreasing trend across time slots, despite our myopic problem formulation.\nNumerical simulations are presented, experimentally corroborating the success\nof the proposed approach and the validity of our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 15:29:13 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1705.07509", "submitter": "Fran\\c{c}ois Koladjo", "authors": "Fran\\c{c}ois Koladjo, Mesrob I. Ohannessian and \\'Elisabeth Gassiat", "title": "A truncation model for estimating Species Richness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a truncation model for abundance distribution in the species\nrichness estimation. This model is inherently semiparametric and incorporates\nan unknown truncation threshold between rare and abundant counts observations.\nUsing the conditional likelihood, we derive a class of estimators for the\nparameters in the model by a stepwise maximisation. The species richness\nestimator is given by the integer maximising the binomial likelihood when all\nother parameters in the model are know. Under regularity conditions, we show\nthat the estimators of the model parameters are asymptotically efficient. We\nrecover the Chao$^{'}$s lower bound estimator of species richeness when the\nmodel is a unicomponent Poisson$^{'}$s model. So, it is an element of our class\nof estimators. In a simulation study, we show the performances of the proposed\nmethod and compare it to some others.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 21:01:10 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Koladjo", "Fran\u00e7ois", ""], ["Ohannessian", "Mesrob I.", ""], ["Gassiat", "\u00c9lisabeth", ""]]}, {"id": "1705.07529", "submitter": "Chiara Sabatti", "authors": "Marina Bogomolov, Christine B. Peterson, Yoav Benjamini, Chiara\n  Sabatti", "title": "Testing hypotheses on a tree: new error rates and controlling strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multiple testing procedure (TreeBH) which addresses the\nchallenge of controlling error rates at multiple levels of resolution.\nConceptually, we frame this problem as the selection of hypotheses which are\norganized hierarchically in a tree structure. We describe a fast algorithm for\nthe proposed sequential procedure, and prove that it controls relevant error\nrates given certain assumptions on the dependence among the p-values. Through\nsimulations, we demonstrate that TreeBH offers the desired guarantees under a\nrange of dependency structures (including one similar to that encountered in\ngenome-wide association studies) and that it has the potential of gaining power\nover alternative methods. We also introduce a modified version of TreeBH which\nwe prove to control the relevant error rates under any dependency structure.\n  We conclude with two case studies: we first analyze data collected as part of\nthe Genotype-Tissue Expression (GTEx) project, which aims to characterize the\ngenetic regulation of gene expression across multiple tissues in the human\nbody, and secondly, data examining the relationship between the gut microbiome\nand colorectal cancer.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 01:24:53 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 18:55:16 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Bogomolov", "Marina", ""], ["Peterson", "Christine B.", ""], ["Benjamini", "Yoav", ""], ["Sabatti", "Chiara", ""]]}, {"id": "1705.07600", "submitter": "Art\\\"ur Manukyan", "authors": "Art\\\"ur Manukyan and Elvan Ceyhan", "title": "Classification Using Proximity Catch Digraphs (Technical Report)", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ random geometric digraphs to construct semi-parametric classifiers.\nThese data-random digraphs are from parametrized random digraph families called\nproximity catch digraphs (PCDs). A related geometric digraph family, class\ncover catch digraph (CCCD), has been used to solve the class cover problem by\nusing its approximate minimum dominating set. CCCDs showed relatively good\nperformance in the classification of imbalanced data sets, and although CCCDs\nhave a convenient construction in $\\mathbb{R}^d$, finding minimum dominating\nsets is NP-hard and its probabilistic behaviour is not mathematically tractable\nexcept for $d=1$. On the other hand, a particular family of PCDs, called\n\\emph{proportional-edge} PCDs (PE-PCDs), has mathematical tractable minimum\ndominating sets in $\\mathbb{R}^d$; however their construction in higher\ndimensions may be computationally demanding. More specifically, we show that\nthe classifiers based on PE-PCDs are prototype-based classifiers such that the\nexact minimum number of prototypes (equivalent to minimum dominating sets) are\nfound in polynomial time on the number of observations. We construct two types\nof classifiers based on PE-PCDs. One is a family of hybrid classifiers depend\non the location of the points of the training data set, and another type is a\nfamily of classifiers solely based on class covers. We assess the\nclassification performance of our PE-PCD based classifiers by extensive Monte\nCarlo simulations, and compare them with that of other commonly used\nclassifiers. We also show that, similar to CCCD classifiers, our classifiers\nare relatively better in classification in the presence of class imbalance.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:09:29 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Manukyan", "Art\u00fcr", ""], ["Ceyhan", "Elvan", ""]]}, {"id": "1705.07605", "submitter": "Marek Omelka", "authors": "Natalie Neumeyer, Marek Omelka, Sarka Hudecova", "title": "A copula approach for dependence modeling in multivariate nonparametric\n  time series", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2018.11.016", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with modeling the dependence structure of two (or\nmore) time-series in the presence of a (possible multivariate) covariate which\nmay include past values of the time series. We assume that the covariate\ninfluences only the conditional mean and the conditional variance of each of\nthe time series but the distribution of the standardized innovations is not\ninfluenced by the covariate and is stable in time. The joint distribution of\nthe time series is then determined by the conditional means, the conditional\nvariances and the marginal distributions of the innovations, which we estimate\nnonparametrically, and the copula of the innovations, which represents the\ndependency structure. We consider a nonparametric as well as a semiparametric\nestimator based on the estimated residuals. We show that under suitable\nassumptions these copula estimators are asymptotically equivalent to estimators\nthat would be based on the unobserved innovations. The theoretical results are\nillustrated by simulations and a real data example.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:27:37 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 15:19:49 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Neumeyer", "Natalie", ""], ["Omelka", "Marek", ""], ["Hudecova", "Sarka", ""]]}, {"id": "1705.07880", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Nicholas J. Foti, Alexander D'Amour, Ryan P. Adams", "title": "Reducing Reparameterization Gradient Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization with noisy gradients has become ubiquitous in statistics and\nmachine learning. Reparameterization gradients, or gradient estimates computed\nvia the \"reparameterization trick,\" represent a class of noisy gradients often\nused in Monte Carlo variational inference (MCVI). However, when these gradient\nestimators are too noisy, the optimization procedure can be slow or fail to\nconverge. One way to reduce noise is to use more samples for the gradient\nestimate, but this can be computationally expensive. Instead, we view the noisy\ngradient as a random variable, and form an inexpensive approximation of the\ngenerating procedure for the gradient sample. This approximation has high\ncorrelation with the noisy gradient by construction, making it a useful control\nvariate for variance reduction. We demonstrate our approach on non-conjugate\nmulti-level hierarchical models and a Bayesian neural net where we observed\ngradient variance reductions of multiple orders of magnitude (20-2,000x).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:51:13 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Miller", "Andrew C.", ""], ["Foti", "Nicholas J.", ""], ["D'Amour", "Alexander", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1705.07941", "submitter": "Raydonal Ospina", "authors": "Patr\\'icia Leone Espinheira and Luana C. Meireles da Silva and Alisson\n  de Oliveira Silva and Raydonal Ospina", "title": "Prediction Measures in Nonlinear Beta Regression Models", "comments": "10 Fig, 20 pag. Submitted to Journal of the Royal Statistical\n  Society. Serie C - Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear models are frequently applied to determine the optimal supply\nnatural gas to a given residential unit based on economical and technical\nfactors, or used to fit biochemical and pharmaceutical assay nonlinear data. In\nthis article we propose PRESS statistics and prediction coefficients for a\nclass of nonlinear beta regression models, namely $P^2$ statistics. We aim at\nusing both prediction coefficients and goodness-of-fit measures as a scheme of\nmodel select criteria. In this sense, we introduce for beta regression models\nunder nonlinearity the use of the model selection criteria based on robust\npseudo-$R^2$ statistics. Monte Carlo simulation results on the finite sample\nbehavior of both prediction-based model selection criteria $P^2$ and the\npseudo-$R^2$ statistics are provided. Three applications for real data are\npresented. The linear application relates to the distribution of natural gas\nfor home usage in S\\~ao Paulo, Brazil. Faced with the economic risk of too\noverestimate or to underestimate the distribution of gas has been necessary to\nconstruct prediction limits and to select the best predicted and fitted model\nto construct best prediction limits it is the aim of the first application.\nAdditionally, the two nonlinear applications presented also highlight the\nimportance of considering both goodness-of-predictive and goodness-of-fit of\nthe competitive models.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 18:39:57 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Espinheira", "Patr\u00edcia Leone", ""], ["da Silva", "Luana C. Meireles", ""], ["Silva", "Alisson de Oliveira", ""], ["Ospina", "Raydonal", ""]]}, {"id": "1705.07950", "submitter": "Kashif Yousuf", "authors": "Kashif Yousuf", "title": "Variable Screening for High Dimensional Time Series", "comments": "Published in the Electronic Journal of Statistics\n  (https://projecteuclid.org/euclid.ejs/1519700498)", "journal-ref": "Electronic Journal of Statistics, Volume 12, Number 1 (2018),\n  667-702", "doi": "10.1214/18-EJS1402", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variable selection is a widely studied problem in high dimensional\nstatistics, primarily since estimating the precise relationship between the\ncovariates and the response is of great importance in many scientific\ndisciplines. However, most of theory and methods developed towards this goal\nfor the linear model invoke the assumption of iid sub-Gaussian covariates and\nerrors. This paper analyzes the theoretical properties of Sure Independence\nScreening (SIS) (Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008)\n849-911]) for high dimensional linear models with dependent and/or heavy tailed\ncovariates and errors. We also introduce a generalized least squares screening\n(GLSS) procedure which utilizes the serial correlation present in the data. By\nutilizing this serial correlation when estimating our marginal effects, GLSS is\nshown to outperform SIS in many cases. For both procedures we prove sure\nscreening properties, which depend on the moment conditions, and the strength\nof dependence in the error and covariate processes, amongst other factors.\nAdditionally, combining these screening procedures with the adaptive Lasso is\nanalyzed. Dependence is quantified by functional dependence measures (Wu [Proc.\nNatl. Acad. Sci. USA 102 (2005) 14150-14154]), and the results rely on the use\nof Nagaev-type and exponential inequalities for dependent random variables. We\nalso conduct simulations to demonstrate the finite sample performance of these\nprocedures, and include a real data application of forecasting the US inflation\nrate.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 18:58:46 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 18:43:20 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Yousuf", "Kashif", ""]]}, {"id": "1705.08020", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Dylan S. Small and Ashkan Ertefaie", "title": "Selective inference for effect modification via the lasso", "comments": "32 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effect modification occurs when the effect of the treatment on an outcome\nvaries according to the level of other covariates and often has important\nimplications in decision making. When there are tens or hundreds of covariates,\nit becomes necessary to use the observed data to select a simpler model for\neffect modification and then make valid statistical inference. We propose a two\nstage procedure to solve this problem. First, we use Robinson's transformation\nto decouple the nuisance parameters from the treatment effect of interest and\nuse machine learning algorithms to estimate the nuisance parameters. Next,\nafter plugging in the estimates of the nuisance parameters, we use the Lasso to\nchoose a low-complexity model for effect modification. Compared to a full model\nconsisting of all the covariates, the selected model is much more\ninterpretable. Compared to the univariate subgroup analyses, the selected model\ngreatly reduces the number of false discoveries. We show that the conditional\nselective inference for the selected model is asymptotically valid given the\nrate assumptions in classical semiparametric regression. Extensive simulation\nstudies are conducted to verify the asymptotic results and an epidemiological\napplication is used to demonstrate the method.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:57:28 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 16:53:31 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 19:49:43 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Small", "Dylan S.", ""], ["Ertefaie", "Ashkan", ""]]}, {"id": "1705.08036", "submitter": "Daniel McDonald", "authors": "Darren Homrighausen and Daniel J. McDonald", "title": "Compressed and Penalized Linear Regression", "comments": "39 pages, 9 figures", "journal-ref": "Journal of Computational and Graphical Statistics (2020), Vol 29,\n  pp. 309--322", "doi": "10.1080/10618600.2019.1660179", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications require methods that are computationally feasible on\nlarge datasets but also preserve statistical efficiency. Frequently, these two\nconcerns are seen as contradictory: approximation methods that enable\ncomputation are assumed to degrade statistical performance relative to exact\nmethods. In applied mathematics, where much of the current theoretical work on\napproximation resides, the inputs are considered to be observed exactly. The\nprevailing philosophy is that while the exact problem is, regrettably,\nunsolvable, any approximation should be as small as possible. However, from a\nstatistical perspective, an approximate or regularized solution may be\npreferable to the exact one. Regularization formalizes a trade-off between\nfidelity to the data and adherence to prior knowledge about the data-generating\nprocess such as smoothness or sparsity. The resulting estimator tends to be\nmore useful, interpretable, and suitable as an input to other methods.\n  In this paper, we propose new methodology for estimation and prediction under\na linear model borrowing insights from the approximation literature. We explore\nthese procedures from a statistical perspective and find that in many cases\nthey improve both computational and statistical performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:03:54 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Homrighausen", "Darren", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "1705.08074", "submitter": "Wei Zheng", "authors": "A. S. Hedayat, Heng Xu and Wei Zheng", "title": "Universally Optimal Designs for the Two-dimensional Interference Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been some major advances in the theory of optimal designs for\ninterference models. However, the majority of them focus on one-dimensional\nlayout of the block and the study for two-dimensional interference model is\nquite limited partly due to technical difficulties. This paper tries to fill\nthis gap. Specifically, it systematically characterizes all possible\nuniversally optimal designs simultaneously. Computational issues are also\naddressed with theoretical backup.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 04:41:05 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Hedayat", "A. S.", ""], ["Xu", "Heng", ""], ["Zheng", "Wei", ""]]}, {"id": "1705.08310", "submitter": "Daniel Kraus", "authors": "Niklas Schallhorn, Daniel Kraus, Thomas Nagler, Claudia Czado", "title": "D-vine quantile regression with discrete variables", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression, the prediction of conditional quantiles, finds\napplications in various fields. Often, some or all of the variables are\ndiscrete. The authors propose two new quantile regression approaches to handle\nsuch mixed discrete-continuous data. Both of them generalize the continuous\nD-vine quantile regression, where the dependence between the response and the\ncovariates is modeled by a parametric D-vine. D-vine quantile regression\nprovides very flexible models, that enable accurate and fast predictions.\nMoreover, it automatically takes care of major issues of classical quantile\nregression, such as quantile crossing and interactions between the covariates.\nThe first approach keeps the parametric estimation of the D-vines, but modifies\nthe formulas to account for the discreteness. The second approach estimates the\nD-vine using continuous convolution to make the discrete variables continuous\nand then estimates the D-vine nonparametrically. A simulation study is\npresented examining for which scenarios the discrete-continuous D-vine quantile\nregression can provide superior prediction abilities. Lastly, the functionality\nof the two introduced methods is demonstrated by a real-world example\npredicting the number of bike rentals.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:24:51 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Schallhorn", "Niklas", ""], ["Kraus", "Daniel", ""], ["Nagler", "Thomas", ""], ["Czado", "Claudia", ""]]}, {"id": "1705.08331", "submitter": "Peter Hoff", "authors": "Peter D. Hoff and Chaoyu Yu", "title": "Exact adaptive confidence intervals for linear regression coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive confidence interval procedure (CIP) for the\ncoefficients in the normal linear regression model. This procedure has a\nfrequentist coverage rate that is constant as a function of the model\nparameters, yet provides smaller intervals than the usual interval procedure,\non average across regression coefficients. The proposed procedure is obtained\nby defining a class of CIPs that all have exact $1-\\alpha$ frequentist\ncoverage, and then selecting from this class the procedure that minimizes a\nprior expected interval width. Such a procedure may be described as\n\"frequentist, assisted by Bayes\" or FAB. We describe an adaptive approach for\nestimating the prior distribution from the data so that exact non-asymptotic\n$1-\\alpha$ coverage is maintained. Additionally, in a \"$p$ growing with $n$\"\nasymptotic scenario, this adaptive FAB procedure is asymptotically\nBayes-optimal among $1-\\alpha$ frequentist CIPs.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:47:43 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 20:19:12 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Hoff", "Peter D.", ""], ["Yu", "Chaoyu", ""]]}, {"id": "1705.08360", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland, Heiko Strathmann, Michael Arbel, Arthur Gretton", "title": "Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families", "comments": null, "journal-ref": "Proceedings of the Twenty-First International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2018), PMLR 84:652-660", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a fast method with statistical guarantees for learning an\nexponential family density model where the natural parameter is in a\nreproducing kernel Hilbert space, and may be infinite-dimensional. The model is\nlearned by fitting the derivative of the log density, the score, thus avoiding\nthe need to compute a normalization constant. Our approach improves the\ncomputational efficiency of an earlier solution by using a low-rank,\nNystr\\\"om-like solution. The new solution retains the consistency and\nconvergence rates of the full-rank solution (exactly in Fisher distance, and\nnearly in other distances), with guarantees on the degree of cost and storage\nreduction. We evaluate the method in experiments on density estimation and in\nthe construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an\nexisting score learning approach using a denoising autoencoder, our estimator\nis empirically more data-efficient when estimating the score, runs faster, and\nhas fewer parameters (which can be tuned in a principled and interpretable\nway), in addition to providing statistical guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:29:00 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 22:38:44 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 16:19:42 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 21:43:03 GMT"}, {"version": "v5", "created": "Tue, 13 Mar 2018 19:06:52 GMT"}, {"version": "v6", "created": "Thu, 14 Jan 2021 05:43:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Strathmann", "Heiko", ""], ["Arbel", "Michael", ""], ["Gretton", "Arthur", ""]]}, {"id": "1705.08393", "submitter": "David Gerard", "authors": "David Gerard and Matthew Stephens", "title": "Unifying and Generalizing Methods for Removing Unwanted Variation Based\n  on Negative Controls", "comments": "34 pages, 6 figures, methods implemented at\n  https://github.com/dcgerard/vicar , results reproducible at\n  https://github.com/dcgerard/ruvb_sims", "journal-ref": null, "doi": "10.5705/ss.202018.0345", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unwanted variation, including hidden confounding, is a well-known problem in\nmany fields, particularly large-scale gene expression studies. Recent proposals\nto use control genes --- genes assumed to be unassociated with the covariates\nof interest --- have led to new methods to deal with this problem. Going by the\nmoniker Removing Unwanted Variation (RUV), there are many versions --- RUV1,\nRUV2, RUV4, RUVinv, RUVrinv, RUVfun. In this paper, we introduce a general\nframework, RUV*, that both unites and generalizes these approaches. This\nunifying framework helps clarify connections between existing methods. In\nparticular we provide conditions under which RUV2 and RUV4 are equivalent. The\nRUV* framework also preserves an advantage of RUV approaches --- their\nmodularity --- which facilitates the development of novel methods based on\nexisting matrix imputation algorithms. We illustrate this by implementing RUVB,\na version of RUV* based on Bayesian factor analysis. In realistic simulations\nbased on real data we found that RUVB is competitive with existing methods in\nterms of both power and calibration, although we also highlight the challenges\nof providing consistently reliable calibration among data sets.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:19:42 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Gerard", "David", ""], ["Stephens", "Matthew", ""]]}, {"id": "1705.08445", "submitter": "Brian Van Koten", "authors": "Aaron R. Dinner, Erik Thiede, Brian Van Koten, Jonathan Weare", "title": "Stratification as a general variance reduction method for Markov chain\n  Monte Carlo", "comments": "52 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Eigenvector Method for Umbrella Sampling (EMUS) belongs to a popular\nclass of methods in statistical mechanics which adapt the principle of\nstratified survey sampling to the computation of free energies. We develop a\ndetailed theoretical analysis of EMUS. Based on this analysis, we show that\nEMUS is an efficient general method for computing averages over arbitrary\ntarget distributions. In particular, we show that EMUS can be dramatically more\nefficient than direct MCMC when the target distribution is multimodal or when\nthe goal is to compute tail probabilities. To illustrate these theoretical\nresults, we present a tutorial application of the method to a problem from\nBayesian statistics.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:59:32 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 14:06:03 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 14:26:46 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Dinner", "Aaron R.", ""], ["Thiede", "Erik", ""], ["Van Koten", "Brian", ""], ["Weare", "Jonathan", ""]]}, {"id": "1705.08526", "submitter": "Peng Ding", "authors": "Peng Ding, Luke W. Miratrix", "title": "Model-free causal inference of binary experimental data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For binary experimental data, we discuss randomization-based inferential\nprocedures that do not need to invoke any modeling assumptions. We also\nintroduce methods for likelihood and Bayesian inference based solely on the\nphysical randomization without any hypothetical super population assumptions\nabout the potential outcomes. These estimators have some properties superior to\nmoment-based ones such as only giving estimates in regions of feasible support.\nDue to the lack of identification of the causal model, we also propose a\nsensitivity analysis approach which allows for the characterization of the\nimpact of the association between the potential outcomes on statistical\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 20:28:57 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Ding", "Peng", ""], ["Miratrix", "Luke W.", ""]]}, {"id": "1705.08527", "submitter": "Elizabeth Ogburn", "authors": "Elizabeth L. Ogburn, Oleg Sofrygin, Ivan Diaz, Mark J. van der Laan", "title": "Causal inference for social network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe semiparametric estimation and inference for causal effects using\nobservational data from a single social network. Our asymptotic result is the\nfirst to allow for dependence of each observation on a growing number of other\nunits as sample size increases. While previous methods have generally\nimplicitly focused on one of two possible sources of dependence among social\nnetwork observations, we allow for both dependence due to transmission of\ninformation across network ties, and for dependence due to latent similarities\namong nodes sharing ties. We describe estimation and inference for new causal\neffects that are specifically of interest in social network settings, such as\ninterventions on network ties and network structure. Using our methods to\nreanalyze the Framingham Heart Study data used in one of the most influential\nand controversial causal analyses of social network data, we find that after\naccounting for network structure there is no evidence for the causal effects\nclaimed in the original paper.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 20:36:30 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 20:04:46 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 14:45:06 GMT"}, {"version": "v4", "created": "Thu, 5 Oct 2017 21:34:53 GMT"}, {"version": "v5", "created": "Mon, 17 Feb 2020 19:13:47 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Ogburn", "Elizabeth L.", ""], ["Sofrygin", "Oleg", ""], ["Diaz", "Ivan", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1705.08580", "submitter": "Bowei Yan", "authors": "Bowei Yan, Purnamrita Sarkar, Xiuyuan Cheng", "title": "Provable Estimation of the Number of Blocks in Block Models", "comments": "12 pages, 4 figure; AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental unsupervised learning problem for\nunlabeled networks which has a broad range of applications. Many community\ndetection algorithms assume that the number of clusters $r$ is known apriori.\nIn this paper, we propose an approach based on semi-definite relaxations, which\ndoes not require prior knowledge of model parameters like many existing convex\nrelaxation methods and recovers the number of clusters and the clustering\nmatrix exactly under a broad parameter regime, with probability tending to one.\nOn a variety of simulated and real data experiments, we show that the proposed\nmethod often outperforms state-of-the-art techniques for estimating the number\nof clusters.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 01:42:50 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 22:34:00 GMT"}, {"version": "v3", "created": "Sun, 18 Mar 2018 19:46:34 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Yan", "Bowei", ""], ["Sarkar", "Purnamrita", ""], ["Cheng", "Xiuyuan", ""]]}, {"id": "1705.08582", "submitter": "Andrea Rotnitzky", "authors": "Andrea Rotnitzky, James Robins and Lucia Babino", "title": "On the multiply robust estimation of the mean of the g-functional", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multiply robust (MR) estimators of the longitudinal g-computation\nformula of Robins (1986). In the first part of this paper we review and extend\nthe recently proposed parametric multiply robust estimators of\nTchetgen-Tchetgen (2009) and Molina, Rotnitzky, Sued and Robins (2017). In the\nsecond part of the paper we derive multiply and doubly robust estimators that\nuse non-parametric machine-learning (ML) estimators of nuisance functions in\nlieu of parametric models. We use sample splitting to avoid the need for\nDonsker conditions, thereby allowing an analyst to select the ML algorithms of\ntheir choosing. We contrast the asymptotic behavior of our non-parametric\ndoubly robust and multiply robust estimators. In particular, we derive formulas\nfor their asymptotic bias. Examining these formulas we conclude that although,\nunder certain data generating laws, the rate at which the bias of the MR\nestimator converges to zero can exceed that of the DR estimator, nonetheless,\nunder most laws, the bias of the DR and MR estimators converge to zero at the\nsame rate.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 02:11:01 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Rotnitzky", "Andrea", ""], ["Robins", "James", ""], ["Babino", "Lucia", ""]]}, {"id": "1705.08656", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en, Finn Lindgren, David Bolin and Mattias Villani", "title": "Efficient Covariance Approximations for Large Sparse Precision Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of sparse precision (inverse covariance) matrices has become popular\nbecause they allow for efficient algorithms for joint inference in\nhigh-dimensional models. Many applications require the computation of certain\nelements of the covariance matrix, such as the marginal variances, which may be\nnon-trivial to obtain when the dimension is large. This paper introduces a fast\nRao-Blackwellized Monte Carlo sampling based method for efficiently\napproximating selected elements of the covariance matrix. The variance and\nconfidence bounds of the approximations can be precisely estimated without\nadditional computational costs. Furthermore, a method that iterates over\nsubdomains is introduced, and is shown to additionally reduce the approximation\nerrors to practically negligible levels in an application on functional\nmagnetic resonance imaging data. Both methods have low memory requirements,\nwhich is typically the bottleneck for competing direct methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 08:32:46 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 10:00:05 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Lindgren", "Finn", ""], ["Bolin", "David", ""], ["Villani", "Mattias", ""]]}, {"id": "1705.08699", "submitter": "Moritz Berger Dr.", "authors": "Moritz Berger, Gerhard Tutz, Matthias Schmid", "title": "Tree-Structured Modelling of Varying Coefficients", "comments": "20 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The varying-coefficient model is a strong tool for the modelling of\ninteractions in generalized regression. It is easy to apply if both the\nvariables that are modified as well as the effect modifiers are known. However,\nin general one has a set of explanatory variables and it is unknown which\nvariables are modified by which covariates. A recursive partitioning strategy\nis proposed that is able to deal with the complex selection problem. The\ntree-structured modelling yields for each covariate, which is modified by other\nvariables, a tree that visualizes the modified effects. The performance of the\nmethod is investigated in simulations and two applications illustrate its\nusefulness.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 11:01:20 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Berger", "Moritz", ""], ["Tutz", "Gerhard", ""], ["Schmid", "Matthias", ""]]}, {"id": "1705.08742", "submitter": "Andrew Spieker", "authors": "Andrew J. Spieker, Arman Oganisian, Emily M. Ko, Jason A. Roy, and\n  Nandita Mitra", "title": "A causal approach to analysis of censored medical costs in the presence\n  of time-varying treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been a growing interest in the development of statistical\nmethods to compare medical costs between treatment groups. When cumulative cost\nis the outcome of interest, right-censoring poses the challenge of informative\nmissingness due to heterogeneity in the rates of cost accumulation across\nsubjects. Existing approaches seeking to address the challenge of informative\ncost trajectories typically rely on inverse probability weighting and target a\nnet \"intent-to-treat\" effect. However, no approaches capable of handling\ntime-dependent treatment and confounding in this setting have been developed to\ndate. A method to estimate the joint causal effect of a treatment regime on\ncost would be of value to inform public policy when comparing interventions. In\nthis paper, we develop a nested g-computation approach to cost analysis in\norder to accommodate time-dependent treatment and repeated outcome measures. We\ndemonstrate that our procedure is reasonably robust to departures from its\ndistributional assumptions and can provide unique insights into fundamental\ndifferences in average cost across time-dependent treatment regimes.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 13:17:43 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Spieker", "Andrew J.", ""], ["Oganisian", "Arman", ""], ["Ko", "Emily M.", ""], ["Roy", "Jason A.", ""], ["Mitra", "Nandita", ""]]}, {"id": "1705.08800", "submitter": "Franck Picard", "authors": "Franck Picard, Patricia Reynaud-Bouret, Etienne Roquain", "title": "Continuous testing for Poisson process intensities: A new perspective on\n  scanning statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel continuous testing framework to test the intensities of\nPoisson Processes. This framework allows a rigorous definition of the complete\ntesting procedure, from an infinite number of hypothesis to joint error rates.\nOur work extends traditional procedures based on scanning windows, by\ncontrolling the family-wise error rate and the false discovery rate in a\nnon-asymptotic manner and in a continuous way. The decision rule is based on a\n\\pvalue process that can be estimated by a Monte-Carlo procedure. We also\npropose new test statistics based on kernels. Our method is applied in\nNeurosciences and Genomics through the standard test of homogeneity, and the\ntwo-sample test.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:49:20 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Picard", "Franck", ""], ["Reynaud-Bouret", "Patricia", ""], ["Roquain", "Etienne", ""]]}, {"id": "1705.08930", "submitter": "Fang Han", "authors": "Fang Han, Zhao Ren, and Yuxin Zhu", "title": "Pairwise Difference Estimation of High Dimensional Partially Linear\n  Model", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a regularized pairwise difference approach for estimating\nthe linear component coefficient in a partially linear model, with consistency\nand exact rates of convergence obtained in high dimensions under mild scaling\nrequirements. Our analysis reveals interesting features such as (i) the\nbandwidth parameter automatically adapts to the model and is actually\ntuning-insensitive; and (ii) the procedure could even maintain fast rate of\nconvergence for $\\alpha$-H\\\"older class of $\\alpha\\leq1/2$. Simulation studies\nshow the advantage of the proposed method, and application of our approach to a\nbrain imaging data reveals some biological patterns which fail to be recovered\nusing competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:05:32 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 18:09:25 GMT"}, {"version": "v3", "created": "Fri, 12 Jan 2018 03:30:16 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Han", "Fang", ""], ["Ren", "Zhao", ""], ["Zhu", "Yuxin", ""]]}, {"id": "1705.08964", "submitter": "Nicolas Brosse", "authors": "Nicolas Brosse, Alain Durmus, \\'Eric Moulines and Marcelo Pereyra", "title": "Sampling from a log-concave distribution with compact support with\n  proximal Langevin Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a detailed theoretical analysis of the Langevin Monte\nCarlo sampling algorithm recently introduced in Durmus et al. (Efficient\nBayesian computation by proximal Markov chain Monte Carlo: when Langevin meets\nMoreau, 2016) when applied to log-concave probability distributions that are\nrestricted to a convex body $\\mathsf{K}$. This method relies on a\nregularisation procedure involving the Moreau-Yosida envelope of the indicator\nfunction associated with $\\mathsf{K}$. Explicit convergence bounds in total\nvariation norm and in Wasserstein distance of order $1$ are established. In\nparticular, we show that the complexity of this algorithm given a first order\noracle is polynomial in the dimension of the state space. Finally, some\nnumerical experiments are presented to compare our method with competing MCMC\napproaches from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 20:48:32 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Brosse", "Nicolas", ""], ["Durmus", "Alain", ""], ["Moulines", "\u00c9ric", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "1705.09031", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran, Peter L. Spirtes", "title": "Fast Causal Inference with Non-Random Missingness by Test-Wise Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real datasets contain values missing not at random (MNAR). In this\nscenario, investigators often perform list-wise deletion, or delete samples\nwith any missing values, before applying causal discovery algorithms. List-wise\ndeletion is a sound and general strategy when paired with algorithms such as\nFCI and RFCI, but the deletion procedure also eliminates otherwise good samples\nthat contain only a few missing values. In this report, we show that we can\nmore efficiently utilize the observed values with test-wise deletion while\nstill maintaining algorithmic soundness. Here, test-wise deletion refers to the\nprocess of list-wise deleting samples only among the variables required for\neach conditional independence (CI) test used in constraint-based searches.\nTest-wise deletion therefore often saves more samples than list-wise deletion\nfor each CI test, especially when we have a sparse underlying graph. Our\ntheoretical results show that test-wise deletion is sound under the justifiable\nassumption that none of the missingness mechanisms causally affect each other\nin the underlying causal graph. We also find that FCI and RFCI with test-wise\ndeletion outperform their list-wise deletion and imputation counterparts on\naverage when MNAR holds in both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 02:52:05 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""], ["Spirtes", "Peter L.", ""]]}, {"id": "1705.09112", "submitter": "Sylwia Bujkiewicz", "authors": "Dan Jackson, Sylwia Bujkiewicz, Martin Law, Richard D Riley, and Ian\n  White", "title": "A matrix-based method of moments for fitting multivariate network\n  meta-analysis models with multiple outcomes and random inconsistency effects", "comments": null, "journal-ref": "Biometrics 2017", "doi": "10.1111/biom.12762", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects meta-analyses are very commonly used in medical statistics.\nRecent methodological developments include multivariate (multiple outcomes) and\nnetwork (multiple treatments) meta-analysis. Here we provide a new model and\ncorresponding estimation procedure for multivariate network meta-analysis, so\nthat multiple outcomes and treatments can be included in a single analysis. Our\nnew multivariate model is a direct extension of a univariate model for network\nmeta-analysis that has recently been proposed. We allow two types of unknown\nvariance parameters in our model, which represent between-study heterogeneity\nand inconsistency. Inconsistency arises when different forms of direct and\nindirect evidence are not in agreement, even having taken between-study\nheterogeneity into account. However the consistency assumption is often assumed\nin practice and so we also explain how to fit a reduced model which makes this\nassumption. Our estimation method extends several other commonly used methods\nfor meta-analysis, including the method proposed by DerSimonian and Laird\n(1986). We investigate the use of our proposed methods in the context of a real\nexample.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 09:53:01 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Jackson", "Dan", ""], ["Bujkiewicz", "Sylwia", ""], ["Law", "Martin", ""], ["Riley", "Richard D", ""], ["White", "Ian", ""]]}, {"id": "1705.09355", "submitter": "Avanti Athreya", "authors": "Keith Levin, Avanti Athreya, Minh Tang, Vince Lyzinski, Youngser Park,\n  Carey E. Priebe", "title": "A central limit theorem for an omnibus embedding of multiple random\n  graphs and implications for multiscale network inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing statistical analyses on collections of graphs is of import to many\ndisciplines, but principled, scalable methods for multi-sample graph inference\nare few. Here we describe an \"omnibus\" embedding in which multiple graphs on\nthe same vertex set are jointly embedded into a single space with a distinct\nrepresentation for each graph. We prove a central limit theorem for this\nembedding and demonstrate how it streamlines graph comparison, obviating the\nneed for pairwise subspace alignments. The omnibus embedding achieves\nnear-optimal inference accuracy when graphs arise from a common distribution\nand yet retains discriminatory power as a test procedure for the comparison of\ndifferent graphs. Moreover, this joint embedding and the accompanying central\nlimit theorem are important for answering multiscale graph inference questions,\nsuch as the identification of specific subgraphs or vertices responsible for\nsimilarity or difference across networks. We illustrate this with a pair of\nanalyses of connectome data derived from dMRI and fMRI scans of human subjects.\nIn particular, we show that this embedding allows the identification of\nspecific brain regions associated with population-level differences. Finally,\nwe sketch how the omnibus embedding can be used to address pressing open\nproblems, both theoretical and practical, in multisample graph inference.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 20:48:15 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 17:56:17 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 02:48:14 GMT"}, {"version": "v4", "created": "Fri, 15 Feb 2019 14:15:27 GMT"}, {"version": "v5", "created": "Wed, 26 Jun 2019 01:49:26 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Levin", "Keith", ""], ["Athreya", "Avanti", ""], ["Tang", "Minh", ""], ["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1705.09417", "submitter": "Amit Meir", "authors": "Amit Meir and Mathias Drton", "title": "Tractable Post-Selection Maximum Likelihood Inference for the Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying standard statistical methods after model selection may yield\ninefficient estimators and hypothesis tests that fail to achieve nominal type-I\nerror rates. The main issue is the fact that the post-selection distribution of\nthe data differs from the original distribution. In particular, the observed\ndata is constrained to lie in a subset of the original sample space that is\ndetermined by the selected model. This often makes the post-selection\nlikelihood of the observed data intractable and maximum likelihood inference\ndifficult. In this work, we get around the intractable likelihood by generating\nnoisy unbiased estimates of the post-selection score function and using them in\na stochastic ascent algorithm that yields correct post-selection maximum\nlikelihood estimates. We apply the proposed technique to the problem of\nestimating linear models selected by the lasso. In an asymptotic analysis the\nresulting estimates are shown to be consistent for the selected parameters and\nto have a limiting truncated normal distribution. Confidence intervals\nconstructed based on the asymptotic distribution obtain close to nominal\ncoverage rates in all simulation settings considered, and the point estimates\nare shown to be superior to the lasso estimates when the true model is sparse.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 02:45:59 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 15:57:21 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Meir", "Amit", ""], ["Drton", "Mathias", ""]]}, {"id": "1705.09464", "submitter": "Genevieve Robin", "authors": "Genevi\\`eve Robin and Christophe Ambroise and St\\'ephane Robin", "title": "Incomplete graphical model inference via latent tree aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical network inference is used in many fields such as genomics or\necology to infer the conditional independence structure between variables, from\nmeasurements of gene expression or species abundances for instance. In many\npractical cases, not all variables involved in the network have been observed,\nand the samples are actually drawn from a distribution where some variables\nhave been marginalized out. This challenges the sparsity assumption commonly\nmade in graphical model inference, since marginalization yields locally dense\nstructures, even when the original network is sparse. We present a procedure\nfor inferring Gaussian graphical models when some variables are unobserved,\nthat accounts both for the influence of missing variables and the low density\nof the original network. Our model is based on the aggregation of spanning\ntrees, and the estimation procedure on the Expectation-Maximization algorithm.\nWe treat the graph structure and the unobserved nodes as missing variables and\ncompute posterior probabilities of edge appearance. To provide a complete\nmethodology, we also propose several model selection criteria to estimate the\nnumber of missing nodes. A simulation study and an illustration flow cytometry\ndata reveal that our method has favorable edge detection properties compared to\nexisting graph inference techniques. The methods are implemented in an R\npackage.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 07:36:00 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 17:12:17 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Robin", "Genevi\u00e8ve", ""], ["Ambroise", "Christophe", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1705.09528", "submitter": "Hang Deng", "authors": "Hang Deng and Cun-Hui Zhang", "title": "Beyond Gaussian Approximation: Bootstrap for Maxima of Sums of\n  Independent Random Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bonferroni adjustment, or the union bound, is commonly used to study rate\noptimality properties of statistical methods in high-dimensional problems.\nHowever, in practice, the Bonferroni adjustment is overly conservative. The\nextreme value theory has been proven to provide more accurate multiplicity\nadjustments in a number of settings, but only on ad hoc basis. Recently,\nGaussian approximation has been used to justify bootstrap adjustments in large\nscale simultaneous inference in some general settings when $n \\gg (\\log p)^7$,\nwhere $p$ is the multiplicity of the inference problem and $n$ is the sample\nsize. The thrust of this theory is the validity of the Gaussian approximation\nfor maxima of sums of independent random vectors in high-dimension. In this\npaper, we reduce the sample size requirement to $n \\gg (\\log p)^5$ for the\nconsistency of the empirical bootstrap and the multiplier/wild bootstrap in the\nKolmogorov-Smirnov distance, possibly in the regime where the Gaussian\napproximation is not available. New comparison and anti-concentration theorems,\nwhich are of considerable interest in and of themselves, are developed as\nexisting ones interweaved with Gaussian approximation are no longer applicable.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 11:01:01 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 06:48:49 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Deng", "Hang", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1705.09561", "submitter": "Andr\\'es F. Barrientos", "authors": "Andr\\'es F. Barrientos, Jerome P. Reiter, Ashwin Machanavajjhala, Yan\n  Chen", "title": "Differentially private significance tests for regression coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data producers seek to provide users access to confidential data without\nunduly compromising data subjects' privacy and confidentiality. One general\nstrategy is to require users to do analyses without seeing the confidential\ndata; for example, analysts only get access to synthetic data or query systems\nthat provide disclosure-protected outputs of statistical models. With synthetic\ndata or redacted outputs, the analyst never really knows how much to trust the\nresulting findings. In particular, if the user did the same analysis on the\nconfidential data, would regression coefficients of interest be statistically\nsignificant or not? We present algorithms for assessing this question that\nsatisfy differential privacy. We describe conditions under which the algorithms\nshould give accurate answers about statistical significance. We illustrate the\nproperties of the proposed methods using artificial and genuine data.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:55:36 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 13:57:26 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Barrientos", "Andr\u00e9s F.", ""], ["Reiter", "Jerome P.", ""], ["Machanavajjhala", "Ashwin", ""], ["Chen", "Yan", ""]]}, {"id": "1705.09591", "submitter": "Annie Lee", "authors": "Annie J. Lee, Karen Marder, Helen Mejia-Santana, Avi Orr-Urtreger, Nir\n  Giladi, Susan Bressman, Yuanjia Wang", "title": "Estimation of Genetic Risk Function with Covariates in the Presence of\n  Missing Genotypes", "comments": "16 pages, 5 tables, 4 figures (7 Supplementary pages, 4 Supplementary\n  tables, 13 Supplementary figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetic epidemiological studies, family history data are collected on\nrelatives of study participants and used to estimate the age-specific risk of\ndisease for individuals who carry a causal mutation. However, a family member's\ngenotype data may not be collected due to the high cost of in-person interview\nto obtain blood sample or death of a relative. Previously, efficient\nnonparametric genotype-specific risk estimation in censored mixture data has\nbeen proposed without considering covariates. With multiple predictive risk\nfactors available, risk estimation requires a multivariate model to account for\nadditional covariates that may affect disease risk simultaneously. Therefore,\nit is important to consider the role of covariates in the genotype-specific\ndistribution estimation using family history data. We propose an estimation\nmethod that permits more precise risk prediction by controlling for individual\ncharacteristics and incorporating interaction effects with missing genotypes in\nrelatives, and thus gene-gene interactions and gene-environment interactions\ncan be handled within the framework of a single model. We examine performance\nof the proposed methods by simulations and apply them to estimate the\nage-specific cumulative risk of Parkinson's disease (PD) in carriers of LRRK2\nG2019S mutation using first-degree relatives who are at genetic risk for PD.\nThe utility of estimated carrier risk is demonstrated through designing a\nfuture clinical trial under various assumptions. Such sample size estimation is\nseen in the Huntington's disease literature using the length of abnormal\nexpansion of a CAG repeat in the HTT gene, but is less common in the PD\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:18:39 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Lee", "Annie J.", ""], ["Marder", "Karen", ""], ["Mejia-Santana", "Helen", ""], ["Orr-Urtreger", "Avi", ""], ["Giladi", "Nir", ""], ["Bressman", "Susan", ""], ["Wang", "Yuanjia", ""]]}, {"id": "1705.09599", "submitter": "Zhanfeng Wang", "authors": "Kani Chen, Yuanyuan Lin, Zhanfeng Wang, Zhiliang Ying", "title": "Nearly Semiparametric Efficient Estimation of Quantile Regression", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a competitive alternative to least squares regression, quantile regression\nis popular in analyzing heterogenous data. For quantile regression model\nspecified for one single quantile level $\\tau$, major difficulties of\nsemiparametric efficient estimation are the unavailability of a parametric\nefficient score and the conditional density estimation. In this paper, with the\nhelp of the least favorable submodel technique, we first derive the\nsemiparametric efficient scores for linear quantile regression models that are\nassumed for a single quantile level, multiple quantile levels and all the\nquantile levels in $(0,1)$ respectively. Our main discovery is a one-step\n(nearly) semiparametric efficient estimation for the regression coefficients of\nthe quantile regression models assumed for multiple quantile levels, which has\nseveral advantages: it could be regarded as an optimal way to pool information\nacross multiple/other quantiles for efficiency gain; it is computationally\nfeasible and easy to implement, as the initial estimator is easily available;\ndue to the nature of quantile regression models under investigation, the\nconditional density estimation is straightforward by plugging in an initial\nestimator. The resulting estimator is proved to achieve the corresponding\nsemiparametric efficiency lower bound under regularity conditions. Numerical\nstudies including simulations and an example of birth weight of children\nconfirms that the proposed estimator leads to higher efficiency compared with\nthe Koenker-Bassett quantile regression estimator for all quantiles of\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:35:43 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Chen", "Kani", ""], ["Lin", "Yuanyuan", ""], ["Wang", "Zhanfeng", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1705.09693", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "Multiplicative component models for replicated point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiplicative semiparametric model for the intensity function\nof replicated point processes. Two examples of applications are given: a\ntemporal one, about the dynamics of Internet auctions, and a spatial one, about\nthe spatial distribution of street robberies in Chicago.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 19:36:34 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Gervini", "Daniel", ""]]}, {"id": "1705.09840", "submitter": "Cornelis Potgieter", "authors": "Sudharshan Samaratunga, Cornelis J. Potgieter", "title": "A Split-Sample Approach for Estimating the Stability Index of a Stable\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of stable distributions is used in practice to model data that\nexhibit heavy tails and/or skewness. The stability index $\\alpha$ of a stable\ndistribution is a measure of tail heaviness and is often of primary interest.\nExisting methods for estimating the index parameter include maximum likelihood\nand methods based on the sample quantiles. In this paper, a new approach for\nestimating the index parameter of a stable distribution is proposed. This new\napproach relies on the location-scale family representation of the class of\nstable distributions and involves repeatedly partitioning the single observed\nsample into two independent samples. An asymptotic likelihood method based on\nsample order statistics, previously used for estimating location and scale\nparameters in two independent samples, is adapted for estimating the stability\nindex. The properties of the proposed method of estimation are explored and the\nresulting estimators are evaluated using a simulation study.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 16:43:18 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Samaratunga", "Sudharshan", ""], ["Potgieter", "Cornelis J.", ""]]}, {"id": "1705.09846", "submitter": "Linh Nghiem", "authors": "Linh Nghiem, Cornelis J. Potgieter", "title": "Phase Function Density Deconvolution with Heteroscedastic Measurement\n  Error of Unknown Type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to properly correct for measurement error when estimating\ndensity functions associated with biomedical variables. These estimators that\nadjust for measurement error are broadly referred to as density deconvolution\nestimators. While most methods in the literature assume the distribution of the\nmeasurement error to be fully known, a recently proposed method based on the\nempirical phase function (EPF) can deal with the situation when the measurement\nerror distribution is unknown. The EPF density estimator has only been\nconsidered in the context of additive and homoscedastic measurement error;\nhowever, the measurement error of many biomedical variables is heteroscedastic\nin nature. In this paper, we developed a phase function approach for density\ndeconvolution when the measurement error has unknown distribution and is\nheteroscedastic. A weighted empirical phase function (WEPF) is proposed where\nthe weights are used to adjust for heteroscedasticity of measurement error. The\nasymptotic properties of the WEPF estimator are evaluated. Simulation results\nshow that the weighting can result in large decreases in mean integrated\nsquared error (MISE) when estimating the phase function. The estimation of the\nweights from replicate observations is also discussed. Finally, the\nconstruction of a deconvolution density estimator using the WEPF is compared to\nan existing deconvolution estimator that adjusts for heteroscedasticity, but\nassumes the measurement error distribution to be fully known. The WEPF\nestimator proves to be competitive, especially when considering that it relies\non the minimal assumption of the distribution of measurement error.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 17:29:40 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 06:46:09 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 01:28:38 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Nghiem", "Linh", ""], ["Potgieter", "Cornelis J.", ""]]}, {"id": "1705.10013", "submitter": "Sungkyu Jung", "authors": "Byungwon Kim, Stephan Huckemann, J\\\"orn Schulz and Sungkyu Jung", "title": "Small sphere distributions for directional data with application to\n  medical imaging", "comments": null, "journal-ref": null, "doi": "10.1111/sjos.12381", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new small-sphere distributional families for modeling multivariate\ndirectional data on $(\\mathbb{S}^{p-1})^K$ for $p \\ge 3$ and $K \\ge 1$. In a\nspecial case of univariate directions in $\\Re^3$, the new densities model\nrandom directions on $\\mathbb{S}^2$ with a tendency to vary along a small\ncircle on the sphere, and with a unique mode on the small circle. The proposed\nmultivariate densities enable us to model association among multivariate\ndirections, and are useful in medical imaging, where multivariate directions\nare used to represent shape and shape changes of 3-dimensional objects. When\nthe underlying objects are rotationally deformed under noise, for instance,\ntwisted and/or bend, corresponding directions tend to follow the proposed\nsmall-sphere distributions. The proposed models have several advantages over\nother methods analyzing small-circle-concentrated data, including inference\nprocedures on the association and small-circle fitting. We demonstrate the use\nof the proposed multivariate small-sphere distributions in analyses of\nskeletally-represented object shapes and human knee gait data.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 02:11:11 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Kim", "Byungwon", ""], ["Huckemann", "Stephan", ""], ["Schulz", "J\u00f6rn", ""], ["Jung", "Sungkyu", ""]]}, {"id": "1705.10061", "submitter": "Bruno Sudret", "authors": "R. Sch\\\"obi and B. Sudret", "title": "Global sensitivity analysis in the context of imprecise probabilities\n  (p-boxes) using sparse polynomial chaos expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-007", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis aims at determining which uncertain input\nparameters of a computational model primarily drives the variance of the output\nquantities of interest. Sobol' indices are now routinely applied in this\ncontext when the input parameters are modelled by classical probability theory\nusing random variables. In many practical applications however, input\nparameters are affected by both aleatory and epistemic (so-called polymorphic)\nuncertainty, for which imprecise probability representations have become\npopular in the last decade. In this paper, we consider that the uncertain input\nparameters are modelled by parametric probability boxes (p-boxes). We propose\ninterval-valued (so-called imprecise) Sobol' indices as an extension of their\nclassical definition. An original algorithm based on the concepts of augmented\nspace, isoprobabilistic transforms and sparse polynomial chaos expansions is\ndevised to allow for the computation of these imprecise Sobol' indices at\nextremely low cost. In particular, phantoms points are introduced to build an\nexperimental design in the augmented space (necessary for the calibration of\nthe sparse PCE) which leads to a smart reuse of runs of the original\ncomputational model. The approach is illustrated on three analytical and\nengineering examples which allows one to validate the proposed algorithms\nagainst brute-force double-loop Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 07:55:25 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Sch\u00f6bi", "R.", ""], ["Sudret", "B.", ""]]}, {"id": "1705.10063", "submitter": "Yukun Liu", "authors": "Jiahua Chen and Yukun Liu", "title": "Small Area Quantile Estimation", "comments": "26 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample surveys are widely used to obtain information about totals, means,\nmedians, and other parameters of finite populations. In many applications,\nsimilar information is desired for subpopulations such as individuals in\nspecific geographic areas and socio-demographic groups. When the surveys are\nconducted at national or similarly high levels, a probability sampling can\nresult in just a few sampling units from many unplanned subpopulations at the\ndesign stage. Cost considerations may also lead to low sample sizes from\nindividual small areas. Estimating the parameters of these subpopulations with\nsatisfactory precision and evaluating their accuracy are serious challenges for\nstatisticians. To overcome the difficulties, statisticians resort to pooling\ninformation across the small areas via suitable model assumptions,\nadministrative archives, and census data. In this paper, we develop an array of\nsmall area quantile estimators. The novelty is the introduction of a\nsemiparametric density ratio model for the error distribution in the unit-level\nnested error regression model. In contrast, the existing methods are usually\nmost effective when the response values are jointly normal. We also propose a\nresampling procedure for estimating the mean square errors of these estimators.\nSimulation results indicate that the new methods have superior performance when\nthe population distributions are skewed and remain competitive otherwise.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 07:59:17 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Chen", "Jiahua", ""], ["Liu", "Yukun", ""]]}, {"id": "1705.10082", "submitter": "Val\\'erie Chavez-Demoulin", "authors": "Marc-Olivier Boldi and Val\\'erie Chavez-Demoulin", "title": "Improving the local scoring algorithm using gradient sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the gradient sampling algorithm to the local scoring algorithm to\nsolve complex estimation problems based on an optimization of an objective\nfunction. This overcomes non-differentiability and non-smoothness of the\nobjective function. The new algorithm estimates the Clarke generalized\nsubgradient used in the local scoring, thus reducing numerical instabilities.\nThe method is applied to quantile regression and to the peaks-over-threshold\nmethod, as two examples. Real applications are provided for a retail store and\ntemperature data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 09:06:50 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Boldi", "Marc-Olivier", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""]]}, {"id": "1705.10220", "submitter": "Yuhao Wang", "authors": "Yuhao Wang, Liam Solus, Karren Dai Yang and Caroline Uhler", "title": "Permutation-based Causal Inference Algorithms with Interventions", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, 2017", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning directed acyclic graphs using both observational and interventional\ndata is now a fundamentally important problem due to recent technological\ndevelopments in genomics that generate such single-cell gene expression data at\na very large scale. In order to utilize this data for learning gene regulatory\nnetworks, efficient and reliable causal inference algorithms are needed that\ncan make use of both observational and interventional data. In this paper, we\npresent two algorithms of this type and prove that both are consistent under\nthe faithfulness assumption. These algorithms are interventional adaptations of\nthe Greedy SP algorithm and are the first algorithms using both observational\nand interventional data with consistency guarantees. Moreover, these algorithms\nhave the advantage that they are nonparametric, which makes them useful also\nfor analyzing non-Gaussian data. In this paper, we present these two algorithms\nand their consistency guarantees, and we analyze their performance on simulated\ndata, protein signaling data, and single-cell gene expression data.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:45:02 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 03:31:57 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Wang", "Yuhao", ""], ["Solus", "Liam", ""], ["Yang", "Karren Dai", ""], ["Uhler", "Caroline", ""]]}, {"id": "1705.10298", "submitter": "Jean-Gabriel  Young", "authors": "Jean-Gabriel Young, Giovanni Petri, Francesco Vaccarino, Alice Patania", "title": "Construction of and efficient sampling from the simplicial configuration\n  model", "comments": "6 pages, 4 figures", "journal-ref": "Phys. Rev. E 96, 032312 (2017)", "doi": "10.1103/PhysRevE.96.032312", "report-no": null, "categories": "physics.soc-ph math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplicial complexes are now a popular alternative to networks when it comes\nto describing the structure of complex systems, primarily because they encode\nmulti-node interactions explicitly. With this new description comes the need\nfor principled null models that allow for easy comparison with empirical data.\nWe propose a natural candidate, the simplicial configuration model. The core of\nour contribution is an efficient and uniform Markov chain Monte Carlo sampler\nfor this model. We demonstrate its usefulness in a short case study by\ninvestigating the topology of three real systems and their randomized\ncounterparts (using their Betti numbers). For two out of three systems, the\nmodel allows us to reject the hypothesis that there is no organization beyond\nthe local scale.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 17:34:00 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 19:06:37 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Young", "Jean-Gabriel", ""], ["Petri", "Giovanni", ""], ["Vaccarino", "Francesco", ""], ["Patania", "Alice", ""]]}, {"id": "1705.10310", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Mevin B. Hooten and Devin S. Johnson", "title": "Imputation Approaches for Animal Movement Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of telemetry data is common in animal ecological studies. While\nthe collection of telemetry data for individual animals has improved\ndramatically, the methods to properly account for inherent uncertainties (e.g.,\nmeasurement error, dependence, barriers to movement) have lagged behind. Still,\nmany new statistical approaches have been developed to infer unknown quantities\naffecting animal movement or predict movement based on telemetry data.\nHierarchical statistical models are useful to account for some of the\naforementioned uncertainties, as well as provide population-level inference,\nbut they often come with an increased computational burden. For certain types\nof statistical models, it is straightforward to provide inference if the latent\ntrue animal trajectory is known, but challenging otherwise. In these cases,\napproaches related to multiple imputation have been employed to account for the\nuncertainty associated with our knowledge of the latent trajectory. Despite the\nincreasing use of imputation approaches for modeling animal movement, the\ngeneral sensitivity and accuracy of these methods have not been explored in\ndetail. We provide an introduction to animal movement modeling and describe how\nimputation approaches may be helpful for certain types of models. We also\nassess the performance of imputation approaches in a simulation study. Our\nsimulation study suggests that inference for model parameters directly related\nto the location of an individual may be more accurate than inference for\nparameters associated with higher-order processes such as velocity or\nacceleration. Finally, we apply these methods to analyze a telemetry data set\ninvolving northern fur seals (Callorhinus ursinus) in the Bering Sea.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:36:23 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 14:56:34 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Scharf", "Henry R.", ""], ["Hooten", "Mevin B.", ""], ["Johnson", "Devin S.", ""]]}, {"id": "1705.10370", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke and Fan Yang", "title": "Covariate Assisted Variable Ranking", "comments": "43 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a linear model $y = X \\beta + z$, $z \\sim N(0, \\sigma^2 I_n)$. The\nGram matrix $\\Theta = \\frac{1}{n} X'X$ is non-sparse, but it is approximately\nthe sum of two components, a low-rank matrix and a sparse matrix, where neither\ncomponent is known to us. We are interested in the Rare/Weak signal setting\nwhere all but a small fraction of the entries of $\\beta$ are nonzero, and the\nnonzero entries are relatively small individually. The goal is to rank the\nvariables in a way so as to maximize the area under the ROC curve.\n  We propose Factor-adjusted Covariate Assisted Ranking (FA-CAR) as a two-step\napproach to variable ranking. In the FA-step, we use PCA to reduce the linear\nmodel to a new one where the Gram matrix is approximately sparse. In the\nCAR-step, we rank variables by exploiting the local covariate structures.\n  FA-CAR is easy to use and computationally fast, and it is effective in\nresolving signal cancellation, a challenge we face in regression models. FA-CAR\nis related to the recent idea of Covariate Assisted Screening and Estimation\n(CASE), but two methods are for different goals and are thus very different.\n  We compare the ROC curve of FA-CAR with some other ranking ideas on numerical\nexperiments, and show that FA-CAR has several advantages. Using a Rare/Weak\nsignal model, we derive the convergence rate of the minimum sure-screening\nmodel size of FA-CAR. Our theoretical analysis contains several new\ningredients, especially a new perturbation bound for PCA.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:33:53 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Yang", "Fan", ""]]}, {"id": "1705.10374", "submitter": "Meitner Cadena", "authors": "Meitner Cadena", "title": "Extensions of the Burr Type XII distribution and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burr type XII (BXII) distribution has been largely used in different\nfields due to its great flexibility for fitting data. These applications have\ntypically involved data showing heavy-tailed behaviors. In order to give more\nflexibility to the BXII distribution, in this paper, modifications to this\ndistribution through the use of parametric functions are introduced. For\ninstance, members of this new family of distributions allow the analysis not\nonly of data containing extreme values as the BXII distribution, but also of\nlight-tailed data. We refer to this new family of distributions as the extended\nBurr Type XII distribution (EBXIID) family. Statistical properties of members\nof the EBXIID family are discussed. The maximum likelihood method is proposed\nfor estimating model parameters. The performance of the new family of\ndistributions is studied using simulations. Applications of the new models to\nreal data sets coming from different domains show that models of the EBXIID\nfamily are an alternative to other known distributions.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:41:43 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Cadena", "Meitner", ""]]}, {"id": "1705.10435", "submitter": "Christopher Kovach", "authors": "Christopher K. Kovach, Hiroyuki Oya and Hiroto Kawasaki", "title": "The Bispectrum and Its Relationship to Phase-Amplitude Coupling", "comments": null, "journal-ref": "NeuroImage 173, 2018, 518 - 539", "doi": "10.1016/j.neuroimage.2018.02.033", "report-no": null, "categories": "stat.ME q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most biological signals are non-Gaussian, reflecting their origins in highly\nnonlinear physiological systems. A versatile set of techniques for studying\nnon-Gaussian signals relies on the spectral representations of higher moments,\nknown as polyspectra, which describe forms of cross-frequency dependence that\ndo not arise in time-invariant Gaussian signals. The most commonly used of\nthese employ the bispectrum. Recently, other measures of cross-frequency\ndependence have drawn interest in EEG literature, in particular those which\naddress phase-amplitude coupling (PAC). Here we demonstrate a close\nrelationship between the bispectrum and popular measures of PAC, which we\nrelate to smoothings of the signal bispectrum, making them fundamentally\nbispectral estimators. Viewed this way, however, conventional PAC measures\nexhibit some unfavorable qualities, including poor bias properties, lack of\ncorrect symmetry and artificial constraints on the spectral range and\nresolution of the estimate. Moreover, information obscured by smoothing in\nmeasures of PAC, but preserved in standard bispectral estimators, may be\ncritical for distinguishing nested oscillations from transient signal features\nand other non-oscillatory causes of \"spurious\" PAC. We propose guidelines for\ngauging the nature and origin of cross-frequency coupling with bispectral\nstatistics. Beyond clarifying the relationship between PAC and the bispectrum,\nthe present work lays out a general framework for the interpretation of the\nbispectrum, which extends to other higher-order spectra. In particular, this\nframework holds promise for the detailed identification of signal features\nrelated to both nested oscillations and transient phenomena. We conclude with a\ndiscussion of some broader theoretical implications of this framework and\nhighlight promising directions for future development.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 02:36:54 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 19:18:39 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kovach", "Christopher K.", ""], ["Oya", "Hiroyuki", ""], ["Kawasaki", "Hiroto", ""]]}, {"id": "1705.10440", "submitter": "Robert Kohn", "authors": "Mohamad A. Khaled and Robert Kohn", "title": "On approximating copulas by finite mixtures", "comments": "26 pages and 1 figure and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas are now frequently used to approximate or estimate multivariate\ndistributions because of their ability to take into account the multivariate\ndependence of the variables while controlling the approximation properties of\nthe marginal densities. Copula based multivariate models can often also be more\nparsimonious than fitting a flexible multivariate model, such as a mixture of\nnormals model, directly to the data. However, to be effective, it is imperative\nthat the family of copula models considered is sufficiently flexible. Although\nfinite mixtures of copulas have been used to construct flexible families of\ncopulas, their approximation properties are not well understood and we show\nthat natural candidates such as mixtures of elliptical copulas and mixtures of\nArchimedean copulas cannot approximate a general copula arbitrarily well. Our\narticle develops fundamental tools for approximating a general copula\narbitrarily well by a mixture and proposes a family of finite mixtures that can\ndo so. We illustrate empirically on a financial data set that our approach for\nestimating a copula can be much more parsimonious and results in a better fit\nthan approximating the copula by a mixture of normal copulas.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 03:00:10 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 10:42:10 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Khaled", "Mohamad A.", ""], ["Kohn", "Robert", ""]]}, {"id": "1705.10466", "submitter": "Kohei Chiba", "authors": "Kohei Chiba", "title": "Estimation of the lead-lag parameter between two stochastic processes\n  driven by fractional Brownian motions", "comments": "Completely rewritten; 32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the lead-lag parameter\nbetween two stochastic processes driven by fractional Brownian motions (fBMs)\nof the Hurst parameter greater than 1/2. First we propose a lead-lag model\nbetween two stochastic processes involving fBMs, and then construct a\nconsistent estimator of the lead-lag parameter with possible convergence rate.\nOur estimator has the following two features. Firstly, we can construct the\nlead-lag estimator without using the Hurst parameters of the underlying fBMs.\nSecondly, our estimator can deal with some non-synchronous and irregular\nobservations. We explicitly calculate possible convergence rate when the\nobservation times are (1) synchronous and equidistant, and (2) given by the\nPoisson sampling scheme. We also present numerical simulations of our results\nusing the R package YUIMA.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 06:11:55 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 05:13:56 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Chiba", "Kohei", ""]]}, {"id": "1705.10488", "submitter": "Sabrina Vettori", "authors": "Sabrina Vettori, Rapha\\\"el Huser, Johan Segers and Marc G. Genton", "title": "Bayesian model averaging over tree-based dependence structures for\n  multivariate extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing the complex dependence structure of extreme phenomena is\nparticularly challenging. To tackle this issue we develop a novel statistical\nalgorithm that describes extremal dependence taking advantage of the inherent\nhierarchical dependence structure of the max-stable nested logistic\ndistribution and that identifies possible clusters of extreme variables using\nreversible jump Markov chain Monte Carlo techniques. Parsimonious\nrepresentations are achieved when clusters of extreme variables are found to be\ncompletely independent. Moreover, we significantly decrease the computational\ncomplexity of full likelihood inference by deriving a recursive formula for the\nnested logistic model likelihood. The algorithm performance is verified through\nextensive simulation experiments which also compare different likelihood\nprocedures. The new methodology is used to investigate the dependence\nrelationships between extreme concentration of multiple pollutants in\nCalifornia and how these pollutants are related to extreme weather conditions.\nOverall, we show that our approach allows for the representation of complex\nextremal dependence structures and has valid applications in multivariate data\nanalysis, such as air pollution monitoring, where it can guide policymaking.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:40:52 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 13:07:14 GMT"}, {"version": "v3", "created": "Sun, 22 Jul 2018 22:35:50 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Vettori", "Sabrina", ""], ["Huser", "Rapha\u00ebl", ""], ["Segers", "Johan", ""], ["Genton", "Marc G.", ""]]}, {"id": "1705.10841", "submitter": "Jorge Fernandez-de-Cossio", "authors": "Jorge Fernandez-de-Cossio, Jorge Fernandez-de-Cossio-Diaz, Yasser\n  Perera", "title": "Genetic interactions from first principles", "comments": "19 pages, 4 figures in the main text and 7 other figures", "journal-ref": "Scientific Reports, 10, (2020), 1-16", "doi": "10.1038/s41598-020-78496-8", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a general statistical model of interactions, starting from\nprobabilistic principles and elementary requirements. Prevailing interaction\nmodels in biomedical researches diverge both mathematically and practically. In\nparticular, genetic interaction inquiries are formulated without an obvious\nmathematical unity. Our model reveals theoretical properties unnoticed so far,\nparticularly valuable for genetic interaction mapping, where mechanistic\ndetails are mostly unknown, distribution of gene variants differ between\npopulations, and genetic susceptibilities are spuriously propagated by linkage\ndisequilibrium. When applied to data of the largest interaction mapping\nexperiment on Saccharomyces Cerevisiae to date, our results imply less aversion\nto positive interactions, detection of well-documented hubs and partial\nremapping of functional regions of the currently known genetic interaction\nlandscape. Assessment of divergent annotations across functional categories\nfurther suggests that positive interactions have a more important role on\nribosome biogenesis than previously realized. The unity of arguments elaborated\nhere enables the analysis of dissimilar interaction models and experimental\ndata with a common framework.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 19:48:47 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 14:20:02 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Fernandez-de-Cossio", "Jorge", ""], ["Fernandez-de-Cossio-Diaz", "Jorge", ""], ["Perera", "Yasser", ""]]}, {"id": "1705.11073", "submitter": "Cachimo Assane", "authors": "Cachimo Assane and Basilio Pereira and Carlos Pereira", "title": "Bayesian significance test for discriminating between survival\n  distributions", "comments": "18 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An evaluation of FBST, Fully Bayesian Significance Test, restricted to\nsurvival models is the main objective of the present paper. A Survival\ndistribution should be chosen among the tree celebrated ones, lognormal, gamma,\nand Weibull. For this discrimination, a linear mixture of the three\ndistributions, for which the mixture weights are defined by a Dirichlet\ndistribution of order three, is an important tool: the FBST is used to test the\nhypotheses defined on the mixture weights space. Another feature of the paper\nis that all three distributions are reparametrized in that all the six\nparameters - two for each distribution - are written as functions of the mean\nand the variance of the population been studied. Note that the three\ndistributions share the same two parameters in the mixture model. The mixture\ndensity has then four parameters, the same two for the three discriminating\ndensities and two for the mixture weights. Some numerical results from\nsimulations with some right-censored data are considered. The\nlognormal-gamma-Weibull model is also applied to a real study with dataset\nbeing composed by patient's survival times of patients in the end-stage of\nchronic kidney failure subjected to hemodialysis procedures; data from Rio de\nJaneiro hospitals. The posterior density of the weights indicates an order of\nthe mixture weights and the FBST is used for discriminating between the three\nsurvival distributions.\n  Keywords: Model choice; Separate Models; Survival distributions; Mixture\nmodel; Significance test; FBST\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 13:10:53 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Assane", "Cachimo", ""], ["Pereira", "Basilio", ""], ["Pereira", "Carlos", ""]]}, {"id": "1705.11140", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Scott W. Linderman and Rajesh Ranganath and\n  David M. Blei", "title": "Variational Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advances in large scale probabilistic inference rely on\nvariational methods. The success of variational approaches depends on (i)\nformulating a flexible parametric family of distributions, and (ii) optimizing\nthe parameters to find the member of this family that most closely approximates\nthe exact posterior. In this paper we present a new approximating family of\ndistributions, the variational sequential Monte Carlo (VSMC) family, and show\nhow to optimize it in variational inference. VSMC melds variational inference\n(VI) and sequential Monte Carlo (SMC), providing practitioners with flexible,\naccurate, and powerful Bayesian inference. The VSMC family is a variational\nfamily that can approximate the posterior arbitrarily well, while still\nallowing for efficient optimization of its parameters. We demonstrate its\nutility on state space models, stochastic volatility models for financial data,\nand deep Markov models of brain neural circuits.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 15:23:42 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 08:58:41 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Linderman", "Scott W.", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}]