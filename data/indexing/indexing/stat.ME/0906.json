[{"id": "0906.0060", "submitter": "Minas Gjoka", "authors": "Minas Gjoka, Maciej Kurant, Carter T. Butts, Athina Markopoulou", "title": "A Walk in Facebook: Uniform Sampling of Users in Online Social Networks", "comments": "published in IEEE INFOCOM '10; IEEE Journal on Selected Areas in\n  Communications (JSAC), Special Issue on Measurement of Internet Topologies,\n  2011 under the title \"Practical Recommendations on Crawling Online Social\n  Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NI physics.data-an physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal in this paper is to develop a practical framework for obtaining a\nuniform sample of users in an online social network (OSN) by crawling its\nsocial graph. Such a sample allows to estimate any user property and some\ntopological properties as well. To this end, first, we consider and compare\nseveral candidate crawling techniques. Two approaches that can produce\napproximately uniform samples are the Metropolis-Hasting random walk (MHRW) and\na re-weighted random walk (RWRW). Both have pros and cons, which we demonstrate\nthrough a comparison to each other as well as to the \"ground truth.\" In\ncontrast, using Breadth-First-Search (BFS) or an unadjusted Random Walk (RW)\nleads to substantially biased results. Second, and in addition to offline\nperformance assessment, we introduce online formal convergence diagnostics to\nassess sample quality during the data collection process. We show how these\ndiagnostics can be used to effectively determine when a random walk sample is\nof adequate size and quality. Third, as a case study, we apply the above\nmethods to Facebook and we collect the first, to the best of our knowledge,\nrepresentative sample of Facebook users. We make it publicly available and\nemploy it to characterize several key properties of Facebook.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2009 05:26:36 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2009 05:25:50 GMT"}, {"version": "v3", "created": "Tue, 12 Oct 2010 23:47:24 GMT"}, {"version": "v4", "created": "Fri, 4 Feb 2011 08:19:03 GMT"}, {"version": "v5", "created": "Fri, 17 Jun 2011 05:37:43 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Gjoka", "Minas", ""], ["Kurant", "Maciej", ""], ["Butts", "Carter T.", ""], ["Markopoulou", "Athina", ""]]}, {"id": "0906.0062", "submitter": "Yannick Deville", "authors": "Yannick Deville (1), Alain Deville (2) ((1) Laboratoire\n  d'Astrophysique de Toulouse-Tarbes, Universite de Toulouse, CNRS, Toulouse,\n  France, (2) IM2NP, Universite de Provence, Marseille, France)", "title": "Effect of indirect dependencies on \"Maximum likelihood blind separation\n  of two quantum states (qubits) with cylindrical-symmetry Heisenberg spin\n  coupling\"", "comments": "6 pages, 0 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous paper [1], we investigated the Blind Source Separation (BSS)\nproblem, for the nonlinear mixing model that we introduced in that paper. We\nproposed to solve this problem by using a maximum likelihood (ML) approach.\nWhen applying the ML approach to BSS problems, one usually determines the\nanalytical expressions of the derivatives of the log-likelihood with respect to\nthe parameters of the considered mixing model. In the literature, these\ncalculations were mainly considered for linear mixtures up to now. They are\nmore complex for nonlinear mixtures, due to dependencies between the considered\nquantities. Moreover, the notations commonly employed by the BSS community in\nsuch calculations may become misleading when using them for nonlinear mixtures,\ndue to the above-mentioned dependencies. In this document, we therefore explain\nthis phenomenon, by showing the effect of indirect dependencies on the\napplication of the ML approach to the mixing model considered in [1]. This\nyields the explicit expression of the complete derivative of the log-likelihood\nassociated to that mixing model.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2009 15:20:13 GMT"}], "update_date": "2009-06-02", "authors_parsed": [["Deville", "Yannick", ""], ["Deville", "Alain", ""]]}, {"id": "0906.0113", "submitter": "Alexandre Patriota", "authors": "Alexandre G. Patriota", "title": "A note on Influence diagnostics in nonlinear mixed-effects elliptical\n  models", "comments": "Paper submitted for possible publication, 6 pages", "journal-ref": null, "doi": "10.1016/j.csda.2010.06.007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides general matrix formulas for computing the score function,\nthe (expected and observed) Fisher information and the $\\Delta$ matrices\n(required for the assessment of local influence) for a quite general model\nwhich includes the one proposed by Russo et al. (2009). Additionally, we also\npresent an expression for the generalized leverage. The matrix formulation has\na considerable advantage, since although the complexity of the postulated\nmodel, all general formulas are compact, clear and have nice forms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 15:18:05 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2009 21:44:28 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2009 18:49:41 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Patriota", "Alexandre G.", ""]]}, {"id": "0906.0177", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis and Raymond Molzon", "title": "Optimal-order bounds on the rate of convergence to normality in the\n  multivariate delta method", "comments": "Version 5: Besides previously given applications to Pearson's,\n  non-central Student's and Hotelling's statistics, added now are applications\n  to sphericity test statistics, a regularized canonical correlation, and\n  maximum likelihood estimators. The title and abstract have changed", "journal-ref": "Electron. J. Stat., 10 (1):1001--1063 (2016)", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform and nonuniform Berry--Esseen (BE) bounds of optimal orders on the\ncloseness to normality for general abstract nonlinear statistics are given,\nwhich are then used to obtain optimal bounds on the rate of convergence in the\ndelta method for vector statistics. Specific applications to Pearson's,\nnon-central Student's and Hotelling's statistics, sphericity test statistics, a\nregularized canonical correlation, and maximum likelihood estimators (MLEs) are\ngiven; all these uniform and nonuniform BE bounds appear to be the first known\nresults of these kinds, except for uniform BE bounds for MLEs. When applied to\nthe well-studied case of the central Student statistic, our general results\ncompare well with known ones in that case, obtained previously by specialized\nmethods. The proofs use a Stein-type method developed by Chen and Shao, a\nCram\\'er-type of tilt transform, exponential and Rosenthal-type inequalities\nfor sums of random vectors established by Pinelis, Sakhanenko, and Utev, as\nwell as a number of other, quite recent results motivated by this study. The\nmethod allows one to obtain bounds with explicit and rather moderate-size\nconstants, at least as far as the uniform bounds are concerned. For instance,\none has the uniform BE bound\n$3.61\\mathbb{E}(Y_1^6+Z_1^6)\\,(1+\\sigma^{-3})/\\sqrt n$ for the Pearson sample\ncorrelation coefficient based on independent identically distributed random\npairs $(Y_1,Z_1),\\dots,(Y_n,Z_n)$ with $\\mathbb{E} Y_1=\\mathbb{E}\nZ_1=\\mathbb{E} Y_1Z_1=0$ and $\\mathbb{E} Y_1^2=\\mathbb{E} Z_1^2=1$, where\n$\\sigma:=\\sqrt{\\mathbb{E} Y_1^2Z_1^2}$.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2009 18:09:54 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2011 15:49:57 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2012 01:28:15 GMT"}, {"version": "v4", "created": "Sun, 24 Mar 2013 19:56:20 GMT"}, {"version": "v5", "created": "Sun, 17 Jan 2016 03:01:12 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Pinelis", "Iosif", ""], ["Molzon", "Raymond", ""]]}, {"id": "0906.0434", "submitter": "Heng Lian", "authors": "Aditya Chopra, Heng Lian", "title": "Total Variation, Adaptive Total Variation and Nonconvex Smoothly Clipped\n  Absolute Deviation Penalty for Denoising Blocky Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation-based image denoising model has been generalized and\nextended in numerous ways, improving its performance in different contexts. We\npropose a new penalty function motivated by the recent progress in the\nstatistical literature on high-dimensional variable selection. Using a\nparticular instantiation of the majorization-minimization algorithm, the\noptimization problem can be efficiently solved and the computational procedure\nrealized is similar to the spatially adaptive total variation model. Our\ntwo-pixel image model shows theoretically that the new penalty function solves\nthe bias problem inherent in the total variation model. The superior\nperformance of the new penalty is demonstrated through several experiments. Our\ninvestigation is limited to \"blocky\" images which have small total variation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2009 07:34:27 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Chopra", "Aditya", ""], ["Lian", "Heng", ""]]}, {"id": "0906.0593", "submitter": "Stephane Chretien", "authors": "Stephane Chretien", "title": "On the modified Basis Pursuit reconstruction for Compressed Sensing with\n  partially known support", "comments": "Withdrawn due to an error in the proof. A new version will be\n  submitted as a section in a future paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this short note is to present a refined analysis of the modified\nBasis Pursuit ($\\ell_1$-minimization) approach to signal recovery in Compressed\nSensing with partially known support, as introduced by Vaswani and Lu. The\nproblem is to recover a signal $x \\in \\mathbb R^p$ using an observation vector\n$y=Ax$, where $A \\in \\mathbb R^{n\\times p}$ and in the highly underdetermined\nsetting $n\\ll p$. Based on an initial and possibly erroneous guess $T$ of the\nsignal's support ${\\rm supp}(x)$, the Modified Basis Pursuit method of Vaswani\nand Lu consists of minimizing the $\\ell_1$ norm of the estimate over the\nindices indexed by $T^c$ only. We prove exact recovery essentially under a\nRestricted Isometry Property assumption of order 2 times the cardinal of $T^c\n\\cap {\\rm supp}(x)$, i.e. the number of missed components.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2009 20:20:09 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2012 18:01:29 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2015 09:23:35 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Chretien", "Stephane", ""]]}, {"id": "0906.0708", "submitter": "Joris L. van Velsen", "authors": "J. L. van Velsen", "title": "A corrected AIC for the selection of seemingly unrelated regressions\n  models", "comments": "9 pages including 1 figure and 3 tables; v2: revtex4, typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bias correction to Akaike's information criterion (AIC) is derived for\nseemingly unrelated regressions models. The correction is of particular use\nwhen the sample size is not much larger than the number of fitted parameters. A\nsmall-sample simulation study indicates that the bias-corrected AIC (AICc)\nprovides better model choices than other model selection criteria.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2009 13:55:17 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2009 16:24:57 GMT"}], "update_date": "2009-06-05", "authors_parsed": [["van Velsen", "J. L.", ""]]}, {"id": "0906.1266", "submitter": "Michael Mayer", "authors": "Michael Mayer", "title": "U-Quantile-Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1948, W. Hoeffding introduced a large class of unbiased estimators called\nU-statistics, defined as the average value of a real-valued m-variate function\nh calculated at all possible sets of m points from a random sample. In the\npresent paper, we investigate the corresponding robust analogue which we call\nU-quantile-statistics. We are concerned with the asymptotic behavior of the\nsample p-quantile of such function h instead of its average. Alternatively,\nU-quantile-statistics can be viewed as quantile estimators for a certain class\nof dependent random variables. Examples are given by a slightly modified\nHodges-Lehmann estimator of location and the median interpoint distance among\nrandom points in space.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2009 11:33:53 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Mayer", "Michael", ""]]}, {"id": "0906.1710", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "The S-Estimator in Change-Point Random Model with Long Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers two-phase random design linear regression models. The\nerrors and the regressors are stationary long-range dependent Gaussian. The\nregression parameters, the scale parameters and the change-point are estimated\nusing a method introduced by Rousseeuw and Yohai(1984). This is called\nS-estimator and it has the property that is more robust than the classical\nestimators; the outliers don't spoil the estimation results. Some asymptotic\nresults, including the strong consistency and the convergence rate of the\nS-estimators, are proved.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2009 12:29:13 GMT"}], "update_date": "2009-06-10", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "0906.2098", "submitter": "Giovanni M. Marchetti", "authors": "Giovanni M. Marchetti, Monia Lupparelli", "title": "Chain graph models of multivariate regression type for categorical data", "comments": "Published in at http://dx.doi.org/10.3150/10-BEJ300 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2011, Vol. 17, No. 3, 827-844", "doi": "10.3150/10-BEJ300", "report-no": "IMS-BEJ-BEJ300", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a class of chain graph models for categorical variables defined by\nwhat we call a multivariate regression chain graph Markov property. First, the\nset of local independencies of these models is shown to be Markov equivalent to\nthose of a chain graph model recently defined in the literature. Next we\nprovide a parametrization based on a sequence of generalized linear models with\na multivariate logistic link function that captures all independence\nconstraints in any chain graph model of this kind.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2009 13:12:28 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2010 08:33:01 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2011 11:52:31 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Marchetti", "Giovanni M.", ""], ["Lupparelli", "Monia", ""]]}, {"id": "0906.2234", "submitter": "Zhongyang Zhang", "authors": "Zhongyang Zhang, Kenneth Lange, Roel Ophoff, Chiara Sabatti", "title": "Reconstructing DNA copy number by penalized estimation and imputation", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS357 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 4, 1749-1773", "doi": "10.1214/10-AOAS357", "report-no": "IMS-AOAS-AOAS357", "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in genomics have underscored the surprising ubiquity of DNA\ncopy number variation (CNV). Fortunately, modern genotyping platforms also\ndetect CNVs with fairly high reliability. Hidden Markov models and algorithms\nhave played a dominant role in the interpretation of CNV data. Here we explore\nCNV reconstruction via estimation with a fused-lasso penalty as suggested by\nTibshirani and Wang [Biostatistics 9 (2008) 18--29]. We mount a fresh attack on\nthis difficult optimization problem by the following: (a) changing the penalty\nterms slightly by substituting a smooth approximation to the absolute value\nfunction, (b) designing and implementing a new MM (majorization--minimization)\nalgorithm, and (c) applying a fast version of Newton's method to jointly update\nall model parameters. Together these changes enable us to minimize the\nfused-lasso criterion in a highly effective way. We also reframe the\nreconstruction problem in terms of imputation via discrete optimization. This\napproach is easier and more accurate than parameter estimation because it\nrelies on the fact that only a handful of possible copy number states exist at\neach SNP. The dynamic programming framework has the added bonus of exploiting\ninformation that the current fused-lasso approach ignores. The accuracy of our\nimputations is comparable to that of hidden Markov models at a substantially\nlower computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2009 00:49:35 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2009 18:36:12 GMT"}, {"version": "v3", "created": "Wed, 5 May 2010 22:57:42 GMT"}, {"version": "v4", "created": "Fri, 7 May 2010 00:31:35 GMT"}, {"version": "v5", "created": "Mon, 10 Jan 2011 07:41:29 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Zhang", "Zhongyang", ""], ["Lange", "Kenneth", ""], ["Ophoff", "Roel", ""], ["Sabatti", "Chiara", ""]]}, {"id": "0906.2691", "submitter": "Alice Whittemore", "authors": "Alice S. Whittemore", "title": "Evaluating Health Risk Models", "comments": "17 pages, 3 tables, and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Interest in targeted disease prevention has stimulated development of models\nthat assign risks to individuals, using their personal covariates. We need to\nevaluate these models, and to quantify the gains achieved by expanding a model\nwith additional covariates. We describe several performance measures for risk\nmodels, and show how they are related. Application of the measures to risk\nmodels for hypothetical populations and for postmenopausal US women illustrate\nseveral points. First, model performance is constrained by the distribution of\ntrue risks in the population. This complicates the comparison of two models if\nthey are applied to populations with different covariate distributions. Second,\nthe Brier Score and the Integrated Discrimination Improvement (IDI) are more\nuseful than the concordance statistic for quantifying precision gains obtained\nfrom model expansion. Finally, these precision gains are apt to be small,\nalthough they may be large for some individuals. We propose a new way to\nidentify these individuals, and show how to quantify how much they gain by\nmeasuring the additional covariates. Those with largest gains could be targeted\nfor cost-efficient covariate assessment.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2009 21:29:45 GMT"}], "update_date": "2009-06-16", "authors_parsed": [["Whittemore", "Alice S.", ""]]}, {"id": "0906.2885", "submitter": "Umberto Amato", "authors": "Umberto Amato, Anestis Antoniadis, Alexander Samarov, Alexander\n  Tsybakov", "title": "Noisy Independent Factor Analysis Model for Density Estimation and\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multivariate density estimation when the unknown\ndensity is assumed to follow a particular form of dimensionality reduction, a\nnoisy independent factor analysis (IFA) model. In this model the data are\ngenerated by a number of latent independent components having unknown\ndistributions and are observed in Gaussian noise. We do not assume that either\nthe number of components or the matrix mixing the components are known. We show\nthat the densities of this form can be estimated with a fast rate. Using the\nmirror averaging aggregation algorithm, we construct a density estimator which\nachieves a nearly parametric rate log^(1/4)n/sqrt(n), independent of the\ndimensionality of the data, as the sample size $n$ tends to infinity. This\nestimator is adaptive to the number of components, their distributions and the\nmixing matrix. We then apply this density estimator to construct nonparametric\nplug-in classifiers and show that they achieve the best obtainable rate of the\nexcess Bayes risk, to within a logarithmic factor independent of the dimension\nof the data. Applications of this classifier to simulated data sets and to real\ndata from a remote sensing experiment show promising results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2009 10:59:35 GMT"}], "update_date": "2009-06-17", "authors_parsed": [["Amato", "Umberto", ""], ["Antoniadis", "Anestis", ""], ["Samarov", "Alexander", ""], ["Tsybakov", "Alexander", ""]]}, {"id": "0906.2999", "submitter": "Elena Kulinskaya", "authors": "Elena Kulinskaya, Michael B. Dollinger and Kirsten Bj{\\o}rkest{\\o}l", "title": "Testing for Homogeneity in Meta-Analysis I. The One Parameter Case:\n  Standardized Mean Difference", "comments": "42 pages, 6 figures. v2: major revision. Added simulations for a\n  chi-square approximation based on the estimated mean of Q. This approximation\n  is recommended as preferrable to a two-moment gamma approximation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis seeks to combine the results of several experiments in order to\nimprove the accuracy of decisions. It is common to use a test for homogeneity\nto determine if the results of the several experiments are sufficiently similar\nto warrant their combination into an overall result. Cochran's Q statistic is\nfrequently used for this homogeneity test. It is often assumed that Q follows a\nchi-square distribution under the null hypothesis of homogeneity, but it has\nlong been known that this asymptotic distribution for Q is not accurate for\nmoderate sample sizes. Here we present formulas for the mean and variance of Q\nunder the null hypothesis which represent O(1/n) corrections to the\ncorresponding chi-square moments in the one parameter case. The formulas are\nfairly complicated, and so we provide a program (available at\nhttp://www.imperial.ac.uk/stathelp/researchprojects/metaanalysis) for making\nthe necessary calculations. We apply the results to the standardized mean\ndifference (Cohen's d-statistic) and consider two approximations: a gamma\ndistribution with estimated shape and scale parameters and the chi-square\ndistribution with fractional degrees of freedom equal to the estimated mean of\nQ. We recommend the latter distribution as an approximate distribution for Q to\nuse for testing the null hypothesis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2009 19:06:22 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2009 19:30:04 GMT"}], "update_date": "2009-08-01", "authors_parsed": [["Kulinskaya", "Elena", ""], ["Dollinger", "Michael B.", ""], ["Bj\u00f8rkest\u00f8l", "Kirsten", ""]]}, {"id": "0906.3090", "submitter": "Patrick Perry", "authors": "Patrick O. Perry, Patrick J. Wolfe", "title": "Minimax rank estimation for subspace tracking", "comments": "10 pages, 4 figures; final version", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp.\n  504-513, 2010", "doi": "10.1109/JSTSP.2010.2048070", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank estimation is a classical model order selection problem that arises in a\nvariety of important statistical signal and array processing systems, yet is\naddressed relatively infrequently in the extant literature. Here we present\nsample covariance asymptotics stemming from random matrix theory, and bring\nthem to bear on the problem of optimal rank estimation in the context of the\nstandard array observation model with additive white Gaussian noise. The most\nsignificant of these results demonstrates the existence of a phase transition\nthreshold, below which eigenvalues and associated eigenvectors of the sample\ncovariance fail to provide any information on population eigenvalues. We then\ndevelop a decision-theoretic rank estimation framework that leads to a simple\nordered selection rule based on thresholding; in contrast to competing\napproaches, however, it admits asymptotic minimax optimality and is free of\ntuning parameters. We analyze the asymptotic performance of our rank selection\nprocedure and conclude with a brief simulation study demonstrating its\npractical efficacy in the context of subspace tracking.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 07:41:36 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2009 22:53:40 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2010 15:38:56 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Perry", "Patrick O.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0906.3204", "submitter": "Marloes Maathuis", "authors": "Peter B\\\"uhlmann, Markus Kalisch and Marloes H. Maathuis", "title": "Variable selection in high-dimensional linear models: partially faithful\n  distributions and the PC-simple algorithm", "comments": "20 pages, 3 figures", "journal-ref": "Biometrika 2010, Vol. 97, No. 2, 261-278", "doi": "10.1093/biomet/asq008", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider variable selection in high-dimensional linear models where the\nnumber of covariates greatly exceeds the sample size. We introduce the new\nconcept of partial faithfulness and use it to infer associations between the\ncovariates and the response. Under partial faithfulness, we develop a\nsimplified version of the PC algorithm (Spirtes et al., 2000), the PC-simple\nalgorithm, which is computationally feasible even with thousands of covariates\nand provides consistent variable selection under conditions on the random\ndesign matrix that are of a different nature than coherence conditions for\npenalty-based approaches like the Lasso. Simulations and application to real\ndata show that our method is competitive compared to penalty-based approaches.\nWe provide an efficient implementation of the algorithm in the R-package pcalg.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 15:03:20 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2009 20:23:24 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2009 10:36:59 GMT"}], "update_date": "2012-01-12", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["Kalisch", "Markus", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "0906.3259", "submitter": "Roberto Fontana", "authors": "Roberto Fontana and Giovanni Pistone", "title": "Generation of Fractional Factorial Designs", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint use of counting functions, Hilbert basis and Markov basis allows to\ndefine a procedure to generate all the fractions that satisfy a given set of\nconstraints in terms of orthogonality. The general case of mixed level designs,\nwithout restrictions on the number of levels of each factor (like primes or\npower of primes) is studied. This new methodology has been experimented on some\nsignificant classes of fractional factorial designs, including mixed level\northogonal arrays.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 17:26:32 GMT"}], "update_date": "2009-06-18", "authors_parsed": [["Fontana", "Roberto", ""], ["Pistone", "Giovanni", ""]]}, {"id": "0906.3465", "submitter": "Genevera I. Allen", "authors": "Genevera I. Allen, Robert Tibshirani", "title": "Transposable regularized covariance models with an application to\n  missing data imputation", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS314 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 764-790", "doi": "10.1214/09-AOAS314", "report-no": "IMS-AOAS-AOAS314", "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data estimation is an important challenge with high-dimensional data\narranged in the form of a matrix. Typically this data matrix is transposable,\nmeaning that either the rows, columns or both can be treated as features. To\nmodel transposable data, we present a modification of the matrix-variate\nnormal, the mean-restricted matrix-variate normal, in which the rows and\ncolumns each have a separate mean vector and covariance matrix. By placing\nadditive penalties on the inverse covariance matrices of the rows and columns,\nthese so-called transposable regularized covariance models allow for maximum\nlikelihood estimation of the mean and nonsingular covariance matrices. Using\nthese models, we formulate EM-type algorithms for missing data imputation in\nboth the multivariate and transposable frameworks. We present theoretical\nresults exploiting the structure of our transposable models that allow these\nmodels and imputation methods to be applied to high-dimensional data.\nSimulations and results on microarray data and the Netflix data show that these\nimputation techniques often outperform existing methods and offer a greater\ndegree of flexibility.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2009 17:42:31 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2009 20:21:11 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2009 18:09:36 GMT"}, {"version": "v4", "created": "Tue, 9 Nov 2010 07:31:12 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Allen", "Genevera I.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "0906.3501", "submitter": "Debashis Paul", "authors": "Debashis Paul, Jie Peng, Prabir Burman", "title": "Semiparametric modeling of autonomous nonlinear dynamical systems with\n  applications", "comments": "43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semi-parametric model for autonomous nonlinear\ndynamical systems and devise an estimation procedure for model fitting. This\nmodel incorporates subject-specific effects and can be viewed as a nonlinear\nsemi-parametric mixed effects model. We also propose a computationally\nefficient model selection procedure. We prove consistency of the proposed\nestimator under suitable regularity conditions. We show by simulation studies\nthat the proposed estimation as well as model selection procedures can\nefficiently handle sparse and noisy measurements. Finally, we apply the\nproposed method to a plant growth data used to study growth displacement rates\nwithin meristems of maize roots under two different experimental conditions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2009 18:44:27 GMT"}], "update_date": "2009-06-19", "authors_parsed": [["Paul", "Debashis", ""], ["Peng", "Jie", ""], ["Burman", "Prabir", ""]]}, {"id": "0906.3662", "submitter": "Martin A. Lindquist", "authors": "Martin A. Lindquist", "title": "The Statistical Analysis of fMRI Data", "comments": "Published in at http://dx.doi.org/10.1214/09-STS282 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 4, 439-464", "doi": "10.1214/09-STS282", "report-no": "IMS-STS-STS282", "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been explosive growth in the number of neuroimaging\nstudies performed using functional Magnetic Resonance Imaging (fMRI). The field\nthat has grown around the acquisition and analysis of fMRI data is\nintrinsically interdisciplinary in nature and involves contributions from\nresearchers in neuroscience, psychology, physics and statistics, among others.\nA standard fMRI study gives rise to massive amounts of noisy data with a\ncomplicated spatio-temporal correlation structure. Statistics plays a crucial\nrole in understanding the nature of the data and obtaining relevant results\nthat can be used and interpreted by neuroscientists. In this paper we discuss\nthe analysis of fMRI data, from the initial acquisition of the raw data to its\nuse in locating brain activity, making inference about brain connectivity and\npredictions about psychological or disease states. Along the way, we illustrate\ninteresting and important issues where statistics already plays a crucial role.\nWe also seek to illustrate areas where statistics has perhaps been\nunderutilized and will have an increased role in the future.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2009 13:09:51 GMT"}], "update_date": "2009-06-22", "authors_parsed": [["Lindquist", "Martin A.", ""]]}, {"id": "0906.3935", "submitter": "Anthony C. Davison", "authors": "Alessandra R. Brazzale, Anthony C. Davison", "title": "Accurate Parametric Inference for Small Samples", "comments": "Published in at http://dx.doi.org/10.1214/08-STS273 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 4, 465-484", "doi": "10.1214/08-STS273", "report-no": "IMS-STS-STS273", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline how modern likelihood theory, which provides essentially exact\ninferences in a variety of parametric statistical problems, may routinely be\napplied in practice. Although the likelihood procedures are based on analytical\nasymptotic approximations, the focus of this paper is not on theory but on\nimplementation and applications. Numerical illustrations are given for logistic\nregression, nonlinear models, and linear non-normal models, and we describe a\nsampling approach for the third of these classes. In the case of logistic\nregression, we argue that approximations are often more appropriate than\n`exact' procedures, even when these exist.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 07:18:12 GMT"}], "update_date": "2009-06-23", "authors_parsed": [["Brazzale", "Alessandra R.", ""], ["Davison", "Anthony C.", ""]]}, {"id": "0906.3953", "submitter": "R. Dennis Cook", "authors": "R. Dennis Cook, Liliana Forzani", "title": "Principal Fitted Components for Dimension Reduction in Regression", "comments": "Published in at http://dx.doi.org/10.1214/08-STS275 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 4, 485-501", "doi": "10.1214/08-STS275", "report-no": "IMS-STS-STS275", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a remedy for two concerns that have dogged the use of principal\ncomponents in regression: (i) principal components are computed from the\npredictors alone and do not make apparent use of the response, and (ii)\nprincipal components are not invariant or equivariant under full rank linear\ntransformation of the predictors. The development begins with principal fitted\ncomponents [Cook, R. D. (2007). Fisher lecture: Dimension reduction in\nregression (with discussion). Statist. Sci. 22 1--26] and uses normal models\nfor the inverse regression of the predictors on the response to gain reductive\ninformation for the forward regression of interest. This approach includes\nmethodology for testing hypotheses about the number of components and about\nconditional independencies among the predictors.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 08:42:24 GMT"}], "update_date": "2009-06-23", "authors_parsed": [["Cook", "R. Dennis", ""], ["Forzani", "Liliana", ""]]}, {"id": "0906.3979", "submitter": "Michael Friendly", "authors": "Michael Friendly", "title": "The Golden Age of Statistical Graphics", "comments": "Published in at http://dx.doi.org/10.1214/08-STS268 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 4, 502-535", "doi": "10.1214/08-STS268", "report-no": "IMS-STS-STS268", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical graphics and data visualization have long histories, but their\nmodern forms began only in the early 1800s. Between roughly 1850 and 1900\n($\\pm10$), an explosive growth occurred in both the general use of graphic\nmethods and the range of topics to which they were applied. Innovations were\nprodigious and some of the most exquisite graphics ever produced appeared,\nresulting in what may be called the ``Golden Age of Statistical Graphics.'' In\nthis article I trace the origins of this period in terms of the infrastructure\nrequired to produce this explosive growth: recognition of the importance of\nsystematic data collection by the state; the rise of statistical theory and\nstatistical thinking; enabling developments of technology; and inventions of\nnovel methods to portray statistical data. To illustrate, I describe some\nspecific contributions that give rise to the appellation ``Golden Age.''\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 14:37:57 GMT"}], "update_date": "2009-06-23", "authors_parsed": [["Friendly", "Michael", ""]]}, {"id": "0906.4013", "submitter": "Nicholas I. Fisher", "authors": "Nicholas I. Fisher, Willem R. van Zwet", "title": "Remembering Wassily Hoeffding", "comments": "Published in at http://dx.doi.org/10.1214/08-STS271 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 4, 536-547", "doi": "10.1214/08-STS271", "report-no": "IMS-STS-STS271", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasssily Hoeffding's terminal illness and untimely death in 1991 put an end\nto efforts that were made to interview him for Statistical Science. An account\nof his scientific work is given in Fisher and Sen [The Collected Works of\nWassily Hoeffding (1994) Springer], but the present authors felt that the\nstatistical community should also be told about the life of this remarkable\nman. He contributed much to statistical science, but will also live on in the\nmemory of those who knew him as a kind and modest teacher and friend, whose\ncourage and learning were matched by a wonderful sense of humor.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 14:12:47 GMT"}], "update_date": "2009-06-23", "authors_parsed": [["Fisher", "Nicholas I.", ""], ["van Zwet", "Willem R.", ""]]}, {"id": "0906.4165", "submitter": "Michael J. Schell", "authors": "Malay Ghosh, Michael J. Schell", "title": "A Conversation with Pranab Kumar Sen", "comments": "Published in at http://dx.doi.org/10.1214/08-STS255 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 4, 548-564", "doi": "10.1214/08-STS255", "report-no": "IMS-STS-STS255", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pranab Kumar Sen was born on November 7, 1937 in Calcutta, India. His father\ndied when Pranab was 10 years old, so his mother raised the family of seven\nchildren. Given his superior performance on an exam, Pranab nearly went into\nmedical school, but did not because he was underage. He received a B.Sc. degree\nin 1955 and an M.Sc. degree in 1957 in statistics from Calcutta University,\ntopping the class both times. Dr. Sen's dissertation on order statistics and\nnonparametrics, under the direction of Professor Hari Kinkar Nandi, was\ncompleted in 1961. After teaching for three years at Calcutta University,\n1961--1964, Professor Sen came to Berkeley as a Visiting Assistant Professor in\n1964. In 1965, he joined the Departments of Statistics and Biostatistics at the\nUniversity of North Carolina at Chapel Hill, where he has remained. Professor\nSen's pioneering contributions have touched nearly every area of statistics. He\nis the first person who, in joint collaboration with Professor S. K.\nChatterjee, developed multivariate rank tests as well as time-sequential\nnonparametric methods. He is also the first person who carried out in-depth\nresearch in sequential nonparametrics culminating in his now famous Wiley book\nSequential Nonparametrics: Invariance Principles and Statistical Inference and\nSIAM monograph. Professor Sen has over 600 research publications. In addition,\nhe has authored or co-authored 11 books and monographs, and has edited or\nco-edited 11 more volumes. He has supervised over 80 Ph.D. students, many of\nwhom have achieved distinction both nationally and internationally. Professor\nSen is the founding co-editor of two international journals: Sequential\nAnalysis and Statistics and Decisions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 05:46:23 GMT"}], "update_date": "2009-06-24", "authors_parsed": [["Ghosh", "Malay", ""], ["Schell", "Michael J.", ""]]}, {"id": "0906.4329", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama", "title": "A Bayes factor with reasonable model selection consistency for ANOVA\n  model", "comments": "a major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the ANOVA model, we propose a new g-prior based Bayes factor without\nintegral representation, with reasonable model selection consistency for any\nasymptotic situations (either number of levels of the factor and/or number of\nreplication in each level goes to infinity). Exact analytic calculation of the\nmarginal density under a special choice of the priors enables such a Bayes\nfactor.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 19:10:34 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2012 10:58:40 GMT"}], "update_date": "2012-06-14", "authors_parsed": [["Maruyama", "Yuzo", ""]]}, {"id": "0906.4754", "submitter": "Nicolas Dobigeon", "authors": "Nicolas Dobigeon, Said Moussaoui, Jean-Yves Tourneret and Cedric\n  Carteret", "title": "Bayesian separation of spectral sources under non-negativity and full\n  additivity constraints", "comments": "v4: minor grammatical changes; Signal Processing, 2009", "journal-ref": "Signal Processing, vol. 89, no. 12, pp. 2657-2669, Dec. 2009", "doi": "10.1016/j.sigpro.2009.05.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of separating spectral sources which are\nlinearly mixed with unknown proportions. The main difficulty of the problem is\nto ensure the full additivity (sum-to-one) of the mixing coefficients and\nnon-negativity of sources and mixing coefficients. A Bayesian estimation\napproach based on Gamma priors was recently proposed to handle the\nnon-negativity constraints in a linear mixture model. However, incorporating\nthe full additivity constraint requires further developments. This paper\nstudies a new hierarchical Bayesian model appropriate to the non-negativity and\nsum-to-one constraints associated to the regressors and regression coefficients\nof linear mixtures. The estimation of the unknown parameters of this model is\nperformed using samples generated using an appropriate Gibbs sampler. The\nperformance of the proposed algorithm is evaluated through simulation results\nconducted on synthetic mixture models. The proposed approach is also applied to\nthe processing of multicomponent chemical mixtures resulting from Raman\nspectroscopy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2009 17:38:32 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2009 21:19:34 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2009 20:30:19 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2009 07:58:17 GMT"}], "update_date": "2010-08-30", "authors_parsed": [["Dobigeon", "Nicolas", ""], ["Moussaoui", "Said", ""], ["Tourneret", "Jean-Yves", ""], ["Carteret", "Cedric", ""]]}, {"id": "0906.4823", "submitter": "De Tao Mao", "authors": "DeTao Mao and Wenyuan Li", "title": "A Bounded Derivative Method for the Maximum Likelihood Estimation on\n  Weibull Parameters", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the basic maximum likelihood estimating function of the two parameters\nWeibull distribution, a simple proof on its global monotonicity is given to\nensure the existence and uniqueness of its solution. The boundary of the\nfunction's first-order derivative is defined based on its scale-free property.\nWith a bounded derivative, the possible range of the root of this function can\nbe determined. A novel root-finding algorithm employing these established\nresults is proposed accordingly, its convergence is proved analytically as\nwell. Compared with other typical algorithms for this problem, the efficiency\nof the proposed algorithm is also demonstrated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2009 00:47:16 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2009 06:47:18 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2009 18:49:12 GMT"}], "update_date": "2009-10-04", "authors_parsed": [["Mao", "DeTao", ""], ["Li", "Wenyuan", ""]]}, {"id": "0906.4980", "submitter": "Patrick J. Wolfe", "authors": "Benjamin P. Olding and Patrick J. Wolfe", "title": "Inference for graphs and networks: Extending classical tools to modern\n  data", "comments": "16 pages, 6 figures; submitted for publication", "journal-ref": null, "doi": "10.1142/9781783263752_0001", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs and networks provide a canonical representation of relational data,\nwith massive network data sets becoming increasingly prevalent across a variety\nof scientific fields. Although tools from mathematics and computer science have\nbeen eagerly adopted by practitioners in the service of network inference, they\ndo not yet comprise a unified and coherent framework for the statistical\nanalysis of large-scale network data. This paper serves as both an introduction\nto the topic and a first step toward formal inference procedures. We develop\nand illustrate our arguments using the example of hypothesis testing for\nnetwork structure. We invoke a generalized likelihood ratio framework and use\nit to highlight the growing number of topics in this area that require strong\ncontributions from statistical science. We frame our discussion in the context\nof previous work from across a variety of disciplines, and conclude by\noutlining fundamental statistical challenges whose solutions will in turn serve\nto advance the science of network inference.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2009 17:20:17 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Olding", "Benjamin P.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0906.5179", "submitter": "Xiaofeng Shao", "authors": "Xiaofeng Shao", "title": "Testing for white noise under unknown dependence and its applications to\n  goodness-of-fit for time series models", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for white noise has been well studied in the literature of\neconometrics and statistics. For most of the proposed test statistics, such as\nthe well-known Box-Pierce's test statistic with fixed lag truncation number,\nthe asymptotic null distributions are obtained under independent and\nidentically distributed assumptions and may not be valid for the dependent\nwhite noise. Due to recent popularity of conditional heteroscedastic models\n(e.g., GARCH models), which imply nonlinear dependence with zero\nautocorrelation, there is a need to understand the asymptotic properties of the\nexisting test statistics under unknown dependence. In this paper, we showed\nthat the asymptotic null distribution of Box-Pierce's test statistic with\ngeneral weights still holds under unknown weak dependence so long as the lag\ntruncation number grows at an appropriate rate with increasing sample size.\nFurther applications to diagnostic checking of the ARMA and FARIMA models with\ndependent white noise errors are also addressed. Our results go beyond earlier\nones by allowing non-Gaussian and conditional heteroscedastic errors in the\nARMA and FARIMA models and provide theoretical support for some empirical\nfindings reported in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 16:29:53 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Shao", "Xiaofeng", ""]]}, {"id": "0906.5190", "submitter": "Tong Zhang", "authors": "Kai Yu, Tong Zhang", "title": "High Dimensional Nonlinear Learning using Local Coordinate Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method for semi-supervised learning on high\ndimensional nonlinear manifolds, which includes a phase of unsupervised basis\nlearning and a phase of supervised function learning. The learned bases provide\na set of anchor points to form a local coordinate system, such that each data\npoint $x$ on the manifold can be locally approximated by a linear combination\nof its nearby anchor points, with the linear weights offering a\nlocal-coordinate coding of $x$. We show that a high dimensional nonlinear\nfunction can be approximated by a global linear function with respect to this\ncoding scheme, and the approximation quality is ensured by the locality of such\ncoding. The method turns a difficult nonlinear learning problem into a simple\nglobal linear learning problem, which overcomes some drawbacks of traditional\nlocal learning methods. The work also gives a theoretical justification to the\nempirical success of some biologically-inspired models using sparse coding of\nsensory data, since a local coding scheme must be sufficiently sparse. However,\nsparsity does not always satisfy locality conditions, and can thus possibly\nlead to suboptimal results. The properties and performances of the method are\nempirically verified on synthetic data, handwritten digit classification, and\nobject recognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 01:22:09 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Yu", "Kai", ""], ["Zhang", "Tong", ""]]}, {"id": "0906.5436", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan, Carey E. Priebe, John C. Wierman", "title": "Relative Density of the Random r-Factor Proximity Catch Digraph for\n  Testing Spatial Patterns of Segregation and Association (Technical Report)", "comments": "65 pages, 45 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report # 644 (at Department of Applied Mathematics and\n  Statistics, The Johns Hopkins University, Baltimore, MD, 21218)", "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical pattern classification methods based on data-random graphs were\nintroduced recently. In this approach, a random directed graph is constructed\nfrom the data using the relative positions of the data points from various\nclasses. Different random graphs result from different definitions of the\nproximity region associated with each data point and different graph statistics\ncan be employed for data reduction. The approach used in this article is based\non a parameterized family of proximity maps determining an associated family of\ndata-random digraphs. The relative arc density of the digraph is used as the\nsummary statistic, providing an alternative to the domination number employed\npreviously. An important advantage of the relative arc density is that,\nproperly re-scaled, it is a U-statistic, facilitating analytic study of its\nasymptotic distribution using standard U-statistic central limit theory. The\napproach is illustrated with an application to the testing of spatial patterns\nof segregation and association. Knowledge of the asymptotic distribution allows\nevaluation of the Pitman and Hodges-Lehmann asymptotic efficacy, and selection\nof the proximity map parameter to optimize efficacy. Notice that the approach\npresented here also has the advantage of validity for data in any dimension.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2009 08:43:35 GMT"}], "update_date": "2009-07-01", "authors_parsed": [["Ceyhan", "Elvan", ""], ["Priebe", "Carey E.", ""], ["Wierman", "John C.", ""]]}, {"id": "0906.5481", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "Relative Edge Density of the Underlying Graphs Based on\n  Proportional-Edge Proximity Catch Digraphs for Testing Bivariate Spatial\n  Patterns (Technical Report)", "comments": "63 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-09-5", "categories": "math.ST math.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of data-random graphs in statistical testing of spatial patterns is\nintroduced recently. In this approach, a random directed graph is constructed\nfrom the data using the relative positions of the points from various classes.\nDifferent random graphs result from different definitions of the proximity\nregion associated with each data point and different graph statistics can be\nemployed for pattern testing. The approach used in this article is based on\nunderlying graphs of a family of data-random digraphs which is determined by a\nfamily of parameterized proximity maps. The relative edge density of the AND-\nand OR-underlying graphs is used as the summary statistic, providing an\nalternative to the relative arc density and domination number of the digraph\nemployed previously. Properly scaled, relative edge density of the underlying\ngraphs is a U-statistic, facilitating analytic study of its asymptotic\ndistribution using standard U-statistic central limit theory. The approach is\nillustrated with an application to the testing of bivariate spatial clustering\npatterns of segregation and association. Knowledge of the asymptotic\ndistribution allows evaluation of the Pitman asymptotic efficiency, hence\nselection of the proximity map parameter to optimize efficiency. Asymptotic\nefficiency and Monte Carlo simulation analysis indicate that the AND-underlying\nversion is better (in terms of power and efficiency) for the segregation\nalternative, while the OR-underlying version is better for the association\nalternative. The approach presented here is also valid for data in higher\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2009 11:16:31 GMT"}], "update_date": "2009-07-01", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "0906.5546", "submitter": "Palaniappan Vellaisamy", "authors": "P. Vellaisamy", "title": "A-Collapsibility of Distribution Dependence and Quantile Regression\n  Coefficients", "comments": "The paper has fifteen pages and has been accepted for publication in\n  Scandinavian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Yule-Simpson paradox notes that an association between random variables\ncan be reversed when averaged over a background variable. Cox and Wermuth\n(2003) introduced the concept of distribution dependence between two random\nvariables X and Y, and developed two dependence conditions, each of which\nguarantees that reversal cannot occur. Ma, Xie and Geng (2006) studied the\ncollapsibility of distribution dependence over a background variable W, under a\nrather strong homogeneity condition. Collapsibility ensures the association\nremains the same for conditional and marginal models, so that Yule-Simpson\nreversal cannot occur. In this paper, we investigate a more general condition\nfor avoiding effect reversal: A-collapsibility. The conditions of Cox and\nWermuth imply A-collapsibility, without assuming homogeneity. In fact, we show\nthat, when W is a binary variable, collapsibility is equivalent to\nA-collapsibility plus homogeneity, and A-collapsibility is equivalent to the\nconditions of Cox and Wermuth. Recently, Cox (2007) extended Cochran's result\non regression coefficients of conditional and marginal models, to quantile\nregression coefficients. The conditions of Cox and Wermuth are sufficient for\nA-collapsibility of quantile regression coefficients. If the conditional\ndistribution of W, given Y = y and X = x, belong to one-dimensional natural\nexponential family, they are also necessary. Some applications of\nA-collapsibility include the analysis of a contingency table, linear regression\nmodels and quantile regression models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2009 14:39:32 GMT"}, {"version": "v2", "created": "Fri, 20 Aug 2010 09:43:16 GMT"}, {"version": "v3", "created": "Tue, 8 Feb 2011 07:55:26 GMT"}, {"version": "v4", "created": "Sat, 8 Oct 2011 14:34:43 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Vellaisamy", "P.", ""]]}, {"id": "0906.5609", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer and Matthew J. Francis", "title": "Entropic Priors and Bayesian Model Selection", "comments": "Presented at MaxEnt 2009, the 29th International Workshop on Bayesian\n  Inference and Maximum Entropy Methods in Science and Engineering (July 5-10,\n  2009, Oxford, Mississippi, USA)", "journal-ref": "AIP Conf.Proc.1193:179-186,2009", "doi": "10.1063/1.3275612", "report-no": null, "categories": "physics.data-an astro-ph.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We demonstrate that the principle of maximum relative entropy (ME), used\njudiciously, can ease the specification of priors in model selection problems.\nThe resulting effect is that models that make sharp predictions are\ndisfavoured, weakening the usual Bayesian \"Occam's Razor\". This is illustrated\nwith a simple example involving what Jaynes called a \"sure thing\" hypothesis.\nJaynes' resolution of the situation involved introducing a large number of\nalternative \"sure thing\" hypotheses that were possible before we observed the\ndata. However, in more complex situations, it may not be possible to explicitly\nenumerate large numbers of alternatives. The entropic priors formalism produces\nthe desired result without modifying the hypothesis space or requiring explicit\nenumeration of alternatives; all that is required is a good model for the prior\npredictive distribution for the data. This idea is illustrated with a simple\nrigged-lottery example, and we outline how this idea may help to resolve a\nrecent debate amongst cosmologists: is dark energy a cosmological constant, or\nhas it evolved with time in some way? And how shall we decide, when the data\nare in?\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2009 19:11:25 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2009 20:56:06 GMT"}], "update_date": "2009-12-07", "authors_parsed": [["Brewer", "Brendon J.", ""], ["Francis", "Matthew J.", ""]]}]