[{"id": "1510.00071", "submitter": "Diam Ba", "authors": "Diam Ba, Cheikh Tidiane Seck and Gane Samb Lo", "title": "Asymptotic confidence bands for copulas based on the local linear kernel\n  estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish asymptotic simultaneous confidence bands for\ncopulas based on the local linear kernel estimator proposed by Chen and Huang\n[1]. For this, we prove under smoothness conditions on the copula function, a\nuniform in bandwidth law of the iterated logarithm for the maximal deviation of\nthis estimator from its expectation. We also show that the bias term converges\nuniformly to zero with a precise rate. The performance of these bands is\nillustrated in a simulation study. An application based on pseudo-panel data is\nalso provided for modeling dependence.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 23:11:21 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Ba", "Diam", ""], ["Seck", "Cheikh Tidiane", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1510.00084", "submitter": "Xiangyu Wang", "authors": "Binyan Jiang, Xiangyu Wang, and Chenlei Leng", "title": "A Direct Approach for Sparse Quadratic Discriminant Analysis", "comments": "Updated to the JMLR format", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic discriminant analysis (QDA) is a standard tool for classification\ndue to its simplicity and flexibility. Because the number of its parameters\nscales quadratically with the number of the variables, QDA is not practical,\nhowever, when the dimensionality is relatively large. To address this, we\npropose a novel procedure named DA-QDA for QDA in analyzing high-dimensional\ndata. Formulated in a simple and coherent framework, DA-QDA aims to directly\nestimate the key quantities in the Bayes discriminant function including\nquadratic interactions and a linear index of the variables for classification.\nUnder appropriate sparsity assumptions, we establish consistency results for\nestimating the interactions and the linear index, and further demonstrate that\nthe misclassification rate of our procedure converges to the optimal Bayes\nrisk, even when the dimensionality is exponentially high with respect to the\nsample size. An efficient algorithm based on the alternating direction method\nof multipliers (ADMM) is developed for finding interactions, which is much\nfaster than its competitor in the literature. The promising performance of\nDA-QDA is illustrated via extensive simulation studies and the analysis of four\nreal datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 01:24:20 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 05:57:40 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 05:00:10 GMT"}, {"version": "v4", "created": "Wed, 5 Sep 2018 08:36:22 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Jiang", "Binyan", ""], ["Wang", "Xiangyu", ""], ["Leng", "Chenlei", ""]]}, {"id": "1510.00094", "submitter": "Ben Sherwood", "authors": "Ben Sherwood", "title": "Variable Selection for Additive Partial Linear Quantile Regression with\n  Missing Covariates", "comments": "36 pages 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard quantile regression model assumes a linear relationship at the\nquantile of interest and that all variables are observed. We relax these\nassumptions by considering a partial linear model while allowing for missing\nlinear covariates. To handle the potential bias caused by missing data we\npropose a weighted objective function using inverse probability weighting. Our\nwork examines estimators using parametric and nonparametric estimates of the\nmissing probability. For variable selection of the linear terms in the presence\nof missing data we consider a penalized and weighted objective function using\nthe non-convex penalties MCP or SCAD. Under standard conditions we demonstrate\nthat the penalized estimator has the oracle property including cases where\n$p>>n$. Theoretical challenges include handling missing data and partial linear\nmodels while working with a nonsmooth loss function and a non-convex penalty\nfunction. The performance of the method is evaluated using Monte Carlo\nsimulations and our methods are applied to model amount of time sober for\npatients leaving a rehabilitation center.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 02:27:39 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 01:41:13 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Sherwood", "Ben", ""]]}, {"id": "1510.00097", "submitter": "Jianfeng Yao", "authors": "Zhaoyuan Li and Jianfeng Yao", "title": "Testing for Heteroscedasticity in High-dimensional Regressions", "comments": "45 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing heteroscedasticity of the errors is a major challenge in\nhigh-dimensional regressions where the number of covariates is large compared\nto the sample size. Traditional procedures such as the White and the\nBreusch-Pagan tests typically suffer from low sizes and powers. This paper\nproposes two new test procedures based on standard OLS residuals. Using the\ntheory of random Haar orthogonal matrices, the asymptotic normality of both\ntest statistics is obtained under the null when the degree of freedom tends to\ninfinity. This encompasses both the classical low-dimensional setting where the\nnumber of variables is fixed while the sample size tends to infinity, and the\nproportional high-dimensional setting where these dimensions grow to infinity\nproportionally. These procedures thus offer a wide coverage of dimensions in\napplications. To our best knowledge, this is the first procedures in the\nliterature for testing heteroscedasticity which are valid for medium and\nhigh-dimensional regressions. The superiority of our proposed tests over the\nexisting methods are demonstrated by extensive simulations and by several real\ndata analyses as well.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 03:02:33 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 07:24:28 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 06:57:05 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Li", "Zhaoyuan", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1510.00112", "submitter": "James Dowty", "authors": "James G. Dowty", "title": "Higher-order asymptotics for the parametric complexity", "comments": "Version 3: Fixed a minor error in the introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parametric complexity is the key quantity in the minimum description\nlength (MDL) approach to statistical model selection. Rissanen and others have\nshown that the parametric complexity of a statistical model approaches a simple\nfunction of the Fisher information volume of the model as the sample size $n$\ngoes to infinity. This paper derives higher-order asymptotic expansions for the\nparametric complexity, in the case of exponential families and independent and\nidentically distributed data. These higher-order approximations are calculated\nfor some examples and are shown to have better finite-sample behaviour than\nRissanen's approximation. The higher-order terms are given as expressions\ninvolving cumulants (or, more naturally, the Amari-Chentsov tensors), and these\nterms are likely to be interesting in themselves since they arise naturally\nfrom the general information-theoretic principles underpinning MDL. The\nderivation given here specializes to an alternative and arguably simpler proof\nof Rissanen's result (for the case considered here), proving for the first time\nthat his approximation is $O(n^{-1})$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 05:42:25 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 02:41:43 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2015 04:21:33 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Dowty", "James G.", ""]]}, {"id": "1510.00137", "submitter": "Myriam Tami", "authors": "Xavier Bry (UM), Christian Lavergne, Myriam Tami (UM)", "title": "EM estimation of a Structural Equation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new estimation method of a Structural Equation\nModel. Our method is based on the EM likelihood-maximization algorithm. We show\nthat this method provides estimators, not only of the coefficients of the\nmodel, but also of its latent factors. Through a simulation study, we\ninvestigate how fast and accurate the method is, and then apply it to real\nenvironmental data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 08:33:47 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Bry", "Xavier", "", "UM"], ["Lavergne", "Christian", "", "UM"], ["Tami", "Myriam", "", "UM"]]}, {"id": "1510.00186", "submitter": "Peter Thwaites", "authors": "Peter A. Thwaites and Jim Q. Smith", "title": "A New Method for tackling Asymmetric Decision Problems", "comments": "12 pages, 5 figures", "journal-ref": "Proceedings of the 10th Workshop on Uncertainty Processing\n  (WUPES'15), page 179, 2015", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chain Event Graphs are probabilistic graphical models designed especially for\nthe analysis of discrete statistical problems which do not admit a natural\nproduct space structure. We show here how they can be used for decision\nanalysis, and describe an optimal decision strategy based on an efficient local\ncomputation message passing scheme. We briefly describe a method for producing\na parsimonious decision CEG, analogous to the parsimonious ID, and touch upon\nthe CEG-analogues of Shachter's barren node deletion and arc reversal for\nID-based solution.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 11:28:11 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Thwaites", "Peter A.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1510.00292", "submitter": "Sergey Kovalchuk", "authors": "Sergey V. Kovalchuk, Aleksey V. Krikunov, Konstantin V. Knyazkov,\n  Sergey S. Kosukhin, Alexander V. Boukhanovsky", "title": "On Classification Issues within Ensemble-Based Complex System Simulation\n  Tasks", "comments": "To be presented at CCS'15 (http://www.ccs2015.org/)", "journal-ref": null, "doi": "10.1007/s00477-016-1324-5", "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary tasks of complex system simulation are often related to the\nissue of uncertainty management. It comes from the lack of information or\nknowledge about the simulated system as well as from restrictions of the model\nset being used. One of the powerful tools for the uncertainty management is\nensemble-based simulation, which uses variation in input or output data, model\nparameters, or available versions of models to improve the simulation\nperformance. Furthermore the system of models for complex system simulation\n(especially in case of hiring ensemble-based approach) can be considered as a\ncomplex system. As a result, the identification of the complex model's\nstructure and parameters provide additional sources of uncertainty to be\nmanaged. Within the presented work we are developing a conceptual and\ntechnological approach to manage the ensemble-based simulation taking into\naccount changing states of both simulated system and system of models within\nthe ensemble-based approach. The states of these systems are considered as a\nsubject of classification with consequent inference of better strategies for\nensemble evolution over the simulation time and ensemble aggregation. Here the\nensemble evolution enables implementation of dynamic reactive solutions which\ncan automatically conform to the changing states of both systems. The ensemble\naggregation can be considered within a scope of averaging (regression way) or\nselection (classification way, which complement the classification mentioned\nearlier) approach. The technological basis for such approach includes\nensemble-based simulation techniques using domain-specific software combined\nwithin a composite application; data science approaches for analysis of\navailable datasets (simulation data, observations, situation assessment etc.);\nand machine learning algorithms for classes identification, ensemble management\nand knowledge acquisition.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 15:57:16 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 14:01:16 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Kovalchuk", "Sergey V.", ""], ["Krikunov", "Aleksey V.", ""], ["Knyazkov", "Konstantin V.", ""], ["Kosukhin", "Sergey S.", ""], ["Boukhanovsky", "Alexander V.", ""]]}, {"id": "1510.00443", "submitter": "Mauro Ribeiro de Oliveira JR", "authors": "Francisco Louzada and Mauro R. de Oliveira Jr and Fernando F. Moreira", "title": "The zero-inflated promotion cure rate regression model applied to fraud\n  propensity in bank loan applications", "comments": "13 pages, 2 figures, 6 tables. arXiv admin note: text overlap with\n  arXiv:1509.05244", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the promotion cure rate model proposed by Chen et al\n(1999), by incorporating excess of zeros in the modelling. Despite allowing to\nrelate the covariates to the fraction of cure, the current approach, which is\nbased on a biological interpretation of the causes that trigger the event of\ninterest, does not enable to relate the covariates to the fraction of zeros.\nThe presence of zeros in survival data, unusual in medical studies, can\nfrequently occur in banking loan portfolios, as presented in Louzada et al\n(2015), where they deal with propensity to fraud in lending loans in a major\nBrazilian bank. To illustrate the new cure rate survival method, the same real\ndataset analyzed in Louzada et al (2015) is fitted here, and the results are\ncompared.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 22:55:52 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Louzada", "Francisco", ""], ["Oliveira", "Mauro R. de", "Jr"], ["Moreira", "Fernando F.", ""]]}, {"id": "1510.00486", "submitter": "Samuel Gross", "authors": "Samuel M. Gross, Jonathan Taylor, Robert Tibshirani", "title": "A Selective Approach to Internal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common goal in modern biostatistics is to form a biomarker signature from\nhigh dimensional gene expression data that is predictive of some outcome of\ninterest. After learning this biomarker signature, an important question to\nanswer is how well it predicts the response compared to classical predictors.\nThis is challenging, because the biomarker signature is an internal predictor\n-- one that has been learned using the same dataset on which we want to\nevaluate it's significance. We propose a new method for approaching this\nproblem based on the technique of selective inference. Simulations show that\nour method is able to properly control the level of the test, and that in\ncertain settings we have more power than sample splitting.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 04:24:49 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Gross", "Samuel M.", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1510.00551", "submitter": "Adrian O Hagan Dr", "authors": "Adrian O'Hagan, Thomas Brendan Murphy, Luca Scrucca and Isobel Claire\n  Gormley", "title": "Investigation of Parameter Uncertainty in Clustering Using a Gaussian\n  Mixture Model Via Jackknife, Bootstrap and Weighted Likelihood Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are a popular tool in model-based clustering. Such a model is\noften fitted by a procedure that maximizes the likelihood, such as the EM\nalgorithm. At convergence, the maximum likelihood parameter estimates are\ntypically reported, but in most cases little emphasis is placed on the\nvariability associated with these estimates. In part this may be due to the\nfact that standard errors are not directly calculated in the model-fitting\nalgorithm, either because they are not required to fit the model, or because\nthey are difficult to compute. The examination of standard errors in\nmodel-based clustering is therefore typically neglected. The widely used R\npackage mclust has recently introduced bootstrap and weighted likelihood\nbootstrap methods to facilitate standard error estimation. This paper provides\nan empirical comparison of these methods (along with the jackknife method) for\nproducing standard errors and confidence intervals for mixture parameters.\nThese methods are illustrated and contrasted in both a simulation study and in\nthe traditional Old Faithful data set and Thyroid data set.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 10:26:57 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 11:18:50 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 16:21:19 GMT"}, {"version": "v4", "created": "Wed, 28 Feb 2018 15:53:16 GMT"}, {"version": "v5", "created": "Mon, 22 Jul 2019 13:23:43 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["O'Hagan", "Adrian", ""], ["Murphy", "Thomas Brendan", ""], ["Scrucca", "Luca", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "1510.00646", "submitter": "Daniele Durante", "authors": "Daniele Durante, Sally Paganin, Bruno Scarpa, David B. Dunson", "title": "Bayesian modeling of networks in complex business intelligence problems", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series C (2017). 66,\n  555-580", "doi": "10.1111/rssc.12168", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex network data problems are increasingly common in many fields of\napplication. Our motivation is drawn from strategic marketing studies\nmonitoring customer choices of specific products, along with co-subscription\nnetworks encoding multiple purchasing behavior. Data are available for several\nagencies within the same insurance company, and our goal is to efficiently\nexploit co-subscription networks to inform targeted advertising of cross-sell\nstrategies to currently mono-product customers. We address this goal by\ndeveloping a Bayesian hierarchical model, which clusters agencies according to\ncommon mono-product customer choices and co-subscription networks. Within each\ncluster, we efficiently model customer behavior via a cluster-dependent mixture\nof latent eigenmodels. This formulation provides key information on\nmono-product customer choices and multiple purchasing behavior within each\ncluster, informing targeted cross-sell strategies. We develop simple algorithms\nfor tractable inference, and assess performance in simulations and an\napplication to business intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 17:13:33 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 18:05:44 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Paganin", "Sally", ""], ["Scarpa", "Bruno", ""], ["Dunson", "David B.", ""]]}, {"id": "1510.00708", "submitter": "Valerio Varano", "authors": "Valerio Varano, Stefano Gabriele, Luciano Teresi, Ian Dryden, Paolo\n  Emilio Puddu, Concetta Torromeo and Paolo Piras", "title": "Comparing Trajectories on the Size and Shape Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that trajectory shape analysis should be performed only\nafter obtaining a proper representation before applying ordination methods. In\nfact, studying the shape of a trajectory means studying how the deformation\nchanges along each path irrespectively of the actual shape to which these\ndeformations apply. The independence of the deformation from the shape to which\nit is applied is critical: it implies that any shape variation between\nindividuals at the beginning of each trajectories must be completely filtered\nout. A Parallel Transport, that can be based on various connection types, is\nnecessary to perform such kind of shape data centering. The Levi Civita\nconnection can also be used to transport a deformation. We demonstrate that\nthis procedure does not preserve deformation even in the ane case. We propose a\nnovel procedure called Direct Transport able to perfectly transport deformation\nin the ane case and to better approximate non ane deformation in comparison to\nexisting tools.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 20:01:36 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Varano", "Valerio", ""], ["Gabriele", "Stefano", ""], ["Teresi", "Luciano", ""], ["Dryden", "Ian", ""], ["Puddu", "Paolo Emilio", ""], ["Torromeo", "Concetta", ""], ["Piras", "Paolo", ""]]}, {"id": "1510.00775", "submitter": "Vladimir Minin", "authors": "Michael D. Karcher, Julia A. Palacios, Trevor Bedford, Marc A.\n  Suchard, Vladimir N. Minin", "title": "Quantifying and mitigating the effect of preferential sampling on\n  phylodynamic inference", "comments": "30 pages, 7 figures plust 7 appendix figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1004789", "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylodynamics seeks to estimate effective population size fluctuations from\nmolecular sequences of individuals sampled from a population of interest. One\nway to accomplish this task formulates an observed sequence data likelihood\nexploiting a coalescent model for the sampled individuals' genealogy and then\nintegrating over all possible genealogies via Monte Carlo or, less efficiently,\nby conditioning on one genealogy estimated from the sequence data. However,\nwhen analyzing sequences sampled serially through time, current methods\nimplicitly assume either that sampling times are fixed deterministically by the\ndata collection protocol or that their distribution does not depend on the size\nof the population. Through simulation, we first show that, when sampling times\ndo probabilistically depend on effective population size, estimation methods\nmay be systematically biased. To correct for this deficiency, we propose a new\nmodel that explicitly accounts for preferential sampling by modeling the\nsampling times as an inhomogeneous Poisson process dependent on effective\npopulation size. We demonstrate that in the presence of preferential sampling\nour new model not only reduces bias, but also improves estimation precision.\nFinally, we compare the performance of the currently used phylodynamic methods\nwith our proposed model through clinically-relevant, seasonal human influenza\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 04:52:50 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Karcher", "Michael D.", ""], ["Palacios", "Julia A.", ""], ["Bedford", "Trevor", ""], ["Suchard", "Marc A.", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1510.00967", "submitter": "Thibaut Horel", "authors": "Panos Toulis, Thibaut Horel, Edoardo M. Airoldi", "title": "The Proximal Robbins-Monro Method", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for parameter estimation with massive datasets has reinvigorated\ninterest in stochastic optimization and iterative estimation procedures.\nStochastic approximations are at the forefront of this recent development as\nthey yield procedures that are simple, general, and fast. However, standard\nstochastic approximations are often numerically unstable. Deterministic\noptimization, on the other hand, increasingly uses proximal updates to achieve\nnumerical stability in a principled manner. A theoretical gap has thus emerged.\nWhile standard stochastic approximations are subsumed by the framework of\nRobbins and Monro (1951), there is no such framework for stochastic\napproximations with proximal updates. In this paper, we conceptualize a\nproximal version of the classical Robbins-Monro procedure. Our theoretical\nanalysis demonstrates that the proposed procedure has important stability\nbenefits over the classical Robbins-Monro procedure, while it retains the best\nknown convergence rates. Exact implementations of the proximal Robbins-Monro\nprocedure are challenging, but we show that approximate implementations lead to\nprocedures that are easy to implement, and still dominate classical procedures\nby achieving numerical stability, practically without tradeoffs. Moreover,\napproximate proximal Robbins-Monro procedures can be applied even when the\nobjective cannot be calculated analytically, and so they generalize stochastic\nproximal procedures currently in use.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 19:07:41 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 00:37:35 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 03:01:41 GMT"}, {"version": "v4", "created": "Sat, 1 Feb 2020 17:50:22 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Toulis", "Panos", ""], ["Horel", "Thibaut", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1510.01064", "submitter": "Jelena Bradic", "authors": "Alexander Hanbo Li and Jelena Bradic", "title": "Boosting in the presence of outliers: adaptive classification with\n  non-convex loss functions", "comments": null, "journal-ref": "Journal of the American Statistical Association: theory and\n  methods, 2017", "doi": "10.1080/01621459.2016.1273116", "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the role and efficiency of the non-convex loss functions\nfor binary classification problems. In particular, we investigate how to design\na simple and effective boosting algorithm that is robust to the outliers in the\ndata. The analysis of the role of a particular non-convex loss for prediction\naccuracy varies depending on the diminishing tail properties of the gradient of\nthe loss -- the ability of the loss to efficiently adapt to the outlying data,\nthe local convex properties of the loss and the proportion of the contaminated\ndata. In order to use these properties efficiently, we propose a new family of\nnon-convex losses named $\\gamma$-robust losses. Moreover, we present a new\nboosting framework, {\\it Arch Boost}, designed for augmenting the existing work\nsuch that its corresponding classification algorithm is significantly more\nadaptable to the unknown data contamination. Along with the Arch Boosting\nframework, the non-convex losses lead to the new class of boosting algorithms,\nnamed adaptive, robust, boosting (ARB). Furthermore, we present theoretical\nexamples that demonstrate the robustness properties of the proposed algorithms.\nIn particular, we develop a new breakdown point analysis and a new influence\nfunction analysis that demonstrate gains in robustness. Moreover, we present\nnew theoretical results, based only on local curvatures, which may be used to\nestablish statistical and optimization properties of the proposed Arch boosting\nalgorithms with highly non-convex loss functions. Extensive numerical\ncalculations are used to illustrate these theoretical properties and reveal\nadvantages over the existing boosting methods when data exhibits a number of\noutliers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 08:50:56 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Li", "Alexander Hanbo", ""], ["Bradic", "Jelena", ""]]}, {"id": "1510.01103", "submitter": "Fredrik S\\\"avje", "authors": "Michael J. Higgins, Fredrik S\\\"avje, Jasjeet S. Sekhon", "title": "Blocking estimators and inference under the Neyman-Rubin model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the variances of estimators for sample average treatment effects\nunder the Neyman-Rubin potential outcomes model for arbitrary blocking\nassignments and an arbitrary number of treatments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 11:19:50 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Higgins", "Michael J.", ""], ["S\u00e4vje", "Fredrik", ""], ["Sekhon", "Jasjeet S.", ""]]}, {"id": "1510.01675", "submitter": "Nikolaus Schweizer", "authors": "Thomas Kruse, Judith C. Schneider, Nikolaus Schweizer", "title": "What's in a ball? Constructing and characterizing uncertainty sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of model risk, it is well-established to replace classical\nexpected values by worst-case expectations over all models within a fixed\nradius from a given reference model. This is the \"robustness\" approach. We show\nthat previous methods for measuring this radius, e.g. relative entropy or\npolynomial divergences, are inadequate for reference models which are\nmoderately heavy-tailed such as lognormal models. Worst cases are either\ninfinitely pessimistic, or they rule out the possibility of fat-tailed \"power\nlaw\" models as plausible alternatives. We introduce a new family of divergence\nmeasures which captures intermediate levels of pessimism.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 17:38:22 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Kruse", "Thomas", ""], ["Schneider", "Judith C.", ""], ["Schweizer", "Nikolaus", ""]]}, {"id": "1510.01685", "submitter": "Selden Crary", "authors": "Selden Crary and Jan Stormann", "title": "Four-Point, 2D, Free-Ranging, IMSPE-Optimal, Twin-Point Designs", "comments": "17 pages, 8 figures, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the discovery of a set of four-point, two-factor, free-ranging,\nputatively IMSPE-optimal designs with a pair of twin points, in the statistical\ndesign of computer experiments, under Gaussian-process,\nfixed-Gaussian-covariance parameter, and zero-nugget assumptions. We conjecture\nthis is the set of free-ranging, twin-point designs with the smallest number of\ndegrees of freedom.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 18:23:04 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 05:47:05 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 01:58:41 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 03:40:24 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Crary", "Selden", ""], ["Stormann", "Jan", ""]]}, {"id": "1510.01757", "submitter": "Clement de Chaisemartin", "authors": "Clement de Chaisemartin, Xavier D'Haultfoeuille", "title": "Fuzzy Differences-in-Differences", "comments": null, "journal-ref": null, "doi": "10.1093/restud/rdx049", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difference-in-differences (DID) is a method to evaluate the effect of a\ntreatment. In its basic version, a \"control group\" is untreated at two dates,\nwhereas a \"treatment group\" becomes fully treated at the second date. However,\nin many applications of the DID method, the treatment rate only increases more\nin the treatment group. In such fuzzy designs, a popular estimator of treatment\neffects is the DID of the outcome divided by the DID of the treatment. We show\nthat this ratio identifies a local average treatment effect only if two\nhomogeneous treatment effect assumptions are satisfied. We then propose two\nalternative estimands that do not rely on any assumption on treatment effects,\nand that can be used when the treatment rate does not change over time in the\ncontrol group. We prove that the corresponding estimators are asymptotically\nnormal. Finally, we use our results to revisit Duflo (2001).\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 20:47:31 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 11:03:20 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2015 15:55:42 GMT"}, {"version": "v4", "created": "Thu, 16 Feb 2017 01:10:18 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["de Chaisemartin", "Clement", ""], ["D'Haultfoeuille", "Xavier", ""]]}, {"id": "1510.01770", "submitter": "Stijn Vansteelandt", "authors": "Stijn Vansteelandt and Vanessa Didelez", "title": "Robustness and efficiency of covariate adjusted linear instrumental\n  variable estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stage least squares (TSLS) estimators and variants thereof are widely\nused to infer the effect of an exposure on an outcome using instrumental\nvariables (IVs). They belong to a wider class of two-stage IV estimators, which\nare based on fitting a conditional mean model for the exposure, and then using\nthe fitted exposure values along with the covariates as predictors in a linear\nmodel for the outcome. We show that standard TSLS estimators enjoy greater\nrobustness to model misspecification than more general two-stage estimators.\nHowever, by potentially using a wrong exposure model, e.g. when the exposure is\nbinary, they tend to be inefficient. In view of this, we study double-robust\nG-estimators instead. These use working models for the exposure, IV and outcome\nbut only require correct specification of either the IV model or the outcome\nmodel to guarantee consistent estimation of the exposure effect. As the finite\nsample performance of the locally efficient G-estimator can be poor, we further\ndevelop G-estimation procedures with improved efficiency and robustness\nproperties under misspecification of some or all working models. Simulation\nstudies and a data analysis demonstrate drastic improvements, with remarkably\ngood performance even when one or more working models are misspecified.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 22:07:33 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Vansteelandt", "Stijn", ""], ["Didelez", "Vanessa", ""]]}, {"id": "1510.01772", "submitter": "Andrew L. Johnson", "authors": "Jos\\'e Luis Preciado Arreola and Andrew L. Johnson", "title": "Estimating Stochastic Production Frontiers: A One-stage Multivariate\n  Semi-Nonparametric Bayesian Concave Regression Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method to estimate a production frontier that\nsatisfies the axioms of monotonicity and concavity in a non-parametric Bayesian\nsetting. An inefficiency term that allows for significant departure from prior\ndistributional assumptions is jointly estimated in a single stage with\nparametric prior assumptions. We introduce heteroscedasticity into the\ninefficiency terms by local hyperplane-specific shrinkage hyperparameters and\nimpose monotonicity using bound-constrained local nonlinear regression. Our\nminimum-of-hyperplanes estimator imposes concavity. Our Monte Carlo simulation\nexperiments demonstrate that the frontier and efficiency estimations are\ncompetitive, economically sound, and allow for the analysis of larger datasets\nthan existing nonparametric methods. We validate the proposed method using data\nfrom 2007-2010 for Japan's concrete industry. The results show that the\nefficiency levels remain relatively high over the time period.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 22:19:48 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Arreola", "Jos\u00e9 Luis Preciado", ""], ["Johnson", "Andrew L.", ""]]}, {"id": "1510.02159", "submitter": "Sergii Babkin", "authors": "Michael Schweinberger, Sergii Babkin and Katherine Ensor", "title": "High-Dimensional Multivariate Time Series With Additional Structure", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 2017, Vol. 26,\n  No. 3, 610-622", "doi": "10.1080/10618600.2016.1265528", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional multivariate time series are challenging due to the\ndependent and high-dimensional nature of the data, but in many applications\nthere is additional structure that can be exploited to reduce computing time\nalong with statistical error. We consider high-dimensional vector\nautoregressive processes with spatial structure, a simple and common form of\nadditional structure. We propose novel high-dimensional methods that take\nadvantage of such structure without making model assumptions about how distance\naffects dependence. We provide non-asymptotic bounds on the statistical error\nof parameter estimators in high-dimensional settings and show that the proposed\napproach reduces the statistical error. An application to air pollution in the\nUS demonstrates that the estimation approach reduces both computing time and\nprediction error and gives rise to results that are meaningful from a\nscientific point of view, in contrast to high-dimensional methods that ignore\nspatial structure. In practice, these high-dimensional methods can be used to\ndecompose high-dimensional multivariate time series into lower-dimensional\nmultivariate time series that can be studied by other methods in more depth.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 22:42:43 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 17:33:30 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 00:28:08 GMT"}, {"version": "v4", "created": "Sun, 7 Aug 2016 20:49:41 GMT"}, {"version": "v5", "created": "Sat, 25 Feb 2017 02:03:02 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Schweinberger", "Michael", ""], ["Babkin", "Sergii", ""], ["Ensor", "Katherine", ""]]}, {"id": "1510.02175", "submitter": "Bai Jiang", "authors": "Bai Jiang, Tung-yu Wu, Charles Zheng, Wing H. Wong", "title": "Learning Summary Statistic for Approximate Bayesian Computation via Deep\n  Neural Network", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": "10.5705/ss.202015.0340", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) methods are used to approximate\nposterior distributions in models with unknown or computationally intractable\nlikelihoods. Both the accuracy and computational efficiency of ABC depend on\nthe choice of summary statistic, but outside of special cases where the optimal\nsummary statistics are known, it is unclear which guiding principles can be\nused to construct effective summary statistics. In this paper we explore the\npossibility of automating the process of constructing summary statistics by\ntraining deep neural networks to predict the parameters from artificially\ngenerated data: the resulting summary statistics are approximately posterior\nmeans of the parameters. With minimal model-specific tuning, our method\nconstructs summary statistics for the Ising model and the moving-average model,\nwhich match or exceed theoretically-motivated summary statistics in terms of\nthe accuracies of the resulting posteriors.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 00:33:51 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 21:20:53 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 04:47:20 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Jiang", "Bai", ""], ["Wu", "Tung-yu", ""], ["Zheng", "Charles", ""], ["Wong", "Wing H.", ""]]}, {"id": "1510.02245", "submitter": "Luca La Rocca", "authors": "G. Consonni and L. La Rocca", "title": "Objective Bayes Covariate-Adjusted Sparse Graphical Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an objective Bayes method for covariance selection in Gaussian\nmultivariate regression models whose error term has a covariance structure\nwhich is Markov with respect to a Directed Acyclic Graph (DAG). The scope is\ncovariate-adjusted sparse graphical model selection, a topic of growing\nimportance especially in the area of genetical genomics (eQTL analysis).\nSpecifically, we provide a closed-form expression for the marginal likelihood\nof any DAG (with small parent sets) whose computation virtually requires no\nsubjective elicitation by the user and involves only conjugate matrix normal\nWishart distributions. This is made possible by a specific form of prior\nassignment, whereby only one prior under the complete DAG model need be\nspecified, based on the notion of fractional Bayes factor. All priors under the\nother DAG models are derived using prior modularity, and global parameter\nindependence, in the terminology of Geiger & Heckerman (2002). Since the\nmarginal likelihood we obtain is constant within each class of Markov\nequivalent DAGs, our method naturally specializes to covariate-adjusted\ndecomposable graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 09:15:40 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Consonni", "G.", ""], ["La Rocca", "L.", ""]]}, {"id": "1510.02302", "submitter": "Pierre Ailliot", "authors": "Pierre Ailliot, Bernard Delyon, Val\\'erie Monbet, Marc Prevosto", "title": "Dependent time changed processes with applications to nonlinear ocean\n  waves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many records in environmental sciences exhibit asymmetric trajectories and\nthere is a need for simple and tractable models which can reproduce such\nfeatures. In this paper we explore an approach based on applying both a time\nchange and a marginal transformation on Gaussian processes. The main\noriginality of the proposed model is that the time change depends on the\nobserved trajectory. We first show that the proposed model is stationary and\nergodic and provide an explicit characterization of the stationary\ndistribution. This result is then used to build both parametric and\nnon-parametric estimates of the time change function whereas the estimation of\nthe marginal transformation is based on up-crossings. Simulation results are\nprovided to assess the quality of the estimates. The model is applied to\nshallow water wave data and it is shown that the fitted model is able to\nreproduce important statistics of the data such as its spectrum and marginal\ndistribution which are important quantities for practical applications. An\nimportant benefit of the proposed model is its ability to reproduce the\nobserved asymmetries between the crest and the troughs and between the front\nand the back of the waves by accelerating the chronometer in the crests and in\nthe front of the waves.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 13:01:51 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Ailliot", "Pierre", ""], ["Delyon", "Bernard", ""], ["Monbet", "Val\u00e9rie", ""], ["Prevosto", "Marc", ""]]}, {"id": "1510.02329", "submitter": "Ren\\'ee Menezes", "authors": "Ren\\'ee Menezes and Leila Mohammadi and Jelle Goeman and Judith Boer", "title": "Analysing multiple types of molecular profiles simultaneously:\n  connecting the needles in the haystack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that a random-effects framework can be used to test the\nassociation between a gene's expression level and the number of DNA copies of a\nset of genes. This gene-set modelling framework was later applied to find\nassociations between mRNA expression and microRNA expression, by defining the\ngene sets using target prediction information.\n  Here, we extend the model introduced by Menezes et al (2009) to consider the\neffect of not just copy number, but also of other molecular profiles such as\nmethylation changes and loss-of-heterozigosity (LOH), on gene expression\nlevels. We will consider again sets of measurements, to improve robustness of\nresults and increase the power to find associations. Our approach can be used\ngenome-wide to find associations, yields a test to help separate true\nassociations from noise and can include confounders.\n  We apply our method to colon and to breast cancer samples, for which\ngenome-wide copy number, methylation and gene expression profiles are\navailable. Our findings include interesting gene expression-regulating\nmechanisms, which may involve only one of copy number or methylation, or both\nfor the same samples. We even are able to find effects due to different\nmolecular mechanisms in different samples.\n  Our method can equally well be applied to cases where other types of\nmolecular (high-dimensional) data are collected, such as LOH, SNP genotype and\nmicroRNA expression data. Computationally efficient, it represents a flexible\nand powerful tool to study associations between high-dimensional datasets. The\nmethod is freely available via the SIM BioConductor package.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 14:10:37 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Menezes", "Ren\u00e9e", ""], ["Mohammadi", "Leila", ""], ["Goeman", "Jelle", ""], ["Boer", "Judith", ""]]}, {"id": "1510.02399", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi and Marco Lippi and Matteo Luciani", "title": "Dynamic Factor Models, Cointegration, and Error Correction Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies Non-Stationary Dynamic Factor Models such that the factors\n$\\mathbf F_t$ are $I(1)$ and singular, i.e. $\\mathbf F_t$ has dimension $r$ and\nis driven by a $q$-dimensional white noise, the common shocks, with $q<r$. We\nshow that $\\mathbf F_t$ is driven by $r-c$ permanent shocks, where $c$ is the\ncointegration rank of $\\mathbf F_t$, and $q-(r-c)<c$ transitory shocks, thus\nthe same result as in the non-singular case for the permanent shocks but not\nfor the transitory shocks. Our main result is obtained by combining the classic\nGranger Representation Theorem with recent results by Anderson and Deistler on\nsingular stochastic vectors: if $(1-L)\\mathbf F_t$ is singular and has {\\it\nrational} spectral density then, for generic values of the parameters, $\\mathbf\nF_t$ has an autoregressive representation with a {\\it finite-degree} matrix\npolynomial fulfilling the restrictions of a Vector Error Correction Mechanism\nwith $c$ error terms. This result is the basis for consistent estimation of\nNon-Stationary Dynamic Factor Models. The relationship between cointegration of\nthe factors and cointegration of the observable variables is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 16:50:42 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 16:24:59 GMT"}, {"version": "v3", "created": "Wed, 11 Jan 2017 18:47:49 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Lippi", "Marco", ""], ["Luciani", "Matteo", ""]]}, {"id": "1510.02425", "submitter": "Vahed Maroufy", "authors": "Vahed Maroufy and Paul Marriott", "title": "Generalizing the Frailty Assumptions in Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Cox's regression hazard model with an unobservable random\nfrailty where no specific distribution is postulated for the frailty variable,\nand the marginal lifetime distribution allows both parametric and\nnon-parametric models. Laplace's approximation method and gradient search on\nsmooth manifolds embedded in Euclidean space are applied, and a non-iterative\nprofile likelihood optimization method is proposed for estimating the\nregression coefficients. The proposed method is compared with the\nExpected-Maximization method developed based on a gamma frailty assumption, and\nalso in the case when the frailty model is misspecified.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 17:58:51 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Maroufy", "Vahed", ""], ["Marriott", "Paul", ""]]}, {"id": "1510.02430", "submitter": "Linbo Wang", "authors": "Thomas S. Richardson, James M. Robins, Linbo Wang", "title": "On Modeling and Estimation for the Relative Risk and Risk Difference", "comments": "To appear in Journal of the American Statistical Association: Theory\n  and Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in formulating models for the relative risk and risk\ndifference is the variation dependence between these parameters and the\nbaseline risk, which is a nuisance model. We address this problem by proposing\nthe conditional log odds-product as a preferred nuisance model. This novel\nnuisance model facilitates maximum-likelihood estimation, but also permits\ndoubly-robust estimation for the parameters of interest. Our approach is\nillustrated via simulations and a data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 18:12:19 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2015 02:25:48 GMT"}, {"version": "v3", "created": "Wed, 13 Apr 2016 16:44:38 GMT"}, {"version": "v4", "created": "Fri, 18 Nov 2016 14:19:20 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""], ["Wang", "Linbo", ""]]}, {"id": "1510.02451", "submitter": "Sebastian Vollmer", "authors": "Alexandre Bouchard-C\\^ot\\'e and Sebastian J. Vollmer and Arnaud Doucet", "title": "The Bouncy Particle Sampler: A Non-Reversible Rejection-Free Markov\n  Chain Monte Carlo Method", "comments": "42 pages, 15 figures, reference in abstract is to arXiv:1112.1263v3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo methods have become standard tools in statistics to\nsample from complex probability measures. Many available techniques rely on\ndiscrete-time reversible Markov chains whose transition kernels build up over\nthe Metropolis-Hastings algorithm. We explore and propose several original\nextensions of an alternative approach introduced recently in Peters and de With\n(2012) where the target distribution of interest is explored using a\ncontinuous-time Markov process. In the Metropolis-Hastings algorithm, a trial\nmove to a region of lower target density, equivalently \"higher energy\", than\nthe current state can be rejected with positive probability. In this\nalternative approach, a particle moves along straight lines continuously around\nthe space and, when facing a high energy barrier, it is not rejected but its\npath is modified by bouncing against this barrier. The resulting non-reversible\nMarkov process provides a rejection-free MCMC sampling scheme. We propose\nseveral original techniques to simulate this continuous-time process exactly in\na wide range of scenarios of interest to statisticians. When the target\ndistribution factorizes as a product of factors involving only subsets of\nvariables, such as the posterior distribution associated to a probabilistic\ngraphical model, it is possible to modify the original algorithm to exploit\nthis structure and update in parallel variables within each clique. We present\nseveral extensions by proposing methods to sample mixed discrete-continuous\ndistributions and distributions restricted to a connected smooth domain. We\nalso show that it is possible to move the particle using a general flow instead\nof straight lines. We demonstrate the efficiency of this methodology through\nsimulations on a variety of applications and show that it can outperform Hybrid\nMonte Carlo schemes in interesting scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 19:17:41 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 20:37:57 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 17:22:04 GMT"}, {"version": "v4", "created": "Wed, 15 Jun 2016 18:58:54 GMT"}, {"version": "v5", "created": "Fri, 18 Nov 2016 22:17:41 GMT"}, {"version": "v6", "created": "Fri, 17 Feb 2017 20:49:02 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Vollmer", "Sebastian J.", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1510.02502", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Daren Wang, Alessandro Rinaldo, Larry Wasserman", "title": "Statistical Analysis of Persistence Intensity Functions", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams are two-dimensional plots that summarize the topological\nfeatures of functions and are an important part of topological data analysis. A\nproblem that has received much attention is how deal with sets of persistence\ndiagrams. How do we summarize them, average them or cluster them? One approach\n-- the persistence intensity function -- was introduced informally by\nEdelsbrunner, Ivanov, and Karasev (2012). Here we provide a modification and\nformalization of this approach. Using the persistence intensity function, we\ncan visualize multiple diagrams, perform clustering and conduct two-sample\ntests.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 20:45:02 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Wang", "Daren", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1510.02594", "submitter": "Nikolas W\\\"olfing", "authors": "Piotr Kokoszka, Matthew Reimherr, Nikolas W\\\"olfing", "title": "A randomness test for functional panels", "comments": "Supplemental material from the authors' homepage or upon request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional panels are collections of functional time series, and arise often\nin the study of high frequency multivariate data. We develop a portmanteau\nstyle test to determine if the cross-sections of such a panel are independent\nand identically distributed. Our framework allows the number of functional\nprojections and/or the number of time series to grow with the sample size. A\nlarge sample justification is based on a new central limit theorem for random\nvectors of increasing dimension. With a proper normalization, the limit is\nstandard normal, potentially making this result easily applicable in other FDA\ncontext in which projections on a subspace of increasing dimension are used.\nThe test is shown to have correct size and excellent power using simulated\npanels whose random structure mimics the realistic dependence encountered in\nreal panel data. It is expected to find application in climatology, finance,\necology, economics, and geophysics. We apply it to Southern Pacific sea surface\ntemperature data, precipitation patterns in the South-West United States, and\ntemperature curves in Germany.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 09:03:03 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2016 15:32:03 GMT"}, {"version": "v3", "created": "Sun, 10 Jul 2016 16:41:24 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kokoszka", "Piotr", ""], ["Reimherr", "Matthew", ""], ["W\u00f6lfing", "Nikolas", ""]]}, {"id": "1510.02702", "submitter": "Joaquin Miguez", "authors": "Eugenia Koblents, Joaquin Miguez, Marco A. Rodriguez, Alexandra M.\n  Schmidt", "title": "A nonlinear population Monte Carlo scheme for the Bayesian estimation of\n  parameters of $\\alpha$-stable distributions", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": "Computational Statistics & Data Analysis, 95, pp.57-74, 2016", "doi": "10.1016/j.csda.2015.09.007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of $\\alpha$-stable distributions enjoys multiple practical\napplications in signal processing, finance, biology and other areas because it\nallows to describe interesting and complex data patterns, such as asymmetry or\nheavy tails, in contrast with the simpler and widely used Gaussian\ndistribution. The density associated with a general $\\alpha$-stable\ndistribution cannot be obtained in closed form, which hinders the process of\nestimating its parameters. A nonlinear population Monte Carlo (NPMC) scheme is\napplied in order to approximate the posterior probability distribution of the\nparameters of an $\\alpha$-stable random variable given a set of random\nrealizations of the latter. The approximate posterior distribution is computed\nby way of an iterative algorithm and it consists of a collection of samples in\nthe parameter space with associated nonlinearly-transformed importance weights.\nA numerical comparison of the main existing methods to estimate the\n$\\alpha$-stable parameters is provided, including the traditional frequentist\ntechniques as well as a Markov chain Monte Carlo (MCMC) and a likelihood-free\nBayesian approach. It is shown by means of computer simulations that the NPMC\nmethod outperforms the existing techniques in terms of parameter estimation\nerror and failure rate for the whole range of values of $\\alpha$, including the\nsmaller values for which most existing methods fail to work properly.\nFurthermore, it is shown that accurate parameter estimates can often be\ncomputed based on a low number of observations. Additionally, numerical results\nbased on a set of real fish displacement data are provided.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:19:09 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Koblents", "Eugenia", ""], ["Miguez", "Joaquin", ""], ["Rodriguez", "Marco A.", ""], ["Schmidt", "Alexandra M.", ""]]}, {"id": "1510.02753", "submitter": "Judith Lok", "authors": "Judith J Lok", "title": "Organic direct and indirect effects with post-treatment common causes of\n  mediator and outcome", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the literature on direct and indirect effects assumes that there are\nno post-treatment common causes of the mediator and the outcome. In contrast to\nnatural direct and indirect effects, organic direct and indirect effects, which\nwere introduced in Lok (2016, 2020), can be extended to provide an\nidentification result for settings with post-treatment common causes of the\nmediator and the outcome. This article provides a definition and an\nidentification result for organic direct and indirect effects in the presence\nof post-treatment common causes of mediator and outcome. These new organic\nindirect and direct effects have interpretations in terms of intervention\neffects. Organic indirect effects in the presence of post-treatment common\ncauses are an addition to indirect effects through multivariate mediators.\nOrganic indirect effects in the presence of post-treatment common causes can be\nused e.g. 1. to predict the effect of the initial treatment if its side affects\nare suppressed through additional interventions or 2. to predict the effect of\na treatment that does not affect the post-treatment common cause and affects\nthe mediator the same way as the initial treatment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 18:12:48 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 20:50:40 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lok", "Judith J", ""]]}, {"id": "1510.02838", "submitter": "Sonja Petrovic", "authors": "Sonja Petrovi\\'c", "title": "A survey of discrete methods in (algebraic) statistics for networks", "comments": "Revised for clarity, minor updates, added example, upon suggestions\n  of people mentioned in the acknowledgements section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM math.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling algorithms, hypergraph degree sequences, and polytopes play a\ncrucial role in statistical analysis of network data. This article offers a\nbrief overview of open problems in this area of discrete mathematics from the\npoint of view of a particular family of statistical models for networks called\nexponential random graph models. The problems and underlying constructions are\nalso related to well-known concepts in commutative algebra and graph-theoretic\nconcepts in computer science. We outline a few lines of recent work that\nhighlight the natural connection between these fields and unify them into some\nopen problems. While these problems are often relevant in discrete mathematics\nin their own right, the emphasis here is on statistical relevance with the hope\nthat these lines of research do not remain disjoint. Suggested specific open\nproblems and general research questions should advance algebraic statistics\ntheory as well as applied statistical tools for rigorous statistical analysis\nof networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 22:12:55 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 19:07:58 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Petrovi\u0107", "Sonja", ""]]}, {"id": "1510.02871", "submitter": "Kelly Cristina Mota Goncalves", "authors": "Carolina Valani Cavalcante and Kelly Cristina Mota Gon\\c{c}alves", "title": "Mixture models applied to heterogeneous populations", "comments": null, "journal-ref": "Cavalcanti, C. V. and Gon\\c{c}alves, K. C. M. (2018) Mixture\n  models applied to heterogeneous populations. Brazilian Journal of\n  Probabability and Statistics. Volume 32, Number 2 (2018), 320-345", "doi": "10.1214/16-BJPS345", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models provide a flexible representation of heterogeneity in a finite\nnumber of latent classes. From the Bayesian point of view, Markov Chain Monte\nCarlo methods provide a way to draw inferences from these models. In\nparticular, when the number of subpopulations is considered unknown, more\nsophisticated methods are required to perform Bayesian analysis. The Reversible\nJump Markov Chain Monte Carlo is an alternative method for computing the\nposterior distribution by simulation in this case. Some problems associated\nwith the Bayesian analysis of these class of models are frequent, such as the\nso-called \"label-switching\" problem. However, as the level of heterogeneity in\nthe population increases, these problems are expected to become less frequent\nand the model's performance to improve. Thus, the aim of this work is to\nevaluate the normal mixture model fit using simulated data under different\nsettings of heterogeneity and prior information about the mixture proportions.\nA simulation study is also presented to evaluate the model's performance\nconsidering the number of components known and estimating it. Finally, the\nmodel is applied to a censored real dataset containing antibody levels of\nCytomegalovirus in individuals.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 03:58:16 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 15:11:47 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Cavalcante", "Carolina Valani", ""], ["Gon\u00e7alves", "Kelly Cristina Mota", ""]]}, {"id": "1510.02934", "submitter": "Cheng Koay", "authors": "Cheng Guan Koay, Ping-Hong Yeh, John M. Ollinger, M. Okan\n  \\.Irfano\\u{g}lu, Carlo Pierpaoli, Peter J. Basser, Terrence R. Oakes, Gerard\n  Riedy", "title": "Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A\n  framework for single-subject analysis in diffusion tensor imaging", "comments": "49 pages, 6 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.neuroimage.2015.11.046", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to develop a framework for single-subject\nanalysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI)\nis capable of testing whether an individual tract as represented by the major\neigenvector of the diffusion tensor and its corresponding angular dispersion\nare significantly different from a group of tracts on a voxel-by-voxel basis.\nThis work develops two complementary statistical tests based on the elliptical\ncone of uncertainty (COU), which is a model of uncertainty or dispersion of the\nmajor eigenvector of the diffusion tensor. The orientation deviation test\nexamines whether the major eigenvector from a single subject is within the\naverage elliptical COU formed by a collection of elliptical COUs. The shape\ndeviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test\nbetween the normalized shape measures (area and circumference) of the\nelliptical cones of uncertainty of the single subject against a group of\ncontrols. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR)\nwere incorporated in the orientation deviation test. The shape deviation test\nuses FDR only. TOADDI was found to be numerically accurate and statistically\neffective. Clinical data from two Traumatic Brain Injury (TBI) patients and one\nnon-TBI subject were tested against the data obtained from a group of 45\nnon-TBI controls to illustrate the application of the proposed framework in\nsingle-subject analysis. The frontal portion of the superior longitudinal\nfasciculus seemed to be implicated in both tests as significantly different\nfrom that of the control group. The TBI patients and the single non-TBI subject\nwere well separated under the shape deviation test at the chosen FDR level of\n0.0005. TOADDI is a simple but novel geometrically based statistical framework\nfor analyzing DTI data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 13:54:07 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 01:35:14 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Koay", "Cheng Guan", ""], ["Yeh", "Ping-Hong", ""], ["Ollinger", "John M.", ""], ["\u0130rfano\u011flu", "M. Okan", ""], ["Pierpaoli", "Carlo", ""], ["Basser", "Peter J.", ""], ["Oakes", "Terrence R.", ""], ["Riedy", "Gerard", ""]]}, {"id": "1510.02950", "submitter": "Alexandre Patriota", "authors": "Alexandre G. Patriota", "title": "A measure of evidence based on the likelihood-ratio statistics", "comments": "25 double-spaced pages, 4 figures. Rewritten version with more\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the likelihood-ratio measure (a) is invariant\nwith respect to dominating sigma-finite measures, (b) satisfies logical\nconsequences which are not satisfied by standard $p$-values, (c) respects\nfrequentist properties, i.e., the type I error can be properly controlled, and,\nunder mild regularity conditions, (d) can be used as an upper bound for\nposterior probabilities. We also discuss a generic application to test whether\nthe genotype frequencies of a given population are under the Hardy-Weinberg\nequilibrium, under inbreeding restrictions or under outbreeding restrictions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 15:21:18 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 22:53:05 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Patriota", "Alexandre G.", ""]]}, {"id": "1510.02967", "submitter": "Daniel Taylor-Rodriguez", "authors": "Daniel Taylor-Rodriguez and Sujit Ghosh", "title": "On the estimation of the order of smoothness of the regression function", "comments": "28 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The order of smoothness chosen in nonparametric estimation problems is\ncritical. This choice balances the tradeoff between model parsimony and data\noverfitting. The most common approach used in this context is cross-validation.\nHowever, cross-validation is computationally time consuming and often precludes\nvalid post-selection inference without further considerations. With this in\nmind, borrowing elements from the objective Bayesian variable selection\nliterature, we propose an approach to select the degree of a polynomial basis.\nAlthough the method can be extended to most series-based smoothers, we focus on\nestimates arising from Bernstein polynomials for the regression function, using\nmixtures of g-priors on the model parameter space and a hierarchical\nspecification for the priors on the order of smoothness. We prove the\nasymptotic predictive optimality for the method, and through simulation\nexperiments, demonstrate that, compared to cross-validation, our approach is\none or two orders of magnitude faster and yields comparable predictive\naccuracy. Moreover, our method provides simultaneous quantification of model\nuncertainty and parameter estimates. We illustrate the method with real\napplications for continuous and binary responses.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 18:25:38 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Taylor-Rodriguez", "Daniel", ""], ["Ghosh", "Sujit", ""]]}, {"id": "1510.03098", "submitter": "Dandan Jiang", "authors": "Dandan Jiang", "title": "Tests for Large Dimensional Covariance Structure Based on Rao's Score\n  Test", "comments": "28 page, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new test for covariance matrices structure based on the\ncorrection to Rao's score test in large dimensional framework. By generalizing\nthe CLT for the linear spectral statistics of large dimensional sample\ncovariance matrices, the test can be applicable for large dimensional\nnon-Gaussian variables in a wider range without the restriction of the 4th\nmoment. Moreover, the amending Rao's score test is also powerful even for the\nultra high dimensionality as $p \\gg n$, which breaks the inherent idea that the\ncorrected tests by RMT can be only used when $p<n$. Finally, we compare the\nproposed test with other high dimensional covariance structure tests to\nevaluate their performances through the simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 20:46:49 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 22:38:46 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Jiang", "Dandan", ""]]}, {"id": "1510.03163", "submitter": "Lixing Zhu", "authors": "Cuizhen Niu and Lixing Zhu", "title": "A robust adaptive-to-model enhancement test for parametric single-index\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the research on checking whether the underlying model is of parametric\nsingle-index structure with outliers in observations, the purpose of this paper\nis two-fold. First, a test that is robust against outliers is suggested. The\nHampel's second-order influence function of the test statistic is proved to be\nbounded. Second, the test fully uses the dimension reduction structure of the\nhypothetical model and automatically adapts to alternative models when the null\nhypothesis is false. Thus, the test can greatly overcome the dimensionality\nproblem and is still omnibus against general alternative models. The\nperformance of the test is demonstrated by both Monte Carlo simulation studies\nand an application to a real dataset.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 07:03:20 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Niu", "Cuizhen", ""], ["Zhu", "Lixing", ""]]}, {"id": "1510.03225", "submitter": "Khanh To Duc", "authors": "Khanh To Duc, Monica Chiogna, Gianfranco Adimari", "title": "Bias-corrected methods for estimating the receiver operating\n  characteristic surface of continuous diagnostic tests", "comments": null, "journal-ref": null, "doi": "10.1214/16-EJS1202", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification bias is a well-known problem that may occur in the evaluation of\npredictive ability of diagnostic tests. When a binary disease status is\nconsidered, various solutions can be found in the literature to correct\ninference based on usual measures of test accuracy, such as the receiver\noperating characteristic (ROC) curve or the area underneath. Evaluation of the\npredictive ability of continuous diagnostic tests in the presence of\nverification bias for a three-class disease status is here discussed. In\nparticular, several verification bias-corrected estimators of the ROC surface\nand of the volume underneath are proposed. Consistency and asymptotic normality\nof the proposed estimators are established and their finite sample behavior is\ninvestigated by means of Monte Carlo simulation studies. Two illustrations are\nalso given.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 11:05:47 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 15:21:32 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Duc", "Khanh To", ""], ["Chiogna", "Monica", ""], ["Adimari", "Gianfranco", ""]]}, {"id": "1510.03245", "submitter": "Gabriele Soffritti", "authors": "Giuliano Galimberti, Annamaria Manisi, Gabriele Soffritti", "title": "A unified framework for model-based clustering, linear regression and\n  multiple cluster structure detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general framework for dealing with both linear regression and clustering\nproblems is described. It includes Gaussian clusterwise linear regression\nanalysis with random covariates and cluster analysis via Gaussian mixture\nmodels with variable selection. It also admits a novel approach for detecting\nmultiple clusterings from possibly correlated sub-vectors of variables, based\non a model defined as the product of conditionally independent Gaussian mixture\nmodels. A necessary condition for the identifiability of such a model is\nprovided. The usefulness and effectiveness of the described methodology are\nillustrated using simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 12:08:51 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Galimberti", "Giuliano", ""], ["Manisi", "Annamaria", ""], ["Soffritti", "Gabriele", ""]]}, {"id": "1510.03516", "submitter": "Jyotishka Datta", "authors": "Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson, and Brandon T.\n  Willard", "title": "Default Bayesian analysis with global-local shrinkage priors", "comments": "28 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a framework for assessing the default nature of a prior\ndistribution using the property of regular variation, which we study for\nglobal-local shrinkage priors. In particular, we demonstrate the horseshoe\npriors, originally designed to handle sparsity, also possess regular variation\nand thus are appropriate for default Bayesian analysis. To illustrate our\nmethodology, we solve a problem of non-informative priors due to Efron (1973),\nwho showed standard flat non-informative priors in high-dimensional normal\nmeans model can be highly informative for nonlinear parameters of interest. We\nconsider four such problems and show global-local shrinkage priors such as the\nhorseshoe and horseshoe+ perform as Efron (1973) requires in each case. We find\nthe reason for this lies in the ability of the global-local shrinkage priors to\nseparate a low-dimensional signal embedded in high-dimensional noise, even for\nnonlinear functions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 03:08:24 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 00:24:31 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Bhadra", "Anindya", ""], ["Datta", "Jyotishka", ""], ["Polson", "Nicholas G.", ""], ["Willard", "Brandon T.", ""]]}, {"id": "1510.03537", "submitter": "Lixing Zhu", "authors": "Xuehu Zhu and Lixing Zhu", "title": "Dimension reduction-based significance testing in nonparametric\n  regression", "comments": "49 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dimension reduction-based adaptive-to-model test is proposed for\nsignificance of a subset of covariates in the context of a nonparametric\nregression model. Unlike existing local smoothing significance tests, the new\ntest behaves like a local smoothing test as if the number of covariates were\njust that under the null hypothesis and it can detect local alternatives\ndistinct from the null at the rate that is only related to the number of\ncovariates under the null hypothesis. Thus, the curse of dimensionality is\nlargely alleviated when nonparametric estimation is inevitably required. In the\ncases where there are many insignificant covariates, the improvement of the new\ntest is very significant over existing local smoothing tests on the\nsignificance level maintenance and power enhancement. Simulation studies and a\nreal data analysis are conducted to examine the finite sample performance of\nthe proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 05:46:34 GMT"}], "update_date": "2016-11-06", "authors_parsed": [["Zhu", "Xuehu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1510.03542", "submitter": "Lixing Zhu", "authors": "Xuehu Zhu, Fei Chen, Xu Guo and Lixing Zhu", "title": "Heteroscedasticity Testing for Regression Models: A Dimension\n  Reduction-based Model Adaptive", "comments": "50 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heteroscedasticity testing is of importance in regression analysis. Existing\nlocal smoothing tests suffer severely from curse of dimensionality even when\nthe number of covariates is moderate because of use of nonparametric\nestimation. In this paper, a dimension reduction-based model adaptive test is\nproposed which behaves like a local smoothing test as if the number of\ncovariates were equal to the number of their linear combinations in the mean\nregression function, in particular, equal to 1 when the mean function contains\na single index. The test statistic is asymptotically normal under the null\nhypothesis such that critical values are easily determined. The finite sample\nperformances of the test are examined by simulations and a real data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 06:05:49 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Zhu", "Xuehu", ""], ["Chen", "Fei", ""], ["Guo", "Xu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1510.03617", "submitter": "Meitner Cadena", "authors": "Meitner Cadena", "title": "A note on the best attainable rates of convergence for estimates of the\n  shape parameter of regular variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hall and Welsh gave in 1984 the lowest bound so far to rates of convergence\nfor estimates of the shape parameter of regular variation. We show that this\nbound can be improved.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 10:38:40 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Cadena", "Meitner", ""]]}, {"id": "1510.03671", "submitter": "Matthias Killiches", "authors": "Matthias Killiches, Daniel Kraus, Claudia Czado", "title": "Model distances for vine copulas in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas are a flexible class of dependence models consisting of\nbivariate building blocks and have proven to be particularly useful in high\ndimensions. Classical model distance measures require multivariate integration\nand thus suffer from the curse of dimensionality. In this paper we provide\nnumerically tractable methods to measure the distance between two vine copulas\neven in high dimensions. For this purpose, we consecutively develop three new\ndistance measures based on the Kullback-Leibler distance, using the result that\nit can be expressed as the sum over expectations of KL distances between\nunivariate conditional densities, which can be easily obtained for vine\ncopulas. To reduce numerical calculations we approximate these expectations on\nadequately designed grids, outperforming Monte Carlo-integration with respect\nto computational time. In numerous examples and applications we illustrate the\nstrengths and weaknesses of the developed distance measures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 13:44:24 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 09:25:45 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Killiches", "Matthias", ""], ["Kraus", "Daniel", ""], ["Czado", "Claudia", ""]]}, {"id": "1510.03771", "submitter": "Gwenael GR Leday", "authors": "Gwena\\\"el G. R. Leday, Mathisca C. M. de Gunst, Gino B. Kpogbezan, Aad\n  W. Van der Vaart, Wessel N. Van Wieringen and Mark A. Van de Wiel", "title": "Gene network reconstruction using global-local shrinkage priors", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing a gene network from high-throughput molecular data is often a\nchallenging task, as the number of parameters to estimate easily is much larger\nthan the sample size. A conventional remedy is to regularize or penalize the\nmodel likelihood. In network models, this is often done locally in the\nneighbourhood of each node or gene. However, estimation of the many\nregularization parameters is often difficult and can result in large\nstatistical uncertainties. In this paper we propose to combine local\nregularization with global shrinkage of the regularization parameters to borrow\nstrength between genes and improve inference. We employ a simple Bayesian model\nwith non-sparse, conjugate priors to facilitate the use of fast variational\napproximations to posteriors. We discuss empirical Bayes estimation of\nhyper-parameters of the priors, and propose a novel approach to rank-based\nposterior thresholding. Using extensive model- and data-based simulations, we\ndemonstrate that the proposed inference strategy outperforms popular (sparse)\nmethods, yields more stable edges, and is more reproducible.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 16:54:41 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Leday", "Gwena\u00ebl G. R.", ""], ["de Gunst", "Mathisca C. M.", ""], ["Kpogbezan", "Gino B.", ""], ["Van der Vaart", "Aad W.", ""], ["Van Wieringen", "Wessel N.", ""], ["Van de Wiel", "Mark A.", ""]]}, {"id": "1510.03781", "submitter": "Haim Bar", "authors": "Haim Y. Bar, James G. Booth, Martin T. Wells", "title": "A Scalable Empirical Bayes Approach to Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model-based empirical Bayes approach to variable selection\nproblems in which the number of predictors is very large, possibly much larger\nthan the number of responses (the so-called 'large p, small n' problem). We\nconsider the multiple linear regression setting, where the response is assumed\nto be a continuous variable and it is a linear function of the predictors plus\nerror. The explanatory variables in the linear model can have a positive effect\non the response, a negative effect, or no effect. We model the effects of the\nlinear predictors as a three-component mixture in which a key assumption is\nthat only a small (unknown) fraction of the candidate predictors have a\nnon-zero effect on the response variable. By treating the coefficients as\nrandom effects we develop an approach that is computationally efficient because\nthe number of parameters that have to be estimated is small, and remains\nconstant regardless of the number of explanatory variables. The model\nparameters are estimated using the EM algorithm which is scalable and leads to\nsignificantly faster convergence, compared with simulation-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 17:09:04 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Bar", "Haim Y.", ""], ["Booth", "James G.", ""], ["Wells", "Martin T.", ""]]}, {"id": "1510.03782", "submitter": "Emily Beerg", "authors": "Jae-kwang Kim, Emily Berg, Taesung Park", "title": "Statistical Matching using Fractional Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical matching is a technique for integrating two or more data sets\nwhen information available for matching records for individual participants\nacross data sets is incomplete. Statistical matching can be viewed as a missing\ndata problem where a researcher wants to perform a joint analysis of variables\nthat are never jointly observed. A conditional independence assumption is often\nused to create imputed data for statistical matching.\n  We consider an alternative approach to statistical matching without using the\nconditional independence assumption. We apply parametric fractional imputation\nof Kim (2011) to create imputed data using an instrumental variable assumption\nto identify the joint distribution. We also present variance estimators\nappropriate for the imputation procedure. We explain how the method applies\ndirectly to the analysis of data from split questionnaire designs and\nmeasurement error models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 17:14:09 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Kim", "Jae-kwang", ""], ["Berg", "Emily", ""], ["Park", "Taesung", ""]]}, {"id": "1510.03959", "submitter": "Paula Griffin", "authors": "Paula J. Griffin, W. Evan Johnson, Eric D. Kolaczyk", "title": "Detection of multiple perturbations in multi-omics biological networks", "comments": "Submitted to Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular mechanism-of-action is of fundamental concern in many biological\nstudies. It is of particular interest for identifying the cause of disease and\nlearning the way in which treatments act against disease. However, pinpointing\nsuch mechanisms is difficult, due to the fact that small perturbations to the\ncell can have wide-ranging downstream effects. Given a snapshot of cellular\nactivity, it can be challenging to tell where a disturbance originated. The\npresence of an ever-greater variety of high-throughput biological data offers\nan opportunity to examine cellular behavior from multiple angles, but also\npresents the statistical challenge of how to effectively analyze data from\nmultiple sources. In this setting, we propose a method for mechanism-of-action\ninference by extending network filtering to multi-attribute data. We first\nestimate a joint Gaussian graphical model across multiple data types using\npenalized regression and filter for network effects. We then apply a set of\nlikelihood ratio tests to identify the most likely site of the original\nperturbation. In addition, we propose a conditional testing procedure to allow\nfor detection of multiple perturbations. We demonstrate this methodology on\npaired gene expression and methylation data from The Cancer Genome Atlas\n(TCGA).\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 03:56:55 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 00:11:28 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Griffin", "Paula J.", ""], ["Johnson", "W. Evan", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "1510.04161", "submitter": "Daniel Kraus", "authors": "Daniel Kraus and Claudia Czado", "title": "D-vine copula based quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression, that is the prediction of conditional quantiles, has\nsteadily gained importance in statistical modeling and financial applications.\nThe authors introduce a new semiparametric quantile regression method based on\nsequentially fitting a likelihood optimal D-vine copula to given data resulting\nin highly flexible models with easily extractable conditional quantiles. As a\nsubclass of regular vine copulas, D-vines enable the modeling of multivariate\ncopulas in terms of bivariate building blocks, a so-called pair-copula\nconstruction (PCC). The proposed algorithm works fast and accurate even in high\ndimensions and incorporates an automatic variable selection by maximizing the\nconditional log-likelihood. Further, typical issues of quantile regression such\nas quantile crossing or transformations, interactions and collinearity of\nvariables are automatically taken care of. In a simulation study the improved\naccuracy and saved computational time of the approach in comparison with\nestablished quantile regression methods is highlighted. An extensive financial\napplication to international credit default swap (CDS) data including stress\ntesting and Value-at-Risk (VaR) prediction demonstrates the usefulness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 15:39:10 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 07:28:42 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 09:46:30 GMT"}, {"version": "v4", "created": "Wed, 16 Nov 2016 12:32:20 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kraus", "Daniel", ""], ["Czado", "Claudia", ""]]}, {"id": "1510.04320", "submitter": "Jyotishka Datta", "authors": "Jyotishka Datta and David B. Dunson", "title": "Inference on High-Dimensional Sparse Count Data", "comments": "20 pages, 7 figures, 2 tables. (This version has a new result\n  regarding tighter control on false discoveries and another real data example.\n  Additional proofs and examples are given in the supplementary file.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of application areas, there is a growing interest in analyzing\nhigh dimensional sparse count data, with sparsity exhibited by an\nover-abundance of zeros and small non-zero counts. Existing approaches for\nanalyzing multivariate count data via Poisson or negative binomial log-linear\nhierarchical models with zero-inflation cannot flexibly adapt to the level and\nnature of sparsity in the data. We develop a new class of continuous\nlocal-global shrinkage priors tailored for sparse counts. Theoretical\nproperties are assessed, including posterior concentration, stronger control on\nfalse discoveries in multiple testing, robustness in posterior mean and\nsuper-efficiency in estimating the sampling density. Simulation studies\nillustrate excellent small sample properties relative to competitors. We apply\nthe method to detect rare mutational hotspots in exome sequencing data and to\nidentify cities most impacted by terrorism.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 21:13:54 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 06:00:17 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Datta", "Jyotishka", ""], ["Dunson", "David B.", ""]]}, {"id": "1510.04342", "submitter": "Stefan Wager", "authors": "Stefan Wager and Susan Athey", "title": "Estimation and Inference of Heterogeneous Treatment Effects using Random\n  Forests", "comments": "To appear in the Journal of the American Statistical Association.\n  Part of the results developed in this paper were made available as an earlier\n  technical report \"Asymptotic Theory for Random Forests\", available at\n  (arXiv:1405.0352)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering challenges -- ranging from personalized\nmedicine to customized marketing recommendations -- require an understanding of\ntreatment effect heterogeneity. In this paper, we develop a non-parametric\ncausal forest for estimating heterogeneous treatment effects that extends\nBreiman's widely used random forest algorithm. In the potential outcomes\nframework with unconfoundedness, we show that causal forests are pointwise\nconsistent for the true treatment effect, and have an asymptotically Gaussian\nand centered sampling distribution. We also discuss a practical method for\nconstructing asymptotic confidence intervals for the true treatment effect that\nare centered at the causal forest estimates. Our theoretical results rely on a\ngeneric Gaussian theory for a large family of random forest algorithms. To our\nknowledge, this is the first set of results that allows any type of random\nforest, including classification and regression forests, to be used for\nprovably valid statistical inference. In experiments, we find causal forests to\nbe substantially more powerful than classical methods based on nearest-neighbor\nmatching, especially in the presence of irrelevant covariates.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 22:54:59 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 00:38:23 GMT"}, {"version": "v3", "created": "Sat, 19 Nov 2016 04:08:22 GMT"}, {"version": "v4", "created": "Mon, 10 Jul 2017 01:15:47 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wager", "Stefan", ""], ["Athey", "Susan", ""]]}, {"id": "1510.04346", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen", "title": "Explicit solutions to a vector time series model and its induced model\n  for business cycles", "comments": "16 page2, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.EC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This article gives the explicit solution to a general vector time series\nmodel that describes interacting, heterogeneous agents that operate under\nuncertainties but according to Keynesian principles, from which a model for\nbusiness cycle is induced by a weighted average of the growth rates of the\nagents in the model. The explicit solution enables a direct simulation of the\ntime series defined by the model and better understanding of the joint behavior\nof the growth rates. In addition, the induced model for business cycles and its\nsolutions are explicitly given and analyzed. The explicit solutions provide a\nbetter understanding of the mathematics of these models and the econometric\nproperties they try to incorporate.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 23:28:35 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Chen", "Xiongzhi", ""]]}, {"id": "1510.04406", "submitter": "Norm Matloff PhD", "authors": "Norman Matloff and Patrick Tendick", "title": "A New Method for Avoiding Data Disclosure While Automatically Preserving\n  Multivariate Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical disclosure limitation (SDL) methods aim to provide analysts\ngeneral access to a data set while limiting the risk of disclosure of\nindividual records. Many methods in the existing literature are aimed only at\nthe case of univariate distributions, but the multivariate case is crucial,\nsince most statistical analyses are multivariate in nature. Yet preserving the\nmultivariate structure of the data can be challenging, especially when both\ncontinuous and categorical variables are present. Here we present a new SDL\nmethod that automatically attains the correct multivariate structure,\nregardless of whether the data are continuous, categorical or mixed. In\naddition, operational methods for assessing data quality and risk will be\nexplored.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 06:00:48 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 23:35:07 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Matloff", "Norman", ""], ["Tendick", "Patrick", ""]]}, {"id": "1510.04439", "submitter": "Lu-Hung Chen", "authors": "Lu-Hung Chen and Ci-Ren Jiang", "title": "Multi-dimensional Functional Principal Component Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-016-9679-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional principal component analysis is one of the most commonly employed\napproaches in functional and longitudinal data analysis and we extend it to\nanalyze functional/longitudinal data observed on a general $d$-dimensional\ndomain. The computational issues emerging in the extension are fully addressed\nwith our proposed solutions. The local linear smoothing technique is employed\nto perform estimation because of its capabilities of performing large-scale\nsmoothing and of handling data with different sampling schemes (possibly on\nirregular domain) in addition to its nice theoretical properties. Besides\ntaking the fast Fourier transform strategy in smoothing, the modern GPGPU\n(general-purpose computing on graphics processing units) architecture is\napplied to perform parallel computation to save computation time. To resolve\nthe out-of-memory issue due to large-scale data, the random projection\nprocedure is applied in the eigendecomposition step. We show that the proposed\nestimators can achieve the classical nonparametric rates for longitudinal data\nand the optimal convergence rates for functional data if the number of\nobservations per sample is of the order $(n/ \\log n)^{d/4}$. Finally, the\nperformance of our approach is demonstrated with simulation studies and the\nfine particulate matter (PM 2.5) data measured in Taiwan.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 08:15:09 GMT"}, {"version": "v2", "created": "Sat, 12 Mar 2016 10:00:53 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Chen", "Lu-Hung", ""], ["Jiang", "Ci-Ren", ""]]}, {"id": "1510.04482", "submitter": "Abhyuday Mandal", "authors": "Adrijo Chakraborty, Gauri Sankar Datta, Abhyuday Mandal", "title": "A two-component normal mixture alternative to the Fay-Herriot model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers a robust hierarchical Bayesian approach to deal with\nrandom effects of small area means when some of these effects assume extreme\nvalues, resulting in outliers. In presence of outliers, the standard\nFay-Herriot model, used for modeling area-level data, under normality\nassumptions of the random effects may overestimate random effects variance,\nthus provides less than ideal shrinkage towards the synthetic regression\npredictions and inhibits borrowing information. Even a small number of\nsubstantive outliers of random effects result in a large estimate of the random\neffects variance in the Fay-Herriot model, thereby achieving little shrinkage\nto the synthetic part of the model or little reduction in posterior variance\nassociated with the regular Bayes estimator for any of the small areas. While a\nscale mixture of normal distributions with known mixing distribution for the\nrandom effects has been found to be effective in presence of outliers, the\nsolution depends on the mixing distribution. As a possible alternative solution\nto the problem, a two-component normal mixture model has been proposed based on\nnoninformative priors on the model variance parameters, regression coefficients\nand the mixing probability. Data analysis and simulation studies based on real,\nsimulated and synthetic data show advantage of the proposed method over the\nstandard Bayesian Fay-Herriot solution derived under normality of random\neffects.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 11:59:58 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 20:52:22 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Chakraborty", "Adrijo", ""], ["Datta", "Gauri Sankar", ""], ["Mandal", "Abhyuday", ""]]}, {"id": "1510.04514", "submitter": "Vahed Maroufy", "authors": "Vahed Maroufy and Paul Marriott", "title": "Mixture Models: Building a Parameter Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the flexibility and popularity of mixture models, their associated\nparameter spaces are often difficult to represent due to fundamental\nidentification problems. This paper looks at a novel way of representing such a\nspace for general mixtures of exponential families, where the parameters are\nidentifiable, interpretable, and, due to a tractable geometric structure, the\nspace allows fast computational algorithms to be constructed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 12:56:44 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Maroufy", "Vahed", ""], ["Marriott", "Paul", ""]]}, {"id": "1510.04638", "submitter": "Mathias Trabs", "authors": "Denis Belomestny and Mathias Trabs", "title": "Low-rank diffusion matrix estimation for high-dimensional time-changed\n  L\\'evy processes", "comments": "39 pages, 5 figures", "journal-ref": "Annales de l'Institut Henri Poincar\\'e, Probabilit\\'es et\n  Statistiques, 54 (3), 1583-1621, 2018", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the diffusion matrix $\\Sigma$ of a high-dimensional,\npossibly time-changed L\\'evy process is studied, based on discrete observations\nof the process with a fixed distance. A low-rank condition is imposed on\n$\\Sigma$. Applying a spectral approach, we construct a weighted least-squares\nestimator with nuclear-norm-penalisation. We prove oracle inequalities and\nderive convergence rates for the diffusion matrix estimator. The convergence\nrates show a surprising dependency on the rank of $\\Sigma$ and are optimal in\nthe minimax sense for fixed dimensions. Theoretical results are illustrated by\na simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 17:30:38 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 20:24:58 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Belomestny", "Denis", ""], ["Trabs", "Mathias", ""]]}, {"id": "1510.04662", "submitter": "David van Dyk", "authors": "Nathan M. Stein, David A. van Dyk, Vinay L. Kashyap, and Aneta\n  Siemiginowska", "title": "Detecting Unspecified Structure in Low-Count Images", "comments": null, "journal-ref": "The Astrophysical Journal (2015), 813, 66 (15pp)", "doi": "10.1088/0004-637X/813/1/66", "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unexpected structure in images of astronomical sources often presents itself\nupon visual inspection of the image, but such apparent structure may either\ncorrespond to true features in the source or be due to noise in the data. This\npaper presents a method for testing whether inferred structure in an image with\nPoisson noise represents a significant departure from a baseline (null) model\nof the image. To infer image structure, we conduct a Bayesian analysis of a\nfull model that uses a multiscale component to allow flexible departures from\nthe posited null model. As a test statistic, we use a tail probability of the\nposterior distribution under the full model. This choice of test statistic\nallows us to estimate a computationally efficient upper bound on a p-value that\nenables us to draw strong conclusions even when there are limited computational\nresources that can be devoted to simulations under the null model. We\ndemonstrate the statistical performance of our method on simulated images.\nApplying our method to an X-ray image of the quasar 0730+257, we find\nsignificant evidence against the null model of a single point source and\nuniform background, lending support to the claim of an X-ray jet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 18:54:13 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Stein", "Nathan M.", ""], ["van Dyk", "David A.", ""], ["Kashyap", "Vinay L.", ""], ["Siemiginowska", "Aneta", ""]]}, {"id": "1510.04813", "submitter": "Juho Piironen", "authors": "Juho Piironen, Aki Vehtari", "title": "Projection predictive model selection for Gaussian processes", "comments": "A few minor changes in text", "journal-ref": "2016 IEEE 26th International Workshop on Machine Learning for\n  Signal Processing (MLSP)", "doi": "10.1109/MLSP.2016.7738829", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for simplification of Gaussian process (GP) models by\nprojecting the information contained in the full encompassing model and\nselecting a reduced number of variables based on their predictive relevance.\nOur results on synthetic and real world datasets show that the proposed method\nimproves the assessment of variable relevance compared to the automatic\nrelevance determination (ARD) via the length-scale parameters. We expect the\nmethod to be useful for improving explainability of the models, reducing the\nfuture measurement costs and reducing the computation time for making new\npredictions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 08:19:38 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 12:16:51 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 10:38:28 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Piironen", "Juho", ""], ["Vehtari", "Aki", ""]]}, {"id": "1510.04841", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "How to (Not) Estimate Gini Coefficients for Fat Tailed Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.EC q-fin.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct measurements of Gini coefficients by conventional arithmetic\ncalculations are a poor estimator, even if paradoxically, they include the\nentire population, as because of super-additivity they cannot lend themselves\nto comparisons between units of different size, and intertemporal analyses are\nvitiated by the population changes. The Gini of aggregated units A and B will\nbe higher than those of A and B computed separately. This effect becomes more\nacute with fatness of tails. When the sample size is smaller than entire\npopulation, the error is extremely high. The conventional literature on Gini\ncoefficients cannot be trusted and comparing countries of different sizes makes\nno sense; nor does it make sense to make claims of \"changes in inequality\"\nbased on conventional measures. We compare the standard methodologies to the\nindirect methods via maximum likelihood estimation of tail exponent. We compare\nto the tail method which is unbiased, with considerably lower error rate. We\nalso consider measurement errors of the tail exponent and suggest a simple but\nefficient methodology to calculate Gini coefficients.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 11:24:16 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "1510.05118", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi and Marc Hallin", "title": "Networks, Dynamic Factors, and the Volatility Analysis of\n  High-Dimensional Financial Series", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12177", "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider weighted directed networks for analysing, over the period\n2000-2013, the interdependencies between volatilities of a large panel of\nstocks belonging to the S\\&P100 index. In particular, we focus on the so-called\n{\\it Long-Run Variance Decomposition Network} (LVDN), where the nodes are\nstocks, and the weight associated with edge $(i,j)$ represents the proportion\nof $h$-step-ahead forecast error variance of variable $i$ accounted for by\nvariable $j$'s innovations. To overcome the curse of dimensionality, we\ndecompose the panel into a component driven by few global, market-wide,\nfactors, and an idiosyncratic one modelled by means of a sparse vector\nautoregression (VAR) model. Inversion of the VAR together with suitable\nidentification restrictions, produces the estimated network, by means of which\nwe can assess how {\\it systemic} each firm is.~Our analysis demonstrates the\nprominent role of financial firms as sources of contagion, especially during\nthe~2007-2008 crisis.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 11:36:25 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 16:48:14 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Hallin", "Marc", ""]]}, {"id": "1510.05144", "submitter": "Yaohui Zeng", "authors": "Yaohui Zeng, Patrick Breheny", "title": "Overlapping group logistic regression with applications to genetic\n  pathway selection", "comments": null, "journal-ref": "Cancer Informatics, 15:179-187, 2016", "doi": "10.4137/CIN.S40043", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering important genes that account for the phenotype of interest has\nlong been challenging in genomewide expression analysis. Analyses such as Gene\nSet Enrichment Analysis (GSEA) that incorporate pathway information have become\nwidespread in hypothesis testing, but pathway-based approaches have been\nlargely absent from regression methods due to the challenges of dealing with\noverlapping pathways and the resulting lack of available software. The R\npackage grpreg is widely used to fit group lasso and other group-penalized\nregression models; in this study, we develop an extension, grpregOverlap, to\nallow for overlapping group structure using the latent variable approach\nproposed by Jacob et al. (2009). We compare this approach to the ordinary lasso\nand to GSEA using both simulated and real data. We find that incorporation of\nprior pathway information substantially improves the accuracy of gene\nexpression classifiers, and we shed light on several ways in which\nhypothesis-testing approaches such as GSEA differ from regression approaches\nwith respect to the analysis of pathway data.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 16:09:12 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 19:42:03 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Zeng", "Yaohui", ""], ["Breheny", "Patrick", ""]]}, {"id": "1510.05149", "submitter": "Mehrdad Jafari-Mamaghani", "authors": "Mehrdad Jafari-Mamaghani", "title": "Robust Non-linear Wiener-Granger Causality For Large High-dimensional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wiener-Granger causality is a widely used framework of causal analysis for\ntemporally resolved events. We introduce a new measure of Wiener-Granger\ncausality based on kernelization of partial canonical correlation analysis with\nspecific advantages in the context of large high-dimensional data. The\nintroduced measure is able to detect non-linear and non-monotonous signals, is\ndesigned to be immune to noise, and offers tunability in terms of computational\ncomplexity in its estimations. Furthermore, we show that, under specified\nconditions, the introduced measure can be regarded as an estimate of\nconditional mutual information (transfer entropy). The functionality of this\nmeasure is assessed using comparative simulations where it outperforms other\nexisting methods. The paper is concluded with an application to climatological\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 17:42:25 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Jafari-Mamaghani", "Mehrdad", ""]]}, {"id": "1510.05189", "submitter": "Fulton Wang", "authors": "Fulton Wang, Cynthia Rudin", "title": "Causal Falling Rule Lists", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (workshop version of\n  previous submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A causal falling rule list (CFRL) is a sequence of if-then rules that\nspecifies heterogeneous treatment effects, where (i) the order of rules\ndetermines the treatment effect subgroup a subject belongs to, and (ii) the\ntreatment effect decreases monotonically down the list. A given CFRL\nparameterizes a hierarchical bayesian regression model in which the treatment\neffects are incorporated as parameters, and assumed constant within\nmodel-specific subgroups. We formulate the search for the CFRL best supported\nby the data as a Bayesian model selection problem, where we perform a search\nover the space of CFRL models, and approximate the evidence for a given CFRL\nmodel using standard variational techniques. We apply CFRL to a census wage\ndataset to identify subgroups of differing wage inequalities between men and\nwomen.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 00:57:00 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 00:28:30 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Wang", "Fulton", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1510.05239", "submitter": "JInglai Li", "authors": "Zhewei Yao, Zixi Hu, Jinglai Li", "title": "A TV-Gaussian prior for infinite-dimensional Bayesian inverse problems\n  and its numerical implementations", "comments": null, "journal-ref": null, "doi": "10.1088/0266-5611/32/7/075006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering problems require to perform Bayesian\ninferences in function spaces, in which the unknowns are of infinite dimension.\nIn such problems, choosing an appropriate prior distribution is an important\ntask. In particular we consider problems where the function to infer is subject\nto sharp jumps which render the commonly used Gaussian measures unsuitable. On\nthe other hand, the so-called total variation (TV) prior can only be defined in\na finite dimensional setting, and does not lead to a well-defined posterior\nmeasure in function spaces. In this work we present a TV-Gaussian (TG) prior to\naddress such problems, where the TV term is used to detect sharp jumps of the\nfunction, and the Gaussian distribution is used as a reference measure so that\nit results in a well-defined posterior measure in the function space. We also\npresent an efficient Markov Chain Monte Carlo (MCMC) algorithm to draw samples\nfrom the posterior distribution of the TG prior. With numerical examples we\ndemonstrate the performance of the TG prior and the efficiency of the proposed\nMCMC algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 13:00:38 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2016 11:40:11 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Yao", "Zhewei", ""], ["Hu", "Zixi", ""], ["Li", "Jinglai", ""]]}, {"id": "1510.05248", "submitter": "David Woods", "authors": "David C. Woods and Susan M. Lewis", "title": "Design of Experiments for Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to review methods of designing screening\nexperiments, ranging from designs originally developed for physical experiments\nto those especially tailored to experiments on numerical models. The strengths\nand weaknesses of the various designs for screening variables in numerical\nmodels are discussed. First, classes of factorial designs for experiments to\nestimate main effects and interactions through a linear statistical model are\ndescribed, specifically regular and nonregular fractional factorial designs,\nsupersaturated designs and systematic fractional replicate designs. Generic\nissues of aliasing, bias and cancellation of factorial effects are discussed.\nSecond, group screening experiments are considered including factorial group\nscreening and sequential bifurcation. Third, random sampling plans are\ndiscussed including Latin hypercube sampling and sampling plans to estimate\nelementary effects. Fourth, a variety of modelling methods commonly employed\nwith screening designs are briefly described. Finally, a novel study\ndemonstrates six screening methods on two frequently-used exemplars, and their\nperformances are compared.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 14:20:33 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Woods", "David C.", ""], ["Lewis", "Susan M.", ""]]}, {"id": "1510.05253", "submitter": "David Woods", "authors": "Anthony C. Atkinson and David C. Woods", "title": "Designs for Generalized Linear Models", "comments": null, "journal-ref": "Handbook of Design and Analysis of Experiments (editors A. Dean,\n  M. Morris, J. Stufken, D. Bingham), chapter 13, 2015, Chapman and Hall/CRC\n  Press", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the design of experiments for generalised linear models,\nincluding optimal design, Bayesian design and designs for models with random\neffects.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 14:53:07 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Atkinson", "Anthony C.", ""], ["Woods", "David C.", ""]]}, {"id": "1510.05284", "submitter": "Werner M\\\"uller", "authors": "Eva Benkov\\'a, Radoslav Harman, Werner G. M\\\"uller", "title": "Privacy sets for constrained space-filling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides typology for space filling into what we call \"soft\" and\n\"hard\" methods along with introducing the central notion of privacy sets for\ndealing with the latter. A heuristic algorithm based on this notion is\npresented and we compare its performance on some well-known examples.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 17:25:43 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Benkov\u00e1", "Eva", ""], ["Harman", "Radoslav", ""], ["M\u00fcller", "Werner G.", ""]]}, {"id": "1510.05417", "submitter": "Toshiki Sato", "authors": "Toshiki Sato, Yuichi Takano, Ryuhei Miyashiro", "title": "Piecewise-Linear Approximation for Feature Subset Selection in a\n  Sequential Logit Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns a method of selecting a subset of features for a\nsequential logit model. Tanaka and Nakagawa (2014) proposed a mixed integer\nquadratic optimization formulation for solving the problem based on a quadratic\napproximation of the logistic loss function. However, since there is a\nsignificant gap between the logistic loss function and its quadratic\napproximation, their formulation may fail to find a good subset of features. To\novercome this drawback, we apply a piecewise-linear approximation to the\nlogistic loss function. Accordingly, we frame the feature subset selection\nproblem of minimizing an information criterion as a mixed integer linear\noptimization problem. The computational results demonstrate that our\npiecewise-linear approximation approach found a better subset of features than\nthe quadratic approximation approach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 10:44:53 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Sato", "Toshiki", ""], ["Takano", "Yuichi", ""], ["Miyashiro", "Ryuhei", ""]]}, {"id": "1510.05526", "submitter": "Jakob S\\\"ohl", "authors": "Richard Nickl and Jakob S\\\"ohl", "title": "Nonparametric Bayesian posterior contraction rates for discretely\n  observed scalar diffusions", "comments": "44 pages", "journal-ref": "Ann. Statist. 45(4) (2017) 1664-1693", "doi": "10.1214/16-AOS1504", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric Bayesian inference in a reflected diffusion model\n$dX_t = b (X_t)dt + \\sigma(X_t) dW_t,$ with discretely sampled observations\n$X_0, X_\\Delta, \\dots, X_{n\\Delta}$. We analyse the nonlinear inverse problem\ncorresponding to the `low frequency sampling' regime where $\\Delta>0$ is fixed\nand $n \\to \\infty$. A general theorem is proved that gives conditions for prior\ndistributions $\\Pi$ on the diffusion coefficient $\\sigma$ and the drift\nfunction $b$ that ensure minimax optimal contraction rates of the posterior\ndistribution over H\\\"older-Sobolev smoothness classes. These conditions are\nverified for natural examples of nonparametric random wavelet series priors.\nFor the proofs we derive new concentration inequalities for empirical processes\narising from discretely observed diffusions that are of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 15:18:10 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 13:30:30 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Nickl", "Richard", ""], ["S\u00f6hl", "Jakob", ""]]}, {"id": "1510.05723", "submitter": "Yolanda Hagar", "authors": "Yolanda Hagar, Mary Hayden, Abudulai Adams Forgor, Patricia Akweongo,\n  Abraham Hodgson, Christine Wiedinmyer, Vanja Dukic", "title": "Models with time-varying predictors for meningitis in Navrongo, Ghana", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"meningitis belt\" is a region in sub-Saharan Africa where annual\noutbreaks of meningitis occur, with large epidemics observed cyclically. While\nwe know that meningitis is heavily dependent on seasonal trends (in particular,\nweather), the exact pathways for contracting the disease are not fully\nunderstood and warrant further investigation. This manuscript examines\nmeningitis trends in the context of survival analysis, quantifying underlying\nseasonal patterns in meningitis rates through the hazard rate for the\npopulation of Navrongo, Ghana. We compare three candidate models: the commonly\nused Poisson generalized linear model, the Bayesian multi-resolution hazard\nmodel, and the Poisson generalized additive model. We compare the accuracy and\nrobustness of the models through the bias, RMSE, and the standard deviation. We\nprovide a detailed case study of meningitis patterns for data collected in\nNavrongo, Ghana.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 23:53:30 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Hagar", "Yolanda", ""], ["Hayden", "Mary", ""], ["Forgor", "Abudulai Adams", ""], ["Akweongo", "Patricia", ""], ["Hodgson", "Abraham", ""], ["Wiedinmyer", "Christine", ""], ["Dukic", "Vanja", ""]]}, {"id": "1510.05893", "submitter": "Pierre-Antoine Thouvenin", "authors": "Pierre-Antoine Thouvenin, Nicolas Dobigeon, Jean-Yves Tourneret", "title": "Online Unmixing of Multitemporal Hyperspectral Images accounting for\n  Spectral Variability", "comments": "27 pages, 11 figures, accepted in IEEE Trans. Image Process., 2016", "journal-ref": null, "doi": "10.1109/TIP.2016.2579309", "report-no": null, "categories": "physics.data-an cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing is aimed at identifying the reference spectral\nsignatures composing an hyperspectral image and their relative abundance\nfractions in each pixel. In practice, the identified signatures may vary\nspectrally from an image to another due to varying acquisition conditions, thus\ninducing possibly significant estimation errors. Against this background,\nhyperspectral unmixing of several images acquired over the same area is of\nconsiderable interest. Indeed, such an analysis enables the endmembers of the\nscene to be tracked and the corresponding endmember variability to be\ncharacterized. Sequential endmember estimation from a set of hyperspectral\nimages is expected to provide improved performance when compared to methods\nanalyzing the images independently. However, the significant size of\nhyperspectral data precludes the use of batch procedures to jointly estimate\nthe mixture parameters of a sequence of hyperspectral images. Provided that\neach elementary component is present in at least one image of the sequence, we\npropose to perform an online hyperspectral unmixing accounting for temporal\nendmember variability. The online hyperspectral unmixing is formulated as a\ntwo-stage stochastic program, which can be solved using a stochastic\napproximation. The performance of the proposed method is evaluated on synthetic\nand real data. A comparison with independent unmixing algorithms finally\nillustrates the interest of the proposed strategy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 13:47:24 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 09:17:00 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 16:05:14 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Thouvenin", "Pierre-Antoine", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1510.06053", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, Youssef M. Marzouk, Karen E. Willcox", "title": "Scalable posterior approximations for large-scale Bayesian inverse\n  problems via likelihood-informed parameter and state reduction", "comments": "35 pages, 12 figures", "journal-ref": "Journal of Computational Physics, Volume 315, 15 June 2016, Pages\n  363-387", "doi": "10.1016/j.jcp.2016.03.055", "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major bottlenecks to the solution of large-scale Bayesian inverse\nproblems are the scaling of posterior sampling algorithms to high-dimensional\nparameter spaces and the computational cost of forward model evaluations. Yet\nincomplete or noisy data, the state variation and parameter dependence of the\nforward model, and correlations in the prior collectively provide useful\nstructure that can be exploited for dimension reduction in this setting--both\nin the parameter space of the inverse problem and in the state space of the\nforward model. To this end, we show how to jointly construct low-dimensional\nsubspaces of the parameter space and the state space in order to accelerate the\nBayesian solution of the inverse problem. As a byproduct of state dimension\nreduction, we also show how to identify low-dimensional subspaces of the data\nin problems with high-dimensional observations. These subspaces enable\napproximation of the posterior as a product of two factors: (i) a projection of\nthe posterior onto a low-dimensional parameter subspace, wherein the original\nlikelihood is replaced by an approximation involving a reduced model; and (ii)\nthe marginal prior distribution on the high-dimensional complement of the\nparameter subspace. We present and compare several strategies for constructing\nthese subspaces using only a limited number of forward and adjoint model\nsimulations. The resulting posterior approximations can rapidly be\ncharacterized using standard sampling techniques, e.g., Markov chain Monte\nCarlo. Two numerical examples demonstrate the accuracy and efficiency of our\napproach: inversion of an integral equation in atmospheric remote sensing,\nwhere the data dimension is very high; and the inference of a heterogeneous\ntransmissivity field in a groundwater system, which involves a partial\ndifferential equation forward model with high dimensional state and parameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 20:45:41 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 01:42:29 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cui", "Tiangang", ""], ["Marzouk", "Youssef M.", ""], ["Willcox", "Karen E.", ""]]}, {"id": "1510.06112", "submitter": "Andrew Landgraf", "authors": "Andrew J. Landgraf and Yoonkyung Lee", "title": "Dimensionality Reduction for Binary Data through the Projection of\n  Natural Parameters", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2020.104668", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) for binary data, known as logistic PCA,\nhas become a popular alternative to dimensionality reduction of binary data. It\nis motivated as an extension of ordinary PCA by means of a matrix\nfactorization, akin to the singular value decomposition, that maximizes the\nBernoulli log-likelihood. We propose a new formulation of logistic PCA which\nextends Pearson's formulation of a low dimensional data representation with\nminimum error to binary data. Our formulation does not require a matrix\nfactorization, as previous methods do, but instead looks for projections of the\nnatural parameters from the saturated model. Due to this difference, the number\nof parameters does not grow with the number of observations and the principal\ncomponent scores on new data can be computed with simple matrix multiplication.\nWe derive explicit solutions for data matrices of special structure and provide\ncomputationally efficient algorithms for solving for the principal component\nloadings. Through simulation experiments and an analysis of medical diagnoses\ndata, we compare our formulation of logistic PCA to the previous formulation as\nwell as ordinary PCA to demonstrate its benefits.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 02:25:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Landgraf", "Andrew J.", ""], ["Lee", "Yoonkyung", ""]]}, {"id": "1510.06253", "submitter": "Bjoern Bornkamp", "authors": "Georg Gutjahr and Bj\\\"orn Bornkamp", "title": "Likelihood Ratio Tests for a Dose-Response Effect using Multiple\n  Nonlinear Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing for a dose-related effect based on a\ncandidate set of (typically nonlinear) dose-response models using\nlikelihood-ratio tests. For the considered models this reduces to assessing\nwhether the slope parameter in these nonlinear regression models is zero or\nnot. A technical problem is that the null distribution (when the slope is zero)\ndepends on non-identifiable parameters, so that standard asymptotic results on\nthe distribution of the likelihood-ratio test no longer apply. Asymptotic\nsolutions for this problem have been extensively discussed in the literature.\nThe resulting approximations however are not of simple form and require\nsimulation to calculate the asymptotic distribution. In addition their\nappropriateness might be doubtful for the case of a small sample size. Direct\nsimulation to approximate the null distribution is numerically unstable due to\nthe non identifiability of some parameters. In this article we derive a\nnumerical algorithm to approximate the exact distribution of the\nlikelihood-ratio test under multiple models for normally distributed data. The\nalgorithm uses methods from differential geometry and can be used to evaluate\nthe distribution under the null hypothesis, but also allows for power and\nsample size calculations. We compare the proposed testing approach to the\nMCP-Mod methodology and alternative methods for testing for a dose-related\ntrend in a dose-finding example data set and simulations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 13:53:09 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Gutjahr", "Georg", ""], ["Bornkamp", "Bj\u00f6rn", ""]]}, {"id": "1510.06319", "submitter": "Kory Johnson", "authors": "Kory D. Johnson, Dongyu Lin, Lyle H. Ungar, Dean P. Foster, and Robert\n  A. Stine", "title": "A Risk Ratio Comparison of $l_0$ and $l_1$ Penalized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an explosion of interest in using $l_1$-regularization in\nplace of $l_0$-regularization for feature selection. We present theoretical\nresults showing that while $l_1$-penalized linear regression never outperforms\n$l_0$-regularization by more than a constant factor, in some cases using an\n$l_1$ penalty is infinitely worse than using an $l_0$ penalty. We also show\nthat the \"optimal\" $l_1$ solutions are often inferior to $l_0$ solutions found\nusing stepwise regression.\n  We also compare algorithms for solving these two problems and show that\nalthough solutions can be found efficiently for the $l_1$ problem, the\n\"optimal\" $l_1$ solutions are often inferior to $l_0$ solutions found using\ngreedy classic stepwise regression. Furthermore, we show that solutions\nobtained by solving the convex $l_1$ problem can be improved by selecting the\nbest of the $l_1$ models (for different regularization penalties) by using an\n$l_0$ criterion. In other words, an approximate solution to the right problem\ncan be better than the exact solution to the wrong problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 16:13:36 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Johnson", "Kory D.", ""], ["Lin", "Dongyu", ""], ["Ungar", "Lyle H.", ""], ["Foster", "Dean P.", ""], ["Stine", "Robert A.", ""]]}, {"id": "1510.06322", "submitter": "Kory Johnson", "authors": "Kory D. Johnson, Robert A. Stine, and Dean P. Foster", "title": "Fitting High-Dimensional Interaction Models with Error Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a renewed interest in polynomial regression in the form of\nidentifying influential interactions between features. In many settings, this\ntakes place in a high-dimensional model, making the number of interactions\nunwieldy or computationally infeasible. Furthermore, it is difficult to analyze\nsuch spaces directly as they are often highly correlated. Standard feature\nselection issues remain such as how to determine a final model which\ngeneralizes well. This paper solves these problems with a sequential algorithm\ncalled Revisiting Alpha-Investing (RAI). RAI is motivated by the principle of\nmarginality and searches the feature-space of higher-order interactions by\ngreedily building upon lower-order terms. RAI controls a notion of false\nrejections and comes with a performance guarantee relative to the best-subset\nmodel. This ensures that signal is identified while providing a valid stopping\ncriterion to prevent over-selection. We apply RAI in a novel setting over a\nfamily of regressions in order to select gene-specific interaction models for\ndifferential expression profiling.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 16:17:45 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 14:10:53 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 15:35:40 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 09:36:04 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Johnson", "Kory D.", ""], ["Stine", "Robert A.", ""], ["Foster", "Dean P.", ""]]}, {"id": "1510.06618", "submitter": "Vincent Loonis", "authors": "Vincent Loonis, Xavier Mary", "title": "Determinantal Sampling Designs", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, recent results about point processes are used in sampling\ntheory. Precisely, we define and study a new class of sampling designs:\ndeterminantal sampling designs. The law of such designs is known, and there\nexists a simple selection algorithm. We compute exactly the variance of linear\nestimators constructed upon these designs by using the first and second order\ninclusion probabilities. Moreover, we obtain asymptotic and finite sample\ntheorems. We construct explicitly fixed size determinantal sampling designs\nwith given first order inclusion probabilities. We also address the search of\noptimal determinantal sampling designs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 13:24:14 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 16:16:35 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Loonis", "Vincent", ""], ["Mary", "Xavier", ""]]}, {"id": "1510.06731", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb, Pasquale Cirillo", "title": "On the shadow moments of apparently infinite-mean phenomena", "comments": null, "journal-ref": "Unifying Themes in Complex Systems IX (2018), Proceedings of the\n  Ninth International Conference on Complex Systems, Alfredo J. Morales ,\n  Carlos Gershenson, Dan Braha, Ali A. Minai, & Yaneer Bar-Yam, Eds., Springer,\n  pp 155-164", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to compute the conditional moments of fat-tailed\nphenomena that, only looking at data, could be mistakenly considered as having\ninfinite mean. This type of problems manifests itself when a random variable Y\nhas a heavy- tailed distribution with an extremely wide yet bounded support. We\nintroduce the concept of dual distribution, by means of a log-transformation\nthat removes the upper bound. The tail of the dual distribution can then be\nstudied using extreme value theory, without making excessive parametric\nassumptions, and the estimates one obtains can be used to study the original\ndistribution and compute its moments by reverting the log- transformation. The\ncentral difference between our approach and a simple truncation is in the\nsmoothness of the transformation between the original and the dual\ndistribution, allowing use of extreme value theory. War casualties, operational\nrisk, environment blight, complex networks and many other econophysics\nphenomena are possible fields of application.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 19:48:31 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 15:15:33 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Taleb", "Nassim Nicholas", ""], ["Cirillo", "Pasquale", ""]]}, {"id": "1510.06817", "submitter": "Tirthankar Dasgupta", "authors": "Jonathan Hennessy, Tirthankar Dasgupta, Luke Miratrix, Cassandra\n  Pattanayak, Pradipta Sarkar", "title": "A conditional randomization test to account for covariate imbalance in\n  randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the conditional randomization test as a way to account for\ncovariate imbalance in randomized experiments. The test accounts for covariate\nimbalance by comparing the observed test statistic to the null distribution of\nthe test statistic conditional on the observed covariate imbalance. We prove\nthat the conditional randomization test has the correct significance level and\nintroduce original notation to describe covariate balance more formally.\nThrough simulation, we verify that conditional randomization tests behave like\nmore traditional forms of covariate adjustmet but have the added benefit of\nhaving the correct conditional significance level. Finally, we apply the\napproach to a randomized product marketing experiment where covariate\ninformation was collected after randomization.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 03:18:00 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 22:16:06 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Hennessy", "Jonathan", ""], ["Dasgupta", "Tirthankar", ""], ["Miratrix", "Luke", ""], ["Pattanayak", "Cassandra", ""], ["Sarkar", "Pradipta", ""]]}, {"id": "1510.06852", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "Correlation structure and variable selection in generalized estimating\n  equations via composite likelihood information criteria", "comments": null, "journal-ref": "Statistics in Medicine, 2016, 35(14), 2377--2390", "doi": "10.1002/sim.6871", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of generalized estimating equations (GEE) is popular in the\nbiostatistics literature for analyzing longitudinal binary and count data. It\nassumes a generalized linear model (GLM) for the outcome variable, and a\nworking correlation among repeated measurements. In this paper, we introduce a\nviable competitor: the weighted scores method for GLM margins. We weight the\nunivariate score equations using a working discretized multivariate normal\nmodel that is a proper multivariate model. Since the weighted scores method is\na parametric method based on likelihood, we propose composite likelihood\ninformation criteria as an intermediate step for model selection. The same\ncriteria can be used for both correlation structure and variable selection.\nSimulations studies and the application example show that our method\noutperforms other existing model selection methods in GEE. From the example, it\ncan be seen that our methods allow for correct analysis, and may change the\ninferential results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 08:13:13 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 18:04:51 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1510.06905", "submitter": "William Flanders", "authors": "W. Dana Flanders, Matthew J. Strickland and Mitchel Klein", "title": "A New Method for Partial Correction of Residual Confounding in\n  Time-Series and other Observational Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Methods now exist to detect residual confounding. One requires\nan \"indicator\" with two key properties: conditional independence of the outcome\n(given exposure and measured covariates) absent confounding and other model\nmiss-specification; and an association with unmeasured confounders (like the\nexposure). We now present a new method for correcting for residual confounding\nin time-series and other epidemiological studies. We argue that estimators from\nmodels that include an indicator with these key properties should have less\nbias than those from models without the indicator.\n  Methods: Using causal reasoning and basic regression theory we present\ntheoretical arguments to support our claims. In simulations, we empirically\nevaluate our approach using a time-series study of ozone effects on emergency\ndepartment visits for asthma (AV). We base simulations on observed data for\nozone, meteorological factors and asthma.\n  Results: In simulations, results from models that included ozone\nconcentrations one day after the AV yielded effect estimators with slightly or\nmodestly less residual confounding.\n  Conclusion: Theory and simulations show that including the indicator based on\nfuture air pollution levels can reduce residual confounding. Our method differs\nfrom available methods because it uses a regression approach involving an\nexposure-based indicator rather than a negative outcome control.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 12:09:32 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Flanders", "W. Dana", ""], ["Strickland", "Matthew J.", ""], ["Klein", "Mitchel", ""]]}, {"id": "1510.06971", "submitter": "Malte S. Kurz", "authors": "Fabian Spanhel and Malte S. Kurz", "title": "The partial vine copula: A dependence measure and approximation based on\n  the simplifying assumption", "comments": "A previous version of this paper was circulated on arXiv under the\n  title \"Simplified vine copula models: Approximations based on the simplifying\n  assumption\"", "journal-ref": "Electronic Journal of Statistics 13 (1), 2019, 1254-1291", "doi": "10.1214/19-EJS1547", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplified vine copulas (SVCs), or pair-copula constructions, have become an\nimportant tool in high-dimensional dependence modeling. So far, specification\nand estimation of SVCs has been conducted under the simplifying assumption,\ni.e., all bivariate conditional copulas of the vine are assumed to be bivariate\nunconditional copulas. We introduce the partial vine copula (PVC) which\nprovides a new multivariate dependence measure and which plays a major role in\nthe approximation of multivariate distributions by SVCs. The PVC is a\nparticular SVC where to any edge a j-th order partial copula is assigned and\nconstitutes a multivariate analogue of the bivariate partial copula. We\ninvestigate to what extent the PVC describes the dependence structure of the\nunderlying copula. We show that the PVC does not minimize the Kullback-Leibler\ndivergence from the true copula and that the best approximation satisfying the\nsimplifying assumption is given by a vine pseudo-copula. However, under\nregularity conditions, step-wise estimators of pair-copula constructions\nconverge to the PVC irrespective of whether the simplifying assumption holds or\nnot. Moreover, we elucidate why the PVC is the best feasible SVC approximation\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 15:35:09 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 11:04:50 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Spanhel", "Fabian", ""], ["Kurz", "Malte S.", ""]]}, {"id": "1510.07144", "submitter": "Lixing Zhu", "authors": "Xuehu Zhu, Xu Guo and Lixing Zhu", "title": "An adaptive-to-model test for partially parametric single-index models", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual marked empirical process-based tests are commonly used in regression\nmodels. However, they suffer from data sparseness in high-dimensional space\nwhen there are many covariates. This paper has three purposes. First, we\nsuggest a partial dimension reduction adaptive-to-model testing procedure that\ncan be omnibus against general global alternative models although it fully use\nthe dimension reduction structure under the null hypothesis. This feature is\nbecause that the procedure can automatically adapt to the null and alternative\nmodels, and thus greatly overcomes the dimensionality problem. Second, to\nachieve the above goal, we propose a ridge-type eigenvalue ratio estimate to\nautomatically determine the number of linear combinations of the covariates\nunder the null and alternatives. Third, a Monte-Carlo approximation to the\nsampling null distribution is suggested. Unlike existing bootstrap\napproximation methods, this gives an approximation as close to the sampling\nnull distribution as possible by fully utilising the dimension reduction model\nstructure under the null. Simulation studies and real data analysis are then\nconducted to illustrate the performance of the new test and compare it with\nexisting tests.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 13:27:30 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Zhu", "Xuehu", ""], ["Guo", "Xu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1510.07158", "submitter": "Yang Li", "authors": "Ke Deng, Yang Li, Weiping Zhu, Jun S. Liu", "title": "Fast Parameter Estimation in Loss Tomography for Networks of General\n  Topology", "comments": "To appear in Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a technique to investigate link-level loss rates of a computer network\nwith low operational cost, loss tomography has received considerable attentions\nin recent years. A number of parameter estimation methods have been proposed\nfor loss tomography of networks with a tree structure as well as a general\ntopological structure. However, these methods suffer from either high\ncomputational cost or insufficient use of information in the data. In this\npaper, we provide both theoretical results and practical algorithms for\nparameter estimation in loss tomography. By introducing a group of novel\nstatistics and alternative parameter systems, we find that the likelihood\nfunction of the observed data from loss tomography keeps exactly the same\nmathematical formulation for tree and general topologies, revealing that\nnetworks with different topologies share the same mathematical nature for loss\ntomography. More importantly, we discover that a re-parametrization of the\nlikelihood function belongs to the standard exponential family, which is convex\nand has a unique mode under regularity conditions. Based on these theoretical\nresults, novel algorithms to find the MLE are developed. Compared to existing\nmethods in the literature, the proposed methods enjoy great computational\nadvantages.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 16:22:36 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Deng", "Ke", ""], ["Li", "Yang", ""], ["Zhu", "Weiping", ""], ["Liu", "Jun S.", ""]]}, {"id": "1510.07180", "submitter": "Eisa Mahmoudi", "authors": "Eisa Mahmoudi and Hamed Mahmoodian", "title": "Normal Power Series Class of Distributions: Model, Properties and\n  Applications", "comments": "Submitted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of distributions, called as normal power series (NPS), which\ncontains the normal one as a particular case, is introduced in this paper. This\nnew class which is obtained by compounding the normal and power series\ndistributions, is presented as an alternative to the class of skew-normal and\nBalakrishnan skew-normal distributions, among others. The density and\ndistribution functions of this new class of distributions, are given by a\nclosed expression which allows us to easily compute probabilities, moments and\nrelated measurements. Estimation of the parameters of this new model by maximum\nlikelihood method via an EM- algorithm is given. Finally, some applications are\nshown as examples.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 19:56:25 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Mahmoudi", "Eisa", ""], ["Mahmoodian", "Hamed", ""]]}, {"id": "1510.07287", "submitter": "Fabrizio Leisen", "authors": "Weixuan Zhu, Juan Miguel Marin and Fabrizio Leisen", "title": "A Bootstrap Likelihood approach to Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing amount of literature focused on Bayesian computational\nmethods to address problems with intractable likelihood. One approach is a set\nof algorithms known as Approximate Bayesian Computational (ABC) methods. One of\nthe problems of these algorithms is that the performance depends on the tuning\nof some parameters, such as the summary statistics, distance and tolerance\nlevel. To bypass this problem, Mengersen, Pudlo and Robert (2013) introduced an\nalternative method based on empirical likelihood, which can be easily\nimplemented when a set of constraints, related to the moments of the\ndistribution, is known. However, the choice of the constraints is sometimes\nchallenging. To overcome this problem, we propose an alternative method based\non a bootstrap likelihood approach. The method is easy to implement and in some\ncases it is faster than the other approaches. The performance of the algorithm\nis illustrated with examples in Population Genetics, Time Series and Stochastic\nDifferential Equations. Finally, we test the method on a real dataset.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 18:58:32 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Zhu", "Weixuan", ""], ["Marin", "Juan Miguel", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1510.07302", "submitter": "Elvan Ceyhan", "authors": "E. Ceyhan, T. Nishino, K.N. Botteron, M.I. Miller, J.T. Ratnanather", "title": "Analysis of Cortical Morphometric Variability Using Labeled Cortical\n  Distance Maps", "comments": "40 pages, 16 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": "KU-EC-15-1", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphometric differences in the anatomy of cortical structures are associated\nwith neuro-developmental and neuropsychiatric disorders. Such differences can\nbe quantized and detected by a powerful tool called Labeled Cortical Distance\nMap (LCDM). The LCDM method pro-vides distances of labeled gray matter (GM)\nvoxels from the GM/white matter (WM) surface for specific cortical structures\n(or tissues). Here we describe a method to analyze morphometric variability in\nthe particular tissue using LCDM distances. To extract more of the information\nprovided by LCDM distances, we perform pooling and censoring of LCDM distances.\nIn particular, we employ Brown-Forsythe (BF) test of homogeneity of variance\n(HOV) on the LCDM distances. HOV analysis of pooled distances provides an\noverall analysis of morphometric variability of the LCDMs due to the disease in\nquestion, while the HOV analysis of censored distances suggests the location(s)\nof significant variation in these differences (i.e., at which distance from the\nGM/WM surface the morphometric variability starts to be significant). We also\ncheck for the influence of assumption violations on the HOV analysis of LCDM\ndistances. In particular, we demonstrate that BF HOV test is robust to\nassumption violations such as the non-normality and within sample dependence of\nthe residuals from the median for pooled and censored distances and are robust\nto data aggregation which occurs in analysis of censored distances. We\nillustrate the methodology on a real data example, namely, LCDM distances of GM\nvoxels in ventral medial prefrontal cortices (VMPFCs) to see the effects of\ndepression or being of high risk to depression on the morphometry of VMPFCs.\nThe methodology used here is also valid for morphometric analysis of other\ncortical structures.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 20:59:58 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Ceyhan", "E.", ""], ["Nishino", "T.", ""], ["Botteron", "K. N.", ""], ["Miller", "M. I.", ""], ["Ratnanather", "J. T.", ""]]}, {"id": "1510.07361", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Tatsuya Kubokawa and Kota Ogasawara", "title": "Empirical Uncertain Bayes Methods in Area-level Models", "comments": "26 pages; to appear in Scandinavian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random effects model can account for the lack of fitting a regression model\nand increase precision of estimating area-level means. However, in case that\nthe synthetic mean provides accurate estimates, the prior distribution may\ninflate an estimation error. Thus it is desirable to consider the uncertain\nprior distribution, which is expressed as the mixture of a one-point\ndistribution and a proper prior distribution. In this paper, we develop an\nempirical Bayes approach for estimating area-level means, using the uncertain\nprior distribution in the context of a natural exponential family, which we\ncall the empirical uncertain Bayes (EUB) method. The regression model\nconsidered in this paper includes the Poisson-gamma and the binomial-beta, and\nthe normal-normal (Fay-Herriot) model, which are typically used in small area\nestimation. We obtain the estimators of hyperparameters based on the marginal\nlikelihood by using a well-known Expectation-Maximization algorithm, and\npropose the EUB estimators of area means. For risk evaluation of the EUB\nestimator, we derive a second-order unbiased estimator of a conditional mean\nsquared error by using some techniques of numerical calculation. Through\nsimulation studies and real data applications, we evaluate a performance of the\nEUB estimator and compare it with the usual empirical Bayes estimator.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 04:15:34 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 04:27:50 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 05:30:40 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""], ["Ogasawara", "Kota", ""]]}, {"id": "1510.07376", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "Weighted scores method for longitudinal ordinal data", "comments": "arXiv admin note: text overlap with arXiv:1510.06852", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending generalized estimating equations (GEE) to ordinal response data\nrequires a conversion of the ordinal response to a vector of binary category\nindicators. That leads to a rather complicated association structure, and the\nintroduction of large matrices when the number of categories and dimension of\nthe cluster are large. To allow a richer specification of working correlation\nassumptions, we adopt the weighted scores method which is essentially an\nextension of the GEE approach, since it can also be applied to families that\nare not in the GLM class. The weighted scores method stems from the lack of a\ntheoretically sound methodology for analyzing multivariate discrete data based\nonly on moments up to second order and it is robust to dependence and nearly as\nefficient as maximum likelihood. There is no need to convert the ordinal\nresponse to binary indicators, thus the weight matrices have smaller dimensions\nand it is not necessary to guess the correlations of indicator variables for\ndifferent categories. We focus on important issues that would interest the data\nanalyst, such as choice of the structure of the correlation matrix and of\nexplanatory variables, comparison of results obtained from our methods versus\nGEE, and insights provided by our method that would be missed with the GEE\nmethod. Our modelling framework is implemented in the package weightedScores\nwithin the open source statistical environment R.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 06:30:39 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 09:51:07 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 18:12:24 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1510.07476", "submitter": "Ihab Sraj", "authors": "Ihab Sraj and Sarah E. Zedler and Omar M. Knio and Charles S. Jackson\n  and Ibrahim Hoteit", "title": "Polynomial Chaos-based Bayesian Inference of K-Profile Parametrization\n  in a General Circulation Model of the Tropical Pacific", "comments": null, "journal-ref": null, "doi": "10.1175/MWR-D-15-0394.1", "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors present a Polynomial Chaos (PC)-based Bayesian inference method\nfor quantifying the uncertainties of the K-Profile Parametrization (KPP) within\nthe MIT General Circulation Model (MITgcm) of the tropical pacific. The\ninference of the uncertain parameters is based on a Markov Chain Monte Carlo\n(MCMC) scheme that utilizes a newly formulated test statistic taking into\naccount the different components representing the structures of turbulent\nmixing on both daily and seasonal timescales in addition to the data quality,\nand filters for the effects of parameter perturbations over those due to\nchanges in the wind. To avoid the prohibitive computational cost of integrating\nthe MITgcm model at each MCMC iteration, we build a surrogate model for the\ntest statistic using the PC method. To filter out the noise in the model\npredictions and avoid related convergence issues, we resort to a\nBasis-Pursuit-DeNoising (BPDN) compressed sensing approach to determine the PC\ncoefficients of a representative surrogate model. The PC surrogate is then used\nto evaluate the test statistic in the MCMC step for sampling the posterior of\nthe uncertain parameters. Results of the posteriors indicate good agreement\nwith the default values for two parameters of the KPP model namely the critical\nbulk and gradient Richardson numbers; while the posteriors of the remaining\nparameters were barely informative.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 13:34:50 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Sraj", "Ihab", ""], ["Zedler", "Sarah E.", ""], ["Knio", "Omar M.", ""], ["Jackson", "Charles S.", ""], ["Hoteit", "Ibrahim", ""]]}, {"id": "1510.07800", "submitter": "Soumen Manna", "authors": "Soumen Manna, Ashish Das", "title": "Optimal two-level designs for partial profile choice experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the existing results of optimal partial profile paired choice\ndesigns and provide new designs for situations where the choice set sizes are\ngreater than two. The optimal designs are obtained under the main effects\nmodels and the broader main effects model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 07:51:18 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Manna", "Soumen", ""], ["Das", "Ashish", ""]]}, {"id": "1510.07840", "submitter": "Denis Allard", "authors": "Marc Bourotte, Denis Allard, Emilio Porcu", "title": "A Flexible Class of Non-separable Cross-Covariance Functions for\n  Multivariate Space-Time Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate space-time data are increasingly available in various scientific\ndisciplines. When analyzing these data, one of the key issues is to describe\nthe multivariate space-time dependencies. Under the Gaussian framework, one\nneeds to propose relevant models for multivariate space-time covariance\nfunctions, i.e. matrix-valued mappings with the additional requirement of\nnon-negative definiteness. We propose a flexible parametric class of\ncross-covariance functions for multivariate space-time Gaussian random fields.\nSpace-time components belong to the (univariate) Gneiting class of space-time\ncovariance functions, with Mat\\'ern or Cauchy covariance functions in the\nspatial margins. The smoothness and scale parameters can be different for each\nvariable. We provide sufficient conditions for positive definiteness. A\nsimulation study shows that the parameters of this model can be efficiently\nestimated using weighted pairwise likelihood, which belongs to the class of\ncomposite likelihood methods. We then illustrate the model on a French dataset\nof weather variables.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 10:27:59 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 08:07:19 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Bourotte", "Marc", ""], ["Allard", "Denis", ""], ["Porcu", "Emilio", ""]]}, {"id": "1510.07850", "submitter": "Bruno Torresani", "authors": "Marie-Christine Roubaud (LATP), Bruno Torr\\'esani (LATP)", "title": "A Bayesian model for microarray datasets merging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aggregation of microarray datasets originating from different studies is\nstill a difficult open problem. Currently, best results are generally obtained\nby the so-called meta-analysis approach, which aggregates results from\nindividual datasets, instead of analyzing aggre-gated datasets. In order to\ntackle such aggregation problems, it is necessary to correct for interstudy\nvariability prior to aggregation. The goal of this paper is to present a new\napproach for microarray datasets merging, based upon explicit modeling of\ninterstudy variability and gene variability. We develop and demonstrate a new\nalgorithm for microarray datasets merging. The underlying model assumes\nnormally distributed intrinsic gene expressions, distorted by a study-dependent\nnonlinear transformation, and study dependent (normally distributed)\nobservation noise. The algorithm addresses both parameter estimation (the\nparameters being gene expression means and variances, observation noise\nvariances and the nonlinear transformations) and data adjustment, and yields as\na result adjusted datasets suitable for aggregation. The method is validated on\ntwo case studies. The first one concerns E. Coli expression data, artificially\ndistorted by given nonlinear transformations and additive observation noise.\nThe proposed method is able to correct for the distortion, and yields adjusted\ndatasets from which the relevant biological effects can be recovered, as shown\nby a standard differential analysis. The second case study concerns the\naggregation of two real prostate cancer datasets. After adjustment using the\nproposed algorithm, a differential analysis performed on adjusted datasets\nyields a larger number of differentially expressed genes (between control and\ntumor data). The proposed method has been implemented using the statistical\nsoftware R 1, and Bioconductor packages 2. The source code (valid for merging\ntwo datasets), as well as the datasets used for the validation, and some\ncomplementary results, are made available on the web site\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 10:43:24 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Roubaud", "Marie-Christine", "", "LATP"], ["Torr\u00e9sani", "Bruno", "", "LATP"]]}, {"id": "1510.08151", "submitter": "Ted Westling", "authors": "Ted Westling and Tyler H. McCormick", "title": "Beyond prediction: A framework for inference with variational\n  approximations in mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a popular method for estimating model parameters and\nconditional distributions in hierarchical and mixed models, which arise\nfrequently in many settings in the health, social, and biological sciences.\nVariational inference in a frequentist context works by approximating\nintractable conditional distributions with a tractable family and optimizing\nthe resulting lower bound on the log-likelihood. The variational objective\nfunction is typically less computationally intensive to optimize than the true\nlikelihood, enabling scientists to fit rich models even with extremely large\ndatasets. Despite widespread use, little is known about the general theoretical\nproperties of estimators arising from variational approximations to the\nlog-likelihood, which hinders their use in inferential statistics. In this\npaper we connect such estimators to profile M-estimation, which enables us to\nprovide regularity conditions for consistency and asymptotic normality of\nvariational estimators. Our theory also motivates three methodological\nimprovements to variational inference: estimation of the asymptotic\nmodel-robust covariance matrix, a one-step correction that improves estimator\nefficiency, and an empirical assessment of consistency. We evaluate the\nproposed results using simulation studies and data on marijuana use from the\nNational Longitudinal Study of Youth.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 01:14:07 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 18:55:54 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2017 16:17:46 GMT"}, {"version": "v4", "created": "Mon, 20 Nov 2017 20:29:44 GMT"}, {"version": "v5", "created": "Thu, 13 Sep 2018 19:00:04 GMT"}, {"version": "v6", "created": "Wed, 9 Jan 2019 14:12:51 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Westling", "Ted", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1510.08178", "submitter": "Xiaotian Zhu", "authors": "Xiaotian Zhu and David R. Hunter", "title": "Clustering Via Finite Nonparametric ICA Mixture Models", "comments": "23 pages, 5 figures, Adv Data Anal Classif (2018)", "journal-ref": null, "doi": "10.1007/s11634-018-0338-x", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of non-parametric multivariate finite mixture models\nby dropping the standard conditional independence assumption and incorporating\nthe independent component analysis (ICA) structure instead. We formulate an\nobjective function in terms of penalized smoothed Kullback Leibler distance and\nintroduce the nonlinear smoothed majorization-minimization independent\ncomponent analysis (NSMM-ICA) algorithm for optimizing this function and\nestimating the model parameters. We have implemented a practical version of\nthis algorithm, which utilizes the FastICA algorithm, in the R package icamix.\nWe illustrate this new methodology using several applications in unsupervised\nlearning and image processing.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 03:21:48 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 04:09:45 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 04:39:36 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Zhu", "Xiaotian", ""], ["Hunter", "David R.", ""]]}, {"id": "1510.08281", "submitter": "Soumen Manna", "authors": "Soumen Manna", "title": "Optimal two-level choice designs for the main effects and specified\n  interaction effects model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choice designs for the main effects model, broader main effects model and\nmain effects plus specified interaction effects model are discussed in this\npaper. Universally optimal choice designs are obtained for all of these models\nusing Hadamard matrix and other combinatorial techniques. Choice experiments\nunder the multinomial logit model for equally attractive options are assumed\nfor finding universally optimal choice designs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 12:27:45 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Manna", "Soumen", ""]]}, {"id": "1510.08437", "submitter": "Omkar Muralidharan", "authors": "Omkar Muralidharan and Amir Najmi", "title": "Second Order Calibration: A Simple Way to Get Approximate Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale machine learning problems involve estimating an unknown\nparameter $\\theta_{i}$ for each of many items. For example, a key problem in\nsponsored search is to estimate the click through rate (CTR) of each of\nbillions of query-ad pairs. Most common methods, though, only give a point\nestimate of each $\\theta_{i}$. A posterior distribution for each $\\theta_{i}$\nis usually more useful but harder to get.\n  We present a simple post-processing technique that takes point estimates or\nscores $t_{i}$ (from any method) and estimates an approximate posterior for\neach $\\theta_{i}$. We build on the idea of calibration, a common\npost-processing technique that estimates\n$\\mathrm{E}\\left(\\theta_{i}\\!\\!\\bigm|\\!\\! t_{i}\\right)$. Our method, second\norder calibration, uses empirical Bayes methods to estimate the distribution of\n$\\theta_{i}\\!\\!\\bigm|\\!\\! t_{i}$ and uses the estimated distribution as an\napproximation to the posterior distribution of $\\theta_{i}$. We show that this\ncan yield improved point estimates and useful accuracy estimates. The method\nscales to large problems - our motivating example is a CTR estimation problem\ninvolving tens of billions of query-ad pairs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 19:58:31 GMT"}], "update_date": "2015-11-27", "authors_parsed": [["Muralidharan", "Omkar", ""], ["Najmi", "Amir", ""]]}, {"id": "1510.08440", "submitter": "Diana Cai", "authors": "Diana Cai, Nathanael Ackerman, Cameron Freer", "title": "Priors on exchangeable directed graphs", "comments": "27 pages, 11 figures", "journal-ref": "Electronic Journal of Statistics 10 (2016), 3490-3515", "doi": "10.1214/16-EJS1185", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed graphs occur throughout statistical modeling of networks, and\nexchangeability is a natural assumption when the ordering of vertices does not\nmatter. There is a deep structural theory for exchangeable undirected graphs,\nwhich extends to the directed case via measurable objects known as digraphons.\nUsing digraphons, we first show how to construct models for exchangeable\ndirected graphs, including special cases such as tournaments, linear orderings,\ndirected acyclic graphs, and partial orderings. We then show how to construct\npriors on digraphons via the infinite relational digraphon model (di-IRM), a\nnew Bayesian nonparametric block model for exchangeable directed graphs, and\ndemonstrate inference on synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 19:59:13 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 16:22:19 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Cai", "Diana", ""], ["Ackerman", "Nathanael", ""], ["Freer", "Cameron", ""]]}, {"id": "1510.08539", "submitter": "Keli Liu", "authors": "Keli Liu and Xiao-Li Meng", "title": "There is Individualized Treatment. Why Not Individualized Inference?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doctors use statistics to advance medical knowledge; we use a medical analogy\nto introduce statistical inference \"from scratch\" and to highlight an\nimprovement. Your doctor, perhaps implicitly, predicts the effectiveness of a\ntreatment for you based on its performance in a clinical trial; the trial\npatients serve as controls for you. The same logic underpins statistical\ninference: to identify the best statistical procedure to use for a problem, we\nsimulate a set of control problems and evaluate candidate procedures on the\ncontrols. Now for the improvement: recent interest in\npersonalized/individualized medicine stems from the recognition that some\nclinical trial patients are better controls for you than others. Therefore,\ntreatment decisions for you should depend only on a subset of relevant\npatients. Individualized statistical inference implements this idea for control\nproblems (rather than patients). Its potential for improving data analysis\nmatches personalized medicine's for improving healthcare. The central\nissue--for both individualized medicine and individualized inference--is how to\nmake the right relevance robustness trade-off: if we exercise too much\njudgement in determining which controls are relevant, our inferences will not\nbe robust. How much is too much? We argue that the unknown answer is the Holy\nGrail of statistical inference.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 01:46:48 GMT"}], "update_date": "2015-11-29", "authors_parsed": [["Liu", "Keli", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1510.08862", "submitter": "Zhenke Wu", "authors": "Zhenke Wu, Maria Deloria-Knoll, Scott Zeger", "title": "Nested Partially-Latent Class Models for Dependent Binary Data;\n  Estimating Disease Etiology", "comments": "30 pages with 5 figures and 1 table; 1 appendix with 4 figures and 1\n  table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Pneumonia Etiology Research for Child Health (PERCH) study seeks to use\nmodern measurement technology to infer the causes of pneumonia for which\ngold-standard evidence is unavailable. The paper describes a latent variable\nmodel designed to infer from case-control data the etiology distribution for\nthe population of cases, and for an individual case given his or her\nmeasurements. We assume each observation is drawn from a mixture model for\nwhich each component represents one cause or disease class. The model addresses\na major limitation of the traditional latent class approach by taking account\nof residual dependence among multivariate binary outcome given disease class,\nhence reduces estimation bias, retains efficiency and offers more valid\ninference. Such \"local dependence\" on a single subject is induced in the model\nby nesting latent subclasses within each disease class. Measurement precision\nand covariation can be estimated using the control sample for whom the class is\nknown. In a Bayesian framework, we use stick-breaking priors on the subclass\nindicators for model-averaged inference across different numbers of subclasses.\nAssessment of model fit and individual diagnosis are done using posterior\nsamples drawn by Gibbs sampling. We demonstrate the utility of the method on\nsimulated and on the motivating PERCH data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:04:49 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Wu", "Zhenke", ""], ["Deloria-Knoll", "Maria", ""], ["Zeger", "Scott", ""]]}, {"id": "1510.08956", "submitter": "Jonas Mueller", "authors": "Jonas Mueller and Tommi Jaakkola", "title": "Principal Differences Analysis: Interpretable Characterization of\n  Differences between Distributions", "comments": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "journal-ref": "Advances in Neural Information Processing Systems 28: 1702-1710,\n  2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce principal differences analysis (PDA) for analyzing differences\nbetween high-dimensional distributions. The method operates by finding the\nprojection that maximizes the Wasserstein divergence between the resulting\nunivariate populations. Relying on the Cramer-Wold device, it requires no\nassumptions about the form of the underlying distributions, nor the nature of\ntheir inter-class differences. A sparse variant of the method is introduced to\nidentify features responsible for the differences. We provide algorithms for\nboth the original minimax formulation as well as its semidefinite relaxation.\nIn addition to deriving some convergence results, we illustrate how the\napproach may be applied to identify differences between cell populations in the\nsomatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our\nbroader framework extends beyond the specific choice of Wasserstein divergence.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 03:06:00 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Mueller", "Jonas", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1510.08986", "submitter": "Matey Neykov", "authors": "Matey Neykov, Yang Ning, Jun S. Liu, Han Liu", "title": "A Unified Theory of Confidence Regions and Testing for High Dimensional\n  Estimating Equations", "comments": "67 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new inferential framework for constructing confidence regions\nand testing hypotheses in statistical models specified by a system of high\ndimensional estimating equations. We construct an influence function by\nprojecting the fitted estimating equations to a sparse direction obtained by\nsolving a large-scale linear program. Our main theoretical contribution is to\nestablish a unified Z-estimation theory of confidence regions for high\ndimensional problems.\n  Different from existing methods, all of which require the specification of\nthe likelihood or pseudo-likelihood, our framework is likelihood-free. As a\nresult, our approach provides valid inference for a broad class of high\ndimensional constrained estimating equation problems, which are not covered by\nexisting methods.\n  Such examples include, noisy compressed sensing, instrumental variable\nregression, undirected graphical models, discriminant analysis and vector\nautoregressive models. We present detailed theoretical results for all these\nexamples. Finally, we conduct thorough numerical simulations, and a real\ndataset analysis to back up the developed theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 07:07:17 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 01:56:21 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Neykov", "Matey", ""], ["Ning", "Yang", ""], ["Liu", "Jun S.", ""], ["Liu", "Han", ""]]}, {"id": "1510.09072", "submitter": "Giovanni Marchetti", "authors": "Giovanni M. Marchetti and Nanny Wermuth", "title": "Palindromic Bernoulli distributions", "comments": "17 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a subclass of joint Bernoulli distributions which has\nthe palindromic property. For such distributions the vector of joint\nprobabilities is unchanged when the order of the elements is reversed. We prove\nfor binary variables that the palindromic property is equivalent to zero\nconstraints on all odd-order interaction parameters, be it in parameterizations\nwhich are log-linear, linear or multivariate logistic. In particular, we derive\nthe one-to-one parametric transformations for these three types of model\nspecifications and give simple closed forms of maximum likelihood estimates.\nSome special cases and a case study are described.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 13:02:10 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 12:17:58 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Marchetti", "Giovanni M.", ""], ["Wermuth", "Nanny", ""]]}, {"id": "1510.09130", "submitter": "Mingjun Zhong", "authors": "Mingjun Zhong, Nigel Goddard, Charles Sutton", "title": "Latent Bayesian melding for integrating individual and population models", "comments": "11 pages, Advances in Neural Information Processing Systems (NIPS),\n  2015. (Spotlight Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical problems, a more coarse-grained model may be suitable for\npopulation-level behaviour, whereas a more detailed model is appropriate for\naccurate modelling of individual behaviour. This raises the question of how to\nintegrate both types of models. Methods such as posterior regularization follow\nthe idea of generalized moment matching, in that they allow matching\nexpectations between two models, but sometimes both models are most\nconveniently expressed as latent variable models. We propose latent Bayesian\nmelding, which is motivated by averaging the distributions over populations\nstatistics of both the individual-level and the population-level models under a\nlogarithmic opinion pool framework. In a case study on electricity\ndisaggregation, which is a type of single-channel blind source separation\nproblem, we show that latent Bayesian melding leads to significantly more\naccurate predictions than an approach based solely on generalized moment\nmatching.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 15:39:06 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Zhong", "Mingjun", ""], ["Goddard", "Nigel", ""], ["Sutton", "Charles", ""]]}]