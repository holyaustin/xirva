[{"id": "0807.0053", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira", "title": "Frequentist and Bayesian measures of confidence via multiscale bootstrap\n  for testing three regions", "comments": null, "journal-ref": "Annals of the Institute of Statistical Mathematics 2010, Vol. 62,\n  pp. 189-208", "doi": "10.1007/s10463-009-0247-z", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new computation method of frequentist $p$-values and Bayesian posterior\nprobabilities based on the bootstrap probability is discussed for the\nmultivariate normal model with unknown expectation parameter vector. The null\nhypothesis is represented as an arbitrary-shaped region. We introduce new\nparametric models for the scaling-law of bootstrap probability so that the\nmultiscale bootstrap method, which was designed for one-sided test, can also\ncomputes confidence measures of two-sided test, extending applicability to a\nwider class of hypotheses. Parameter estimation is improved by the two-step\nmultiscale bootstrap and also by including higher-order terms. Model selection\nis important not only as a motivating application of our method, but also as an\nessential ingredient in the method. A compromise between frequentist and\nBayesian is attempted by showing that the Bayesian posterior probability with\nan noninformative prior is interpreted as a frequentist $p$-value of\n``zero-sided'' test.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2008 01:33:17 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Shimodaira", "Hidetoshi", ""]]}, {"id": "0807.1001", "submitter": "Claudia Tarantola Dr", "authors": "Ioannis Ntzoufras and Claudia Tarantola", "title": "Bayesian Analysis of Marginal Log-Linear Graphical Models for Three Way\n  Contingency Tables", "comments": "34 pages, 7 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the Bayesian analysis of graphical models of marginal\nindependence for three way contingency tables. We use a marginal log-linear\nparametrization, under which the model is defined through suitable\nzero-constraints on the interaction parameters calculated within marginal\ndistributions. We undertake a comprehensive Bayesian analysis of these models,\ninvolving suitable choices of prior distributions, estimation, model\ndetermination, as well as the allied computational issues. The methodology is\nillustrated with reference to two real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2008 12:39:53 GMT"}], "update_date": "2008-07-08", "authors_parsed": [["Ntzoufras", "Ioannis", ""], ["Tarantola", "Claudia", ""]]}, {"id": "0807.1005", "submitter": "Peter Grunwald", "authors": "Tim van Erven, Peter Grunwald and Steven de Rooij", "title": "Catching Up Faster by Switching Sooner: A Prequential Solution to the\n  AIC-BIC Dilemma", "comments": "A preliminary version of a part of this paper appeared at the NIPS\n  2007 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging, model selection and its approximations such as BIC\nare generally statistically consistent, but sometimes achieve slower rates og\nconvergence than other methods such as AIC and leave-one-out cross-validation.\nOn the other hand, these other methods can br inconsistent. We identify the\n\"catch-up phenomenon\" as a novel explanation for the slow convergence of\nBayesian methods. Based on this analysis we define the switch distribution, a\nmodification of the Bayesian marginal distribution. We show that, under broad\nconditions,model selection and prediction based on the switch distribution is\nboth consistent and achieves optimal convergence rates, thereby resolving the\nAIC-BIC dilemma. The method is practical; we give an efficient implementation.\nThe switch distribution has a data compression interpretation, and can thus be\nviewed as a \"prequential\" or MDL method; yet it is different from the MDL\nmethods that are usually considered in the literature. We compare the switch\ndistribution to Bayes factor model selection and leave-one-out\ncross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2008 12:57:23 GMT"}], "update_date": "2008-09-17", "authors_parsed": [["van Erven", "Tim", ""], ["Grunwald", "Peter", ""], ["de Rooij", "Steven", ""]]}, {"id": "0807.1106", "submitter": "Debashis Paul", "authors": "Debashis Paul, Jie Peng", "title": "Principal components analysis for sparsely observed correlated\n  functional data using a kernel smoothing approach", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the covariance kernel\nand its eigenvalues and eigenfunctions from sparse, irregularly observed, noise\ncorrupted and (possibly) correlated functional data. We present a method based\non pre-smoothing of individual sample curves through an appropriate kernel. We\nshow that the naive empirical covariance of the pre-smoothed sample curves\ngives highly biased estimator of the covariance kernel along its diagonal. We\nattend to this problem by estimating the diagonal and off-diagonal parts of the\ncovariance kernel separately. We then present a practical and efficient method\nfor choosing the bandwidth for the kernel by using an approximation to the\nleave-one-curve-out cross validation score. We prove that under standard\nregularity conditions on the covariance kernel and assuming i.i.d. samples, the\nrisk of our estimator, under $L^2$ loss, achieves the optimal nonparametric\nrate when the number of measurements per curve is bounded. We also show that\neven when the sample curves are correlated in such a way that the noiseless\ndata has a separable covariance structure, the proposed method is still\nconsistent and we quantify the role of this correlation in the risk of the\nestimator.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2008 20:02:50 GMT"}], "update_date": "2008-07-09", "authors_parsed": [["Paul", "Debashis", ""], ["Peng", "Jie", ""]]}, {"id": "0807.1201", "submitter": "Federico Bassetti", "authors": "Federico Bassetti", "title": "Quantitative comparisons between finitary posterior distributions and\n  Bayesian posterior distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main object of Bayesian statistical inference is the determination of\nposterior distributions. Sometimes these laws are given for quantities devoid\nof empirical value. This serious drawback vanishes when one confines oneself to\nconsidering a finite horizon framework. However, assuming infinite\nexchangeability gives rise to fairly tractable {\\it a posteriori} quantities,\nwhich is very attractive in applications. Hence, with a view to a\nreconciliation between these two aspects of the Bayesian way of reasoning, in\nthis paper we provide quantitative comparisons between posterior distributions\nof finitary parameters and posterior distributions of allied parameters\nappearing in usual statistical models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2008 09:52:38 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Bassetti", "Federico", ""]]}, {"id": "0807.1271", "submitter": "Tom Trigano", "authors": "T. Trigano and U. Isserles and Y. Ritov", "title": "Semiparametric curve alignment and shift density estimation for\n  biological data", "comments": "30 pages ; v5 : minor changes and correction in the proof of\n  Proposition 3.1", "journal-ref": null, "doi": "10.1109/TSP.2011.2113179", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that we observe a large number of curves, all of them with identical,\nalthough unknown, shape, but with a different random shift. The objective is to\nestimate the individual time shifts and their distribution. Such an objective\nappears in several biological applications like neuroscience or ECG signal\nprocessing, in which the estimation of the distribution of the elapsed time\nbetween repetitive pulses with a possibly low signal-noise ratio, and without a\nknowledge of the pulse shape is of interest. We suggest an M-estimator leading\nto a three-stage algorithm: we split our data set in blocks, on which the\nestimation of the shifts is done by minimizing a cost criterion based on a\nfunctional of the periodogram; the estimated shifts are then plugged into a\nstandard density estimator. We show that under mild regularity assumptions the\ndensity estimate converges weakly to the true shift distribution. The theory is\napplied both to simulations and to alignment of real ECG signals. The estimator\nof the shift distribution performs well, even in the case of low\nsignal-to-noise ratio, and is shown to outperform the standard methods for\ncurve alignment.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2008 16:46:02 GMT"}, {"version": "v2", "created": "Wed, 6 May 2009 08:26:33 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2010 06:15:38 GMT"}, {"version": "v4", "created": "Fri, 13 Aug 2010 14:54:06 GMT"}, {"version": "v5", "created": "Fri, 10 Dec 2010 11:26:12 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Trigano", "T.", ""], ["Isserles", "U.", ""], ["Ritov", "Y.", ""]]}, {"id": "0807.1574", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Khageswor Giri", "title": "Large-Sample Confidence Intervals for the Treatment Difference in a\n  Two-Period Crossover Trial, Utilizing Prior Information", "comments": "This version is the same as v1. This paper has been accepted for\n  publication in Statistics and Probability Letters", "journal-ref": "Statistics and Probability Letters, 79, 652-658 (2009)", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a two-treatment, two-period crossover trial, with responses that are\ncontinuous random variables. We find a large-sample frequentist 1-alpha\nconfidence interval for the treatment difference that utilizes the uncertain\nprior information that there is no differential carryover effect.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2008 05:38:36 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2008 23:39:02 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Kabaila", "Paul", ""], ["Giri", "Khageswor", ""]]}, {"id": "0807.2275", "submitter": "Marc Coram", "authors": "Ethan Anderes, Marc Coram", "title": "Two Dimensional Density Estimation using Smooth Invertible\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating a smooth invertible transformation f\nwhen observing independent samples X_1, ..., X_n ~ P \\circ f, where P is a\nknown measure. We focus on the two dimensional case where P and f are defined\non R^2. We present a flexible class of smooth invertible transformations in two\ndimensions with variational equations for optimizing over the classes, then\nstudy the problem of estimating the transformation f by penalized maximum\nlikelihood estimation. We apply our methodology to the case when P \\circ f has\na density with respect to Lebesgue measure on R^2 and demonstrate improvements\nover kernel density estimation on three examples.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2008 22:23:45 GMT"}], "update_date": "2008-07-16", "authors_parsed": [["Anderes", "Ethan", ""], ["Coram", "Marc", ""]]}, {"id": "0807.2598", "submitter": "Robb Muirhead", "authors": "Morris L. Eaton and Robb J. Muirhead", "title": "A decomposition result for the Haar distribution on the orthogonal group", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let H be a Haar distributed random matrix on the group of pxp real orthogonal\nmatrices. Partition H into four blocks: (1) the (1,1) element, (2)the rest of\nthe first row, (3) the rest of the first column, and (4)the remaining\n(p-1)x(p-1) matrix. The marginal distribution of (1) is well known. In this\npaper, we give the conditional distribution of (2) and (3) given (1), and the\nconditional distribution of (4) given (1), (2), (3). This conditional\nspecification uniquely determines the Haar distribution. The two conditional\ndistributions involve well known probability distributions namely, the uniform\ndistribution on the unit sphere in p-1 dimensional space and the Haar\ndistribution on (p-2)x(p-2) orthogonal matrices. Our results show how to\nconstruct the Haar distribution on pxp orthogonal matrices from the Haar\ndistribution on (p-2)x(p-2) orthogonal matrices coupled with the uniform\ndistribution on the unit sphere in p-1 dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2008 16:02:42 GMT"}], "update_date": "2008-07-17", "authors_parsed": [["Eaton", "Morris L.", ""], ["Muirhead", "Robb J.", ""]]}, {"id": "0807.2767", "submitter": "Jean-Michel Marin", "authors": "Aude Grelaud, Christian Robert, Jean-Michel Marin, Francois Rodolphe,\n  Jean-Francois Taly", "title": "ABC likelihood-freee methods for model choice in Gibbs random fields", "comments": "19 pages, 5 figures, to appear in Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs random fields (GRF) are polymorphous statistical models that can be\nused to analyse different types of dependence, in particular for spatially\ncorrelated data. However, when those models are faced with the challenge of\nselecting a dependence structure from many, the use of standard model choice\nmethods is hampered by the unavailability of the normalising constant in the\nGibbs likelihood. In particular, from a Bayesian perspective, the computation\nof the posterior probabilities of the models under competition requires special\nlikelihood-free simulation techniques like the Approximate Bayesian Computation\n(ABC) algorithm that is intensively used in population genetics. We show in\nthis paper how to implement an ABC algorithm geared towards model choice in the\ngeneral setting of Gibbs random fields, demonstrating in particular that there\nexists a sufficient statistic across models. The accuracy of the approximation\nto the posterior probabilities can be further improved by importance sampling\non the distribution of the models. The practical aspects of the method are\ndetailed through two applications, the test of an iid Bernoulli model versus a\nfirst-order Markov chain, and the choice of a folding structure for two\nproteins.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2008 12:27:20 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2009 16:11:48 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2009 09:40:08 GMT"}], "update_date": "2009-04-03", "authors_parsed": [["Grelaud", "Aude", ""], ["Robert", "Christian", ""], ["Marin", "Jean-Michel", ""], ["Rodolphe", "Francois", ""], ["Taly", "Jean-Francois", ""]]}, {"id": "0807.3113", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos and G.P. Nason", "title": "A note on state space representations of locally stationary wavelet time\n  series", "comments": "8 pages, 3 figures", "journal-ref": "Statistics and Probability Letters (2009), 79, pp. 50-54.", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we show that the locally stationary wavelet process can be\ndecomposed into a sum of signals, each of which following a moving average\nprocess with time-varying parameters. We then show that such moving average\nprocesses are equivalent to state space models with stochastic design\ncomponents. Using a simple simulation step, we propose a heuristic method of\nestimating the above state space models and then we apply the methodology to\nforeign exchange rates data.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2008 18:56:35 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Triantafyllopoulos", "K.", ""], ["Nason", "G. P.", ""]]}, {"id": "0807.3151", "submitter": "Ioana Cosma", "authors": "Ioana A. Cosma and Masoud Asgharian", "title": "Principle of detailed balance and convergence assessment of Markov Chain\n  Monte Carlo methods and simulated annealing", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods are employed to sample from a given\ndistribution of interest, whenever either the distribution does not exist in\nclosed form, or, if it does, no efficient method to simulate an independent\nsample from it is available. Although a wealth of diagnostic tools for\nconvergence assessment of MCMC methods have been proposed in the last two\ndecades, the search for a dependable and easy to implement tool is ongoing. We\npresent in this article a criterion based on the principle of detailed balance\nwhich provides a qualitative assessment of the convergence of a given chain.\nThe criterion is based on the behaviour of a one-dimensional statistic, whose\nasymptotic distribution under the assumption of stationarity is derived; our\nresults apply under weak conditions and have the advantage of being completely\nintuitive. We implement this criterion as a stopping rule for simulated\nannealing in the problem of finding maximum likelihood estimators for\nparameters of a 20-component mixture model. We also apply it to the problem of\nsampling from a 10-dimensional funnel distribution via slice sampling and the\nMetropolis-Hastings algorithm. Furthermore, based on this convergence criterion\nwe define a measure of efficiency of one algorithm versus another.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2008 11:18:29 GMT"}], "update_date": "2008-07-22", "authors_parsed": [["Cosma", "Ioana A.", ""], ["Asgharian", "Masoud", ""]]}, {"id": "0807.3719", "submitter": "Tao Shi", "authors": "Tao Shi, Mikhail Belkin, Bin Yu", "title": "Data spectroscopy: Eigenspaces of convolution operators and clustering", "comments": "Published in at http://dx.doi.org/10.1214/09-AOS700 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2009, Vol. 37, No. 6B, 3960-3984", "doi": "10.1214/09-AOS700", "report-no": "IMS-AOS-AOS700", "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on obtaining clustering information about a distribution\nfrom its i.i.d. samples. We develop theoretical results to understand and use\nclustering information contained in the eigenvectors of data adjacency matrices\nbased on a radial kernel function with a sufficiently fast tail decay. In\nparticular, we provide population analyses to gain insights into which\neigenvectors should be used and when the clustering information for the\ndistribution can be recovered from the sample. We learn that a fixed number of\ntop eigenvectors might at the same time contain redundant clustering\ninformation and miss relevant clustering information. We use this insight to\ndesign the data spectroscopic clustering (DaSpec) algorithm that utilizes\nproperly selected eigenvectors to determine the number of clusters\nautomatically and to group the data accordingly. Our findings extend the\nintuitions underlying existing spectral techniques such as spectral clustering\nand Kernel Principal Components Analysis, and provide new understanding into\ntheir usability and modes of failure. Simulation studies and experiments on\nreal-world data are conducted to show the potential of our algorithm. In\nparticular, DaSpec is found to handle unbalanced groups and recover clusters of\ndifferent shapes better than the competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2008 17:33:29 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2009 09:42:20 GMT"}], "update_date": "2009-11-20", "authors_parsed": [["Shi", "Tao", ""], ["Belkin", "Mikhail", ""], ["Yu", "Bin", ""]]}, {"id": "0807.3734", "submitter": "Guilherme  Rocha", "authors": "Guilherme V. Rocha, Peng Zhao, Bin Yu", "title": "A path following algorithm for Sparse Pseudo-Likelihood Inverse\n  Covariance Estimation (SPLICE)", "comments": "33 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given n observations of a p-dimensional random vector, the covariance matrix\nand its inverse (precision matrix) are needed in a wide range of applications.\nSample covariance (e.g. its eigenstructure) can misbehave when p is comparable\nto the sample size n. Regularization is often used to mitigate the problem.\n  In this paper, we proposed an l1-norm penalized pseudo-likelihood estimate\nfor the inverse covariance matrix. This estimate is sparse due to the l1-norm\npenalty, and we term this method SPLICE. Its regularization path can be\ncomputed via an algorithm based on the homotopy/LARS-Lasso algorithm.\nSimulation studies are carried out for various inverse covariance structures\nfor p=15 and n=20, 1000. We compare SPLICE with the l1-norm penalized\nlikelihood estimate and a l1-norm penalized Cholesky decomposition based\nmethod. SPLICE gives the best overall performance in terms of three metrics on\nthe precision matrix and ROC curve for model selection. Moreover, our\nsimulation results demonstrate that the SPLICE estimates are positive-definite\nfor most of the regularization path even though the restriction is not\nenforced.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2008 19:24:42 GMT"}], "update_date": "2008-07-24", "authors_parsed": [["Rocha", "Guilherme V.", ""], ["Zhao", "Peng", ""], ["Yu", "Bin", ""]]}, {"id": "0807.4010", "submitter": "Inge Koch", "authors": "Inge Koch, Kanta Naito", "title": "Prediction of multivariate responses with a select number of principal\n  components", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2008_278", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method and algorithm for predicting multivariate\nresponses in a regression setting. Research into classification of High\nDimension Low Sample Size (HDLSS) data, in particular microarray data, has made\nconsiderable advances, but regression prediction for high-dimensional data with\ncontinuous responses has had less attention. Recently Bair et al (2006)\nproposed an efficient prediction method based on supervised principal component\nregression (PCR). Motivated by the fact that a larger number of principal\ncomponents results in better regression performance, this paper extends the\nmethod of Bair et al in several ways: a comprehensive variable ranking is\ncombined with a selection of the best number of components for PCR, and the new\nmethod further extends to regression with multivariate responses. The new\nmethod is particularly suited to HDLSS problems. Applications to simulated and\nreal data demonstrate the performance of the new method. Comparisons with Bair\net al (2006) show that for high-dimensional data in particular the new ranking\nresults in a smaller number of predictors and smaller errors.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2008 07:11:36 GMT"}], "update_date": "2008-07-28", "authors_parsed": [["Koch", "Inge", ""], ["Naito", "Kanta", ""]]}, {"id": "0807.4086", "submitter": "Daniel Commenges", "authors": "D. Commenges, A. Sayyareh, L. Letenneur, J. Guedj, A. Bar-Hen", "title": "Estimating a difference between Kullback-Leibler risks by a normalized\n  difference of AIC", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AIC is commonly used for model selection but the precise value of AIC has no\ndirect interpretation. We are interested in quantifying a difference of risks\nbetween two models. This may be useful for both an explanatory point of view or\nfor prediction, where a simpler model may be preferred if it does nearly as\nwell as a more complex model. The difference of risks can be interpreted by\nlinking the risks with relative errors in the computation of probabilities and\nlooking at the values obtained for simple models. A scale of values going from\nnegligible to large is proposed. We propose a normalization of a difference of\nAkaike criteria for estimating the difference of expected Kullback-Leibler\nrisks between maximum likelihood estimators of the distribution in two\ndifferent models. The variability of this statistic can be estimated. Thus, an\ninterval can be constructed which contains the true difference of expected\nKullback-Leibler risks with a pre-specified probability. A simulation study\nshows that the method works and it is illustrated on two examples. The first is\na study of the relationship between body-mass index and depression in elderly\npeople. The second is the choice between models of HIV dynamics, where one\nmodel makes the distinction between activated CD4+ T lymphocytes and the other\ndoes not.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2008 12:36:09 GMT"}], "update_date": "2008-07-28", "authors_parsed": [["Commenges", "D.", ""], ["Sayyareh", "A.", ""], ["Letenneur", "L.", ""], ["Guedj", "J.", ""], ["Bar-Hen", "A.", ""]]}, {"id": "0807.4231", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "QR-Adjustment for Clustering Tests Based on Nearest Neighbor Contingency\n  Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-08-5", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial interaction between two or more classes of points may cause\nspatial clustering patterns such as segregation or association, which can be\ntested using a nearest neighbor contingency table (NNCT). A NNCT is constructed\nusing the frequencies of class types of points in nearest neighbor (NN) pairs.\nFor the NNCT-tests, the null pattern is either complete spatial randomness\n(CSR) of the points from two or more classes (called CSR independence) or\nrandom labeling (RL). The distributions of the NNCT-test statistics depend on\nthe number of reflexive NNs (denoted by $R$) and the number of shared NNs\n(denoted by $Q$), both of which depend on the allocation of the points. Hence\n$Q$ and $R$ are fixed quantities under RL, but random variables under CSR\nindependence. Using their observed values in NNCT analysis makes the\ndistributions of the NNCT-test statistics conditional on $Q$ and $R$ under CSR\nindependence. In this article, I use the empirically estimated expected values\nof $Q$ and $R$ under CSR independence pattern to remove the conditioning of\nNNCT-tests (such a correction is called the \\emph{QR-adjustment}, henceforth).\nI present a Monte Carlo simulation study to compare the conditional NNCT-tests\nand QR-adjusted tests under CSR independence and segregation and association\nalternatives. I demonstrate that QR-adjustment does not significantly improve\nthe empirical size estimates under CSR independence and power estimates under\nsegregation or association alternatives. For illustrative purposes, I apply the\nconditional and empirically corrected tests on two example data sets.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2008 11:10:07 GMT"}], "update_date": "2008-07-29", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "0807.4236", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "On the Use of Nearest Neighbor Contingency Tables for Testing Spatial\n  Segregation", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-08-4", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two or more classes (or types) of points, nearest neighbor contingency\ntables (NNCTs) are constructed using nearest neighbor (NN) frequencies and are\nused in testing spatial segregation of the classes. Pielou's test of\nindependence, Dixon's cell-specific, class-specific, and overall tests are the\ntests based on NNCTs (i.e., they are NNCT-tests). These tests are designed and\nintended for use under the null pattern of random labeling (RL) of completely\nmapped data. However, it has been shown that Pielou's test is not appropriate\nfor testing segregation against the RL pattern while Dixon's tests are. In this\narticle, we compare Pielou's and Dixon's NNCT-tests; introduce the one-sided\nversions of Pielou's test; extend the use of NNCT-tests for testing complete\nspatial randomness (CSR) of points from two or more classes (which is called\n\\emph{CSR independence}, henceforth). We assess the finite sample performance\nof the tests by an extensive Monte Carlo simulation study and demonstrate that\nDixon's tests are also appropriate for testing CSR independence; but Pielou's\ntest and the corresponding one-sided versions are liberal for testing CSR\nindependence or RL. Furthermore, we show that Pielou's tests are only\nappropriate when the NNCT is based on a random sample of (base, NN) pairs. We\nalso prove the consistency of the tests under their appropriate null\nhypotheses. Moreover, we investigate the edge (or boundary) effects on the\nNNCT-tests and compare the buffer zone and toroidal edge correction methods for\nthese tests. We illustrate the tests on a real life and an artificial data set.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2008 12:40:19 GMT"}], "update_date": "2008-07-29", "authors_parsed": [["Ceyhan", "Elvan", ""]]}]