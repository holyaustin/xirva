[{"id": "1502.00060", "submitter": "Xing He", "authors": "Xing He, Robert Caiming Qiu, Qian Ai, Yinshuang Cao, Jie Gu, Zhijian\n  Jin", "title": "A Random Matrix Theoretical Approach to Early Event Detection in Smart\n  Grid", "comments": "12 pages, 11 figures, submitted to IEEE Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power systems are developing very fast nowadays, both in size and in\ncomplexity; this situation is a challenge for Early Event Detection (EED). This\npaper proposes a data- driven unsupervised learning method to handle this\nchallenge. Specifically, the random matrix theories (RMTs) are introduced as\nthe statistical foundations for random matrix models (RMMs); based on the RMMs,\nlinear eigenvalue statistics (LESs) are defined via the test functions as the\nsystem indicators. By comparing the values of the LES between the experimental\nand the theoretical ones, the anomaly detection is conducted. Furthermore, we\ndevelop 3D power-map to visualize the LES; it provides a robust auxiliary\ndecision-making mechanism to the operators. In this sense, the proposed method\nconducts EED with a pure statistical procedure, requiring no knowledge of\nsystem topologies, unit operation/control models, etc. The LES, as a key\ningredient during this procedure, is a high dimensional indictor derived\ndirectly from raw data. As an unsupervised learning indicator, the LES is much\nmore sensitive than the low dimensional indictors obtained from supervised\nlearning. With the statistical procedure, the proposed method is universal and\nfast; moreover, it is robust against traditional EED challenges (such as error\naccumulations, spurious correlations, and even bad data in core area). Case\nstudies, with both simulated data and real ones, validate the proposed method.\nTo manage large-scale distributed systems, data fusion is mentioned as another\ndata processing ingredient.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 03:07:40 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 05:40:40 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["He", "Xing", ""], ["Qiu", "Robert Caiming", ""], ["Ai", "Qian", ""], ["Cao", "Yinshuang", ""], ["Gu", "Jie", ""], ["Jin", "Zhijian", ""]]}, {"id": "1502.00161", "submitter": "Helio M. de Oliveira", "authors": "V.VV. Vermehren and H.M. de Oliveira", "title": "Close expressions for Meyer Wavelet and Scale Function", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": "10.14209/SBRT.2015.2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many continuous wavelets are defined in the frequency domain and do not have\nanalytical expressions in the time domain. Meyer wavelet is ordinarily defined\nin this way. In this note, we derive new straightforward analytical expressions\nfor both the wavelet and scale function for the Meyer basis. The validity of\nthese expressions is corroborated by numerical computations, yielding no\napproximation error.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 20:54:03 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Vermehren", "V. VV.", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1502.00384", "submitter": "Young-Geun Choi", "authors": "Young-Geun Choi, Chi Tim Ng, and Johan Lim", "title": "Regularized LRT for Large Scale Covariance Matrices: One Sample Problem", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.jspi.2016.06.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main theme of this paper is a modification of the likelihood ratio test\n(LRT) for testing high dimensional covariance matrix. Recently, the correct\nasymptotic distribution of the LRT for a large-dimensional case (the case $p/n$\napproaches to a constant $\\gamma \\in (0,1]$) is specified by researchers. The\ncorrect procedure is named as corrected LRT. Despite of its correction, the\ncorrected LRT is a function of sample eigenvalues that are suffered from\nredundant variability from high dimensionality and, subsequently, still does\nnot have full power in differentiating hypotheses on the covariance matrix. In\nthis paper, motivated by the successes of a linearly shrunken covariance matrix\nestimator (simply shrinkage estimator) in various applications, we propose a\nregularized LRT that uses, in defining the LRT, the shrinkage estimator instead\nof the sample covariance matrix. We compute the asymptotic distribution of the\nregularized LRT, when the true covariance matrix is the identity matrix and a\nspiked covariance matrix. The obtained asymptotic results have applications in\ntesting various hypotheses on the covariance matrix. Here, we apply them to\ntesting the identity of the true covariance matrix, which is a long standing\nproblem in the literature, and show that the regularized LRT outperforms the\ncorrected LRT, which is its non-regularized counterpart. In addition, we\ncompare the power of the regularized LRT to those of recent non-likelihood\nbased procedures.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 07:37:50 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 11:55:59 GMT"}, {"version": "v3", "created": "Fri, 10 Jun 2016 12:26:44 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Choi", "Young-Geun", ""], ["Ng", "Chi Tim", ""], ["Lim", "Johan", ""]]}, {"id": "1502.00412", "submitter": "Giacomo Aletti", "authors": "Andrea Ghiglietti, Francesca Ieva, Anna Maria Paganoni and Giacomo\n  Aletti", "title": "On linear regression models in infinite dimensional spaces with scalar\n  response", "comments": null, "journal-ref": "Stat Papers (2017) 58: 527-548", "doi": "10.1007/s00362-015-0710-2", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In functional linear regression, the parameters estimation involves solving a\nnon necessarily well-posed problem and it has points of contact with a range of\nmethodologies, including statistical smoothing, deconvolution and projection on\nfinite-dimensional subspaces. We discuss the standard approach based explicitly\non functional principal components analysis, nevertheless the choice of the\nnumber of basis components remains something subjective and not always properly\ndiscussed and justified. In this work we discuss inferential properties of\nleast square estimation in this context with different choices of projection\nsubspaces, as well as we study asymptotic behaviour increasing the dimension of\nsubspaces.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 09:14:18 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Ghiglietti", "Andrea", ""], ["Ieva", "Francesca", ""], ["Paganoni", "Anna Maria", ""], ["Aletti", "Giacomo", ""]]}, {"id": "1502.00465", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "Local optimization-based statistical inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a local optimization-based approach to test statistical\nhypotheses and to construct confidence intervals. This approach can be viewed\nas an extension of bootstrap, and yields asymptotically valid tests and\nconfidence intervals as long as there exist consistent estimators of unknown\nparameters. We present simple algorithms including a neighborhood bootstrap\nmethod to implement the approach. Several examples in which theoretical\nanalysis is not easy are presented to show the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 13:20:48 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 02:31:14 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 01:47:14 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1502.00471", "submitter": "Joong-Ho Won", "authors": "Sang-Yun Oh, Bala Rajaratnam, Joong-Ho Won", "title": "Towards a sparse, scalable, and stably positive definite (inverse)\n  covariance estimator", "comments": "19 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional covariance estimation and graphical models is a contemporary\ntopic in statistics and machine learning having widespread applications. An\nimportant line of research in this regard is to shrink the extreme spectrum of\nthe covariance matrix estimators. A separate line of research in the literature\nhas considered sparse inverse covariance estimation which in turn gives rise to\ngraphical models. In practice, however, a sparse covariance or inverse\ncovariance matrix which is simultaneously well-conditioned and at the same time\ncomputationally tractable is desired. There has been little research at the\nconfluence of these three topics. In this paper we consider imposing a\ncondition number constraint to various types of losses used in covariance and\ninverse covariance matrix estimation. When the loss function can be decomposed\nas a sum of an orthogonally invariant function of the estimate and its inner\nproduct with a function of the sample covariance matrix, we show that a\nsolution path algorithm can be derived, involving a series of ordinary\ndifferential equations. The path algorithm is attractive because it provides\nthe entire family of estimates for all possible values of the condition number\nbound, at the same computational cost of a single estimate with a fixed upper\nbound. An important finding is that the proximal operator for the condition\nnumber constraint, which turns out to be very useful in regularizing loss\nfunctions that are not orthogonally invariant and may yield\nnon-positive-definite estimates, can be efficiently computed by this path\nalgorithm. As a concrete illustration of its practical importance, we develop\nan operator-splitting algorithm that imposes a guarantee of well-conditioning\nas well as positive definiteness to recently proposed convex pseudo-likelihood\nbased graphical model selection methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 13:34:24 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 08:45:56 GMT"}, {"version": "v3", "created": "Mon, 27 Jun 2016 02:37:59 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Oh", "Sang-Yun", ""], ["Rajaratnam", "Bala", ""], ["Won", "Joong-Ho", ""]]}, {"id": "1502.00483", "submitter": "Subhash Lele", "authors": "Subhash R. Lele", "title": "Is non-informative Bayesian analysis appropriate for wildlife\n  management: survival of San Joaquin Kit Fox and declines in amphibian\n  populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational convenience has led to widespread use of Bayesian inference\nwith vague or flat priors to analyze state-space models in ecology. Vague\npriors are claimed to be objective and to let the data speak. Neither of these\nclaims is valid. Statisticians have criticized the use of vague priors from\nphilosophical to computational to pragmatic reasons. Ecologists, however,\ndismiss such criticisms as empty philosophical wonderings with no practical\nimplications. We illustrate that use of vague priors in population viability\nanalysis and occupancy models can have significant impact on the analysis and\ncan lead to strikingly different managerial decisions. Given the wide spread\napplicability of the hierarchical models and uncritical use of non-informative\nBayesian analysis in ecology, researchers should be cautious about using the\nvague priors as a default choice in practical situations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 14:07:50 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lele", "Subhash R.", ""]]}, {"id": "1502.00552", "submitter": "Cecilia Earls", "authors": "Cecilia Earls and Giles Hooker", "title": "Adapted Variational Bayes for Functional Data Registration, Smoothing,\n  and Prediction", "comments": "Additional details are included in this version in response to\n  reviewer comments. All main results are unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for functional data registration that compares favorably\nto the best methods of functional data registration currently available. It\nalso extends current inferential capabilities for unregistered data by\nproviding a flexible probabilistic framework that 1) allows for functional\nprediction in the context of registration and 2) can be adapted to include\nsmoothing and registration in one model. The proposed inferential framework is\na Bayesian hierarchical model where the registered functions are modeled as\nGaussian processes. To address the computational demands of inference in\nhigh-dimensional Bayesian models, we propose an adapted form of the variational\nBayes algorithm for approximate inference that performs similarly to MCMC\nsampling methods for well-defined problems. The efficiency of the adapted\nvariational Bayes (AVB) algorithm allows variability in a predicted registered,\nwarping, and unregistered function to be depicted separately via bootstrapping.\nTemperature data related to the el-ni\\~no phenomenon is used to demonstrate the\nunique inferential capabilities for prediction provided by this model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 17:10:54 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 13:21:50 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 14:28:01 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Earls", "Cecilia", ""], ["Hooker", "Giles", ""]]}, {"id": "1502.00555", "submitter": "Renato J Cintra", "authors": "P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "A Discrete Tchebichef Transform Approximation for Image and Video Coding", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": "IEEE Signal Processing Letters, vol. 22, issue 8, pp. 1137-1141,\n  2015", "doi": "10.1109/LSP.2015.2389899", "report-no": null, "categories": "stat.ME cs.CV cs.MM cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a low-complexity approximation for the discrete\nTchebichef transform (DTT). The proposed forward and inverse transforms are\nmultiplication-free and require a reduced number of additions and bit-shifting\noperations. Numerical compression simulations demonstrate the efficiency of the\nproposed transform for image and video coding. Furthermore, Xilinx Virtex-6\nFPGA based hardware realization shows 44.9% reduction in dynamic power\nconsumption and 64.7% lower area when compared to the literature.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 14:07:44 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Oliveira", "P. A. M.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1502.00587", "submitter": "Cecilia Earls", "authors": "Cecilia Earls and Giles Hooker", "title": "Combining Functional Data Registration and Factor Analysis", "comments": "The paper was updated with a better real data example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the definition of functional data registration to encompass a\nlarger class of registered functions. In contrast to traditional registration\nmodels, we allow for registered functions that have more than one primary\ndirection of variation. The proposed Bayesian hierarchical model simultaneously\nregisters the observed functions and estimates the two primary factors that\ncharacterize variation in the registered functions. Each registered function is\nassumed to be predominantly composed of a linear combination of these two\nprimary factors, and the function-specific weights for each observation are\nestimated within the registration model. We show how these estimated weights\ncan easily be used to classify functions after registration using both\nsimulated data and a juggling data set.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 19:19:50 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 20:01:37 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Earls", "Cecilia", ""], ["Hooker", "Giles", ""]]}, {"id": "1502.00592", "submitter": "Renato J Cintra", "authors": "C. J. Tablada, F. M. Bayer, R. J. Cintra", "title": "A Class of DCT Approximations Based on the Feig-Winograd Algorithm", "comments": "26 pages, 4 figures, 5 tables, fixed arithmetic complexity in Table\n  IV", "journal-ref": "Signal Processing, vol. 113, pp. 38-51, August 2015", "doi": "10.1016/j.sigpro.2015.01.011", "report-no": null, "categories": "stat.ME cs.CV cs.MM cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of matrices based on a parametrization of the Feig-Winograd\nfactorization of 8-point DCT is proposed. Such parametrization induces a matrix\nsubspace, which unifies a number of existing methods for DCT approximation. By\nsolving a comprehensive multicriteria optimization problem, we identified\nseveral new DCT approximations. Obtained solutions were sought to possess the\nfollowing properties: (i) low multiplierless computational complexity, (ii)\northogonality or near orthogonality, (iii) low complexity invertibility, and\n(iv) close proximity and performance to the exact DCT. Proposed approximations\nwere submitted to assessment in terms of proximity to the DCT, coding\nperformance, and suitability for image compression. Considering Pareto\nefficiency, particular new proposed approximations could outperform various\nexisting methods archived in literature.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 19:39:46 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 21:25:18 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Tablada", "C. J.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1502.00812", "submitter": "Aad van der Vaart", "authors": "Aad van der Vaart", "title": "Higher Order Tangent Spaces and Influence Functions", "comments": "Published in at http://dx.doi.org/10.1214/14-STS478 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 679-686", "doi": "10.1214/14-STS478", "report-no": "IMS-STS-STS478", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review higher order tangent spaces and influence functions and their use\nto construct minimax efficient estimators for parameters in high-dimensional\nsemiparametric models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 11:18:40 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["van der Vaart", "Aad", ""]]}, {"id": "1502.00829", "submitter": "Peter Spirtes", "authors": "Peter Spirtes, Jiji Zhang", "title": "A Uniformly Consistent Estimator of Causal Effects under the\n  $k$-Triangle-Faithfulness Assumption", "comments": "Published in at http://dx.doi.org/10.1214/13-STS429 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 662-678", "doi": "10.1214/13-STS429", "report-no": "IMS-STS-STS429", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spirtes, Glymour and Scheines [Causation, Prediction, and Search (1993)\nSpringer] described a pointwise consistent estimator of the Markov equivalence\nclass of any causal structure that can be represented by a directed acyclic\ngraph for any parametric family with a uniformly consistent test of conditional\nindependence, under the Causal Markov and Causal Faithfulness assumptions.\nRobins et al. [Biometrika 90 (2003) 491-515], however, proved that there are no\nuniformly consistent estimators of Markov equivalence classes of causal\nstructures under those assumptions. Subsequently, Kalisch and B\\\"{u}hlmann [J.\nMach. Learn. Res. 8 (2007) 613-636] described a uniformly consistent estimator\nof the Markov equivalence class of a linear Gaussian causal structure under the\nCausal Markov and Strong Causal Faithfulness assumptions. However, the Strong\nFaithfulness assumption may be false with high probability in many domains. We\ndescribe a uniformly consistent estimator of both the Markov equivalence class\nof a linear Gaussian causal structure and the identifiable structural\ncoefficients in the Markov equivalence class under the Causal Markov assumption\nand the considerably weaker k-Triangle-Faithfulness assumption.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 12:13:42 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Spirtes", "Peter", ""], ["Zhang", "Jiji", ""]]}, {"id": "1502.00950", "submitter": "Helio M. de Oliveira", "authors": "M.M.S. Lira, H.M. de Oliveira, M.A. Carvalho Jr, R.M. Campello de\n  Souza", "title": "Compactly Supported Wavelets Derived From Legendre Polynomials:\n  Spherical Harmonic Wavelets", "comments": "6 pages, 6 figures, 1 table In: Computational Methods in Circuits and\n  Systems Applications, WSEAS press, pp.211-215, 2003. ISBN: 960-8052-88-2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new family of wavelets is introduced, which is associated with Legendre\npolynomials. These wavelets, termed spherical harmonic or Legendre wavelets,\npossess compact support. The method for the wavelet construction is derived\nfrom the association of ordinary second order differential equations with\nmultiresolution filters. The low-pass filter associated with Legendre\nmultiresolution analysis is a linear phase finite impulse response filter\n(FIR).\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 18:23:32 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Lira", "M. M. S.", ""], ["de Oliveira", "H. M.", ""], ["Carvalho", "M. A.", "Jr"], ["de Souza", "R. M. Campello", ""]]}, {"id": "1502.00973", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen, R.W. Doerge and Sanat K. Sarkar", "title": "A weighted FDR procedure under discrete and heterogeneous null\n  distributions", "comments": "29 pages; in this version: (1) new weights from arXiv:1812.06551v2\n  are employed by the wFDR procedure to ensure its conservativeness under\n  independence; (2) non-asymptotic upper bound is provided for the wFDR\n  procedure that uses plug-in weights that are induced by the proportion\n  estimator of arxiv:1410.4274v2; (3) simulation study under dependence uses\n  more a complicated block dependence structure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple testing with false discovery rate (FDR) control has been widely\nconducted in the ``discrete paradigm\" where p-values have discrete and\nheterogeneous null distributions. However, in this scenario existing FDR\nprocedures often lose some power and may yield unreliable inference, and for\nthis scenario there does not seem to be an FDR procedure that partitions\nhypotheses into groups, employs data-adaptive weights and is non-asymptotically\nconservative. We propose a weighted FDR procedure for multiple testing in the\ndiscrete paradigm that efficiently adapts to both the heterogeneity and\ndiscreteness of p-value distributions. We theoretically justify the\nnon-asymptotic conservativeness of the weighted FDR procedure under\nindependence, and show via simulation studies that, for multiple testing based\non p-values of Binomial test or Fisher's exact test, it is more powerful than\nsix other procedures. The weighted FDR procedure is applied to a drug safety\nstudy and a differential methylation study based on discrete data, where it\nmakes more discoveries than two existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 19:41:41 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 01:47:25 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 23:15:07 GMT"}, {"version": "v4", "created": "Sun, 3 Sep 2017 20:32:25 GMT"}, {"version": "v5", "created": "Sun, 21 Jul 2019 18:16:57 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chen", "Xiongzhi", ""], ["Doerge", "R. W.", ""], ["Sarkar", "Sanat K.", ""]]}, {"id": "1502.01002", "submitter": "Alexander Wong", "authors": "Alexander Wong, Xiao Yu Wang, and Maud Gorbet", "title": "Bayesian-based deconvolution fluorescence microscopy using dynamically\n  updated nonparametric nonstationary expectation estimates", "comments": "13", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy is widely used for the study of biological specimens.\nDeconvolution can significantly improve the resolution and contrast of images\nproduced using fluorescence microscopy; in particular, Bayesian-based methods\nhave become very popular in deconvolution fluorescence microscopy. An ongoing\nchallenge with Bayesian-based methods is in dealing with the presence of noise\nin low SNR imaging conditions. In this study, we present a Bayesian-based\nmethod for performing deconvolution using dynamically updated nonparametric\nnonstationary expectation estimates that can improve the fluorescence\nmicroscopy image quality in the presence of noise, without explicit use of\nspatial regularization.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 20:34:28 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Wong", "Alexander", ""], ["Wang", "Xiao Yu", ""], ["Gorbet", "Maud", ""]]}, {"id": "1502.01038", "submitter": "Renato J Cintra", "authors": "H. M. de Oliveira, R. J. Cintra, R. M. Campello de Souza", "title": "A Factorization Scheme for Some Discrete Hartley Transform Matrices", "comments": "10 pages, 4 figures, 2 tables, International Conference on System\n  Engineering, Communications and Information Technologies, 2001, Punta Arenas.\n  ICSECIT 2001 Proceedings. Punta Arenas: Universidad de Magallanes, 2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete transforms such as the discrete Fourier transform (DFT) and the\ndiscrete Hartley transform (DHT) are important tools in numerical analysis. The\nsuccessful application of transform techniques relies on the existence of\nefficient fast transforms. In this paper some fast algorithms are derived. The\ntheoretical lower bound on the multiplicative complexity for the DFT/DHT are\nachieved. The approach is based on the factorization of DHT matrices.\nAlgorithms for short blocklengths such as $N \\in \\{3, 5, 6, 12, 24 \\}$ are\npresented.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 16:06:29 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["de Oliveira", "H. M.", ""], ["Cintra", "R. J.", ""], ["de Souza", "R. M. Campello", ""]]}, {"id": "1502.01073", "submitter": "Matz Haugen", "authors": "Matz A. Haugen, Bala Rajaratnam, Paul Switzer", "title": "Extracting Common Time Trends from Concurrent Time Series: Maximum\n  Autocorrelation Factors with Application to Tree Ring Time Series Data", "comments": "38 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Concurrent time series commonly arise in various applications, including when\nmonitoring the environment such as in air quality measurement networks, weather\nstations, oceanographic buoys, or in paleo form such as lake sediments, tree\nrings, ice cores, or coral isotopes, with each monitoring or sampling site\nproviding one of the time series. The goal in such applications is to extract a\ncommon time trend or signal in the observed data. Other examples where the goal\nis to extract a common time trend for multiple time series are in stock price\ntime series, neurological time series, and quality control time series. For\nthis purpose we develop properties of MAF [Maximum Autocorrelation Factors]\nthat linearly combines time series in order to maximize the resulting SNR\n[signal-to-noise-ratio] where there are multiple smooth signals present in the\ndata. Equivalence is established in a regression setting between MAF and CCA\n[Canonical Correlation Analysis] even though MAF does not require specific\nsignal knowledge as opposed to CCA. We proceed to derive the theoretical\nproperties of MAF and quantify the SNR advantages of MAF in comparison with PCA\n[Principal Components Analysis], a commonly used method for linearly combining\ntime series, and compare their statistical sample properties. MAF and PCA are\nthen applied to real and simulated data sets to illustrate MAFs efficacy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 01:03:06 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 17:41:23 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2015 18:18:44 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Haugen", "Matz A.", ""], ["Rajaratnam", "Bala", ""], ["Switzer", "Paul", ""]]}, {"id": "1502.01106", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Ayanendranath Basu", "title": "Robust Bounded Influence Tests for Independent Non-Homogeneous\n  Observations", "comments": "To appear in Statistica Sinica (2017)", "journal-ref": null, "doi": "10.5705/ss.202015.0320", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments often yield non-identically distributed data for statistical\nanalysis. Tests of hypothesis under such set-ups are generally performed using\nthe likelihood ratio test, which is non-robust with respect to outliers and\nmodel misspecification. In this paper, we consider the set-up of\nnon-identically but independently distributed observations and develop a\ngeneral class of test statistics for testing parametric hypothesis based on the\ndensity power divergence. The proposed tests have bounded influence functions,\nare highly robust with respect to data contamination, have high power against\ncontiguous alternatives, and are consistent at any fixed alternative. The\nmethodology is illustrated by the simple and generalized linear regression\nmodels with fixed covariates.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 06:22:49 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 19:13:51 GMT"}, {"version": "v3", "created": "Fri, 9 Sep 2016 09:23:13 GMT"}, {"version": "v4", "created": "Fri, 7 Jul 2017 19:03:50 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1502.01115", "submitter": "Thais Rodrigues", "authors": "Thais Rodrigues and Yanan Fan", "title": "Regression Adjustment for Noncrossing Bayesian Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-stage approach is proposed to overcome the problem in quantile\nregression, where separately fitted curves for several quantiles may cross. The\nstandard Bayesian quantile regression model is applied in the first stage,\nfollowed by a Gaussian process regression adjustment, which monotonizes the\nquantile function whilst borrowing strength from nearby quantiles. The two\nstage approach is computationally efficient, and more general than existing\ntechniques. The method is shown to be competitive with alternative approaches\nvia its performance in simulated examples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 07:58:35 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Rodrigues", "Thais", ""], ["Fan", "Yanan", ""]]}, {"id": "1502.01118", "submitter": "Francesca Greselin", "authors": "L.A. Garcia-Escudero and A. Gordaliza and F. Greselin and S. Ingrassia\n  and A. Mayo-Iscar", "title": "Robust estimation of mixtures of regressions with random covariates, via\n  trimming and constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust estimator for a wide family of mixtures of linear regression is\npresented. Robustness is based on the joint adoption of the Cluster Weighted\nModel and of an estimator based on trimming and restrictions. The selected\nmodel provides the conditional distribution of the response for each group, as\nin mixtures of regression, and further supplies local distributions for the\nexplanatory variables. A novel version of the restrictions has been devised,\nunder this model, for separately controlling the two sources of variability\nidentified in it. This proposal avoids singularities in the log-likelihood,\ncaused by approximate local collinearity in the explanatory variables or local\nexact fit in regressions, and reduces the occurrence of spurious local\nmaximizers. In a natural way, due to the interaction between the model and the\nestimator, the procedure is able to resist the harmful influence of bad\nleverage points along the estimation of the mixture of regressions, which is\nstill an open issue in the literature. The given methodology defines a\nwell-posed statistical problem, whose estimator exists and is consistent to the\ncorresponding solution of the population optimum, under widely general\nconditions. A feasible EM algorithm has also been provided to obtain the\ncorresponding estimation. Many simulated examples and two real datasets have\nbeen chosen to show the ability of the procedure, on the one hand, to detect\nanomalous data, and, on the other hand, to identify the real cluster\nregressions without the influence of contamination.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 08:17:40 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Garcia-Escudero", "L. A.", ""], ["Gordaliza", "A.", ""], ["Greselin", "F.", ""], ["Ingrassia", "S.", ""], ["Mayo-Iscar", "A.", ""]]}, {"id": "1502.01148", "submitter": "Christian P. Robert", "authors": "Peter J. Green (Bristol), Krzysztof {\\L}atuszy\\'nski (Warwick),\n  Marcelo Pereyra (Bristol) and Christian P. Robert (Paris-Dauphine and\n  Warwick)", "title": "Bayesian computation: a perspective on the current state, and sampling\n  backwards and forwards", "comments": "30 pages (incl. 8 pages of references), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decades have seen enormous improvements in computational inference\nbased on statistical models, with continual enhancement in a wide range of\ncomputational tools, in competition. In Bayesian inference, first and foremost,\nMCMC techniques continue to evolve, moving from random walk proposals to\nLangevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical\nand algorithmic inputs opening wider access to practitioners. However, this\nimpressive evolution in capacity is confronted by an even steeper increase in\nthe complexity of the models and datasets to be addressed. The difficulties of\nmodelling and then handling ever more complex datasets most likely call for a\nnew type of tool for computational inference that dramatically reduce the\ndimension and size of the raw data while capturing its essential aspects.\nApproximate models and algorithms may thus be at the core of the next\ncomputational revolution.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 10:38:42 GMT"}, {"version": "v2", "created": "Sun, 8 Feb 2015 11:51:19 GMT"}, {"version": "v3", "created": "Sat, 9 May 2015 08:17:58 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Green", "Peter J.", "", "Bristol"], ["\u0141atuszy\u0144ski", "Krzysztof", "", "Warwick"], ["Pereyra", "Marcelo", "", "Bristol"], ["Robert", "Christian P.", "", "Paris-Dauphine and\n  Warwick"]]}, {"id": "1502.01194", "submitter": "Murray Pollock", "authors": "M. Pollock, A.M. Johansen, K. {\\L}atuszy\\'nski, G.O. Roberts", "title": "Discussion of \"Sequential Quasi-Monte-Carlo Sampling\" by M. Gerber and\n  N. Chopin", "comments": "Journal of the Royal Statistical Society B, (In Press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this comment we consider whether QMC methods can be further embedded\nwithin SMC schemes in settings in which the transition density of the latent\nprocess is intractable and pseudo-marginal methods are deployed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 13:42:26 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Pollock", "M.", ""], ["Johansen", "A. M.", ""], ["\u0141atuszy\u0144ski", "K.", ""], ["Roberts", "G. O.", ""]]}, {"id": "1502.01260", "submitter": "Pierre-Antoine Thouvenin", "authors": "Pierre-Antoine Thouvenin, Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Hyperspectral unmixing with spectral variability using a perturbed\n  linear mixing model", "comments": "to appear, IEEE Transactions on Signal Processing, 2015", "journal-ref": null, "doi": "10.1109/TSP.2015.2486746", "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a mixed hyperspectral data set, linear unmixing aims at estimating the\nreference spectral signatures composing the data - referred to as endmembers -\ntheir abundance fractions and their number. In practice, the identified\nendmembers can vary spectrally within a given image and can thus be construed\nas variable instances of reference endmembers. Ignoring this variability\ninduces estimation errors that are propagated into the unmixing procedure. To\naddress this issue, endmember variability estimation consists of estimating the\nreference spectral signatures from which the estimated endmembers have been\nderived as well as their variability with respect to these references. This\npaper introduces a new linear mixing model that explicitly accounts for spatial\nand spectral endmember variabilities. The parameters of this model can be\nestimated using an optimization algorithm based on the alternating direction\nmethod of multipliers. The performance of the proposed unmixing method is\nevaluated on synthetic and real data. A comparison with state-of-the-art\nalgorithms designed to model and estimate endmember variability allows the\ninterest of the proposed unmixing solution to be appreciated.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 17:22:08 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 14:02:48 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Thouvenin", "Pierre-Antoine", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1502.01377", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, V. S. Dimitrov", "title": "The Arithmetic Cosine Transform: Exact and Approximate Algorithms", "comments": "17 pages, 3 figures", "journal-ref": "IEEE Transactions on Signal Processing, vol. 58, no. 6, pp.\n  3076-3085, June 2010", "doi": "10.1109/TSP.2010.2045781", "report-no": null, "categories": "cs.NA math.NA stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new class of transform method --- the\narithmetic cosine transform (ACT). We provide the central mathematical\nproperties of the ACT, necessary in designing efficient and accurate\nimplementations of the new transform method. The key mathematical tools used in\nthe paper come from analytic number theory, in particular the properties of the\nRiemann zeta function. Additionally, we demonstrate that an exact signal\ninterpolation is achievable for any block-length. Approximate calculations were\nalso considered. The numerical examples provided show the potential of the ACT\nfor various digital signal processing applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 22:10:21 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1502.01493", "submitter": "Pierre Dupont", "authors": "Samuel Branders, Roberto D'Ambrosio and Pierre Dupont", "title": "A mixture Cox-Logistic model for feature selection from survival and\n  classification data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an original approach for jointly fitting survival times\nand classifying samples into subgroups. The Coxlogit model is a generalized\nlinear model with a common set of selected features for both tasks. Survival\ntimes and class labels are here assumed to be conditioned by a common risk\nscore which depends on those features. Learning is then naturally expressed as\nmaximizing the joint probability of subgroup labels and the ordering of\nsurvival events, conditioned to a common weight vector. The model is estimated\nby minimizing a regularized log-likelihood through a coordinate descent\nalgorithm.\n  Validation on synthetic and breast cancer data shows that the proposed\napproach outperforms a standard Cox model or logistic regression when both\npredicting the survival times and classifying new samples into subgroups. It is\nalso better at selecting informative features for both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 10:45:54 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Branders", "Samuel", ""], ["D'Ambrosio", "Roberto", ""], ["Dupont", "Pierre", ""]]}, {"id": "1502.01510", "submitter": "Michael Betancourt", "authors": "M. J. Betancourt", "title": "The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data\n  Subsampling", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte\nCarlo produces computationally efficient Monte Carlo estimators, even with\nrespect to complex and high-dimensional target distributions. When confronted\nwith data-intensive applications, however, the algorithm may be too expensive\nto implement, leaving us to consider the utility of approximations such as data\nsubsampling. In this paper I demonstrate how data subsampling fundamentally\ncompromises the efficient exploration of Hamiltonian flow and hence the\nscalable performance of Hamiltonian Monte Carlo itself.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 11:38:47 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Betancourt", "M. J.", ""]]}, {"id": "1502.01527", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universit\\'e Paris-Dauphine, University of\n  Warwick, and CREST)", "title": "Some comments about A. Ronald Gallant's \"Reflections on the Probability\n  Space Induced by Moment Conditions with Implications for Bayesian Inference\"", "comments": "6 pages, submitted to the Journal of Financial Econometrics for the\n  discussion of Gallant (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is commenting on Ronald Gallant's (2015) reflections on the\nconstruction of Bayesian prior distributions from moment conditions. The main\nconclusion is that the paper does not deliver a working principle that could\njustify inference based on such priors.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 13:10:43 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, University of\n  Warwick, and CREST"]]}, {"id": "1502.01974", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley, Christopher K. Wikle, Scott H. Holan", "title": "Regionalization of Multiscale Spatial Processes using a Criterion for\n  Spatial Aggregation Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modifiable areal unit problem and the ecological fallacy are known\nproblems that occur when modeling multiscale spatial processes. We investigate\nhow these forms of spatial aggregation error can guide a regionalization over a\nspatial domain of interest. By \"regionalization\" we mean a specification of\ngeographies that define the spatial support for areal data. This topic has been\nstudied vigorously by geographers, but has been given less attention by spatial\nstatisticians. Thus, we propose a criterion for spatial aggregation error\n(CAGE), which we minimize to obtain an optimal regionalization. To define CAGE\nwe draw a connection between spatial aggregation error and a new multiscale\nrepresentation of the Karhunen-Loeve (K-L) expansion. This relationship between\nCAGE and the multiscale K-L expansion leads to illuminating theoretical\ndevelopments including: connections between spatial aggregation error, squared\nprediction error, spatial variance, and a novel extension of Obled-Creutin\neigenfunctions. The effectiveness of our approach is demonstrated through an\nanalysis of two datasets, one using the American Community Survey and one\nrelated to environmental ocean winds.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 18:14:13 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 16:30:23 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Wikle", "Christopher K.", ""], ["Holan", "Scott H.", ""]]}, {"id": "1502.02008", "submitter": "Alireza Mahani", "authors": "Alireza S. Mahani, Asad Hasan, Marshall Jiang, Mansour T.A. Sharabiani", "title": "Stochastic Newton Sampler: R Package sns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package sns implements Stochastic Newton Sampler (SNS), a\nMetropolis-Hastings Monte Carlo Markov Chain algorithm where the proposal\ndensity function is a multivariate Gaussian based on a local, second-order\nTaylor series expansion of log-density. The mean of the proposal function is\nthe full Newton step in Newton-Raphson optimization algorithm. Taking advantage\nof the local, multivariate geometry captured in log-density Hessian allows SNS\nto be more efficient than univariate samplers, approaching independent sampling\nas the density function increasingly resembles a multivariate Gaussian. SNS\nrequires the log-density Hessian to be negative-definite everywhere in order to\nconstruct a valid proposal function. This property holds, or can be easily\nchecked, for many GLM-like models. When initial point is far from density peak,\nrunning SNS in non-stochastic mode by taking the Newton step, augmented with\nwith line search, allows the MCMC chain to converge to high-density areas\nfaster. For high-dimensional problems, partitioning of state space into\nlower-dimensional subsets, and applying SNS to the subsets within a Gibbs\nsampling framework can significantly improve the mixing of SNS chains. In\naddition to the above strategies for improving convergence and mixing, sns\noffers diagnostics and visualization capabilities, as well as a function for\nsample-based calculation of Bayesian predictive posterior distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 19:59:55 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Mahani", "Alireza S.", ""], ["Hasan", "Asad", ""], ["Jiang", "Marshall", ""], ["Sharabiani", "Mansour T. A.", ""]]}, {"id": "1502.02049", "submitter": "Renato J Cintra", "authors": "L. R. Soares, H. M. de Oliveira, R. J. Cintra", "title": "The Fourier-Like and Hartley-Like Wavelet Analysis Based on Hilbert\n  Transforms", "comments": "7 pages, 10 figures, Anais do XXII Simp\\'osio Brasileiro de\n  Telecomunica\\c{c}\\~oes, Campinas, 2005", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA math.ST physics.data-an stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In continuous-time wavelet analysis, most wavelet present some kind of\nsymmetry. Based on the Fourier and Hartley transform kernels, a new wavelet\nmultiresolution analysis is proposed. This approach is based on a pair of\northogonal wavelet functions and is named as the Fourier-Like and Hartley-Like\nwavelet analysis. A Hilbert transform analysis on the wavelet theory is also\nincluded.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 21:16:31 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Soares", "L. R.", ""], ["de Oliveira", "H. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1502.02123", "submitter": "Marcela Svarc", "authors": "Ricardo Fraiman, Yanina Gimenez and Marcela Svarc", "title": "Feature Selection for Functional Data", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of feature selection when the data is\nfunctional, we study several statistical procedures including classification,\nregression and principal components. One advantage of the blinding procedure is\nthat it is very flexible since the features are defined by a set of functions,\nrelevant to the problem being studied, proposed by the user. Our method is\nconsistent under a set of quite general assumptions, and produces good results\nwith the real data examples that we analyze.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 10:57:42 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 10:56:12 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2015 13:28:51 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Fraiman", "Ricardo", ""], ["Gimenez", "Yanina", ""], ["Svarc", "Marcela", ""]]}, {"id": "1502.02166", "submitter": "Helio M. de Oliveira", "authors": "H.M. de Oliveira and G.A.A. de Araujo", "title": "Compactly Supported One-cyclic Wavelets Derived from Beta Distributions", "comments": "7 pages, 4 figures. Journal of Communication and Information Systems,\n  former Journal of the Brazilian Telecommunications Society, ISSN 1980-6604\n  pp.105-111", "journal-ref": "Journal of Communication and Information Systems ISSN 1980-6604\n  pp.105-111", "doi": "10.14209/jcis.2005.17", "report-no": null, "categories": "math.CA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New continuous wavelets of compact support are introduced, which are related\nto the beta distribution. They can be built from probability distributions\nusing 'blur'derivatives. These new wavelets have just one cycle, so they are\ntermed unicycle wavelets. They can be viewed as a soft variety of Haar wavelets\nwhose shape is fine-tuned by two parameters a and b. Close expressions for beta\nwavelets and scale functions as well as their spectra are derived. Their\nimportance is due to the Central Limit Theorem applied for compactly supported\nsignals.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 17:39:10 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["de Oliveira", "H. M.", ""], ["de Araujo", "G. A. A.", ""]]}, {"id": "1502.02501", "submitter": "Pascal Vallet", "authors": "Pascal Vallet and Xavier Mestre and Philippe Loubaton", "title": "A CLT for an improved subspace estimator with observations of increasing\n  dimensions", "comments": "Adding remark 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with subspace estimation in the small sample size regime,\nwhere the number of samples is comparable in magnitude with the observation\ndimension. The traditional estimators, mostly based on the sample correlation\nmatrix, are known to perform well as long as the number of available samples is\nmuch larger than the observation dimension. However, in the small sample size\nregime, the performance degrades. Recently, based on random matrix theory\nresults, a new subspace estimator was introduced, which was shown to be\nconsistent in the asymptotic regime where the number of samples and the\nobservation dimension converge to infinity at the same rate. In practice, this\nestimator outperforms the traditional ones even for certain scenarios where the\nobservation dimension is small and of the same order of magnitude as the number\nof samples. In this paper, we address a performance analysis of this recent\nestimator, by proving a central limit theorem in the above asymptotic regime.\nWe propose an accurate approximation of the mean square error, which can be\nevaluated numerically.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:36:29 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 08:11:35 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Vallet", "Pascal", ""], ["Mestre", "Xavier", ""], ["Loubaton", "Philippe", ""]]}, {"id": "1502.02512", "submitter": "Helio M. de Oliveira", "authors": "H.M. de Oliveira", "title": "The Adaptive Mean-Linkage Algorithm: A Bottom-Up Hierarchical Cluster\n  Technique", "comments": "4 pages, 2 figures, 2 tables. Congresso Brasileiro de Automatica CBA,\n  Natal, RN, Brazil, 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a variant of the classical hierarchical cluster analysis is\nreported. This agglomerative (bottom-up) cluster technique is referred to as\nthe Adaptive Mean-Linkage Algorithm. It can be interpreted as a linkage\nalgorithm where the value of the threshold is conveniently up-dated at each\ninteraction. The superiority of the adaptive clustering with respect to the\naverage-linkage algorithm follows because it achieves a good compromise on\nthreshold values: Thresholds based on the cut-off distance are sufficiently\nsmall to assure the homogeneity and also large enough to guarantee at least a\npair of merging sets. This approach is applied to a set of possible\nsubstituents in a chemical series.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:57:58 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["de Oliveira", "H. M.", ""]]}, {"id": "1502.02534", "submitter": "Eric Chicken", "authors": "Prabhakar Chalise and Eric Chicken and Daniel McGee", "title": "Time Scales in Epidemiological Analysis: An Empirical Comparison", "comments": null, "journal-ref": "International Journal of Statistics and Probability 5 (2016) 91 -\n  101", "doi": "10.5539/ijsp.v5n3p91", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox proportional hazards model is routinely used to analyze time-to-event\ndata. To use this model requires the definition of a unique well-defined time\nscale. Most often, observation time is used as the time scale for both clinical\nand observational studies. Recently after a suggestion that it may be a more\nappropriate scale, chronological age has begun to appear as the time scale used\nin some reports. There appears to be no general consensus about which time\nscale is appropriate for any given analysis. It has been suggested that if the\nbaseline hazard is exponential or if the age-at-entry is independent of\ncovariates used in the model, then the two time scales provide similar results.\nIn this report we provide an empirical examination of the results using the two\ndifferent time scales using a large collection of data sets to examine the\nrelationship between systolic blood pressure and coronary heart disease death\n(CHD death). We demonstrate, in this empirical example that the two time-scales\ncan lead to differing results even when these two conditions appear to hold.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:13:48 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Chalise", "Prabhakar", ""], ["Chicken", "Eric", ""], ["McGee", "Daniel", ""]]}, {"id": "1502.02536", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "Nested Sequential Monte Carlo Methods", "comments": "Extended version of paper published in Proceedings of the 32nd\n  International Conference on Machine Learning (ICML), Lille, France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose nested sequential Monte Carlo (NSMC), a methodology to sample from\nsequences of probability distributions, even where the random variables are\nhigh-dimensional. NSMC generalises the SMC framework by requiring only\napproximate, properly weighted, samples from the SMC proposal distribution,\nwhile still resulting in a correct SMC algorithm. Furthermore, NSMC can in\nitself be used to produce such properly weighted samples. Consequently, one\nNSMC sampler can be used to construct an efficient high-dimensional proposal\ndistribution for another NSMC sampler, and this nesting of the algorithm can be\ndone to an arbitrary degree. This allows us to consider complex and\nhigh-dimensional models using SMC. We show results that motivate the efficacy\nof our approach on several filtering problems with dimensions in the order of\n100 to 1 000.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:15:31 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 07:04:58 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2015 11:01:25 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1502.02574", "submitter": "Christian Hennig", "authors": "Christian Hennig and Chien-Ju Lin", "title": "Flexible parametric bootstrap for testing homogeneity against clustering\n  and assessing the number of clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two notoriously hard problems in cluster analysis, estimating the\nnumber of clusters, and checking whether the population to be clustered is not\nactually homogeneous. Given a dataset, a clustering method and a cluster\nvalidation index, this paper proposes to set up null models that capture\nstructural features of the data that cannot be interpreted as indicating\nclustering. Artificial datasets are sampled from the null model with parameters\nestimated from the original dataset. This can be used for testing the null\nhypothesis of a homogeneous population against a clustering alternative. It can\nalso be used to calibrate the validation index for estimating the number of\nclusters, by taking into account the expected distribution of the index under\nthe null model for any given number of clusters. The approach is illustrated by\nthree examples, involving various different clustering techniques (partitioning\naround medoids, hierarchical methods, a Gaussian mixture model), validation\nindexes (average silhouette width, prediction strength and BIC), and issues\nsuch as mixed type data, temporal and spatial autocorrelation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 17:37:08 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Hennig", "Christian", ""], ["Lin", "Chien-Ju", ""]]}, {"id": "1502.02601", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, L. C. R\\^ego, H. M. de Oliveira, R. M. Campello de Souza", "title": "On a Density for Sets of Integers", "comments": "7 pages, 5 figures; VII Encontro Regional de Matem\\'atica Aplicada e\n  Computacional, 2007, Recife", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A relationship between the Riemann zeta function and a density on integer\nsets is explored. Several properties of the examined density are derived.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 18:58:04 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Cintra", "R. J.", ""], ["R\u00eago", "L. C.", ""], ["de Oliveira", "H. M.", ""], ["de Souza", "R. M. Campello", ""]]}, {"id": "1502.02614", "submitter": "Ben Klemens", "authors": "Ben Klemens", "title": "A Useful Algebraic System of Statistical Models", "comments": "U.S. Census Bureau working paper #2014-06", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper proposes a single form for statistical models that accommodates a\nbroad range of models, from ordinary least squares to agent-based\nmicrosimulations. The definition makes it almost trivial to define morphisms to\ntransform and combine existing models to produce new models. It offers a\nunified means of expressing and implementing methods that are typically given\ndisparate treatment in the literature, including transformations via\ndifferentiable functions, Bayesian updating, multi-level and other types of\ncomposed models, Markov chain Monte Carlo, and several other common procedures.\nIt especially offers benefit to simulation-type models, because of the value in\nbeing able to build complex models from simple parts, easily calculate\nrobustness measures for simulation statistics and, where appropriate, test\nhypotheses. Running examples will be given using Apophenia, an open-source\nsoftware library based on the model form and transformations described here.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 19:20:16 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Klemens", "Ben", ""]]}, {"id": "1502.02708", "submitter": "Eliane  Pinheiro", "authors": "E.C. Pinheiro, S.L.P. Ferrari", "title": "A comparative review of generalizations of the Gumbel extreme value\n  distribution with an application to wind speed data", "comments": "35 pages and 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized extreme value distribution and its particular case, the\nGumbel extreme value distribution, are widely applied for extreme value\nanalysis. The Gumbel distribution has certain drawbacks because it is a\nnon-heavy-tailed distribution and is characterized by constant skewness and\nkurtosis. The generalized extreme value distribution is frequently used in this\ncontext because it encompasses the three possible limiting distributions for a\nnormalized maximum of infinite samples of independent and identically\ndistributed observations. However, the generalized extreme value distribution\nmight not be a suitable model when each observed maximum does not come from a\nlarge number of observations. Hence, other forms of generalizations of the\nGumbel distribution might be preferable. Our goal is to collect in the present\nliterature the distributions that contain the Gumbel distribution embedded in\nthem and to identify those that have flexible skewness and kurtosis, are\nheavy-tailed and could be competitive with the generalized extreme value\ndistribution. The generalizations of the Gumbel distribution are described and\ncompared using an application to a wind speed data set and Monte Carlo\nsimulations. We show that some distributions suffer from overparameterization\nand coincide with other generalized Gumbel distributions with a smaller number\nof parameters, i.e., are non-identifiable. Our study suggests that the\ngeneralized extreme value distribution and a mixture of two extreme value\ndistributions should be considered in practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 22:10:19 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 13:36:19 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Pinheiro", "E. C.", ""], ["Ferrari", "S. L. P.", ""]]}, {"id": "1502.02984", "submitter": "Yue Wang", "authors": "Guoqiang Yu, David J. Miller, Carl D. Langefeld, David M. Herrington,\n  and Yue Wang", "title": "Asymmetric Independence Model for Detecting Interactions between\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting complex interactions among risk factors in case-control studies is\na fundamental task in clinical and population research. However, though\nhypothesis testing using logistic regression (LR) is a convenient solution, the\nLR framework is poorly powered and ill-suited under several common\ncircumstances in practice including missing or unmeasured risk factors,\nimperfectly correlated \"surrogates\", and multiple disease sub-types. The\nweakness of LR in these settings is related to the way in which the null\nhypothesis is defined. Here we propose the Asymmetric Independence Model (AIM)\nas a biologically-inspired alternative to LR, based on the key observation that\nthe mechanisms associated with acquiring a \"disease\" versus maintaining\n\"health\" are asymmetric. We prove mathematically that, unlike LR, AIM is a\nrobust model under the abovementioned confounding scenarios. Further, we\nprovide a mathematical definition of a \"synergistic\" interaction, and prove\nthat theoretically AIM has better power than LR for such interactions. We then\nexperimentally show the superior performance of AIM as compared to LR on both\nsimulations and four real datasets. While the principal application here\ninvolves genetic or environmental variables in the life sciences, our\nmethodology is readily applied to other types of measurements and inferences,\ne.g. in the social sciences.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 16:53:32 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Yu", "Guoqiang", ""], ["Miller", "David J.", ""], ["Langefeld", "Carl D.", ""], ["Herrington", "David M.", ""], ["Wang", "Yue", ""]]}, {"id": "1502.03035", "submitter": "Anders Bredahl Kock", "authors": "Laurent Callot, Mehmet Caner, Anders Bredahl Kock, Juan Andres\n  Riquelme", "title": "Sharp Threshold Detection Based on Sup-norm Error rates in\n  High-dimensional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new estimator, the thresholded scaled Lasso, in high dimensional\nthreshold regressions. First, we establish an upper bound on the $\\ell_\\infty$\nestimation error of the scaled Lasso estimator of Lee et al. (2012). This is a\nnon-trivial task as the literature on high-dimensional models has focused\nalmost exclusively on $\\ell_1$ and $\\ell_2$ estimation errors. We show that\nthis sup-norm bound can be used to distinguish between zero and non-zero\ncoefficients at a much finer scale than would have been possible using\nclassical oracle inequalities. Thus, our sup-norm bound is tailored to\nconsistent variable selection via thresholding.\n  Our simulations show that thresholding the scaled Lasso yields substantial\nimprovements in terms of variable selection. Finally, we use our estimator to\nshed further empirical light on the long running debate on the relationship\nbetween the level of debt (public and private) and GDP growth.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 18:48:19 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Callot", "Laurent", ""], ["Caner", "Mehmet", ""], ["Kock", "Anders Bredahl", ""], ["Riquelme", "Juan Andres", ""]]}, {"id": "1502.03042", "submitter": "Leo Duan", "authors": "Leo L. Duan, Xia Wang and Rhonda D. Szczesniak", "title": "Functional Gaussian Process Model for Bayesian Nonparametric Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process is a theoretically appealing model for nonparametric\nanalysis, but its computational cumbersomeness hinders its use in large scale\nand the existing reduced-rank solutions are usually heuristic. In this work, we\npropose a novel construction of Gaussian process as a projection from fixed\ndiscrete frequencies to any continuous location. This leads to a valid\nstochastic process that has a theoretic support with the reduced rank in the\nspectral density, as well as a high-speed computing algorithm. Our method\nprovides accurate estimates for the covariance parameters and concise form of\npredictive distribution for spatial prediction. For non-stationary data, we\nadopt the mixture framework with a customized spectral dependency structure.\nThis enables clustering based on local stationarity, while maintains the joint\nGaussianness. Our work is directly applicable in solving some of the challenges\nin the spatial data, such as large scale computation, anisotropic covariance,\nspatio-temporal modeling, etc. We illustrate the uses of the model via\nsimulations and an application on a massive dataset.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 18:57:58 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:15:06 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Duan", "Leo L.", ""], ["Wang", "Xia", ""], ["Szczesniak", "Rhonda D.", ""]]}, {"id": "1502.03155", "submitter": "Yuan Liao", "authors": "Victor Chernozhukov, Christian Hansen, Yuan Liao", "title": "A lava attack on the recovery of sums of dense and sparse signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT econ.EM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common high-dimensional methods for prediction rely on having either a sparse\nsignal model, a model in which most parameters are zero and there are a small\nnumber of non-zero parameters that are large in magnitude, or a dense signal\nmodel, a model with no large parameters and very many small non-zero\nparameters. We consider a generalization of these two basic models, termed here\na \"sparse+dense\" model, in which the signal is given by the sum of a sparse\nsignal and a dense signal. Such a structure poses problems for traditional\nsparse estimators, such as the lasso, and for traditional dense estimation\nmethods, such as ridge estimation. We propose a new penalization-based method,\ncalled lava, which is computationally efficient. With suitable choices of\npenalty parameters, the proposed method strictly dominates both lasso and\nridge. We derive analytic expressions for the finite-sample risk function of\nthe lava estimator in the Gaussian sequence model. We also provide an deviation\nbound for the prediction risk in the Gaussian regression model with fixed\ndesign. In both cases, we provide Stein's unbiased estimator for lava's\nprediction risk. A simulation example compares the performance of lava to\nlasso, ridge, and elastic net in a regression example using feasible,\ndata-dependent penalty parameters and illustrates lava's improved performance\nrelative to these benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 00:29:16 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2015 00:09:24 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""], ["Liao", "Yuan", ""]]}, {"id": "1502.03175", "submitter": "Brandon Willard", "authors": "Nicholas G. Polson, James G. Scott and Brandon T. Willard", "title": "Proximal Algorithms in Statistics and Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop proximal methods for statistical learning. Proximal\npoint algorithms are useful in statistics and machine learning for obtaining\noptimization solutions for composite functions. Our approach exploits\nclosed-form solutions of proximal operators and envelope representations based\non the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes.\nEnvelope representations lead to novel proximal algorithms for statistical\noptimisation of composite objective functions which include both non-smooth and\nnon-convex objectives. We illustrate our methodology with regularized Logistic\nand Poisson regression and non-convex bridge penalties with a fused lasso norm.\nWe provide a discussion of convergence of non-descent algorithms with\nacceleration and for non-convex functions. Finally, we provide directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 02:21:49 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 18:22:40 GMT"}, {"version": "v3", "created": "Sat, 30 May 2015 22:01:39 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Willard", "Brandon T.", ""]]}, {"id": "1502.03193", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Tatsuya Kubokawa", "title": "Consistent Estimation in Box-Cox Transformed Linear Mixed Models", "comments": "This paper has been withdrawn by the author due to a crucial error", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Box-Cox transformation is applied to the linear mixed models for\nanalyzing positive and grouped data. The problem in using Box Cox\ntransformation is that the maximum likelihood estimator of the transformation\nparameter is generally inconsistent. To fix it, we suggest a simple and\nconsistent estimator for the transformation parameter based on the moment\nmethod. The consistent estimator is used to construct consistent estimators of\nthe parameters involved in the model and under some conditions, the estimators\nof model parameters are shown to be consistent under $m\\to\\infty$, where $m$ is\nthe number of groups. Moreover, in estimation of the expectation of the\n(future) observations, it is shown that the resulting estimators also hold\nconsistency owing to the consistent estimator of the transformation parameter.\nThe proposed estimating method is compared with the maximum likelihood method\nvia simulation and it is shown that the proposed estimator works better than\nthe maximum likelihood estimator. Finally, the proposed method is applied to a\nreal data set.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 04:43:33 GMT"}, {"version": "v2", "created": "Thu, 12 Feb 2015 08:00:19 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 17:02:56 GMT"}, {"version": "v4", "created": "Tue, 6 Sep 2016 05:47:00 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1502.03215", "submitter": "Pratyaydipta Rudra", "authors": "Smarajit Bose, Amita Pal, Jhimli Mallick, Sunil Kumar and Pratyaydipta\n  Rudra", "title": "A Hybrid Approach for Improved Content-based Image Retrieval using\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of Content-Based Image Retrieval (CBIR) methods is essentially\nto extract, from large (image) databases, a specified number of images similar\nin visual and semantic content to a so-called query image. To bridge the\nsemantic gap that exists between the representation of an image by low-level\nfeatures (namely, colour, shape, texture) and its high-level semantic content\nas perceived by humans, CBIR systems typically make use of the relevance\nfeedback (RF) mechanism. RF iteratively incorporates user-given inputs\nregarding the relevance of retrieved images, to improve retrieval efficiency.\nOne approach is to vary the weights of the features dynamically via feature\nreweighting. In this work, an attempt has been made to improve retrieval\naccuracy by enhancing a CBIR system based on color features alone, through\nimplicit incorporation of shape information obtained through prior segmentation\nof the images. Novel schemes for feature reweighting as well as for\ninitialization of the relevant set for improved relevance feedback, have also\nbeen proposed for boosting performance of RF- based CBIR. At the same time, new\nmeasures for evaluation of retrieval accuracy have been suggested, to overcome\nthe limitations of existing measures in the RF context. Results of extensive\nexperiments have been presented to illustrate the effectiveness of the proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 08:23:05 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Bose", "Smarajit", ""], ["Pal", "Amita", ""], ["Mallick", "Jhimli", ""], ["Kumar", "Sunil", ""], ["Rudra", "Pratyaydipta", ""]]}, {"id": "1502.03301", "submitter": "Helio M. de Oliveira", "authors": "N.S. Santos-Magalhaes, H.M. de Oliveira and A.J. Alves", "title": "On Preparing a List of Random treatment Assigns", "comments": "8 pages. XIV Reuniao Anual da Federacao de Sociedades de Biologia\n  Experimental FeSBE, Caxamb\\'u, Brazil, 1999", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the foundations of a computer oriented approach for\npreparing a list of random treatment assignments to be adopted in randomised\ncontrolled trials. Software is presented which can be applied in the earliest\nstage of clinical trials and bioequivalence assays. This allocation of patients\nto treatment in clinical trials ensures exactly equal treatment numbers. The\ninvestigation of the randomness properties of an assignment leads to the\nconcept of a 'strong randomised list'. The new approach introduced in this note\nis based on thresholds and produces a strong randomised list of treatment\nassignments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 13:25:57 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Santos-Magalhaes", "N. S.", ""], ["de Oliveira", "H. M.", ""], ["Alves", "A. J.", ""]]}, {"id": "1502.03339", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos", "title": "A Bayesian Nonparametric IRT Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a flexible Bayesian nonparametric Item Response Theory\n(IRT) model, which applies to dichotomous or polytomous item responses, and\nwhich can apply to either unidimensional or multidimensional scaling. This is\nan infinite-mixture IRT model, with person ability and item difficulty\nparameters, and with a random intercept parameter that is assigned a mixing\ndistribution, with mixing weights a probit function of other person and item\nparameters. As a result of its flexibility, the Bayesian nonparametric IRT\nmodel can provide outlier-robust estimation of the person ability parameters\nand the item difficulty parameters in the posterior distribution. The\nestimation of the posterior distribution of the model is undertaken by standard\nMarkov chain Monte Carlo (MCMC) methods based on slice sampling. This mixture\nIRT model is illustrated through the analysis of real data obtained from a\nteacher preparation questionnaire, consisting of polytomous items, and\nconsisting of other covariates that describe the examinees (teachers). For\nthese data, the model obtains zero outliers and an R-squared of one. The paper\nconcludes with a short discussion of how to apply the IRT model for the\nanalysis of item response data, using menu-driven software that was developed\nby the author.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 15:37:52 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Karabatsos", "George", ""]]}, {"id": "1502.03391", "submitter": "Vince Lyzinski", "authors": "Vince Lyzinski, Youngser Park, Carey E. Priebe, Michael W. Trosset", "title": "Fast Embedding for JOFC Using the Raw Stress Criterion", "comments": "43 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Joint Optimization of Fidelity and Commensurability (JOFC) manifold\nmatching methodology embeds an omnibus dissimilarity matrix consisting of\nmultiple dissimilarities on the same set of objects. One approach to this\nembedding optimizes the preservation of fidelity to each individual\ndissimilarity matrix together with commensurability of each given observation\nacross modalities via iterative majorization of a raw stress error criterion by\nsuccessive Guttman transforms. In this paper, we exploit the special structure\ninherent to JOFC to exactly and efficiently compute the successive Guttman\ntransforms, and as a result we are able to greatly speed up the JOFC procedure\nfor both in-sample and out-of-sample embedding. We demonstrate the scalability\nof our implementation on both real and simulated data examples.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 17:49:00 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 05:09:38 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 18:28:11 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""], ["Trosset", "Michael W.", ""]]}, {"id": "1502.03400", "submitter": "Helio M. de Oliveira", "authors": "R.M. Campello de Souza and H.M. de Oliveira", "title": "Eigensequences for Multiuser Communication over the Real Adder Channel", "comments": "6 pages, 1 figure, 1 table. VI International Telecommunications\n  Symposium (ITS2006)", "journal-ref": null, "doi": "10.1109/ITS.2006.4433415", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape-invariant signals under the Discrete Fourier Transform are\ninvestigated, leading to a class of eigenfunctions for the unitary discrete\nFourier operator. Such invariant sequences (eigensequences) are suggested as\nuser signatures over the real adder channel (t-RAC) and a multiuser\ncommunication system over the t-RAC is presented.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 18:39:29 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["de Souza", "R. M. Campello", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1502.03416", "submitter": "Chia Chye Yee", "authors": "Yves Atchade and Chia Chye Yee", "title": "On The Sparse Bayesian Learning Of Linear Models", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is a re-examination of the sparse Bayesian learning (SBL) of linear\nregression models of Tipping (2001) in a high-dimensional setting. We propose a\nhard-thresholded version of the SBL estimator that achieves, for orthogonal\ndesign matrices, the non-asymptotic estimation error rate of $\\sigma\\sqrt{s\\log\np}/\\sqrt{n}$, where $n$ is the sample size, $p$ the number of regressors,\n$\\sigma$ is the regression model standard deviation, and $s$ the number of\nnon-zero regression coefficients. We also establish that with high-probability\nthe estimator identifies the non-zero regression coefficients. In our\nsimulations we found that sparse Bayesian learning regression performs better\nthan lasso (Tibshirani (1996)) when the signal to be recovered is strong.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 19:58:46 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Atchade", "Yves", ""], ["Yee", "Chia Chye", ""]]}, {"id": "1502.03486", "submitter": "Joshua Patrick", "authors": "Joshua Patrick, Jane Harvill, and Justin Sims", "title": "Spline-backfitted kernel forecasting for functional-coefficient\n  autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three methods for forecasting a time series modeled using a\nfunctional coefficient autoregressive model (FCAR) fit via spline-backfitted\nlocal linear (SBLL) smoothing. The three methods are a \"naive\" plug-in method,\na bootstrap method, and a multistage method. We present asymptotic results of\nthe SBLL estimation method for FCAR models and show the estimators are oracally\nefficient. The three forecasting methods are compared through simulation. We\nfind that the naive method performs just as well as the multistage method and\neven outperforms it in some situations. We apply the naive and multistage\nmethods to solar irradiance data and compare forecasts based on our method to\nthose of a linear AR model, the model most commonly applied in the solar energy\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 23:28:59 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 21:28:40 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Patrick", "Joshua", ""], ["Harvill", "Jane", ""], ["Sims", "Justin", ""]]}, {"id": "1502.03609", "submitter": "Juho Kopra", "authors": "Juho Kopra, Tommi H\\\"ark\\\"anen, Hanna Tolonen, Juha Karvanen", "title": "Correcting for non-ignorable missingness in smoking trends", "comments": "in Stat, 2015", "journal-ref": null, "doi": "10.1002/sta4.73", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data missing not at random (MNAR) is a major challenge in survey sampling. We\npropose an approach based on registry data to deal with non-ignorable\nmissingness in health examination surveys. The approach relies on follow-up\ndata available from administrative registers several years after the survey.\nFor illustration we use data on smoking prevalence in Finnish National FINRISK\nstudy conducted in 1972-1997. The data consist of measured survey information\nincluding missingness indicators, register-based background information and\nregister-based time-to-disease survival data. The parameters of missingness\nmechanism are estimable with these data although the original survey data are\nMNAR. The underlying data generation process is modelled by a Bayesian model.\nThe results indicate that the estimated smoking prevalence rates in Finland may\nbe significantly affected by missing data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 11:37:19 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Kopra", "Juho", ""], ["H\u00e4rk\u00e4nen", "Tommi", ""], ["Tolonen", "Hanna", ""], ["Karvanen", "Juha", ""]]}, {"id": "1502.03619", "submitter": "Marwane  Ben Hcine", "authors": "Marwane Ben Hcine, Ridha Bouallegue", "title": "On the Approximation of the Sum of Lognormals by a Log Skew Normal\n  Distribution", "comments": "16 pages, 13 figures. arXiv admin note: substantial text overlap with\n  arXiv:1501.02347, arXiv:1501.02344", "journal-ref": "International Journal of Computer Networks & Communications\n  (IJCNC) Vol.7, No.1, January 2015", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been proposed to approximate the sum of lognormal RVs.\nHowever the accuracy of each method relies highly on the region of the\nresulting distribution being examined, and the individual lognormal parameters,\ni.e., mean and variance. There is no such method which can provide the needed\naccuracy for all cases. This paper propose a universal yet very simple\napproximation method for the sum of Lognormals based on log skew normal\napproximation. The main contribution on this work is to propose an analytical\nmethod for log skew normal parameters estimation. The proposed method provides\nhighly accurate approximation to the sum of lognormal distributions over the\nwhole range of dB spreads for any correlation coefficient. Simulation results\nshow that our method outperforms all previously proposed methods and provides\nan accuracy within 0.01 dB for all cases.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 11:56:58 GMT"}], "update_date": "2015-02-14", "authors_parsed": [["Hcine", "Marwane Ben", ""], ["Bouallegue", "Ridha", ""]]}, {"id": "1502.03850", "submitter": "Hui Li", "authors": "Hui Li", "title": "On Nonsymmetric Nonparametric Measures of Dependence", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on recent progress in research on copula based dependence measures, we\nreview the original Renyi's axioms on symmetric measures and propose a new set\nof axioms that applies to nonsymmetric measures. We show that nonsymmetric\nmeasures can actually better characterize the relationship between a pair of\nrandom variables including both independence and complete dependence. The new\nmeasures also satisfy the Data Processing Inequality (DPI) on the * product on\ncopulas, which leads to nice features including the invariance of dependence\nmeasure under bijective transformation on one of the random variables. The\nissues with symmetric measures are also clarified.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 22:32:39 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Li", "Hui", ""]]}, {"id": "1502.03853", "submitter": "Manjari Narayan", "authors": "Manjari Narayan, Genevera I. Allen and Steffie Tomson", "title": "Two Sample Inference for Populations of Graphical Models with\n  Applications to Functional Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Gaussian Graphical Models (GGM) are popularly used in neuroimaging studies\nbased on fMRI, EEG or MEG to estimate functional connectivity, or relationships\nbetween remote brain regions. In multi-subject studies, scientists seek to\nidentify the functional brain connections that are different between two groups\nof subjects, i.e. connections present in a diseased group but absent in\ncontrols or vice versa. This amounts to conducting two-sample large scale\ninference over network edges post graphical model selection, a novel problem we\ncall Population Post Selection Inference. Current approaches to this problem\ninclude estimating a network for each subject, and then assuming the subject\nnetworks are fixed, conducting two-sample inference for each edge. These\napproaches, however, fail to account for the variability associated with\nestimating each subject's graph, thus resulting in high numbers of false\npositives and low statistical power. By using resampling and random\npenalization to estimate the post selection variability together with proper\nrandom effects test statistics, we develop a new procedure we call $R^{3}$ that\nsolves these problems. Through simulation studies we show that $R^{3}$ offers\nmajor improvements over current approaches in terms of error control and\nstatistical power. We apply our method to identify functional connections\npresent or absent in autistic subjects using the ABIDE multi-subject fMRI\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 22:48:32 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Narayan", "Manjari", ""], ["Allen", "Genevera I.", ""], ["Tomson", "Steffie", ""]]}, {"id": "1502.03939", "submitter": "Bruno Sudret", "authors": "R. Schoebi, B. Sudret, J. Wiart", "title": "Polynomial-Chaos-based Kriging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulation has become the standard tool in many engineering fields\nfor designing and optimizing systems, as well as for assessing their\nreliability. To cope with demanding analysis such as optimization and\nreliability, surrogate models (a.k.a meta-models) have been increasingly\ninvestigated in the last decade. Polynomial Chaos Expansions (PCE) and Kriging\nare two popular non-intrusive meta-modelling techniques. PCE surrogates the\ncomputational model with a series of orthonormal polynomials in the input\nvariables where polynomials are chosen in coherency with the probability\ndistributions of those input variables. On the other hand, Kriging assumes that\nthe computer model behaves as a realization of a Gaussian random process whose\nparameters are estimated from the available computer runs, i.e. input vectors\nand response values. These two techniques have been developed more or less in\nparallel so far with little interaction between the researchers in the two\nfields. In this paper, PC-Kriging is derived as a new non-intrusive\nmeta-modeling approach combining PCE and Kriging. A sparse set of orthonormal\npolynomials (PCE) approximates the global behavior of the computational model\nwhereas Kriging manages the local variability of the model output. An adaptive\nalgorithm similar to the least angle regression algorithm determines the\noptimal sparse set of polynomials. PC-Kriging is validated on various benchmark\nanalytical functions which are easy to sample for reference results. From the\nnumerical investigations it is concluded that PC-Kriging performs better than\nor at least as good as the two distinct meta-modeling techniques. A larger gain\nin accuracy is obtained when the experimental design has a limited size, which\nis an asset when dealing with demanding computational models.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 10:53:52 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Schoebi", "R.", ""], ["Sudret", "B.", ""], ["Wiart", "J.", ""]]}, {"id": "1502.04206", "submitter": "Luiz Max Carvalho PhD", "authors": "Luiz Max de Carvalho and Daniel A.M. Villela and Flavio Codeco Coelho\n  and Leonardo Soares Bastos", "title": "Combining probability distributions: Extending the logarithmic pooling\n  approach", "comments": "Massively updated manuscript; submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Combining distributions is an important issue in decision theory and Bayesian\ninference. Logarithmic pooling is a popular method to aggregate expert opinions\nby using a set of weights that reflect the reliability of each information\nsource. However, the resulting pooled distribution depends heavily on set of\nweights given to each opinion/prior and thus careful consideration must be\ngiven to the choice of weights. In this paper we review and extend the\nstatistical theory of logarithmic pooling, focusing on the assignment of the\nweights using a hierarchical prior distribution. We explore several statistical\napplications, such as the estimation of survival probabilities, meta-analysis\nand Bayesian melding of deterministic models of population growth and\nepidemics. We show that it is possible learn the weights from data, although\nidentifiability issues may arise for some configurations of priors and data.\nFurthermore, we show how the hierarchical approach leads to posterior\ndistributions that are able to accommodate prior-data conflict in complex\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 12:53:16 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 14:44:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["de Carvalho", "Luiz Max", ""], ["Villela", "Daniel A. M.", ""], ["Coelho", "Flavio Codeco", ""], ["Bastos", "Leonardo Soares", ""]]}, {"id": "1502.04221", "submitter": "Renato J Cintra", "authors": "A. Madanayake, R. J. Cintra, D. Onen, V. S. Dimitrov, N. T.\n  Rajapaksha, L. T. Bruton, A. Edirisuriya", "title": "A Row-parallel 8$\\times$8 2-D DCT Architecture Using Algebraic Integer\n  Based Exact Computation", "comments": "28 pages, 9 figures, 7 tables, corrected typos", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  vol. 22, no. 6, pp. 915--929, 2012", "doi": "10.1109/TCSVT.2011.2181232", "report-no": null, "categories": "cs.AR cs.DM math.NT stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algebraic integer (AI) based time-multiplexed row-parallel architecture\nand two final-reconstruction step (FRS) algorithms are proposed for the\nimplementation of bivariate AI-encoded 2-D discrete cosine transform (DCT). The\narchitecture directly realizes an error-free 2-D DCT without using FRSs between\nrow-column transforms, leading to an 8$\\times$8 2-D DCT which is entirely free\nof quantization errors in AI basis. As a result, the user-selectable accuracy\nfor each of the coefficients in the FRS facilitates each of the 64 coefficients\nto have its precision set independently of others, avoiding the leakage of\nquantization noise between channels as is the case for published DCT designs.\nThe proposed FRS uses two approaches based on (i) optimized Dempster-Macleod\nmultipliers and (ii) expansion factor scaling. This architecture enables\nlow-noise high-dynamic range applications in digital video processing that\nrequires full control of the finite-precision computation of the 2-D DCT. The\nproposed architectures and FRS techniques are experimentally verified and\nvalidated using hardware implementations that are physically realized and\nverified on FPGA chip. Six designs, for 4- and 8-bit input word sizes, using\nthe two proposed FRS schemes, have been designed, simulated, physically\nimplemented and measured. The maximum clock rate and block-rate achieved among\n8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a\npixel rate of 8$\\times$307.787$\\approx$2.462 GHz if eventually embedded in a\nreal-time video-processing system. The equivalent frame rate is about 1187.35\nHz for the image size of 1920$\\times$1080. All implementations are functional\non a Xilinx Virtex-6 XC6VLX240T FPGA device.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 16:14:05 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Onen", "D.", ""], ["Dimitrov", "V. S.", ""], ["Rajapaksha", "N. T.", ""], ["Bruton", "L. T.", ""], ["Edirisuriya", "A.", ""]]}, {"id": "1502.04237", "submitter": "Wen-Xin Zhou", "authors": "Jianqing Fan, Qi-Man Shao, Wen-Xin Zhou", "title": "Are Discoveries Spurious? Distributions of Maximum Spurious Correlations\n  and Their Applications", "comments": null, "journal-ref": null, "doi": "10.1214/17-AOS1575", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, many exciting variable selection methods have been\ndeveloped for finding a small group of covariates that are associated with the\nresponse from a large pool. Can the discoveries from these data mining\napproaches be spurious due to high dimensionality and limited sample size? Can\nour fundamental assumptions about the exogeneity of the covariates needed for\nsuch variable selection be validated with the data? To answer these questions,\nwe need to derive the distributions of the maximum spurious correlations given\na certain number of predictors, namely, the distribution of the correlation of\na response variable $Y$ with the best $s$ linear combinations of $p$ covariates\n$\\mathbf{X}$, even when $\\mathbf{X}$ and $Y$ are independent. When the\ncovariance matrix of $\\mathbf{X}$ possesses the restricted eigenvalue property,\nwe derive such distributions for both a finite $s$ and a diverging $s$, using\nGaussian approximation and empirical process techniques. However, such a\ndistribution depends on the unknown covariance matrix of $\\mathbf{X}$. Hence,\nwe use the multiplier bootstrap procedure to approximate the unknown\ndistributions and establish the consistency of such a simple bootstrap\napproach. The results are further extended to the situation where the residuals\nare from regularized fits. Our approach is then used to construct the upper\nconfidence limit for the maximum spurious correlation and to test the\nexogeneity of the covariates. The former provides a baseline for guarding\nagainst false discoveries and the latter tests whether our fundamental\nassumptions for high-dimensional model selection are statistically valid. Our\ntechniques and results are illustrated with both numerical examples and real\ndata analysis.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 19:42:19 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 00:14:48 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 21:14:04 GMT"}, {"version": "v4", "created": "Fri, 21 Jul 2017 14:14:14 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Fan", "Jianqing", ""], ["Shao", "Qi-Man", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1502.04269", "submitter": "Berk Ustun", "authors": "Berk Ustun and Cynthia Rudin", "title": "Supersparse Linear Integer Models for Optimized Medical Scoring Systems", "comments": "This version reflects our findings on SLIM as of January 2016\n  (arXiv:1306.5860 and arXiv:1405.4047 are out-of-date). The final published\n  version of this articled is available at http://www.springerlink.com", "journal-ref": null, "doi": "10.1007/s10994-015-5528-6", "report-no": null, "categories": "stat.ML cs.DM cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring systems are linear classification models that only require users to\nadd, subtract and multiply a few small numbers in order to make a prediction.\nThese models are in widespread use by the medical community, but are difficult\nto learn from data because they need to be accurate and sparse, have coprime\ninteger coefficients, and satisfy multiple operational constraints. We present\na new method for creating data-driven scoring systems called a Supersparse\nLinear Integer Model (SLIM). SLIM scoring systems are built by solving an\ninteger program that directly encodes measures of accuracy (the 0-1 loss) and\nsparsity (the $\\ell_0$-seminorm) while restricting coefficients to coprime\nintegers. SLIM can seamlessly incorporate a wide range of operational\nconstraints related to accuracy and sparsity, and can produce highly tailored\nmodels without parameter tuning. We provide bounds on the testing and training\naccuracy of SLIM scoring systems, and present a new data reduction technique\nthat can improve scalability by eliminating a portion of the training data\nbeforehand. Our paper includes results from a collaboration with the\nMassachusetts General Hospital Sleep Laboratory, where SLIM was used to create\na highly tailored scoring system for sleep apnea screening\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 01:26:41 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 14:46:40 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 17:34:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1502.04329", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz and Moritz Berger", "title": "Item focussed Trees for the Identification of Items in Differential Item\n  Functioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for the identification of differential item functioning (DIF) by\nusing recursive partitioning techniques is proposed. We assume an extension of\nthe Rasch model that allows for DIF being induced by an arbitrary number of\ncovariates for each item. Recursive partitioning on the item level results in\none tree for each item and leads to simultaneous selection of items and\nvariables that induce DIF. For each item it is possible to detect groups of\nsubjects with different item difficulties, defined by combinations of\ncharacteristics that are not pre-specified. An algorithm is proposed that is\nbased on permutation tests. Various simulation studies, including the\ncomparison with traditional approaches to identify items with DIF, show the\napplicability and the competitive performance of the method. Two applications\nillustrate the usefulness and the advantages of the new method.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 16:22:44 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Tutz", "Gerhard", ""], ["Berger", "Moritz", ""]]}, {"id": "1502.04528", "submitter": "Long Feng", "authors": "Long Feng", "title": "Scalar-Invariant Test for High-Dimensional Regression Coefficients", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This article is concerned with simultaneous tests on linear regression\ncoefficients in high-dimensional settings. When the dimensionality is larger\nthan the sample size, the classic $F$-test is not applicable since the sample\ncovariance matrix is not invertible. In order to overcome this issue, both\nGoeman, Finos and van Houwelingen (2011) and Zhong and Chen (2011) proposed\ntheir test procedures after excluding the $(\\X^{'}\\X)^{-1}$ term in\n$F$-statistics. However, both these two test are not invariant under the group\nof scalar transformations. In order to treat those variables in a `fair' way,\nwe proposed a new test statistic and establish its asymptotically normal under\ncertain mild conditions. Simulation studies showed that our test procedure\nperforms very well in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 13:29:47 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Feng", "Long", ""]]}, {"id": "1502.04558", "submitter": "Long Feng", "authors": "Long Feng", "title": "High Dimensional Rank Tests for Sphericity", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sphericity test plays a key role in many statistical problems. We propose\nSpearman's rho-type rank test and Kendall's tau-type rank test for sphericity\nin the high dimensional settings. We show that these two tests are equivalent.\nThanks to the \"blessing of dimension\", we do not need to estimate any nuisance\nparameters. Without estimating the location parameter, we can allow the\ndimension to be arbitrary large. Asymptotic normality of these two tests are\nalso established under elliptical distributions. Simulations demonstrate that\nthey are very robust and efficient in a wide range of settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 14:44:42 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Feng", "Long", ""]]}, {"id": "1502.04733", "submitter": "Weichen Wang", "authors": "Jianqing Fan and Weichen Wang", "title": "Asymptotics of Empirical Eigen-structure for Ultra-high Dimensional\n  Spiked Covariance Model", "comments": "50 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the asymptotic distributions of the spiked eigenvalues and\neigenvectors under a generalized and unified asymptotic regime, which takes\ninto account the spike magnitude of leading eigenvalues, sample size, and\ndimensionality. This new regime allows high dimensionality and diverging\neigenvalue spikes and provides new insights into the roles the leading\neigenvalues, sample size, and dimensionality play in principal component\nanalysis. The results are proven by a technical device, which swaps the role of\nrows and columns and converts the high-dimensional problems into\nlow-dimensional ones. Our results are a natural extension of those in Paul\n(2007) to more general setting with new insights and solve the rates of\nconvergence problems in Shen et al. (2013). They also reveal the biases of the\nestimation of leading eigenvalues and eigenvectors by using principal component\nanalysis, and lead to a new covariance estimator for the approximate factor\nmodel, called shrinkage principal orthogonal complement thresholding (S-POET),\nthat corrects the biases. Our results are successfully applied to outstanding\nproblems in estimation of risks of large portfolios and false discovery\nproportions for dependent test statistics and are illustrated by simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 21:44:14 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 05:04:14 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Fan", "Jianqing", ""], ["Wang", "Weichen", ""]]}, {"id": "1502.04740", "submitter": "Yan Sun", "authors": "Yan Sun, Jennifer Loveland, and Isaac Blackhurst", "title": "Conditional Heteroskedasticity of Return Range Processes", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Price range contains important information about the asset volatility, and\nhas long been considered an important indicator for it. In this paper, we\npropose to jointly model the [low, high] price range as a random interval and\nintroduce an interval-valued GARCH (Int-GARCH) model for the corresponding\n[low, high] return range process. Model properties are presented under the\ngeneral framework of random sets, and the parameters are estimated by a\nmetric-based conditional least squares (CLS) method. Our empirical analysis of\nthe daily return range data of Dow Jones component stocks yields very\ninteresting results.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 22:28:31 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Sun", "Yan", ""], ["Loveland", "Jennifer", ""], ["Blackhurst", "Isaac", ""]]}, {"id": "1502.04742", "submitter": "Necla Gunduz", "authors": "Necla Gunduz and Ernest Fokoue", "title": "On the Predictive Properties of Binary Link Functions", "comments": "17 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:math-ph/0607066 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a theoretical and computational justification of the long\nheld claim that of the similarity of the probit and logit link functions often\nused in binary classification. Despite this widespread recognition of the\nstrong similarities between these two link functions, very few (if any)\nresearchers have dedicated time to carry out a formal study aimed at\nestablishing and characterizing firmly all the aspects of the similarities and\ndifferences. This paper proposes a definition of both structural and predictive\nequivalence of link functions-based binary regression models, and explores the\nvarious ways in which they are either similar or dissimilar. From a predictive\nanalytics perspective, it turns out that not only are probit and logit\nperfectly predictively concordant, but the other link functions like cauchit\nand complementary log log enjoy very high percentage of predictive equivalence.\nThroughout this paper, simulated and real life examples demonstrate all the\nequivalence results that we prove theoretically.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 22:39:57 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Gunduz", "Necla", ""], ["Fokoue", "Ernest", ""]]}, {"id": "1502.04765", "submitter": "Chao Zheng", "authors": "Davide Ferrari and Chao Zheng", "title": "Reliable inference for complex models by discriminative composite\n  likelihood estimation", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composite likelihood estimation has an important role in the analysis of\nmultivariate data for which the full likelihood function is intractable. An\nimportant issue in composite likelihood inference is the choice of the weights\nassociated with lower-dimensional data sub-sets, since the presence of\nincompatible sub-models can deteriorate the accuracy of the resulting\nestimator. In this paper, we introduce a new approach for simultaneous\nparameter estimation by tilting, or re-weighting, each sub-likelihood component\ncalled discriminative composite likelihood estimation (D-McLE). The\ndata-adaptive weights maximize the composite likelihood function, subject to\nmoving a given distance from uniform weights; then, the resulting weights can\nbe used to rank lower-dimensional likelihoods in terms of their influence in\nthe composite likelihood function. Our analytical findings and numerical\nexamples support the stability of the resulting estimator compared to\nestimators constructed using standard composition strategies based on uniform\nweights. The properties of the new method are illustrated through simulated\ndata and real spatial data on multivariate precipitation extremes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 01:46:59 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 07:09:02 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Ferrari", "Davide", ""], ["Zheng", "Chao", ""]]}, {"id": "1502.04800", "submitter": "Davide Ferrari", "authors": "Davide Ferrari and Guoqi Qian", "title": "Parsimonious and Efficient Likelihood Composition by Gibbs Sampling", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional maximum likelihood estimator (MLE) is often of limited use in\ncomplex high-dimensional data due to the intractability of the underlying\nlikelihood function. Maximum composite likelihood estimation (McLE) avoids full\nlikelihood specification by combining a number of partial likelihood objects\ndepending on small data subsets, thus enabling inference for complex data. A\nfundamental difficulty in making the McLE approach practicable is the selection\nfrom numerous candidate likelihood objects for constructing the composite\nlikelihood function. In this paper, we propose a flexible Gibbs sampling scheme\nfor optimal selection of sub-likelihood components. The sampled composite\nlikelihood functions are shown to converge to the one maximally informative on\nthe unknown parameters in equilibrium, since sub-likelihood objects are chosen\nwith probability depending on the variance of the corresponding McLE. A\npenalized version of our method generates sparse likelihoods with a relatively\nsmall number of components when the data complexity is intense. Our algorithms\nare illustrated through numerical examples on simulated data as well as real\ngenotype SNP data from a case-control study.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 05:23:35 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Ferrari", "Davide", ""], ["Qian", "Guoqi", ""]]}, {"id": "1502.04890", "submitter": "Leonid Torgovitski", "authors": "Leonid Torgovitski", "title": "Algorithm for overlapping estimation of common change-sets in spatial\n  data of fixed size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible class of estimates for \"common change in the mean\" sets\nin spatio-temporal data. We rely on a scan type approach by subdividing the\nspatial observations into suitable overlapping regions to which classical CUSUM\n(cumulative sums) estimates may then be applied separately. The aggregated\n\"local\" estimates are used to construct consistent \"global\" estimates of the\nchange set(s) by taking the overlapping structure into account. The domain and\nthe change regions may have irregular shapes and the suggested procedure is\nespecially suited for estimation of multiple change regions. The performance is\ndemonstrated in a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 13:40:36 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Torgovitski", "Leonid", ""]]}, {"id": "1502.05367", "submitter": "Damien Challet", "authors": "Damien Challet", "title": "One- and two-sample nonparametric tests for the signal-to-noise ratio\n  based on record statistics", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new family of nonparametric statistics, the r-statistics, is introduced. It\nconsists of counting the number of records of the cumulative sum of the sample.\nThe single-sample r-statistic is almost as powerful as Student's t-statistic\nfor Gaussian and uniformly distributed variables, and more powerful than the\nsign and Wilcoxon signed-rank statistics as long as the data are not too\nheavy-tailed.\n  Three two-sample parametric r-statistics are proposed, one with a higher\nspecificity but a smaller sensitivity than Mann-Whitney U-test and the other\none a higher sensitivity but a smaller specificity. A nonparametric two-sample\nr-statistic is introduced, whose power is very close to that of Welch statistic\nfor Gaussian or uniformly distributed variables.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 20:20:24 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 16:15:49 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Challet", "Damien", ""]]}, {"id": "1502.05427", "submitter": "Zo\\'e van Havre", "authors": "Zoe van Havre (Universite Paris-Dauphine & QUT, Brisbane), Nicole\n  White (QUT, Brisbane), Judith Rousseau (Universite Paris-Dauphine), and\n  Kerrie Mengersen (QUT, Brisbane)", "title": "Overfitting Bayesian Mixture Models with an Unknown Number of Components", "comments": null, "journal-ref": "Plos One, 10(7), e0131739 (2015)", "doi": "10.1371/journal.pone.0131739", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes solutions to three issues pertaining to the estimation of\nfinite mixture models with an unknown number of components: the\nnon-identifiability induced by overfitting the number of components, the mixing\nlimitations of standard Markov Chain Monte Carlo (MCMC) sampling techniques,\nand the related label switching problem. An overfitting approach is used to\nestimate the number of components in a finite mixture model via a Zmix\nalgorithm. Zmix provides a bridge between multidimensional samplers and test\nbased estimation methods, whereby priors are chosen to encourage extra groups\nto have weights approaching zero. MCMC sampling is made possible by the\nimplementation of prior parallel tempering, an extension of parallel tempering.\nZmix can accurately estimate the number of components, posterior parameter\nestimates and allocation probabilities given a sufficiently large sample size.\nThe results will reflect uncertainty in the final model and will report the\nrange of possible candidate models and their respective estimated probabilities\nfrom a single run. Label switching is resolved with a computationally\nlight-weight method, Zswitch, developed for overfitted mixtures by exploiting\nthe intuitiveness of allocation-based relabelling algorithms and the precision\nof label-invariant loss functions. Four simulation studies are included to\nillustrate Zmix and Zswitch, as well as three case studies from the literature.\nAll methods are available as part of the R package Zmix, which can currently be\napplied to univariate Gaussian mixture models\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 22:06:25 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 04:23:50 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["van Havre", "Zoe", "", "Universite Paris-Dauphine & QUT, Brisbane"], ["White", "Nicole", "", "QUT, Brisbane"], ["Rousseau", "Judith", "", "Universite Paris-Dauphine"], ["Mengersen", "Kerrie", "", "QUT, Brisbane"]]}, {"id": "1502.05455", "submitter": "Long Feng", "authors": "Long Feng and Fasheng Sun", "title": "A Note on High Dimensional Two Sample Mean Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we propose a new scalar and shift transform invariant test\nstatistic for the high-dimensional two-sample location test. The expectation of\nour test is exactly zero under the null hypothesis. And we allow the dimension\ncould be arbitrary large. Theoretical results and simulation comparison show\nthe good performance of our test.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 02:10:20 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Feng", "Long", ""], ["Sun", "Fasheng", ""]]}, {"id": "1502.05503", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann, Jukka Corander, Ritabrata Dutta, and Samuel Kaski", "title": "Classification and Bayesian Optimization for Likelihood-Free Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some statistical models are specified via a data generating process for which\nthe likelihood function cannot be computed in closed form. Standard\nlikelihood-based inference is then not feasible but the model parameters can be\ninferred by finding the values which yield simulated data that resemble the\nobserved data. This approach faces at least two major difficulties: The first\ndifficulty is the choice of the discrepancy measure which is used to judge\nwhether the simulated data resemble the observed data. The second difficulty is\nthe computationally efficient identification of regions in the parameter space\nwhere the discrepancy is low. We give here an introduction to our recent work\nwhere we tackle the two difficulties through classification and Bayesian\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 09:09:27 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""], ["Dutta", "Ritabrata", ""], ["Kaski", "Samuel", ""]]}, {"id": "1502.05600", "submitter": "Graciela Boente", "authors": "Ana M. Bianco, Graciela Boente and Isabel M. Rodrigues", "title": "Conditional tests for elliptical symmetry using robust estimators", "comments": "In press in Communications in Statistics: Theory and Methods, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a procedure for testing the hypothesis that the\nunderlying distribution of the data is elliptical when using robust location\nand scatter estimators instead of the sample mean and covariance matrix. Under\nmild assumptions that include elliptical distributions without first moments,\nwe derive the test statistic asymptotic behaviour under the null hypothesis and\nunder special alternatives. Numerical experiments allow to compare the\nbehaviour of the tests based on the sample mean and covariance matrix with that\nbased on robust estimators, under various elliptical distributions and\ndifferent alternatives. This comparison was done looking not only at the\nobserved level and power but we rather use the size-corrected relative exact\npower which provides a tool to assess the test statistic skill to detect\nalternatives. We also provide a numerical comparison with other competing\ntests.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 15:38:22 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Bianco", "Ana M.", ""], ["Boente", "Graciela", ""], ["Rodrigues", "Isabel M.", ""]]}, {"id": "1502.05815", "submitter": "Mercedes Conde-Amboage", "authors": "Mercedes Conde-Amboage, C\\'esar S\\'anchez-Sellero and Wenceslao\n  Gonz\\'alez-Manteiga", "title": "A lack-of-fit test for quantile regression models with high-dimensional\n  covariates", "comments": "14 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new lack-of-fit test for quantile regression models that is\nsuitable even with high-dimensional covariates. The test is based on the\ncumulative sum of residuals with respect to unidimensional linear projections\nof the covariates. The test adapts concepts proposed by Escanciano (Econometric\nTheory, 22, 2006) to cope with many covariates to the test proposed by He and\nZhu (Journal of the American Statistical Association, 98, 2003). To approximate\nthe critical values of the test, a wild bootstrap mechanism is used, similar to\nthat proposed by Feng et al. (Biometrika, 98, 2011). An extensive simulation\nstudy was undertaken that shows the good performance of the new test,\nparticularly when the dimension of the covariate is high. The test can also be\napplied and performs well under heteroscedastic regression models. The test is\nillustrated with real data about the economic growth of 161 countries.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 10:03:07 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Conde-Amboage", "Mercedes", ""], ["S\u00e1nchez-Sellero", "C\u00e9sar", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "1502.05933", "submitter": "Ghislain Durif", "authors": "G. Durif, L. Modolo, J. Michaelsson, J. E. Mold, S. Lambert-Lacroix\n  and F. Picard", "title": "High Dimensional Classification with combined Adaptive Sparse PLS and\n  Logistic Regression", "comments": "9 pages, 3 figures, 4 tables + Supplementary Materials 8 pages, 3\n  figures, 10 tables", "journal-ref": "Bioinformatics 34, 485-493 (2018)", "doi": "10.1093/bioinformatics/btx571", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The high dimensionality of genomic data calls for the development\nof specific classification methodologies, especially to prevent over-optimistic\npredictions. This challenge can be tackled by compression and variable\nselection, which combined constitute a powerful framework for classification,\nas well as data visualization and interpretation. However, current proposed\ncombinations lead to instable and non convergent methods due to inappropriate\ncomputational frameworks. We hereby propose a stable and convergent approach\nfor classification in high dimensional based on sparse Partial Least Squares\n(sparse PLS). Results: We start by proposing a new solution for the sparse PLS\nproblem that is based on proximal operators for the case of univariate\nresponses. Then we develop an adaptive version of the sparse PLS for\nclassification, which combines iterative optimization of logistic regression\nand sparse PLS to ensure convergence and stability. Our results are confirmed\non synthetic and experimental data. In particular we show how crucial\nconvergence and stability can be when cross-validation is involved for\ncalibration purposes. Using gene expression data we explore the prediction of\nbreast cancer relapse. We also propose a multicategorial version of our method\non the prediction of cell-types based on single-cell expression data.\nAvailability: Our approach is implemented in the plsgenomics R-package.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 16:56:59 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 09:07:41 GMT"}, {"version": "v3", "created": "Wed, 6 May 2015 09:18:08 GMT"}, {"version": "v4", "created": "Fri, 12 May 2017 11:59:22 GMT"}, {"version": "v5", "created": "Wed, 30 Aug 2017 16:06:38 GMT"}], "update_date": "2021-04-10", "authors_parsed": [["Durif", "G.", ""], ["Modolo", "L.", ""], ["Michaelsson", "J.", ""], ["Mold", "J. E.", ""], ["Lambert-Lacroix", "S.", ""], ["Picard", "F.", ""]]}, {"id": "1502.06045", "submitter": "P. Richard Hahn", "authors": "P. Richard Hahn", "title": "Model specification via sequential coherence and backward induction", "comments": "25", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how to specify probability models for data analysis via\na backward induction procedure. The new approach yields coherent, prior-free\nuncertainty assessment. After presenting some intuition-building examples, the\nnew approach is applied to a kernel density estimator, which leads to a novel\nmethod for computing point-wise credible intervals in nonparametric density\nestimation. The new approach has two additional advantages; 1) the posterior\nmean density can be accurately approximated without resorting to Monte Carlo\nsimulation and 2) concentration bounds are easily established as a function of\nsample size.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 00:29:53 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Hahn", "P. Richard", ""]]}, {"id": "1502.06197", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "On Online Control of False Discovery Rate", "comments": "31 pages, 6 figures (minor edits)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypotheses testing is a core problem in statistical inference and\narises in almost every scientific field. Given a sequence of null hypotheses\n$\\mathcal{H}(n) = (H_1,..., H_n)$, Benjamini and Hochberg\n\\cite{benjamini1995controlling} introduced the false discovery rate (FDR)\ncriterion, which is the expected proportion of false positives among rejected\nnull hypotheses, and proposed a testing procedure that controls FDR below a\npre-assigned significance level. They also proposed a different criterion,\ncalled mFDR, which does not control a property of the realized set of tests;\nrather it controls the ratio of expected number of false discoveries to the\nexpected number of discoveries.\n  In this paper, we propose two procedures for multiple hypotheses testing that\nwe will call \"LOND\" and \"LORD\". These procedures control FDR and mFDR in an\n\\emph{online manner}. Concretely, we consider an ordered --possibly infinite--\nsequence of null hypotheses $\\mathcal{H} = (H_1,H_2,H_3,...)$ where, at each\nstep $i$, the statistician must decide whether to reject hypothesis $H_i$\nhaving access only to the previous decisions. To the best of our knowledge, our\nwork is the first that controls FDR in this setting. This model was introduced\nby Foster and Stine \\cite{alpha-investing} whose alpha-investing rule only\ncontrols mFDR in online manner.\n  In order to compare different procedures, we develop lower bounds on the\ntotal discovery rate under the mixture model and prove that both LOND and LORD\nhave nearly linear number of discoveries. We further propose adjustment to LOND\nto address arbitrary correlation among the $p$-values. Finally, we evaluate the\nperformance of our procedures on both synthetic and real data comparing them\nwith alpha-investing rule, Benjamin-Hochberg method and a Bonferroni procedure.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 09:07:07 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 00:39:16 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1502.06241", "submitter": "Jeffrey Miller", "authors": "Jeffrey W. Miller and Matthew T. Harrison", "title": "Mixture models with a prior on the number of components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural Bayesian approach for mixture models with an unknown number of\ncomponents is to take the usual finite mixture model with Dirichlet weights,\nand put a prior on the number of components---that is, to use a mixture of\nfinite mixtures (MFM). While inference in MFMs can be done with methods such as\nreversible jump Markov chain Monte Carlo, it is much more common to use\nDirichlet process mixture (DPM) models because of the relative ease and\ngenerality with which DPM samplers can be applied. In this paper, we show that,\nin fact, many of the attractive mathematical properties of DPMs are also\nexhibited by MFMs---a simple exchangeable partition distribution, restaurant\nprocess, random measure representation, and in certain cases, a stick-breaking\nrepresentation. Consequently, the powerful methods developed for inference in\nDPMs can be directly applied to MFMs as well. We illustrate with simulated and\nreal data, including high-dimensional gene expression data.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 15:53:22 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Miller", "Jeffrey W.", ""], ["Harrison", "Matthew T.", ""]]}, {"id": "1502.06254", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "The fundamental nature of the log loss function", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard loss functions used in the literature on probabilistic\nprediction are the log loss function, the Brier loss function, and the\nspherical loss function; however, any computable proper loss function can be\nused for comparison of prediction algorithms. This note shows that the log loss\nfunction is most selective in that any prediction algorithm that is optimal for\na given data sequence (in the sense of the algorithmic theory of randomness)\nunder the log loss function will be optimal under any computable proper mixable\nloss function; on the other hand, there is a data sequence and a prediction\nalgorithm that is optimal for that sequence under either of the two other\nstandard loss functions but not under the log loss function.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 17:58:05 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 15:00:20 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "1502.06412", "submitter": "J\\'an Somor\\v{c}\\'ik", "authors": "R\\'obert T\\'oth and J\\'an Somor\\v{c}\\'ik", "title": "On a non-parametric confidence interval for the regression slope", "comments": "Changes in Version 2: (1) added references to the usage of the R\n  package \"mblm\" in other sciences; (2) expanded Abstract and Introduction; (3)\n  updated affiliation of the first author. Changes in Version 3 : (1) Condition\n  2 itroduced at a later stage; (2) true confidence levels of the Theil's CI\n  added to Table 1. Changes in Version 4: section `A real life example' added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an application of the Tukey's methodology in Theil's\nregression to obtain a confidence interval for the true slope in the straight\nline regression model with not necessarily normal errors. This specific\napproach is implemented since 2005 in a package of the software R; however,\nwithout any theoretical background. We illustrate by Monte Carlo simulations,\nthat this methodology, unlike the classical Theil's approach based on Kendall's\ntau, seriously deflates the true confidence level of the resulting interval. We\nprovide also rigorous proofs in case of four data points (in general) and in\ncase of five data points (under some additional conditions); together with a\nreal life methods usage example in the latter case. Summing up, we demonstrate\nthat one should never combine statistical methods without checking the\nassumptions of their usage and we also give a warning to the already wide\ncommunity of R users of Theil's regression from various fields of science.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 13:20:34 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 09:14:43 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 13:08:43 GMT"}, {"version": "v4", "created": "Fri, 9 Sep 2016 10:05:37 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["T\u00f3th", "R\u00f3bert", ""], ["Somor\u010d\u00edk", "J\u00e1n", ""]]}, {"id": "1502.06440", "submitter": "Erlis Ruli", "authors": "Erlis Ruli, Nicola Sartori, Laura Ventura", "title": "Improved Laplace Approximation for Marginal Likelihoods", "comments": "24 pages", "journal-ref": "Electronic Journal of Statistics 10(2), 3986-4009, 2016", "doi": "10.1214/16-EJS1218", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical applications often involve the calculation of intractable\nmultidimensional integrals. The Laplace formula is widely used to approximate\nsuch integrals. However, in high-dimensional or small sample size problems, the\nshape of the integrand function may be far from that of the Gaussian density,\nand thus the standard Laplace approximation can be inaccurate. We propose an\nimproved Laplace approximation that reduces the asymptotic error of the\nstandard Laplace formula by one order of magnitude, thus leading to third-order\naccuracy. We also show, by means of practical examples of various complexity,\nthat the proposed method is extremely accurate, even in high dimensions,\nimproving over the standard Laplace formula. Such examples also demonstrate\nthat the accuracy of the proposed method is comparable with that of other\nexisting methods, which are computationally more demanding. An R implementation\nof the improved Laplace approximation is also provided through the R package\niLaplace available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 14:20:45 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 12:56:33 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ruli", "Erlis", ""], ["Sartori", "Nicola", ""], ["Ventura", "Laura", ""]]}, {"id": "1502.06449", "submitter": "Gertraud Malsiner-Walli", "authors": "Gertraud Malsiner-Walli and Sylvia Fr\\\"uhwirth-Schnatter and Bettina\n  Gr\\\"un", "title": "Identifying Mixtures of Mixtures Using Bayesian Estimation", "comments": "49 pages", "journal-ref": null, "doi": "10.1080/10618600.2016.1200472", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of a finite mixture of normal distributions in model-based clustering\nallows to capture non-Gaussian data clusters. However, identifying the clusters\nfrom the normal components is challenging and in general either achieved by\nimposing constraints on the model or by using post-processing procedures.\nWithin the Bayesian framework we propose a different approach based on sparse\nfinite mixtures to achieve identifiability. We specify a hierarchical prior\nwhere the hyperparameters are carefully selected such that they are reflective\nof the cluster structure aimed at. In addition this prior allows to estimate\nthe model using standard MCMC sampling methods. In combination with a\npost-processing approach which resolves the label switching issue and results\nin an identified model, our approach allows to simultaneously (1) determine the\nnumber of clusters, (2) flexibly approximate the cluster distributions in a\nsemi-parametric way using finite mixtures of normals and (3) identify\ncluster-specific parameters and classify observations. The proposed approach is\nillustrated in two simulation studies and on benchmark data sets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 14:44:01 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 14:39:08 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 12:17:48 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Malsiner-Walli", "Gertraud", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Gr\u00fcn", "Bettina", ""]]}, {"id": "1502.06481", "submitter": "Karthik Sriram", "authors": "Karthik Sriram", "title": "A Sandwich Likelihood Correction for Bayesian Quantile Regression based\n  on the Misspecified Asymmetric Laplace Density", "comments": null, "journal-ref": "Statistics and Probability Letters, volume 107 (2015), pg 18-26", "doi": "10.1016/j.spl.2015.07.035", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sandwich likelihood correction is proposed to remedy an inferential\nlimitation of the Bayesian quantile regression approach based on the\nmisspecified asymmetric Laplace density, by leveraging the benefits of the\napproach. Supporting theoretical results and simulations are presented.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 16:10:40 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 09:54:51 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Sriram", "Karthik", ""]]}, {"id": "1502.06557", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Iteratively reweighted adaptive lasso for conditional heteroscedastic\n  time series with applications to AR-ARCH type processes", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 100 (2016) 773-793", "doi": "10.1016/j.csda.2015.11.016", "report-no": null, "categories": "stat.ME q-fin.CP stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage algorithms are of great importance in almost every area of\nstatistics due to the increasing impact of big data. Especially time series\nanalysis benefits from efficient and rapid estimation techniques such as the\nlasso. However, currently lasso type estimators for autoregressive time series\nmodels still focus on models with homoscedastic residuals. Therefore, an\niteratively reweighted adaptive lasso algorithm for the estimation of time\nseries models under conditional heteroscedasticity is presented in a\nhigh-dimensional setting. The asymptotic behaviour of the resulting estimator\nis analysed. It is found that the proposed estimation procedure performs\nsubstantially better than its homoscedastic counterpart. A special case of the\nalgorithm is suitable to compute the estimated multivariate AR-ARCH type models\nefficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCH\nor ARMA-GARCH are discussed. Finally, different simulation results and\napplications to electricity market data and returns of metal prices are shown.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 19:14:39 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 23:05:43 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1502.06559", "submitter": "Diane  Donovan Dr", "authors": "Kevin Burrage, Pamela Burrage, Diane Donovan and Bevan Thompson", "title": "Populations of models, Experimental Designs and coverage of parameter\n  space by Latin Hypercube and Orthogonal Sampling", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have used simulations to make a conjecture about the\ncoverage of a $t$ dimensional subspace of a $d$ dimensional parameter space of\nsize $n$ when performing $k$ trials of Latin Hypercube sampling. This takes the\nform $P(k,n,d,t)=1-e^{-k/n^{t-1}}$. We suggest that this coverage formula is\nindependent of $d$ and this allows us to make connections between building\nPopulations of Models and Experimental Designs. We also show that Orthogonal\nsampling is superior to Latin Hypercube sampling in terms of allowing a more\nuniform coverage of the $t$ dimensional subspace at the sub-block size level.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 10:29:35 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Burrage", "Kevin", ""], ["Burrage", "Pamela", ""], ["Donovan", "Diane", ""], ["Thompson", "Bevan", ""]]}, {"id": "1502.06930", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla and James G. Scott", "title": "Tensor decomposition with generalized lasso penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for penalized tensor decomposition (PTD) that\nestimates smoothly varying latent factors in multi-way data. This generalizes\nexisting work on sparse tensor decomposition and penalized matrix\ndecompositions, in a manner parallel to the generalized lasso for regression\nand smoothing problems. Our approach presents many nontrivial challenges at the\nintersection of modeling and computation, which are studied in detail. An\nefficient coordinate-wise optimization algorithm for (PTD) is presented, and\nits convergence properties are characterized. The method is applied both to\nsimulated data and real data on flu hospitalizations in Texas. These results\nshow that our penalized tensor decomposition can offer major improvements on\nexisting methods for analyzing multi-way data that exhibit smooth spatial or\ntemporal features.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 20:03:05 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 05:19:53 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 01:40:38 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Scott", "James G.", ""]]}, {"id": "1502.06988", "submitter": "Adam Loy", "authors": "Adam Loy, Heike Hofmann, Dianne Cook", "title": "Model Choice and Diagnostics for Linear Mixed-Effects Models Using\n  Statistics on Street Corners", "comments": "52 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of linear mixed-effects (LME) models means that traditional\ndiagnostics are rendered less effective. This is due to a breakdown of\nasymptotic results, boundary issues, and visible patterns in residual plots\nthat are introduced by the model fitting process. Some of these issues are well\nknown and adjustments have been proposed. Working with LME models typically\nrequires that the analyst keeps track of all the special circumstances that may\narise. In this paper we illustrate a simpler but generally applicable approach\nto diagnosing LME models. We explain how to use new visual inference methods\nfor these purposes. The approach provides a unified framework for diagnosing\nLME fits and for model selection. We illustrate the use of this approach on\nseveral commonly available data sets. A large-scale Amazon Turk study was used\nto validate the methods. R code is provided for the analyses.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 22:05:01 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 19:18:07 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 22:17:24 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Loy", "Adam", ""], ["Hofmann", "Heike", ""], ["Cook", "Dianne", ""]]}, {"id": "1502.07113", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "{\\L}ukasz Kidzi\\'nski", "title": "Functional Time Series", "comments": "Article presented in the Bruxelless Summer School of Mathematics 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous advances in data collection and storage techniques allow us to\nobserve and record real-life processes in great detail. Examples include\nfinancial transaction data, fMRI images, satellite photos, earths pollution\ndistribution in time etc. Due to the high dimensionality of such data,\nclassical statistical tools become inadequate and inefficient. The need for new\nmethods emerges and one of the most prominent techniques in this context is\nfunctional data analysis (FDA).\n  The main objective of this article is to present techniques of the analysis\nof temporal dependence in FDA. Such dependence occurs, for example, if the data\nconsist of a continuous time process which has been cut into segments, days for\ninstance. We are then in the context of so-called functional time series.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 10:24:16 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Kidzi\u0144ski", "\u0141ukasz", ""]]}, {"id": "1502.07125", "submitter": "Danang Qoyyimi", "authors": "Danang Teguh Qoyyimi and Ricardas Zitikis", "title": "Measuring association via lack of co-monotonicity: the LOC index and a\n  problem of educational assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring association, or the lack of it, between variables plays an\nimportant role in a variety of research areas, including education, which is of\nour primary interest in this paper. Given, for example, student marks on\nseveral study subjects, we may for a number of reasons be interested in\nmeasuring the lack of co-monotonicity (LOC) between the marks, which rarely\nfollow monotone, let alone linear, patterns. For this purpose, in this paper we\nexplore a novel approach based on a LOC index, which is related to, yet\nsubstantially different from, Eckhard Liebscher's recently suggested\ncoefficient of monotonically increasing dependence. To illustrate the new\ntechnique, we analyze a data-set of student marks on mathematics, reading and\nspelling.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 11:07:35 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 23:31:55 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Qoyyimi", "Danang Teguh", ""], ["Zitikis", "Ricardas", ""]]}, {"id": "1502.07189", "submitter": "Rafal Kulik", "authors": "Rafa{\\l} Kulik and Zhigang Tong", "title": "Multivariate Tail Estimation: Conditioning on an extreme event", "comments": "8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regularly varying random vectors. Our goal is to estimate in a\nnon-parametric way some characteristics related to conditioning on an extreme\nevent, like the tail dependence coefficient. We introduce a quasi-spectral\ndecomposition that allow to improve efficiency of estimators. Asymptotic\nnormality of estimators is based on weak convergence of tail empirical\nprocesses. Theoretical results are supported by simulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 14:56:37 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Kulik", "Rafa\u0142", ""], ["Tong", "Zhigang", ""]]}, {"id": "1502.07246", "submitter": "Federico Bassetti", "authors": "Federico Bassetti, Roberto Casarin, Francesco Ravazzolo", "title": "Bayesian Nonparametric Calibration and Combination of Predictive\n  Distributions", "comments": "arXiv admin note: text overlap with arXiv:1305.2026 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian approach to predictive density calibration and\ncombination that accounts for parameter uncertainty and model set\nincompleteness through the use of random calibration functionals and random\ncombination weights. Building on the work of Ranjan, R. and Gneiting, T. (2010)\nand Gneiting, T. and Ranjan, R. (2013), we use infinite beta mixtures for the\ncalibration. The proposed Bayesian nonparametric approach takes advantage of\nthe flexibility of Dirichlet process mixtures to achieve any continuous\ndeformation of linearly combined predictive distributions. The inference\nprocedure is based on Gibbs sampling and allows accounting for uncertainty in\nthe number of mixture components, mixture weights, and calibration parameters.\nThe weak posterior consistency of the Bayesian nonparametric calibration is\nprovided under suitable conditions for unknown true density. We study the\nmethodology in simulation examples with fat tails and multimodal densities and\napply it to density forecasts of daily S&P returns and daily maximum wind speed\nat the Frankfurt airport.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 17:06:05 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 08:26:56 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Bassetti", "Federico", ""], ["Casarin", "Roberto", ""], ["Ravazzolo", "Francesco", ""]]}, {"id": "1502.07458", "submitter": "Edouard Ollier", "authors": "Edouard Ollier, Adeline Samson, Xavier Delavenne, Vivian Viallon", "title": "A SAEM Algorithm for Fused Lasso Penalized Non Linear Mixed Effect\n  Models: Application to Group Comparison in Pharmacokinetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non linear mixed effect models are classical tools to analyze non linear\nlongitudinal data in many fields such as population Pharmacokinetic. Groups of\nobservations are usually compared by introducing the group affiliations as\nbinary covariates with a reference group that is stated among the groups. This\napproach is relatively limited as it allows only the comparison of the\nreference group to the others. In this work, we propose to compare the groups\nusing a penalized likelihood approach. Groups are described by the same\nstructural model but with parameters that are group specific. The likelihood is\npenalized with a fused lasso penalty that induces sparsity on the differences\nbetween groups for both fixed effects and variances of random effects. A\npenalized Stochastic Approximation EM algorithm is proposed that is coupled to\nAlternating Direction Method Multipliers to solve the maximization step. An\nextensive simulation study illustrates the performance of this algorithm when\ncomparing more than two groups. Then the approach is applied to real data from\ntwo pharmacokinetic drug-drug interaction trials.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:50:40 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 09:39:18 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Ollier", "Edouard", ""], ["Samson", "Adeline", ""], ["Delavenne", "Xavier", ""], ["Viallon", "Vivian", ""]]}, {"id": "1502.07505", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A mixed effect model for bivariate meta-analysis of diagnostic test\n  accuracy studies using a copula representation of the random effects\n  distribution", "comments": null, "journal-ref": "Statistics in Medicine, 2015, 34(29):3842--3865", "doi": "10.1002/sim.6595", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic test accuracy studies typically report the number of true\npositives, false positives, true negatives and false negatives. There usually\nexists a negative association between the number of true positives and true\nnegatives, because studies that adopt less stringent criterion for declaring a\ntest positive invoke higher sensitivities and lower specificities. A\ngeneralized linear mixed model (GLMM) is currently recommended to synthesize\ndiagnostic test accuracy studies. We propose a copula mixed model for bivariate\nmeta-analysis of diagnostic test accuracy studies. Our general model includes\nthe GLMM as a special case and can also operate on the original scale of\nsensitivity and specificity. Summary receiver operating characteristic curves\nare deduced for the proposed model through quantile regression techniques and\ndifferent characterizations of the bivariate random effects distribution. Our\ngeneral methodology is demonstrated with an extensive simulation study and\nillustrated by re-analysing the data of two published meta-analyses. Our study\nsuggests that there can be an improvement on GLMM in fit to data and makes the\nargument for moving to copula random effects models. Our modelling framework is\nimplemented in the package CopulaREMADA within the open source statistical\nenvironment R.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 11:11:18 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1502.07603", "submitter": "Ashkan Ertefaie", "authors": "Ashkan Ertefaie, Dylan Small, James H. Flory and Sean Hennessy", "title": "Selection bias when using instrumental variable methods to compare two\n  treatments but more than two treatments are available", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable (IV) methods are widely used to adjust for the bias in\nestimating treatment effects caused by unmeasured confounders in observational\nstudies. In this manuscript, we provide empirical and theoretical evidence that\nthe IV methods may result in biased treatment effects if applied on a data set\nin which subjects are preselected based on their received treatments. We frame\nthis as a selection bias problem and propose a procedure that identifies the\ntreatment effect of interest as a function of a vector of sensitivity\nparameters. We also list assumptions under which analyzing the preselected data\ndoes not lead to a biased treatment effect estimate. The performance of the\nproposed method is examined using simulation studies. We applied our method on\nThe Health Improvement Network (THIN) database to estimate the comparative\neffect of metformin and sulfonylureas on weight gain among diabetic patients.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 15:49:35 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 16:22:16 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Ertefaie", "Ashkan", ""], ["Small", "Dylan", ""], ["Flory", "James H.", ""], ["Hennessy", "Sean", ""]]}, {"id": "1502.07638", "submitter": "Christian P. Robert", "authors": "Clara Grazian, Ilaria Masiani, and Christian P. Robert", "title": "A discussion of \"Bayesian model selection based on proper scoring rules\"\n  by A.P. Dawid and M. Musio", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is a discussion of the article \"Bayesian model selection based on\nproper scoring rules\" by A.P. Dawid and M. Musio, to appear in Bayesian\nAnalysis. While appreciating the concepts behind the use of proper scoring\nrules, including the inclusion of improper priors, we point out here some\npossible practical difficulties with the advocated approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 17:14:48 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Grazian", "Clara", ""], ["Masiani", "Ilaria", ""], ["Robert", "Christian P.", ""]]}, {"id": "1502.07685", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick", "title": "Covariance Matrices and Influence Scores for Mean Field Variational\n  Bayes", "comments": "28 pages, 5 figures, submitted to ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is that it underestimates the uncertainty of\nmodel variables (sometimes severely) and provides no information about model\nvariable covariance. We develop a fast, general methodology for exponential\nfamilies that augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We also show how LRVB can be\nused to quickly calculate a measure of the influence of individual data points\non parameter point estimates. We demonstrate the accuracy and scalability of\nour method by learning Gaussian mixture models for both simulated and real\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 19:08:39 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""]]}, {"id": "1502.07766", "submitter": "Tyrus Berry", "authors": "Tyrus Berry and John Harlim", "title": "Semiparametric forecasting and filtering: correcting low-dimensional\n  model error in parametric models", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2015.12.043", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semiparametric forecasting and filtering are introduced as a method of\naddressing model errors arising from unresolved physical phenomena. While\ntraditional parametric models are able to learn high-dimensional systems from\nsmall data sets, their rigid parametric structure makes them vulnerable to\nmodel error. On the other hand, nonparametric models have a very flexible\nstructure, but they suffer from the curse-of-dimensionality and are not\npractical for high-dimensional systems. The semiparametric approach loosens the\nstructure of a parametric model by fitting a data-driven nonparametric model\nfor the parameters. Given a parametric dynamical model and a noisy data set of\nhistorical observations, an adaptive Kalman filter is used to extract a\ntime-series of the parameter values. A nonparametric forecasting model for the\nparameters is built by projecting the discrete shift map onto a data-driven\nbasis of smooth functions. Existing techniques for filtering and forecasting\nalgorithms extend naturally to the semiparametric model which can effectively\ncompensate for model error, with forecasting skill approaching that of the\nperfect model. Semiparametric forecasting and filtering are a generalization of\nstatistical semiparametric models to time-dependent distributions evolving\nunder dynamical systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 01:02:24 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Berry", "Tyrus", ""], ["Harlim", "John", ""]]}, {"id": "1502.07831", "submitter": "Shaojun Guo", "authors": "Shaojun Guo, Yazhen Wang and Qiwei Yao", "title": "High Dimensional and Banded Vector Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of vector autoregressive models with banded coefficient\nmatrices. The setting represents a type of sparse structure for\nhigh-dimensional time series, though the implied autocovariance matrices are\nnot banded. The structure is also practically meaningful when the order of\ncomponent time series is arranged appropriately. The convergence rates for the\nestimated banded autoregressive coefficient matrices are established. We also\npropose a Bayesian information criterion for determining the width of the bands\nin the coefficient matrices, which is proved to be consistent. By exploring\nsome approximate banded structure for the auto-covariance functions of banded\nvector autoregressive processes, consistent estimators for the auto-covariance\nmatrices are constructed.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 09:00:36 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 15:00:51 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Guo", "Shaojun", ""], ["Wang", "Yazhen", ""], ["Yao", "Qiwei", ""]]}, {"id": "1502.07963", "submitter": "Dominik Rothenh\\\"ausler", "authors": "Dominik Rothenh\\\"ausler, Nicolai Meinshausen, Peter B\\\"uhlmann", "title": "Confidence Intervals for Maximin Effects in Inhomogeneous Large-Scale\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One challenge of large-scale data analysis is that the assumption of an\nidentical distribution for all samples is often not realistic. An optimal\nlinear regression might, for example, be markedly different for distinct groups\nof the data. Maximin effects have been proposed as a computationally attractive\nway to estimate effects that are common across all data without fitting a\nmixture distribution explicitly. So far just point estimators of the common\nmaximin effects have been proposed in Meinshausen and B\\\"uhlmann (2014). Here\nwe propose asymptotically valid confidence regions for these effects.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 16:42:07 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Rothenh\u00e4usler", "Dominik", ""], ["Meinshausen", "Nicolai", ""], ["B\u00fchlmann", "Peter", ""]]}]