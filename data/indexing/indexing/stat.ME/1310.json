[{"id": "1310.0039", "submitter": "Luis Pericchi", "authors": "Luis Pericchi and Carlos Pereira", "title": "Changing the paradigm of fixed significance levels: Testing Hypothesis\n  by Minimizing Sum of Errors Type I and Type II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our purpose, is to put forward a change in the paradigm of testing by\ngeneralizing a very natural idea exposed by Morris DeGroot (1975) aiming to an\napproach that is attractive to all schools of statistics, in a procedure better\nsuited for the needs of science. DeGroot's seminal idea is to base testing\nstatistical hypothesis on minimizing the weighted sum of type I plus type II\nerror instead of of the prevailing paradigm which is fixing type I error and\nminimizing type II error. DeGroot's result is that in simple vs simple\nhypothesis the optimal criterion is to reject, according to the likelihood\nratio as the evidence (ordering) statistics using a fixed threshold value,\ninstead of a fixed tail probability. By defining expected type I and type II\nerrors, we generalize DeGroot's approach and find that the optimal region is\ndefined by the ratio of evidences, that is, averaged likelihoods (with respect\nto a prior measure) and a threshold fixed. This approach yields an optimal\ntheory in complete generality, which the Classical Theory of Testing does not.\nThis can be seen as a Bayes-Non-Bayes compromise: the criteria (weighted sum of\ntype I and type II errors) is Frequentist, but the test criterion is the ratio\nof marginalized likelihood, which is Bayesian. We give arguments, to push the\ntheory still further, so that the weighting measures (priors)of the likelihoods\ndoes not have to be proper and highly informative, but just predictively\nmatched, that is that predictively matched priors, give rise to the same\nevidence (marginal likelihoods) using minimal (smallest) training samples. The\ntheory that emerges, similar to the theories based on Objective Bayes\napproaches, is a powerful response to criticisms of the prevailing approach of\nhypothesis testing, see for example Ioannidis (2005) and Siegfried (2010) among\nmany others.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 20:14:10 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Pericchi", "Luis", ""], ["Pereira", "Carlos", ""]]}, {"id": "1310.0150", "submitter": "Bin Yu", "authors": "Bin Yu", "title": "Stability", "comments": "Published in at http://dx.doi.org/10.3150/13-BEJSP14 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1484-1500", "doi": "10.3150/13-BEJSP14", "report-no": "IMS-BEJ-BEJSP14", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility is imperative for any scientific discovery. More often than\nnot, modern scientific findings rely on statistical analysis of\nhigh-dimensional data. At a minimum, reproducibility manifests itself in\nstability of statistical results relative to \"reasonable\" perturbations to data\nand to the model used. Jacknife, bootstrap, and cross-validation are based on\nperturbations to data, while robust statistics methods deal with perturbations\nto models. In this article, a case is made for the importance of stability in\nstatistics. Firstly, we motivate the necessity of stability for interpretable\nand reliable encoding models from brain fMRI signals. Secondly, we find strong\nevidence in the literature to demonstrate the central role of stability in\nstatistical inference, such as sensitivity analysis and effect detection.\nThirdly, a smoothing parameter selector based on estimation stability (ES),\nES-CV, is proposed for Lasso, in order to bring stability to bear on\ncross-validation (CV). ES-CV is then utilized in the encoding models to reduce\nthe number of predictors by 60% with almost no loss (1.3%) of prediction\nperformance across over 2,000 voxels. Last, a novel \"stability\" argument is\nseen to drive new results that shed light on the intriguing interactions\nbetween sample to sample variability and heavier tail error distribution (e.g.,\ndouble-exponential) in high-dimensional regression models with $p$ predictors\nand $n$ independent samples. In particular, when\n$p/n\\rightarrow\\kappa\\in(0.3,1)$ and the error distribution is\ndouble-exponential, the Ordinary Least Squares (OLS) is a better estimator than\nthe Least Absolute Deviation (LAD) estimator.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 05:54:35 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Yu", "Bin", ""]]}, {"id": "1310.0173", "submitter": "Stephen M. Stigler", "authors": "Stephen M. Stigler", "title": "The True Title of Bayes's Essay", "comments": "Published in at http://dx.doi.org/10.1214/13-STS438 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 3, 283-288", "doi": "10.1214/13-STS438", "report-no": "IMS-STS-STS438", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New evidence is presented that Richard Price gave Thomas Bayes's famous essay\na very different title from the commonly reported one. It is argued that this\nimplies Price almost surely and Bayes not improbably embarked upon this work\nseeking a defensive tool to combat David Hume on an issue in theology.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 07:53:10 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Stigler", "Stephen M.", ""]]}, {"id": "1310.0188", "submitter": "Hau-tieng Wu", "authors": "Noureddine El Karoui, Hau-tieng Wu", "title": "Graph connection Laplacian and random matrices with random blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.SP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph connection Laplacian (GCL) is a modern data analysis technique that is\nstarting to be applied for the analysis of high dimensional and massive\ndatasets. Motivated by this technique, we study matrices that are akin to the\nones appearing in the null case of GCL, i.e the case where there is no\nstructure in the dataset under investigation. Developing this understanding is\nimportant in making sense of the output of the algorithms based on GCL. We\nhence develop a theory explaining the behavior of the spectral distribution of\na large class of random matrices, in particular random matrices with random\nblock entries of fixed size. Part of the theory covers the case where there is\nsignificant dependence between the blocks. Numerical work shows that the\nagreement between our theoretical predictions and numerical simulations is\ngenerally very good.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 08:38:13 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2014 00:27:44 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Karoui", "Noureddine El", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1310.0236", "submitter": "Thordis Thorarinsdottir", "authors": "Thordis L. Thorarinsdottir, Michael Scheuerer and Christopher Heinz", "title": "Assessing the calibration of high-dimensional ensemble forecasts using\n  rank histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any decision making process that relies on a probabilistic forecast of future\nevents necessarily requires a calibrated forecast. This paper proposes new\nmethods for empirically assessing forecast calibration in a multivariate\nsetting where the probabilistic forecast is given by an ensemble of equally\nprobable forecast scenarios. Multivariate properties are mapped to a single\ndimension through a pre-rank function and the calibration is subsequently\nassessed visually through a histogram of the ranks of the observation's\npre-ranks. Average ranking assigns a pre-rank based on the average univariate\nrank while band depth ranking employs the concept of functional band depth\nwhere the centrality of the observation within the forecast ensemble is\nassessed. Several simulation examples and a case study of temperature forecast\ntrajectories at Berlin Tegel Airport in Germany demonstrate that both\nmultivariate ranking methods can successfully detect various sources of\nmiscalibration and scale efficiently to high dimensional settings.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 10:56:53 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 20:59:22 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Thorarinsdottir", "Thordis L.", ""], ["Scheuerer", "Michael", ""], ["Heinz", "Christopher", ""]]}, {"id": "1310.0260", "submitter": "Ernesto Barrios", "authors": "Ernesto Barrios, Antonio Lijoi, Luis E. Nieto-Barajas, Igor Pr\\\"unster", "title": "Modeling with Normalized Random Measure Mixture Models", "comments": "Published in at http://dx.doi.org/10.1214/13-STS416 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 3, 313-334", "doi": "10.1214/13-STS416", "report-no": "IMS-STS-STS416", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet process mixture model and more general mixtures based on\ndiscrete random probability measures have been shown to be flexible and\naccurate models for density estimation and clustering. The goal of this paper\nis to illustrate the use of normalized random measures as mixing measures in\nnonparametric hierarchical mixture models and point out how possible\ncomputational issues can be successfully addressed. To this end, we first\nprovide a concise and accessible introduction to normalized random measures\nwith independent increments. Then, we explain in detail a particular way of\nsampling from the posterior using the Ferguson-Klass representation. We develop\na thorough comparative analysis for location-scale mixtures that considers a\nset of alternatives for the mixture kernel and for the nonparametric component.\nSimulation results indicate that normalized random measure mixtures potentially\nrepresent a valid default choice for density estimation problems. As a\nbyproduct of this study an R package to fit these models was produced and is\navailable in the Comprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 11:57:00 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Barrios", "Ernesto", ""], ["Lijoi", "Antonio", ""], ["Nieto-Barajas", "Luis E.", ""], ["Pr\u00fcnster", "Igor", ""]]}, {"id": "1310.0275", "submitter": "Daniel Yekutieli Dr.", "authors": "Daniel Yekutieli", "title": "Optimal exact tests for composite alternative hypotheses on cross\n  tabulated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methodology for constructing exact significance tests for cross\ntabulated data for \"difficult\" composite alternative hypotheses that have no\nnatural test statistic. We construct a test for discovering Simpson's Paradox\nand a general test for discovering positive dependence between two ordinal\nvariables. Our tests are Bayesian extensions of the likelihood ratio test, they\nare optimal with respect to the prior distribution, and are also closely\nrelated to Bayes factors and Bayesian FDR controlling testing procedures.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 13:09:17 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2013 10:50:48 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Yekutieli", "Daniel", ""]]}, {"id": "1310.0333", "submitter": "Nikolai Leonenko", "authors": "Danijel Grahovac, Mofei Jia, Nikolai N. Leonenko, Emanuele Taufer", "title": "Asymptotic Properties of the Partition Function and Applications in Tail\n  Index Inference of Heavy-Tailed Data", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The so-called partition function is a sample moment statistic based on blocks\nof data and it is often used in the context of multifractal processes.\n  It will be shown that its behaviour is strongly influenced by the tail of the\ndistribution underlying the data either in i.i.d. and weakly dependent cases.\n  These results will be exploited to develop graphical and estimation methods\nfor the tail index of a distribution. The performance of the tools proposed is\nanalyzed and compared with other methods by means of simulations and examples.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 14:58:24 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Grahovac", "Danijel", ""], ["Jia", "Mofei", ""], ["Leonenko", "Nikolai N.", ""], ["Taufer", "Emanuele", ""]]}, {"id": "1310.0364", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "Segregation Indices for Disease Clustering", "comments": "31 pages, 13 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-13-1", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial clustering has important implications in various fields. In\nparticular, disease clustering is of major public concern in epidemiology. In\nthis article, we propose the use of two distance-based segregation indices to\ntest the significance of disease clustering among subjects whose locations are\nfrom a homogeneous or an inhomogeneous population. We derive their asymptotic\ndistributions and compare them with other distance-based disease clustering\ntests in terms of empirical size and power by extensive Monte Carlo\nsimulations. The null pattern we consider is the random labeling (RL) of cases\nand controls to the given locations. Along this line, we investigate the\nsensitivity of the size of these tests to the underlying background pattern\n(e.g., clustered or homogenous) on which the RL is applied, the level of\nclustering and number of clusters, or differences in relative abundances of the\nclasses. We demonstrate that differences in relative abundance has the highest\nimpact on the empirical sizes of the tests. We also propose various non-RL\npatterns as alternatives to the RL pattern and assess the empirical power\nperformance of the tests under these alternatives. We illustrate the methods on\ntwo real-life examples from epidemiology.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 16:05:20 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2013 18:56:07 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "1310.0376", "submitter": "Nicolas Dobigeon", "authors": "Olivier Besson and Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Joint Bayesian estimation of close subspaces from noisy measurements", "comments": "Submitted for publication in IEEE Signal Process. Letters", "journal-ref": null, "doi": "10.1109/LSP.2013.2296138", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we consider two sets of observations defined as subspace\nsignals embedded in noise and we wish to analyze the distance between these two\nsubspaces. The latter entails evaluating the angles between the subspaces, an\nissue reminiscent of the well-known Procrustes problem. A Bayesian approach is\ninvestigated where the subspaces of interest are considered as random with a\njoint prior distribution (namely a Bingham distribution), which allows the\ncloseness of the two subspaces to be adjusted. Within this framework, the\nminimum mean-square distance estimator of both subspaces is formulated and\nimplemented via a Gibbs sampler. A simpler scheme based on alternative maximum\na posteriori estimation is also presented. The new schemes are shown to provide\nmore accurate estimates of the angles between the subspaces, compared to\nsingular value decomposition based independent estimation of the two subspaces.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 16:28:59 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Besson", "Olivier", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1310.0379", "submitter": "Hui Jiang", "authors": "Hui Jiang, Julia Salzman", "title": "A penalized likelihood approach for robust estimation of isoform\n  expression", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra high-throughput sequencing of transcriptomes (RNA-Seq) has enabled the\naccurate estimation of gene expression at individual isoform level. However,\nsystematic biases introduced during the sequencing and mapping processes as\nwell as incompleteness of the transcript annotation databases may cause the\nestimates of isoform abundances to be unreliable, and in some cases, highly\ninaccurate. This paper introduces a penalized likelihood approach to detect and\ncorrect for such biases in a robust manner. Our model extends those previously\nproposed by introducing bias parameters for reads. An L1 penalty is used for\nthe selection of non-zero bias parameters. We introduce an efficient algorithm\nfor model fitting and analyze the statistical properties of the proposed model.\nOur experimental studies on both simulated and real datasets suggest that the\nmodel has the potential to improve isoform-specific gene expression estimates\nand identify incompletely annotated gene models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 16:33:38 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Jiang", "Hui", ""], ["Salzman", "Julia", ""]]}, {"id": "1310.0595", "submitter": "Stefano Favaro", "authors": "Stefano Favaro, Yee Whye Teh", "title": "MCMC for Normalized Random Measure Mixture Models", "comments": "Published in at http://dx.doi.org/10.1214/13-STS422 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 3, 335-359", "doi": "10.1214/13-STS422", "report-no": "IMS-STS-STS422", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the use of Markov chain Monte Carlo methods for posterior\nsampling in Bayesian nonparametric mixture models with normalized random\nmeasure priors. Making use of some recent posterior characterizations for the\nclass of normalized random measures, we propose novel Markov chain Monte Carlo\nmethods of both marginal type and conditional type. The proposed marginal\nsamplers are generalizations of Neal's well-regarded Algorithm 8 for Dirichlet\nprocess mixture models, whereas the conditional sampler is a variation of those\nrecently introduced in the literature. For both the marginal and conditional\nmethods, we consider as a running example a mixture model with an underlying\nnormalized generalized Gamma process prior, and describe comparative simulation\nresults demonstrating the efficacies of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 07:03:54 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Favaro", "Stefano", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1310.0606", "submitter": "Ruth Heller", "authors": "Ruth Heller, Marina Bogomolov, and Yoav Benjamini", "title": "Deciding whether follow-up studies have replicated findings in a\n  preliminary large-scale \"omics' study\"", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences of the United\n  States of America (PNAS), 2014 vol. 111 no. 46, 16262-16267", "doi": "10.1073/pnas.1314814111", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formal method to declare that findings from a primary study have\nbeen replicated in a follow-up study. Our proposal is appropriate for primary\nstudies that involve large-scale searches for rare true positives (i.e. needles\nin a haystack). Our proposal assigns an $r$-value to each finding; this is the\nlowest false discovery rate at which the finding can be called replicated.\nExamples are given and software is available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 07:26:26 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 10:09:26 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Heller", "Ruth", ""], ["Bogomolov", "Marina", ""], ["Benjamini", "Yoav", ""]]}, {"id": "1310.0628", "submitter": "Anne M. Presanis", "authors": "Anne M. Presanis, David Ohlssen, David J. Spiegelhalter, Daniela De\n  Angelis", "title": "Conflict Diagnostics in Directed Acyclic Graphs, with Applications in\n  Bayesian Evidence Synthesis", "comments": "Published in at http://dx.doi.org/10.1214/13-STS426 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 3, 376-397", "doi": "10.1214/13-STS426", "report-no": "IMS-STS-STS426", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex stochastic models represented by directed acyclic graphs (DAGs) are\nincreasingly employed to synthesise multiple, imperfect and disparate sources\nof evidence, to estimate quantities that are difficult to measure directly. The\nvarious data sources are dependent on shared parameters and hence have the\npotential to conflict with each other, as well as with the model. In a Bayesian\nframework, the model consists of three components: the prior distribution, the\nassumed form of the likelihood and structural assumptions. Any of these\ncomponents may be incompatible with the observed data. The detection and\nquantification of such conflict and of data sources that are inconsistent with\neach other is therefore a crucial component of the model criticism process. We\nfirst review Bayesian model criticism, with a focus on conflict detection,\nbefore describing a general diagnostic for detecting and quantifying conflict\nbetween the evidence in different partitions of a DAG. The diagnostic is a\np-value based on splitting the information contributing to inference about a\n\"separator\" node or group of nodes into two independent groups and testing\nwhether the two groups result in the same inference about the separator\nnode(s). We illustrate the method with three comprehensive examples: an\nevidence synthesis to estimate HIV prevalence; an evidence synthesis to\nestimate influenza case-severity; and a hierarchical growth model for rat\nweights.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 09:21:28 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Presanis", "Anne M.", ""], ["Ohlssen", "David", ""], ["Spiegelhalter", "David J.", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1310.0661", "submitter": "Guido Consonni", "authors": "Guido Consonni, Jonathan J. Forster, Luca La Rocca", "title": "The Whetstone and the Alum Block: Balanced Objective Bayesian Comparison\n  of Nested Models for Discrete Data", "comments": "Published in at http://dx.doi.org/10.1214/13-STS433 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 3, 398-423", "doi": "10.1214/13-STS433", "report-no": "IMS-STS-STS433", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When two nested models are compared, using a Bayes factor, from an objective\nstandpoint, two seemingly conflicting issues emerge at the time of choosing\nparameter priors under the two models. On the one hand, for moderate sample\nsizes, the evidence in favor of the smaller model can be inflated by\ndiffuseness of the prior under the larger model. On the other hand,\nasymptotically, the evidence in favor of the smaller model typically\naccumulates at a slower rate. With reference to finitely discrete data models,\nwe show that these two issues can be dealt with jointly, by combining intrinsic\npriors and nonlocal priors in a new unified class of priors. We illustrate our\nideas in a running Bernoulli example, then we apply them to test the equality\nof two proportions, and finally we deal with the more general case of logistic\nregression models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 11:07:59 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Consonni", "Guido", ""], ["Forster", "Jonathan J.", ""], ["La Rocca", "Luca", ""]]}, {"id": "1310.0732", "submitter": "Victor Picheny", "authors": "Victor Picheny", "title": "Multiobjective optimization using Gaussian process emulators via\n  stepwise uncertainty reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of expensive computer models with the help of Gaussian process\nemulators in now commonplace. However, when several (competing) objectives are\nconsidered, choosing an appropriate sampling strategy remains an open question.\nWe present here a new algorithm based on stepwise uncertainty reduction\nprinciples to address this issue. Optimization is seen as a sequential\nreduction of the volume of the excursion sets below the current best solutions,\nand our sampling strategy chooses the points that give the highest expected\nreduction. Closed-form formulae are provided to compute the sampling criterion,\navoiding the use of cumbersome simulations. We test our method on numerical\nexamples, showing that it provides an efficient trade-off between exploration\nand intensification.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 15:06:19 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Picheny", "Victor", ""]]}, {"id": "1310.0740", "submitter": "Maurizio Filippone", "authors": "Maurizio Filippone and Mark Girolami", "title": "Pseudo-Marginal Bayesian Inference for Gaussian Processes", "comments": "14 pages double column", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenges that arise when adopting Gaussian Process priors in\nprobabilistic modeling are how to carry out exact Bayesian inference and how to\naccount for uncertainty on model parameters when making model-based predictions\non out-of-sample data. Using probit regression as an illustrative working\nexample, this paper presents a general and effective methodology based on the\npseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses\nboth of these issues. The results presented in this paper show improvements\nover existing sampling methods to simulate from the posterior distribution over\nthe parameters defining the covariance function of the Gaussian Process prior.\nThis is particularly important as it offers a powerful tool to carry out full\nBayesian inference of Gaussian Process based hierarchic statistical models in\ngeneral. The results also demonstrate that Monte Carlo based integration of all\nmodel parameters is actually feasible in this class of models providing a\nsuperior quantification of uncertainty in predictions. Extensive comparisons\nwith respect to state-of-the-art probabilistic classifiers confirm this\nassertion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 15:29:28 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 10:41:39 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 08:53:34 GMT"}, {"version": "v4", "created": "Mon, 7 Apr 2014 09:42:58 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Filippone", "Maurizio", ""], ["Girolami", "Mark", ""]]}, {"id": "1310.0847", "submitter": "Ivan Medovikov", "authors": "Ivan Medovikov", "title": "Non-Parametric Weighted Tests for Independence Based on Empirical Copula\n  Process", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of flexible non-parametric tests for the presence of\ndependence between components of a random vector based on weighted\nCram\\'{e}r-von Mises functionals of the empirical copula process. The weights\nact as a tuning parameter and are shown to significantly influence the power of\nthe test, making it more sensitive to different types of dependence. Asymptotic\nproperties of the test are stated in the general case, for an arbitrary bounded\nand integrable weighting function, and computational formulas for a number of\nweighted statistics are provided. Several issues relating to the choice of the\nweights are discussed, and a simulation study is conducted to investigate the\npower of the test under a variety of dependence alternatives. The greatest gain\nin power is found to occur when weights are set proportional to true deviations\nfrom independence copula.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 21:34:36 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 21:30:12 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Medovikov", "Ivan", ""]]}, {"id": "1310.1034", "submitter": "Jukka Kohonen", "authors": "Jukka Kohonen and Jukka Corander", "title": "Computing Exact Clustering Posteriors with Subset Convolution", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exponential-time exact algorithm is provided for the task of clustering n\nitems of data into k clusters. Instead of seeking one partition, posterior\nprobabilities are computed for summary statistics: the number of clusters, and\npairwise co-occurrence. The method is based on subset convolution, and yields\nthe posterior distribution for the number of clusters in O(n * 3^n) operations,\nor O(n^3 * 2^n) using fast subset convolution. Pairwise co-occurrence\nprobabilities are then obtained in O(n^3 * 2^n) operations. This is\nconsiderably faster than exhaustive enumeration of all partitions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 17:01:34 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Kohonen", "Jukka", ""], ["Corander", "Jukka", ""]]}, {"id": "1310.1068", "submitter": "Matthias Steinr\\\"{u}cken", "authors": "Matthias Steinr\\\"ucken, Anand Bhaskar, Yun S. Song", "title": "A novel spectral method for inferring general diploid selection from\n  time series genetic data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS764 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2203-2222", "doi": "10.1214/14-AOAS764", "report-no": "IMS-AOAS-AOAS764", "categories": "q-bio.PE math.FA stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of time series genetic variation data from\nexperimental evolution studies and ancient DNA samples has created new\nopportunities to identify genomic regions under selective pressure and to\nestimate their associated fitness parameters. However, it is a challenging\nproblem to compute the likelihood of nonneutral models for the population\nallele frequency dynamics, given the observed temporal DNA data. Here, we\ndevelop a novel spectral algorithm to analytically and efficiently integrate\nover all possible frequency trajectories between consecutive time points. This\nadvance circumvents the limitations of existing methods which require\nfine-tuning the discretization of the population allele frequency space when\nnumerically approximating requisite integrals. Furthermore, our method is\nflexible enough to handle general diploid models of selection where the\nheterozygote and homozygote fitness parameters can take any values, while\nprevious methods focused on only a few restricted models of selection. We\ndemonstrate the utility of our method on simulated data and also apply it to\nanalyze ancient DNA data from genetic loci associated with coat coloration in\nhorses. In contrast to previous studies, our exploration of the full fitness\nparameter space reveals that a heterozygote advantage form of balancing\nselection may have been acting on these loci.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 19:02:12 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 11:55:42 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Steinr\u00fccken", "Matthias", ""], ["Bhaskar", "Anand", ""], ["Song", "Yun S.", ""]]}, {"id": "1310.1076", "submitter": "Ping Li", "authors": "Ping Li, Cun-Hui Zhang, Tong Zhang", "title": "Compressed Counting Meets Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (sparse signal recovery) has been a popular and important\nresearch topic in recent years. By observing that natural signals are often\nnonnegative, we propose a new framework for nonnegative signal recovery using\nCompressed Counting (CC). CC is a technique built on maximally-skewed p-stable\nrandom projections originally developed for data stream computations. Our\nrecovery procedure is computationally very efficient in that it requires only\none linear scan of the coordinates. Our analysis demonstrates that, when\n0<p<=0.5, it suffices to use M= O(C/eps^p log N) measurements so that all\ncoordinates will be recovered within eps additive precision, in one scan of the\ncoordinates. The constant C=1 when p->0 and C=pi/2 when p=0.5. In particular,\nwhen p->0 the required number of measurements is essentially M=K\\log N, where K\nis the number of nonzero coordinates of the signal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 19:48:44 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""], ["Zhang", "Tong", ""]]}, {"id": "1310.1127", "submitter": "Veerabhadran Baladandayuthapani", "authors": "Rajesh Talluri, Veerabhadran Baladandayuthapani and Bani K. Mallick", "title": "Bayesian sparse graphical models and their mixtures using lasso\n  selection priors", "comments": "under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Bayesian methods for Gaussian graphical models that lead to sparse\nand adaptively shrunk estimators of the precision (inverse covariance) matrix.\nOur methods are based on lasso-type regularization priors leading to\nparsimonious parameterization of the precision matrix, which is essential in\nseveral applications involving learning relationships among the variables. In\nthis context, we introduce a novel type of selection prior that develops a\nsparse structure on the precision matrix by making most of the elements exactly\nzero, in addition to ensuring positive definiteness -- thus conducting model\nselection and estimation simultaneously. We extend these methods to finite and\ninfinite mixtures of Gaussian graphical models for clustered data using\nDirichlet process priors. We discuss appropriate posterior simulation schemes\nto implement posterior inference in the proposed models, including the\nevaluation of normalizing constants that are functions of parameters of\ninterest which result from the restrictions on the correlation matrix. We\nevaluate the operating characteristics of our method via several simulations\nand in application to real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 22:01:11 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Talluri", "Rajesh", ""], ["Baladandayuthapani", "Veerabhadran", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1310.1183", "submitter": "Linglong Kong", "authors": "Hongtu Zhu, Jianqing Fan, Linglong Kong", "title": "Spatially Varying Coefficient Model for Neuroimaging Data with Jump\n  Discontinuities", "comments": null, "journal-ref": "Journal of the American Statistical Association Volume 109, Issue\n  507, 2014", "doi": "10.1080/01621459.2014.881742", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent work on studying massive imaging data in various\nneuroimaging studies, we propose a novel spatially varying coefficient model\n(SVCM) to spatially model the varying association between imaging measures in a\nthree-dimensional (3D) volume (or 2D surface) with a set of covariates. Two key\nfeatures of most neuorimaging data are the presence of multiple piecewise\nsmooth regions with unknown edges and jumps and substantial spatial\ncorrelations. To specifically account for these two features, SVCM includes a\nmeasurement model with multiple varying coefficient functions, a jumping\nsurface model for each varying coefficient function, and a functional principal\ncomponent model. We develop a three-stage estimation procedure to\nsimultaneously estimate the varying coefficient functions and the spatial\ncorrelations. The estimation procedure includes a fast multiscale adaptive\nestimation and testing procedure to independently estimate each varying\ncoefficient function, while preserving its edges among different\npiecewise-smooth regions. We systematically investigate the asymptotic\nproperties (e.g., consistency and asymptotic normality) of the multiscale\nadaptive parameter estimates. We also establish the uniform convergence rate of\nthe estimated spatial covariance function and its associated eigenvalue and\neigenfunctions. Our Monte Carlo simulation and real data analysis have\nconfirmed the excellent performance of SVCM.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 06:50:37 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 20:22:39 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Zhu", "Hongtu", ""], ["Fan", "Jianqing", ""], ["Kong", "Linglong", ""]]}, {"id": "1310.1282", "submitter": "Kukatharmini Tharmaratnam", "authors": "Linn Cecilie Bergersen, Kukatharmini Tharmaratnam and Ingrid K. Glad", "title": "Monotone Splines Lasso", "comments": "19 pages, 4 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of variable selection and estimation in\nnonparametric additive regression models for high-dimensional data. In recent\nyears, several methods have been proposed to model nonlinear relationships when\nthe number of covariates exceeds the number of observations by using spline\nbasis functions and group penalties. Nonlinear {\\it monotone} effects on the\nresponse play a central role in many situations, in particular in medicine and\nbiology. We construct the monotone splines lasso (MS-lasso) to select variables\nand estimate effects using monotone spline (I-splines). The additive components\nin the model are represented by their I-spline basis function expansion and the\ncomponent selection becomes that of selecting the groups of coefficients in the\nI-spline basis function expansion. We use a recent procedure, called\ncooperative lasso, to select sign-coherent groups, that is selecting the groups\nwith either exclusively non-negative or non-positive coefficients. This leads\nto the selection of important covariates that have nonlinear monotone\nincreasing or decreasing effect on the response. We also introduce an adaptive\nversion of the MS-lasso which reduces both the bias and the number of false\npositive selections considerably. We compare the MS-lasso and the adaptive\nMS-lasso with other existing methods for variable selection in high dimensions\nby simulation and illustrate the method on two relevant genomic data sets.\nResults indicate that the (adaptive) MS-lasso has excellent properties compared\nto the other methods both by means of estimation and selection, and can be\nrecommended for high-dimensional monotone regression.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 14:13:48 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Bergersen", "Linn Cecilie", ""], ["Tharmaratnam", "Kukatharmini", ""], ["Glad", "Ingrid K.", ""]]}, {"id": "1310.1404", "submitter": "Luke Bornn", "authors": "Michael Cherkassky and Luke Bornn", "title": "Sequential Monte Carlo Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a flexible and efficient framework for handling\nmulti-armed bandits, combining sequential Monte Carlo algorithms with\nhierarchical Bayesian modeling techniques. The framework naturally encompasses\nrestless bandits, contextual bandits, and other bandit variants under a single\ninferential model. Despite the model's generality, we propose efficient Monte\nCarlo algorithms to make inference scalable, based on recent developments in\nsequential Monte Carlo methods. Through two simulation studies, the framework\nis shown to outperform other empirical methods, while also naturally scaling to\nmore complex problems for which existing approaches can not cope. Additionally,\nwe successfully apply our framework to online video-based advertising\nrecommendation, and show its increased efficacy as compared to current state of\nthe art bandit algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 20:19:56 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Cherkassky", "Michael", ""], ["Bornn", "Luke", ""]]}, {"id": "1310.1533", "submitter": "Peter B\\\"{u}hlmann", "authors": "Peter B\\\"uhlmann, Jonas Peters, Jan Ernest", "title": "CAM: Causal additive models, high-dimensional order search and penalized\n  regression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1260 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2526-2556", "doi": "10.1214/14-AOS1260", "report-no": "IMS-AOS-AOS1260", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 03:12:34 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 12:31:45 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["Peters", "Jonas", ""], ["Ernest", "Jan", ""]]}, {"id": "1310.1800", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Generalized Negative Binomial Processes and the Representation of\n  Cluster Structures", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces the concept of a cluster structure to define a joint\ndistribution of the sample size and its exchangeable random partitions. The\ncluster structure allows the probability distribution of the random partitions\nof a subset of the sample to be dependent on the sample size, a feature not\npresented in a partition structure. A generalized negative binomial process\ncount-mixture model is proposed to generate a cluster structure, where in the\nprior the number of clusters is finite and Poisson distributed and the cluster\nsizes follow a truncated negative binomial distribution. The number and sizes\nof clusters can be controlled to exhibit distinct asymptotic behaviors. Unique\nmodel properties are illustrated with example clustering results using a\ngeneralized Polya urn sampling scheme. The paper provides new methods to\ngenerate exchangeable random partitions and to control both the cluster-number\nand cluster-size distributions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 14:35:45 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1310.1878", "submitter": "Dimitrios V. Vougas Dr", "authors": "Dimitrios V. Vougas", "title": "Estimation for Unit Root Testing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit estimation and computation of the Dickey Fuller (DF) and DF-type\ntests. Firstly, we show that the usual one step approach, based on the \"DF\nautoregression\", is likely to be subject to misspecification. Secondly, we\nclarify a neglected two step approach for estimation of the DF test. (In fact,\nwe introduce a new two step DF autoregression.) This method is always correctly\nspecified and efficient under the circumstances. However, it is either\nneglected or misused in unit root testing literature. The commonly employed\nhybrid of the (correct) two step method is shown to be inefficient, even\nasymptotically. Finally, we further improve/robustify the proposed two step\nmethod by employing the missing initial observations. Our finally proposed\nmethod is to be used in unit root testing, since it is a new DF autoregression\nthat retains the missing observations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 18:20:42 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2013 17:42:31 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Vougas", "Dimitrios V.", ""]]}, {"id": "1310.1969", "submitter": "Emmanuel Candes", "authors": "Malgorzata Bogdan, Ewout van den Berg, Weijie Su and Emmanuel Candes", "title": "Statistical estimation and testing via the sorted L1 norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method for sparse regression and variable selection,\nwhich is inspired by modern ideas in multiple testing. Imagine we have\nobservations from the linear model y = X beta + z, then we suggest estimating\nthe regression coefficients by means of a new estimator called SLOPE, which is\nthe solution to minimize 0.5 ||y - Xb\\|_2^2 + lambda_1 |b|_(1) + lambda_2\n|b|_(2) + ... + lambda_p |b|_(p); here, lambda_1 >= \\lambda_2 >= ... >=\n\\lambda_p >= 0 and |b|_(1) >= |b|_(2) >= ... >= |b|_(p) is the order statistic\nof the magnitudes of b. The regularizer is a sorted L1 norm which penalizes the\nregression coefficients according to their rank: the higher the rank, the\nlarger the penalty. This is similar to the famous BHq procedure [Benjamini and\nHochberg, 1995], which compares the value of a test statistic taken from a\nfamily to a critical threshold that depends on its rank in the family. SLOPE is\na convex program and we demonstrate an efficient algorithm for computing the\nsolution. We prove that for orthogonal designs with p variables, taking\nlambda_i = F^{-1}(1-q_i) (F is the cdf of the errors), q_i = iq/(2p), controls\nthe false discovery rate (FDR) for variable selection. When the design matrix\nis nonorthogonal there are inherent limitations on the FDR level and the power\nwhich can be obtained with model selection methods based on L1-like penalties.\nHowever, whenever the columns of the design matrix are not strongly correlated,\nwe demonstrate empirically that it is possible to select the parameters\nlambda_i as to obtain FDR control at a reasonable level as long as the number\nof nonzero coefficients is not too large. At the same time, the procedure\nexhibits increased power over the lasso, which treats all coefficients equally.\nThe paper illustrates further estimation properties of the new selection rule\nthrough comprehensive simulation studies.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 22:31:20 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2013 18:36:35 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Bogdan", "Malgorzata", ""], ["Berg", "Ewout van den", ""], ["Su", "Weijie", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1310.2076", "submitter": "Yunjin Choi", "authors": "Yunjin Choi and Robert Tibshirani", "title": "An Investigation of Methods for Handling Missing Data with Penalized\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate methods for penalized regression in the presence of missing\nobservations. This paper introduces a method for estimating the parameters\nwhich compensates for the missing observations. We first, derive an unbiased\nestimator of the objective function with respect to the missing data and then,\nmodify the criterion to ensure convexity. Finally, we extend our approach to a\nfamily of models that embraces the mean imputation method. These approaches are\ncompared to the mean imputation method, one of the simplest methods for dealing\nwith missing observations problem, via simulations. We also investigate the\nproblem of making predictions when there are missing values in the test set.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 10:26:30 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Choi", "Yunjin", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1310.2236", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "Analysis of AneuRisk65 data: warped logistic discrimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the AneuRisk65 curvature functions using a likelihood-based\nwarping method for sparsely sampled curves, and combine it with logistic\nregression in order to discriminate subjects with aneurysms at or after the\nterminal bifurcation of the internal carotid artery (the most life-threatening)\nfrom subjects with no aneurysms or aneurysms along the carotid artery (the less\nserious). Significantly lower misclassification rates are obtained when the\nwarping functions are included in the logistic discrimination model, rather\nthan being treated as mere nuisance parameters.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 19:41:42 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Gervini", "Daniel", ""]]}, {"id": "1310.2534", "submitter": "Nicholas Heard", "authors": "Nicholas Heard and Melissa Turcotte", "title": "Monte Carlo convergence of rival samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often necessary to make sampling-based statistical inference about many\nprobability distributions in parallel. Given a finite computational resource,\nthis article addresses how to optimally divide sampling effort between the\nsamplers of the different distributions. Formally approaching this decision\nproblem requires both the specification of an error criterion to assess how\nwell each group of samples represent their underlying distribution, and a loss\nfunction to combine the errors into an overall performance score. For the first\npart, a new Monte Carlo divergence error criterion based on Jensen-Shannon\ndivergence is proposed. Using results from information theory, approximations\nare derived for estimating this criterion for each target based on a single\nrun, enabling adaptive sample size choices to be made during sampling.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 16:16:51 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2014 21:33:06 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 14:39:58 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 09:21:08 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Heard", "Nicholas", ""], ["Turcotte", "Melissa", ""]]}, {"id": "1310.2598", "submitter": "Jonathan Landy", "authors": "Jonathan Landy", "title": "Statistical Mechanics of Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling often involves identifying an optimal estimate to some\nunderlying probability distribution known to satisfy some given constraints. I\nshow here that choosing as estimate the centroid, or center of mass, of the set\nconsistent with the constraints formally minimizes an objective measure of the\nexpected error. Further, I obtain a useful approximation to this point, valid\nin the thermodynamic limit, that immediately provides much information relating\nto the full solution set's geometry. For weak constraints, the centroid is\nclose to the popular maximum entropy solution, whereas for strong constraints\nthe two are far apart. Because of this, centroid inference is often\nsubstantially more accurate. The results I present allow for its\nstraightforward application.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 22:22:47 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Landy", "Jonathan", ""]]}, {"id": "1310.2816", "submitter": "Jun Zhu", "authors": "Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang", "title": "Gibbs Max-margin Topic Models with Data Augmentation", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-margin learning is a powerful approach to building classifiers and\nstructured output predictors. Recent work on max-margin supervised topic models\nhas successfully integrated it with Bayesian topic models to discover\ndiscriminative latent semantic structures and make accurate predictions for\nunseen testing data. However, the resulting learning problems are usually hard\nto solve because of the non-smoothness of the margin loss. Existing approaches\nto building max-margin supervised topic models rely on an iterative procedure\nto solve multiple latent SVM subproblems with additional mean-field assumptions\non the desired posterior distributions. This paper presents an alternative\napproach by defining a new max-margin loss. Namely, we present Gibbs max-margin\nsupervised topic models, a latent variable Gibbs classifier to discover hidden\ntopic representations for various tasks, including classification, regression\nand multi-task learning. Gibbs max-margin supervised topic models minimize an\nexpected margin loss, which is an upper bound of the existing margin loss\nderived from an expected prediction rule. By introducing augmented variables\nand integrating out the Dirichlet variables analytically by conjugacy, we\ndevelop simple Gibbs sampling algorithms with no restricting assumptions and no\nneed to solve SVM subproblems. Furthermore, each step of the\n\"augment-and-collapse\" Gibbs sampling algorithms has an analytical conditional\ndistribution, from which samples can be easily drawn. Experimental results\ndemonstrate significant improvements on time efficiency. The classification\nperformance is also significantly improved over competitors on binary,\nmulti-class and multi-label classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 13:47:40 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Ning", ""], ["Perkins", "Hugh", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2888", "submitter": "Oksana Chkrebtii", "authors": "Oksana A. Chkrebtii, Erin K. Cameron, David A. Campbell, Erin M. Bayne", "title": "Transdimensional Approximate Bayesian Computation for Inference on\n  Invasive Species Models with Latent Variables of Unknown Dimension", "comments": "includes supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate information on patterns of introduction and spread of non-native\nspecies is essential for making predictions and management decisions. In many\ncases, estimating unknown rates of introduction and spread from observed data\nrequires evaluating intractable variable-dimensional integrals. In general,\ninference on the large class of models containing latent variables of large or\nvariable dimension precludes exact sampling techniques. Approximate Bayesian\ncomputation (ABC) methods provide an alternative to exact sampling but rely on\ninefficient conditional simulation of the latent variables. To accomplish this\ntask efficiently, a new transdimensional Monte Carlo sampler is developed for\napproximate Bayesian model inference and used to estimate rates of introduction\nand spread for the non-native earthworm species Dendrobaena octaedra (Savigny)\nalong roads in the boreal forest of northern Alberta. Using low and high\nestimates of introduction and spread rates, the extent of earthworm invasions\nin northeastern Alberta was simulated to project the proportion of suitable\nhabitat invaded in the year following data collection.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 17:09:38 GMT"}, {"version": "v2", "created": "Sat, 20 Sep 2014 18:44:49 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 21:53:44 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Chkrebtii", "Oksana A.", ""], ["Cameron", "Erin K.", ""], ["Campbell", "David A.", ""], ["Bayne", "Erin M.", ""]]}, {"id": "1310.2905", "submitter": "Christian P. Robert", "authors": "E. Moreno, F.-J. Vazquez-Polo, and C.P. Robert", "title": "Two discussions of the paper \"Bayesian measures of model complexity and\n  fit\" by D. Spiegelhalter et al., Read before The Royal Statistical Society at\n  a meeting organized by the Research Section on Wednesday, March 13th, 2002", "comments": "4 pages, to appear in the Journal of the Royal Statistical Society,\n  Series B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the written discussions of the paper \"Bayesian measures of model\ncomplexity and fit\" by D. Spiegelhalter et al. (2002), following the\ndiscussions given at the Annual Meeting of the Royal Statistical Society in\nNewcastle-upon-Tyne on September 3rd, 2013.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 18:22:37 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2013 08:14:52 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Moreno", "E.", ""], ["Vazquez-Polo", "F. -J.", ""], ["Robert", "C. P.", ""]]}, {"id": "1310.2926", "submitter": "Maria Rizzo", "authors": "Gabor J. Szekely and Maria L. Rizzo", "title": "Partial Distance Correlation with Methods for Dissimilarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance covariance and distance correlation are scalar coefficients that\ncharacterize independence of random vectors in arbitrary dimension. Properties,\nextensions, and applications of distance correlation have been discussed in the\nrecent literature, but the problem of defining the partial distance correlation\nhas remained an open question of considerable interest. The problem of partial\ndistance correlation is more complex than partial correlation partly because\nthe squared distance covariance is not an inner product in the usual linear\nspace. For the definition of partial distance correlation we introduce a new\nHilbert space where the squared distance covariance is the inner product. We\ndefine the partial distance correlation statistics with the help of this\nHilbert space, and develop and implement a test for zero partial distance\ncorrelation. Our intermediate results provide an unbiased estimator of squared\ndistance covariance, and a neat solution to the problem of distance correlation\nfor dissimilarities rather than distances.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 19:23:29 GMT"}, {"version": "v2", "created": "Sat, 22 Mar 2014 18:33:42 GMT"}, {"version": "v3", "created": "Tue, 8 Jul 2014 20:25:50 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Szekely", "Gabor J.", ""], ["Rizzo", "Maria L.", ""]]}, {"id": "1310.2931", "submitter": "Stefan Wager", "authors": "Stefan Wager, Nick Chamandy, Omkar Muralidharan, and Amir Najmi", "title": "Feedback Detection for Live Predictors", "comments": "Advances in Neural Information Processing Systems (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predictor that is deployed in a live production system may perturb the\nfeatures it uses to make predictions. Such a feedback loop can occur, for\nexample, when a model that predicts a certain type of behavior ends up causing\nthe behavior it predicts, thus creating a self-fulfilling prophecy. In this\npaper we analyze predictor feedback detection as a causal inference problem,\nand introduce a local randomization scheme that can be used to detect\nnon-linear feedback in real-world problems. We conduct a pilot study for our\nproposed methodology using a predictive system currently deployed as a part of\na search engine.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 19:57:45 GMT"}, {"version": "v2", "created": "Sat, 1 Nov 2014 01:48:35 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Wager", "Stefan", ""], ["Chamandy", "Nick", ""], ["Muralidharan", "Omkar", ""], ["Najmi", "Amir", ""]]}, {"id": "1310.3161", "submitter": "Ayse Kizilersu", "authors": "Markus Kreer, Ayse Kizilersu, Anthony W. Thomas", "title": "Fractional Poisson processes and their representation by infinite\n  systems of ordinary differential equations", "comments": "15 pages", "journal-ref": "Statistics and Probability Letters84 (2014), pp. 27-32", "doi": "10.1016/j.spl.2013.09.028", "report-no": "ADP-13-16/T836", "categories": "math.CA math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional Poisson processes, a rapidly growing area of non-Markovian\nstochastic processes, are useful in statistics to describe data from counting\nprocesses when waiting times are not exponentially distributed. We show that\nthe fractional Kolmogorov-Feller equations for the probabilities at time t can\nbe representated by an infinite linear system of ordinary differential\nequations of first order in a transformed time variable. These new equations\nresemble a linear version of the discrete coagulation-fragmentation equations,\nwell-known from the non-equilibrium theory of gelation, cluster-dynamics and\nphase transitions in physics and chemistry.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 15:16:42 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Kreer", "Markus", ""], ["Kizilersu", "Ayse", ""], ["Thomas", "Anthony W.", ""]]}, {"id": "1310.3403", "submitter": "Emanuela Dreassi prof", "authors": "Ray Chambers, Emanuela Dreassi, Nicola Salvati", "title": "Disease Mapping via Negative Binomial Regression M-quantiles", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": "10.1002/sim.6256", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a semi-parametric approach to ecological regression for disease\nmapping, based on modelling the regression M-quantiles of a Negative Binomial\nvariable. The proposed method is robust to outliers in the model covariates,\nincluding those due to measurement error, and can account for both spatial\nheterogeneity and spatial clustering. A simulation experiment based on the\nwell-known Scottish lip cancer data set is used to compare the M-quantile\nmodelling approach and a random effects modelling approach for disease mapping.\nThis suggests that the M-quantile approach leads to predicted relative risks\nwith smaller root mean square error than standard disease mapping methods. The\npaper concludes with an illustrative application of the M-quantile approach,\nmapping low birth weight incidence data for English Local Authority Districts\nfor the years 2005-2010.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2013 16:45:33 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Chambers", "Ray", ""], ["Dreassi", "Emanuela", ""], ["Salvati", "Nicola", ""]]}, {"id": "1310.3574", "submitter": "Pritam Ranjan", "authors": "Neil A. Spencer, Pritam Ranjan, Franklin Mendivil", "title": "Isomorphism Check for $2^n$ Factorial Designs with Randomization\n  Restrictions", "comments": "25 pages (accepted in Journal of Statistical Theory and Practice)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial designs with randomization restrictions are often used in\nindustrial experiments when a complete randomization of trials is impractical.\nIn the statistics literature, the analysis, construction and isomorphism of\nfactorial designs has been extensively investigated. Much of the work has been\non a case-by-case basis -- addressing completely randomized designs, randomized\nblock designs, split-plot designs, etc. separately. In this paper we take a\nmore unified approach, developing theoretical results and an efficient\nrelabeling strategy to both construct and check the isomorphism of multi-stage\nfactorial designs with randomization restrictions. The examples presented in\nthis paper particularly focus on split-lot designs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 06:41:37 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 02:45:24 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 01:18:08 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Spencer", "Neil A.", ""], ["Ranjan", "Pritam", ""], ["Mendivil", "Franklin", ""]]}, {"id": "1310.3791", "submitter": "Robert Cousins", "authors": "Robert D. Cousins", "title": "The Jeffreys-Lindley Paradox and Discovery Criteria in High Energy\n  Physics", "comments": "v4: Continued editing for clarity. Figure added. v5: Minor fixes to\n  biblio. Same as published version except for minor copy-edits, Synthese\n  (2014). v6: fix typos, and restore garbled sentence at beginning of Sec 4 to\n  v3", "journal-ref": "Synthese 194 395-432 (2017) (published first online 30 July 2014)\n  erratum doi:10.1007/s11229-015-0687-3", "doi": "10.1007/s11229-014-0525-z", "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jeffreys-Lindley paradox displays how the use of a p-value (or number of\nstandard deviations z) in a frequentist hypothesis test can lead to an\ninference that is radically different from that of a Bayesian hypothesis test\nin the form advocated by Harold Jeffreys in the 1930s and common today. The\nsetting is the test of a well-specified null hypothesis (such as the Standard\nModel of elementary particle physics, possibly with \"nuisance parameters\")\nversus a composite alternative (such as the Standard Model plus a new force of\nnature of unknown strength). The p-value, as well as the ratio of the\nlikelihood under the null hypothesis to the maximized likelihood under the\nalternative, can strongly disfavor the null hypothesis, while the Bayesian\nposterior probability for the null hypothesis can be arbitrarily large. The\nacademic statistics literature contains many impassioned comments on this\nparadox, yet there is no consensus either on its relevance to scientific\ncommunication or on its correct resolution. The paradox is quite relevant to\nfrontier research in high energy physics. This paper is an attempt to explain\nthe situation to both physicists and statisticians, in the hope that further\nprogress can be made.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 19:03:59 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 20:32:14 GMT"}, {"version": "v3", "created": "Sat, 1 Mar 2014 01:44:39 GMT"}, {"version": "v4", "created": "Sat, 28 Jun 2014 15:07:08 GMT"}, {"version": "v5", "created": "Sat, 9 Aug 2014 16:24:47 GMT"}, {"version": "v6", "created": "Sat, 23 Aug 2014 15:14:31 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Cousins", "Robert D.", ""]]}, {"id": "1310.3899", "submitter": "Yuan Liao", "authors": "Jianqing Fan, Yuan Liao, Jiawei Yao", "title": "Power Enhancement in High Dimensional Cross-Sectional Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique to boost the power of testing a high-dimensional\nvector $H:\\btheta=0$ against sparse alternatives where the null hypothesis is\nviolated only by a couple of components. Existing tests based on quadratic\nforms such as the Wald statistic often suffer from low powers due to the\naccumulation of errors in estimating high-dimensional parameters. More powerful\ntests for sparse alternatives such as thresholding and extreme-value tests, on\nthe other hand, require either stringent conditions or bootstrap to derive the\nnull distribution and often suffer from size distortions due to the slow\nconvergence. Based on a screening technique, we introduce a \"power enhancement\ncomponent\", which is zero under the null hypothesis with high probability, but\ndiverges quickly under sparse alternatives. The proposed test statistic\ncombines the power enhancement component with an asymptotically pivotal\nstatistic, and strengthens the power under sparse alternatives. The null\ndistribution does not require stringent regularity conditions, and is\ncompletely determined by that of the pivotal statistic. As a byproduct, the\npower enhancement component also consistently identifies the elements that\nviolate the null hypothesis. As specific applications, the proposed methods are\napplied to testing the factor pricing models and validating the cross-sectional\nindependence in panel data models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 02:02:31 GMT"}, {"version": "v2", "created": "Sat, 16 Aug 2014 18:55:29 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Fan", "Jianqing", ""], ["Liao", "Yuan", ""], ["Yao", "Jiawei", ""]]}, {"id": "1310.4066", "submitter": "Nancy L. Garcia", "authors": "Ronaldo Dias, Nancy L. Garcia, Guilherme Ludwig and Marley A. Saraiva", "title": "Aggregated functional data model for Near-Infrared Spectroscopy\n  calibration and prediction", "comments": "27 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration and prediction for NIR spectroscopy data are performed based on a\nfunctional interpretation of the Beer-Lambert formula. Considering that, for\neach chemical sample, the resulting spectrum is a continuous curve obtained as\nthe summation of overlapped absorption spectra from each analyte plus a\nGaussian error, we assume that each individual spectrum can be expanded as a\nlinear combination of B-splines basis. Calibration is then performed using two\nprocedures for estimating the individual analytes curves: basis smoothing and\nsmoothing splines. Prediction is done by minimizing the square error of\nprediction. To assess the variance of the predicted values, we use a\nleave-one-out jackknife technique. Departures from the standard error models\nare discussed through a simulation study, in particular, how correlated errors\nimpact on the calibration step and consequently on the analytes' concentration\nprediction. Finally, the performance of our methodology is demonstrated through\nthe analysis of two publicly available datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 14:17:57 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Dias", "Ronaldo", ""], ["Garcia", "Nancy L.", ""], ["Ludwig", "Guilherme", ""], ["Saraiva", "Marley A.", ""]]}, {"id": "1310.4195", "submitter": "Bani Mallick", "authors": "Lin Zhang, Abhra Sarkar, Bani K. Mallick", "title": "Bayesian Low Rank and Sparse Covariance Matrix Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating high-dimensional covariance matrices of\na particular structure, which is a summation of low rank and sparse matrices.\nThis covariance structure has a wide range of applications including factor\nanalysis and random effects models. We propose a Bayesian method of estimating\nthe covariance matrices by representing the covariance model in the form of a\nfactor model with unknown number of latent factors. We introduce binary\nindicators for factor selection and rank estimation for the low rank component\ncombined with a Bayesian lasso method for the sparse component estimation.\nSimulation studies show that our method can recover the rank as well as the\nsparsity of the two components respectively. We further extend our method to a\ngraphical factor model where the graphical model of the residuals as well as\nselecting the number of factors is of interest. We employ a hyper-inverse\nWishart prior for modeling decomposable graphs of the residuals, and a Bayesian\ngraphical lasso selection method for unrestricted graphs. We show through\nsimulations that the extended models can recover both the number of latent\nfactors and the graphical model of the residuals successfully when the sample\nsize is sufficient relative to the dimension.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 20:41:40 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Zhang", "Lin", ""], ["Sarkar", "Abhra", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1310.4371", "submitter": "Weidong Liu", "authors": "Weidong Liu and Qi-Man Shao", "title": "Phase Transition and Regularized Bootstrap in Large Scale $t$-tests with\n  False Discovery Rate Control", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying Benjamini and Hochberg (B-H) method to multiple Student's $t$ tests\nis a popular technique in gene selection in microarray data analysis. Because\nof the non-normality of the population, the true p-values of the hypothesis\ntests are typically unknown. Hence, it is common to use the standard normal\ndistribution N(0,1), Student's $t$ distribution $t_{n-1}$ or the bootstrap\nmethod to estimate the p-values. In this paper, we first study N(0,1) and\n$t_{n-1}$ calibrations. We prove that, when the population has the finite 4-th\nmoment and the dimension $m$ and the sample size $n$ satisfy $\\log\nm=o(n^{1/3})$, B-H method controls the false discovery rate (FDR) at a given\nlevel $\\alpha$ asymptotically with p-values estimated from N(0,1) or $t_{n-1}$\ndistribution. However, a phase transition phenomenon occurs when $\\log m\\geq\nc_{0}n^{1/3}$. In this case, the FDR of B-H method may be larger than $\\alpha$\nor even tends to one. In contrast, the bootstrap calibration is accurate for\n$\\log m=o(n^{1/2})$ as long as the underlying distribution has the sub-Gaussian\ntails. However, such light tailed condition can not be weakened in general. The\nsimulation study shows that for the heavy tailed distributions, the bootstrap\ncalibration is very conservative. In order to solve this problem, a regularized\nbootstrap correction is proposed and is shown to be robust to the tails of the\ndistributions. The simulation study shows that the regularized bootstrap method\nperforms better than the usual bootstrap method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 13:37:04 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Liu", "Weidong", ""], ["Shao", "Qi-Man", ""]]}, {"id": "1310.4678", "submitter": "Michael Vogt", "authors": "Michael Vogt, Holger Dette", "title": "Detecting gradual changes in locally stationary processes", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1297 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 2, 713-740", "doi": "10.1214/14-AOS1297", "report-no": "IMS-AOS-AOS1297", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide range of applications, the stochastic properties of the observed\ntime series change over time. The changes often occur gradually rather than\nabruptly: the properties are (approximately) constant for some time and then\nslowly start to change. In many cases, it is of interest to locate the time\npoint where the properties start to vary. In contrast to the analysis of abrupt\nchanges, methods for detecting smooth or gradual change points are less\ndeveloped and often require strong parametric assumptions. In this paper, we\ndevelop a fully nonparametric method to estimate a smooth change point in a\nlocally stationary framework. We set up a general procedure which allows us to\ndeal with a wide variety of stochastic properties including the mean,\n(auto)covariances and higher moments. The theoretical part of the paper\nestablishes the convergence rate of the new estimator. In addition, we examine\nits finite sample performance by means of a simulation study and illustrate the\nmethodology by two applications to financial return data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 12:49:50 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 04:57:13 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Vogt", "Michael", ""], ["Dette", "Holger", ""]]}, {"id": "1310.4887", "submitter": "Justin Bleich", "authors": "Justin Bleich, Adam Kapelner, Edward I. George, Shane T. Jensen", "title": "Variable selection for BART: An application to gene regulation", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS755 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1750-1781", "doi": "10.1214/14-AOAS755", "report-no": "IMS-AOAS-AOAS755", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of discovering gene regulatory networks, which are\ndefined as sets of genes and the corresponding transcription factors which\nregulate their expression levels. This can be viewed as a variable selection\nproblem, potentially with high dimensionality. Variable selection is especially\nchallenging in high-dimensional settings, where it is difficult to detect\nsubtle individual effects and interactions between predictors. Bayesian\nAdditive Regression Trees [BART, Ann. Appl. Stat. 4 (2010) 266-298] provides a\nnovel nonparametric alternative to parametric regression approaches, such as\nthe lasso or stepwise regression, especially when the number of relevant\npredictors is sparse relative to the total number of available predictors and\nthe fundamental relationships are nonlinear. We develop a principled\npermutation-based inferential approach for determining when the effect of a\nselected predictor is likely to be real. Going further, we adapt the BART\nprocedure to incorporate informed prior information about variable importance.\nWe present simulations demonstrating that our method compares favorably to\nexisting parametric and nonparametric procedures in a variety of data settings.\nTo demonstrate the potential of our approach in a biological context, we apply\nit to the task of inferring the gene regulatory network in yeast (Saccharomyces\ncerevisiae). We find that our BART-based procedure is best able to recover the\nsubset of covariates with the largest signal compared to other variable\nselection methods. The methods developed in this work are readily available in\nthe R package bartMachine.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 02:38:25 GMT"}, {"version": "v2", "created": "Thu, 20 Mar 2014 00:31:03 GMT"}, {"version": "v3", "created": "Wed, 3 Dec 2014 08:38:24 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Bleich", "Justin", ""], ["Kapelner", "Adam", ""], ["George", "Edward I.", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1310.5103", "submitter": "Mu Zhu", "authors": "Wanhua Su, Yan Yuan, Mu Zhu", "title": "Threshold-free Evaluation of Medical Tests for Classification and\n  Prediction: Average Precision versus Area Under the ROC Curve", "comments": "The first two authors contributed equally to this paper, and should\n  be regarded as co-first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When evaluating medical tests or biomarkers for disease classification, the\narea under the receiver-operating characteristic (ROC) curve is a widely used\nperformance metric that does not require us to commit to a specific decision\nthreshold. For the same type of evaluations, a different metric known as the\naverage precision (AP) is used much more widely in the information retrieval\nliterature. We study both metrics in some depths in order to elucidate their\ndifference and relationship. More specifically, we explain mathematically why\nthe AP may be more appropriate if the earlier part of the ROC curve is of\ninterest. We also address practical matters, deriving an expression for the\nasymptotic variance of the AP, as well as providing real-world examples\nconcerning the evaluation of protein biomarkers for prostate cancer and the\nassessment of digital versus film mammography for breast cancer screening.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 17:28:39 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Su", "Wanhua", ""], ["Yuan", "Yan", ""], ["Zhu", "Mu", ""]]}, {"id": "1310.5227", "submitter": "Tomasz Suslo", "authors": "T. Suslo", "title": "Reformulating the Kriging Algorithm to the Practicing Miner", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main aim of applied geostatistics is to derive `mean' and `variance' of\nore to the practicing miner. This paper suggests brand-new approach to the\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2013 13:11:16 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Suslo", "T.", ""]]}, {"id": "1310.5288", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, John P. Cunningham", "title": "GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian\n  Processes", "comments": "13 Pages, 9 Figures, 1 Table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are typically used for smoothing and interpolation on\nsmall datasets. We introduce a new Bayesian nonparametric framework -- GPatt --\nenabling automatic pattern extrapolation with Gaussian processes on large\nmultidimensional datasets. GPatt unifies and extends highly expressive kernels\nand fast exact inference techniques. Without human intervention -- no hand\ncrafting of kernel features, and no sophisticated initialisation procedures --\nwe show that GPatt can solve large scale pattern extrapolation, inpainting, and\nkernel discovery problems, including a problem with 383400 training points. We\nfind that GPatt significantly outperforms popular alternative scalable Gaussian\nprocess methods in speed and accuracy. Moreover, we discover profound\ndifferences between each of these methods, suggesting expressive kernels,\nnonparametric representations, and exact inference are useful for modelling\nlarge scale multidimensional patterns.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2013 01:26:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 16:58:35 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 14:10:34 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Gilboa", "Elad", ""], ["Nehorai", "Arye", ""], ["Cunningham", "John P.", ""]]}, {"id": "1310.5336", "submitter": "Tsung-I Lin", "authors": "Tsung-I Lin, Pal H. Wu, Geoffrey J. McLachlan, Sharon X. Lee", "title": "The skew-t factor analysis model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis is a classical data reduction technique that seeks a\npotentially lower number of unobserved variables that can account for the\ncorrelations among the observed variables. This paper presents an extension of\nthe factor analysis model by assuming jointly a restricted version of\nmultivariate skew t distribution for the latent factors and unobservable\nerrors, called the skew-t factor analysis model. The proposed model shows\nrobustness to violations of normality assumptions of the underlying latent\nfactors and provides flexibility in capturing extra skewness as well as heavier\ntails of the observed data. A computationally feasible ECM algorithm is\ndeveloped for computing maximum likelihood estimates of the parameters. The\nusefulness of the proposed methodology is illustrated by a real-life example\nand results also demonstrates its better performance over various existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2013 14:58:34 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 14:49:46 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Lin", "Tsung-I", ""], ["Wu", "Pal H.", ""], ["McLachlan", "Geoffrey J.", ""], ["Lee", "Sharon X.", ""]]}, {"id": "1310.5440", "submitter": "Ozgur Asar", "authors": "Ozgur Asar, Ozlem Ilk", "title": "Marginally specified models for analyzing multivariate longitudinal\n  binary data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Marginally specified models have recently become a popular tool for discrete\nlongitudinal data analysis. Nonetheless, they introduce complex constraint\nequations and model fitting algorithms. Moreover, there is a lack of available\nsoftware to fit these models. In this paper, we propose a three-level\nmarginally specified model for analysis of multivariate longitudinal binary\nresponse data. The implicit function theorem is introduced to approximately\nsolve the marginal constraint equations explicitly. Furthermore, the use of\n\\textit{probit} link enables direct solutions to the convolution equations. We\npropose an R package \\textbf{pnmtrem} to fit the model. A simulation study is\nconducted to examine the properties of the estimator. We illustrate the model\non the Iowa Youth and Families Project data set.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 07:15:34 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 21:44:12 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Asar", "Ozgur", ""], ["Ilk", "Ozlem", ""]]}, {"id": "1310.5663", "submitter": "Steven Prestwich D", "authors": "S. D. Prestwich, R. Rossi, S. A. Tarim and B. Hnich", "title": "Mean-Based Error Measures for Intermittent Demand Forecasting", "comments": null, "journal-ref": "International Journal of Production Research, Taylor & Francis,\n  Vol. 52(22):6782-6791, 2014", "doi": "10.1080/00207543.2014.917771", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To compare different forecasting methods on demand series we require an error\nmeasure. Many error measures have been proposed, but when demand is\nintermittent some become inapplicable, some give counter-intuitive results, and\nthere is no agreement on which is best. We argue that almost all known measures\nrank forecasters incorrectly on intermittent demand series. We propose several\nnew error measures with wider applicability, and correct forecaster ranking on\nseveral intermittent demand patterns. We call these \"mean-based\" error measures\nbecause they evaluate forecasts against the (possibly time-dependent) mean of\nthe underlying stochastic process instead of point demands.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 12:53:20 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Prestwich", "S. D.", ""], ["Rossi", "R.", ""], ["Tarim", "S. A.", ""], ["Hnich", "B.", ""]]}, {"id": "1310.5677", "submitter": "Alex Goldstein", "authors": "Alex Goldstein, Andreas Buja", "title": "Penalized Split Criteria for Interpretable Trees", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes techniques for growing classification and regression\ntrees designed to induce visually interpretable trees. This is achieved by\npenalizing splits that extend the subset of features used in a particular\nbranch of the tree. After a brief motivation, we summarize existing methods and\nintroduce new ones, providing illustrative examples throughout. Using a number\nof real classification and regression datasets, we find that these procedures\ncan offer more interpretable fits than the CART methodology with very modest\nincreases in out-of-sample loss.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 19:06:00 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Goldstein", "Alex", ""], ["Buja", "Andreas", ""]]}, {"id": "1310.5726", "submitter": "Baptiste Gregorutti", "authors": "Baptiste Gregorutti, Bertrand Michel, Philippe Saint-Pierre", "title": "Correlation and variable importance in random forests", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-016-9646-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about variable selection with the random forests algorithm in\npresence of correlated predictors. In high-dimensional regression or\nclassification frameworks, variable selection is a difficult task, that becomes\neven more challenging in the presence of highly correlated predictors. Firstly\nwe provide a theoretical study of the permutation importance measure for an\nadditive regression model. This allows us to describe how the correlation\nbetween predictors impacts the permutation importance. Our results motivate the\nuse of the Recursive Feature Elimination (RFE) algorithm for variable selection\nin this context. This algorithm recursively eliminates the variables using\npermutation importance measure as a ranking criterion. Next various simulation\nexperiments illustrate the efficiency of the RFE algorithm for selecting a\nsmall number of variables together with a good prediction error. Finally, this\nselection algorithm is tested on the Landsat Satellite data from the UCI\nMachine Learning Repository.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 20:47:05 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2013 14:52:31 GMT"}, {"version": "v3", "created": "Tue, 11 Mar 2014 22:54:34 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 12:47:08 GMT"}, {"version": "v5", "created": "Mon, 18 Apr 2016 16:38:48 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Gregorutti", "Baptiste", ""], ["Michel", "Bertrand", ""], ["Saint-Pierre", "Philippe", ""]]}, {"id": "1310.5791", "submitter": "T. Tony Cai", "authors": "T. Tony Cai, Anru Zhang", "title": "ROP: Matrix recovery via rank-one projections", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1267 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 102-138", "doi": "10.1214/14-AOS1267", "report-no": "IMS-AOS-AOS1267", "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of low-rank matrices is of significant interest in a range of\ncontemporary applications. In this paper, we introduce a rank-one projection\nmodel for low-rank matrix recovery and propose a constrained nuclear norm\nminimization method for stable recovery of low-rank matrices in the noisy case.\nThe procedure is adaptive to the rank and robust against small perturbations.\nBoth upper and lower bounds for the estimation accuracy under the Frobenius\nnorm loss are obtained. The proposed estimator is shown to be rate-optimal\nunder certain conditions. The estimator is easy to implement via convex\nprogramming and performs well numerically. The techniques and main results\ndeveloped in the paper also have implications to other related statistical\nproblems. An application to estimation of spiked covariance matrices from\none-dimensional random projections is considered. The results demonstrate that\nit is still possible to accurately estimate the covariance matrix of a\nhigh-dimensional distribution based only on one-dimensional projections.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 03:30:29 GMT"}, {"version": "v2", "created": "Sun, 17 Aug 2014 13:20:54 GMT"}, {"version": "v3", "created": "Tue, 9 Dec 2014 10:24:46 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1310.5811", "submitter": "Mathew McLean", "authors": "Mathew W. McLean, Giles Hooker, David Ruppert", "title": "Restricted Likelihood Ratio Tests for Linearity in Scalar-on-Function\n  Regression", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-014-9473-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedure for testing the linearity of a scalar-on-function\nregression relationship. To do so, we use the functional generalized additive\nmodel (FGAM), a recently developed extension of the functional linear model.\nFor a functional covariate X(t), the FGAM models the mean response as the\nintegral with respect to t of F{X(t),t} where F is an unknown bivariate\nfunction. The FGAM can be viewed as the natural functional extension of\ngeneralized additive models. We show how the functional linear model can be\nrepresented as a simple mixed model nested within the FGAM. Using this\nrepresentation, we then consider restricted likelihood ratio tests for zero\nvariance components in mixed models to test the null hypothesis that the\nfunctional linear model holds. The methods are general and can also be applied\nto testing for interactions in a multivariate additive model or for testing for\nno effect in the functional linear model. The performance of the proposed tests\nis assessed on simulated data and in an application to measuring diesel truck\nemissions, where strong evidence of nonlinearities in the relationship between\nthe functional predictor and the response are found.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 06:42:25 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["McLean", "Mathew W.", ""], ["Hooker", "Giles", ""], ["Ruppert", "David", ""]]}, {"id": "1310.5927", "submitter": "Eve  Leconte", "authors": "Sandrine Casanova, Eve Leconte", "title": "A nonparametric model-based estimator for the cumulative distribution\n  function of a right censored variable in a finite population", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survey analysis, the estimation of the cumulative distribution function\n(cdf) is of great interest: it allows for instance to derive quantiles\nestimators or other non linear parameters derived from the cdf. We consider the\ncase where the response variable is a right censored duration variable. In this\nframework, the classical estimator of the cdf is the Kaplan-Meier estimator. As\nan alternative, we propose a nonparametric model-based estimator of the cdf in\na finite population. The new estimator uses auxiliary information brought by a\ncontinuous covariate and is based on nonparametric median regression adapted to\nthe censored case. The bias and variance of the prediction error of the\nestimator are estimated by a bootstrap procedure adapted to censoring. The new\nestimator is compared by model-based simulations to the Kaplan-Meier estimator\ncomputed with the sampled individuals: a significant gain in precision is\nbrought by the new method whatever the size of the sample and the censoring\nrate. Welfare duration data are used to illustrate the new methodology.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 14:11:52 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 12:52:25 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Casanova", "Sandrine", ""], ["Leconte", "Eve", ""]]}, {"id": "1310.5951", "submitter": "Jesse Windle", "authors": "Jesse Windle and Carlos M. Carvalho", "title": "A Tractable State-Space Model for Symmetric Positive-Definite Matrices", "comments": "22 pages: 16 pages main manuscript, 4 pages appendix, 2 pages\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian analysis of state-space models includes computing the posterior\ndistribution of the system's parameters as well as filtering, smoothing, and\npredicting the system's latent states. When the latent states wander around\n$\\mathbb{R}^n$ there are several well-known modeling components and\ncomputational tools that may be profitably combined to achieve these tasks.\nHowever, there are scenarios, like tracking an object in a video or tracking a\ncovariance matrix of financial assets returns, when the latent states are\nrestricted to a curve within $\\mathbb{R}^n$ and these models and tools do not\nimmediately apply. Within this constrained setting, most work has focused on\nfiltering and less attention has been paid to the other aspects of Bayesian\nstate-space inference, which tend to be more challenging. To that end, we\npresent a state-space model whose latent states take values on the manifold of\nsymmetric positive-definite matrices and for which one may easily compute the\nposterior distribution of the latent states and the system's parameters, in\naddition to filtered distributions and one-step ahead predictions. Deploying\nthe model within the context of finance, we show how one can use realized\ncovariance matrices as data to predict latent time-varying covariance matrices.\nThis approach out-performs factor stochastic volatility.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 15:12:42 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2013 16:55:17 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Windle", "Jesse", ""], ["Carvalho", "Carlos M.", ""]]}, {"id": "1310.5966", "submitter": "Sourabh Bhattacharya", "authors": "Noirrit K. Chandra and Sourabh Bhattacharya", "title": "Non-marginal Decisions: A Novel Bayesian Multiple Testing Procedure", "comments": "A significantly updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of multiple testing when the hypotheses\nare dependent. In most of the existing literature, either Bayesian or\nnon-Bayesian, the decision rules mainly focus on the validity of the test\nprocedure rather than actually utilizing the dependency to increase efficiency.\nMoreover, the decisions regarding different hypotheses are marginal in the\nsense that they do not depend upon each other directly. However, in realistic\nsituations, the hypotheses are usually dependent, and hence it is desirable\nthat the decisions regarding the dependent hypotheses are taken jointly.\n  In this article we develop a novel Bayesian multiple testing procedure that\ncoherently takes this requirement into consideration. Our method, which is\nbased on new notions of error and non-error terms, substantially enhances\nefficiency by judicious exploitation of the dependence structure among the\nhypotheses. We prove that our method minimizes the posterior expected loss\nassociated with a an additive \"0-1\" loss function, we also prove theoretical\nresults on the relevant error probabilities, establishing the coherence and\nusefulness of our method. The optimal decision configuration is not available\nin closed form and we propose a novel and efficient simulated annealing\nalgorithm for the purpose of optimization, which is also generically applicable\nto binary optimization problems.\n  Numerical studies demonstrate that in dependent situations, our method\nperforms significantly better than some existing popular conventional multiple\ntesting methods, in terms of accuracy and power control. Moreover, application\nof our ideas to a real, spatial data set associated with radionuclide\nconcentration in Rongelap islands yielded insightful results.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 15:48:35 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 13:37:32 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 18:01:02 GMT"}, {"version": "v4", "created": "Thu, 21 Sep 2017 16:40:56 GMT"}, {"version": "v5", "created": "Mon, 9 Jul 2018 16:10:04 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Chandra", "Noirrit K.", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1310.6150", "submitter": "Pierre Latouche", "authors": "P. Latouche and S Robin", "title": "Variational Bayes model averaging for graphon functions and motif\n  frequencies inference in W-graph models", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-015-9607-0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  W-graph refers to a general class of random graph models that can be seen as\na random graph limit. It is characterized by both its graphon function and its\nmotif frequencies. In this paper, relying on an existing variational Bayes\nalgorithm for the stochastic block models along with the corresponding weights\nfor model averaging, we derive an estimate of the graphon function as an\naverage of stochastic block models with increasing number of blocks. In the\nsame framework, we derive the variational posterior frequency of any motif. A\nsimulation study and an illustration on a social network complete our work.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 08:22:24 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 08:53:05 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Latouche", "P.", ""], ["Robin", "S", ""]]}, {"id": "1310.6224", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Ryan P. Browne and Paul D. McNicholas", "title": "A Mixture of SDB Skew-t Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1016/j.ecosta.2017.05.001", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of skew-t distributions offer a flexible choice for model-based\nclustering. A mixture model of this sort can be implemented using a variety of\nformulations of the skew-t distribution. Herein we develop a mixture of skew-t\nfactor analyzers model for clustering of high-dimensional data using a flexible\nformulation of the skew-t distribution. Methodological details of our approach,\nwhich represents an extension of the mixture of factor analyzers model to a\nflexible skew-t distribution, are outlined and details of parameter estimation\nare provided. Clustering results are illustrated and compared to an alternative\nformulation of the mixture of skew-t factor analyzers model as well as the\nmixture of factor analyzers model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 13:46:50 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 17:23:33 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Murray", "Paula M.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1310.6322", "submitter": "Zhishi Wang", "authors": "Zhishi Wang, Qiuling He, Bret Larget, Michael A. Newton", "title": "A multi-functional analyzer uses parameter constraints to improve the\n  efficiency of model-based gene-set analysis", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS777 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 225-246", "doi": "10.1214/14-AOAS777", "report-no": "IMS-AOAS-AOAS777", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model-based methodology for integrating gene-set information\nwith an experimentally-derived gene list. The methodology uses a previously\nreported sampling model, but takes advantage of natural constraints in the\nhigh-dimensional discrete parameter space in order to work from a more\nstructured prior distribution than is currently available. We show how the\nnatural constraints are expressed in terms of linear inequality constraints\nwithin a set of binary latent variables. Further, the currently available prior\ngives low probability to these constraints in complex systems, such as Gene\nOntology (GO), thus reducing the efficiency of statistical inference. We\ndevelop two computational advances to enable posterior inference within the\nconstrained parameter space: one using integer linear programming for\noptimization and one using a penalized Markov chain sampler. Numerical\nexperiments demonstrate the utility of the new methodology for a multivariate\nintegration of genomic data with GO or related information systems. Compared to\navailable methods, the proposed multi-functional analyzer covers more reported\ngenes without mis-covering nonreported genes, as demonstrated on genome-wide\ndata from association studies of type 2 diabetes and from RNA interference\nstudies of influenza.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 18:30:36 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 05:33:30 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Wang", "Zhishi", ""], ["He", "Qiuling", ""], ["Larget", "Bret", ""], ["Newton", "Michael A.", ""]]}, {"id": "1310.6602", "submitter": "Julie Josse", "authors": "Julie Josse and Sylvain Sardy", "title": "Adaptive Shrinkage of singular values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To recover a low rank structure from a noisy matrix, truncated singular value\ndecomposition has been extensively used and studied. Recent studies suggested\nthat the signal can be better estimated by shrinking the singular values. We\npursue this line of research and propose a new estimator offering a continuum\nof thresholding and shrinking functions. To avoid an unstable and costly\ncross-validation search, we propose new rules to select two thresholding and\nshrinking parameters from the data. In particular we propose a generalized\nStein unbiased risk estimation criterion that does not require knowledge of the\nvariance of the noise and that is computationally fast. A Monte Carlo\nsimulation reveals that our estimator outperforms the tested methods in terms\nof mean squared error on both low-rank and general signal matrices across\ndifferent signal to noise ratio regimes. In addition, it accurately estimates\nthe rank of the signal when it is detectable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 13:15:39 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 23:43:42 GMT"}, {"version": "v3", "created": "Sat, 22 Nov 2014 22:34:50 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Josse", "Julie", ""], ["Sardy", "Sylvain", ""]]}, {"id": "1310.6643", "submitter": "Alexander Torgovitsky", "authors": "Matthew A. Masten and Alexander Torgovitsky", "title": "Instrumental Variables Estimation of a Generalized Correlated Random\n  Coefficients Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study identification and estimation of the average treatment effect in a\ncorrelated random coefficients model that allows for first stage heterogeneity\nand binary instruments. The model also allows for multiple endogenous variables\nand interactions between endogenous variables and covariates. Our\nidentification approach is based on averaging the coefficients obtained from a\ncollection of ordinary linear regressions that condition on different\nrealizations of a control function. This identification strategy suggests a\ntransparent and computationally straightforward estimator of a trimmed average\ntreatment effect constructed as the average of kernel-weighted linear\nregressions. We develop this estimator and establish its\n$\\sqrt{n}$--consistency and asymptotic normality. Monte Carlo simulations show\nexcellent finite-sample performance that is comparable in precision to the\nstandard two-stage least squares estimator. We apply our results to analyze the\neffect of air pollution on house prices, and find substantial heterogeneity in\nfirst stage instrument effects as well as heterogeneity in treatment effects\nthat is consistent with household sorting.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 15:26:16 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 18:05:02 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Masten", "Matthew A.", ""], ["Torgovitsky", "Alexander", ""]]}, {"id": "1310.6957", "submitter": "Mingyi Hong", "authors": "Mingyi Hong, Xiangfeng Wang, Meisam Razaviyayn and Zhi-Quan Luo", "title": "Iteration Complexity Analysis of Block Coordinate Descent Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a unified iteration complexity analysis for a\nfamily of general block coordinate descent (BCD) methods, covering popular\nmethods such as the block coordinate gradient descent (BCGD) and the block\ncoordinate proximal gradient (BCPG), under various different coordinate update\nrules. We unify these algorithms under the so-called Block Successive\nUpper-bound Minimization (BSUM) framework, and show that for a broad class of\nmulti-block nonsmooth convex problems, all algorithms covered by the BSUM\nframework achieve a global sublinear iteration complexity of $O(1/r)$, where r\nis the iteration index. Moreover, for the case of block coordinate minimization\n(BCM) where each block is minimized exactly, we establish the sublinear\nconvergence rate of $O(1/r)$ without per block strong convexity assumption.\nFurther, we show that when there are only two blocks of variables, a special\nBSUM algorithm with Gauss-Seidel rule can be accelerated to achieve an improved\nrate of $O(1/r^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 15:30:04 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 14:30:08 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Hong", "Mingyi", ""], ["Wang", "Xiangfeng", ""], ["Razaviyayn", "Meisam", ""], ["Luo", "Zhi-Quan", ""]]}, {"id": "1310.7211", "submitter": "Scott Holan", "authors": "Aaron T. Porter, Christopher K. Wikle, Scott H. Holan", "title": "Small Area Estimation via Multivariate Fay-Herriot Models with Latent\n  Spatial Dependence", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fay-Herriot model is a standard model for direct survey estimators in\nwhich the true quantity of interest, the superpopulation mean, is latent and\nits estimation is improved through the use of auxiliary covariates. In the\ncontext of small area estimation, these estimates can be further improved by\nborrowing strength across spatial region or by considering multiple outcomes\nsimultaneously. We provide here two formulations to perform small area\nestimation with Fay-Herriot models that include both multivariate outcomes and\nlatent spatial dependence. We consider two model formulations, one in which the\noutcome-by-space dependence structure is separable and one that accounts for\nthe cross dependence through the use of a generalized multivariate conditional\nautoregressive (GMCAR) structure. The GMCAR model is shown in a state-level\nexample to produce smaller mean square prediction errors, relative to\nequivalent census variables, than the separable model and the state-of-the-art\nmultivariate model with unstructured dependence between outcomes and no spatial\ndependence. In addition, both the GMCAR and the separable models give smaller\nmean squared prediction error than the state-of-the-art model when conducting\nsmall area estimation on county level data from the American Community Survey.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2013 16:37:01 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Porter", "Aaron T.", ""], ["Wikle", "Christopher K.", ""], ["Holan", "Scott H.", ""]]}, {"id": "1310.7269", "submitter": "Patrick Perry", "authors": "Patrick O. Perry, Natesh S. Pillai", "title": "Degrees of freedom for combining regression with factor analysis", "comments": "34 pages, 8 figures; includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the AGEMAP genomics study, researchers were interested in detecting genes\nrelated to age in a variety of tissue types. After not finding many age-related\ngenes in some of the analyzed tissue types, the study was criticized for having\nlow power. It is possible that the low power is due to the presence of\nimportant unmeasured variables, and indeed we find that a latent factor model\nappears to explain substantial variability not captured by measured covariates.\nWe propose including the estimated latent factors in a multiple regression\nmodel. The key difficulty in doing so is assigning appropriate degrees of\nfreedom to the estimated factors to obtain unbiased error variance estimators\nand enable valid hypothesis testing. When the number of responses is large\nrelative to the sample size, treating the estimated factors like observed\ncovariates leads to a downward bias in the variance estimates. Many ad-hoc\nsolutions to this problem have been proposed in the literature without the\nbackup of a careful theoretical analysis. Using recent results from random\nmatrix theory, we derive a simple, easy to use expression for degrees of\nfreedom. Our estimate gives a principled alternative to ad-hoc approaches in\ncommon use. Extensive simulation results show excellent agreement between the\nproposed estimator and its theoretical value. Applying our methodology to the\nAGEMAP genomics study, we found an order of magnitude increase in the number of\nsignificant genes. Although we focus on the AGEMAP study, the methods developed\nin this paper are widely applicable to other multivariate models, and thus are\nof independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2013 23:13:56 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 15:01:25 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Perry", "Patrick O.", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1310.7643", "submitter": "Jorge M. Ramirez", "authors": "Jorge M. Ramirez, Enrique A. Thomann, Edward C. Waymire", "title": "Advection-Dispersion Across Interfaces", "comments": "Published in at http://dx.doi.org/10.1214/13-STS442 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 487-509", "doi": "10.1214/13-STS442", "report-no": "IMS-STS-STS442", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article concerns a systemic manifestation of small scale interfacial\nheterogeneities in large scale quantities of interest to a variety of diverse\napplications spanning the earth, biological and ecological sciences. Beginning\nwith formulations in terms of partial differential equations governing the\nconservative, advective-dispersive transport of mass concentrations in\ndivergence form, the specific interfacial heterogeneities are introduced in\nterms of (spatial) discontinuities in the diffusion coefficient across a\nlower-dimensional hypersurface. A pathway to an equivalent stochastic\nformulation is then developed with special attention to the interfacial effects\nin various functionals such as first passage times, occupation times and local\ntimes. That an appreciable theory is achievable within a framework of\napplications involving one-dimensional models having piecewise constant\ncoefficients greatly facilitates our goal of a gentle introduction to some\nrather dramatic mathematical consequences of interfacial effects that can be\nused to predict structure and to inform modeling.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 22:45:35 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 06:10:42 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Ramirez", "Jorge M.", ""], ["Thomann", "Enrique A.", ""], ["Waymire", "Edward C.", ""]]}, {"id": "1310.8037", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Ria Van Hecke, Stanislav Volgushev", "title": "Misspecification in copula-based regression", "comments": "Keywords: curse of dimensionality, semiparametric inference, copulae,\n  pairwise copulae, vine copulae 14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper Noh et al. (2013) proposed a new semiparametric estimate of\na regression function with a multivariate predictor, which is based on a\nspecification of the dependence structure between the predictor and the\nresponse by means of a parametric copula. This paper investigates the effect\nwhich occurs under misspecification of the parametric model. We demonstrate\nthat even for a one or two dimensional predictor the error caused by a \\wrong\"\nspecification of the parametric family is rather severe, if the regression is\nnot monotone in one of the components of the predictor. Moreover, we also show\nthat these problems occur for all of the commonly used copula families and we\nillustrate in several examples that the copula-based regression may lead to\ninvalid results even when more exible copula models such as vine copulae (with\nthe common parametric families) are used in the estimation procedure.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 06:35:15 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Dette", "Holger", ""], ["Van Hecke", "Ria", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "1310.8176", "submitter": "Cristian Meza", "authors": "Rolando De la Cruz, Cristian Meza, Ana Arribas-Gil and Raymond J.\n  Carroll", "title": "Bayesian Regression Analysis of Data with Random Effects Covariates from\n  Nonlinear Longitudinal Measurements", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models for a wide class of response variables and longitudinal\nmeasurements consist on a mixed-effects model to fit longitudinal trajectories\nwhose random effects enter as covariates in a generalized linear model for the\nprimary response. They provide a useful way to asses association between these\ntwo kinds of data, which in clinical studies are often collected jointly on a\nseries of individuals and may help understanding, for instance, the mechanisms\nof recovery of a certain disease or the efficacy of a given therapy. The most\ncommon joint model in this framework is based on a linear mixed model for the\nlongitudinal data. However, for complex datasets the linearity assumption may\nbe too restrictive. Some works have considered generalizing this setting with\nthe use of a nonlinear mixed-effects model for the longitudinal trajectories\nbut the proposed estimation procedures based on likelihood approximations have\nbeen shown De la Cruz et al. (2011) to exhibit some computational efficiency\nproblems. In this article we propose an MCMC-based estimation procedure in the\njoint model with a nonlinear mixed-effects model for the longitudinal data and\na generalized linear model for the primary response. Moreover, we consider that\nthe errors in the longitudinal model may be correlated. We apply our method to\nthe analysis of hormone levels measured at the early stages of pregnancy that\ncan be used to predict normal versus abnormal pregnancy outcomes. We also\nconduct a simulation study to asses the importance of modelling correlated\nerrors and quantify the consequences of model misspecification.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 14:48:17 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 13:55:55 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["De la Cruz", "Rolando", ""], ["Meza", "Cristian", ""], ["Arribas-Gil", "Ana", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1310.8192", "submitter": "Andrew Finley Dr.", "authors": "Andrew O. Finley, Sudipto Banerjee, Alan E.Gelfand", "title": "spBayes for large univariate and multivariate point-referenced\n  spatio-temporal data models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we detail the reformulation and rewrite of core functions in\nthe spBayes R package. These efforts have focused on improving computational\nefficiency, flexibility, and usability for point-referenced data models.\nAttention is given to algorithm and computing developments that result in\nimproved sampler convergence rate and efficiency by reducing parameter space;\ndecreased sampler run-time by avoiding expensive matrix computations, and;\nincreased scalability to large datasets by implementing a class of predictive\nprocess models that attempt to overcome computational hurdles by representing\nspatial processes in terms of lower-dimensional realizations. Beyond these\ngeneral computational improvements for existing model functions, we detail new\nfunctions for modeling data indexed in both space and time. These new functions\nimplement a class of dynamic spatio-temporal models for settings where space is\nviewed as continuous and time is taken as discrete.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 15:16:32 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Finley", "Andrew O.", ""], ["Banerjee", "Sudipto", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1310.8244", "submitter": "Maikol Sol\\'is", "authors": "Jean-Michel Loubes, Clement Marteau and Maikol Sol\\'is", "title": "Rates of convergence in conditional covariance matrix with nonparametric\n  entries estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X\\in \\mathbb{R}^p$ and $Y\\in \\mathbb{R}$ be two random variables. We\nestimate the conditional covariance matrix\n$\\mathrm{Cov}\\left(\\mathrm{E}\\left[\\boldsymbol{X}\\vert Y\\right]\\right)$\napplying a plug-in kernel-based algorithm to its entries. Next, we investigate\nthe estimators rate of convergence under smoothness hypotheses on the density\nfunction of $(\\boldsymbol{X},Y)$. In a high-dimensional context, we improve the\nconsistency the whole matrix estimator by providing a decreasing structure over\nthe $\\mathrm{Cov}\\left(\\mathrm{E}\\left[\\boldsymbol{X}\\vert Y\\right]\\right)$\nentries. We illustrate a sliced inverse regression setting for time series\nmatching the conditions of our estimator\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 17:50:37 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 16:26:15 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2015 10:33:42 GMT"}, {"version": "v4", "created": "Fri, 9 Feb 2018 20:19:26 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Loubes", "Jean-Michel", ""], ["Marteau", "Clement", ""], ["Sol\u00eds", "Maikol", ""]]}, {"id": "1310.8339", "submitter": "Santu Ghosh", "authors": "Santu Ghosh and Alan M. Polansky", "title": "Smoothed and Iterated Bootstrap Confidence Regions for Parameter Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of confidence regions for parameter vectors is a difficult\nproblem in the nonparametric setting, particularly when the sample size is not\nlarge. The bootstrap has shown promise in solving this problem, but empirical\nevidence often indicates that some bootstrap methods have difficulty in\nmaintaining the correct coverage probability, while other methods may be\nunstable, often resulting in very large confidence regions. One way to improve\nthe performance of a bootstrap confidence region is to restrict the shape of\nthe region in such a way that the error term of an expansion is as small an\norder as possible. To some extent, this can be achieved by using the bootstrap\nto construct an ellipsoidal confidence region. This paper studies the effect of\nusing the smoothed and iterated bootstrap methods to construct an ellipsoidal\nconfidence region for a parameter vector. The smoothed estimate is based on a\nmultivariate kernel density estimator. This paper establishes a bandwidth\nmatrix for the smoothed bootstrap procedure that reduces the asymptotic\ncoverage error of the bootstrap percentile method ellipsoidal confidence\nregion. We also provide an analytical adjustment to the nominal level to reduce\nthe computational cost of the iterated bootstrap method. Simulations\ndemonstrate that the methods can be successfully applied in practice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 22:39:13 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Ghosh", "Santu", ""], ["Polansky", "Alan M.", ""]]}, {"id": "1310.8633", "submitter": "Zuofeng Shang", "authors": "Guang Cheng, Hao Helen Zhang and Zuofeng Shang", "title": "Sparse and Efficient Estimation for Partial Spline Models with\n  Increasing Dimension", "comments": "34 pages, 6 figures, 10 tables, published at Annals of the Institute\n  of Statistical Mathematics 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider model selection and estimation for partial spline models and\npropose a new regularization method in the context of smoothing splines. The\nregularization method has a simple yet elegant form, consisting of roughness\npenalty on the nonparametric component and shrinkage penalty on the parametric\ncomponents, which can achieve function smoothing and sparse estimation\nsimultaneously. We establish the convergence rate and oracle properties of the\nestimator under weak regularity conditions. Remarkably, the estimated\nparametric components are sparse and efficient, and the nonparametric component\ncan be estimated with the optimal rate. The procedure also has attractive\ncomputational properties. Using the representer theory of smoothing splines, we\nreformulate the objective function as a LASSO-type problem, enabling us to use\nthe LARS algorithm to compute the solution path. We then extend the procedure\nto situations when the number of predictors increases with the sample size and\ninvestigate its asymptotic properties in that context. Finite-sample\nperformance is illustrated by simulations.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 18:41:42 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 03:31:48 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Cheng", "Guang", ""], ["Zhang", "Hao Helen", ""], ["Shang", "Zuofeng", ""]]}]